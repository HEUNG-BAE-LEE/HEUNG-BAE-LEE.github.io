{"meta":{"title":"DataLatte's IT Blog","subtitle":"DataLatte's IT Blog using Hexo","description":"DataLatteê°€ Data Science ê³µë¶€ë¥¼ í•˜ë©´ì„œ ì •ë¦¬í•´ ë†“ëŠ” ë¸”ë¡œê·¸","author":"HeungBae Lee","url":"https://heung-bae-lee.github.io","root":"/"},"pages":[],"posts":[{"title":"ë‚´ê°€ ì •ë¦¬í•˜ëŠ” ìë£Œêµ¬ì¡° 06 - í™(heap)","slug":"data_structure_07","date":"2020-05-17T14:39:31.000Z","updated":"2020-05-18T08:18:34.415Z","comments":true,"path":"2020/05/17/data_structure_07/","link":"","permalink":"https://heung-bae-lee.github.io/2020/05/17/data_structure_07/","excerpt":"","text":"12 ê²°ê³¼ 12","categories":[],"tags":[]},{"title":"Linear combination, vector equation, Four views of matrix multiplication","slug":"linear_algebra_03","date":"2020-05-14T06:36:03.000Z","updated":"2020-05-16T10:52:45.868Z","comments":true,"path":"2020/05/14/linear_algebra_03/","link":"","permalink":"https://heung-bae-lee.github.io/2020/05/14/linear_algebra_03/","excerpt":"","text":"Linear combination, vector equation, Four views of matrix multiplication ì•„ë˜ ë‚´ìš©ì€ ê¹€ë„í˜• ë°•ì‚¬ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜ ê°•ì˜ì•ˆ edwithì˜ ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ ì„ í˜•ëŒ€ìˆ˜ ê°•ì˜ì™€ KOCWì˜ í•œì–‘ëŒ€í•™êµ ì´ìƒí™” êµìˆ˜ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ ê°•ì˜ë¥¼ ë³´ê³  ì •ë¦¬í•œ ë‚´ìš©ì´ë‹¤. ì•ì„œ ì–¸ê¸‰í–ˆë˜ ê²ƒê³¼ ê°™ì´ ë²¡í„°ì— ìŠ¤ì¹¼ë¼ë¥¼ ê³±í•˜ì—¬ ë”í•œ ì¡°í•©ì´ ì„ í˜• ì¡°í•©ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ì¼ë°˜ì ìœ¼ë¡œ ì—°ë¦½ë°©ì •ì‹ìœ¼ë¡œ ìƒê°í•˜ê¸° ë³´ë‹¤ëŠ” column formìœ¼ë¡œ ë‚˜íƒ€ë‚´ linear combinationìœ¼ë¡œ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ solutionì„ ì°¾ëŠ”ë° ê¸°í•˜í•™ì ìœ¼ë¡œë‚˜ ì§ê´€ì ìœ¼ë¡œ í•´ì„í•˜ê¸° ë” ì‰½ë‹¤. ì´ì œ ë¶€í„° ê·¸ ì´ìœ ì— ëŒ€í•´ì„œ ì‚´í´ ë³¼ ê²ƒì´ë‹¤. ì´ì „ì—ëŠ” ì•„ë˜ ìˆ˜ì‹ì—ì„œ Uniqueí•œ Solutionì´ ì¡´ì¬í•˜ë ¤ë©´ í–‰ë ¬ Aì— ëŒ€í•œ ì—­í–‰ë ¬ì´ ì¡´ì¬í•´ì•¼ í•œë‹¤ê³  ì„¤ëª…í–ˆë‹¤. ìˆ˜ì‹ì ìœ¼ë¡œ ì´í•´ë¥¼ í–ˆë‹¤ë©´ ì´ì œëŠ” ê¸°í•˜í•™ì ìœ¼ë¡œ ì‚´í´ë³´ì. Solutionì˜ ì§‘í•©ì´ ì–´ë–¤ í˜•íƒœì¸ì§€ ì–´ë””ì— ì†í•´ìˆëŠ”ì§€ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € Vector Spaceì— ëŒ€í•´ ì•Œê³  ìˆì–´ì•¼í•œë‹¤. Vector Space space : set(ì§‘í•©) closed under addition and scalar multiplication ì¦‰, ë§ì…ˆê³¼ ê³±ì…ˆì— ëŒ€í•˜ì—¬ ë‹«í˜€ìˆëŠ” ì§‘í•©ì„ Spaceë¼ê³  í•œë‹¤. ìœ„ì—ì„œì˜ ë²¡í„°ì˜ ì„ í˜•ì¡°í•©ì„ ë³´ë©´ scalarì™€ì˜ ê³±ì…ˆì—ì˜í•´ ë‹«í˜€ìˆê³  ë²¡í„°ê°„ì˜ ë§ì…ˆì— ëŒ€í•´ ë‹«í˜€ ìˆìœ¼ë¯€ë¡œ í•˜ë‚˜ì˜ spaceë¥¼ ê°–ëŠ”ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì¢€ ë” ì—„ë°€íˆ ë§í•˜ìë©´ ë‹¤ìŒì„ ë§Œì¡±í•´ì•¼ í•œë‹¤. for any vectors $x, y \\in R^{n}$, any scalar $c \\in R$ 1) $x+y = y+x$ 2) $x+(y+z) = (x+y)+z$ 3) There is a zero-vector such that $x+o = o+x = x $ ì—¬ê¸°ì„œ 0(zero-vector)ëŠ” ë²¡í„°ì˜ ë§ì…ˆì—°ì‚°ì— ëŒ€í•œ í•­ë“±ì›ì´ë‹¤. ì´ ì²˜ëŸ¼ í•­ë“±ì›ì´ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•  ê²½ìš°ì—ëŠ” í•­ìƒ, êµí™˜ë²•ì¹™ì´ ì„±ë¦½í•˜ëŠ” ì§€ë„ í™•ì¸ í•´ì•¼í•œë‹¤. ìœ„ì˜ ì‚¬ì‹¤ë¡œ ë¶€í„° vector Spaceê°€ ë˜ë ¤ë©´ ì›ì ì„ í¬í•¨í•´ì•¼í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì°¸ê³ ë¡œ ì›ì ì„ í¬í•¨í•˜ì§€ ì•ŠëŠ” ê³µê°„ì„ ì•„í•€ ê³µê°„(Affine Space)ë¼ê³  í•œë‹¤. 4) For each vectore $x, x+(-x) = (-x) + x = o$ $-x$ ë²¡í„°ì˜ ë§ì…ˆ ì—°ì‚°ì— ëŒ€í•œ ì—­ì›ì´ë‹¤. ë˜í•œ -xëŠ” uniqueí•˜ë‹¤. 5) $1 \\cdot x=x$ 6) $c(x+y) = cx+cy$ 7) $(c_{1}+c_{2})x=c_{1}x+c_{2}x$ ì´í•´ê°€ ì˜ ê°€ì§€ ì•Šì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì˜ˆì‹œë¥¼ ë“¤ìë©´ 2ì°¨ ë‹¤í•­í•¨ìˆ˜ê°€ ì•„ë˜ì™€ ê°™ì´ ì¡´ì¬í•  ë•Œì˜ vector SpaceëŠ” 3ì°¨ì›ì´ë‹¤. \\begin{align} a x^{2}+ b x +c \\Rightarrow \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} \\in R^{3} \\end{align}Subspace ì„ì˜ì˜ ë²¡í„°ë“¤ì— ëŒ€í•œ ì„ í˜•ì¡°í•©ì˜ ê²°ê³¼ë„ ì—¬ì „íˆ ê·¸ setì— ëŒ€í•´ ë‹«í˜€ ìˆìœ¼ë©´ì„œë„ ì „ì²´ Vector Spaceì— ë¶€ë¶„ì§‘í•©ì„ Subspaceë¼ê³  í•œë‹¤. ì„ì˜ì˜ ë²¡í„°ë“¤ì„ ê°€ì§€ê³  ì˜¨ê°– ì¢…ë¥˜ì˜ linear combinationì„ í•´ì„œ vector spaceë¥¼ ë§Œë“¤ì–´ ë‚´ëŠ” ê²ƒì„ Spanningì´ë¼ê³  í•˜ê³ , ê³µê°„ì„ ë§Œë“  ë²¡í„°ë“¤ì€ íŠ¹ì •í•œ vector spaceë¥¼ spaní•œë‹¤ë¼ê³  í‘œí˜„í•œë‹¤. all linear combination of vectors $\\{ v_{1}, v_{2}, \\cdots ,v_{n} \\}$ construct a vector space $= \\{v_{1}, v_{2}, \\cdots ,v_{n} \\}$ span vector space Spanì˜ ê°œë…ì— ëŒ€í•œ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ì•„ë˜ì™€ ê°™ì€ ë‘ ê°œì˜ vectorë¥¼ ê°–ê³  ì–´ë–¤ í˜•íƒœë¡œ linear combinationì„ í•˜ë˜ì§€ spaní•  ìˆ˜ ìˆëŠ” vector ê³µê°„ì€ x, y í‰ë©´ì´ë¼ëŠ” 2ì°¨ì› ê³µê°„ë°–ì— ì•ˆëœë‹¤. ê° ë²¡í„°ì˜ ì°¨ì›ì€ 3ì°¨ì›ì´ì§€ë§Œ ì „ì²´ vector spaceì˜ ì°¨ì›ì€ 2ì´ë‹¤. ì•„ë˜ì˜ ë²¡í„°ë“¤ì˜ linear combinationì€ 2ì°¨ì› ê³µê°„(x,y í‰ë©´)ì„ spaní•œë‹¤. ë˜‘ê°™ì€ vector spaceë¼ê³  í•˜ë”ë¼ê³  spaní•˜ëŠ” vectorë“¤ì˜ ì¡°í•©ì€ ë¬´í•œí•˜ë‹¤. \\begin{align} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\end{align}\\begin{align} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} \\end{align}\\begin{align} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix} \\end{align}Column Space of A (C(A)) ì„ì˜ì˜ í–‰ë ¬ Aì˜ ì—´ ë²¡í„°ì˜ ëª¨ë“  ì„ í˜•ì¡°í•©(Linear combination)ì˜ ì§‘í•©ì„ Column Spaceë¼ê³  í•œë‹¤. \\begin{align} A = \\begin{bmatrix} a_{1} & a_{2} & \\cdots & a_{N} \\end{bmatrix} \\Rightarrow \\sum^{n}_{i=1} c_{i} a_{i} \\end{align}\\begin{align} Ax= \\begin{bmatrix} a_{1} & a_{2} & \\cdots & a_{M} \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{M} \\end{bmatrix} = x_1 a_1 + x_2 a_2 + \\cdots + x_M a_M = b \\end{align} if $b \\in C(A)$, there is at least one solution. else if $b \\notin C(A)$ ë²¡í„° $x$ë¼ëŠ” í•´ê°€ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì€ ëª¨ë“  Combination ì¤‘ í•˜ë‚˜ê°€ ë²¡í„° $b$ì´ë¼ëŠ” ì´ì•¼ê¸°ì´ë¯€ë¡œ ë²¡í„° $b$ê°€ column spaceì— ì†í•˜ë©´ í•´ê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì´ê³  ì†í•˜ì§€ ì•Šìœ¼ë©´ í•´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. spaní•œ ê²°ê³¼ê°€ ëª¨ë“  spaceë¥¼ ë§Œë“œëŸ¬ ë‚´ëŠëƒ ê·¸ë ‡ì§€ ì•ŠëŠ”ëƒëŠ” í–‰ë ¬ Aì˜ inverseê°€ ì¡´ì¬í•˜ëŠëƒ ì•ˆí•˜ëŠ”ëƒì— ë‹¬ë ¤ ìˆë‹¤. $Ax=b$ ì—ì„œ uniqueí•œ solutionì´ ì¡´ì¬í•  ì¡°ê±´ì€ ì„ì˜ì˜ í–‰ë ¬ $A$ì— ëŒ€í•˜ì—¬ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ëŠ” ê²ƒì´ë‹¤. ì—­í–‰ë ¬($A^{-1}$)ì´ ì¡´ì¬í•œë‹¤ë©´ í•­ìƒ $b \\in C(A)$ì´ë‹¤. left Null spaceì™€ ìƒí˜¸ ë³´ì™„ê´€ê³„ ì „ì²´ Spaceì˜ ì°¨ì› = Column spaceì˜ ì°¨ì› + left Null spaceì˜ ì°¨ì› Example) ì•„ë˜ì™€ ê°™ì€ í–‰ë ¬ $A$ëŠ” ì—­í–‰ë ¬(A^{-1})ì´ ì¡´ì¬í•˜ì§€ ì•Šê¸°ì— ëª¨ë“  3ì°¨ì› ê³µê°„ì˜ ë²¡í„°ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ solutionì€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë¬´ìˆ˜íˆ ë§ì„ ê²ƒì´ë‹¤. \\begin{align} A= \\begin{bmatrix} 1 & -2 & 1 \\\\ 2 & 1 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix} \\end{align} ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ non-zero ë²¡í„°ì´ë©° ì„œë¡œ ìƒìˆ˜ë°°ë¡œ ë§Œë“¤ì–´ì§ˆ ìˆ˜ ì—†ëŠ” 3ì°¨ì› í¬ê¸°ì˜ $v_{1}, v_{2}$ ë²¡í„°ë“¤ì€ 3ì°¨ì› ìƒì—ì„œ í‰ë©´ì„ spaní•œë‹¤. ë§Œì•½ ë²¡í„° $b$ê°€ column ë²¡í„°ë“¤ì˜ linear combinationìœ¼ë¡œ í‘œí˜„ë˜ì–´ì§ˆ ìˆ˜ ìˆë‹¤ë©´, column spaceì•ˆì— ë²¡í„° $b$ê°€ í¬í•¨ë˜ì–´ìˆë‹¤ëŠ” ì´ì•¼ê¸°ê°€ ëœë‹¤. Null Space of A (N(A)) Set of vectors such that $Ax = 0$, $N(A)=\\{ x| A x = 0 \\}$ spaceëŠ” ë§ì…ˆê³¼ ê³±ì…ˆì— ëŒ€í•˜ì—¬ ë‹«í˜€ìˆì–´ì•¼ í•˜ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ ì¦ëª…í•´ ë³´ì¼ ìˆ˜ ìˆë‹¤. 1) closed under addition for $Ax_{1}=0, Ax_{2}=0$ $x_{1}+x_{2} \\in N(A)$ì´ë ¤ë©´, $A(x_{1}+x_{2})=0$ì¸ì§€ í™•ì¸í•´ì£¼ë©´ ëœë‹¤. $Ax_{1} +Ax_{2} = 0 + 0 = 0$ ì´ë¯€ë¡œ $x_{1}+x_{2} \\in N(A)$ì´ë‹¤. 2) closed under scalar multiplication for $Ax=0, for any c cx \\in N(A)?$ $A(cx)=cAx=0$ì´ë¯€ë¡œ $cx \\in N(A)$ì´ë‹¤. Row spaceì™€ ìƒí˜¸ ë³´ì™„ê´€ê³„ ì „ì²´ Spaceì˜ ì°¨ì› = Row spaceì˜ ì°¨ì› + Null spaceì˜ ì°¨ì› ë¹„ëŒ€ì¹­ êµ¬ì¡°ì¸ rectangular systemì—ì„œëŠ” ë¯¸ì§€ìˆ˜í•˜ê³  ë°©ì •ì‹ì˜ ê°œìˆ˜ê°€ ë˜‘ê°™ì§€ ì•Šì€ ë¹„ëŒ€ì¹­ì ì¸ êµ¬ì¡°ì´ê¸° ë•Œë¬¸ì— í•´ê°€ ì—†ì„ ë•Œë„ ìˆê² ì§€ë§Œ í•´ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ëŠ” ë¬´ìˆ˜íˆ ë§ë‹¤. ë¬´ìˆ˜íˆ ë§ì€ í•´ ì§‘í•©ì¸ ê²½ìš° $Ax=b$ì— ëŒ€í•œ í•´ëŠ” $Ax=0$ì¸ ë°©ì •ì‹ì˜ í•´ì§‘í•©ê³¼ ìƒë‹¹íˆ ì—°ê´€ì´ ë†’ì€ë° ê·¸ ì´ìœ ëŠ” $Ax=b$ì— ëŒ€í•œ í•´ì§‘í•©ì€ $Ax=0$ì¸ ê²½ìš°ì— ëŒ€í•œ í•´ì¸ Null spaceì™€ ë‚˜ë¨¸ì§€ë¡œ ë‚˜ëˆ„ì–´ì§€ê¸° ë•Œë¬¸ì´ë‹¤. \\begin{align} \\boxed{\\begin{matrix} \\phantom{} & \\phantom{} & \\phantom{} & \\phantom{} & \\phantom{} \\\\ & & M & &\\\\ \\phantom{} & \\phantom{} & \\phantom{} & \\phantom{} & \\phantom{} \\\\ \\end{matrix}} \\, \\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ v \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} = \\boxed{\\begin{matrix} \\phantom{} \\\\ Mv \\\\ \\phantom{} \\end{matrix}} \\end{align}Rectangular systemì—ì„œì˜ í•´ì§‘í•© = Null space + particular solution ì§€ê¸ˆë¶€í„° ê·¸ì— ëŒ€í•œ ì˜ˆì‹œë¥¼ í’€ë©´ì„œ ì´í•´í•´ ë³¼ ê²ƒì´ë‹¤. ì•„ë˜ ìˆ˜ì‹ì— ëŒ€í•œ í•´ì§‘í•©ì„ êµ¬í•´ë³´ì. \\begin{align} \\begin{bmatrix} 1 & 3 & 3 & 2 \\\\ 2 & 6 & 9 & 7 \\\\ -1 & -3 & 3 & 4 \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\\\ w \\\\ z \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\end{align} ê°€ìš°ìŠ¤ ì†Œê±°ë²•ì„ ì‚¬ìš©í•˜ì—¬ í–‰ë ¬ì„ í’€ì–´ì¤€ë‹¤. ì²« ë²ˆì§¸ í–‰ì— 2ë¥¼ ê³±í•˜ì—¬ ë‘ ë²ˆì§¸ í–‰ì— ë¹¼ì¤€ë‹¤. ì²« ë²ˆì§¸ í–‰ì„ ì„¸ ë²ˆì§¸ í–‰ì— ë”í•´ì¤€ë‹¤. ìœ„ì˜ 2ë‹¨ê³„ë¥¼ ê±°ì¹˜ë©´ ì•„ë˜ì™€ ê°™ì€ í–‰ë ¬ì„ ì–»ê²Œ ë  ê²ƒì´ë‹¤. ì•„ë˜ í–‰ë ¬ì²˜ëŸ¼ A_{2,2}=0ì´ ë˜ì–´ pivotì´ ë˜ì§€ ëª»í•˜ë¯€ë¡œ ê·¸ ë‹¤ìŒ 0ì´ ì•„ë‹Œ ìˆ˜ê°€ ìˆëŠ” A_{2,3}ì´ pivotì˜ ìë¦¬ë¥¼ ê°–ëŠ”ë‹¤. \\begin{align} \\begin{bmatrix} 1 & 3 & 3 & 2 \\\\ 0 & 0 & 3 & 3 \\\\ 0 & 0 & 6 & 6 \\end{bmatrix} \\end{align} ë‘ ë²ˆì§¸ í–‰ì—ì„œ 2ë¥¼ ê³±í•´ ì„¸ ë²ˆì§¸ í–‰ì—ì„œ ë¹¼ì¤€ë‹¤. \\begin{align} \\begin{bmatrix} 1 & 3 & 3 & 2 \\\\ 0 & 0 & 3 & 3 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\\\ w \\\\ z \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\end{align} ëª¨ë“  pivotì˜ ìë¦¬ë¥¼ 1ë¡œ ë§Œë“¤ì–´ ì£¼ì–´ì•¼ í•˜ë¯€ë¡œ ë‘ ë²ˆì§¸ í–‰ì„ 3ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¤€ë‹¤. \\begin{align} \\begin{bmatrix} 1 & 3 & 3 & 2 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\end{align} ë§ˆì§€ë§‰ìœ¼ë¡œ pivotì´ ì¡´ì¬í•˜ëŠ” columnì— pivotì´ ì¡´ì¬í•˜ëŠ” ìë¦¬ë¥¼ ì œì™¸í•˜ê³ ëŠ” 0ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ê¸° ìœ„í•´ ìœ„ì—ì„œ ì†Œê±°í•´ì¤€ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì •ë¦¬í•´ì¤€ë‹¤. ì†Œê±°í•´ì£¼ê¸° ìœ„í•´ ë‘ ë²ˆì§¸ í–‰ì— 3ì„ ê³±í•œë’¤ ì²« ë²ˆì§¸ í–‰ì—ì„œ ë¹¼ì£¼ë©´ ì•„ë˜ì™€ ê°™ì€ í–‰ë ¬ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. \\begin{align} \\begin{bmatrix} 1 & 3 & 0 & -1 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\end{align} ìµœì¢…ì ìœ¼ë¡œ ì–»ì€ í–‰ë ¬ì„ ë‹¤ì‹œ ì²˜ìŒ ë°©ì •ì‹ì— ë§ì¶°ì„œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. pivot variable : u, w free variable : v, z \\begin{align} \\begin{bmatrix} 1 & 3 & 0 & -1 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\\\ w \\\\ z \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix} \\end{align} ìœ„ì˜ ë°©ì •ì‹ì„ í’€ì–´ ì¨ì„œ row formìœ¼ë¡œ ë°”ê¿” ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤. ë˜í•œ pivot variableì„ free variableì˜ í˜•íƒœë¡œ í‘œí˜„í•˜ë©´ í–‰ë ¬ì˜ Null spaceë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ë˜í•œ pivot variableì˜ ê°œìˆ˜ê°€ í•´ë‹¹ í–‰ë ¬ì˜ Null spaceì˜ ì°¨ì›ì´ë‹¤. ì˜ˆì‹œì˜ í–‰ë ¬ì€ $Dim(N(A))=2$ì´ë‹¤. u + 3v -z =0 \\\\ w+z = 0u = =3v + z \\\\ w = -z\\begin{align} \\begin{bmatrix} u \\\\ v \\\\ w \\\\ z \\end{bmatrix} = \\begin{bmatrix} -3v+z \\\\ v \\\\ -z \\\\ z \\end{bmatrix} \\end{align} ìœ„ì™€ ê°™ì´ ë°©ì •ì‹ì„ ëª¨ë‘ í’€ì§€ ì•Šì•„ë„ ê·¸ ì „ Row Reduced form í–‰ë ¬ì—ì„œ Null spaceë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. Row Reduced formì—ì„œ free variableë“¤ì˜ columnì— ê³„ìˆ˜ë“¤ì„ pivot variableì¸ ê²½ìš°ì—ëŠ” ë¶€í˜¸ë¥¼ ë°”ê¿”ì£¼ê³  ìê¸° ìì‹ ì˜ ìˆœì„œì¸ ìë¦¬ì—ëŠ” 1ì„ ë‚˜ë¨¸ì§€ free variable ìë¦¬ì—ëŠ” 0ì„ ë„£ì–´ì£¼ë©´ ëœë‹¤. \\begin{align} \\begin{bmatrix} 1 & 3 & 0 & -1 \\\\ 0 & 0 & 1 & 1 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix} \\end{align} ìì„¸í•˜ê²Œ ì„¤ëª…í•˜ìë©´ pivot variableì„ free variableì˜ í˜•íƒœë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ë¯€ë¡œ ë¨¼ì € free variableì˜ columnì¸ ë‘ ë²ˆì§¸ ì—´ê³¼ ë„¤ ë²ˆì§¸ ì—´ë§Œ ê°€ì ¸ì™€ ìœ„ì˜ ë°©ë²• ì²˜ëŸ¼ ì§„í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ë‘ free variable ë²¡í„°ì˜ ì¡°í•©ì„ êµ¬í•  ìˆ˜ ìˆìœ¼ë©° ì´ ì¡°í•©ì´ Null spaceì´ë‹¤. \\begin{align} v \\begin{bmatrix} -3 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + z \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 1 \\end{bmatrix} \\in N(A) \\end{align} ë§Œì•½ ìœ„ì˜ ì˜ˆì‹œ ë¬¸ì œê°€ $Ax=b$ë¥¼ í‘¸ëŠ” ë¬¸ì œë¼ê³  ê°€ì •í•´ë³´ì. \\begin{align} \\begin{bmatrix} 1 & 3 & 3 & 2 \\\\ 2 & 6 & 9 & 7 \\\\ -1 & -3 & 3 & 4 \\end{bmatrix} \\begin{bmatrix} u \\\\ v \\\\ w \\\\ z \\end{bmatrix} = \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ b_{3} \\end{bmatrix} \\end{align}\\begin{align} \\begin{bmatrix} 1 & 3 & 3 & 2 | b_{1} \\\\ 0 & 0 & 3 & 3 | b_{2} - 2b_{1} \\\\ 0 & 0 & 6 & 6 | b_{3}+b_{1} \\end{bmatrix} \\end{align} í–‰ë ¬ Aì˜ column vectorë“¤ì˜ linear combinationì´ $b$ì´ì–´ì•¼ í•˜ë¯€ë¡œ í•´ê°€ ì¡´ì¬í•œë‹¤ë©´ $b \\in C(A)$ì¼ ê²ƒì´ë‹¤. ê·¸ëŸ°ë° ì—¬ê¸°ì„œëŠ” ê·¸ $b$ë¼ëŠ” ë²¡í„°ê°€ $5b_{1} - 2b_{2} + b_{3}=0$ì„ì„ ë§Œì¡±í•´ì•¼ í•´ê°€ ì¡´ì¬í•œë‹¤. ê·¸ ì´ìœ ëŠ” ì•„ë˜ í–‰ë ¬ì„ ì‚´í´ë³´ë©´ ë§ˆì§€ë§‰ í–‰ì€ ë¯¸ì§€ìˆ˜ë“¤ê³¼ ê³±ì„ í•´ë„ 0ì´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. \\begin{align} \\begin{bmatrix} 1 & 3 & 3 & 2 | b_{1} \\\\ 0 & 0 & 3 & 3 | b_{2} - 2b_{1} \\\\ 0 & 0 & 0 & 0 | 5b_{1} - 2b_{2} + b_{3} \\end{bmatrix} \\end{align} $5b_{1} - 2b_{2} + b_{3}=0$ë¥¼ ë§Œì¡±í•˜ëŠ” ë²¡í„° $b$ëŠ” í•´ë¥¼ ì°¾ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ê±´ì„ ë§Œì¡±ì‹œí‚¤ëŠ” ì¡°í•©ë§Œ ì°¾ëŠ”ë‹¤ë©´ í•´ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ í•´ëŠ” ë¬´í•œí•˜ë‹¤. ì•„ë˜ì˜ ì˜ˆì‹œì—ì„œëŠ” ë²¡í„° $b$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ í•  ê²ƒì´ë‹¤. \\begin{align} \\begin{bmatrix} 1 \\\\ 5 \\\\ 5 \\end{bmatrix} \\end{align}\\begin{align} \\begin{bmatrix} 1 & 3 & 3 & 2 | 1 \\\\ 0 & 0 & 3 & 3 | 3 \\\\ 0 & 0 & 0 & 0 | 0 \\end{bmatrix} \\end{align}\\begin{align} \\begin{bmatrix} 1 & 3 & 3 & 2 | 1 \\\\ 0 & 0 & 1 & 1 | 1 \\\\ 0 & 0 & 0 & 0 | 0 \\end{bmatrix} \\end{align}\\begin{align} \\begin{bmatrix} 1 & 3 & 0 & -1 | -2 \\\\ 0 & 0 & 1 & 1 | 1 \\\\ 0 & 0 & 0 & 0 | 0 \\end{bmatrix} \\end{align} $ X = Null space + particular solution$ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ìˆìŒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ì›ë˜ ì›ì ì„ ì§€ë‚˜ëŠ” í‰ë©´ì¸ Null spaceì´ì—ˆëŠ”ë° particular solutionë§Œí¼ í‰ìƒì´ë™ëœ í‰ë©´ì´ í•´ ì§‘í•©ì´ ë˜ì—ˆë‹¤ëŠ” ê²ƒìœ¼ë¡œ í•´ì„ í•  ìˆ˜ ìˆë‹¤. \\begin{align} \\begin{bmatrix} u \\\\ v \\\\ w \\\\ z \\end{bmatrix} = v \\begin{bmatrix} -3 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} + z \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 1 \\end{bmatrix} + \\begin{bmatrix} -2 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix} \\in N(A) \\end{align} í–‰ë ¬ì˜ ê³±ì„ ì´ì œëŠ” linear combinationì˜ ê´€ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì‚´í´ë³´ì. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë³´ë“¯ì´ í–‰ë ¬ì„ column vectorë¡œ ë‚˜ëˆ„ì–´ linear combinationìœ¼ë¡œ í–‰ë ¬ì˜ ê³±ì„ ë³¼ ìˆ˜ ìˆë‹¤. ì•„ë˜ì™€ ê°™ì´ column vectorë§ê³  row vectorë¥¼ ì‚¬ìš©í•œ ê³±ì˜ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. ìœ„ì˜ ê²½ìš°ì™€ì˜ ì°¨ì´ëŠ” ì™¼ìª½ì˜ í–‰ë ¬ì— ê´€í•œ ê´€ì ìœ¼ë¡œ ë°”ë¼ë³¼ë•ŒëŠ” column vectorë“¤ì˜ linear combination ì˜¤ë¥¸ìª½ì˜ í–‰ë ¬ì˜ ê´€ì ìœ¼ë¡œ ê³±ì„ ê³„ì‚°í•  ê²½ìš°ëŠ” row vectorë“¤ì˜ linear combinationìœ¼ë¡œ ê³„ì‚° í•  ìˆ˜ ìˆë‹¤. ìœ„ì˜ ê°œë…ë“¤ì„ ìƒê°í•˜ì—¬ ì™¸ì ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ ë‘ í–‰ë ¬ì˜ ì™¸ì ì„ 2ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê³„ì‚°í•˜ë©´ ì†ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.","categories":[{"name":"linear algebra","slug":"linear-algebra","permalink":"https://heung-bae-lee.github.io/categories/linear-algebra/"}],"tags":[]},{"title":"ì„ í˜• ì‹œìŠ¤í…œ(Linear system)","slug":"linear_algebra_02","date":"2020-05-13T07:50:33.000Z","updated":"2020-05-14T13:42:58.924Z","comments":true,"path":"2020/05/13/linear_algebra_02/","link":"","permalink":"https://heung-bae-lee.github.io/2020/05/13/linear_algebra_02/","excerpt":"","text":"ì•„ë˜ ë‚´ìš©ì€ ê¹€ë„í˜• ë°•ì‚¬ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜ ê°•ì˜ì•ˆ edwithì˜ ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ ì„ í˜•ëŒ€ìˆ˜ ê°•ì˜ì™€ KOCWì˜ í•œì–‘ëŒ€í•™êµ ì´ìƒí™” êµìˆ˜ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ ê°•ì˜ë¥¼ ë³´ê³  ì •ë¦¬í•œ ë‚´ìš©ì´ë‹¤. ì„ í˜• ì¡°í•© ë²¡í„°/í–‰ë ¬ì— ë‹¤ìŒì²˜ëŸ¼ ìŠ¤ì¹¼ë¼ê°’ì„ ê³±í•œ í›„ ë”í•˜ê±°ë‚˜ ëº€ ê²ƒì„ ë²¡í„°/í–‰ë ¬ì˜ ì„ í˜•ì¡°í•©(linear combination)ì´ë¼ê³  í•œë‹¤. ë²¡í„°ë‚˜ í–‰ë ¬ì„ ì„ í˜•ì¡°í•©í•´ë„ í¬ê¸°ëŠ” ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤. \\begin{align} c_1x_1 + c_2x_2 + c_3x_3 + \\cdots + c_Lx_L = x \\end{align}\\begin{align} c_1A_1 + c_2A_2 + c_3A_3 + \\cdots + c_LA_L = A \\end{align}\\begin{align} c_1, c_2, \\ldots, c_L \\in \\mathbf{R} \\end{align}\\begin{align} x_1, x_2, \\ldots, x_L, x \\in \\mathbf{R}^M \\end{align}\\begin{align} A_1, A_2, \\ldots, A_L, A \\in \\mathbf{R}^{M \\times N} \\end{align} ë²¡í„°ë‚˜ í–‰ë ¬ì˜ í¬ê¸°ë¥¼ ì§ì‚¬ê°í˜•ìœ¼ë¡œ í‘œì‹œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} \\begin{matrix} c_1\\,\\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ x_1 \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} & + & c_2\\,\\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ x_2 \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} & + & \\cdots \\!\\!\\!\\!& + & c_L\\,\\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ x_L \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} \\end{matrix} \\end{align}\\begin{align} \\begin{matrix} c_1\\,\\boxed{\\begin{matrix} \\phantom{} & \\phantom{} & \\phantom{} \\\\ & A_1 & \\\\ \\phantom{} & \\phantom{} & \\phantom{} \\end{matrix}} & + & c_2\\,\\boxed{\\begin{matrix} \\phantom{} & \\phantom{} & \\phantom{} \\\\ & A_2 & \\\\ \\phantom{} & \\phantom{} & \\phantom{} \\end{matrix}} & + & \\cdots & + & c_L\\,\\boxed{\\begin{matrix} \\phantom{} & \\phantom{} & \\phantom{} \\\\ & A_L & \\\\ \\phantom{} & \\phantom{} & \\phantom{} \\end{matrix}} \\end{matrix} \\end{align}í–‰ë ¬ê³¼ ë²¡í„°ì˜ ê³± í–‰ë ¬ì˜ ê³±ì…ˆ ì¤‘ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ê²ƒì€ ì•„ë˜ì™€ ê°™ì€ í˜•íƒœì˜ í–‰ë ¬ $M$ê³¼ ë²¡í„° $v$ì˜ ê³±ì´ë‹¤. \\begin{align} \\boxed{\\begin{matrix} \\phantom{} & \\phantom{} & \\phantom{} & \\phantom{} & \\phantom{} \\\\ & & M & &\\\\ \\phantom{} & \\phantom{} & \\phantom{} & \\phantom{} & \\phantom{} \\\\ \\end{matrix}} \\, \\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ v \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} = \\boxed{\\begin{matrix} \\phantom{} \\\\ Mv \\\\ \\phantom{} \\end{matrix}} \\end{align}ì—´ ë²¡í„°ì˜ ì„ í˜•ì¡°í•© í–‰ë ¬ Xì™€ ë²¡í„° $w$ì˜ ê³±ì€ í–‰ë ¬ $X$ë¥¼ ì´ë£¨ëŠ” ì—´ë²¡í„° $c_{1}, c_{2}, \\ldots ,c_{M}$ì„ ë’¤ì— ê³±í•´ì§€ëŠ” ë²¡í„° $w$ì˜ ê° ì„±ë¶„ $w_{1}, w_{1}, \\ldots, w_{M}$ìœ¼ë¡œ ì„ í˜•ì¡°í•©(linear combination)ì„ í•œ ê²°ê³¼ ë²¡í„°ì™€ ê°™ë‹¤. \\begin{align} Xw= \\begin{bmatrix} c_1 & c_2 & \\cdots & c_M \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_M \\end{bmatrix} = w_1 c_1 + w_2 c_2 + \\cdots + w_M c_M \\end{align} ë²¡í„°ì˜ í¬ê¸°ë¥¼ ì§ì‚¬ê°í˜•ìœ¼ë¡œ í‘œì‹œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} \\begin{bmatrix} \\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ c_{1_{\\phantom{1}}} \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} & \\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ c_{2_{\\phantom{1}}} \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} & \\cdots & \\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ c_M \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_M \\end{bmatrix} \\\\ &=& \\begin{matrix} w_1\\,\\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ c_{1_{\\phantom{1}}} \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} & + & w_2\\,\\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ c_{2_{\\phantom{1}}} \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}}& + & \\cdots \\!\\!\\!\\!& + & w_M\\,\\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ c_M \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} \\end{matrix} \\end{align} ë²¡í„°ì˜ ì„ í˜•ì¡°í•©ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì‘ìš©ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‘ ì´ë¯¸ì§€ ë²¡í„°ì˜ ì„ í˜•ì¡°í•©ì€ ë‘ ì´ë¯¸ì§€ë¥¼ ì„ì–´ë†“ì€ ëª¨í•‘(morphing)íš¨ê³¼ë¥¼ ì–»ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 1234567891011121314151617181920212223242526from sklearn.datasets import fetch_olivetti_facesfaces = fetch_olivetti_faces()f, ax = plt.subplots(1, 3)ax[0].imshow(faces.images[6], cmap=plt.cm.bone)ax[0].grid(False)ax[0].set_xticks([])ax[0].set_yticks([])ax[0].set_title(\"image 1: $x_1$\")ax[1].imshow(faces.images[10], cmap=plt.cm.bone)ax[1].grid(False)ax[1].set_xticks([])ax[1].set_yticks([])ax[1].set_title(\"image 2: $x_2$\")new_face = 0.7 * faces.images[6] + 0.3 * faces.images[10]ax[2].imshow(new_face, cmap=plt.cm.bone)ax[2].grid(False)ax[2].set_xticks([])ax[2].set_yticks([])ax[2].set_title(\"image 3: $0.7x_1 + 0.3x_2$\")plt.show() ì„ í˜• ì‹œìŠ¤í…œ ì˜ˆì‹œì—¬ëŸ¬ ê°œì˜ ë²¡í„°ì— ëŒ€í•œ ê°€ì¤‘í•© ë™ì‹œ ê³„ì‚° ë²¡í„° í•˜ë‚˜ì˜ ê°€ì¤‘í•©ì€ $w^{T} x$ ë˜ëŠ” $x^{T}w$ë¡œ í‘œì‹œí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë°°ì› ë‹¤. ê·¸ëŸ°ë° ë§Œì•½ ì´ë ‡ê²Œ $w$ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•œ ê°€ì¤‘í•©ì„ í•˜ë‚˜ì˜ ë²¡í„° $x$ê°€ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ë²¡í„° $x_{1}, x_{2}, \\ldots, x_{M}$ê°œì— ëŒ€í•´ì„œ ëª¨ë‘ ê³„ì‚°í•´ì•¼ í•œë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ? ì˜ˆë¥¼ ë“¤ì–´ ì„ í˜•íšŒê·€ ëª¨í˜•ì„ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ë°ì´í„° $x_{1}, x_{2}, \\ldots, x_{N}$ê°œì˜ ë°ì´í„° ëª¨ë‘ì— ëŒ€í•´ ì˜ˆì¸¡ê°’ $y_{1}, y_{2}, \\ldots, y_{M}$ì„ í•œêº¼ë²ˆì— ê³„ì‚°í•˜ê³  ì‹¶ë‹¤ë©´ ë‹¤ìŒì²˜ëŸ¼ ë°ì´í„° í–‰ë ¬ $X$ë¥¼ ì‚¬ìš©í•˜ì—¬ $\\hat{y} = Xw$ë¼ëŠ” ìˆ˜ì‹ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. \\begin{align} \\begin{aligned} \\hat{y} = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\vdots \\\\ \\hat{y}_M \\\\ \\end{bmatrix} &= \\begin{bmatrix} w_1 x_{1,1} + w_2 x_{1,2} + \\cdots + w_N x_{1,N} \\\\ w_1 x_{2,1} + w_2 x_{2,2} + \\cdots + w_N x_{2,N} \\\\ \\vdots \\\\ w_1 x_{M,1} + w_2 x_{M,2} + \\cdots + w_N x_{M,N} \\\\ \\end{bmatrix} \\\\ &= \\begin{bmatrix} x_{1,1} & x_{1,2} & \\cdots & x_{1,N} \\\\ x_{2,1} & x_{2,2} & \\cdots & x_{2,N} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_{M,1} & x_{M,2} & \\cdots & x_{M,N} \\\\ \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_N \\end{bmatrix} \\\\ &= \\begin{bmatrix} x_1^T \\\\ x_2^T \\\\ \\vdots \\\\ x_M^T \\\\ \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_N \\end{bmatrix} \\\\ &= X w \\end{aligned} \\end{align} ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ëª¸ë¬´ê²Œ, í‚¤ ê·¸ë¦¬ê³  í¡ì—°ì—¬ë¶€ë¥¼ í†µí•´ ê¸°ëŒ€ ìˆ˜ëª…ì„ ì˜ˆì¸¡í•˜ê³ ì í•œë‹¤ê³  ê°€ì •í•˜ì. ê·¸ë ‡ë‹¤ë©´ ê° featureë§ˆë‹¤ ê°–ëŠ” ê°€ì¤‘ì¹˜ë“¤ì„ êµ¬í•´ì•¼ í•  ê²ƒì´ë‹¤. 3ê°œì˜ ì—°ë¦½ë°©ì •ì‹ì„ í‘¸ëŠ” ê²ƒê³¼ ë™ì¼í•œ ë¬¸ì œì´ë‹¤. ì´ ë¬¸ì œë¥¼ í–‰ë ¬ì„ í†µí•´ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ë‹¤. ê²°êµ­ì—ëŠ” í–‰ë ¬ê³¼ ë²¡í„°ì˜ ê³±ì„ í†µí•´ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¦‰, íšŒê·€(regression)ë¥¼ í•˜ëŠ” ê²ƒì´ë‹¤. ë§ˆì§€ë§‰ ì§ˆë¬¸ì²˜ëŸ¼ ì–´ë–»ê²Œ ë²¡í„° $x$ë¥¼ êµ¬í•  ìˆ˜ ìˆì„ê¹Œ? ì—­í–‰ë ¬ì„ í†µí•´ êµ¬í•  ìˆ˜ ìˆë‹¤. ê°„ë‹¨íˆ 2x2ì˜ í–‰ë ¬ì— ëŒ€í•œ ì—­í–‰ë ¬ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì€ $AA^{-1} = A^{-1}A$ê°€ ì„±ë¦½ë˜ì–´ì•¼ í•˜ëŠ” ì „ì œì¡°ê±´ì´ë¼ëŠ” ì ì´ë‹¤. ëª¨ë“  $A_{nxn}$ í–‰ë ¬ì´ ì—­í–‰ë ¬ì„ ê°–ì§€ëŠ” ì•ŠëŠ”ë‹¤. $det(A_{nxn}) \\neq 0$ì¼ ë•Œë§Œ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ê²Œ ëœë‹¤. ì°¸ê³ ë¡œ ëŒ€ê°í–‰ë ¬ì˜ ì—­í–‰ë ¬ì€ ê° ëŒ€ê°ì›ì†Œì˜ ì—­ìˆ˜ê°€ ë˜ì–´ ì—­í–‰ë ¬ì„ êµ¬í•˜ê¸° ì‰½ê¸° ë•Œë¬¸ì— í›„ì— í–‰ë ¬ì˜ ë¶„í•´ë¥¼ í• ë•Œ ëŒ€ê° í–‰ë ¬ì„ ìì£¼ ì‚¬ìš©í•œë‹¤. ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ëŠ” ì•„ë˜ì™€ ê°™ì´ ì—­í–‰ë ¬ì„ ê³„ì‚°í•˜ì—¬ ê°„ë‹¨íˆ ë¬¸ì œë¥¼ í•´ê²° í•  ìˆ˜ ìˆë‹¤. ì—­í–‰ë ¬ì´ ì¡´ì¬í•œë‹¤ë©´ì˜ ê°€ì •ì•„ë˜ ì„¤ëª…í•˜ì˜€ê¸°ì— ê·¸ë ‡ë‹¤ë©´ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ëŠ” ì–´ë–»ê²Œ í•´ê²°í•˜ì—¬ì•¼ í•  ì§€ ì˜ë¬¸ì´ ë“¤ ê²ƒì´ë‹¤. ë¨¼ì €, ì„ í˜• ì‹œìŠ¤í…œì—ì„œì˜ í•´ë¥¼ êµ¬í•  ê²½ìš°ì—ëŠ” í¬ê²Œ 3ê°€ì§€ê°€ ì¡´ì¬í•œë‹¤. 1) Unique solution 2) No Solution 3) Infinity Solutions ìœ„ì—ì„œ ì–¸ê¸‰í•œ ë¬¸ì œì˜ ê²½ìš°ì—ëŠ” ë¯¸ì§€ìˆ˜ì˜ ê°œìˆ˜(ë²¡í„° $x$ì˜ ì°¨ì›)ì™€ ë°©ì •ì‹ì˜ ê°œìˆ˜ê°€ ë™ì¼í•˜ë©°, ì •ë°©í–‰ë ¬ì— ëŒ€í•œ determinantê°€ ì¡´ì¬í•˜ì—¬ ì—­í–‰ë ¬ì´ ê³„ì‚°ë  ìˆ˜ ìˆì–´ Uniqueí•œ Solutionì„ ê°–ëŠ”ë‹¤. ì—¬ê¸°ì„œ ì£¼ì˜í•  ê²ƒì€ ì—­í–‰ë ¬ì„ ê°–ëŠ”ë‹¤ë©´ Uniqueí•œ Solutionì„ ê°–ëŠ”ë‹¤ëŠ” ì ì´ë‹¤. determinantê°€ 0ì´ ì•„ë‹ˆì–´ì•¼ ì—­í–‰ë ¬ì´ ì¡´ì¬í•œë‹¤. ì•„ë˜ ì£¼ì†Œë¥¼ í†µí•´ ê¸¸ë²„íŠ¸ ìŠ¤íŠ¸ë­ êµìˆ˜ë‹˜ì˜ ê°•ì˜ì—ì„œ determinantì˜ ê°œë…ì„ ì¢€ ë” ìì„¸íˆ ì•Œ ìˆ˜ ìˆë‹¤. í˜¹ì‹œ ì˜ì–´ ê°•ì˜ë¼ ì–´ë ¤ì›€ì´ ìˆìœ¼ì‹  ë¶„ë“¤ì€ ê¹€ë„í˜• ë°•ì‚¬ë‹˜ ê°•ì˜ì•ˆ - í–‰ë ¬ì˜ ì„±ì§ˆì„ ì°¸ê³ í•˜ê¸¸ ê¶Œí•œë‹¤. determinant=0ì´ê¸° ë•Œë¬¸ì— ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ëŠ” í•´ê°€ ë¬´ìˆ˜íˆ ë§ê±°ë‚˜, í•´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì´ë‹¤. ìš°ë¦¬ê°€ ì ‘í•˜ëŠ” ë°ì´í„°ì˜ shapeì€ ëŒ€ë¶€ë¶„ ì •ë°©í–‰ë ¬ë¡œ ì´ë£¨ì–´ì ¸ ìˆì§€ ì•Šë‹¤. ê·¸ë ‡ë‹¤ë©´ ì •ë°©í–‰ë ¬ì´ ì•„ë‹Œ ê²½ìš°ì—ëŠ” ì—­í–‰ë ¬ì„ êµ¬í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì–´ë–»ê²Œ í•´ë¥¼ êµ¬í•  ìˆ˜ ìˆì„ê¹Œë¼ëŠ” ì˜ë¬¸ì´ ë“¤ ê²ƒì´ë‹¤.ìš°ì„  ê°„ë‹¨íˆ ì˜ˆì „ì— ë°°ì› ë˜ ê²½í—˜ì„ í† ëŒ€ë¡œ ìƒê°í•´ ë³´ì. ë¯¸ì§€ìˆ˜ì˜ ìˆ˜ì™€ ë°©ì •ì‹ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•´ ë³¼ ë•Œ ì—°ë¦½ë°©ì •ì‹ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì„¸ ì¢…ë¥˜ê°€ ìˆì„ ìˆ˜ ìˆë‹¤. ë°©ì •ì‹ì˜ ìˆ˜ê°€ ë¯¸ì§€ìˆ˜ì˜ ìˆ˜ì™€ ê°™ë‹¤. (m=n) ë°©ì •ì‹ì˜ ìˆ˜ê°€ ë¯¸ì§€ìˆ˜ì˜ ìˆ˜ë³´ë‹¤ ì ë‹¤. (m &lt; n) ë°©ì •ì‹ì˜ ìˆ˜ê°€ ë¯¸ì§€ìˆ˜ì˜ ìˆ˜ë³´ë‹¤ ë§ë‹¤. (m &gt; n) 1ë²ˆì˜ ê²½ìš°, ì¦‰ ë°©ì •ì‹ì˜ ìˆ˜ê°€ ë¯¸ì§€ìˆ˜ì˜ ìˆ˜ì™€ ê°™ì€ ê²½ìš°ëŠ” ì•ì—ì„œ ë‹¤ë£¨ì—ˆë‹¤. 2ë²ˆì˜ ê²½ìš°, ë°©ì •ì‹ì˜ ê°œìˆ˜ê°€ ë¯¸ì§€ìˆ˜ì˜ ê°œìˆ˜ë³´ë‹¤ ì ë‹¤ë©´ í’€ë ¤ëŠ” ë¯¸ì§€ìˆ˜ì˜ ê°œìˆ˜ê°€ ë” ë§ê¸° ë•Œë¬¸ì— í•´ê°€ ë¬´ìˆ˜íˆ ë§ì´ ì¡´ì¬í•  ê²ƒì´ë‹¤. ì•„ë˜ì˜ ìˆ˜ì‹ì—ì„œ ë¯¸ì§€ìˆ˜ëŠ” 3ê°œì§€ë§Œ ë°©ì •ì‹ì€ 2ê°œ ë¿ì´ë‹¤. ì´ ë•ŒëŠ” $x_{2}$ê°€ ì–´ë–¤ ê°’ì´ ë˜ë”ë¼ë„ $x_{1}=x_{3}=2-x_{2}$ë§Œ ë§Œì¡±í•˜ë©´ ë˜ë¯€ë¡œ ë¬´í•œíˆ ë§ì€ í•´ê°€ ì¡´ì¬í•œë‹¤. \\begin{align} \\begin{matrix} x_1 & + & x_2 & & & = & 2 \\\\ & & x_2 & + & x_3 & = & 2 \\\\ \\end{matrix} \\end{align}\\begin{align} x = \\begin{bmatrix} 2 \\\\ 0 \\\\ 2 \\end{bmatrix} ,\\;\\; x = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} ,\\;\\; x = \\begin{bmatrix} 0 \\\\ 2 \\\\ 0 \\end{bmatrix} ,\\;\\; \\cdots \\end{align} 3ë²ˆì˜ ê²½ìš°, ë°˜ëŒ€ë¡œ ë°©ì •ì‹ì˜ ê°œìˆ˜ê°€ ë¯¸ì§€ìˆ˜ì˜ ê°œìˆ˜ë³´ë‹¤ ë§ë‹¤ë©´ í’€ë ¤ëŠ” ë¯¸ì§€ìˆ˜ì˜ ê°œìˆ˜ë³´ë‹¤ ì ê¸° ë•Œë¬¸ì— í•´ê°€ ì—†ì„ ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ ì„ í˜• ì—°ë¦½ë°©ì •ì‹ì„ ìƒê°í•´ ë³´ì. ë¯¸ì§€ìˆ˜ëŠ” 3ê°œì§€ë§Œ ë°©ì •ì‹ì€ 4ê°œë‹¤. \\begin{align} \\begin{matrix} x_1 & + & x_2 & & & = & 2 \\\\ & & x_2 & + & x_3 & = & 2 \\\\ x_1 & + & x_2 & + & x_3 & = & 3 \\\\ x_1 & + & x_2 & + & 2x_3 & = & 5 \\\\ \\end{matrix} \\end{align} ìœ„ì˜ 3ê°œ ë°©ì •ì‹ì„ ë™ì‹œì— ë§Œì¡±í•˜ëŠ” í•´ëŠ” $x_{1}=x_{2}=x_{3}=1$ì¸ë° ì´ ê°’ì€ 4ë²ˆì§¸ ë°©ì •ì‹ì„ ë§Œì¡±í•˜ì§€ ëª»í•œë‹¤. ë”°ë¼ì„œ 4ê°œì˜ ë°©ì •ì‹ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” í•´ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. \\begin{align} x_1 + x_2 + 2x_3 = 4 \\end{align} ì„ í˜• ì˜ˆì¸¡ëª¨í˜•ì„ êµ¬í•˜ëŠ” ë¬¸ì œëŠ” ê³„ìˆ˜í–‰ë ¬ì´ íŠ¹ì§•í–‰ë ¬ $X$, ë¯¸ì§€ìˆ˜ ë²¡í„°ê°€ ê°€ì¤‘ì¹˜ ë²¡í„° $w$ì¸ ì„ í˜• ì—°ë¦½ë°©ì •ì‹ ë¬¸ì œì´ë‹¤. ê·¸ëŸ°ë° ë³´í†µ ë°ì´í„°ì˜ ìˆ˜ëŠ” ì…ë ¥ì°¨ì›ë³´ë‹¤ í° ê²½ìš°ê°€ ë§ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë©´ì , ì¸µìˆ˜, í•œê°•ì´ ë³´ì´ëŠ”ì§€ì˜ ì—¬ë¶€ë¡œ ì§‘ê°’ì„ ê²°ì •í•˜ëŠ” ëª¨í˜•ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ ë”± 3ê°€êµ¬ì˜ ì•„íŒŒíŠ¸ ê°€ê²©ë§Œ ì¡°ì‚¬í•˜ëŠ” ê²½ìš°ëŠ” ì—†ì„ ê²ƒì´ë‹¤. ë³´í†µì€ 10ê°€êµ¬ í˜¹ì€ 100ê°€êµ¬ì˜ ì•„íŒŒíŠ¸ ê°€ê²©ì„ ìˆ˜ì§‘í•˜ì—¬ ì´ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ë‹¤ì‹œ ë§í•´ ì„ í˜• ì˜ˆì¸¡ ëª¨í˜•ì„ êµ¬í•  ë•ŒëŠ” 3ë²ˆê³¼ ê°™ì€ ê²½ìš°ê°€ ë§ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë–„ëŠ” ì„ í˜• ì—°ë¦½ë°©ì •ì‹ì˜ í•´ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì„ í˜• ì—°ë¦½ë°©ì •ì‹ì„ í‘¸ëŠ” ë°©ì‹ìœ¼ë¡œëŠ” ì„ í˜• ì˜ˆì¸¡ëª¨í˜•ì˜ ê°€ì¤‘ì¹˜ ë²¡í„°ë¥¼ êµ¬í•  ìˆ˜ ì—†ë‹¤.","categories":[{"name":"linear algebra","slug":"linear-algebra","permalink":"https://heung-bae-lee.github.io/categories/linear-algebra/"}],"tags":[]},{"title":"ì„ í˜•ëŒ€ìˆ˜ ìš”ì†Œ(Elements in linear algebra)","slug":"linear_algebra_01","date":"2020-05-12T13:41:59.000Z","updated":"2020-05-13T07:51:30.883Z","comments":true,"path":"2020/05/12/linear_algebra_01/","link":"","permalink":"https://heung-bae-lee.github.io/2020/05/12/linear_algebra_01/","excerpt":"","text":"ì•„ë˜ ë‚´ìš©ì€ ê¹€ë„í˜• ë°•ì‚¬ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜ ê°•ì˜ì•ˆ edwithì˜ ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ ì„ í˜•ëŒ€ìˆ˜ ê°•ì˜ì™€ KOCWì˜ í•œì–‘ëŒ€í•™êµ ì´ìƒí™” êµìˆ˜ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ ê°•ì˜ë¥¼ ë³´ê³  ì •ë¦¬í•œ ë‚´ìš©ì´ë‹¤. Linearity(ì„ í˜•ì„±) í–‰ë ¬ë¡œ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë‹¤ ì„ í˜•ì„±ì´ë¼ëŠ” ê²ƒì„ ë§Œì¡±í•´ì•¼ í•œë‹¤. ì„ í˜•ì„±ì€ ì•„ë˜ ë‘ ê°€ì§€ ì¡°ê±´ì„ ë§Œì¡±í•  ë•Œ ì„ í˜•ì„±ì„ ê°–ëŠ”ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. 1) Superposition(ì¤‘ì²©ì˜ ì›ë¦¬) f(x_{1} + x_{2}) = f(x_{1}) + f(x_{2}) 2) Homogeniety f(ax) = af(x), aëŠ” constant(ìƒìˆ˜) ì˜ˆì‹œ 1)$y=mx=f(x)$; ì›ì ì„ ì§€ë‚˜ëŠ” ì§ì„  -&gt; Linearityë¥¼ ê²°ì •í•˜ëŠ” ì¤‘ìš”í•œ ì¡°ê±´ìœ¼ë¡œ ì›ì ì„ ì§€ë‚˜ëŠ”ì§€ë¥¼ í™•ì¸í•´ì•¼ í•œë‹¤. ì§ì„ ì´ë¼ê³  í•´ì„œ ë‹¤ Linearityë¥¼ ë§Œì¡±í•˜ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. m(a_{1}x_{1} + a_{2}x_{2}) = a_{1}mx_{1} + a_{2}mx_{2} = a_{1}f(x_{1}) + a_{2}f(x_{2}) ì˜ˆì‹œ 2) $y = mx+n = f(x), n \\neq 0$ m(a_{1}x_{1} + a_{2}x_{2}) + n \\neq a_{1}(m x_{1} + n) + a_{2}(m x_{2} + n) ìœ„ì™€ ê°™ìœ¼ë¯€ë¡œ ì„ í˜•ì„±ì„ ê°–ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. Elements of Linear Algebra ì„ í˜•ëŒ€ìˆ˜ì—ì„œ ë‹¤ë£¨ëŠ” ë°ì´í„°ëŠ” ê°œìˆ˜ë‚˜ í˜•íƒœì— ë”°ë¼ í¬ê²Œ ìŠ¤ì¹¼ë¼(scalar), ë²¡í„°(vector), í–‰ë ¬(matrix), í…ì„œ(tensor) ìœ í˜•ìœ¼ë¡œ ë‚˜ë‰œë‹¤. ìŠ¤ì¹¼ë¼ëŠ” ìˆ«ì í•˜ë‚˜ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ì´ê³ , ë²¡í„°ëŠ” ì—¬ëŸ¬ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„° ë ˆì½”ë“œ(data record)ì´ë©°, í–‰ë ¬ì€ ì´ëŸ¬í•œ ë²¡í„°, ì¦‰ ë°ì´í„° ë ˆì½”ë“œê°€ ì—¬ëŸ¿ì¸ ë°ì´í„° ì§‘í•©ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. í…ì„œëŠ” ê°™ì€ í¬ê¸°ì˜ í–‰ë ¬ì´ ì—¬ëŸ¬ ê°œ ìˆëŠ” ê²ƒì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. ìŠ¤ì¹¼ë¼(Scalar) ìŠ¤ì¹¼ë¼ëŠ” í•˜ë‚˜ì˜ ìˆ«ìë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„°ë¥¼ ë§í•œë‹¤. ìŠ¤ì¹¼ë¼ëŠ” ë³´í†µ $x$ì™€ ê°™ì´ ì•ŒíŒŒë²³ ì†Œë¬¸ìë¡œ í‘œê¸°í•˜ë©° ì‹¤ìˆ˜(real number)ì¸ ìˆ«ì ì¤‘ì˜ í•˜ë‚˜ì´ë¯€ë¡œ ì‹¤ìˆ˜ ì§‘í•© $ğ‘$ì˜ ì›ì†Œë¼ëŠ” ì˜ë¯¸ì—ì„œ ë‹¤ìŒì²˜ëŸ¼ í‘œê¸°í•œë‹¤. \\begin{align} x \\in \\mathbf{R} \\end{align}ë²¡í„°(Vector) ë²¡í„°ëŠ” ì—¬ëŸ¬ ê°œì˜ ìˆ«ìê°€ íŠ¹ì •í•œ ìˆœì„œëŒ€ë¡œ ëª¨ì—¬ ìˆëŠ” ê²ƒì„ ë§í•œë‹¤. ì‚¬ì‹¤ ëŒ€ë¶€ë¶„ì˜ ë°ì´í„° ë ˆì½”ë“œëŠ” ì—¬ëŸ¬ ê°œì˜ ìˆ«ìë¡œ ì´ë£¨ì–´ì§„ ê²½ìš°ê°€ ë§ë‹¤. í•˜ë‚˜ì˜ ë¬¶ìŒ(tuple)ìœ¼ë¡œ ë¬¶ì–´ë†“ëŠ” ê²ƒì´ ì¢‹ë‹¤. ì´ë•Œ ìˆ«ìì˜ ìˆœì„œê°€ ë°”ë€Œë©´ ì–´ë–¤ ìˆ«ìê°€ ì–´ë–¤ í”¼ì²˜ì— ë§¤í•‘ë˜ëŠ” ê°’ì¸ì§€ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ìˆ«ìì˜ ìˆœì„œë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì´ëŸ° ë°ì´í„° ë¬¶ìŒì„ ì„ í˜•ëŒ€ìˆ˜ì—ì„œëŠ” ë²¡í„°ë¼ê³  ë¶€ë¥¸ë‹¤. ì´ë•Œ ë²¡í„°ëŠ” ë³µìˆ˜ì˜ ê°€ë¡œì¤„, ì¦‰ í–‰(row)ì„ ê°€ì§€ê³  í•˜ë‚˜ì˜ ì„¸ë¡œì¤„, ì¦‰ ì—´(column)ì„ ê°€ì§€ëŠ” í˜•íƒœë¡œ ìœ„ì—ì„œ ì•„ë˜ë¡œ ë‚´ë ¤ì¨ì„œ í‘œê¸°í•´ì•¼ í•œë‹¤. í•˜ë‚˜ì˜ ë²¡í„°ë¥¼ ì´ë£¨ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜ê°€ $ğ‘›$ê°œì´ë©´ ì´ ë²¡í„°ë¥¼ $n$-ì°¨ì› ë²¡í„°($n$-dimensional vector)ë¼ê³  í•˜ë©° ë‹¤ìŒì²˜ëŸ¼ í‘œê¸°í•œë‹¤. \\begin{align} x = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{N} \\\\ \\end{bmatrix} \\end{align}\\begin{align} x \\in \\mathbf{R}^N \\end{align} ìˆœì„œëŠ” ì°¨ì›ì„ ì˜ë¯¸í•œë‹¤. ì¦‰, ìœ„ì—ì„œ ë²¡í„° $x$ëŠ” 1ì°¨ì›ì— $x_1$ 2ì°¨ì›ì— $x_2$, Nì°¨ì›ì— $x_N$ì´ë¼ëŠ” ê°’ì„ ê°–ëŠ”ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì°¸ê³ ë¡œ ìˆœì„œê°€ ì •í•´ì ¸ ìˆì§€ ì•Šì€ ë°°ì—´(array)ëŠ” ì§‘í•©(set)ì´ë‹¤. ë˜í•œ ë²¡í„°ë¼ê³  í•˜ëŠ” ê²ƒì€ í•œë°©í–¥ìœ¼ë¡œë§Œ ìˆëŠ” 1-dimensionalë¡œ ì¡´ì¬í•˜ëŠ” ë°°ì—´ ë˜ëŠ” í•˜ë‚˜ì˜ ìˆ«ìê°€ ëœë‹¤. í–‰ë ¬(Matrix) í–‰ë ¬(Matrix)ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 2-Dimensional ë°°ì—´(array)ë¥¼ ì˜ë¯¸í•œë‹¤. í–‰ë ¬ì€ ë³µìˆ˜ì˜ ì°¨ì›ì„ ê°€ì§€ëŠ” ë°ì´í„° ë ˆì½”ë“œê°€ ë‹¤ì‹œ ì—¬ëŸ¬ ê°œ ìˆëŠ” ê²½ìš°ì˜ ë°ì´í„°ë¥¼ í•©ì³ì„œ í‘œê¸°í•œ ê²ƒì´ë‹¤. í–‰ë ¬ì€ ë³´í†µ $ğ‘‹$ì™€ ê°™ì´ ì•ŒíŒŒë²³ ëŒ€ë¬¸ìë¡œ í‘œê¸°í•œë‹¤. \\begin{align} X = \\begin{bmatrix} \\boxed{\\begin{matrix} x_{1, 1} & x_{1, 2} & x_{1, 3} & x_{1, 4}\\end{matrix}} \\\\ \\begin{matrix} x_{2, 1} & x_{2, 2} & x_{2, 3} & x_{2, 4}\\end{matrix} \\\\ \\begin{matrix} x_{3, 1} & x_{3, 2} & x_{3, 3} & x_{3, 4}\\end{matrix} \\\\ \\begin{matrix} x_{4, 1} & x_{4, 2} & x_{4, 3} & x_{4, 4}\\end{matrix} \\\\ \\begin{matrix} x_{5, 1} & x_{5, 2} & x_{5, 3} & x_{5, 4}\\end{matrix} \\\\ \\begin{matrix} x_{6, 1} & x_{6, 2} & x_{6, 3} & x_{6, 4}\\end{matrix} \\\\ \\end{bmatrix} \\end{align} ìŠ¤ì¹¼ë¼ì™€ ë²¡í„°ë„ ìˆ˜í•™ì ìœ¼ë¡œëŠ” í–‰ë ¬ì— ì†í•œë‹¤. ìŠ¤ì¹¼ë¼ëŠ” ì—´ê³¼ í–‰ì˜ ìˆ˜ê°€ ê°ê° 1ì¸ í–‰ë ¬ì´ê³  ë²¡í„°ëŠ” ì—´ì˜ ìˆ˜ê°€ 1ì¸ í–‰ë ¬ì´ë‹¤. ê·¸ë˜ì„œ ìŠ¤ì¹¼ë¼ëŠ” $ \\begin{align} a \\in \\mathbf{R}^{1\\times 1} \\end{align}$ ë²¡í„°ëŠ” $\\begin{align} x \\in \\mathbf{R}^{n\\times 1} \\end{align}$ í…ì„œ(Tensor) í…ì„œëŠ” ê°™ì€ í¬ê¸°ì˜ í–‰ë ¬ì´ ì—¬ëŸ¬ ê°œ ê°™ì´ ë¬¶ì—¬ ìˆëŠ” ê²ƒì„ ë§í•œë‹¤. ì—„ê²©í•œ ìˆ˜í•™ì  ì •ì˜ë¡œëŠ” í…ì„œëŠ” ë‹¤ì°¨ì› ë°°ì—´ë¡œ í‘œí˜„ë˜ëŠ” ì‚¬ìƒ(mapping)ìœ¼ë¡œ ë‹¤ì°¨ì› ë°°ì—´ ìì²´ë¥¼ ëœ»í•˜ì§€ ì•ŠëŠ”ë‹¤. í•˜ì§€ë§Œ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ë¶„ì•¼ì—ì„œëŠ” í”íˆ ë‹¤ì°¨ì› ë°°ì—´ì„ í…ì„œë¼ê³  ë¶€ë¥´ë¯€ë¡œ ì—¬ê¸°ì—ì„œëŠ” ì´ëŸ¬í•œ ì •ì˜ë¥¼ ë”°ë¥´ë„ë¡ í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ ì»¬ëŸ¬ ì´ë¯¸ì§€ëŠ” 2ì°¨ì›ì˜ í–‰ë ¬ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì‚¬ì‹¤ ë¹¨ê°•, ì´ˆë¡, íŒŒë‘ì˜ ë°ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” 3ê°€ì§€ì˜ ì´ë¯¸ì§€ê°€ ê²¹ì¹œ ê²ƒì´ë‹¤. ì»¬ëŸ¬ ì´ë¯¸ì§€ì—ì„œëŠ” ê°ê°ì˜ ìƒ‰ì„ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ì„ ì±„ë„(channel)ì´ë¼ê³  í•œë‹¤. ì˜ˆì œ ì´ë¯¸ì§€ëŠ” í¬ê¸°ê°€ 768 x 1024ì´ê³  3ê°œì˜ ì±„ë„ì´ ìˆìœ¼ë¯€ë¡œ 768 x 1024 x 3 í¬ê¸°ì˜ 3ì°¨ì› í…ì„œë‹¤. 1234from scipy import misc # íŒ¨í‚¤ì§€ ì„í¬íŠ¸img_rgb = misc.face() # ì»¬ëŸ¬ ì´ë¯¸ì§€ ë¡œë“œimg_rgb.shape # ë°ì´í„°ì˜ ëª¨ì–‘ ê²°ê³¼1(768, 1024, 3) 123456789101112131415161718192021plt.subplot(221)plt.imshow(img_rgb, cmap=plt.cm.gray) # ì»¬ëŸ¬ ì´ë¯¸ì§€ ì¶œë ¥plt.axis(\"off\")plt.title(\"RGB ì»¬ëŸ¬ ì´ë¯¸ì§€\")plt.subplot(222)plt.imshow(img_rgb[:, :, 0], cmap=plt.cm.gray) # red ì±„ë„ ì¶œë ¥plt.axis(\"off\")plt.title(\"Red ì±„ë„\")plt.subplot(223)plt.imshow(img_rgb[:, :, 1], cmap=plt.cm.gray) # green ì±„ë„ ì¶œë ¥plt.axis(\"off\")plt.title(\"Green ì±„ë„\")plt.subplot(224)plt.imshow(img_rgb[:, :, 2], cmap=plt.cm.gray) # blue ì±„ë„ ì¶œë ¥plt.axis(\"off\")plt.title(\"Blue ì±„ë„\")plt.show() ì „ì¹˜ ì „ì¹˜(transpose)ì—°ì‚°ì€ í–‰ë ¬ì—ì„œ ê°€ì¥ ê¸°ë³¸ì´ ë˜ëŠ” ì—°ì‚°ìœ¼ë¡œ í–‰ë ¬ì˜ í–‰ê³¼ ì—´ì„ ë°”ê¾¸ëŠ” ì—°ì‚°ì„ ì˜ë¯¸í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ê¸°ë³¸ì ìœ¼ë¡œ ë²¡í„°ëŠ” ì—´ë²¡í„°(column vector)ë¥¼ ì‚¬ìš©í•˜ë©°, í–‰ ë²¡í„°(row vector)ë¥¼ í‘œê¸°í• ë•ŒëŠ” ì—´ë²¡í„°ì˜ ì „ì¹˜í–‰ë ¬ë¡œ í‘œê¸°í•´ì¤€ë‹¤. ì „ì¹˜ ì—°ì‚°ê³¼ í–‰ ë²¡í„°, ì—´ ë²¡í„°ë¥¼ ì´ìš©í•˜ë©´ ë‹¤ìŒì²˜ëŸ¼ í–‰ë ¬ì„ ë³µìˆ˜ì˜ ì—´ ë²¡í„° $c_{i}$, ë˜ëŠ” ë³µìˆ˜ì˜ í–‰ ë²¡í„° $r^{T}_{j}$ì„ í•©ì¹œ(concatenated)í˜•íƒœë¡œ í‘œê¸°í•  ìˆ˜ë„ ìˆë‹¤. \\begin{align} X = \\begin{bmatrix} c_1 & c_2 & \\cdots & c_M \\end{bmatrix} = \\begin{bmatrix} r_1^T \\\\ r_2^T \\\\ \\vdots \\\\ r_N^T \\\\ \\end{bmatrix} \\end{align} ìœ„ ì‹ì—ì„œ í–‰ë ¬ê³¼ ë²¡í„°ì˜ í¬ê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} X \\in \\mathbf{R}^{N\\times M} \\end{align}\\begin{align} c_i \\in \\mathbf{R}^{N \\times 1} \\; (i=1,\\cdots,M) \\end{align}\\begin{align} r_j^T \\in \\mathbf{R}^{1 \\times M} \\; (j=1,\\cdots,N) \\end{align} ë²¡í„°ì˜ ëª¨ì–‘ì„ ì§ì‚¬ê°í˜•ìœ¼ë¡œ í‘œì‹œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} X = \\begin{bmatrix} \\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ c_1 \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} & \\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ c_2 \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} & \\cdots & \\boxed{\\begin{matrix} \\phantom{\\LARGE\\mathstrut} \\\\ c_M \\\\ \\phantom{\\LARGE\\mathstrut} \\end{matrix}} \\end{bmatrix} = \\begin{bmatrix} \\boxed{\\begin{matrix} \\phantom{} & \\phantom{} & r_1^T & \\phantom{} & \\phantom{} \\end{matrix}} \\\\ \\boxed{\\begin{matrix} \\phantom{} & \\phantom{} & r_2^T & \\phantom{} & \\phantom{} \\end{matrix}} \\\\ \\vdots \\\\ \\boxed{\\begin{matrix} \\phantom{} & \\phantom{} & r_N^T & \\phantom{} & \\phantom{} \\end{matrix}} \\\\ \\end{bmatrix} \\end{align}íŠ¹ìˆ˜í•œ ë²¡í„°ì™€ í–‰ë ¬ ëª‡ ê°€ì§€ íŠ¹ìˆ˜í•œ ë²¡í„°ì™€ í–‰ë ¬ì€ ë³„ë„ì˜ ê¸°í˜¸ë‚˜ ì´ë¦„ì´ ë¶™ëŠ”ë‹¤. ì˜ë²¡í„° ëª¨ë“  ì›ì†Œê°€ 0ì¸ $N$ì°¨ì› ë²¡í„°ëŠ” ì˜ë²¡í„°(zeros-vector)ë¼ê³  í•˜ë©° ë‹¤ìŒì²˜ëŸ¼ í‘œê¸°í•œë‹¤. ë¬¸ë§¥ìœ¼ë¡œ ë²¡í„°ì˜ í¬ê¸°ë¥¼ ì•Œ ìˆ˜ ìˆì„ ë•ŒëŠ” í¬ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì•„ë˜ ì²¨ì $N$ì„ ìƒëµí•  ìˆ˜ ìˆë‹¤. \\begin{align} \\mathbf{0}_N = \\mathbf{0} = 0 = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ \\end{bmatrix} \\end{align}\\begin{align} 0 \\in \\mathbf{R}^{N \\times 1} \\end{align}ì¼ë²¡í„° ëª¨ë“  ì›ì†Œê°€ 1ì¸ $N$ì°¨ì› ë²¡í„°ëŠ” ì¼ë²¡í„°(ones-vector)ë¼ê³  í•˜ë©° ë‹¤ìŒì²˜ëŸ¼ í‘œê¸°í•œë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ë¬¸ë§¥ìœ¼ë¡œ ë²¡í„°ì˜ í¬ê¸°ë¥¼ ì•Œ ìˆ˜ ìˆì„ ë•ŒëŠ” í¬ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì•„ë˜ ì²¨ì $N$ì„ ìƒëµí•  ìˆ˜ ìˆë‹¤. \\begin{align} \\mathbf{1}_N = \\mathbf{1} = 1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\\\ \\end{bmatrix} \\end{align}\\begin{align} 1 \\in \\mathbf{R}^{N \\times 1} \\end{align}ì •ë°©í–‰ë ¬ í–‰ì˜ ê°œìˆ˜ì™€ ì—´ì˜ ê°œìˆ˜ê°€ ê°™ì€ í–‰ë ¬ì„ ì •ë°©í–‰ë ¬(square matrix)ë¼ê³  í•œë‹¤. ëŒ€ê°í–‰ë ¬ í–‰ë ¬ì—ì„œ í–‰ê³¼ ì—´ì´ ê°™ì€ ìœ„ì¹˜ë¥¼ ì£¼ ëŒ€ê°(main diagonal) ë˜ëŠ” ê°„ë‹¨íˆ ëŒ€ê°(diagonal)ì´ë¼ê³  í•œë‹¤. ëŒ€ê° ìœ„ì¹˜ì— ìˆì§€ ì•Šì€ ê²ƒë“¤ì€ ë¹„ëŒ€ê°(off-diagonal)ì´ë¼ê³  í•œë‹¤. ëª¨ë“  ë¹„ëŒ€ê° ìš”ì†Œê°€ 0ì¸ í–‰ë ¬ì„ ëŒ€ê°í–‰ë ¬(diagonal matrix)ì´ë¼ê³  í•œë‹¤. \\begin{align} D = \\begin{bmatrix} d_{1} & 0 & \\cdots & 0 \\\\ 0 & d_{2} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & d_{N} \\\\ \\end{bmatrix} \\end{align}\\begin{align} D \\in \\mathbf{R}^{N \\times N} \\end{align} ëŒ€ê°í–‰ë ¬ì´ ë˜ë ¤ë©´ ë¹„ëŒ€ê° ì„±ë¶„ì´ 0ì´ê¸°ë§Œ í•˜ë©´ ë˜ê³  ëŒ€ê°ì„±ë¶„ì€ 0ì´ë“  ì•„ë‹ˆë“  ìƒê´€ì—†ë‹¤. ë˜í•œ ë°˜ë“œì‹œ ì •ë°©í–‰ë ¬ì¼ í•„ìš”ë„ ì—†ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ í–‰ë ¬ë„ ëŒ€ê°í–‰ë ¬ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. \\begin{align} D = \\begin{bmatrix} d_{1} & 0 & \\cdots & 0 \\\\ 0 & d_{2} & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & d_{M} \\\\ 0 & 0 & \\cdots & 0 \\\\ 0 & 0 & \\cdots & 0 \\\\ 0 & 0 & \\cdots & 0 \\\\ \\end{bmatrix} \\end{align}\\begin{align} D \\in \\mathbf{R}^{N \\times M} \\end{align}í•­ë“±í–‰ë ¬ ëŒ€ê°í–‰ë ¬ ì¤‘ì—ì„œë„ ëª¨ë“  ëŒ€ê°ì„±ë¶„ì˜ ê°’ì´ 1ì¸ ëŒ€ê°í–‰ë ¬ì„ í•­ë“±í–‰ë ¬(identity matrix)ì´ë¼ê³  í•œë‹¤. í•­ë“±í–‰ë ¬ì€ ë³´í†µ ì•ŒíŒŒë²³ ëŒ€ë¬¸ì $I$ë¡œ í‘œê¸°í•œë‹¤. (Eë¡œ í‘œê¸°í•˜ëŠ” ë°©ì‹ë„ ìˆë‹¤.) ì„ì˜ì˜ í–‰ë ¬ Aì— ëŒ€í•´ í–‰ë ¬ê³±ì„ í•´ë„ ì„ì˜ì˜ í–‰ë ¬ Aê°€ ë‚˜ì˜¤ë„ë¡ í•˜ëŠ” í•­ë“±ì› ê°œë…ê³¼ ë™ì¼í•˜ë‹¤. A I = I A = A\\begin{align} I = \\begin{bmatrix} 1 & 0 & \\cdots & 0 \\\\ 0 & 1 & \\cdots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\cdots & 1 \\\\ \\end{bmatrix} \\end{align}\\begin{align} I \\in \\mathbf{R}^{N \\times N} \\end{align} ì°¸ê³ ë¡œ ì¶”í›„ì— ë§í•˜ê² ì§€ë§Œ, í•­ë“±ì›ì˜ ê°œë…ì´ ìˆë‹¤ë©´ ì—­ì›ì˜ ê°œë…ì— ëŒ€í•´ ìƒê°í•´ ë³¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ì—­ì›ì˜ ì •ì˜ì— ë§ê²Œ ì„ì˜ì˜ í–‰ë ¬ì— ëŒ€í•œ ê³±ì…ˆì—ëŒ€í•œ ì—­ì›ì„ ì°¾ëŠ”ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ êµ¬í•  ìˆ˜ ìˆë‹¤. ì¦‰ í–‰ë ¬ì—ì„œ ì„ì˜ì˜ í–‰ë ¬ Aì— ê³±ì…ˆì— ëŒ€í•œ ì—­ì›ì€ ì—­í–‰ë ¬ì„ ì˜ë¯¸í•œë‹¤. A A^{-1} = A^{-1} A = IëŒ€ì¹­í–‰ë ¬ ë§Œì•½ ì „ì¹˜ì—°ì‚°ì„ í†µí•´ì„œ ì–»ì€ ì „ì¹˜í–‰ë ¬ê³¼ ì›ë˜ì˜ í–‰ë ¬ì´ ê°™ìœ¼ë©´ ëŒ€ì¹­í–‰ë ¬(symmetric matrix)ë¼ê³  í•œë‹¤. ì •ë°©í–‰ë ¬ë§Œì´ ëŒ€ì¹­í–‰ë ¬ì´ ë  ìˆ˜ ìˆë‹¤. \\begin{align} S^{T} = S \\end{align}\\begin{align} S \\in \\mathbf{R}^{N \\times N} \\end{align}ë²¡í„°ì˜ í–‰ë ¬ê³¼ ì—°ì‚° ë²¡í„°ì™€ í–‰ë ¬ë„ ìˆ«ìì²˜ëŸ¼ ë§ì…ˆ, ëº„ì…ˆ, ê³±ì…ˆ ë“±ì˜ ì—°ì‚°ì„ í•  ìˆ˜ ìˆë‹¤. ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ì— ìˆì–´ì„œ ì„ í˜•ëŒ€ìˆ˜ë¥¼ ì•Œê³ ìˆì–´ì•¼ í•˜ëŠ” ì´ìœ ëŠ” ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì§€ë§Œ ê·¸ ì¤‘ í•œê°€ì§€ë¡œ ë²¡í„°ì™€ í–‰ë ¬ì˜ ì—°ì‚°ì„ ì´ìš©í•˜ë©´ ëŒ€ëŸ‰ì˜ ë°ì´í„°ì— ëŒ€í•œ ê³„ì‚°ì„ ê°„ë‹¨í•œ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë¬¼ë¡  ë²¡í„°ì™€ í–‰ë ¬ì— ëŒ€í•œ ì—°ì‚°ì€ ìˆ«ìì˜ ì‚¬ì¹™ ì—°ì‚°ê³¼ëŠ” ëª‡ ê°€ì§€ ë‹¤ë¥¸ ì ì´ ìˆìœ¼ë¯€ë¡œ ì´ëŸ¬í•œ ì°¨ì´ë¥¼ ì˜ ì•Œì•„ì•¼ í•œë‹¤. ë²¡í„°/í–‰ë ¬ì˜ ë§ì…ˆê³¼ ëº„ì…ˆ ê°™ì€ í¬ê¸°ë¥¼ ê°€ì§„ ë‘ ê°œì˜ ë²¡í„°ë‚˜ í–‰ë ¬ì€ ë§ì…ˆê³¼ ëº„ì…ˆì„ í•  ìˆ˜ ìˆë‹¤. ë‘ ë²¡í„°ì™€ í–‰ë ¬ì—ì„œ ê°™ì€ ìœ„ì¹˜ì— ìˆëŠ” ì›ì†Œë¼ë¦¬ ë§ì…ˆê³¼ ëº„ì…ˆì„ í•˜ë©´ ëœë‹¤. ì´ëŸ¬í•œ ì—°ì‚°ì„ ìš”ì†Œë³„(element-wise)ì—°ì‚°ì´ë¼ê³  í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë²¡í„° $x$ì™€ $y$ê°€ ë‹¤ìŒê³¼ ê°™ìœ¼ë©´, \\begin{align} x= \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\\\ \\end{bmatrix} ,\\;\\; y= \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ \\end{bmatrix} \\end{align} ë²¡í„° $ğ‘¥$ ì™€ $ğ‘¦$ ì˜ ë§ì…ˆ $ğ‘¥+ğ‘¦$ì™€ ëº„ì…ˆ $ğ‘¥âˆ’ğ‘¦$ëŠ” ë‹¤ìŒì²˜ëŸ¼ ê³„ì‚°í•œë‹¤. \\begin{align} x + y = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\\\ \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 10 + 0 \\\\ 11 + 1 \\\\ 12 + 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 12 \\\\ 14 \\\\ \\end{bmatrix} \\end{align}\\begin{align} x - y = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\\\ \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 1 \\\\ 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 10 - 0 \\\\ 11 - 1 \\\\ 12 - 2 \\\\ \\end{bmatrix} = \\begin{bmatrix} 10 \\\\ 10 \\\\ 10 \\\\ \\end{bmatrix} \\end{align} í–‰ë ¬ë„ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ë§ì…ˆê³¼ ëº„ì…ˆì„ í•  ìˆ˜ ìˆë‹¤. \\begin{align} \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} + \\begin{bmatrix} 10 & 20 \\\\ 30 & 40 \\\\ \\end{bmatrix} -\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} = \\begin{bmatrix} 14 & 24 \\\\ 34 & 44 \\end{bmatrix} \\end{align}ìŠ¤ì¹¼ë¼ì™€ ë²¡í„°/í–‰ë ¬ì˜ ê³±ì…ˆ ë²¡í„° $x$ë˜ëŠ” í–‰ë ¬ $A$ì— ìŠ¤ì¹¼ë¼ê°’ $c$ë¥¼ ê³±í•˜ëŠ” ê²ƒì€ ë²¡í„° $x$ ë˜ëŠ” í–‰ë ¬ $A$ì˜ ëª¨ë“  ì›ì†Œì— ìŠ¤ì¹¼ë¼ê°’ $c$ë¥¼ ê³±í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. \\begin{align} c \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} cx_1 \\\\ cx_2 \\end{bmatrix} \\end{align}\\begin{align} c \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix} = \\begin{bmatrix} ca_{11} & ca_{12} \\\\ ca_{21} & ca_{22} \\end{bmatrix} \\end{align}ë¸Œë¡œë“œìºìŠ¤íŒ… ì›ë˜ ë§ì…ˆê³¼ ëº„ì…ˆì€ í¬ê¸°(ì°¨ì›)ê°€ ê°™ì€ ë‘ ë²¡í„°ì— ëŒ€í•´ì„œë§Œ í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ Numpyì—ì„œ ë²¡í„°ì™€ ìŠ¤ì¹¼ë¼ì˜ ê²½ìš°ì—ëŠ” ê´€ë¡€ì ìœ¼ë¡œ ë‹¤ìŒì²˜ëŸ¼ 1-ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤ì¹¼ë¼ë¥¼ ë²¡í„°ë¡œ ì—°ì‚°ì„ í—ˆìš©í•œë‹¤. ì´ë¥¼ ë¸Œë¡œë“œìºìŠ¤íŒ…(broadcasting)ì´ë¼ê³  í•œë‹¤. \\begin{align} \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\\\ \\end{bmatrix} - 10 = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\\\ \\end{bmatrix} - 10\\cdot \\mathbf{1} = \\begin{bmatrix} 10 \\\\ 11 \\\\ 12 \\\\ \\end{bmatrix} - \\begin{bmatrix} 10 \\\\ 10 \\\\ 10 \\\\ \\end{bmatrix} \\end{align} ë°ì´í„° ë¶„ì„ì—ì„œëŠ” ì›ë˜ì˜ ë°ì´í„° ë²¡í„° $x$ê°€ ì•„ë‹ˆë¼ ê·¸ ë°ì´í„° ë²¡í„°ì˜ ê° ì›ì†Œì˜ í‰ê· ê°’ì„ ëº€ í‰ê· ì œê±°(mean removed) ë²¡í„° í˜¹ì€ 0-í‰ê· (zero-mean)ë²¡í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. \\begin{align} x = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\end{bmatrix} \\;\\; \\rightarrow \\;\\; x - m = \\begin{bmatrix} x_1 - m\\\\ x_2 - m \\\\ \\vdots \\\\ x_N - m \\end{bmatrix} \\end{align} ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, í–‰ë ¬ì˜ ê³±ì…ˆì— ëŒ€í•œ êµí™˜ë²•ì¹™ì€ ì„±ë¦½í•˜ì§€ ì•ŠëŠ”ë‹¤. ë¬¼ë¡ , $AB=BA$ì¸ ê²½ìš°ë„ ì¡´ì¬í•˜ì§€ë§Œ, í•­ìƒ ì¡´ì¬í•˜ì§„ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. ë²¡í„°ì™€ ë²¡í„°ì˜ ê³±ì…ˆ ë²¡í„° $x$ì™€ ë²¡í„° $y$ì˜ ë‚´ì ì€ ë‹¤ìŒì²˜ëŸ¼ í‘œê¸° í•œë‹¤. \\begin{align} x^T y \\end{align} ë‚´ì ì€ ë‹¤ìŒì²˜ëŸ¼ ì (dot)ìœ¼ë¡œ í‘œê¸°í•˜ëŠ” ê²½ìš°ë„ ìˆì–´ì„œ ë‹· í”„ë¡œë•íŠ¸(dot product)ë¼ê³ ë„ ë¶€ë¥´ê³  &lt; x, y &gt; ê¸°í˜¸ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ë„ ìˆë‹¤. \\begin{align} x \\cdot y = \\, < x, y > \\, = x^T y \\end{align} ë‘ ë²¡í„°ë¥¼ ë‚´ì í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì¡°ê±´ì´ ë§Œì¡±ë˜ì–´ì•¼ í•œë‹¤. ìš°ì„  ë‘ ë²¡í„°ì˜ ì°¨ì›(ê¸¸ì´)ì´ ê°™ì•„ì•¼ í•œë‹¤. ì•ì˜ ë²¡í„°ê°€ í–‰ ë²¡í„°ì´ê³  ë’¤ì˜ ë²¡í„°ê°€ ì—´ ë²¡í„°ì—¬ì•¼ í•œë‹¤. ì´ë•Œ ë‚´ì ì˜ ê²°ê³¼ëŠ” ìŠ¤ì¹¼ë¼ê°’ì´ ë˜ë©° ë‹¤ìŒì²˜ëŸ¼ ê³„ì‚°í•œë‹¤. ìš°ì„  ê°™ì€ ìœ„ì¹˜ì— ìˆëŠ” ì›ì†Œë“¤ì„ ìš”ì†Œë³„ ê³±ì…ˆì²˜ëŸ¼ ê³±í•œ ë‹¤ìŒ, ê·¸ ê°’ë“¤ì„ ë‹¤ì‹œ ëª¨ë‘ ë”í•´ì„œ í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ê°’ìœ¼ë¡œ ë§Œë“ ë‹¤. \\begin{align} x^T y = \\begin{bmatrix} x_{1} & x_{2} & \\cdots & x_{N} \\end{bmatrix} \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{N} \\\\ \\end{bmatrix} = x_1 y_1 + \\cdots + x_N y_N = \\sum_{i=1}^N x_i y_i \\end{align}\\begin{align} x \\in \\mathbf{R}^{N \\times 1}, y \\in \\mathbf{R}^{N \\times 1}, x^T y \\in \\mathbf{R} \\end{align}í–‰ë ¬ê³¼ í–‰ë ¬ì˜ ê³±ì…ˆ ë²¡í„°ì˜ ê³±ì…ˆì„ ì •ì˜í•œ í›„ì—ëŠ” ì´ë¥¼ ì´ìš©í•˜ì—¬ í–‰ë ¬ì˜ ê³±ì…ˆì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤. í–‰ë ¬ê³¼ í–‰ë ¬ì„ ê³±í•˜ë©´ í–‰ë ¬ì´ ëœë‹¤. ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. $A$ í–‰ë ¬ê³¼ $B$ í–‰ë ¬ì„ ê³±í•œ ê²°ê³¼ê°€ $C$ í–‰ë ¬ì´ ëœë‹¤ê³  í•˜ì. $C$ì˜ $i$ë²ˆì§¸ í–‰, $j$ë²ˆì§¸ ì—´ì˜ ì›ì†Œ $c_{ij}$ì˜ ê°’ì€ $A$ í–‰ë ¬ì˜ $i$ë²ˆì§¸ í–‰ ë²¡í„° $a^{T}_{i}$ì™€ $B$ í–‰ë ¬ì˜ $j$ë²ˆì§¸ ì—´ ë²¡í„° $b_{j}$ì˜ ê³±ì´ë‹¤. \\begin{align} C = AB \\; \\rightarrow \\; c_{ij} = a_i^T b_j \\end{align} ì´ ì •ì˜ê°€ ì„±ë¦½í•˜ë ¤ë©´ ì•ì˜ í–‰ë ¬ $A$ì˜ ì—´ì˜ìˆ˜ê°€ ë’¤ì˜ í–‰ë ¬ $B$ì˜ í–‰ì˜ ìˆ˜ì™€ ì¼ì¹˜í•´ì•¼ë§Œ í•œë‹¤. \\begin{align} A \\in \\mathbf{R}^{N \\times L} , \\; B \\in \\mathbf{R}^{L \\times M} \\; \\rightarrow \\; AB \\in \\mathbf{R}^{N \\times M} \\end{align}êµí™˜ ë²•ì¹™ê³¼ ë¶„ë°°ë²•ì¹™ í–‰ë ¬ì˜ ê³±ì…ˆì€ ê³±í•˜ëŠ” í–‰ë ¬ì˜ ìˆœì„œë¥¼ ë°”ê¾¸ëŠ” êµí™˜ ë²•ì¹™ì´ ì„±ë¦½í•˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ ë§ì…ˆì— ëŒ€í•œ ë¶„ë°° ë²•ì¹™ì€ ì„±ë¦½í•œë‹¤. \\begin{align} AB \\neq BA \\end{align}\\begin{align} A(B + C) = AB + AC \\end{align}\\begin{align} (A + B)C = AC + BC \\end{align} ì „ì¹˜ ì—°ì‚°ë„ ë§ˆì°¬ê°€ì§€ë¡œ ë§ˆì°¬ê°€ì§€ë¡œ ë§ì…ˆ/ëº„ì…ˆì— ëŒ€í•´ ë¶„ë°° ë²•ì¹™ì´ ì„±ë¦½í•œë‹¤. \\begin{align} (A + B)^T = A^T + B^T \\end{align} ì „ì¹˜ ì—°ì‚° ê³±ì…ˆì˜ ê²½ìš°ì—ëŠ” ë¶„ë°° ë²•ì¹™ì´ ì„±ë¦½í•˜ê¸°ëŠ” í•˜ì§€ë§Œ ì „ì¹˜ ì—°ì‚°ì´ ë¶„ë°°ë˜ë©´ì„œ ê³±ì…ˆì˜ ìˆœì„œê°€ ë°”ë€ë‹¤. \\begin{align} (AB)^T = B^T A^T \\end{align}\\begin{align} (ABC)^T = C^T B^T A^T \\end{align} ê³±ì…ˆì˜ ì—°ê²° ì—°ì†ëœ í–‰ë ¬ì˜ ê³±ì…ˆì€ ê³„ì‚° ìˆœì„œë¥¼ ì„ì˜ì˜ ìˆœì„œë¡œ í•´ë„ ìƒê´€ì—†ë‹¤. \\begin{align} ABC = (AB)C = A(BC) \\end{align}\\begin{align} ABCD = ((AB)C)D = (AB)(CD) = A(BCD) = A(BC)D \\end{align}","categories":[{"name":"linear algebra","slug":"linear-algebra","permalink":"https://heung-bae-lee.github.io/categories/linear-algebra/"}],"tags":[]},{"title":"Ensemble Learning - 01","slug":"machine_learning_14","date":"2020-05-02T12:00:10.000Z","updated":"2020-05-16T10:52:38.331Z","comments":true,"path":"2020/05/02/machine_learning_14/","link":"","permalink":"https://heung-bae-lee.github.io/2020/05/02/machine_learning_14/","excerpt":"","text":"Ensemble Learningì´ë€? ëª¨í˜• ê²°í•©(model combining)ë°©ë²•ì€ ì•™ìƒë¸” ë°©ë²•ë¡ (ensemble methods)ë¼ê³ ë„ í•œë‹¤. ì´ëŠ” íŠ¹ì •í•œ í•˜ë‚˜ì˜ ì˜ˆì¸¡ ë°©ë²•ì´ ì•„ë‹ˆë¼ ë³µìˆ˜ì˜ ì˜ˆì¸¡ëª¨í˜•ì„ ê²°í•©í•˜ì—¬ ë” ë‚˜ì€ ì„±ëŠ¥ì˜ ì˜ˆì¸¡ì„ í•˜ë ¤ëŠ” ì‹œë„ì´ë‹¤. ëª¨í˜• ê²°í•© ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ì¼ë°˜ì ìœ¼ë¡œ ê³„ì‚°ëŸ‰ì€ ì¦ê°€í•˜ì§€ë§Œ ë‹¤ìŒê³¼ ê°™ì€ íš¨ê³¼ê°€ ìˆë‹¤. ë‹¨ì¼ ëª¨í˜•ì„ ì‚¬ìš©í•  ë•Œ ë³´ë‹¤ ì„±ëŠ¥ ë¶„ì‚°ì´ ê°ì†Œí•˜ê¸°ì— ê³¼ìµœì í™”(overfitting)ì„ ë°©ì§€í•œë‹¤. ê°œë³„ ëª¨í˜•ì´ ì„±ëŠ¥ì´ ì•ˆì¢‹ì„ ê²½ìš°ì—ëŠ” ê²°í•© ëª¨í˜•ì˜ ì„±ëŠ¥ì´ ë” í–¥ìƒëœë‹¤. Ensemble Learningì€ ìºê¸€ì´ë‚˜ ë‹¤ë¥¸ ëŒ€íšŒì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ìë‘í•˜ë©° ì—¬ëŸ¬ ì°¨ë¡€ ìš°ìŠ¹ì„ ì°¨ì§€í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê·¸ë§Œí¼ ê°•ë ¥í•˜ì§€ë§Œ, í˜„ì—…ì—ì„œëŠ” Ensemble Learningì„ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ë§¤ìš° ë†’ë‹¤. ì™œëƒí•˜ë©´ êµ‰ì¥íˆ ê°•ë ¥í•˜ì§€ë§Œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ê³¼ì˜ ì„±ëŠ¥ì°¨ì´ê°€ ì—„ì²­ë‚˜ê²Œ ì°¨ì´ë‚˜ëŠ” ê²ƒì´ ì•„ë‹ˆë©°, ì‹¤ì œ Domainì—ì„œ ì¤‘ìš”í•œ ë³€ìˆ˜ê°€ ë¬´ì—‡ì¸ì§€ì™€ ê°™ì€ ì›ì¸ì„ ì°¾ëŠ” feature selection ë¶€ë¶„ì´ ë” ì¤‘ìš”í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë¬¼ë¡ , ì„±ëŠ¥ì¸¡ë©´ì´ ì¤‘ìš”í•œ Domainë¶„ì•¼ì—ì„œëŠ” Ensemble Learningì´ ì¤‘ìš”í•  ê²ƒì´ë‹¤. Ensembleì´ë¼ëŠ” ì˜ë¯¸ì˜ ì‚¬ì „ì  ì •ì˜ëŠ” â€˜í•©ì°½ë‹¨â€™, â€˜ì¡°í™”â€™ë¼ëŠ” ì˜ë¯¸ë¥¼ ì§€ë‹Œë‹¤. ì¦‰, ë¨¸ì‹ ëŸ¬ë‹ì—ì„œì˜ ê°œë…ì€ ì—¬ëŸ¬ê°œì˜ ëª¨ë¸ì„ ì¡°í•©ì„ ì‹œí‚¨ë‹¤ë¼ëŠ” ì˜ë¯¸ë¡œ ë°›ì•„ë“¤ì¼ ìˆ˜ ìˆë‹¤. í†µê³„í•™ì—ì„œì˜ ëŒ€ìˆ˜ì˜ ë²•ì¹™ì´ë¼ëŠ” ê°œë…ì´ ìˆëŠ”ë°, í° ëª¨ì§‘ë‹¨ì—ì„œ ë¬´ì‘ìœ„ë¡œ ë½‘ì€ í‘œë³¸ì˜ ìˆ˜ê°€ ë§ì•„ ì§ˆìˆ˜ë¡(ë³´í†µì€ 30ê°œì´ìƒì˜ ê´€ì¸¡ì´) ëª¨ì§‘ë‹¨ì˜ í‰ê· ì— ê°€ê¹Œìš¸ í™•ë¥ ì´ ë†’ì•„ì§„ë‹¤ëŠ” ê°œë…ì´ë‹¤. ë§ì€ ì‹œí–‰ì˜ ê²°ê³¼ê°€ ìˆ˜í•™ì ìœ¼ë¡œ í•©ë¦¬ì ì¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ëŠ”ë°, Ensemble learningì— ì ìš©í•˜ì—¬ ìƒê°í•´ë³´ë©´ ë‹¤ìˆ˜ì˜ ëª¨ë¸ì´ ë” í•©ë¦¬ì ì¸ ì„±ëŠ¥ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒìœ¼ë¡œ í•´ì„í•  ìˆ˜ë„ ìˆë‹¤. í•˜ì§€ë§Œ ì•„ë˜ì—ì„œ í•©ì¹˜ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ ìì²´ê°€ ë–¨ì–´ì§€ëŠ” ëª¨ë¸ì„ ê°€ì§€ê³  Ensemble learningì„ ì§„í–‰í•œë‹¤ê³  í•´ë„ ì„±ëŠ¥ì„ ì˜¬ë¦´ ìˆ˜ëŠ” ì—†ë‹¤. ì•„ë˜ì—ì„œëŠ” ì´ì§„ë¶„ë¥˜ì— ëŒ€í•´ì„œë§Œ ì–¸ê¸‰í–ˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ classification ë¬¸ì œì—ì„œëŠ” One VS Rest ë°©ì‹ìœ¼ë¡œ ë¬¸ì œë¥¼ í’€ê¸°ì— ì´ì§„ ë¶„ë¥˜ ë¿ë§Œì•„ë‹ˆë¼, classê°€ ì—¬ëŸ¬ê°œì¸ multi class ë¬¸ì œì—ì„œë„ ì ìš©ë˜ëŠ” ë‚´ìš©ì´ë‹¤. ì•„ë˜ì—ì„œì™€ ê°™ì´ ê°ê°ì˜ ì„±ëŠ¥ì´ 0.5ì¸ ë¶„ë¥˜ê¸°ë“¤ì„ votingì„ í†µí•´ ê²°ê³¼ë¥¼ ë‚´ê²Œ ë˜ëŠ”ë°, ê°ê°ì˜ weakí•œ ë¶„ë¥˜ê¸°ë“¤ì˜ ì¡°í•©ì„ í†µí•´ ìµœì¢…ì ìœ¼ë¡œëŠ” 0.625ë¼ëŠ” ì„±ëŠ¥ì„ ë‚´ê²Œ ëœë‹¤. ë‹¤ìˆ˜ê²° ëª¨í˜•ì´ ê°œë³„ ëª¨í˜•ë³´ë‹¤ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ì´ìœ ëŠ” ë‹¤ìŒ ì‹¤í—˜ì—ì„œë„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ë§Œì•½ ê°œë³„ ëª¨í˜•ì´ ì •ë‹µì„ ì¶œë ¥í•  í™•ë¥ ì´ $p$ì¸ ê²½ìš°ì— ì„œë¡œ ë‹¤ë¥´ê³  ë…ë¦½ì ì¸ ëª¨í˜• $N$ê°œë¥¼ ëª¨ì•„ì„œ ë‹¤ìˆ˜ê²° ëª¨í˜•ì„ ë§Œë“¤ë©´ ì •ë‹µì„ ì¶œë ¥í•  í™•ë¥ ì´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤. \\sum_{k>\\frac{N}{2}}^N \\binom N k p^k (1-p)^{N-k}123456789101112131415def total_error(p, N): te = 0.0 for k in range(int(np.ceil(N/2)), N + 1): te += sp.misc.comb(N, k) * p**k * (1-p)**(N-k) return tex = np.linspace(0, 1, 100)plt.plot(x, x, 'g:', lw=3, label=\"ê°œë³„ ëª¨í˜•\")plt.plot(x, total_error(x, 10), 'b-', label=\"ë‹¤ìˆ˜ê²° ëª¨í˜• (N=10)\")plt.plot(x, total_error(x, 100), 'r-', label=\"ë‹¤ìˆ˜ê²° ëª¨í˜• (N=100)\")plt.xlabel(\"ê°œë³„ ëª¨í˜•ì˜ ì„±ëŠ¥\")plt.ylabel(\"ë‹¤ìˆ˜ê²° ëª¨í˜•ì˜ ì„±ëŠ¥\")plt.legend(loc=0)plt.show() ê°ê°ì˜ ë¶„ë¥˜ê¸°(ëª¨ë¸)ë¥¼ í†µí•´ ìµœì¢…ì ìœ¼ë¡œëŠ” í•´ë‹¹ ë°ì´í„°ë“¤ì˜ decision boundaryì˜ í‰ê· ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì•„ë˜ì—ì„œì™€ ê°™ì´ Test dataì— ëŒ€í•´ ì¼ì •ë¶€ë¶„ biasë˜ëŠ” ë¶€ë¶„ì„ ì¤„ì´ê¸° ìœ„í•´ overfittingì´ ì˜ ë˜ëŠ” íŠ¸ë¦¬ê¸°ë°˜ì˜ ëª¨í˜•ì„ ì£¼ë¡œ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ì‚¬ìš©í•œë‹¤. Ensemble Learningì˜ ì¢…ë¥˜ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ì‰½ê²Œ ë§í•˜ë©´ Baggingì€ ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” Treeê¸°ë°˜ì´ë‚˜ ì„ í˜•íšŒê·€ ë¶„ì„ ê°™ì€ ê²½ìš°ëŠ” ë™ì¼í•œ featureì™€ ë™ì¼í•œ dataë¥¼ ì‚¬ìš©í–ˆì„ ê²½ìš° ë™ì¼í•œ ê²°ê³¼ë¥¼ ë‚´ì£¼ê¸° ë•Œë¬¸ì—, ëª¨ë¸ì„ ì—¬ëŸ¬ ê°œ ë§Œë“¤ê¸° ìœ„í•´ì„œ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ì„œ ê° ëª¨ë¸ì— fittingì‹œí‚¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. Random ForestëŠ” ë°ì´í„° ë¿ë§Œ ì•„ë‹ˆë¼ featureë“¤ì˜ ì„ íƒë„ ê° ëª¨ë¸ë³„ë¡œ ë‹¬ë¦¬í•˜ì—¬ fittingí•˜ëŠ” ê²ƒì´ë©°, Boostingì€ ë¶„ë¥˜ê¸°ê°€ í‹€ë¦¬ê²Œ ì˜ˆì¸¡í•œ ë°ì´í„°ë“¤ì— ëŒ€í•´ ê·¸ ë‹¤ìŒ í•™ìŠµê¸°ëŠ” ì¢€ ë” í•™ìŠµì„ ì˜ í•  ìˆ˜ ìˆë„ë¡ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ê°œë…ì´ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ Stackingì€ ì„±ëŠ¥ìˆœìœ¼ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸°ëŠ” ìºê¸€ì—ì„œëŠ” 0.1%ë¼ë„ ì˜¬ë¦¬ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©ë˜ì–´ ì§€ëŠ”ë°, ë‹¤ë¥¸ Ensemble ê¸°ë²•ë“¤ ë³´ë‹¤ ë§ì€ ì„±ëŠ¥ì„ ë†’ì´ì§€ëŠ” ëª»í•˜ì—¬ ì˜ ì‚¬ìš©ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. êµ‰ì¥íˆ ë§ì€ í•™ìŠµ ì—°ì‚°ëŸ‰ì„ í•„ìš”ë¡œ í•˜ê¸° ë•Œë¬¸ì— ì‹¤ì œ Domainì—ì„œ ì‚¬ìš©ë˜ì–´ì§€ê¸°ì—ëŠ” ì‰½ì§€ ì•Šë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í•œ ê²ƒê³¼ ê°™ì´ ëª¨í˜• ê²°í•© ë°©ë²•ì€ í¬ê²Œ ë‚˜ëˆ„ì–´ ì·¨í•©(aggregation) ë°©ë²•ë¡ ê³¼ ë¶€ìŠ¤íŒ…(boosting)ë°©ë²•ë¡ ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ì·¨í•© ë°©ë²•ë¡ ì€ ì‚¬ìš©í•  ëª¨í˜•ì˜ ì§‘í•©ì´ ì´ë¯¸ ê²°ì •ë˜ì–´ìˆë‹¤. ë¶€ìŠ¤íŒ… ë°©ë²•ë¡ ì€ ì‚¬ìš©í•  ëª¨í˜•ì„ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë ¤ê°„ë‹¤. ê° ë°©ë²•ë¡ ì˜ ëŒ€í‘œì ì¸ ë°©ë²•ë“¤ì€ ì•„ë˜ì™€ ê°™ë‹¤. ì·¨í•© ë°©ë²•ë¡  ë‹¤ìˆ˜ê²°(Majority Voting) ë°°ê¹…(Bagging) ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forests) ë¶€ìŠ¤íŒ… ë°©ë²•ë¡  ì—ì´ë‹¤ë¶€ìŠ¤íŠ¸(AdaBoost) ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŠ¸(Gradient Boost) ë‹¤ìˆ˜ê²° ë°©ë²•(Voting) ë‹¤ìˆ˜ê²° ë°©ë²•ì€ ê°€ì¥ ë‹¨ìˆœí•œ ëª¨í˜• ê²°í•© ë°©ë²•ìœ¼ë¡œ ì „í˜€ ë‹¤ë¥¸ ëª¨í˜•ë„ ê²°í•©í•  ìˆ˜ ìˆë‹¤. ë‹¤ìˆ˜ê²° ë°©ë²•ì€ Hard Votingê³¼ Soft Voting ë‘ ê°€ì§€ë¡œ ë‚˜ë‰˜ì–´ì§„ë‹¤. hard voting: ë‹¨ìˆœ íˆ¬í‘œ. ê°œë³„ ëª¨í˜•ì˜ ê²°ê³¼ ê¸°ì¤€ soft voting: ê°€ì¤‘ì¹˜ íˆ¬í‘œ. ê°œë³„ ëª¨í˜•ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì˜ í•© ê¸°ì¤€ ì¼ë°˜ì ìœ¼ë¡œ hard votingë³´ë‹¤ëŠ” soft votingì´ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì¢‹ì•„ì„œ ë” ë§ì´ ì‚¬ìš©ëœë‹¤. Scikit-Learnì˜ ensemble ì„œë¸Œ íŒ¨í‚¤ì§€ëŠ” ë‹¤ìˆ˜ê²° ë°©ë²•ì„ ìœ„í•œ VotingClassifierí´ë˜ìŠ¤ë¥¼ ì œê³µí•œë‹¤. ì…ë ¥ì¸ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. estimators: ê°œë³„ ëª¨í˜• ëª©ë¡, ë¦¬ìŠ¤íŠ¸ë‚˜ named parameter í˜•ì‹ìœ¼ë¡œ ì…ë ¥ voting: ë¬¸ìì—´ {hard, soft} hard votingê³¼ soft voting ì„ íƒ. ë””í´íŠ¸ëŠ” hard weights: ì‚¬ìš©ì ê°€ì¤‘ì¹˜ ë¦¬ìŠ¤íŠ¸ ë‹¤ìŒê³¼ ê°™ì€ ì˜ˆì œ ë°ì´í„°ë¥¼ ê°€ì§€ëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ìƒê°í•´ë³´ì. 1234567891011X = np.array([[0, -0.5], [-1.5, -1.5], [1, 0.5], [-3.5, -2.5], [0, 1], [1, 1.5], [-2, -0.5]])y = np.array([1, 1, 1, 2, 2, 2, 2])x_new = [0, -1.5]plt.scatter(X[y == 1, 0], X[y == 1, 1], s=100, marker='o', c='r', label=\"í´ë˜ìŠ¤ 1\")plt.scatter(X[y == 2, 0], X[y == 2, 1], s=100, marker='x', c='b', label=\"í´ë˜ìŠ¤ 2\")plt.scatter(x_new[0], x_new[1], s=100, marker='^', c='g', label=\"í…ŒìŠ¤íŠ¸ ë°ì´í„°\")plt.xlabel(\"x1\")plt.ylabel(\"x2\")plt.title(\"ì´ì§„ ë¶„ë¥˜ ì˜ˆì œ ë°ì´í„°\")plt.legend()plt.show() ë¨¼ì €, ì´ ë¬¸ì œë¥¼ 3ê°€ì§€ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ í’€ì–´ë³¼ ê²ƒì´ë‹¤. ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨í˜• QDA ëª¨í˜• ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜• ë§ˆì§€ë§‰ìœ¼ë¡œ 3ê°€ì§€ ëª¨í˜•ì„ ë‹¤ìˆ˜ê²°ë¡œ í•©ì¹œ ëª¨í˜•ì„ VotingClassifierí´ë˜ìŠ¤ë¡œ ë§Œë“¤ì—ˆë‹¤. ë‹¤ë§Œ 3ê°€ì§€ ëª¨í˜•ì˜ ê°€ì¤‘ì¹˜ê°€ ê°ê° 1,1,2ë¡œ ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë†’ì˜€ë‹¤. ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì´, ë¡œì§€ìŠ¤í‹± íšŒê·€ëª¨í˜•ê³¼ ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì€ í´ë˜ìŠ¤ 1ì´ë¼ëŠ” ê²°ê³¼ë¥¼ ë³´ì´ì§€ë§Œ QDAëª¨í˜•ì€ í´ë˜ìŠ¤ 2ë¼ëŠ” ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤. ì†Œí”„íŠ¸ ë°©ì‹ì˜ ë‹¤ìˆ˜ê²° ëª¨í˜•ì€ í´ë˜ìŠ¤ 2ë¼ëŠ” ê²°ë¡ ì„ ë³´ì¸ë‹¤. ë§Œì•½ í•˜ë“œ ë°©ì‹ì˜ ë‹¤ìˆ˜ê²° ëª¨í˜•ì´ì—ˆë‹¤ë©´ ì˜ˆì¸¡ ê²°ê³¼ëŠ” í´ë˜ìŠ¤ 1ì´ ë  ê²ƒì´ë‹¤. 1234567891011121314151617181920212223242526from sklearn.linear_model import LogisticRegressionfrom sklearn.naive_bayes import GaussianNBfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysisfrom sklearn.ensemble import VotingClassifiermodel1 = LogisticRegression(random_state=1)model2 = QuadraticDiscriminantAnalysis()model3 = GaussianNB()ensemble = VotingClassifier(estimators=[('lr', model1), ('qda', model2), ('gnb', model3)], voting='soft')probas = [c.fit(X, y).predict_proba([x_new]) for c in (model1, model2, model3, ensemble)]class1_1 = [pr[0, 0] for pr in probas]class2_1 = [pr[0, 1] for pr in probas]ind = np.arange(4)width = 0.35 # bar widthp1 = plt.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color='green')p2 = plt.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width, color='lightgreen')p3 = plt.bar(ind, [0, 0, 0, class1_1[-1]], width, color='blue')p4 = plt.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color='steelblue')plt.xticks(ind + 0.5 * width, ['ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨í˜•', 'QDA ëª¨í˜•', 'ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œë² ì´ì¦ˆ', 'ì†Œí”„íŠ¸ ë‹¤ìˆ˜ê²° ëª¨í˜•'])plt.ylim([0, 1.1])plt.title('ì„¸ê°€ì§€ ë‹¤ë¥¸ ë¶„ë¥˜ ëª¨í˜•ê³¼ ì†Œí”„íŠ¸ ë‹¤ìˆ˜ê²° ëª¨í˜•ì˜ ë¶„ë¥˜ ê²°ê³¼')plt.legend([p1[0], p2[0]], ['í´ë˜ìŠ¤ 1', 'í´ë˜ìŠ¤ 2'], loc='upper left')plt.show() 12345678910111213141516171819from itertools import productx_min, x_max = -4, 2y_min, y_max = -3, 2xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.005), np.arange(y_min, y_max, 0.005))f, axarr = plt.subplots(2, 2)for idx, clf, tt in zip(product([0, 1], [0, 1]), [model1, model2, model3, ensemble], ['ë¡œì§€ìŠ¤í‹± íšŒê·€', 'QDA', 'ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œë² ì´ì¦ˆ', 'ë‹¤ìˆ˜ê²° ëª¨í˜•']): Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.2, cmap=mpl.cm.jet) axarr[idx[0], idx[1]].scatter( X[:, 0], X[:, 1], c=y, alpha=0.5, s=50, cmap=mpl.cm.jet) axarr[idx[0], idx[1]].scatter(x_new[0], x_new[1], marker='x') axarr[idx[0], idx[1]].set_title(tt)plt.tight_layout()plt.show() ì•„ë˜ ê° ëª¨í˜•ë³„ë¡œ decision boundaryë¥¼ ì‚´í´ ë³´ì•˜ì„ ê²½ìš° ì–´ë– í•œ ìƒê°ì´ ë“œëŠ”ê°€? í•„ìì˜ ìƒê°ì—” ë¬¼ë¡  ì „ì œì¡°ê±´ì´ ì•„ë˜ train ë°ì´í„°ê°€ ëª¨ì§‘ë‹¨ì˜ ë¶„í¬ë¥¼ ëŒ€í‘œí•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë“¤ì´ë¼ëŠ” ê°€ì •í•˜ì— soft ë°©ì‹ìœ¼ë¡œ í•œ ê²°ê³¼ëŠ” ì˜³ì§€ ëª»í•œ ê²°ê³¼ë¼ê³  ìƒê°í•œë‹¤. 2í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë˜ê¸°ì—” 1 í´ë˜ìŠ¤ê°€ ë§ì€ ì˜ì—­ì— ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ë ‡ê²Œ ì‹œê°í™”ë¥¼ í†µí•´ ì‚´í´ë³´ëŠ” ë°©ë²•ë„ ê²°ê³¼ì— ëŒ€í•œ ê²€ì¦ì„ ìœ„í•´ í•„ìš”í•  ê²ƒì´ë‹¤. í—ˆë‚˜, ë‹¤ë³€ëŸ‰ì¸ ê²½ìš°ëŠ” ëª‡ê°€ì§€ ì¤‘ìš”í•œ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ì„œë§Œ ì‹œê°í™”ë¥¼ í•´ ë³¸ë‹¤ë˜ê°€ ì•„ë‹ˆë©´ í•´ë‹¹ ì¡°í•©ë“¤ì— ëŒ€í•´ ëª¨ë‘ ê·¸ë ¤ë³´ëŠ” ê²ƒë„ ë•Œë¡  ì¢‹ì€ ë°©ë²•ì¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ì•ì„œ ëª¨í˜• ê²°í•©ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë…ë¦½ì ì¸ ëª¨í˜•ì˜ ìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ì„±ëŠ¥ í–¥ìƒì´ ì¼ì–´ë‚  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ì•Œì•˜ë‹¤. ê°ê° ë‹¤ë¥¸ í™•ë¥  ëª¨í˜•ì„ ì‚¬ìš©í•˜ëŠ”ë°ì—ëŠ” í•œê³„ê°€ ìˆìœ¼ë¯€ë¡œ ë³´í†µì€ ë°°ê¹… ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê°™ì€ í™•ë¥  ëª¨í˜•ì„ ì“°ì§€ë§Œ ì„œë¡œ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” ë‹¤ìˆ˜ì˜ ëª¨í˜•ì„ ë§Œë“ ë‹¤. ë°°ê¹…(Bagging) Baggingì€ Bootstrap Aggregatingì˜ ì•½ìë¡œ Samplingì„ í•˜ëŠ” ë°©ì‹ì´ Bootstrapë°©ì‹ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ë³µì›ì¶”ì¶œì˜ ë°©ì‹ì´ë¼ê³  ìƒê°í•˜ë©´ëœë‹¤. í•˜ë‚˜ì˜ ëª¨ë¸ì— ëŒ€í•˜ì—¬ ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ê²½ìš° í•´ë‹¹ ëª¨ë¸ì— ë“¤ì–´ê°€ìˆëŠ” ë°ì´í„°ëŠ” ì¤‘ë³µëœ ë°ì´í„°ê°€ ìˆì„ ìˆ˜ ìˆë‹¤.(ì˜¤ë¥¸ìª½ ì²«ë²ˆì§¸ ë°ì´í„°ì„¸íŠ¸ì—ì„œì™€ ê°™ì´) ê°™ì€ ë°ì´í„° ìƒ˜í”Œì„ ì¤‘ë³µì‚¬ìš©(replacement)í•˜ì§€ ì•Šìœ¼ë©´: Pasting ê°™ì€ ë°ì´í„° ìƒ˜í”Œì„ ì¤‘ë³µì‚¬ìš©(replacement)í•˜ë©´: Bagging ë°ì´í„°ê°€ ì•„ë‹ˆë¼ ë‹¤ì°¨ì› ë…ë¦½ ë³€ìˆ˜ ì¤‘ ì¼ë¶€ ì°¨ì›ì„ ì„ íƒí•˜ëŠ” ê²½ìš°ì—ëŠ”: Random Subspaces ë°ì´í„° ìƒ˜í”Œê³¼ ë…ë¦½ ë³€ìˆ˜ ì°¨ì› ëª¨ë‘ ì¼ë¶€ë§Œ ëœë¤í•˜ê²Œ ì‚¬ìš©í•˜ë©´: Random Patches ì´ë ‡ê²Œ ì¶”ì¶œí•˜ëŠ” ë°ì´í„°ëŠ” ì „ì²´ ë°ì´í„° ì¤‘ ì•½ 63%ì •ë„ë§Œ ì¶”ì¶œì„ í•˜ê²Œ ëœë‹¤. ì•„ë˜ ì²« ë²ˆì§¸ ê·¸ë¦¼ì—ì„œëŠ” ë°‘ì¤„ì´ ê·¸ëŸ¬ì§„ ì›ì˜ ë°ì´í„°ëŠ” ì¶”ì¶œë˜ì§€ ì•ŠëŠ” ë°ì´í„°ë“¤ì´ë‹¤. 2ë²ˆì§¸ ê·¸ë¦¼ì€ Bootstrap sizeê°€ 5ë¼ë©´ 5ê°œì”© 12ê°œì˜ ë°ì´í„° setë¥¼ ë³µì›ì¶”ì¶œì„ í†µí•˜ì—¬ ë½‘ëŠ” ê²ƒì´ë‹¤. ì—¬ê¸°ì„œì˜ $k$ëŠ” ì„ì˜ë¡œ ì •í•  ìˆ˜ ìˆë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ì¶”ì¶œë˜ì§€ ì•Šì€ ë°ì´í„° setì´ ìˆëŠ” ê²ƒì€ í•™ìŠµì— í™œìš©ë˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ë‘ë©´ ë°ì´í„°ë¥¼ ë‚­ë¹„í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•˜ë‹¤. ë¬¼ë¡  Test setì„ ë¯¸ë¦¬ ë‚˜ëˆ„ì–´ ë†“ê³  í•´ë‹¹ Test setì„ predictioní•œ ê²°ê³¼ë¥¼ votingí•˜ì—¬ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì§€ë§Œ, ì‚¬ìš©ë˜ì§€ ì•Šì€ ë°ì´í„°(Out-of_Bag data)ì— ëŒ€í•´ì„œë„ ëª¨ë¸ë³„ ì„±ëŠ¥ì„ ê³„ì‚°í•œë‹¤. íŠ¸ë¦¬(Tree)ì™€ ë°°ê¹…(Bagging)ì„ ë¹„êµí•˜ìë©´ ê¹Šì´ ì„±ì¥í•œ íŠ¸ë¦¬ëŠ” overfittingì´ êµ‰ì¥íˆ ì‹¬í•´ì§€ê¸° ë•Œë¬¸ì— ë¶„ì‚°ì´ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì— í¸í–¥ì€ ì¤„ì–´ë“¤ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ë°°ê¹…ì€ ì´ëŸ¬í•œ íŠ¸ë¦¬ë“¤ì„ ê²°í•©ì‹œí‚¤ë¯€ë¡œ í¸í–¥ì´ ìœ ì§€ë˜ë©°, ë¶„ì‚¬ì€ ê°ì†Œí•˜ëŠ” ëª¨ë¸ì´ ë  ê²ƒì´ë‹¤. í•™ìŠµë°ì´í„°ì˜ noiseì— robustí•˜ë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨í˜•í•´ì„ì´ ì–´ë ¤ì›Œì§€ëŠ” ë‹¨ì ì´ ìˆë‹¤. ì´ëŸ¬í•œ ë‹¨ì ì´ ì‹¤ì œ Domainì—ì„œ ì‚¬ìš©ë˜ì§€ ëª»í•˜ëŠ” ì´ìœ ê°€ ë  ìˆ˜ ìˆë‹¤. Scikit-Learnì˜ ensemble ì„œë¸Œ íŒ¨í‚¤ì§€ëŠ” ë°°ê¹… ëª¨í˜• ê²°í•©ì„ ìœ„í•œ BaggingClassifier í´ë˜ìŠ¤ë¥¼ ì œê³µí•œë‹¤. ì‚¬ìš©ë²•ì€ ì•„ë˜ì™€ ê°™ë‹¤. ì°¸ê³ ë¡œ BaggingRegressorë„ ì¡´ì¬í•˜ë©° ì‚¬ìš©ë²•ì€ ë™ì¼í•˜ë‹¤. base_estimator: ê¸°ë³¸ëª¨í˜• n_estimators: ëª¨í˜• ê°¯ìˆ˜. default=10 bootstrap: ë°ì´í„° ì¤‘ë³µ ì‚¬ìš© ì—¬ë¶€. default=True max_samples: ë°ì´í„° ìƒ˜í”Œ ì¤‘ ì„ íƒí•  ìƒ˜í”Œì˜ ìˆ˜ í˜¹ì€ ë¹„ìœ¨. default=1.0 bootstrap_features: featureì˜ ì¤‘ë³µ ì‚¬ìš© ì—¬ë¶€. default=False max_features: ë‹¤ì°¨ì› ë…ë¦½ ë³€ìˆ˜ ì¤‘ ì„ íƒí•  ì°¨ì›ì˜ ìˆ˜ í˜¹ì€ ë¹„ìœ¨. default=1.0 12345678910111213141516171819202122232425262728from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVCfrom sklearn.ensemble import BaggingClassifieriris = load_iris()X, y = iris.data[:, [0, 2]], iris.targetmodel1 = DecisionTreeClassifier(max_depth=10, random_state=0).fit(X, y)model2 = BaggingClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100, random_state=0).fit(X, y)x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))plt.subplot(121)Z1 = model1.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)plt.contourf(xx, yy, Z1, alpha=0.6, cmap=mpl.cm.jet)plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=\"k\")plt.title(\"ê°œë³„ ëª¨í˜•\")plt.subplot(122)Z2 = model2.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)plt.contourf(xx, yy, Z2, alpha=0.6, cmap=mpl.cm.jet)plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=\"k\")plt.title(\"ë°°ê¹… ëª¨í˜•\")plt.suptitle(\"ë¶“ê½ƒ ë°ì´í„°ì˜ ë¶„ë¥˜ ê²°ê³¼\")plt.tight_layout()plt.show() ì™¼ìª½ì€ ë‹¨ì¼ ëª¨í˜•ìœ¼ë¡œ max_depth=10ìœ¼ë¡œ ì„¤ì •í•œ Decision treeì˜ decision boundaryì˜ ëª¨ìŠµì´ë‹¤. íŠ¸ë¦¬ì˜ ê¹Šì´ê°€ ê¹Šìœ¼ë¯€ë¡œ ê³¼ìµœì í™”(overfitting)ì´ ë°œìƒë˜ì—ˆë‹¤. ì˜¤ë¥¸ìª½ì˜ ê·¸ë¦¼ì€ max_depth=2ë¡œ ì„¤ì •í•œ Decision treeëª¨í˜•ì„ 100ê°œ ê²°í•©í•œ ë°°ê¹… ëª¨í˜•ì˜ decision boundary ëª¨ìŠµì´ë‹¤. ë¬¼ë¡  depthë¥¼ ì‘ê²Œí•˜ì—¬ ê¸°ë³¸ì ì¸ ëª¨í˜•ìì²´ë„ ê³¼ìµœì í™”(overfitting)ë¥¼ ë°©ì§€í•˜ì˜€ì§€ë§Œ, ë°°ê¹…ì„ í•¨ìœ¼ë¡œì¨ ëª¨í˜•ì˜ ë¶„ì‚°ì´ ì¤„ì–´ë“¤ë©° ë” train dataì— robustí•˜ê²Œ decision boundaryê°€ ê·¸ë ¤ì§„ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest) ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë°°ê¹…ì€ ì—¬ëŸ¬ ëª¨ë¸ë“¤ì„ ê²°í•©í•˜ì§€ë§Œ ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ë“¤ê°„ì˜ ì‚¬ìš©ë˜ì–´ì§€ëŠ” ë°ì´í„°ê°€ ë™ì¼í•œ ì§‘í•©ë“¤ì´ ìˆì„ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ Ensemble Learningì˜ ê°œë…ì—ì„œ ì–¸ê¸‰í–ˆì—ˆë˜ ê° ëª¨ë¸ë³„ ë…ë¦½ì´ë¼ëŠ” ê°€ì •ì— í¬ê²Œ ìœ„ë°˜ë˜ì–´ì§„ë‹¤. ê²°êµ­ ë¹„ìŠ·í•œ íŠ¸ë¦¬ê°€ ë§Œë“¤ì–´ì§€ê²Œ ë˜ì–´ ëª¨ë¸ë“¤ê°„ì˜ ê³µë¶„ì‚°ì´ í¬ê²Œ ë˜ì–´ ëª¨ë¸ì´ ë§ì•„ì§ì— ë”°ë¼ ì ì  ì „ì²´ Ensemble ëª¨ë¸ì˜ ë¶„ì‚°ì€ ì»¤ì§„ë‹¤ëŠ” ê²ƒì´ë‹¤. ë¶„ì‚°ì´ ì»¤ì§„ë‹¤ë©´ í¸í–¥ì´ ê°ì†Œë˜ì–´ ë” ì¢‹ì€ê²ƒì´ ì•„ë‹Œê°€ë¼ê³  ìƒê°ì´ ë“¤ìˆ˜ë„ ìˆê² ì§€ë§Œ, ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì˜ ë³€ë™í­ì´ ë„ˆë¬´ í¬ê²Œ ë˜ë©´(ë¶„ì‚°ì´ í¬ê²Œ ë˜ì–´) ê·¸ë§Œí¼ ë¶ˆí™•ì‹ ì„±ë„ ë†’ì•„ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ê²Œë‹¤ê°€, ì• ì´ˆì— ë‹¤ì–‘í•œ ëª¨ë¸ì— ëŒ€í•œ ê²°í•©ì„ í•œ Ensemble ëª¨ë¸ì„ ë§Œë“¤ë ¤ê³  í•œ ì˜ë„ì¡°ì°¨ ë³€ì§ˆë˜ì–´ì§„ë‹¤. ëœë¤í¬ë ˆìŠ¤íŠ¸(Random Forest)ëŠ” ì˜ì‚¬ ê²°ì • ë‚˜ë¬´(Decision Tree)ë¥¼ ê°œë³„ ëª¨í˜•ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ëª¨í˜• ê²°í•© ë°©ë²•ì„ ë§í•œë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ë°ì´í„° íŠ¹ì§•ì°¨ì›ì˜ ì¼ë¶€ë§Œ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•œë‹¤. í•˜ì§€ë§Œ ë…¸ë“œ ë¶„ë¦¬ì‹œ ëª¨ë“  ë…ë¦½ ë³€ìˆ˜ë“¤ì„ ë¹„êµí•˜ì—¬ ìµœì„ ì˜ ë…ë¦½ ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë…ë¦½ ë³€ìˆ˜ ì°¨ì›ì„ ëœë¤í•˜ê²Œ ê°ì†Œì‹œí‚¨ ë‹¤ìŒ ê·¸ ì¤‘ì—ì„œ ë…ë¦½ ë³€ìˆ˜ë¥¼ ì„ íƒí•œë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ê°œë³„ ëª¨í˜•ë“¤ ì‚¬ì´ì˜ ìƒê´€ê´€ê³„ê°€ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì— ëª¨í˜• ì„±ëŠ¥ì˜ ë³€ë™ì´ ê°ì†Œí•˜ëŠ” íš¨ê³¼ê°€ ìˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì„ ê·¹ë‹¨ì ìœ¼ë¡œ ì ìš©í•œ ê²ƒì´ Extremely Randomized Trees ëª¨í˜•ìœ¼ë¡œ ì´ ê²½ìš°ì—ëŠ” ê° ë…¸ë“œì—ì„œ ëœë¤í•˜ê²Œ ë…ë¦½ ë³€ìˆ˜ë¥¼ ì„ íƒí•œë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ì™€ Extremely Randomized Trees ëª¨í˜•ì€ ê°ê° RandomForestClassifier í´ë˜ìŠ¤ì™€ ExtraTreesClassifier í´ë˜ìŠ¤ë¡œ êµ¬í˜„ë˜ì–´ ìˆë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” CPU ë³‘ë ¬ ì²˜ë¦¬ë„ íš¨ê³¼ì ìœ¼ë¡œ ìˆ˜í–‰ë˜ì–´ ë¹ ë¥¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— ë’¤ì— ì†Œê°œí•  ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ë³´ë‹¤ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì•½ê°„ ë–¨ì–´ì§€ë”ë¼ë„ ëœë¤ í¬ë ˆìŠ¤íŠ¸ë¡œ ì¼ë‹¨ ê¸°ë°˜ ëª¨ë¸ì„ ë¨¼ì € êµ¬ì¶•í•˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ë©€í‹° ì½”ì–´ í™˜ê²½ì—ì„œëŠ” RandomForestClassifier ìƒì„±ìì™€ GridSearchCV ìƒì„± ì‹œ n_jobs = -1 íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€í•˜ë©´ ëª¨ë“  CPU ì½”ì–´ì„ ì´ìš©í•´ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. 12345678910111213141516171819202122232425262728from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVCfrom sklearn.ensemble import RandomForestClassifieriris = load_iris()X, y = iris.data[:, [0, 2]], iris.targetmodel1 = DecisionTreeClassifier(max_depth=10, random_state=0).fit(X, y)model2 = RandomForestClassifier(max_depth=2, n_estimators=100, random_state=0).fit(X, y)x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))plt.subplot(121)Z1 = model1.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)plt.contourf(xx, yy, Z1, alpha=0.6, cmap=mpl.cm.jet)plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=\"k\")plt.title(\"ê°œë³„ ëª¨í˜•\")plt.subplot(122)Z2 = model2.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)plt.contourf(xx, yy, Z2, alpha=0.6, cmap=mpl.cm.jet)plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=\"k\")plt.title(\"ë°°ê¹… ëª¨í˜•\")plt.suptitle(\"ë¶“ê½ƒ ë°ì´í„°ì˜ ë¶„ë¥˜ ê²°ê³¼\")plt.tight_layout()plt.show() ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì˜¤ë¥¸ìª½ì€ max_depth=2ë¡œ ì„¤ì •í•˜ê³  ëª¨í˜•ì˜ ìˆ˜ë¥¼ 100ê°œë¡œ í•œ RandomForest ëª¨ë¸ì˜ decision boundaryì˜ ì‹œê°í™”í•œ ê²ƒì´ë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ì¥ì  ì¤‘ í•˜ë‚˜ëŠ” ê° ë…ë¦½ ë³€ìˆ˜ì˜ ì¤‘ìš”ë„(feature importance)ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. í¬ë ˆìŠ¤íŠ¸ ì•ˆì—ì„œ ì‚¬ìš©ëœ ëª¨ë“  ë…¸ë“œì— ëŒ€í•´ ì–´ë–¤ ë…ë¦½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì˜€ê³  ê·¸ ë…¸ë“œì—ì„œ ì–»ì€ information gainì„ êµ¬í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ê°ê°ì˜ ë…ë¦½ ë³€ìˆ˜ë“¤ì´ ì–»ì–´ë‚¸ information gainì˜ í‰ê· ì„ ë¹„êµí•˜ë©´ ì–´ë–¤ ë…ë¦½ ë³€ìˆ˜ê°€ ì¤‘ìš”í•œì§€ë¥¼ ë¹„êµí•  ìˆ˜ ìˆë‹¤. 1234567891011121314151617181920from sklearn.datasets import make_classificationfrom sklearn.ensemble import ExtraTreesClassifierX, y = make_classification(n_samples=1000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0, n_classes=2, random_state=0, shuffle=False)forest = ExtraTreesClassifier(n_estimators=250, random_state=0)forest.fit(X, y)importances = forest.feature_importances_std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)indices = np.argsort(importances)[::-1]plt.title(\"íŠ¹ì„± ì¤‘ìš”ë„\")plt.bar(range(X.shape[1]), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")plt.xticks(range(X.shape[1]), indices)plt.xlim([-1, X.shape[1]])plt.show() ë‹¤ìŒì€ ì˜¬ë¦¬ë² í‹° ì–¼êµ´ ì‚¬ì§„ì„ Extreme ëœë¤ í¬ë ˆìŠ¤íŠ¸ë¡œ êµ¬í•œ ë’¤ íŠ¹ì§•(pixel) ì¤‘ìš”ë„ë¥¼ ì´ë¯¸ì§€ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤. 123456789101112131415161718from sklearn.datasets import fetch_olivetti_facesfrom sklearn.ensemble import ExtraTreesClassifierdata = fetch_olivetti_faces()X = data.datay = data.targetforest = ExtraTreesClassifier(n_estimators=1000, random_state=0)forest.fit(X, y)importances = forest.feature_importances_importances = importances.reshape(data.images[0].shape)plt.figure(figsize=(8, 8))plt.imshow(importances, cmap=plt.cm.bone_r)plt.grid(False)plt.title(\"í”½ì…€ ì¤‘ìš”ë„(pixel importance)\")plt.show() Boosting ì•ì—ì„œ ì–¸ê¸‰í–ˆë˜ Baggingì´ë‚˜ Random ForestsëŠ” ë¶€íŠ¸ìŠ¤íŠ¸ë© ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë½‘ê¸´í•´ë„ ê° ëª¨ë¸ì— ëŒ€í•´ ë…ë¦½ì ì´ë¼ê³  ê°€ì •í•˜ì§€ë§Œ, Boostingì€ resamplingì„ í•  ë•Œ ì˜¤ë¶„ë¥˜ëœ ë°ì´í„°ì— ë” ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ì„œ ì˜¤ë¶„ë¥˜ëœ ë°ì´í„°ê°€ ë½‘í í™•ë¥ ì´ ë†’ë„ë¡ í•˜ì—¬ ë³µì› ì¶”ì¶œì„ í•˜ê³  ë‹¤ì‹œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ë“¤ì´ Sequentialí•œ ê²ƒì´ë‹¤. ë¶€ìŠ¤íŠ¸(boost) ë°©ë²•ì€ ë¯¸ë¦¬ ì •í•´ì§„ ê°¯ìˆ˜ì˜ ëª¨í˜• ì§‘í•©ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•˜ë‚˜ì˜ ëª¨í˜•ì—ì„œ ì‹œì‘í•˜ì—¬ ëª¨í˜• ì§‘í•©ì— í¬í•¨í•  ê°œë³„ ëª¨í˜•ì„ í•˜ë‚˜ì”© ì¶”ê°€í•œë‹¤. ëª¨í˜•ì˜ ì§‘í•©ì€ ìœ„ì›íšŒ(commit) $C$ë¼ê³  í•˜ê³  $m$ê°œì˜ ëª¨í˜•ì„ í¬í•¨í•˜ëŠ” ìœ„ì›íšŒë¥¼ $C_{m}$ìœ¼ë¡œ í‘œì‹œí•œë‹¤. ìœ„ì›íšŒì— ë“¤ì–´ê°€ëŠ” ê°œë³„ ëª¨í˜•ì„ ì•½ ë¶„ë¥˜ê¸°(weak classifier)ë¼ê³  í•˜ë©° $k$ë¡œ í‘œì‹œí•œë‹¤. ë¶€ìŠ¤íŠ¸ ë°©ë²•ì˜ íŠ¹ì§•ì€ í•œë²ˆì— í•˜ë‚˜ì”© ëª¨í˜•ì„ ì¶”ê°€í•œë‹¤ëŠ” ê²ƒì´ë‹¤. C_1 = \\{ k_1 \\}C_2 = C_1 \\cup k_2 = \\{ k_1, k_2 \\}C_3 = C_2 \\cup k_3 = \\{ k_1, k_2, k_3 \\}\\vdotsC_m = C_{m-1} \\cup k_m = \\{ k_1, k_2, \\ldots, k_m \\} ê·¸ë¦¬ê³  më²ˆì§¸ë¡œ ìœ„ì›íšŒì— ì¶”ê°€í•  ê°œë³„ ëª¨í˜• $k_{m}$ì˜ ì„ íƒ ê¸°ì¤€ì€ ê·¸ ì „ë‹¨ê³„ì˜ ìœ„ì›íšŒ $C_{m-1}$ì˜ ì„±ëŠ¥ì„ ë³´ì™„í•˜ëŠ” ê²ƒì´ë‹¤. ìœ„ì›íšŒ $C_{m}$ì˜ ìµœì¢… ê²°ì •ì€ ë‹¤ìˆ˜ê²° ë°©ë²•ì„ ì‚¬ìš©í•˜ì§€ ì•Šê³  ê°ê°ì˜ ê°œë³„ ëª¨í˜•ì˜ ì¶œë ¥ì„ ê°€ì¤‘ì¹˜ $\\alpha$ë¡œ ê°€ì¤‘ ì„ í˜•ì¡°í•©í•œ ê°’ì„ íŒë³„í•¨ìˆ˜ë¡œ ì‚¬ìš©í•œë‹¤. ë˜í•œ ë¶€ìŠ¤íŠ¸ ë°©ë²•ì€ ì´ì§„ ë¶„ë¥˜ì—ë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©° $y$ê°’ì€ 1 ë˜ëŠ” -1ì˜ ê°’ì„ ê°€ì§„ë‹¤. y = -1 \\text{ or } 1C_{m}(x_i) = \\text{sign} \\left( \\alpha_1k_1(x_i) + \\cdots + \\alpha_{m}k_{m}(x_i) \\right)AdaBoost(ì—ì´ë‹¤ë¶€ìŠ¤íŠ¸) ì—ì´ë‹¤ ë¶€ìŠ¤íŠ¸(adaboost)ë¼ëŠ” ì´ë¦„ì€ ì ì‘ ë¶€ìŠ¤íŠ¸(adaptive boost)ë¼ëŠ” ìš©ì–´ì—ì„œ ë‚˜ì™”ë‹¤. ì—ì´ë‹¤ë¶€ìŠ¤íŠ¸ëŠ” ìœ„ì›íšŒì— ë„£ì„ ê°œë³„ ëª¨í˜• $k_{m}$ì„ ì„ ë³„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œí•™ìŠµë°ì´í„° ì§‘í•©ì˜ $i$ë²ˆì§¸ ë°ì´í„°ì— ê°€ì¤‘ì¹˜ $w_{i}$ë¥¼ ì£¼ê³  ë¶„ë¥˜ ëª¨í˜•ì´ í‹€ë¦¬ê²Œ ì˜ˆì¸¡í•œ ë°ì´í„°ì˜ ê°€ì¤‘ì¹˜ë¥¼ í•©í•œ ê°’ì„ ì†ì‹¤í•¨ìˆ˜ $L$ë¡œ ì‚¬ìš©í•œë‹¤. ì´ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ëª¨í˜•ì´ k_{m}ìœ¼ë¡œ ì„ íƒëœë‹¤. L_m = \\sum_{i=1}^N w_{m,i} I\\left(k_m(x_i) \\neq y_i\\right) ìœ„ ì‹ì—ì„œ $I$ëŠ” $k(x_{i}) \\neq y_{i}$ë¼ëŠ” ì¡°ê±´ì´ ë§Œì¡±ë˜ë©´ 1, ì•„ë‹ˆë©´ 0ì„ ê°–ëŠ” indicator functionì´ë‹¤. ì¦‰ ì˜ˆì¸¡ì„ í‹€ë¦¬ê²Œí•œ ë°ì´í„°ë“¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ì˜ í•©ì´ë‹¤. ìœ„ì›íšŒ $C_{m}$ì— í¬í•¨ë  ê°œë³„ ëª¨í˜• $k_{m}$ì´ ì„ íƒëœ í›„ì—ëŠ” ê°€ì¤‘ì¹˜ $\\alpha_{m}$ì„ ê²°ì •í•´ì•¼ í•œë‹¤. ì´ ê°’ì€ ë‹¤ìŒì²˜ëŸ¼ ê³„ì‚°í•œë‹¤. \\epsilon_m = \\dfrac{\\sum_{i=1}^N w_{m,i} I\\left(k_m(x_i) \\neq y_i\\right)}{\\sum_{i=1}^N w_{m,i}}\\alpha_m = \\frac{1}{2}\\log\\left( \\frac{1 - \\epsilon_m}{\\epsilon_m}\\right) ë°ì´í„°ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ $w_{m, i}$ëŠ” ìµœì´ˆì—ëŠ”$(m=1)$ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ ë™ì¼í•œ ê°’ì„ ê°–ì§€ë§Œ, ìœ„ì›íšŒê°€ ì¦ê°€í•˜ë©´ì„œ ê°’ì´ ë°”ë€ë‹¤. ê°€ì¤‘ì¹˜ì˜ ê°’ì€ ì§€ì‹œí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ„ì›íšŒ $C_{m-1}$ì´ ë§ì¶˜ ë¬¸ì œëŠ” ì‘ê²Œ, í‹€ë¦° ë¬¸ì œëŠ” í¬ê²Œ í™•ëŒ€(boosting)ëœë‹¤. w_{m,i} = w_{m-1,i} \\exp (-y_iC_{m-1}) = \\begin{cases} w_{m-1,i}e^{-1} & \\text{ if } C_{m-1} = y_i\\\\ w_{m-1,i}e & \\text{ if } C_{m-1} \\neq y_i \\end{cases} $m$ë²ˆì§¸ ë©¤ë²„ì˜ ëª¨ë“  í›„ë³´ì— ëŒ€í•´ ìœ„ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ ê°€ì¥ ê°’ì´ ì‘ì€ í›„ë³´ë¥¼ $m$ë²ˆì§¸ ë©¤ë²„ë¡œ ì„ ì •í•œë‹¤. ì—ì´ë‹¤ ë¶€ìŠ¤íŒ…ì€ ì‚¬ì‹¤ ë‹¤ìŒê³¼ ê°™ì€ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” $C_{m}$ì„ ì°¾ì•„ê°€ëŠ” ë°©ë²•ì´ë¼ëŠ” ê²ƒì„ ì¦ëª…í•  ìˆ˜ ìˆë‹¤. L_m = \\sum_{i=1}^N \\exp(âˆ’y_i C_m(x_i)) ê°œë³„ ë©¤ë²„ $k_{m}$ê³¼ ìœ„ì›íšŒ ê´€ê³„ëŠ” C_m(x_i) = \\sum_{j=1}^m \\alpha_j k_j(x_i) = C_{m-1}(x_i) + \\alpha_m k_m(x_i) ì´ê³  ì´ ì‹ì„ ëŒ€ì…í•˜ë©´ \\begin{eqnarray} L_m &=& \\sum_{i=1}^N \\exp(âˆ’y_i C_m(x_i)) \\\\ &=& \\sum_{i=1}^N \\exp\\left(âˆ’y_iC_{m-1}(x_i) - \\alpha_m y_i k_m(x_i) \\right) \\\\ &=& \\sum_{i=1}^N \\exp(âˆ’y_iC_{m-1}(x_i)) \\exp\\left(-\\alpha_m y_i k_m(x_i)\\right) \\\\ &=& \\sum_{i=1}^N w_{m,i} \\exp\\left(-\\alpha_m y_i k_m(x_i)\\right) \\\\ \\end{eqnarray} $y_{i}$ì™€ $k_{M}(x_{i})$ 1 ë˜ëŠ” -1ê°’ë§Œ ê°€ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ì´ìš©í•˜ë©´, \\begin{eqnarray} L_m &=& e^{-\\alpha_m}\\sum_{k_m(x_i) = y_i} w_{m,i} + e^{\\alpha_m}\\sum_{k_m(x_i) \\neq y_i} w_{m,i} \\\\ &=& \\left(e^{\\alpha_m}-e^{-\\alpha_m}\\right) \\sum_{i=1}^N w_{m,i} I\\left(k_m(x_i) \\neq y_i\\right) + e^{-\\alpha_m}\\sum_{i=1}^N w_{m,i} \\end{eqnarray} $L_{m}$ì„ ìµœì†Œí™”í•˜ë ¤ë©´ $\\sum_{i=1}^N w_{m,i} I\\left(k_m(x_i) \\neq y_i\\right)$ì„ ìµœì†Œí™”í•˜ëŠ” $k_{m}$ í•¨ìˆ˜ë¥¼ ì°¾ì€ ë‹¤ìŒ $L_{m}$ì„ ìµœì†Œí™”í•˜ëŠ” $\\alpha_{m}$ì„ ì°¾ì•„ì•¼ í•œë‹¤. \\dfrac{d L_m}{d \\alpha_m} = 0 ì´ ì¡°ê±´ìœ¼ë¡œë¶€í„° $\\alpha_{m}$ ê³µì‹ì„ ìœ ë„í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒì€ Scikit-Learnì˜ ensemble ì„œë¸ŒíŒ¨í‚¤ì§€ê°€ ì œê³µí•˜ëŠ” AdaBoostClassifier í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ ì˜ˆì¸¡ì„ í•˜ëŠ” ì˜ˆì´ë‹¤. ì•½ë¶„ë¥˜ê¸°ë¡œëŠ” ê¹Šì´ê°€ 1ì¸ ë‹¨ìˆœí•œ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¥¼ ì±„íƒí•˜ì˜€ë‹¤. ì—¬ê¸°ì—ì„œëŠ” ê° í‘œë³¸ ë°ì´í„°ì˜ ê°€ì¤‘ì¹˜ ê°’ì„ ì•Œì•„ë³´ê¸° ìœ„í•´ ê¸°ì¡´ì˜ AdaBoostClassifier í´ë˜ìŠ¤ë¥¼ ì„œë¸Œ í´ë˜ì‹±í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì†ì„±ìœ¼ë¡œ ì €ì¥í•˜ë„ë¡ ìˆ˜ì •í•œ ëª¨í˜•ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from sklearn.datasets import make_gaussian_quantilesfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import AdaBoostClassifierX1, y1 = make_gaussian_quantiles(cov=2., n_samples=100, n_features=2, n_classes=2, random_state=1)X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5, n_samples=200, n_features=2, n_classes=2, random_state=1)X = np.concatenate((X1, X2))y = np.concatenate((y1, - y2 + 1))class MyAdaBoostClassifier(AdaBoostClassifier): def __init__(self, base_estimator=None, n_estimators=50, learning_rate=1., algorithm='SAMME.R', random_state=None): super(MyAdaBoostClassifier, self).__init__( base_estimator=base_estimator, n_estimators=n_estimators, learning_rate=learning_rate, random_state=random_state) self.sample_weight = [None] * n_estimators def _boost(self, iboost, X, y, sample_weight, random_state): sample_weight, estimator_weight, estimator_error = \\ super(MyAdaBoostClassifier, self)._boost(iboost, X, y, sample_weight, random_state) self.sample_weight[iboost] = sample_weight.copy() return sample_weight, estimator_weight, estimator_errormodel_ada = MyAdaBoostClassifier(DecisionTreeClassifier(max_depth=1, random_state=0), n_estimators=20)model_ada.fit(X, y)def plot_result(model, title=\"ë¶„ë¥˜ê²°ê³¼\", legend=False, s=50): x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02), np.arange(x2_min, x2_max, 0.02)) if isinstance(model, list): Y = model[0].predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape) for i in range(len(model) - 1): Y += model[i + 1].predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape) else: Y = model.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape) cs = plt.contourf(xx1, xx2, Y, cmap=plt.cm.Paired, alpha=0.5) for i, n, c in zip(range(2), \"01\", \"br\"): idx = np.where(y == i) plt.scatter(X[idx, 0], X[idx, 1], c=c, s=s, alpha=0.5, label=\"Class %s\" % n) plt.xlim(x1_min, x1_max) plt.ylim(x2_min, x2_max) plt.xlabel('x1') plt.ylabel('x2') plt.title(title) plt.colorbar(cs) if legend: plt.legend() plt.grid(False)plot_result(model_ada, \"ì—ì´ë‹¤ë¶€ìŠ¤íŠ¸(m=20) ë¶„ë¥˜ ê²°ê³¼\") 123456789101112131415161718plt.figure(figsize=(10, 15))plt.subplot(421);plot_result(model_ada.estimators_[0], \"1ë²ˆ ë¶„ë¥˜ëª¨í˜•ì˜ ë¶„ë¥˜ ê²°ê³¼\", s=10)plt.subplot(422);plot_result(model_ada.estimators_[1], \"2ë²ˆ ë¶„ë¥˜ëª¨í˜•ì˜ ë¶„ë¥˜ ê²°ê³¼\", s=(4000*model_ada.sample_weight[0]).astype(int))plt.subplot(423);plot_result(model_ada.estimators_[2], \"3ë²ˆ ë¶„ë¥˜ëª¨í˜•ì˜ ë¶„ë¥˜ ê²°ê³¼\", s=(4000*model_ada.sample_weight[1]).astype(int))plt.subplot(424);plot_result(model_ada.estimators_[3], \"4ë²ˆ ë¶„ë¥˜ëª¨í˜•ì˜ ë¶„ë¥˜ ê²°ê³¼\", s=(4000*model_ada.sample_weight[2]).astype(int))plt.subplot(425);plot_result(model_ada.estimators_[4], \"5ë²ˆ ë¶„ë¥˜ëª¨í˜•ì˜ ë¶„ë¥˜ ê²°ê³¼\", s=(4000*model_ada.sample_weight[3]).astype(int))plt.subplot(426);plot_result(model_ada.estimators_[5], \"6ë²ˆ ë¶„ë¥˜ëª¨í˜•ì˜ ë¶„ë¥˜ ê²°ê³¼\", s=(4000*model_ada.sample_weight[4]).astype(int))plt.subplot(427);plot_result(model_ada.estimators_[6], \"7ë²ˆ ë¶„ë¥˜ëª¨í˜•ì˜ ë¶„ë¥˜ ê²°ê³¼\", s=(4000*model_ada.sample_weight[5]).astype(int))plt.subplot(428);plot_result(model_ada.estimators_[7], \"8ë²ˆ ë¶„ë¥˜ëª¨í˜•ì˜ ë¶„ë¥˜ ê²°ê³¼\", s=(4000*model_ada.sample_weight[6]).astype(int))plt.tight_layout() Adaboost ëª¨í˜•ì˜ ì •ê·œí™” Adaboost ëª¨í˜•ì´ ê³¼ìµœì í™”(overfitting)ê°€ ë˜ëŠ” ê²½ìš°ì—ëŠ” í•™ìŠµ ì†ë„(learning rate)ë¥¼ ì¡°ì •í•˜ì—¬ ì •ê·œí™”ë¥¼ í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” í•„ìš”í•œ ë©¤ë²„ì˜ ìˆ˜ë¥¼ ê°•ì œë¡œ ì¦ê°€ì‹œì¼œì„œ ê³¼ìµœì í™”ë¥¼ ë§‰ëŠ” ì—­í• ì„ í•œë‹¤. ì¦‰, ìƒˆë¡­ê²Œ ì ìš©ë˜ëŠ” ëª¨í˜•ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì—¬ì„œ ë™ì¼í•œ ëª¨í˜•ì˜ íšŸìˆ˜ë¥¼ ê±°ì¹˜ë”ë¼ë„ ê°€ì¤‘ì¹˜ê°€ í¬ê²Œ ì˜í–¥ì„ ë°›ì§€ ì•Šë„ë¡ í•˜ì—¬ ê³¼ìµœì í™”ë¥¼ ì—†ì• ëŠ” ë°©ë²•ì´ë‹¤. C_m = C_{m-1} + \\mu \\alpha_m k_m AdaBoostClassifier í´ë˜ìŠ¤ì—ì„œëŠ” learning_rateì¸ìˆ˜ë¥¼ 1ë³´ë‹¤ ì ê²Œ ì£¼ë©´ ìƒˆë¡œìš´ ë©¤ë²„ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°•ì œë¡œ ë‚®ì¶˜ë‹¤. ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… (Gradient boosting) ê¸°ë³¸ì ìœ¼ë¡œ ë¶€ìŠ¤íŒ…ì€ ë‹¤ìŒ Roundì—ì„œ ì´ì „ì— ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„°ë“¤ì— ëŒ€í•œ ì²˜ë¦¬ë¥¼ ì–´ë–»ê²Œ í•˜ëŠëƒì— ë”°ë¼ ì¢…ë¥˜ë³„ë¡œ ì°¨ì´ê°€ ì¡´ì¬í•œë‹¤. Gradient Boostingì€ ì´ì „ Roundì˜ ë¶„ë¥˜ê¸°ë¡œ ì˜ˆì¸¡í•œ errorë¥¼ ë‹¤ìŒ Roundì˜ ë¶„ë¥˜ê¸°ê°€ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµí•˜ë©´ì„œ ì§„í–‰í•œë‹¤. ì´ì „ ëª¨ë¸ì˜ errorë¥¼ ë‹¤ìŒ ëª¨ë¸ì´ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œë” í•™ìŠµì‹œì¼œ í•´ë‹¹ ë¶„ë¥˜ê¸°ë“¤ì˜ í•™ìŠµëœ ê²°ê³¼ë¥¼ ê³„ì†í•´ì„œ í•©í•´ ë‚˜ê°€ë©´ ë§ˆì§€ë§‰ì—ëŠ” ìµœì†Œí•œì˜ errorë§Œ ë‚¨ìœ¼ë¯€ë¡œ, errorë¥¼ ìµœëŒ€í•œ ì¤„ì¼ ìˆ˜ ìˆê²Œ ëœë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ ê²ƒê³¼ ê°™ì´ errorë¥¼ ì˜ˆì¸¡í•˜ê²Œ í•˜ë¯€ë¡œ ì´í•´í•˜ê¸° ì‰½ê²Œ regressionì„ í†µí•œ ì˜ˆì‹œë¡œ ì„¤ëª…í•˜ê² ë‹¤. ì²˜ìŒ ëª¨ë¸ì˜ errorë¥¼ ë‹¤ìŒ ëª¨ë¸ì€ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•˜ë¯€ë¡œ ì´ì „ ëª¨ë¸ë³´ë‹¤ ì˜¤ì°¨ê°€ ë” ì¤„ì–´ë“¤ ê²ƒì´ë‹¤. ê·¸ ë‹¤ìŒ ëª¨ë¸ë„ ì´ì „ ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ í•™ìŠµí•˜ê²Œ ë˜ë¯€ë¡œ ë” ì˜¤ì°¨ê°€ ì¤„ì–´ë“¤ ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ìµœì¢…ì ìœ¼ë¡œëŠ” errorê°€ ìµœëŒ€í•œ 0ì— ê°€ê¹Œì›Œì§ˆ ë•Œ ê¹Œì§€ í•™ìŠµí•˜ì—¬ train setì— ëŒ€í•´ì„œëŠ” ê³¼ìµœì í™”ê°€ ì´ë£¨ì–´ ì§ˆ ê²ƒì´ë‹¤. ìµœì¢…ì ìœ¼ë¡œëŠ” í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ errorë¥¼ ì‘ê²Œ í•˜ëŠ” ê²ƒì´ë¯€ë¡œ ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ negative gradientë¥¼ ìµœì†Œí™”ì‹œí‚¤ë©´ì„œ í•™ìŠµ ë  ê²ƒì´ë‹¤. ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŠ¸ ëª¨í˜•ì€ ë³€ë¶„ë²•(calculus of variations)ì„ ì‚¬ìš©í•œ ëª¨í˜•ì´ë‹¤. í•™ìŠµ $f(x)$ë¥¼ ìµœì†Œí™”í•˜ëŠ” $x$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ gradient descent ë°©ë²•ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ìˆë‹¤. x_{m} = x_{m-1} - \\alpha_m \\dfrac{df}{dx} ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŠ¸ ëª¨í˜•ì—ì„œëŠ” ì†ì‹¤ ë²”í•¨ìˆ˜(loss functional) $L(y, C_{m-1})$ì„ ìµœì†Œí™”í•˜ëŠ” ê°œë³„ ë¶„ë¥˜í•¨ìˆ˜ $k_{m}$ë¥¼ ì°¾ëŠ”ë‹¤. ì´ë¡ ì ìœ¼ë¡œ ê°€ì¥ ìµœì ì˜ í•¨ìˆ˜ëŠ” ë²”í•¨ìˆ˜ì˜ ë¯¸ë¶„ì´ë‹¤. C_{m} = C_{m-1} - \\alpha_m \\dfrac{\\delta L(y, C_{m-1})}{\\delta C_{m-1}} = C_{m-1} + \\alpha_m k_m ë”°ë¼ì„œ ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŠ¸ ëª¨í˜•ì€ ë¶„ë¥˜/íšŒê·€ ë¬¸ì œì— ìƒê´€ì—†ì´ ê°œë³„ ë©¤ë²„ ëª¨í˜•ìœ¼ë¡œ íšŒê·€ë¶„ì„ ëª¨í˜•ì„ ì‚¬ìš©í•œë‹¤. ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” íšŒê·€ë¶„ì„ ëª¨í˜•ì€ ì˜ì‚¬ê²°ì • íšŒê·€ë‚˜ë¬´(decision tree regression model)ëª¨í˜•ì´ë‹¤. ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŠ¸ ëª¨í˜•ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ ë©¤ë²„ì™€ ê·¸ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì‚°í•œë‹¤. $-\\tfrac{\\delta L(y, C_m)}{\\delta C_m}$ë¥¼ ëª©í‘œê°’ìœ¼ë¡œ ê°œë³„ ë©¤ë²„ ëª¨í˜• $k_{m}$ì„ ì°¾ëŠ”ë‹¤. $ \\left( y - (C_{m-1} + \\alpha_m k_m) \\right)^2 $ ë¥¼ ìµœì†Œí™”í•˜ëŠ” ìŠ¤í…ì‚¬ì´ì¦ˆ $\\alpha_{m}$ì„ ì°¾ëŠ”ë‹¤. $ C_m = C_{m-1} + \\alpha_m k_m $ ìµœì¢… ëª¨í˜•ì„ ê°±ì‹ í•œë‹¤. ë§Œì•½ ì†ì‹¤ ë²”í•¨ìˆ˜ê°€ ì˜¤ì°¨ ì œê³± í˜•íƒœë¼ë©´ L(y, C_{m-1}) = \\dfrac{1}{2}(y - C_{m-1})^2 ë²”í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ì‹¤ì œ ëª©í‘œê°’ $y$ì™€ $C_{m-1}$ê³¼ì˜ ì°¨ì´ ì¦‰, ì”ì°¨(residual)ê°€ ëœë‹¤. -\\dfrac{dL(y, C_m)}{dC_m} = y - C_{m-1} Scikit-Learnì˜ GradientBoostingClassifierëŠ” ì•½í•œ í•™ìŠµê¸°ì˜ ìˆœì°¨ì ì¸ ì˜ˆì¸¡ ì˜¤ë¥˜ ë³´ì •ì„ í†µí•´ í•™ìŠµì„ ìˆ˜í–‰í•˜ë¯€ë¡œ ë©€í‹° CPU ì½”ì–´ ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ë”ë¼ë„ ë³‘ë ¬ì²˜ë¦¬ê°€ ì§€ì›ë˜ì§€ ì•Šì•„ì„œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ê²½ìš° í•™ìŠµì— ë§¤ìš° ë§ì€ ì‹œê°„ì´ í•„ìš”í•˜ë‹¤. ë˜í•œ ì¼ë°˜ì ìœ¼ë¡œ GBMì´ ëœë¤ í¬ë ˆìŠ¤íŠ¸ë³´ë‹¤ëŠ” ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì¡°ê¸ˆ ë›°ì–´ë‚œ ê²½ìš°ê°€ ë§ë‹¤. ê·¸ëŸ¬ë‚˜ ìˆ˜í–‰ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³ , í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ ë…¸ë ¥ë„ ë” í•„ìš”í•˜ë‹¤. loss: ê²½ì‚¬ í•˜ê°•ë²•ì—ì„œ ì‚¬ìš©í•  ë¹„ìš© í•¨ìˆ˜ë¥¼ ì €ì¥í•œë‹¤. íŠ¹ë³„í•œ ì´ìœ ê°€ ì—†ìœ¼ë©´ defaultì¸ â€˜devianceâ€™ë¥¼ ê·¸ëŒ€ë¡œ ì ìš©í•œë‹¤. learning_rate: GBMì´ í•™ìŠµì„ ì§„í–‰í•  ë•Œë§ˆë‹¤ ì ìš©í•˜ëŠ” í•™ìŠµë¥ ì´ë‹¤. Weak learnerê°€ ìˆœì°¨ì ìœ¼ë¡œ ì˜¤ë¥˜ ê°’ì„ ë³´ì •í•´ ë‚˜ê°€ëŠ” ë° ì ìš©í•˜ëŠ” ê³„ìˆ˜ì´ë‹¤. 0~1 ì‚¬ì´ì˜ ê°’ì„ ì§€ì •í•  ìˆ˜ ìˆìœ¼ë©° default=0.1ì´ë‹¤. ë„ˆë¬´ ì‘ì€ ê°’ì„ ì ìš©í•˜ë©´ ì—…ë°ì´íŠ¸ ë˜ëŠ” ê°’ì´ ì‘ì•„ì ¸ì„œ ìµœì†Œ ì˜¤ë¥˜ ê°’ì„ ì°¾ì•„ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë†’ì•„ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. í•˜ì§€ë§Œ ë§ì€ weak learnerëŠ” ìˆœì°¨ì ì¸ ë°˜ë³µì´ í•„ìš”í•´ì„œ ìˆ˜í–‰ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³ , ë˜ ë„ˆë¬´ ì‘ê²Œ ì„¤ì •í•˜ë©´ ëª¨ë“  weak learnerì˜ ë°˜ë³µì´ ì™„ë£Œë¼ë„ ìµœì†Œ ì˜¤ë¥˜ ê°’ì„ ì°¾ì§€ ëª»í•  ìˆ˜ ìˆë‹¤. ë°˜ëŒ€ë¡œ í° ê°’ì„ ì ìš©í•˜ë©´ ìµœì†Œ ì˜¤ë¥˜ ê°’ì„ ì°¾ì§€ ëª»í•˜ê³  ê·¸ëƒ¥ ì§€ë‚˜ì±  ë²„ë ¤ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ì§€ë§Œ, ë¹ ë¥¸ ìˆ˜í–‰ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ëŸ¬í•œ íŠ¹ì„± ë•Œë¬¸ì— learning_rateëŠ” n_estimatorsì™€ ìƒí˜¸ ë³´ì™„ì ìœ¼ë¡œ ì¡°í•©í•´ ì‚¬ìš©í•œë‹¤. learning_rateë¥¼ ì‘ê²Œí•˜ê³  n_estimatorsë¥¼ í¬ê²Œ í•˜ë©´ ë” ì´ìƒ ì„±ëŠ¥ì´ ì¢‹ì•„ì§€ì§€ ì•ŠëŠ” í•œê³„ì ê¹Œì§€ëŠ” ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì¡°ê¸ˆì”© ì¢‹ì•„ì§ˆ ìˆ˜ ìˆë‹¤. subsample: weak learnerê°€ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ë°ì´í„°ì˜ ìƒ˜í”Œë§ ë¹„ìœ¨ì´ë‹¤. default=1ì´ë©°, ì´ëŠ” ì „ì²´ í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.(0.5ì´ë©´ í•™ìŠµë°ì´í„°ì˜ 50%ë¥¼ ì˜ë¯¸) ê³¼ì í•©ì´ ì—¼ë ¤ë˜ëŠ” ê²½ìš° subsampleì„ 1ë³´ë‹¤ ì‘ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•œë‹¤. 123from sklearn.ensemble import GradientBoostingClassifiermodel_grad = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=0) 12%%timemodel_grad.fit(X, y) ê²°ê³¼12345678910111213CPU times: user 50 ms, sys: 0 ns, total: 50 msWall time: 50.4 msGradientBoostingClassifier(criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance', max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100, n_iter_no_change=None, presort='auto', random_state=0, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) 1plot_result(model_grad) 1234567plt.subplot(121)plot_result(model_grad.estimators_[3][0])plt.subplot(122)plot_result([model_grad.estimators_[0][0], model_grad.estimators_[1][0], model_grad.estimators_[2][0], model_grad.estimators_[3][0]]) XGBoost XGboostëŠ” GBMì— ê¸°ë°˜í•˜ê³  ìˆì§€ë§Œ, GBMì˜ ë‹¨ì ì¸ ëŠë¦° ìˆ˜í–‰ ì‹œê°„ ë° ê³¼ì í•© ê·œì œ(Regularization) ë¶€ì¬ ë“±ì˜ ë¬¸ì œë¥¼ í•´ê²°í•´ì„œ ë§¤ìš° ê°ê´‘ì„ ë°›ê³  ìˆë‹¤. íŠ¹íˆ XGBoostëŠ” ë³‘ë ¬ CPU í™˜ê²½ì—ì„œ ë³‘ë ¬ í•™ìŠµì´ ê°€ëŠ¥í•´ ê¸°ì¡´ GBMë³´ë‹¤ ë¹ ë¥´ê²Œ í•™ìŠµì„ ì™„ë£Œí•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒì€ XGboostì˜ ì¥ì ì´ë‹¤. í•­ëª© ì„¤ëª… ë›°ì–´ë‚œ ì˜ˆì¸¡ì„±ëŠ¥ ì¼ë°˜ì ìœ¼ë¡œ ë¶„ë¥˜ì™€ íšŒê·€ ì˜ì—­ì—ì„œ ë›°ì–´ë‚œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤. GBM ëŒ€ë¹„ ë¹ ë¥¸ ìˆ˜í–‰ ì‹œê°„ ì¼ë°˜ì ì¸ GBMì€ ìˆœì°¨ì ìœ¼ë¡œ Weak Learnerê°€ ê°€ì¤‘ì¹˜ë¥¼ ì¦ê°í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ì „ë°˜ì ìœ¼ë¡œ \\\\ ì†ë„ê°€ ëŠë¦¬ë‹¤. í•˜ì§€ë§Œ XGBoostëŠ” ë³‘ë ¬ ìˆ˜í–‰ ë° ë‹¤ì–‘í•œ ê¸°ëŠ¥ìœ¼ë¡œ GBMì— ë¹„í•´ ë¹ ë¥¸ ìˆ˜í–‰ ì„±ëŠ¥ì„ ë³´ì¥í•œë‹¤.\\\\ ì•„ì‰½ê²Œë„ XGBoostê°€ ì¼ë°˜ì ì¸ GBMì— ë¹„í•´ ìˆ˜í–‰ ì‹œê°„ì´ ë¹ ë¥´ë‹¤ëŠ” ê²ƒì´ì§€, ë‹¤ë¥¸ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜\\\\ (ì˜ˆë¥¼ ë“¤ì–´ ëœë¤ í¬ë ˆìŠ¤íŠ¸)ì— ë¹„í•´ì„œ ë¹ ë¥´ë‹¤ëŠ” ì˜ë¯¸ëŠ” ì•„ë‹ˆë‹¤. ê³¼ì í•© ê·œì œ\\\\ (Regularization) í‘œì¤€ GBMì˜ ê²½ìš° ê³¼ì í•© ê·œì œ ê¸°ëŠ¥ì´ ì—†ìœ¼ë‚˜ XGBoostëŠ” ìì²´ì— ê³¼ì í•© ê·œì œ ê¸°ëŠ¥ìœ¼ë¡œ ê³¼ì í•©ì— \\\\ ì¢€ ë” ê°•í•œ ë‚´êµ¬ì„±ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. Tree pruning (ë‚˜ë¬´ ê°€ì§€ì¹˜ê¸°) ì¼ë°˜ì ìœ¼ë¡œ GBMì€ ë¶„í•  ì‹œ ë¶€ì • ì†ì‹¤ì´ ë°œìƒí•˜ë©´ ë¶„í• ì„ ë” ì´ìƒ ìˆ˜í–‰í•˜ì§€ ì•Šì§€ë§Œ, ì´ëŸ¬í•œ ë°©ì‹ë„ ìì¹«\\\\ ì§€ë‚˜ì¹˜ê²Œ ë§ì€ ë¶„í• ì„ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ë‹¤ë¥¸ GBMê³¼ ë§ˆì°¬ê°€ì§€ë¡œ XGBoostë„ max_depth íŒŒë¼ë¯¸í„°ë¡œ \\\\ ë¶„í•  ê¹Šì´ë¥¼ ì¡°ì •í•˜ê¸°ë„ í•˜ì§€ë§Œ, tree pruningìœ¼ë¡œ ë” ì´ìƒ ê¸ì • ì´ë“ì´ ì—†ëŠ” ë¶„í• ì„ ê°€ì§€ì¹˜ê¸° í•´ì„œ\\\\ ë¶„í•  ìˆ˜ë¥¼ ë” ì¤„ì´ëŠ” ì¶”ê°€ì ì¸ ì¥ì ì„ ê°€ì§€ê³  ìˆë‹¤. ìì²´ ë‚´ì¥ëœ êµì°¨ ê²€ì¦ XGBoostëŠ” ë°˜ë³µ ìˆ˜í–‰ ì‹œë§ˆë‹¤ ë‚´ë¶€ì ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ì„¸íŠ¸ì™€ í‰ê°€ ë°ì´í„° ì„¸íŠ¸ì— ëŒ€í•œ êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•´\\\\ ìµœì í™”ëœ ë°˜ë³µ ìˆ˜í–‰ íšŸìˆ˜ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ì§€ì •ëœ ë°˜ë³µ íšŸìˆ˜ê°€ ì•„ë‹ˆë¼ êµì°¨ ê²€ì¦ì„ í†µí•´ í‰ê°€ ë°ì´í„° ì„¸íŠ¸ì˜\\\\ í‰ê°€ê°’ì´ ìµœì í™” ë˜ë©´ ë°˜ë³µì„ ì¤‘ê°„ì— ë©ˆì¶œ ìˆ˜ ìˆëŠ” ì¡°ê¸° ì¤‘ë‹¨ ê¸°ëŠ¥ì´ ìˆë‹¤. ê²°ì†ê°’ ìì²´ ì²˜ë¦¬ XGBoostëŠ” ê²°ì†ê°’ì„ ìì²´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆë‹¤. XGBoostì˜ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” C/C++ë¡œ ì‘ì„±ë¼ ìˆë‹¤. XGBoost ê°œë°œ ê·¸ë£¹ì€ íŒŒì´ì¬ì—ì„œë„ XGBoostë¥¼ êµ¬ë™í•  ìˆ˜ ìˆë„ë¡ íŒŒì´ì¬ íŒ¨í‚¤ì§€ë¥¼ ì œê³µí•œë‹¤. ì´ íŒŒì´ì¬ íŒ¨í‚¤ì§€ì˜ ì—­í• ì€ ëŒ€ë¶€ë¶„ C/C++ í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í˜¸ì¶œí•˜ëŠ” ê²ƒì´ë‹¤. 12345# windowìš©# conda install -c anaconda py-xgboost# Linuxìš©conda install -c conda-forge xgboost python ë˜í¼ ëª¨ë“ˆê³¼ Scikit-Learn ë˜í¼ XGBoost ëª¨ë“ˆì˜ ì¼ë¶€ hyper-parameterëŠ” ì•½ê°„ ë‹¤ë¥´ë¯€ë¡œ ì´ì— ëŒ€í•œ ì£¼ì˜ê°€ í•„ìš”í•˜ë‹¤. python ë˜í¼ XGBoost ëª¨ë“ˆ XGboost ê³ ìœ ì˜ í”„ë ˆì„ì›Œí¬ë¥¼ python ì–¸ì–´ ê¸°ë°˜ì—ì„œ êµ¬í˜„í•œ ê²ƒìœ¼ë¡œ ë³„ë„ì˜ APIê¸°ë°˜ì„ ê°–ê³  ìˆì–´ Scikit-Learn í”„ë ˆì„ì›Œí¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ê²ƒì´ ì•„ë‹ˆê¸°ì— Scikit-Learnì˜ fit(), predict() ë©”ì„œë“œ ê°™ì€ Scikit-Learn ê³ ìœ ì˜ ì•„í‚¤í…ì²˜ì™€ ë‹¤ë¥¸ ë‹¤ì–‘í•œ ìœ í‹¸ë¦¬í‹°(cross_val_score, GridSearchCV, Pipeline ë“±)ì™€ í•¨ê»˜ ì‚¬ìš©ë  ìˆ˜ ì—†ë‹¤. ì¼ë°˜ parameter ì¼ë°˜ì ìœ¼ë¡œ ì‹¤í–‰ ì‹œ threadì˜ ê°œìˆ˜ë‚˜ silent ëª¨ë“œ ë“±ì˜ ì„ íƒì„ ìœ„í•œ parameterë¡œì„œ default parameter ê°’ì„ ë°”ê¾¸ëŠ” ê²½ìš°ëŠ” ê±°ì˜ ì—†ë‹¤. booster : gbtree(tree based model) ë˜ëŠ” gblinear(linear model)ì„ íƒ default=gbtree silent : default=0ì´ë©°, ì¶œë ¥ ë©”ì‹œì§€ë¥¼ ë‚˜íƒ€ë‚´ê³  ì‹¶ì§€ ì•Šì„ ê²½ìš° 1ë¡œ ì„¤ì •í•œë‹¤. nthread : CPUì˜ ì‹¤í–‰ thread ê°œìˆ˜ë¥¼ ì¡°ì •í•˜ë©°, defaultëŠ” CPUì˜ ì „ì²´ threadë¥¼ ë‹¤ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. Multi Core/thread CPU ì‹œìŠ¤í…œì—ì„œ ì „ì²´ CPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì¼ë¶€ CPUë§Œ ì‚¬ìš©í•´ ML ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ë™í•˜ëŠ” ê²½ìš°ì— ë³€ê²½í•œë‹¤. Booster parameter tree ìµœì í™”, Boosting, Regularization ë“±ê³¼ ê´€ë ¨ parameter ë“±ì„ ì§€ì¹­í•œë‹¤. eta [default=0.3, alias:learning_rate] : GBMì˜ í•™ìŠµë¥ (learning rate)ê³¼ ê°™ì€ parameterì´ë‹¤. 0~1 ì‚¬ì´ì˜ ê°’ì„ ì§€ì •í•˜ë©° Boosting stepì„ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•  ë•Œ ì—…ë°ì´íŠ¸ë˜ëŠ” í•™ìŠµë¥  ê°’. python ë˜í¼ ê¸°ë°˜ì˜ xgboostë¥¼ ì´ìš©í•  ê²½ìš° default=0.3 scikit-learn ë˜í¼ë¥¼ ì´ìš©í•  ê²½ìš° etaëŠ” learning_rateë¡œ ëŒ€ì²´ë˜ë©°, default=0.1ì´ë‹¤. ë³´í†µì€ 0.01~0.2 ì‚¬ì´ì˜ ê°’ì„ ì„ í˜¸í•œë‹¤. num_boost_rounds : GBMì˜ n_estimatorsì™€ ê°™ì€ parameterì´ë‹¤. min_child_weight[default=1] : GBMì˜ min_child_leafì™€ ìœ ì‚¬í•¨(ë˜‘ê°™ì§€ëŠ” ì•ŠìŒ). ê³¼ì í•©ì„ ì¡°ì ˆí•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤. gamma [default=0, alias: min_split_loss] : treeì˜ leaf ë…¸ë“œë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë‚˜ëˆŒì§€ë¥¼ ê²°ì •í•  ìµœì†Œ ì†ì‹¤ ê°ì†Œ ê°’ì´ë‹¤. í•´ë‹¹ ê°’ë³´ë‹¤ í° ì†ì‹¤(loss)ì´ ê°ì†Œëœ ê²½ìš°ì— leaf ë…¸ë“œë¥¼ ë¶„ë¦¬í•œë‹¤. ê°’ì´ í´ìˆ˜ë¡ ê³¼ì í•© ê°ì†Œ íš¨ê³¼ê°€ ìˆë‹¤. max_depth [default=6] : tree ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì˜ max_depthì™€ ê°™ë‹¤. 0ì„ ì§€ì •í•˜ë©´ ê¹Šì´ì— ì œí•œì´ ì—†ë‹¤. Max_depthê°€ ë†’ìœ¼ë©´ íŠ¹ì • feature ì¡°ê±´ì— íŠ¹í™”ë˜ì–´ ë£° ì¡°ê±´ì´ ë§Œë“¤ì–´ì§€ë¯€ë¡œ ê³¼ì í•© ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§€ë©° ë³´í†µì€ 3~10ì‚¬ì´ì˜ ê°’ì„ ì ìš©í•œë‹¤. sub_sample [default=1] : GBMì˜ subsampleê³¼ ë™ì¼í•˜ë‹¤. treeê°€ ì»¤ì ¸ì„œ ê³¼ì í•©ë˜ëŠ” ê²ƒì„ ì œì–´í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ìƒ˜í”Œë§í•˜ëŠ” ë¹„ìœ¨ì„ ì§€ì •í•œë‹¤. sub_sample=0.5ë¡œ ì§€ì •í•˜ë©´ ì „ì²´ ë°ì´í„°ì˜ ì ˆë°˜ì„ treeë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©í•œë‹¤. 0ì—ì„œ 1ì‚¬ì´ì˜ ê°’ì´ ê°€ëŠ¥í•˜ë‚˜ ì¼ë°˜ì ìœ¼ë¡œ 0.5~1ì‚¬ì´ì˜ ê°’ì„ ì‚¬ìš©í•œë‹¤. colsample_bytree [default=1] : GBMì˜ max_featuresì™€ ìœ ì‚¬í•˜ë‹¤. tree ìƒì„±ì— í•„ìš”í•œ feature(column)ë¥¼ ì„ì˜ë¡œ ìƒ˜í”Œë§ í•˜ëŠ” ë° ì‚¬ìš©ëœë‹¤. ë§¤ìš° ë§ì€ featureê°€ ìˆëŠ” ê²½ìš° ê³¼ì í•©ì„ ì¡°ì •í•˜ëŠ” ë° ì ìš©í•œë‹¤. lambda [default=1, alias:reg_lambda] : L2 Regularization ì ìš© ê°’ì´ë‹¤. feature ê°œìˆ˜ê°€ ë§ì„ ê²½ìš° ì ìš©ì„ ê²€í† í•˜ë©° ê°’ì´ í´ìˆ˜ë¡ ê³¼ì í•© ê°ì†Œ íš¨ê³¼ê°€ ìˆë‹¤. alpha : L1 Regularization ì ìš©ê°’ì´ë‹¤. feature ê°œìˆ˜ê°€ ë§ì„ ê²½ìš° ì ìš©ì„ ê²€í† í•˜ë©° ê°’ì´ í´ìˆ˜ë¡ ê°ì†Œ íš¨ê³¼ê°€ ìˆë‹¤. scale_pos_weight [default=1] : íŠ¹ì • ê°’ìœ¼ë¡œ ì¹˜ìš°ì¹œ ë¹„ëŒ€ì¹­í•œ í´ë˜ìŠ¤ë¡œ êµ¬ì„±ëœ ë°ì´í„° ì„¸íŠ¸ì˜ ê· í˜•ì„ ìœ ì§€í•˜ê¸° ìœ„í•œ paramterì´ë‹¤. í•™ìŠµ task parameter í•™ìŠµ ìˆ˜í–‰ ì‹œì˜ ê°ì²´ í•¨ìˆ˜, í‰ê°€ë¥¼ ìœ„í•œ ì§€í‘œ ë“±ì„ ì„¤ì •í•˜ëŠ” parameterì´ë‹¤. objective : ìµœì†Ÿê°’ì„ ê°€ì ¸ì•¼í•  ì†ì‹¤ í•¨ìˆ˜(loss function)ì„ ì •ì˜í•œë‹¤. XGBoostëŠ” ë§ì€ ìœ í˜•ì˜ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤í•¨ìˆ˜ëŠ” ì´ì§„ ë¶„ë¥˜ì¸ì§€ ë‹¤ì¤‘ ë¶„ë¥˜ì¸ì§€ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤. binary:logistic : ì´ì§„ ë¶„ë¥˜ì¼ ë•Œ ì ìš©í•œë‹¤. multi:softmax : ë‹¤ì¤‘ ë¶„ë¥˜ì¼ ë•Œ ì ìš©í•œë‹¤. ì†ì‹¤ í•¨ìˆ˜ê°€ multi:softmax ì¼ ê²½ìš°ì—ëŠ” label classì˜ ê°œìˆ˜ì¸ num_class parameterë¥¼ ì§€ì •í•´ì•¼ í•œë‹¤. multi:softprob : multi:softmaxì™€ ìœ ì‚¬í•˜ë‚˜ ê°œë³„ label classì˜ í•´ë‹¹ë˜ëŠ” ì˜ˆì¸¡ í™•ë¥ ì„ ë°˜í™˜í•œë‹¤. eval_metric : ê²€ì¦ì— ì‚¬ìš©ë˜ëŠ” í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤. defaultëŠ” íšŒê·€ì¸ ê²½ìš° rmse, ë¶„ë¥˜ì¸ ê²½ìš° errorì´ë‹¤. ë‹¤ìŒì€ eval_metricì˜ ê°’ ìœ í˜•ì´ë‹¤. rmse : Root Mean Square Error mae : Mean Absolute Error logloss : Negative log-likelihood error : Binary classification error rate (0.5 threshold) merror : Multiclass classification error rate mlogloss : Multiclass logloss auc : Area under the curve ëŒ€ë¶€ë¶„ì˜ hyper parameterëŠ” Booster paramterì— ì†í•œë‹¤. Scikit-Learn ë˜í¼ XGBoost ëª¨ë“ˆ XGboost íŒ¨í‚¤ì§€ì˜ Scikit-Learn ë˜í¼ í´ë˜ìŠ¤ëŠ” XGBClassifier, XGBRegressorì´ë‹¤. ì´ë¥¼ ì´ìš©í•˜ë©´ Scikit-Learn estimatorê°€ í•™ìŠµì„ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” fit(), predict() ì™€ ê°™ì€ í‘œì¤€ Scikit-Learn ê°œë°œ í”„ë¡œì„¸ìŠ¤ ë° ë‹¤ì–‘í•œ ìœ í‹¸ë¦¬í‹°ë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤. ê³¼ì í•©(overfitting) ë¬¸ì œê°€ ì‹¬ê°í•˜ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì ìš©í•  ê²ƒì„ ê³ ë ¤í•  ìˆ˜ ìˆë‹¤. eta ê°’ì„ ë‚®ì¶˜ë‹¤.(0.01~0.1) eta ê°’ì„ ë‚®ì¶œ ê²½ìš° num_round(ë˜ëŠ” n_estimators)ëŠ” ë°˜ëŒ€ë¡œ ë†’ì—¬ì¤˜ì•¼ í•œë‹¤. max_depth ê°’ì„ ë‚®ì¶˜ë‹¤. min_child_weight ê°’ì„ ë†’ì¸ë‹¤. gamma ê°’ì„ ë†’ì¸ë‹¤. ë˜í•œ subsampleê³¼ colsample_bytreeë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒë„ treeê°€ ë„ˆë¬´ ë³µì¡í•˜ê²Œ ìƒì„±ë˜ëŠ” ê²ƒì„ ë§‰ì•„ ê³¼ì í•© ë¬¸ì œì— ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤. XGBoost ìì²´ì ìœ¼ë¡œ êµì°¨ ê²€ì¦, ì„±ëŠ¥ í‰ê°€, feature ì¤‘ìš”ë„ ë“±ì˜ ì‹œê°í™” ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆë‹¤. ë˜í•œ XGBoostëŠ” ê¸°ë³¸ GBMì—ì„œ ë¶€ì¡±í•œ ë‹¤ë¥¸ ì—¬ëŸ¬ ê°€ì§€ ì„±ëŠ¥ í–¥ìƒ ê¸°ëŠ¥ì´ ìˆë‹¤. ê·¸ ì¤‘ì— ìˆ˜í–‰ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ëŒ€í‘œì ì¸ ê¸°ëŠ¥ìœ¼ë¡œ Early Stopping ê¸°ëŠ¥ì´ ìˆë‹¤. ê¸°ë³¸ GBMì˜ ê²½ìš° ì§€ì •ëœ íšŸìˆ˜ë¥¼ ë‹¤ ì™„ë£Œí•´ì•¼ í•œë‹¤. í—ˆë‚˜, XGBoostì™€ LightGBMì€ ëª¨ë‘ early Stopping ê¸°ëŠ¥ì´ ìˆì–´ì„œ n_estimatorsì— ì§€ì •í•œ Boosting ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í•˜ì§€ ì•Šë”ë¼ë„ ì˜ˆì¸¡ ì˜¤ë¥˜ê°€ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ë°˜ë³µì„ ëê¹Œì§€ ìˆ˜í–‰í•˜ì§€ ì•Šê³  ì¤‘ì§€í•´ ìˆ˜í–‰ ì‹œê°„ì„ ê°œì„  í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ n_estimators=200, early Stopping íŒŒë¼ë¯¸í„° ê°’ì„ 50ìœ¼ë¡œ ì„¤ì •í•˜ë©´, 1ë¶€í„° 200íšŒê¹Œì§€ Boostingì„ ë°˜ë³µí•˜ë‹¤ê°€ 50íšŒë¥¼ ë°˜ë³µí•˜ëŠ” ë™ì•ˆ í•™ìŠµ ì˜¤ë¥˜ê°€ ê°ì†Œí•˜ì§€ ì•Šìœ¼ë©´ ë” ì´ìƒ Boostingì„ ì§„í–‰í•˜ì§€ ì•Šê³  ì¢…ë£Œí•œë‹¤.(ê°€ë ¹ 100íšŒì—ì„œ í•™ìŠµ ì˜¤ë¥˜ ê°’ì´ 0.8ì¸ë°, 101íšŒ~150íšŒ ë°˜ë³µí•˜ëŠ” ë™ì•ˆ ì˜ˆì¸¡ ì˜¤ë¥˜ê°€ 0.8ë³´ë‹¤ ì‘ì€ ê°’ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ Boostingì„ ì¢…ë£Œí•œë‹¤.) ì•„ë˜ëŠ” python ë˜í¼ì˜ Xgboost ì‚¬ìš©ë²•ì„ ê°„ë‹¨íˆ ì •ë¦¬í•´ ë†“ì€ ê²ƒì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ XGBoostëŠ” GBMê³¼ëŠ” ë‹¤ë¥´ê²Œ ë³‘ë ¬ì²˜ë¦¬ì™€ early Stopping ë“±ìœ¼ë¡œ ë¹ ë¥¸ ìˆ˜í–‰ì‹œê°„ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ì§€ë§Œ, CPU ì½”ì–´ê°€ ë§ì§€ ì•Šì€ ê°œì¸ìš© PCì—ì„œëŠ” ìˆ˜í–‰ì‹œê°„ í–¥ìƒì„ ê²½í—˜í•˜ê¸° ì–´ë ¤ìš¸ ìˆ˜ë„ ìˆë‹¤. 123456789101112131415161718import xgboost as xgbfrom xgboost import plot_importanceimport pandas as pdimport numpy as npfrom sklearn.datasets import load_breast_cancerfrom sklearn.model_selection import train_test_splitimport warningswarnings.filterwarnings('ignore')dataset = load_breast_cancer()X_features = dataset.datay_label = dataset.targetcancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)cancer_df['target'] = y_labelprint(dataset.target_names)print(cancer_df['target'].value_counts()) ê²°ê³¼123['malignant', 'benign']1 3570 212 12X_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=156)print(X_train.shape, X_test.shape) ê²°ê³¼1(455, 30) (114, 30) python ë˜í¼ XGboostê°€ Scikit-Learn ë˜í¼ XGboostì™€ ì°¨ì´ì ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì§€ë§Œ, ê°€ì¥ í° ì°¨ì´ëŠ” í•™ìŠµìš© ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ì„¸íŠ¸ë¥¼ ìœ„í•´ ë³„ë„ì˜ ê°ì²´ì¸ DMatrixë¥¼ ìƒì„±í•œë‹¤ëŠ” ì ì´ë‹¤. DmatrixëŠ” ì£¼ë¡œ numpy ì…ë ¥ parameterë¥¼ ë°›ì•„ì„œ ë§Œë“¤ì–´ì§€ëŠ” XGBoostë§Œì˜ ì „ìš© ë°ì´í„° ì„¸íŠ¸ì´ì§€ë§Œ numpyì´ì™¸ì— libsvm txt í¬ë§· íŒŒì¼, xgboost ì´ì§„ ë²„í¼ íŒŒì¼ì„ parameterë¡œ ì…ë ¥ë°›ì•„ ë³€í™˜í•  ìˆ˜ ìˆë‹¤. dataëŠ” í”¼ì²˜ ë°ì´í„° ì„¸íŠ¸ì´ë©°, labelì€ classificationì˜ ê²½ìš°ì—ëŠ” label ë°ì´í„° ì„¸íŠ¸, regressionì˜ ê²½ìš°ì—ëŠ” ìˆ«ìí˜•ì¸ ì¢…ì†ê°’ ë°ì´í„° ì„¸íŠ¸ì´ë‹¤. 12dtrain = xgb.DMatrix(data=X_train, label=y_train)dtest = xgb.DMatrix(data=X_test, label=y_test) early_stopping_rounds íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•´ ì¡°ê¸° ì¤‘ë‹¨ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ eval_setê³¼ eval_metricì´ í•¨ê»˜ ì„¤ì •ë˜ì•¼ í•œë‹¤. XGboostëŠ” ë°˜ë³µë§ˆë‹¤ eval_setìœ¼ë¡œ ì§€ì •ëœ ë°ì´í„° ì„¸íŠ¸ì—ì„œ eval_metricì˜ ì§€ì •ëœ í‰ê°€ ì§€í‘œë¡œ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ì¸¡ì •í•œë‹¤. 12345678910111213params = &#123;'max_depth':3, 'eta':0.1, 'objective':'binary:logistic', 'eval_metric':'logloss', 'early_stoppings':100 &#125;num_rounds = 400# train ë°ì´í„° ì„¸íŠ¸ëŠ” 'train', evaluation(test) ë°ì´í„° ì„¸íŠ¸ëŠ” 'eval'ë¡œ ëª…ì‹œí•œë‹¤.wlist = [(dtrain, 'train'), (dtest, 'eval')]# í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì™€ early stopping íŒŒë¼ë¯¸í„°ë¥¼ train() í•¨ìˆ˜ì˜ íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬xgb_model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_rounds, evals=wlist) python ë˜í¼ xgboostëŠ” predict() ë©”ì„œë“œê°€ ì˜ˆì¸¡ ê²°ê³¼ê°’ì´ ì•„ë‹Œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆëŠ” í™•ë¥  ê°’ì„ ë°˜í™˜í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì˜ˆì¸¡ í™•ë¥ ì´ 0.5ë³´ë‹¤ í¬ë©´ 1, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€ 123456pred_probs = xgb_model.predict(dtest)print('predict() ìˆ˜í–‰ ê²°ê³¼ê°’ì„ 10ê°œë§Œ í‘œì‹œ, ì˜ˆì¸¡ í™”ê·¤ê°’ìœ¼ë¡œ í‘œì‹œëœ')print(np.round(pred_probs[:10], 3))preds = [1 if prob &gt; 0.5 else 0 for prob in pred_probs]print('ì˜ˆì¸¡ê°’ 10ê°œë§Œ í‘œì‹œ:', preds[:10]) ê²°ê³¼123predict() ìˆ˜í–‰ ê²°ê³¼ê°’ì„ 10ê°œë§Œ í‘œì‹œ, ì˜ˆì¸¡ í™”ê·¤ê°’ìœ¼ë¡œ í‘œì‹œëœ[0.95 0.003 0.9 0.086 0.993 1. 1. 0.999 0.998 0. ]ì˜ˆì¸¡ê°’ 10ê°œë§Œ í‘œì‹œ: [1, 0, 1, 0, 1, 1, 1, 1, 1, 0] xgboost íŒ¨í‚¤ì§€ì— ë‚´ì¥ëœ ì‹œê°í™” ê¸°ëŠ¥ ì¤‘ plot_importance() APIëŠ” featureì˜ ì¤‘ìš”ë„ë¥¼ ë§‰ëŒ€ê·¸ë˜í”„ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ë‹¤. ê¸°ë³¸ í‰ê°€ ì§€í‘œë¡œ f1-scoreë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ ê° featureì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. Scikit-Learnì€ Estimator ê°ì²´ì˜ feature_importances_ ì†ì„±ì„ ì´ìš©í•´ ì§ì ‘ ì‹œê°í™” ì½”ë“œë¥¼ ì‘ì„±í•´ì•¼ í•˜ì§€ë§Œ, xgboost íŒ¨í‚¤ì§€ëŠ” plot_importance()ë¥¼ ì´ìš©í•´ ë°”ë¡œ í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤. plot_importance() í˜¸ì¶œ ì‹œ íŒŒë¼ë¯¸í„°ë¡œ ì•ì—ì„œ í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ ê°ì²´ ë° Matplotlibì˜ ax ê°ì²´ë¥¼ ì…ë ¥í•˜ê¸°ë§Œ í•˜ë©´ ëœë‹¤. ë‚´ì¥ëœ plot_importance() ì´ìš© ì‹œ ìœ ì˜í•  ì ì€ xgboost numpy ê¸°ë°˜ì˜ feature ë°ì´í„°í„°ë¡œ í•™ìŠµì‹œì— í”¼ì²˜ëª…ì„ ì œëŒ€ë¡œ ì•Œ ìˆ˜ ê°€ ì—†ìœ¼ë¯€ë¡œ f0, f1ì™€ ê°™ì´ feature ìˆœì„œë³„ë¡œ fì ë’¤ì— ìˆœì„œë¥¼ ë¶™ì—¬ì„œ X ì¶•ì— featureë“¤ë¡œ ë‚˜ì—´í•œë‹¤. 123456from xgboost import plot_importanceimport matplotlib.pyplot as plt%matplotlib inlinefig, ax = plt.subplots(figsize=(10, 12))plot_importance(xgb_model, ax=ax) ë˜í•œ, Decision Treeì—ì„œ ë³´ì—¬ì¤€ tree ê¸°ë°˜ ê·œì¹™ êµ¬ì¡°ë„ xgboostì—ì„œ ì‹œê°í™”í•  ìˆ˜ ìˆë‹¤. xgboost ëª¨ë“ˆì˜ to_graphviz() APIë¥¼ ì´ìš©í•˜ë©´ jupyter notebookì— ë°”ë¡œ ê·œì¹™ tree êµ¬ì¡°ë¥¼ ê·¸ë¦´ ìˆ˜ ìˆë‹¤. xgboost.cv() APIë¥¼ í†µí•´ GridSearchCVì™€ ìœ ì‚¬í•œ ê¸°ì¦ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ëŠ” Scikit-Learn ë˜í¼ì˜ xgboostì˜ ì‚¬ìš©ë²•ì„ ì •ë¦¬í•´ ë†“ì€ ê²ƒì´ë‹¤. ì•ì˜ python ë˜í¼ì™€ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. 1234567from xgboost import XGBClassifierfrom sklearn.metrics import confusion_matrixxgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3, random_state=156)xgb_wrapper.fit(X_train, y_train)w_preds = xgb_wrapper.predict(X_test)print(confusion_matrix(y_test, w_preds)) ê²°ê³¼12[[35 2] [ 1 76]] early stopping ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ì•„ë˜ì™€ ê°™ë‹¤. ì„±ëŠ¥ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ë°ì´í„° ì„¸íŠ¸ëŠ” í•™ìŠµ ë°ì´í„°ê°€ ì•„ë‹ˆë¼ ë³„ë„ì˜ ë°ì´í„° ì„¸íŠ¸ì´ì–´ì•¼ í•œë‹¤. í—ˆë‚˜, ì•„ë˜ ë°ì´í„° ìì²´ì˜ í¬ê¸°ê°€ ì‘ê¸° ë•Œë¬¸ì— í‰ê°€ìš©ìœ¼ë¡œ ì‚¬ìš©í•´ ë³´ì•˜ë‹¤. í—ˆë‚˜, ì ˆëŒ€ ì•„ë˜ì™€ ê°™ì´ evalsì— test ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë©´ ì•ˆëœë‹¤. ë§Œì¼ test dataë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´ predictí•˜ëŠ” ê²½ìš°ì—ëŠ” í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•Šì€ ë˜ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ì•¼í•œë‹¤. ë˜í•œ, early stoppingì„ ë„ˆë¬´ ì ê²Œ ì¡ëŠ”ë‹¤ë©´ ì „ì—­ ìµœì í™”ê°€ ì´ë£¨ì–´ì§€ì§€ ì•Šì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜í•˜ì 12345xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3, random_state=156)evals = [(X_test, y_test)]xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric='logloss', eval_set=evals, verbose=True)w_preds = xgb_wrapper.predict(X_test)print(confusion_matrix(y_test, w_preds)) ê²°ê³¼12[[34 3] [ 1 76]] LightGBM LightGBMì€ XGBoostì™€ í•¨ê»˜ ë¶€ìŠ¤íŒ… ê³„ì—´ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ê°€ì¥ ê°ê´‘ ë°›ê³  ìˆë‹¤. XGBoostëŠ” ë§¤ìš° ë›°ì–´ë‚œ ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ì´ì§€ë§Œ, XGBoostì—ì„œ GridSearchCVë¡œ hyper parameter íŠœë‹ì„ ìˆ˜í–‰í•˜ë‹¤ ë³´ë©´ ì—¬ì „íˆ í•™ìŠµì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦°ë‹¤. ë¬¼ë¡  GBM ë³´ë‹¤ëŠ” ë¹ ë¥´ì§€ë§Œ, ëŒ€ìš©ëŸ‰ ë°ì´í„°ì˜ ê²½ìš° ë§Œì¡±í•  ë§Œí•œ í•™ìŠµ ì„±ëŠ¥ì„ ê¸°ëŒ€í•˜ë ¤ë©´ ë§ì€ CPU Coreë¥¼ ê°€ì§„ ì‹œìŠ¤í…œì—ì„œ ë†’ì€ ë³‘ë ¬ë„ë¡œ í•™ìŠµì„ ì§„í–‰í•´ì•¼ í•œë‹¤. LightGBMì˜ ê°€ì¥ í° ì¥ì ì€ XGBoostë³´ë‹¤ í•™ìŠµì— ê±¸ë¦¬ëŠ” ì‹œê°„ì´ í›¨ì”¬ ì ë‹¤ëŠ” ì ì´ë‹¤. ë˜í•œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ë„ ìƒëŒ€ì ìœ¼ë¡œ ì ë‹¤. LightGBMì´ XGBoostë³´ë‹¤ 2ë…„ í›„ì— ë§Œë“¤ì–´ì§€ë‹¤ë³´ë‹ˆ XGBoostì˜ ì¥ì ì€ ê³„ìŠ¹í•˜ê³  ë‹¨ì ì€ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°œë°œë˜ì—ˆê¸° ë•Œë¬¸ì— ì˜ˆì¸¡ ì„±ëŠ¥ì—ì„œì˜ ì°¨ì´ëŠ” ê±°ì˜ ì—†ì§€ë§Œ, ê¸°ëŠ¥ìƒì˜ ë‹¤ì–‘ì„±ì´ ë” ë†’ë‹¤. LightGBMì˜ í•œ ê°€ì§€ ë‹¨ì ìœ¼ë¡œ ì•Œë ¤ì§„ ê²ƒì€ ì ì€ ë°ì´í„° ì„¸íŠ¸ì— ì ìš©í•  ê²½ìš° ê³¼ì í•©ì´ ë°œìƒí•˜ê¸° ì‰½ë‹¤ëŠ” ê²ƒì´ë‹¤. ì ì€ ë°ì´í„° ì„¸íŠ¸ì˜ ê¸°ì¤€ì€ ì• ë§¤í•˜ì§€ë§Œ, ì¼ë°˜ì ìœ¼ë¡œ 10,000ê±´ ì´í•˜ì˜ ë°ì´í„° ì„¸íŠ¸ ì •ë„ë¼ê³  LightGBM ê³µì‹ ë¬¸ì„œì—ì„œ ê¸°ìˆ í•˜ê³  ìˆë‹¤. LightGBMì€ ì¼ë°˜ GBM ê³„ì—´ì˜ íŠ¸ë¦¬ ë¶„í•  ë°©ë²•ê³¼ ë‹¤ë¥´ê²Œ leaf ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í• (Leaf Wise) ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤. ê¸°ì¡´ì˜ ëŒ€ë¶€ë¶„ íŠ¸ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì€ íŠ¸ë¦¬ì˜ ê¹Šì´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì´ê¸° ìœ„í•œ ê· í˜• íŠ¸ë¦¬ ë¶„í• (Level wise)ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤. ì¦‰, ìµœëŒ€í•œ ê· í˜• ì¡íŒ íŠ¸ë¦¬ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë¶„í• í•˜ê¸° ë•Œë¬¸ì— íŠ¸ë¦¬ì˜ ê¹Šì´ê°€ ìµœì†Œí™”ë  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ê· í˜•ì¡íŒ íŠ¸ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ì´ìœ ëŠ” ê³¼ì í•©(overfitting)ì— ë³´ë‹¤ ë” ê°•í•œ êµ¬ì¡°ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤ê³  ì•Œë ¤ì ¸ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë°˜ëŒ€ë¡œ ê· í˜•ì„ ë§ì¶”ê¸° ìœ„í•œ ì‹œê°„ì´ í•„ìš”í•˜ë‹¤ëŠ” ìƒëŒ€ì ì¸ ë‹¨ì ì´ ìˆë‹¤. í•˜ì§€ë§Œ, LightGBMì˜ leaf ì¤‘ì‹¬ íŠ¸ë¦¬ ë¶„í•  ë°©ì‹ì€ íŠ¸ë¦¬ì˜ ê· í˜•ì„ ë§ì¶”ì§€ ì•Šê³ , ìµœëŒ€ ì†ì‹¤ ê°’(max delta loss)ì„ ê°€ì§€ëŠ” leaf ë…¸ë“œë¥¼ ì§€ì†ì ìœ¼ë¡œ ë¶„í• í•˜ë©´ì„œ íŠ¸ë¦¬ì˜ ê¹Šì´ê°€ ê¹Šì–´ì§€ê³  ë¹„ëŒ€ì¹­ì ì¸ ê·œì¹™ íŠ¸ë¦¬ê°€ ìƒì„±ëœë‹¤. í•˜ì§€ë§Œ ì´ë ‡ê²Œ ìµœëŒ€ ì†ì‹¤ê°’ì„ ê°€ì§€ëŠ” leaf ë…¸ë“œë¥¼ ì§€ì†ì ìœ¼ë¡œ ë¶„í• í•´ ìƒì„±ëœ ê·œì¹™ íŠ¸ë¦¬ëŠ” í•™ìŠµì„ ë°˜ë³µí• ìˆ˜ë¡ ê²°êµ­ì€ ê· í˜• íŠ¸ë¦¬ ë¶„í•  ë°©ì‹ë³´ë‹¤ ì˜ˆì¸¡ ì˜¤ë¥˜ ì†ì‹¤ì„ ìµœì†Œí™” í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ LightGBMì˜ êµ¬í˜„ ì‚¬ìƒì´ë‹¤. LightGBM ì„¤ì¹˜ ë°©ë²• Windowì— ì„¤ì¹˜í•  ê²½ìš°ì—ëŠ” Visual Studio Build tool 2015 ì´ìƒì´ ì„¤ì¹˜ë¼ìˆì–´ì•¼ í•œë‹¤. ì•„ë‚˜ì½˜ë‹¤ í”„ë¡¬í”„íŠ¸ë¥¼ ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰í•œ ë‹¤ìŒ ì•„ë˜ ëª…ë ¤ì–´ ì‹¤í–‰ 1conda install -c conda-forge lightgbm LightGBM í•˜ì´í¼ íŒŒë¼ë¯¸í„°ëŠ” XGBoostì™€ ë§ì€ ë¶€ë¶„ì´ ìœ ì‚¬í•˜ì§€ë§Œ íŠ¸ë¦¬ì˜ ë¶„í•  ë°©ì‹ì´ ë‹¤ë¥´ë¯€ë¡œ ì˜ˆë¥¼ ë“¤ì–´ max_depthë¥¼ ë§¤ìš° í¬ê²Œ ê°€ì ¸ê°€ëŠ” ê²ƒê³¼ ê°™ì´ íŠ¸ë¦¬ íŠ¹ì„±ì— ë§ê²Œ ì„¤ì •í•´ ì£¼ì–´ì•¼ í•  ê²ƒì´ë‹¤. ì£¼ìš” íŒŒë¼ë¯¸í„° num_iterations [default = 100] : ë°˜ë³µ ìˆ˜í–‰í•˜ë ¤ëŠ” íŠ¸ë¦¬ì˜ ê°œìˆ˜ë¥¼ ì§€ì •í•œë‹¤. í¬ê²Œ ì§€ì •í• ìˆ˜ë¡ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë†’ì•„ì§ˆìˆ˜ ìˆìœ¼ë‚˜, ë„ˆë¬´ í¬ê²Œ ì§€ì •í•˜ë©´ ì˜¤íˆë ¤ ê³¼ì í•©ìœ¼ë¡œ ì„±ëŠ¥ì´ ì €í•˜ ë  ìˆ˜ ìˆë‹¤. Scikit-Learn GBMê³¼ XGBoostì˜ Scikit-Learn í˜¸í™˜ í´ë˜ìŠ¤ì˜ n_estimatorsì™€ ê°™ì€ íŒŒë¼ë¯¸í„°ì´ë¯€ë¡œ LightGBMì˜ Scikit-Learn í˜¸í™˜ í´ë˜ìŠ¤ì—ì„œëŠ” n_estimatorsë¡œ ì´ë¦„ì´ ë³€ê²½ëœë‹¤. learning_rate [default = 0.1] : 0ì—ì„œ 1ì‚¬ì´ì˜ ê°’ì„ ì§€ì •í•˜ë©° Boosting ìŠ¤í…ì„ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•  ë•Œ ì—…ë°ì´íŠ¸ë˜ëŠ” í•™ìŠµë¡¤ê°’ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ n_estimatorsë¥¼ í¬ê²Œí•˜ê³  learning_rateë¥¼ ì‘ê²Œí•´ì„œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìœ¼ë‚˜, ë§ˆì°¬ê°€ì§€ë¡œ ê³¼ì í•©(overfitting) ì´ìŠˆì™€ í•™ìŠµ ì‹œê°„ì´ ê¸¸ì–´ì§€ëŠ” ë¶€ì •ì ì¸ ì˜í–¥ë„ ê³ ë ¤í•´ì•¼í•œë‹¤. GBM, XGBoostì˜ learning_rateì™€ ê°™ì€ íŒŒë¼ë¯¸í„°ì´ë‹¤. max_depth [default=1] : íŠ¸ë¦¬ ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì˜ max_depthì™€ ê°™ë‹¤. 0ë³´ë‹¤ ì‘ì€ ê°’ì„ ì§€ì •í•˜ë©´ ê¹Šì´ì— ì œí•œì´ ì—†ë‹¤. ì§€ê¸ˆê¹Œì§€ ì†Œê°œí•œ Depth Wise ë°©ì‹ì˜ íŠ¸ë¦¬ì™€ ë‹¤ë¥´ê²Œ LightGBMì€ Leaf wise ê¸°ë°˜ì´ë¯€ë¡œ ê¹Šì´ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë” ê¹Šë‹¤. min_data_in_leaf [default=20] : Decision Treeì˜ min_samples_leafì™€ ê°™ì€ íŒŒë¼ë¯¸í„°ì´ë‹¤. í•˜ì§€ë§Œ Scikit-Learn ë˜í¼ LightGBM í´ë˜ìŠ¤ì¸ LightGBMClassifierì—ì„œëŠ” min_child_samples íŒŒë¼ë¯¸í„°ë¡œ ì´ë¦„ì´ ë³€ê²½ëœë‹¤. ìµœì¢… ê²°ì • í´ë˜ìŠ¤ì¸ Leaf ë…¸ë“œê°€ ë˜ê¸° ìœ„í•´ì„œ ìµœì†Œí•œìœ¼ë¡œ í•„ìš”í•œ ë ˆì½”ë“œ(ë°ì´í„°) ìˆ˜ì´ë©°, ê³¼ì í•©ì„ ì œì–´í•˜ê¸° ìœ„í•œ íŒŒë¼ë¯¸í„°ì´ë‹¤. num_leaves [default=31] : í•˜ë‚˜ì˜ íŠ¸ë¦¬ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìµœëŒ€ Leaf ê°œìˆ˜ì´ë‹¤. boosting [default=gbdt] : Boosting íŠ¸ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ìˆ í•œë‹¤. gbdt : ì¼ë°˜ì ì¸ ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ê²°ì •íŠ¸ë¦¬ rf : ëœë¤í¬ë ˆìŠ¤íŠ¸ bagging_fraction [default=1.0] : íŠ¸ë¦¬ê°€ ì»¤ì ¸ì„œ ê³¼ì í•©ë˜ëŠ” ê²ƒì„ ì œì–´í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„° ìƒ˜í”Œë§í•˜ëŠ” ë¹„ìœ¨ì„ ì§€ì •í•œë‹¤. Scikit-Learnì˜ GBMê³¼ XGBoostì˜ sub_sample íŒŒë¼ë¯¸í„°ì™€ ë™ì¼í•˜ê¸°ì— Scikit-Learn ë˜í¼ LightGBMì¸ LightGBMClassifierì—ì„œëŠ” sub_sampleë¡œ ë™ì¼í•˜ê²Œ íŒŒë¼ë¯¸í„° ì´ë¦„ì´ ë³€ê²½ëœë‹¤. feature_fraction [default=1.0] : ê°œë³„ íŠ¸ë¦¬ë¥¼ í•™ìŠµí•  ë•Œë§ˆë‹¤ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ëŠ” featureì˜ ë¹„ìœ¨ì´ë‹¤. ê³¼ì í•©ì„ ë§‰ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤. GBMì˜ max_featuresì™€ ìœ ì‚¬í•˜ë©°, XGBClassifierì˜ colsample_bytreeì™€ ë˜‘ê°™ìœ¼ë¯€ë¡œ LightGBMClassifierì—ì„œëŠ” ë™ì¼í•˜ê²Œ colsample_bytreeë¡œ ë³€ê²½ëœë‹¤. lambda_l2 [default=0.0] : L2 Regulation ì œì–´ë¥¼ ìœ„í•œ ê°’ì´ë‹¤. feature ê°œìˆ˜ê°€ ë§ì„ ê²½ìš° ì ìš©ì„ ê²€í† í•˜ë©° ê°’ì´ í´ìˆ˜ë¡ ê³¼ì í•© ê°ì†Œ íš¨ê³¼ê°€ ìˆë‹¤. XGBClassifierì˜ reg_lambdaì™€ ë™ì¼í•˜ë¯€ë¡œ LightGBMClassifierì—ì„œëŠ” reg_lambdaë¡œ ë³€ê²½ëœë‹¤. lambda_l1 [default=0.0] : L1 Regulation ì œì–´ë¥¼ ìœ„í•œ ê°‘ì´ë‹¤. L2ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ê³¼ì í•© ì œì–´ë¥¼ ìœ„í•œ ê²ƒì´ë©°, XGBClassifierì˜ reg_alphaì™€ ë™ì¼í•˜ë¯€ë¡œ LightGBMClassifierì—ì„œëŠ” reg_alphaë¡œ ë³€ê²½ëœë‹¤. Learning Task íŒŒë¼ë¯¸í„° objective : ìµœì†Ÿê°’ì„ ê°€ì ¸ì•¼ í•  ì†ì‹¤í•¨ìˆ˜(loss function)ì„ ì •ì˜í•œë‹¤. XGBoostì˜ objective íŒŒë¼ë¯¸í„°ì™€ ë™ì¼í•˜ë‹¤. ì• í”Œë¦¬ì¼€ì´ì…˜ ìœ í˜•, ì¦‰ regression, multiclass classification, binary classificationdlì¸ì§€ì— ë”°ë¼ objectiveì¸ ì†ì‹¤í•¨ìˆ˜ê°€ ì§€ì •ëœë‹¤. í•˜ì´í¼ íŒŒë¼ë¯¸í„° íŠœë‹ ë°©ì•ˆ num_leavesì˜ ê°œìˆ˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ min_child_samples(min_data_in_leaf), max_depthë¥¼ í•¨ê»˜ ì¡°ì •í•˜ë©´ì„œ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ ê¸°ë³¸ íŠœë‹ ë°©ì•ˆì´ë‹¤. num_leavesëŠ” ê°œë³„ íŠ¸ë¦¬ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìµœëŒ€ Leafì˜ ê°œìˆ˜ì´ê³  LightGBM ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì œì–´í•˜ëŠ” ì£¼ìš” íŒŒë¼ë¯¸í„°ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ num_leavesì˜ ê°œìˆ˜ë¥¼ ë†’ì´ë©´ ì •í™•ë„ê°€ ë†’ì•„ì§€ì§€ë§Œ, ë°˜ëŒ€ë¡œ íŠ¸ë¦¬ì˜ ê¹Šì´ê°€ ê¹Šì–´ì§€ê³  ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ì»¤ì ¸ì„œ ê³¼ì í•© ì˜í–¥ë„ê°€ ì»¤ì§„ë‹¤. min_data_in_leafëŠ” Scikit-Learn ë˜í¼ í´ë˜ìŠ¤ì—ì„œëŠ” min_child_samplesë¡œ ì´ë¦„ì´ ë°”ë€ë‹¤. ê³¼ì í•©ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ì´ë‹¤. num_leavesì™€ í•™ìŠµ ë°ì´í„°ì˜ í¬ê¸°ì— ë”°ë¼ ë‹¬ë¼ì§€ì§€ë§Œ, ë³´í†µ í° ê°’ìœ¼ë¡œ ì„¤ì •í•˜ë©´ íŠ¸ë¦¬ê°€ ê¹Šì–´ì§€ëŠ” ê²ƒì„ ë°©ì§€í•œë‹¤. max_depthëŠ” ëª…ì‹œì ìœ¼ë¡œ ê¹Šì´ì˜ í¬ê¸°ë¥¼ ì œí•œí•œë‹¤. num_leaves, min_data_in_leafì™€ ê²°í•©í•´ ê³¼ì í•©ì„ ê°œì„ í•˜ëŠ”ë° ì‚¬ìš©í•œë‹¤. 12 ê²°ê³¼12 12 ê²°ê³¼12 12 ê²°ê³¼12 12 ê²°ê³¼12","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"ë‚´ê°€ ì •ë¦¬í•˜ëŠ” ìë£Œêµ¬ì¡° 05 - íŠ¸ë¦¬(Tree)","slug":"data_structure_06","date":"2020-05-02T10:27:21.000Z","updated":"2020-05-18T08:18:36.267Z","comments":true,"path":"2020/05/02/data_structure_06/","link":"","permalink":"https://heung-bae-lee.github.io/2020/05/02/data_structure_06/","excerpt":"","text":"ëŒ€í‘œì ì¸ ë°ì´í„° êµ¬ì¡°7: íŠ¸ë¦¬1. íŠ¸ë¦¬ (Tree) êµ¬ì¡° íŠ¸ë¦¬: Nodeì™€ Branchë¥¼ ì´ìš©í•´ì„œ, ì‚¬ì´í´ì„ ì´ë£¨ì§€ ì•Šë„ë¡ êµ¬ì„±í•œ ë°ì´í„° êµ¬ì¡° ì‹¤ì œë¡œ ì–´ë””ì— ë§ì´ ì‚¬ìš©ë˜ë‚˜? íŠ¸ë¦¬ ì¤‘ ì´ì§„ íŠ¸ë¦¬ (Binary Tree) í˜•íƒœì˜ êµ¬ì¡°ë¡œ, íƒìƒ‰(ê²€ìƒ‰) ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ì„ ìœ„í•´ ë§ì´ ì‚¬ìš©ë¨ 2. ì•Œì•„ë‘˜ ìš©ì–´ Node: íŠ¸ë¦¬ì—ì„œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ê¸°ë³¸ ìš”ì†Œ (ë°ì´í„°ì™€ ë‹¤ë¥¸ ì—°ê²°ëœ ë…¸ë“œì— ëŒ€í•œ Branch ì •ë³´ í¬í•¨) Root Node: íŠ¸ë¦¬ ë§¨ ìœ„ì— ìˆëŠ” ë…¸ë“œ Level: ìµœìƒìœ„ ë…¸ë“œë¥¼ Level 0ìœ¼ë¡œ í•˜ì˜€ì„ ë•Œ, í•˜ìœ„ Branchë¡œ ì—°ê²°ëœ ë…¸ë“œì˜ ê¹Šì´ë¥¼ ë‚˜íƒ€ëƒ„ Parent Node: ì–´ë–¤ ë…¸ë“œì˜ ë‹¤ìŒ ë ˆë²¨ì— ì—°ê²°ëœ ë…¸ë“œ Child Node: ì–´ë–¤ ë…¸ë“œì˜ ìƒìœ„ ë ˆë²¨ì— ì—°ê²°ëœ ë…¸ë“œ Leaf Node (Terminal Node): Child Nodeê°€ í•˜ë‚˜ë„ ì—†ëŠ” ë…¸ë“œ Sibling (Brother Node): ë™ì¼í•œ Parent Nodeë¥¼ ê°€ì§„ ë…¸ë“œ Depth: íŠ¸ë¦¬ì—ì„œ Nodeê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìµœëŒ€ Level 4. ìë£Œ êµ¬ì¡° ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ì˜ ì¥ì ê³¼ ì£¼ìš” ìš©ë„ ì£¼ìš” ìš©ë„: ë°ì´í„° ê²€ìƒ‰(íƒìƒ‰) ì¥ì : íƒìƒ‰ ì†ë„ë¥¼ ê°œì„ í•  ìˆ˜ ìˆìŒ ë‹¨ì ì€ ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ ì´í•´ í›„ì— ì‚´í´ë³´ê¸°ë¡œ í•¨ ì•„ë˜ ì´ë¯¸ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ ì•Œê³ ë¦¬ì¦˜ì€ 3 stepì•ˆì— 27ì´ë¼ëŠ” ìˆ«ìë¥¼ ì°¾ì•„ë‚´ì§€ë§Œ, ë°°ì—´(array)ì—ì„œëŠ” í•˜ë‚˜ì”© ë¹„êµí•˜ë©´ ì°¾ê¸° ë•Œë¬¸ì— í›¨ì”¬ ë§ì€ stepì„ ê±°ì³ì•¼ í•œë‹¤. ì´ì™€ ê°™ì´ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ë°ì´í„°ë¥¼ ì €ì¥í•´ ë†“ì„ ë•Œ ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ë¥¼ ë§ì´ ì‚¬ìš©í•œë‹¤. ì´ì§„íŠ¸ë¦¬ì™€ ì •ë ¬ëœ ë°°ì—´ê°„ì˜ íƒìƒ‰ ë¹„êµ (ì¶œì²˜: https://www.mathwarehouse.com/programming/gifs/binary-search-tree.php#binary-search-tree-insertion-node) 5. íŒŒì´ì¬ ê°ì²´ì§€í–¥ í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ êµ¬í˜„í•˜ê¸°5.1. ë…¸ë“œ í´ë˜ìŠ¤ ë§Œë“¤ê¸° 12345class Node: def __init__(self, value): self.value = value self.left = None self.right = None 5.2. ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ì— ë°ì´í„° ë„£ê¸° ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ ì¡°ê±´ì— ë¶€í•©í•˜ê²Œ ë°ì´í„°ë¥¼ ë„£ì–´ì•¼ í•¨ ê°’ì´ ê°™ê±°ë‚˜ í¬ë©´ ì˜¤ë¥¸ìª½ì— ìœ„ì¹˜í•˜ë„ë¡í•˜ê³ , ì‘ìœ¼ë©´ ì™¼ìª½ì— ìœ„ì¹˜í•˜ë„ë¡ ì½”ë“œë¥¼ ì‘ì„±í•¨. 123456789101112131415161718192021222324252627282930class NodeMgmt: def __init__(self, head): self.head = head def insert(self, value): self.current_node = self.head while True: if value &lt; self.current_node.value: if self.current_node.left != None: self.current_node = self.current_node.left else: self.current_node.left = Node(value) break else: if self.current_node.right != None: self.current_node = self.current_node.right else: self.current_node.right = Node(value) break def search(self, target): self.current_node = self.head while self.current_node: if self.currnet_node.value == target: return self.current_node elif self.current_node.value &gt; target: self.current_node=self.current_node.left else: self.current_node=self.current_node.right return False 12345678head = Node(1)BST = NodeMgmt(head)BST.insert(2)BST.insert(3)BST.insert(0)BST.insert(4)BST.insert(8)BST.search(-1) ê²°ê³¼ 1False 5.4. ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ ì‚­ì œ ë§¤ìš° ë³µì¡í•¨. ê²½ìš°ë¥¼ ë‚˜ëˆ„ì–´ì„œ ì´í•´í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ 5.4.1. Leaf Node ì‚­ì œ Leaf Node: Child Node ê°€ ì—†ëŠ” Node ì‚­ì œí•  Nodeì˜ Parent Nodeê°€ ì‚­ì œí•  Nodeë¥¼ ê°€ë¦¬í‚¤ì§€ ì•Šë„ë¡ í•œë‹¤. 5.4.2. Child Node ê°€ í•˜ë‚˜ì¸ Node ì‚­ì œ ì‚­ì œí•  Nodeì˜ Parent Nodeê°€ ì‚­ì œí•  Nodeì˜ Child Nodeë¥¼ ê°€ë¦¬í‚¤ë„ë¡ í•œë‹¤. 5.4.3. Child Node ê°€ ë‘ ê°œì¸ Node ì‚­ì œ ì‚­ì œí•  Nodeì˜ ì˜¤ë¥¸ìª½ ìì‹ ì¤‘, ê°€ì¥ ì‘ì€ ê°’ì„ ì‚­ì œí•  Nodeì˜ Parent Nodeê°€ ê°€ë¦¬í‚¤ë„ë¡ í•œë‹¤. ì‚­ì œí•  Nodeì˜ ì™¼ìª½ ìì‹ ì¤‘, ê°€ì¥ í° ê°’ì„ ì‚­ì œí•  Nodeì˜ Parent Nodeê°€ ê°€ë¦¬í‚¤ë„ë¡ í•œë‹¤. 5.4.3.1. ì‚­ì œí•  Nodeì˜ ì˜¤ë¥¸ìª½ ìì‹ì¤‘, ê°€ì¥ ì‘ì€ ê°’ì„ ì‚­ì œí•  Nodeì˜ Parent Nodeê°€ ê°€ë¦¬í‚¤ê²Œ í•  ê²½ìš° ì‚­ì œí•  Nodeì˜ ì˜¤ë¥¸ìª½ ìì‹ ì„ íƒ ì˜¤ë¥¸ìª½ ìì‹ì˜ ê°€ì¥ ì™¼ìª½ì— ìˆëŠ” Nodeë¥¼ ì„ íƒ í•´ë‹¹ Nodeë¥¼ ì‚­ì œí•  Nodeì˜ Parent Nodeì˜ ì™¼ìª½ Branchê°€ ê°€ë¦¬í‚¤ê²Œ í•¨ í•´ë‹¹ Nodeì˜ ì™¼ìª½ Branchê°€ ì‚­ì œí•  Nodeì˜ ì™¼ìª½ Child Nodeë¥¼ ê°€ë¦¬í‚¤ê²Œ í•¨ í•´ë‹¹ Nodeì˜ ì˜¤ë¥¸ìª½ Branchê°€ ì‚­ì œí•  Nodeì˜ ì˜¤ë¥¸ìª½ Child Nodeë¥¼ ê°€ë¦¬í‚¤ê²Œ í•¨ ë§Œì•½ í•´ë‹¹ Nodeê°€ ì˜¤ë¥¸ìª½ Child Nodeë¥¼ ê°€ì§€ê³  ìˆì—ˆì„ ê²½ìš°ì—ëŠ”, í•´ë‹¹ Nodeì˜ ë³¸ë˜ Parent Nodeì˜ ì™¼ìª½ Branchê°€ í•´ë‹¹ ì˜¤ë¥¸ìª½ Child Nodeë¥¼ ê°€ë¦¬í‚¤ê²Œ í•¨ 5.5. ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ ì‚­ì œ ì½”ë“œ êµ¬í˜„ê³¼ ë¶„ì„5.5.1 ì‚­ì œí•  Node íƒìƒ‰ ì‚­ì œí•  Nodeê°€ ì—†ëŠ” ê²½ìš°ë„ ì²˜ë¦¬í•´ì•¼ í•¨ ì´ë¥¼ ìœ„í•´ ì‚­ì œí•  Nodeê°€ ì—†ëŠ” ê²½ìš°ëŠ” Falseë¥¼ ë¦¬í„´í•˜ê³ , í•¨ìˆ˜ë¥¼ ì¢…ë£Œ ì‹œí‚´ 12345678910111213141516171819def delete(self, value): searched = False self.current_node = self.head self.parent = self.head while self.current_node: if self.current_node.value == value: searched = True break elif value &lt; self.current_node.value: self.parent = self.current_node self.current_node = self.current_node.left else: self.parent = self.current_node self.current_node = self.current_node.right if searched == False: return False### ì´í›„ë¶€í„° Caseë“¤ì„ ë¶„ë¦¬í•´ì„œ, ì½”ë“œ ì‘ì„± 5.5.2. Case1: ì‚­ì œí•  Nodeê°€ Leaf Nodeì¸ ê²½ìš° 1234567# self.current_node ê°€ ì‚­ì œí•  Node, self.parentëŠ” ì‚­ì œí•  Nodeì˜ Parent Nodeì¸ ìƒíƒœ if self.current_node.left == None and self.current_node.right == None: if value &lt; self.parent.value: self.parent.left = None else: self.parent.right = None del self.current_node 5.5.2. Case2: ì‚­ì œí•  Nodeê°€ Child Nodeë¥¼ í•œ ê°œ ê°€ì§€ê³  ìˆì„ ê²½ìš° 12345678910if self.current_node.left != None and self.current_node.right == None: if value &lt; self.parent.value: self.parent.left = self.current_node.left else: self.parent.right = self.current_node.leftelif self.current_node.left == None and self.current_node.right != None: if value &lt; self.parent.value: self.parent.left = self.current_node.right else: self.parent.right = self.current_node.right 5.5.3. Case3-1: ì‚­ì œí•  Nodeê°€ Child Nodeë¥¼ ë‘ ê°œ ê°€ì§€ê³  ìˆì„ ê²½ìš° (ì‚­ì œí•  Nodeê°€ Parent Node ì™¼ìª½ì— ìˆì„ ë•Œ) ê¸°ë³¸ ì‚¬ìš© ê°€ëŠ¥ ì „ëµ ì‚­ì œí•  Nodeì˜ ì˜¤ë¥¸ìª½ ìì‹ ì¤‘, ê°€ì¥ ì‘ì€ ê°’ì„ ì‚­ì œí•  Nodeì˜ Parent Nodeê°€ ê°€ë¦¬í‚¤ë„ë¡ í•œë‹¤. ì‚­ì œí•  Nodeì˜ ì™¼ìª½ ìì‹ ì¤‘, ê°€ì¥ í° ê°’ì„ ì‚­ì œí•  Nodeì˜ Parent Nodeê°€ ê°€ë¦¬í‚¤ë„ë¡ í•œë‹¤. ê¸°ë³¸ ì‚¬ìš© ê°€ëŠ¥ ì „ëµ ì¤‘, 1ë²ˆ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë¥¼ êµ¬í˜„í•˜ê¸°ë¡œ í•¨ ê²½ìš°ì˜ ìˆ˜ê°€ ë˜ë‹¤ì‹œ ë‘ê°€ì§€ê°€ ìˆìŒ Case3-1-1: ì‚­ì œí•  Nodeê°€ Parent Nodeì˜ ì™¼ìª½ì— ìˆê³ , ì‚­ì œí•  Nodeì˜ ì˜¤ë¥¸ìª½ ìì‹ ì¤‘, ê°€ì¥ ì‘ì€ ê°’ì„ ê°€ì§„ Nodeì˜ Child Nodeê°€ ì—†ì„ ë•Œ Case3-1-2: ì‚­ì œí•  Nodeê°€ Parent Nodeì˜ ì™¼ìª½ì— ìˆê³ , ì‚­ì œí•  Nodeì˜ ì˜¤ë¥¸ìª½ ìì‹ ì¤‘, ê°€ì¥ ì‘ì€ ê°’ì„ ê°€ì§„ Nodeì˜ ì˜¤ë¥¸ìª½ì— Child Nodeê°€ ìˆì„ ë•Œ ê°€ì¥ ì‘ì€ ê°’ì„ ê°€ì§„ Nodeì˜ Child Nodeê°€ ì™¼ìª½ì— ìˆì„ ê²½ìš°ëŠ” ì—†ìŒ, ì™œëƒí•˜ë©´ ì™¼ìª½ Nodeê°€ ìˆë‹¤ëŠ” ê²ƒì€ í•´ë‹¹ Nodeë³´ë‹¤ ë” ì‘ì€ ê°’ì„ ê°€ì§„ Nodeê°€ ìˆë‹¤ëŠ” ëœ»ì´ê¸° ë•Œë¬¸ì„ 1234567891011121314if self.current_node.left != None and self.current_node.right != None: # case3 if value &lt; self.parent.value: # case3-1 self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.change_node.left if self.change_node.right != None: self.change_node_parent.left = self.change_node.right else: self.change_node_parent.left = None self.parent.left = self.change_node self.change_node.right = self.current_node.right self.change_node.left = self.current_node.left 5.5.4. Case3-2: ì‚­ì œí•  Nodeê°€ Child Nodeë¥¼ ë‘ ê°œ ê°€ì§€ê³  ìˆì„ ê²½ìš° (ì‚­ì œí•  Nodeê°€ Parent Node ì˜¤ë¥¸ìª½ì— ìˆì„ ë•Œ) ê¸°ë³¸ ì‚¬ìš© ê°€ëŠ¥ ì „ëµ ì‚­ì œí•  Nodeì˜ ì˜¤ë¥¸ìª½ ìì‹ ì¤‘, ê°€ì¥ ì‘ì€ ê°’ì„ ì‚­ì œí•  Nodeì˜ Parent Nodeê°€ ê°€ë¦¬í‚¤ë„ë¡ í•œë‹¤. ì‚­ì œí•  Nodeì˜ ì™¼ìª½ ìì‹ ì¤‘, ê°€ì¥ í° ê°’ì„ ì‚­ì œí•  Nodeì˜ Parent Nodeê°€ ê°€ë¦¬í‚¤ë„ë¡ í•œë‹¤. ê¸°ë³¸ ì‚¬ìš© ê°€ëŠ¥ ì „ëµ ì¤‘, 1ë²ˆ ì „ëµì„ ì‚¬ìš©í•˜ì—¬ ì½”ë“œë¥¼ êµ¬í˜„í•˜ê¸°ë¡œ í•¨ ê²½ìš°ì˜ ìˆ˜ê°€ ë˜ë‹¤ì‹œ ë‘ê°€ì§€ê°€ ìˆìŒ Case3-2-1: ì‚­ì œí•  Nodeê°€ Parent Nodeì˜ ì˜¤ë¥¸ìª½ì— ìˆê³ , ì‚­ì œí•  Nodeì˜ ì˜¤ë¥¸ìª½ ìì‹ ì¤‘, ê°€ì¥ ì‘ì€ ê°’ì„ ê°€ì§„ Nodeì˜ Child Nodeê°€ ì—†ì„ ë•Œ Case3-2-2: ì‚­ì œí•  Nodeê°€ Parent Nodeì˜ ì˜¤ë¥¸ìª½ì— ìˆê³ , ì‚­ì œí•  Nodeì˜ ì˜¤ë¥¸ìª½ ìì‹ ì¤‘, ê°€ì¥ ì‘ì€ ê°’ì„ ê°€ì§„ Nodeì˜ ì˜¤ë¥¸ìª½ì— Child Nodeê°€ ìˆì„ ë•Œ ê°€ì¥ ì‘ì€ ê°’ì„ ê°€ì§„ Nodeì˜ Child Nodeê°€ ì™¼ìª½ì— ìˆì„ ê²½ìš°ëŠ” ì—†ìŒ, ì™œëƒí•˜ë©´ ì™¼ìª½ Nodeê°€ ìˆë‹¤ëŠ” ê²ƒì€ í•´ë‹¹ Nodeë³´ë‹¤ ë” ì‘ì€ ê°’ì„ ê°€ì§„ Nodeê°€ ìˆë‹¤ëŠ” ëœ»ì´ê¸° ë•Œë¬¸ì„ 12345678910111213else: self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.change_node.left if self.change_node.right != None: self.change_node_parent.left = self.change_node.right else: self.change_node_parent.left = None self.parent.right = self.change_node self.change_node.left = self.current_node.left self.change_node.right = self.current_node.right 5.5.5. íŒŒì´ì¬ ì „ì²´ ì½”ë“œ êµ¬í˜„ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108class Node: def __init__(self, value): self.value = value self.left = None self.right = Noneclass NodeMgmt: def __init__(self, head): self.head = head def insert(self, value): self.current_node = self.head while True: if value &lt; self.current_node.value: if self.current_node.left != None: self.current_node = self.current_node.left else: self.current_node.left = Node(value) break else: if self.current_node.right != None: self.current_node = self.current_node.right else: self.current_node.right = Node(value) break def search(self, value): self.current_node = self.head while self.current_node: if self.current_node.value == value: return True elif value &lt; self.current_node.value: self.current_node = self.current_node.left else: self.current_node = self.current_node.right return False def delete(self, value): # ì‚­ì œí•  ë…¸ë“œ íƒìƒ‰ searched = False self.current_node = self.head self.parent = self.head while self.current_node: if self.current_node.value == value: searched = True break elif value &lt; self.current_node.value: self.parent = self.current_node self.current_node = self.current_node.left else: self.parent = self.current_node self.current_node = self.current_node.right if searched == False: return False # case1 if self.current_node.left == None and self.current_node.right == None: if value &lt; self.parent.value: self.parent.left = None else: self.parent.right = None # case2 elif self.current_node.left != None and self.current_node.right == None: if value &lt; self.parent.value: self.parent.left = self.current_node.left else: self.parent.right = self.current_node.left elif self.current_node.left == None and self.current_node.right != None: if value &lt; self.parent.value: self.parent.left = self.current_node.right else: self.parent.right = self.current_node.right # case 3 elif self.current_node.left != None and self.current_node.right != None: # case3-1 if value &lt; self.parent.value: self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.change_node.left if self.change_node.right != None: self.change_node_parent.left = self.change_node.right else: self.change_node_parent.left = None self.parent.left = self.change_node self.change_node.right = self.current_node.right self.change_node.left = self.current_node.left # case 3-2 else: self.change_node = self.current_node.right self.change_node_parent = self.current_node.right while self.change_node.left != None: self.change_node_parent = self.change_node self.change_node = self.change_node.left if self.change_node.right != None: self.change_node_parent.left = self.change_node.right else: self.change_node_parent.left = None self.parent.right = self.change_node self.change_node.right = self.current_node.right self.change_node.left = self.current_node.left return True ì°¸ê³ : http://ejklike.github.io/2018/01/09/traversing-a-binary-tree-1.html 5.5.6. íŒŒì´ì¬ ì „ì²´ ì½”ë“œ í…ŒìŠ¤íŠ¸ random ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš© random.randint(ì²«ë²ˆì§¸ ìˆ«ì, ë§ˆì§€ë§‰ ìˆ«ì): ì²«ë²ˆì§¸ ìˆ«ìë¶€í„° ë§ˆì§€ë§‰ ìˆ«ì ì‚¬ì´ì— ìˆëŠ” ìˆ«ìë¥¼ ëœë¤í•˜ê²Œ ì„ íƒí•´ì„œ ë¦¬í„´ ì˜ˆ: random.randint(0, 99): 0ì—ì„œ 99ê¹Œì§€ ìˆ«ìì¤‘ íŠ¹ì • ìˆ«ìë¥¼ ëœë¤í•˜ê²Œ ì„ íƒí•´ì„œ ë¦¬í„´í•´ì¤Œ 1234567891011121314151617181920212223242526272829303132# 0 ~ 999 ìˆ«ì ì¤‘ì—ì„œ ì„ì˜ë¡œ 100ê°œë¥¼ ì¶”ì¶œí•´ì„œ, ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ì— ì…ë ¥, ê²€ìƒ‰, ì‚­ì œimport random# 0 ~ 999 ì¤‘, 100 ê°œì˜ ìˆ«ì ëœë¤ ì„ íƒ# set ìë£Œí˜•ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬(BST)ì—ì„œ ì…ë ¥, ê²€ìƒ‰, ì‚­ì œí•  ê²½ìš° ì¤‘ë³µì´ìˆëŠ” ìˆ«ìì˜ ê²½ìš°# ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¤‘ë³µì´ ì—†ë„ë¡ í•´ì„œ 100ê°œë¥¼ ë½‘ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì´ ì½”ë“œë¥¼ ì‘ì„±í•˜ì˜€ë‹¤.bst_nums = set()while len(bst_nums) != 100: bst_nums.add(random.randint(0, 999))# print (bst_nums)# ì„ íƒëœ 100ê°œì˜ ìˆ«ìë¥¼ ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ì— ì…ë ¥, ì„ì˜ë¡œ ë£¨íŠ¸ë…¸ë“œëŠ” 500ì„ ë„£ê¸°ë¡œ í•¨head = Node(500)binary_tree = NodeMgmt(head)for num in bst_nums: binary_tree.insert(num)# ì…ë ¥í•œ 100ê°œì˜ ìˆ«ì ê²€ìƒ‰ (ê²€ìƒ‰ ê¸°ëŠ¥ í™•ì¸)for num in bst_nums: if binary_tree.search(num) == False: print ('search failed', num)# ì…ë ¥í•œ 100ê°œì˜ ìˆ«ì ì¤‘ 10ê°œì˜ ìˆ«ìë¥¼ ëœë¤ ì„ íƒdelete_nums = set()bst_nums = list(bst_nums)while len(delete_nums) != 10: delete_nums.add(bst_nums[random.randint(0, 99)])# ì„ íƒí•œ 10ê°œì˜ ìˆ«ìë¥¼ ì‚­ì œ (ì‚­ì œ ê¸°ëŠ¥ í™•ì¸)for del_num in delete_nums: if binary_tree.delete(del_num) == False: print('delete failed', del_num) 6. ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ì˜ ì‹œê°„ ë³µì¡ë„ì™€ ë‹¨ì 6.1. ì‹œê°„ ë³µì¡ë„ (íƒìƒ‰ì‹œ) depth (íŠ¸ë¦¬ì˜ ë†’ì´) ë¥¼ hë¼ê³  í‘œê¸°í•œë‹¤ë©´, O(h) nê°œì˜ ë…¸ë“œë¥¼ ê°€ì§„ë‹¤ë©´, $h = log_2{n} $ ì— ê°€ê¹Œìš°ë¯€ë¡œ, ì‹œê°„ ë³µì¡ë„ëŠ” $ O(log{n}) $ ì°¸ê³ : ë¹…ì˜¤ í‘œê¸°ë²•ì—ì„œ $log{n}$ ì—ì„œì˜ logì˜ ë°‘ì€ 10ì´ ì•„ë‹ˆë¼, 2ì…ë‹ˆë‹¤. í•œë²ˆ ì‹¤í–‰ì‹œë§ˆë‹¤, 50%ì˜ ì‹¤í–‰í•  ìˆ˜ë„ ìˆëŠ” ëª…ë ¹ì„ ì œê±°í•œë‹¤ëŠ” ì˜ë¯¸. ì¦‰ 50%ì˜ ì‹¤í–‰ì‹œê°„ì„ ë‹¨ì¶•ì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨ (ì¶œì²˜: https://www.mathwarehouse.com/programming/gifs/binary-search-tree.php#binary-search-tree-insertion-node) 6.2. ì´ì§„ íƒìƒ‰ íŠ¸ë¦¬ ë‹¨ì  í‰ê·  ì‹œê°„ ë³µì¡ë„ëŠ” $ O(log{n}) $ ì´ì§€ë§Œ, ì´ëŠ” íŠ¸ë¦¬ê°€ ê· í˜•ì¡í˜€ ìˆì„ ë•Œì˜ í‰ê·  ì‹œê°„ë³µì¡ë„ì´ë©°, ë‹¤ìŒ ì˜ˆì™€ ê°™ì´ êµ¬ì„±ë˜ì–´ ìˆì„ ê²½ìš°, ìµœì•…ì˜ ê²½ìš°ëŠ” ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ë“±ê³¼ ë™ì¼í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ ( $O(n)$ )","categories":[{"name":"C/C++/ìë£Œêµ¬ì¡°","slug":"C-C-ìë£Œêµ¬ì¡°","permalink":"https://heung-bae-lee.github.io/categories/C-C-ìë£Œêµ¬ì¡°/"}],"tags":[]},{"title":"ë‚´ê°€ ì •ë¦¬í•˜ëŠ” ìë£Œêµ¬ì¡° 04 - í•´ì‰¬ í…Œì´ë¸”","slug":"data_structure_05","date":"2020-04-30T14:32:54.000Z","updated":"2020-05-02T11:14:21.913Z","comments":true,"path":"2020/04/30/data_structure_05/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/30/data_structure_05/","excerpt":"","text":"ëŒ€í‘œì ì¸ ë°ì´í„° êµ¬ì¡°6: í•´ì‰¬ í…Œì´ë¸” (Hash Table)1. í•´ì‰¬ êµ¬ì¡° Hash Table: í‚¤(Key)ì— ë°ì´í„°(Value)ë¥¼ ì €ì¥í•˜ëŠ” ë°ì´í„° êµ¬ì¡° Keyë¥¼ í†µí•´ ë°”ë¡œ ë°ì´í„°ë¥¼ ë°›ì•„ì˜¬ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì†ë„ê°€ íšê¸°ì ìœ¼ë¡œ ë¹¨ë¼ì§ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬(Dictionary) íƒ€ì…ì´ í•´ì‰¬ í…Œì´ë¸”ì˜ ì˜ˆ: Keyë¥¼ ê°€ì§€ê³  ë°”ë¡œ ë°ì´í„°(Value)ë¥¼ êº¼ëƒ„ ë³´í†µ ë°°ì—´ë¡œ ë¯¸ë¦¬ Hash Table ì‚¬ì´ì¦ˆë§Œí¼ ìƒì„± í›„ì— ì‚¬ìš© (ê³µê°„ê³¼ íƒìƒ‰ ì‹œê°„ì„ ë§ë°”ê¾¸ëŠ” ê¸°ë²•) ë°°ì—´ì„ Hash Tableì„ ë§Œë“œëŠ”ë° ì‚¬ìš©í•˜ì§€ë§Œ, ì—¬ëŸ¬ í‚¤ì— í•´ë‹¹í•˜ëŠ” ì£¼ì†Œê°€ ë™ì¼í•  ê²½ìš° ì¶©ëŒì„ í•´ê²°í•˜ê¸° ìœ„í•´ í•´ì‰¬í…Œì´ë¸”ì˜ ê³µê°„ì„ ëŠ˜ë¦¼ìœ¼ë¡œì¸í•´ì„œ ë°°ì—´ë³´ë‹¨ ë§ì€ ì €ì¥ê³µê°„ì´ í•„ìš”í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ ë‹¨, íŒŒì´ì¬ì—ì„œëŠ” í•´ì‰¬ë¥¼ ë³„ë„ êµ¬í˜„í•  ì´ìœ ê°€ ì—†ìŒ - ë”•ì…”ë„ˆë¦¬ íƒ€ì…ì„ ì‚¬ìš©í•˜ë©´ ë¨ 2. ì•Œì•„ë‘˜ ìš©ì–´ í•´ì‰¬(Hash): ì„ì˜ ê°’(ë°ì´í„°)ì„ ê³ ì • ê¸¸ì´ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒ í•´ì‰¬ í…Œì´ë¸”(Hash Table): í‚¤ ê°’ì˜ ì—°ì‚°ì— ì˜í•´ ì§ì ‘ ì ‘ê·¼ì´ ê°€ëŠ¥í•œ ë°ì´í„° êµ¬ì¡° í•´ì‹± í•¨ìˆ˜(Hashing Function): Keyì— ëŒ€í•´ ì‚°ìˆ  ì—°ì‚°ì„ ì´ìš©í•´ ë°ì´í„° ìœ„ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ìˆëŠ” í•¨ìˆ˜ í•´ì‰¬ ê°’(Hash Value) ë˜ëŠ” í•´ì‰¬ ì£¼ì†Œ(Hash Address): Keyë¥¼ í•´ì‹± í•¨ìˆ˜ë¡œ ì—°ì‚°í•´ì„œ, í•´ì‰¬ ê°’ì„ ì•Œì•„ë‚´ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ì‰¬ í…Œì´ë¸”ì—ì„œ í•´ë‹¹ Keyì— ëŒ€í•œ ë°ì´í„° ìœ„ì¹˜ë¥¼ ì¼ê´€ì„±ìˆê²Œ ì°¾ì„ ìˆ˜ ìˆìŒ ìŠ¬ë¡¯(Slot): í•œ ê°œì˜ ë°ì´í„°ë¥¼ ì €ì¥í•  ìˆ˜ ìˆëŠ” ê³µê°„ ì €ì¥í•  ë°ì´í„°ì— ëŒ€í•´ Keyë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ë³„ë„ í•¨ìˆ˜ë„ ì¡´ì¬í•  ìˆ˜ ìˆìŒ 3. ê°„ë‹¨í•œ í•´ì‰¬ ì˜ˆ3.1. hash table ë§Œë“¤ê¸° ì°¸ê³ : íŒŒì´ì¬ list comprehension - https://www.fun-coding.org/PL&amp;OOP5-2.html 12hash_table = list([i for i in range(10)])hash_table ê²°ê³¼ 1[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 3.2. ì´ë²ˆì—” ì´ˆê°„ë‹¨ í•´ì‰¬ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ë³´ì. ë‹¤ì–‘í•œ í•´ì‰¬ í•¨ìˆ˜ ê³ ì•ˆ ê¸°ë²•ì´ ìˆìœ¼ë©°, ê°€ì¥ ê°„ë‹¨í•œ ë°©ì‹ì´ Division ë²• (ë‚˜ëˆ„ê¸°ë¥¼ í†µí•œ ë‚˜ë¨¸ì§€ ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•)ì´ë‹¤. 12def hash_func(key): return key % 5 3.3. í•´ì‰¬ í…Œì´ë¸”ì— ì €ì¥í•´ë³´ê² ë‹¤.- ë°ì´í„°ì— ë”°ë¼ í•„ìš”ì‹œ key ìƒì„± ë°©ë²• ì •ì˜ê°€ í•„ìš”í•¨12345678data1 = 'Andy'data2 = 'Dave'data3 = 'Trump'data4 = 'Anthor'## ord(): ë¬¸ìì˜ ASCII(ì•„ìŠ¤í‚¤)ì½”ë“œ ë¦¬í„´print (ord(data1[0]), ord(data2[0]), ord(data3[0]))print (ord(data1[0]), hash_func(ord(data1[0])))print (ord(data1[0]), ord(data4[0])) ê²°ê³¼ 12365 68 8465 065 65 3.3.2. í•´ì‰¬ í…Œì´ë¸”ì— ê°’ ì €ì¥ ì˜ˆ data:value ì™€ ê°™ì´ data ì™€ valueë¥¼ ë„£ìœ¼ë©´, í•´ë‹¹ dataì— ëŒ€í•œ keyë¥¼ ì°¾ì•„ì„œ, í•´ë‹¹ keyì— ëŒ€ì‘í•˜ëŠ” í•´ì‰¬ì£¼ì†Œì— valueë¥¼ ì €ì¥í•˜ëŠ” ì˜ˆ 1234def storage_data(data, value): key = ord(data[0]) hash_address = hash_func(key) hash_table[hash_address] = value 3.4. í•´ì‰¬ í…Œì´ë¸”ì—ì„œ íŠ¹ì • ì£¼ì†Œì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ë„ ë§Œë“¤ì–´ë³´ì. 1234storage_data('Andy', '01055553333')storage_data('Dave', '01044443333')storage_data('Trump', '01022223333')# storage_data('Anthor', '01046723456') 3.5. ì‹¤ì œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³ , ì½ì–´ë³´ê² ë‹¤. 1234def get_data(data): key = ord(data[0]) hash_address = hash_func(key) return hash_table[hash_address] 1get_data('Andy') ê²°ê³¼ 1'01046723456' 4. ìë£Œ êµ¬ì¡° í•´ì‰¬ í…Œì´ë¸”ì˜ ì¥ë‹¨ì ê³¼ ì£¼ìš” ìš©ë„ ì¥ì  ë°ì´í„° ì €ì¥/ì½ê¸° ì†ë„ê°€ ë¹ ë¥´ë‹¤. (ê²€ìƒ‰ ì†ë„ê°€ ë¹ ë¥´ë‹¤.) í•´ì‰¬ëŠ” í‚¤ì— ëŒ€í•œ ë°ì´í„°ê°€ ìˆëŠ”ì§€(ì¤‘ë³µ) í™•ì¸ì´ ì‰¬ì›€ ë‹¨ì  ì¼ë°˜ì ìœ¼ë¡œ ì €ì¥ê³µê°„ì´ ì¢€ë” ë§ì´ í•„ìš”í•˜ë‹¤. ì—¬ëŸ¬ í‚¤ì— í•´ë‹¹í•˜ëŠ” ì£¼ì†Œê°€ ë™ì¼í•  ê²½ìš° ì¶©ëŒì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë³„ë„ ìë£Œêµ¬ì¡°ê°€ í•„ìš”í•¨ ì£¼ìš” ìš©ë„ ê²€ìƒ‰ì´ ë§ì´ í•„ìš”í•œ ê²½ìš° ì €ì¥, ì‚­ì œ, ì½ê¸°ê°€ ë¹ˆë²ˆí•œ ê²½ìš° ìºì‰¬ êµ¬í˜„ì‹œ (ì¤‘ë³µ í™•ì¸ì´ ì‰½ê¸° ë•Œë¬¸) 5. í”„ë¡œê·¸ë˜ë° ì—°ìŠµ ì—°ìŠµ1: ë¦¬ìŠ¤íŠ¸ ë³€ìˆ˜ë¥¼ í™œìš©í•´ì„œ í•´ì‰¬ í…Œì´ë¸” êµ¬í˜„í•´ë³´ê¸° 1. í•´ì‰¬ í•¨ìˆ˜: key % 8 2. í•´ì‰¬ í‚¤ ìƒì„±: hash(data) ë°‘ì—ì„œ ì‚¬ìš©ë˜ëŠ” hashí•¨ìˆ˜ëŠ” pythonì„ ìƒˆë¡œ ì‹œì‘í•  ë•Œë§ˆë‹¤ ì¶œë ¥ë˜ëŠ” ê°’ì´ ë³€í™”ë˜ë¯€ë¡œ ì£¼ì˜í•˜ì. 123456789101112131415hash_table = list([0 for i in range(8)])def get_key(data): return hash(data)def hash_function(key): return key % 8def save_data(data, value): hash_address = hash_function(get_key(data)) hash_table[hash_address] = valuedef read_data(data): hash_address = hash_function(get_key(data)) return hash_table[hash_address] 123save_data('Dave', '0102030200')save_data('Andy', '01033232200')read_data('Dave') ê²°ê³¼ 1'0102030200' 1hash_table ê²°ê³¼ 1[0, 0, '01033232200', 0, 0, 0, '0102030200', 0] 6. ì¶©ëŒ(Collision) í•´ê²° ì•Œê³ ë¦¬ì¦˜ (ì¢‹ì€ í•´ì‰¬ í•¨ìˆ˜ ì‚¬ìš©í•˜ê¸°) í•´ì‰¬ í…Œì´ë¸”ì˜ ê°€ì¥ í° ë¬¸ì œëŠ” ì¶©ëŒ(Collision)ì˜ ê²½ìš°ì´ë‹¤.ì´ ë¬¸ì œë¥¼ ì¶©ëŒ(Collision) ë˜ëŠ” í•´ì‰¬ ì¶©ëŒ(Hash Collision)ì´ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. 6.1. Chaining ê¸°ë²• ê°œë°© í•´ìŠ ë˜ëŠ” Open Hashing ê¸°ë²• ì¤‘ í•˜ë‚˜: í•´ì‰¬ í…Œì´ë¸” ì €ì¥ê³µê°„ ì™¸ì˜ ê³µê°„ì„ í™œìš©í•˜ëŠ” ê¸°ë²• ì¶©ëŒì´ ì¼ì–´ë‚˜ë©´, ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ë¼ëŠ” ìë£Œ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•´ì„œ, ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°ì´í„°ë¥¼ ì¶”ê°€ë¡œ ë’¤ì— ì—°ê²°ì‹œì¼œì„œ ì €ì¥í•˜ëŠ” ê¸°ë²• ì—°ìŠµ2: ì—°ìŠµ1ì˜ í•´ì‰¬ í…Œì´ë¸” ì½”ë“œì— Chaining ê¸°ë²•ìœ¼ë¡œ ì¶©ëŒí•´ê²° ì½”ë“œë¥¼ ì¶”ê°€í•´ë³´ê¸° 1. í•´ì‰¬ í•¨ìˆ˜: key % 8 2. í•´ì‰¬ í‚¤ ìƒì„±: hash(data) 123456789101112131415161718192021hash_table = list([0 for i range(8)])def get_key(data): return hash(data)def hashing_function(key): return key % 8def save_data(data, value): key=get(data) hash_address=hashing_function(key) if hash_table[hash_address] != 0: hash_table[hash_address]=value else: prev_value = hash_table[hash_address] hash_table[hash_address] = [prev_value, value]def read_data(data): key=get(data) hash_address=hashing_function(key) return hash_table[hash_address] ë‚´ê°€ ë§Œë“  í•¨ìˆ˜ëŠ” ë™ì¼í•œ í‚¤ê°’ì— ì—¬ëŸ¬ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ Chaining ê¸°ë²•ìœ¼ë¡œ ì¶©ë™í•´ê²°ì„ ë°©ì§€í•œ ì½”ë“œê°€ ì•„ë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì¢‹ì•˜ìœ¼ë‚˜, ì•„ë˜ì™€ ê°™ì´ í•´ë‹¹ keyê°’ì„ ê°–ê³ ìˆì–´ì•¼ ë™ì¼í•œ í‚¤ê°’ì´ ì•„ë‹Œ ê°ê° ê³ ìœ ì˜ í‚¤ê°’ì„ ê°€ì ¸ ë°ì´í„°ë¥¼ ì½ì–´ë“¤ì¼ ê²½ìš°ì—ë„ ì •í™•í•˜ê²Œ íŠ¹ì • í‚¤ê°’ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. 123456789101112131415161718192021222324252627282930313233hash_table = list([0 for i in range(8)])def get_key(data): return hash(data)def hash_function(key): return key % 8def save_data(data, value): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: # í•´ë‹¹ hash_addressê°’ì— ì´ë¯¸ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš° í‚¤ê°’ê³¼ ë™ì¼í•œ ê²ƒì´ë¼ë©´ # ê°’ì„ ì €ì¥í•˜ì§€ë§Œ ê·¸ë ‡ì§€ ì•Šë‹¤ë©´ ë¦¬ìŠ¤íŠ¸ë¡œ [index, value]í˜•ì‹ìœ¼ë¡œ ì €ì¥ for index in range(len(hash_table[hash_address])): if hash_table[hash_address][index][0] == index_key: hash_table[hash_address][index][1] = value return hash_table[hash_address].append([index_key, value]) else: hash_table[hash_address] = [[index_key, value]]def read_data(data): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: for index in range(len(hash_table[hash_address])): if hash_table[hash_address][index][0] == index_key: return hash_table[hash_address][index][1] return None else: return None 123print (hash('Dave') % 8)print (hash('Da') % 8)print (hash('Data') % 8) ê²°ê³¼ 123663 ë™ì¼í•œ hashí•¨ìˆ˜ ê°’ì„ ê°–ì§€ë§Œ ë™ì¼ ì£¼ì†Œì—ì„œ ì €ì¥ì„í•˜ì§€ë§Œ í‚¤ê°’ì„ ë‹¤ë¥´ê²Œ í•˜ì—¬ ì €ì¥í•œë‹¤. ë˜í•œ, í˜„ì¬ëŠ” Chaning ê¸°ë²•ì„ ì‚¬ìš©í–ˆìœ¼ë¯€ ë§í¬ë“œë¦¬ìŠ¤íŠ¸ë¡œ ë™ì¼í•œ ì£¼ì†Œì•ˆì— ì €ì¥ë˜ì–´ ì—°ê²°ë˜ì–´ìˆìŒì„ í™•ì¸í•˜ì. 1234save_data('Da', '1201023010')save_data('Dave', '3301023010')print(read_data('Da'))print(read_data('Dave')) ê²°ê³¼ 1212010230103301023010 1hash_table ê²°ê³¼ 12345678[0, 0, 0, 0, 0, 0, [[-8338113502661437674, '1201023010'], [909867193312922558, '3301023010']], 0] 6.2. Linear Probing ê¸°ë²• íì‡„ í•´ìŠ ë˜ëŠ” Close Hashing ê¸°ë²• ì¤‘ í•˜ë‚˜: í•´ì‰¬ í…Œì´ë¸” ì €ì¥ê³µê°„ ì•ˆì—ì„œ ì¶©ëŒ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê¸°ë²• ì¶©ëŒì´ ì¼ì–´ë‚˜ë©´, í•´ë‹¹ hash addressì˜ ë‹¤ìŒ addressë¶€í„° ë§¨ ì²˜ìŒ ë‚˜ì˜¤ëŠ” ë¹ˆê³µê°„ì— ì €ì¥í•˜ëŠ” ê¸°ë²• ì €ì¥ê³µê°„ í™œìš©ë„ë¥¼ ë†’ì´ê¸° ìœ„í•œ ê¸°ë²• ì—°ìŠµ3: ì—°ìŠµ1ì˜ í•´ì‰¬ í…Œì´ë¸” ì½”ë“œì— Linear Probling ê¸°ë²•ìœ¼ë¡œ ì¶©ëŒí•´ê²° ì½”ë“œë¥¼ ì¶”ê°€í•´ë³´ê¸° 1. í•´ì‰¬ í•¨ìˆ˜: key % 8 2. í•´ì‰¬ í‚¤ ìƒì„±: hash(data) 12345678910111213141516171819202122232425262728293031hash_table = list([0 for i in range(8)])def get_key(data): return hash(data)def hash_function(key): return key % 8def save_data(data, value): index_key=get_key(data) hash_address=hash_function(index_key) if hash_table[hash_address] !=0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: hash_table[index] = [index_key, value] elif hash_table[index][0] == index_key: hash_table[index][1] = value else: hash_table[hash_address] = [index_key, value]def read_data(data): index_key=get_key(data) hash_address=hash_function(index_key) if hash_table[hash_address] == 0: return None else: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: return None elif hash_table[index][0] == index_key: return hash_table[index][1] read_dataí•¨ìˆ˜ì—ì„œ linear probling ê¸°ë²•ì€ í•´ë‹¹ hash_addressì— ê°’ì´ ì €ì¥ë˜ì–´ìˆë‹¤ë©´(ë™ì¼í•œ í‚¤ê°’ì€ ì—…ë°ì´íŠ¸í•˜ì§€ë§Œ) ë‹¤ìŒ ì£¼ì†Œì— ê°’ì„ ì €ì¥í•˜ë¯€ë¡œ ë§Œì•½ ë‹¤ìŒ ì£¼ì†Œ ì¤‘ 0ê°’ì´ ìˆë‹¤ë©´ í•´ë‹¹ ë°ì´í„°ë¥¼ ì €ì¥í•œì ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë¯€ë¡œ ê·¸ì— í•´ë‹¹í•˜ëŠ” ì½”ë“œë„ ì‘ì„±í•´ì£¼ì–´ì•¼ í•¨ì„ ìœ ì˜í•˜ì! 12345678910111213141516171819202122232425262728293031323334hash_table = list([0 for i in range(8)])def get_key(data): return hash(data)def hash_function(key): return key % 8def save_data(data, value): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: hash_table[index] = [index_key, value] return elif hash_table[index][0] == index_key: hash_table[index][1] = value return else: hash_table[hash_address] = [index_key, value]def read_data(data): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: return None elif hash_table[index][0] == index_key: return hash_table[index][1] else: return None 123print (hash('dk') % 8)print (hash('dw') % 8)print (hash('dc') % 8) ê²°ê³¼ 123422 1234save_data('dk', '01200123123')save_data('dw', '3333333333')save_data('dc', '23456781234')read_data('dc') ê²°ê³¼ 1'23456781234' 6.3. ë¹ˆë²ˆí•œ ì¶©ëŒì„ ê°œì„ í•˜ëŠ” ê¸°ë²• í•´ì‰¬ í•¨ìˆ˜ì„ ì¬ì •ì˜ ë° í•´ì‰¬ í…Œì´ë¸” ì €ì¥ê³µê°„ì„ í™•ëŒ€ ì˜ˆë¥¼ ë“¤ì–´ ì €ì¥í•˜ê³ ìí•˜ëŠ” ë°ì´í„°ê°€ 8ê°œë¼ë©´ ì´ì— 2ë°°ì— í•´ë‹¹í•˜ëŠ” ê³µê°„ì„ í•´ì‰¬ í…Œì´ë¸” êµ¬ì¡°ë¡œ ì €ì¥ê³µê°„ì„ ë§Œë“¤ì–´ ë‘ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ì˜ˆ: 1234hash_table = list([None for i in range(16)])def hash_function(key): return key % 16 ì°¸ê³ : í•´ì‰¬ í•¨ìˆ˜ì™€ í‚¤ ìƒì„± í•¨ìˆ˜ íŒŒì´ì¬ì˜ hash() í•¨ìˆ˜ëŠ” ì‹¤í–‰í•  ë•Œë§ˆë‹¤, ê°’ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ ìœ ëª…í•œ í•´ì‰¬ í•¨ìˆ˜ë“¤ì´ ìˆìŒ: SHA(Secure Hash Algorithm, ì•ˆì „í•œ í•´ì‹œ ì•Œê³ ë¦¬ì¦˜) ì–´ë–¤ ë°ì´í„°ë„ ìœ ì¼í•œ ê³ ì •ëœ í¬ê¸°ì˜ ê³ ì •ê°’ì„ ë¦¬í„´í•´ì£¼ë¯€ë¡œ, í•´ì‰¬ í•¨ìˆ˜ë¡œ ìœ ìš©í•˜ê²Œ í™œìš© ê°€ëŠ¥ SHA-1 1234567import hashlibdata = 'test'.encode()hash_object = hashlib.sha1()hash_object.update(data)hex_dig = hash_object.hexdigest()print (hex_dig) ê²°ê³¼ 1a94a8fe5ccb19ba61c4c0873d391e987982fbbd3 SHA-256 1234567import hashlibdata = 'test'.encode()hash_object = hashlib.sha256()hash_object.update(data)hex_dig = hash_object.hexdigest()print (hex_dig) ê²°ê³¼ 19f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08 ì—°ìŠµ4: ì—°ìŠµ2ì˜ Chaining ê¸°ë²•ì„ ì ìš©í•œ í•´ì‰¬ í…Œì´ë¸” ì½”ë“œì— í‚¤ ìƒì„± í•¨ìˆ˜ë¥¼ sha256 í•´ì‰¬ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½í•´ë³´ê¸° 1. í•´ì‰¬ í•¨ìˆ˜: key % 8 2. í•´ì‰¬ í‚¤ ìƒì„±: hash(data) 123456789101112131415161718192021222324252627282930313233343536373839import hashlibhash_table = list([0 for i in range(8)])def get_key(data): hash_object = hashlib.sha256() hash_object.update(data.encode()) hex_dig = hash_object.hexdigest() return int(hex_dig, 16)def hash_function(key): return key % 8def save_data(data, value): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: hash_table[index] = [index_key, value] return elif hash_table[index][0] == index_key: hash_table[index][1] = value return else: hash_table[hash_address] = [index_key, value]def read_data(data): index_key = get_key(data) hash_address = hash_function(index_key) if hash_table[hash_address] != 0: for index in range(hash_address, len(hash_table)): if hash_table[index] == 0: return None elif hash_table[index][0] == index_key: return hash_table[index][1] else: return None 123print (get_key('dw') % 8)print (get_key('deo') % 8)print (get_key('dh') % 8) ê²°ê³¼ 123200 123save_data('deo', '01200123123')save_data('dh', '3333333333')read_data('dh') ê²°ê³¼ 1'3333333333' 7. ì‹œê°„ ë³µì¡ë„ ì¼ë°˜ì ì¸ ê²½ìš°(Collisionì´ ì—†ëŠ” ê²½ìš°)ëŠ” O(1) ìµœì•…ì˜ ê²½ìš°(Collisionì´ ëª¨ë‘ ë°œìƒí•˜ëŠ” ê²½ìš°)ëŠ” O(n) í•´ì‰¬ í…Œì´ë¸”ì˜ ê²½ìš°, ì¼ë°˜ì ì¸ ê²½ìš°ë¥¼ ê¸°ëŒ€í•˜ê³  ë§Œë“¤ê¸° ë•Œë¬¸ì—, ì‹œê°„ ë³µì¡ë„ëŠ” O(1) ì´ë¼ê³  ë§í•  ìˆ˜ ìˆìŒ ê²€ìƒ‰ì—ì„œ í•´ì‰¬ í…Œì´ë¸”ì˜ ì‚¬ìš© ì˜ˆ 16ê°œì˜ ë°°ì—´ì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³ , ê²€ìƒ‰í•  ë•Œ O(n) 16ê°œì˜ ë°ì´í„° ì €ì¥ê³µê°„ì„ ê°€ì§„ ìœ„ì˜ í•´ì‰¬ í…Œì´ë¸”ì— ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³ , ê²€ìƒ‰í•  ë•Œ O(1)","categories":[{"name":"C/C++/ìë£Œêµ¬ì¡°","slug":"C-C-ìë£Œêµ¬ì¡°","permalink":"https://heung-bae-lee.github.io/categories/C-C-ìë£Œêµ¬ì¡°/"}],"tags":[]},{"title":"ë‚´ê°€ ì •ë¦¬í•˜ëŠ” ìë£Œêµ¬ì¡° 03 - ì‹œê°„ë³µì¡ë„","slug":"data_structure_04","date":"2020-04-30T09:12:50.000Z","updated":"2020-05-02T10:29:17.292Z","comments":true,"path":"2020/04/30/data_structure_04/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/30/data_structure_04/","excerpt":"","text":"ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ í‘œí˜„ ë°©ë²•1. ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ê³„ì‚°ì´ í•„ìš”í•œ ì´ìœ í•˜ë‚˜ì˜ ë¬¸ì œë¥¼ í‘¸ëŠ” ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ì–‘í•  ìˆ˜ ìˆìŒ ì •ìˆ˜ì˜ ì ˆëŒ€ê°’ êµ¬í•˜ê¸° 1, -1 -&gt;&gt; 1 ë°©ë²•1: ì •ìˆ˜ê°’ì„ ì œê³±í•œ ê°’ì— ë‹¤ì‹œ ë£¨íŠ¸ë¥¼ ì”Œìš°ê¸° ë°©ë²•2: ì •ìˆ˜ê°€ ìŒìˆ˜ì¸ì§€ í™•ì¸í•´ì„œ, ìŒìˆ˜ì¼ ë•Œë§Œ, -1ì„ ê³±í•˜ê¸° ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ ì¤‘ ì–´ëŠ ì•Œê³ ë¦¬ì¦˜ì´ ë” ì¢‹ì€ì§€ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•´, ë³µì¡ë„ë¥¼ ì •ì˜í•˜ê³  ê³„ì‚°í•¨ 2. ì•Œê³ ë¦¬ì¦˜ ë³µì¡ë„ ê³„ì‚° í•­ëª© ì‹œê°„ ë³µì¡ë„: ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ì†ë„ ê³µê°„ ë³µì¡ë„: ì•Œê³ ë¦¬ì¦˜ì´ ì‚¬ìš©í•˜ëŠ” ë©”ëª¨ë¦¬ ì‚¬ì´ì¦ˆ ê°€ì¥ ì¤‘ìš”í•œ ì‹œê°„ ë³µì¡ë„ë¥¼ ê¼­ ì´í•´í•˜ê³  ê³„ì‚°í•  ìˆ˜ ìˆì–´ì•¼ í•¨ ì•Œê³ ë¦¬ì¦˜ ì‹œê°„ ë³µì¡ë„ì˜ ì£¼ìš” ìš”ì†Œ ë°˜ë³µë¬¸ì´ ì§€ë°°í•œë‹¤. ìƒê°í•´ë³´ê¸°: ìë™ì°¨ë¡œ ì„œìš¸ì—ì„œ ë¶€ì‚°ì„ ê°€ê¸° ìœ„í•´, ë‹¤ìŒê³¼ ê°™ì´ í•­ëª©ì„ ë‚˜ëˆ„ì—ˆì„ ë•Œ, ê°€ì¥ ì´ ì‹œê°„ì— ì˜í–¥ì„ ë§ì´ ë¯¸ì¹  ê²ƒ ê°™ì€ ìš”ì†ŒëŠ”? 5ë²ˆ!!! * ì˜ˆ: - ìë™ì°¨ë¡œ ì„œìš¸ì—ì„œ ë¶€ì‚°ê°€ê¸° 1. ìë™ì°¨ ë¬¸ì—´ê¸° 2. ìë™ì°¨ ë¬¸ë‹«ê¸° 3. ìë™ì°¨ ìš´ì „ì„ ë“±ë°›ì´ ì¡°ì •í•˜ê¸° 4. ìë™ì°¨ ì‹œë™ê±¸ê¸° 5. `ìë™ì°¨ë¡œ ì„œìš¸ì—ì„œ ë¶€ì‚°ê°€ê¸°` 6. ìë™ì°¨ ì‹œë™ë„ê¸° 7. ìë™ì°¨ ë¬¸ì—´ê¸° 8. ìë™ì°¨ ë¬¸ë‹«ê¸° ë§ˆì°¬ê°€ì§€ë¡œ, í”„ë¡œê·¸ë˜ë°ì—ì„œ ì‹œê°„ ë³µì¡ë„ì— ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë¯¸ì¹˜ëŠ” ìš”ì†ŒëŠ” ë°˜ë³µë¬¸ ì…ë ¥ì˜ í¬ê¸°ê°€ ì»¤ì§€ë©´ ì»¤ì§ˆìˆ˜ë¡ ë°˜ë³µë¬¸ì´ ì•Œê³ ë¦¬ì¦˜ ìˆ˜í–‰ ì‹œê°„ì„ ì§€ë°°í•¨ ì•Œê³ ë¦¬ì¦˜ ì„±ëŠ¥ í‘œê¸°ë²• Big O (ë¹…-ì˜¤) í‘œê¸°ë²•: O(N) ì•Œê³ ë¦¬ì¦˜ ìµœì•…ì˜ ì‹¤í–‰ ì‹œê°„ì„ í‘œê¸° ê°€ì¥ ë§ì´/ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•¨ ì•„ë¬´ë¦¬ ìµœì•…ì˜ ìƒí™©ì´ë¼ë„, ì´ì •ë„ì˜ ì„±ëŠ¥ì€ ë³´ì¥í•œë‹¤ëŠ” ì˜ë¯¸ì´ê¸° ë•Œë¬¸ Î© (ì˜¤ë©”ê°€) í‘œê¸°ë²•: Î©(N) ì˜¤ë©”ê°€ í‘œê¸°ë²•ì€ ì•Œê³ ë¦¬ì¦˜ ìµœìƒì˜ ì‹¤í–‰ ì‹œê°„ì„ í‘œê¸° Î˜ (ì„¸íƒ€) í‘œê¸°ë²•: Î˜(N) ì˜¤ë©”ê°€ í‘œê¸°ë²•ì€ ì•Œê³ ë¦¬ì¦˜ í‰ê·  ì‹¤í–‰ ì‹œê°„ì„ í‘œê¸° ì‹œê°„ ë³µì¡ë„ ê³„ì‚°ì€ ë°˜ë³µë¬¸ì´ í•µì‹¬ ìš”ì†Œì„ì„ ì¸ì§€í•˜ê³ , ê³„ì‚° í‘œê¸°ëŠ” ìµœìƒ, í‰ê· , ìµœì•… ì¤‘, ìµœì•…ì˜ ì‹œê°„ì¸ Big-O í‘œê¸°ë²•ì„ ì¤‘ì‹¬ìœ¼ë¡œ ìµíˆë©´ ë¨ 3. ëŒ€ë¬¸ì O í‘œê¸°ë²• ë¹… ì˜¤ í‘œê¸°ë²•, Big-O í‘œê¸°ë²• ì´ë¼ê³ ë„ ë¶€ë¦„ O(ì…ë ¥) ì…ë ¥ n ì— ë”°ë¼ ê²°ì •ë˜ëŠ” ì‹œê°„ ë³µì¡ë„ í•¨ìˆ˜ O(1), O($log n$), O(n), O(n$log n$), O($n^2$), O($2^n$), O(n!)ë“±ìœ¼ë¡œ í‘œê¸°í•¨ ì…ë ¥ n ì˜ í¬ê¸°ì— ë”°ë¼ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì‹œê°„ ë³µì¡ë„ê°€ ëŠ˜ì–´ë‚  ìˆ˜ ìˆìŒ O(1) &lt; O($log n$) &lt; O(n) &lt; O(n$log n$) &lt; O($n^2$) &lt; O($2^n$) &lt; O(n!) ì°¸ê³ : log n ì˜ ë² ì´ìŠ¤ëŠ” 2 : $log_2 n$ ë‹¨ìˆœí•˜ê²Œ ì…ë ¥ nì— ë”°ë¼, ëª‡ë²ˆ ì‹¤í–‰ì´ ë˜ëŠ”ì§€ë¥¼ ê³„ì‚°í•˜ë©´ ë©ë‹ˆë‹¤. í‘œí˜„ì‹ì— ê°€ì¥ í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” n ì˜ ë‹¨ìœ„ë¡œ í‘œê¸°í•©ë‹ˆë‹¤. nì´ 1ì´ë“  100ì´ë“ , 1000ì´ë“ , 10000ì´ë“  ì‹¤í–‰ì„ ë¬´ì¡°ê±´ 2íšŒ(ìƒìˆ˜íšŒ) ì‹¤í–‰í•œë‹¤: O(1) 12if n &gt; 10: print(n) nì— ë”°ë¼, në²ˆ, n + 10 ë²ˆ, ë˜ëŠ” 3n + 10 ë²ˆë“± ì‹¤í–‰í•œë‹¤: O(n) 1234variable = 1for num in range(3): for index in range(n): print(index) nì— ë”°ë¼, $n^2$ë²ˆ, $n^2$ + 1000 ë²ˆ, 100$n^2$ - 100, ë˜ëŠ” 300$n^2$ + 1ë²ˆë“± ì‹¤í–‰í•œë‹¤: O($n^2$) 1234567891011121314151617181920212223242526272829303132333435 variable = 1 for i in range(300): for num in range(n): for index in range(n): print(index) ``` &lt;img src=\"http://www.fun-coding.org/00_Images/bigo.png\" width=400/&gt;* ë¹… ì˜¤ ì…ë ¥ê°’ í‘œê¸° ë°©ë²• - ì˜ˆ: - ë§Œì•½ ì‹œê°„ ë³µì¡ë„ í•¨ìˆ˜ê°€ 2$n^2$ + 3n ì´ë¼ë©´ - ê°€ì¥ ë†’ì€ ì°¨ìˆ˜ëŠ” 2$n^2$ - ìƒìˆ˜ëŠ” ì‹¤ì œ í° ì˜í–¥ì´ ì—†ìŒ - ê²°êµ­ ë¹… ì˜¤ í‘œê¸°ë²•ìœ¼ë¡œëŠ” O($n^2$) (ì„œìš¸ë¶€í„° ë¶€ì‚°ê¹Œì§€ ê°€ëŠ” ìë™ì°¨ì˜ ì˜ˆë¥¼ ìƒê¸°)### 4. ì‹¤ì œ ì•Œê³ ë¦¬ì¦˜ì„ ì˜ˆë¡œ ê° ì•Œê³ ë¦¬ì¦˜ì˜ ì‹œê°„ ë³µì¡ë„ì™€ ë¹… ì˜¤ í‘œê¸°ë²• ì•Œì•„ë³´ê¸°&lt;div class=\"alert alert-block alert-warning\"&gt;&lt;strong&gt;&lt;font color=\"blue\" size=\"3em\"&gt;ì—°ìŠµ1: 1ë¶€í„° nê¹Œì§€ì˜ í•©ì„ êµ¬í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ ì‘ì„±í•´ë³´ê¸°&lt;/font&gt;&lt;/strong&gt;&lt;/div&gt;### ì•Œê³ ë¦¬ì¦˜1: 1ë¶€í„° nê¹Œì§€ì˜ í•©ì„ êµ¬í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜1* í•©ì„ ê¸°ë¡í•  ë³€ìˆ˜ë¥¼ ë§Œë“¤ê³  0ì„ ì €ì¥* nì„ 1ë¶€í„° 1ì”© ì¦ê°€í•˜ë©´ì„œ ë°˜ë³µ* ë°˜ë³µë¬¸ ì•ˆì—ì„œ í•©ì„ ê¸°ë¡í•  ë³€ìˆ˜ì— 1ì”© ì¦ê°€ëœ ê°’ì„ ë”í•¨* ë°˜ë³µì´ ëë‚˜ë©´ í•©ì„ ì¶œë ¥--------``` bashdef sum_all(n): total = 0 for num in range(1, n + 1): total += num return total 12%%timeitsum_all(100) ê²°ê³¼14.62 Âµs Â± 217 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each) ì‹œê°„ ë³µì¡ë„ êµ¬í•˜ê¸° 1ë¶€í„° nê¹Œì§€ì˜ í•©ì„ êµ¬í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜1 ì…ë ¥ nì— ë”°ë¼ ë§ì…ˆì„ n ë²ˆ í•´ì•¼ í•¨ (ë°˜ë³µë¬¸!) ì‹œê°„ ë³µì¡ë„: n, ë¹… ì˜¤ í‘œê¸°ë²•ìœ¼ë¡œëŠ” O(n) ìœ„ì˜ í•¨ìˆ˜ì˜ ì‹œê°„ë³µì¡ë„ëŠ” O(n)ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì´ë³´ë‹¤ ë” ë¹ ë¥´ê²Œ í•¨ìˆ˜ë¥¼ ì‘ì„±í•  ìˆœ ì—†ì„ê¹Œ ìš°ë¦¬ëŠ” ì´ë¯¸ ì–´ë ¸ì„ë•Œ në²ˆì§¸ ê¹Œì§€ ìˆ˜ì˜ í•©ì„ êµ¬í•˜ëŠ” ê³µì‹ì„ ì•Œê³ ìˆë‹¤. ì•Œê³ ë¦¬ì¦˜2: 1ë¶€í„° nê¹Œì§€ì˜ í•©ì„ êµ¬í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜2 $\\frac { n (n + 1) }{ 2 }$ 12def sum_all(n): return int(n * (n + 1) / 2) 12%%timeitsum_all(100) ê²°ê³¼ 1248 ns Â± 3.71 ns per loop (mean Â± std. dev. of 7 runs, 1000000 loops each) ì‹œê°„ ë³µì¡ë„ êµ¬í•˜ê¸° 1ë¶€í„° nê¹Œì§€ì˜ í•©ì„ êµ¬í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜2 ì…ë ¥ nì´ ì–´ë–»ë“  ê°„ì—, ê³±ì…ˆ/ë§ì…ˆ/ë‚˜ëˆ—ì…ˆ í•˜ë©´ ë¨ (ë°˜ë³µë¬¸ì´ ì—†ìŒ!) ì‹œê°„ ë³µì¡ë„: 1, ë¹… ì˜¤ í‘œê¸°ë²•ìœ¼ë¡œëŠ” O(1) ì–´ëŠ ì•Œê³ ë¦¬ì¦˜ì´ ì„±ëŠ¥ì´ ì¢‹ì€ê°€ìš”? ì•Œê³ ë¦¬ì¦˜1 vs ì•Œê³ ë¦¬ì¦˜2 O(n) vs O(1) ì´ì™€ ê°™ì´, ë™ì¼í•œ ë¬¸ì œë¥¼ í‘¸ëŠ” ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ì–‘í•  ìˆ˜ ìˆìŒì–´ëŠ ì•Œê³ ë¦¬ì¦˜ì´ ë³´ë‹¤ ì¢‹ì€ì§€ë¥¼ ê°ê´€ì ìœ¼ë¡œ ë¹„êµí•˜ê¸° ìœ„í•´, ë¹… ì˜¤ í‘œê¸°ë²•ë“±ì˜ ì‹œê°„ë³µì¡ë„ ê³„ì‚°ë²•ì„ ì‚¬ìš©í•¨ ì´í›„ ìë£Œêµ¬ì¡°, ì•Œê³ ë¦¬ì¦˜ë¶€í„°ëŠ” ë¹… ì˜¤ í‘œê¸°ë²•ìœ¼ë¡œ ì„±ëŠ¥ì„ ê³„ì‚°í•´ë³´ë©´ì„œ, ë¹… ì˜¤ í‘œê¸°ë²•ê³¼ ê³„ì‚°ë°©ë²•ì— ìµìˆ™í•´ì§€ê¸°ë¡œ í•˜ì.","categories":[{"name":"C/C++/ìë£Œêµ¬ì¡°","slug":"C-C-ìë£Œêµ¬ì¡°","permalink":"https://heung-bae-lee.github.io/categories/C-C-ìë£Œêµ¬ì¡°/"}],"tags":[]},{"title":"ë‚´ê°€ ì •ë¦¬í•˜ëŠ” ìë£Œêµ¬ì¡° 02 Linked List","slug":"data_structure_03","date":"2020-04-27T19:41:46.000Z","updated":"2020-04-28T19:32:56.681Z","comments":true,"path":"2020/04/28/data_structure_03/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/28/data_structure_03/","excerpt":"","text":"ëŒ€í‘œì ì¸ ë°ì´í„° êµ¬ì¡°: ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ (Linked List)1. ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ (Linked List) êµ¬ì¡° ì—°ê²° ë¦¬ìŠ¤íŠ¸ë¼ê³ ë„ í•¨ ë°°ì—´ì€ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°ëœ ê³µê°„ì— ë°ì´í„°ë¥¼ ë‚˜ì—´í•˜ëŠ” ë°ì´í„° êµ¬ì¡° ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë¯¸ë¦¬ ì—°ê²°ëœ ê³µê°„ì„ ì˜ˆì•½ì„ í•´ë†“ì•„ì•¼ í•œë‹¤ëŠ” ê²ƒì´ ë‹¨ì ! ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ëŠ” ìœ„ì™€ ê°™ì€ ë°°ì—´ì˜ ë‹¨ì ì„ ë³´ì™„í•˜ê³ ì ë–¨ì–´ì§„ ê³³ì— ì¡´ì¬í•˜ëŠ” ë°ì´í„°ë¥¼ í™”ì‚´í‘œë¡œ ì—°ê²°í•´ì„œ ê´€ë¦¬í•˜ëŠ” ë°ì´í„° êµ¬ì¡° ë³¸ë˜ Cì–¸ì–´ì—ì„œëŠ” ì£¼ìš”í•œ ë°ì´í„° êµ¬ì¡°ì´ì§€ë§Œ, íŒŒì´ì¬ì€ ë¦¬ìŠ¤íŠ¸ íƒ€ì…ì´ ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ì˜ ê¸°ëŠ¥ì„ ëª¨ë‘ ì§€ì› ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ ê¸°ë³¸ êµ¬ì¡°ì™€ ìš©ì–´ ë…¸ë“œ(Node): ë°ì´í„° ì €ì¥ ë‹¨ìœ„ (ë°ì´í„°ê°’, í¬ì¸í„°) ë¡œ êµ¬ì„± í¬ì¸í„°(pointer): ê° ë…¸ë“œ ì•ˆì—ì„œ, ë‹¤ìŒì´ë‚˜ ì´ì „ì˜ ë…¸ë“œì™€ì˜ ì—°ê²° ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆëŠ” ê³µê°„ ì¼ë°˜ì ì¸ ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœ(ì¶œì²˜: wikipedia, https://en.wikipedia.org/wiki/Linked_list) 2. ê°„ë‹¨í•œ ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ ì˜ˆNode êµ¬í˜„ ë³´í†µ íŒŒì´ì¬ì—ì„œ ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ êµ¬í˜„ì‹œ, íŒŒì´ì¬ í´ë˜ìŠ¤ë¥¼ í™œìš©í•¨ íŒŒì´ì¬ ê°ì²´ì§€í–¥ ë¬¸ë²• ì´í•´ í•„ìš” ì°¸ê³ : https://www.fun-coding.org/PL&amp;OOP1-3.html ê°„ë‹¨í•œ ë…¸ë“œ 1 1234class Node: def __init__(self, data): self.data = data self.next = None ê°„ë‹¨í•œ ë…¸ë“œ 2 1234class Node: def __init__(self, data, next=None): self.data = data self.next = next Node 123456789101112131415161718192021class Node: def __init__(self, data): self.__data=data self.__next=None @property def data(self): return self.__data @data.setter def data(self, data): self.__data=data @property def next(self): return self.__next @next.setter def next(self, n): self.__next=n Nodeì™€ Node ì—°ê²°í•˜ê¸° (í¬ì¸í„° í™œìš©) 12345678node1 = Node(1)node2 = Node(2)print(node1.data)node1.data=2print(node1.data)node1.next = node2print(node1.next.data) ê²°ê³¼ 123122 3. ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ì˜ ì¥ë‹¨ì  (ì „í†µì ì¸ Cì–¸ì–´ì—ì„œì˜ ë°°ì—´ê³¼ ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸) ì¥ì  ë¯¸ë¦¬ ë°ì´í„° ê³µê°„ì„ ë¯¸ë¦¬ í• ë‹¹í•˜ì§€ ì•Šì•„ë„ ë¨ ë°°ì—´ì€ ë¯¸ë¦¬ ë°ì´í„° ê³µê°„ì„ í• ë‹¹ í•´ì•¼ í•¨ ë‹¨ì  ì—°ê²°ì„ ìœ„í•œ ë³„ë„ ë°ì´í„° ê³µê°„ì´ í•„ìš”í•˜ë¯€ë¡œ, ì €ì¥ê³µê°„ íš¨ìœ¨ì´ ë†’ì§€ ì•ŠìŒ ì—°ê²° ì •ë³´ë¥¼ ì°¾ëŠ” ì‹œê°„ì´ í•„ìš”í•˜ë¯€ë¡œ ì ‘ê·¼ ì†ë„ê°€ ëŠë¦¼ ê·¸ì— ë°˜í•´ ë°°ì—´ì€ ì¸ë±ì‹±ì„ í†µí•´ ë¹ ë¥´ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŒ ì¤‘ê°„ ë°ì´í„° ì‚­ì œì‹œ, ì•ë’¤ ë°ì´í„°ì˜ ì—°ê²°ì„ ì¬êµ¬ì„±í•´ì•¼ í•˜ëŠ” ë¶€ê°€ì ì¸ ì‘ì—… í•„ìš” 4. ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ì˜ ë³µì¡í•œ ê¸°ëŠ¥1 (ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ ë°ì´í„° ì‚¬ì´ì— ë°ì´í„°ë¥¼ ì¶”ê°€) ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ëŠ” ìœ ì§€ ê´€ë¦¬ì— ë¶€ê°€ì ì¸ êµ¬í˜„ì´ í•„ìš”í•¨ (ì¶œì²˜: wikipedia, https://en.wikipedia.org/wiki/Linked_list) ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ì˜ ì‚¬ì´ì— ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ê¸° 12345678910class Node: def __init__(self, data, next=None): self.data = data self.next = nextdef add(data): node = head while node.next: node = node.next node.next = Node(data) 1234node1 = Node(1)head = node1for index in range(2, 10): add(index) ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ ë°ì´í„° ì¶œë ¥í•˜ê¸°(ê²€ìƒ‰í•˜ê¸°)12345node = headwhile node.next: print(node.data) node = node.nextprint (node.data) ê²°ê³¼123456789123456789 node3ì´ ë“¤ì–´ê°ˆ ìë¦¬ë¥¼ ì•„ë˜ì™€ ê°™ì´ 1ì˜ ë°ì´í„°ê°’ì„ ê°–ëŠ” ë…¸ë“œ ë‹¤ìŒì— ìœ„ì¹˜ì‹œí‚¤ë ¤ë©´ 1ì˜ ê°’ì„ ê°–ëŠ” ë…¸ë“œì˜ nextê°’ì„ ë¨¼ì € ì €ì¥í•´ì¤€ë’¤, ë‹¤ìŒì— node3ì„ ì—°ê²°í•´ì¤€ë‹¤. ê·¸ë¦¬ê³  ë‹¤ì‹œ node3ì˜ nextê°€ ì´ì „ 1ì˜ ê°’ì„ ê°–ëŠ” nodeì˜ nextë¥¼ ê°€ë¦¬í‚¤ê²Œ í•˜ë©´ëœë‹¤.12345678910111213node3 = Node(1.5)node = headsearch = Truewhile search: if node.data == 1: search = False else: node = node.nextnode_next = node.nextnode.next = node3node3.next = node_next ì´ì œ ì›í•˜ëŠ”ëŒ€ë¡œ nodeê°€ ì‚½ì… ë˜ì—ˆëŠ”ì§€ ì‚´í´ë³´ì. 12345node = headwhile node.next: print(node.data) node = node.nextprint (node.data) ê²°ê³¼1234567891011.523456789 Pseudo Code for Single Linked List ìœ„ì—ì„œ ì–¸ê¸‰í•œ ê¸°ëŠ¥ë“¤ì„ í¬í•¨í•œ Single linked Listë¥¼ ê°ì²´ë¡œ ë§Œë“¤ì–´ ë³¸ë‹¤. ë¨¼ì €, Single linked Listì— í•„ìš”í•œ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ì¸ nextë¥¼ ì°¾ìœ¼ë ¤ë©´ ì‹œì‘ì ì„ ì •í•´ì£¼ì–´ì•¼ í•  ê²ƒì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ headì™€ ë¦¬ìŠ¤íŠ¸ì˜ ì „ì²´ ì‚¬ì´ì¦ˆë¥¼ ë‚˜íƒ€ë‚´ëŠ” n_sizeë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤. ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•´ì£¼ëŠ” addì™€ í•´ë‹¹ ë°ì´í„° ê°’ì„ ê°–ê³ ìˆëŠ” ë…¸ë“œì˜ ìœ„ì¹˜ì™€ ë°ì´í„°ê°’ì„ ì¶œë ¥í•´ì£¼ëŠ” searchë¥¼ ë§Œë“ ë‹¤. addë¥¼ ë§Œë“¤ ë•Œ ì£¼ì˜í•  ì ì€headê°€ ìƒˆë¡­ê²Œ ì¶”ê°€ë˜ëŠ” ë°ì´í„°ê°€ ë˜ë„ë¡ë§Œë“¤ì–´ ì¤€ë‹¤ëŠ” ê²ƒì„ ìƒê°í•´ì£¼ì–´ì•¼ í•œë‹¤. nextë¡œ ì—°ê²°í•´ì£¼ëŠ” ìˆœì„œë¥¼ í†µí•´ ë§Œë“¤ì–´ì¤€ë‹¤. searchë¥¼ ë§Œë“¤ë•Œ ì£¼ì˜ì ì€ whileë¬¸ì„ ëŒë©´ì„œ head ë¶€í„° ì‹œì‘í•˜ì—¬ ì°¨ë¡€ëŒ€ë¡œ ì°¾ëŠ”ê°’ê³¼ ë¹„êµí•´ ì£¼ëŠ”ë° ì°¾ëŠ”ê°’ì´ í•´ë‹¹ Linked listì— ê¼­ ìˆë‹¤ëŠ” ë³´ì¥ì´ ì—†ê¸° ë•Œë¬¸ì— data.next = None (ë§ˆì§€ë§‰ ë…¸ë“œ)ì¸ ê²½ìš°ì—ëŠ” cur.dataê°€ ì—†ì–´ errorë¥¼ ë°œìƒì‹œí‚¤ê²Œ ëœë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ìš°ì„  curì˜ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ”ì§€ ë¶€í„° ìƒê°í•˜ì. ë˜í•œ, Node ìì²´ë¥¼ ë°˜í™˜í•´ì£¼ëŠ” ì‹ìœ¼ë¡œ ì‘ì„±í•´ì•¼ í•  ê²ƒì´ë‹¤. Linked listëŠ” ë…¸ë“œì˜ ì—°ê²°ì²´ì´ê¸° ë•Œë¬¸ì´ë‹¤. deleteí•¨ìˆ˜ëŠ” pythonì˜ garbage collectionì„ í†µí•´ headì˜€ë˜ nodeì˜ ì—°ê²°ì„ ëŠì–´ ì¤Œìœ¼ë¡œì¨ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. traverseí•¨ìˆ˜ëŠ” ì œë„ˆë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ì ì¸ ë…¸ë“œê°’ì„ ë³´ì—¬ì£¼ëŠ” ì œë„ˆë ˆì´í„°ëŠ” ì œë„ˆë ˆì´í„° ê°ì²´ì—ì„œ next ë©”ì„œë“œë¥¼ í˜¸ì¶œí•  ë•Œë§ˆë‹¤ í•¨ìˆ˜ ì•ˆì˜ yieldê¹Œì§€ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©° yieldì—ì„œ ê°’ì„ ë°œìƒì‹œí‚¨ë‹¤.(generate) yieldë¥¼ ì‚¬ìš©í•˜ë©´ ê°’ì„ í•¨ìˆ˜ ë°”ê¹¥ìœ¼ë¡œ ì „ë‹¬í•˜ë©´ì„œ ì½”ë“œ ì‹¤í–‰ì„ í•¨ìˆ˜ ë°”ê¹¥ì— ì–‘ë³´í•˜ê²Œëœë‹¤. ë”°ë¼ì„œ yieldëŠ” í˜„ì¬ í•¨ìˆ˜ë¥¼ ì ì‹œ ì¤‘ë‹¨í•˜ê³  í•¨ìˆ˜ ë°”ê¹¥ì˜ ì½”ë“œê°€ ì‹¤í–‰ë˜ë„ë¡ ë§Œë“ ë‹¤. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394class Node: def __init__(self, data): self.__data=data self.__next=None @property def data(self): return self.__data @data.setter def data(self, data): self.__data=data @property def next(self): return self.__next @next.setter def next(self, n): self.__next=nclass Single_Linked_List: def __init__(self): self.head=None self.n_size=0 def empty(self): if self.n_size==0: return True else : return False def size(self): return self.n_size def add(self, data): node=Node(data) if self.head==None: self.head=node self.n_size+=1 else: cur=self.head while cur: if cur.next == None: cur.next=node self.n_size += 1 cur=node.next else: cur = cur.next def add_after(self, data, target): node=Node(data) cur=self.head while cur: if cur.data == target: cur_next=cur.next cur.next=node node.next=cur_next break else: cur=cur.next def search(self, data): \"\"\" ì´ í•¨ìˆ˜ëŠ” ë¦¬ìŠ¤íŠ¸ì˜ search ëª…ë ¹ê³¼ ê°™ì´ ë™ì¼í•œ ê°’ì„ ê°–ëŠ” ì²˜ìŒ ë§Œë‚˜ëŠ” ë…¸ë“œë¥¼ ë°˜í™˜í•œë‹¤. \"\"\" cur=self.head while cur: if cur.data == data: return cur else: cur=cur.next def delete(self, data): \"\"\" ì´ í•¨ìˆ˜ëŠ” pythonì˜ garbage collectionì˜ ê°œë…ì„ í†µí•´ ì—°ê²°ì„ ëŠì–´ ì œê±°í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•˜ì˜€ë‹¤. \"\"\" cur = self.head while cur: if cur.data == data: previous.next =cur.next break previous = cur cur = cur.next self.n_size -= 1 def traverse(self): cur=self.head while cur: yield cur cur=cur.next print the list 123456def show_list(slist): print('data size : &#123;&#125;'.format(slist.size())) g=slist.traverse() for node in g: print(node.data, end= ' ') print() ë¦¬ìŠ¤íŠ¸ì— ì‚½ì… ë° ë°ì´í„° í™•ì¸ 1234567891011slist=Single_Linked_List()print('ë°ì´í„° ì‚½ì…')slist.add(3)slist.add(1)slist.add(5)slist.add(2)slist.add(7)slist.add(8)slist.add(3)show_list(slist) ê²°ê³¼ 123ë°ì´í„° ì‚½ì…data size : 73 1 5 2 7 8 3 ë¦¬ìŠ¤íŠ¸ ì¤‘ê°„ì— ì‚½ì… 12slist.add_after(7, 5)show_list(slist) ê²°ê³¼ 12data size : 73 1 5 7 2 7 8 3 search 123456789print('ë°ì´í„° íƒìƒ‰')target=7res=slist.search(target)if res: print('ë°ì´í„° &#123;&#125; ê²€ìƒ‰ ì„±ê³µ'.format(res.data))else: print('ë°ì´í„° &#123;&#125; íƒìƒ‰ ì‹¤íŒ¨'.format(target))res=Noneprint() ê²°ê³¼ 12ë°ì´í„° íƒìƒ‰ë°ì´í„° 7 ê²€ìƒ‰ ì„±ê³µ delete 12345print('ë°ì´í„° ì‚­ì œ')slist.delete(5)slist.delete(7)slist.delete(8)show_list(slist) ê²°ê³¼ 123ë°ì´í„° ì‚­ì œdata size : 43 1 2 7 3 7. ë‹¤ì–‘í•œ ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ë”ë¸” ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸(Doubly linked list) ê¸°ë³¸ êµ¬ì¡° ì´ì¤‘ ì—°ê²° ë¦¬ìŠ¤íŠ¸ë¼ê³ ë„ í•¨ ì¥ì : ì–‘ë°©í–¥ìœ¼ë¡œ ì—°ê²°ë˜ì–´ ìˆì–´ì„œ ë…¸ë“œ íƒìƒ‰ì´ ì–‘ìª½ìœ¼ë¡œ ëª¨ë‘ ê°€ëŠ¥(ì¶œì²˜: wikipedia, https://en.wikipedia.org/wiki/Linked_list) ìœ„ì—ì„œ ì–¸ê¸‰í•œ ê²ƒê³¼ ê°™ì´ ì–‘ë°©í–¥ì—ì„œ ëª¨ë‘ íƒìƒ‰ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì„ ì£¼ì˜í•˜ì. ì–‘ë°©í–¥ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„  ìœ„ì—ì„œ ì •ì˜í–ˆë˜ ë…¸ë“œì˜ ë°©í–¥ì„ ì¶”ê°€í•˜ì—¬ ì¬ì •ì˜í•´ì£¼ì–´ì•¼í•œë‹¤. 1234567891011121314151617181920212223242526272829303132class Node: def __init__(self,data=None): self.__data = data self. __next = None self.__before = None # ì†Œë©¸ì : ê°ì²´ê°€ ë©”ëª¨ë¦¬ì—ì„œ ì‚¬ë¼ì§ˆ ë•Œ ë°˜ë“œì‹œ í•œë²ˆ í˜¸ì¶œí•˜ëŠ” ê²ƒì„ ë³´ì¥ def __del__(self): print('&#123;&#125; is deleted'.format(self.__data)) @property def data(self): return self.__data @data.setter def data(self, data): self.__data = data @property def next(self): return self.__next @next.setter def next(self, next): self.__next = next @property def before(self): return self.__before @before.setter def before(self, before): self.__before = before Pseudo Code for Single Linked List ê°€ì¥ ë¨¼ì € single linked listì™€ ë‹¤ë¥´ê²Œ headë§Œ ì¡´ì¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ tailë„ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì„ ìœ ì˜í•˜ì. í•„ìš”í•œ ê¸°ëŠ¥ì„ ë‚˜ì—´í•´ë³´ì. ëŒ€ëµì ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì„œ ìƒê°í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ë“¤ì´ í•„ìš”í•  ê²ƒì´ë‹¤. size empty add head after tail before mid search head -&gt; tail tail -&gt; head delete head tail mid insert_after, insert_beforeëŠ” ifë¬¸ì˜ ìˆœì„œë¥¼ ë°”ê¾¸ì–´ ì£¼ì–´ ë§í¬ë“œë¦¬ìŠ¤íŠ¸ë‚´ì— ë…¸ë“œê°€ 1ê°œì§œë¦¬ì¸ ê²½ìš°ì—ëŠ” ê°ê° add_firstì™€ add_lastë¥¼ í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•˜ë¯€ë¡œ í•¨ìˆ˜ë¡œ ê³„ì‚°í•˜ëŠ” ê²ƒìœ¼ë¡œ ëŒ€ì²´í•´ì£¼ì—ˆë‹¤. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142class DoubleLinkedList: def __init__(self): self.head = None self.tail = None self.d_size = 0 def empty(self): if self.d_size==0: return True else: return False def size(self): return self.d_size def add_first(self, data): if self.empty(): new=Node(data) self.head=new self.tail=new self.d_size+=1 else: new=Node(data) new.next=self.head self.head.before=new self.head=new self.d_size+=1 def add_last(self, data): if self.empty(): new=Node(data) self.head=new self.tail=new self.d_size+=1 else: new=Node(data) self.tail.next=new new.before=self.tail self.tail=new self.d_size+=1 def insert_after(self, data, node): if node == self.tail: self.add_last(data) elif node == self.head: new=Node(data) new.next=self.head.next new.before=self.head self.head.next.before=new self.head.next=new self.d_size+=1 else: new=Node(data) new.next=node.next new.before=node node.next.before=new node.next=new self.d_size+=1 def insert_before(self, data, node): if node == self.head: print(\"this\") self.add_first(data) elif node == self.tail: print(\"is\") new=Node(data) new.next=self.tail new.before=self.tail.before self.tail.before.next=new self.tail.before=new self.d_size+=1 else: print(\"shit\") new=Node(data) new.next=node new.before=node.before node.before.next=new node.before=new self.d_size+=1 def search_forward(self, data): cur=self.head while cur: if cur.data==data: return cur else: cur=cur.next if cur == None: print(\"ë¦¬ìŠ¤íŠ¸ ì•ˆì— ì°¾ëŠ” ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\") def search_backward(self, data): cur=self.tail while cur: if cur.data == data: return cur else: cur=cur.before if cur == None: print(\"ë¦¬ìŠ¤íŠ¸ ì•ˆì— ì°¾ëŠ” ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\") def delete_first(self): new_head=self.head.next self.head.next=None self.head=new_head new_head.before=None self.d_size-=1 def delete_last(self): new_tail=self.tail.before self.tail.before=None self.tail=new_tail self.tail.next=None self.d_size-=1 def delete_node(self, node): if node == self.head: self.delete_first() elif node == self.tail: self.delete_last() else: node.before.next=node.next node.next.before=node.before def traverse(self, start=True): \"\"\" start=True --&gt; from head start=False --&gt; from tail \"\"\" if start: cur=self.head while cur: yield cur cur=cur.next else: cur=self.tail while cur: yield cur cur=cur.before print the linked list 123456def show_list(dlist, start=True): print('data size : &#123;&#125;'.format(dlist.size())) g=dlist.traverse(start) for node in g: print(node.data, end=' ') print() make a double linked list 1dlist=DoubleLinkedList() insert 1 - add_first 123456print('ë°ì´í„° ì‚½ì… -add_first')dlist.add_first(1)dlist.add_first(2)dlist.add_first(3)dlist.add_first(5)show_list(dlist, start=True) ê²°ê³¼ 123ë°ì´í„° ì‚½ì… -add_firstdata size : 45 3 2 1 insert 2 - add_last 123456print('ë°ì´í„° ì‚½ì… -add_last')dlist.add_last(1)dlist.add_last(2)dlist.add_last(3)dlist.add_last(5)show_list(dlist, start=True) ê²°ê³¼ 123ë°ì´í„° ì‚½ì… -add_lastdata size : 85 3 2 1 1 2 3 5 insert 3 - insert_after 123print('ë°ì´í„° ì‚½ì… - insert_after')dlist.insert_after(4, dlist.search_forward(5))show_list(dlist, start=True) ê²°ê³¼ 123ë°ì´í„° ì‚½ì… - insert_afterdata size : 95 4 3 2 1 1 2 3 5 insert 4 - insert_before 123print('ë°ì´í„° ì‚½ì… - insert_before')dlist.insert_before(4, node=dlist.search_forward(5))show_list(dlist, start=True) ê²°ê³¼ 1234ë°ì´í„° ì‚½ì… - insert_beforethisdata size : 104 5 4 3 2 1 1 2 3 5 search 123456789print('ë°ì´í„° íƒìƒ‰')target=2#res=dlist.search_forward(target)res=dlist.search_backward(target)if res: print('ë°ì´í„° &#123;&#125; íƒìƒ‰ ì„±ê³µ'.format(res.data))else: print('ë°ì´í„° &#123;&#125; íƒìƒ‰ ì‹¤íŒ¨'.format(target))res=None ê²°ê³¼ 12ë°ì´í„° íƒìƒ‰ë°ì´í„° 2 íƒìƒ‰ ì„±ê³µ delete_first 123dlist.delete_first()dlist.delete_first()show_list(dlist, start=True) ê²°ê³¼ 12344 is deleted5 is deleteddata size : 84 3 2 1 1 2 3 5 delete_last 123dlist.delete_last()dlist.delete_last()show_list(dlist, start=True) ê²°ê³¼ 12345 is deleted3 is deleteddata size : 64 3 2 1 1 2 delete_node 12dlist.delete_node(dlist.search_backward(1))show_list(dlist, start=True) ê²°ê³¼ 1231 is deleteddata size : 64 3 2 1 2 ìœ„ì˜ í…ŒìŠ¤íŠ¸ì™€ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ë‚˜ headì™€ tailì„ Nodeë¡œ ë§Œë“¤ì–´ headì™€ tailì— ê´€í•´ ì¢€ë” ì§ê´€ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆê³ , Node ì¶”ê°€ì‹œ ì¡°ê¸ˆì˜ ì´ì ì´ ë” ìˆê²Œë” êµ¬í˜„í•´ë³¸ ë˜ ë‹¤ë¥¸ ì½”ë“œì´ë‹¤. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118class DoubleLinkedList: def __init__(self): self.head = Node() self.tail = Node() self.head.next = self.tail self.tail.before = self.head self.d_size = 0 def empty(self): if self.d_size==0: return True else : return False def size(self): return self.d_size def add_first(self, data): #ìƒˆ ë…¸ë“œë¥¼ ìƒì„± new_node = Node(data) new_node.next = self.head.next new_node.before = self.head #ìœ„ì˜ ë‘ê°€ì§€ëŠ” ìˆœì„œê°€ ë°”ë€Œì–´ë„ ë¨! self.head.next.before = new_node self.head.next = new_node self.d_size+=1 def add_last(self, data): new_node = Node(data) new_node.next = self.tail new_node.before = self.tail.before self.tail.before.next = new_node self.tail.before = new_node self.d_size+=1 def insert_after(self, data, node): new_node = Node(data) new_node.next = node.next new_node.before = node #new_node.before = node.next.before node.next.before = new_node node.next = new_node self.d_size+=1 def insert_before(self, data, node): new_node = Node(data) new_node.next = node new_node.before = node.before node.before.next = new_node node.before = new_node self.d_size+=1 def search_forward(self, data): current = self.head.next while(current is not self.tail): if(current.data == data): return current else: current = current.next return None def search_backward(self, data): current = self.tail.before while(current is not self.head): # ì£¼ì†Œê°’ì„ ë¹„êµí•˜ê¸° ë•Œë¬¸ì— !=ì´ ì•„ë‹Œ is notìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ ë” ì¢‹ìŒ!! if(current.data == data): return current else: current = current.before return None def delete_first(self): if self.empty(): return self.head.next = self.head.next.next self.head.next.before = self.head self.d_size-=1 def delete_last(self): if self.empty(): return self.tail.before=self.tail.before.before self.tail.before.next=self.tail self.d_size-=1 def delete_node(self, node): node.before.next=node.next #reference countê°€ 0ì´ë˜ì–´ ì‚¬ë¼ì§! node.next.before=node.before self.d_size-=1 def traverse(self, start=True): \"\"\" start=True --&gt; from head start=False --&gt; from tail \"\"\" if start: cur=self.head while cur is not self.tail: yield cur cur=cur.next else: cur=self.tail.before while cur is not self.head: yield cur cur=cur.before ìœ„ì˜ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ Pythonì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ êµ¬í˜„í•´ë³´ì•˜ë‹¤. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109from double_linked_list import DoubleLinkedListclass PseudoList(DoubleLinkedList): #posëŠ” íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì˜ ì¸ë±ìŠ¤ì™€ ë¹„ìŠ·í•˜ê²Œ #ë°ì´í„°ì˜ ìœ„ì¹˜ë¥¼ ë‚˜íƒ€ëƒ„. #ì¸ë±ìŠ¤ì²˜ëŸ¼ 0ì´ ì²«ë²ˆì§¸ ìœ„ì¹˜ë¥¼ ì˜ë¯¸ def __init__(self, *args): super().__init__() for elem in args: self.add_last(elem) #ì „ì—­í•¨ìˆ˜ len(list)ì„ í˜¸ì¶œí•  ë•Œ ì´ í•¨ìˆ˜ê°€ í˜¸ì¶œ def __len__(self): return self.size() #íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì˜ appendëŠ” ë§¨ ë’¤ì— ë°ì´í„°ë¥¼ ì¶”ê°€ #ë”ë¸” ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” add_last í•¨ìˆ˜ë¥¼ ì‚¬ìš© def append(self, data): #ë˜í•‘ í•¨ìˆ˜ self.add_last(data) #ì¸ìë¡œ posë¥¼ ë°›ìœ¼ë©´ posì— ìœ„ì¹˜í•œ ë…¸ë“œë¥¼ ë°˜í™˜í•œë‹¤. def __find_position(self, pos): if pos &gt;=self.size(): raise IndexError('list index out of range') cur = self.head.next for _ in range(pos): cur = cur.next return cur #posì— ìœ„ì¹˜í•œ ë…¸ë“œë¥¼ êµ¬í•œ ë’¤ --&gt; __find_position() #ê·¸ ë…¸ë“œì˜ ì•ì— ë°ì´í„°ë¥¼ ì‚½ì… --&gt; insert_before() def insert(self, pos, data): node = self.__find_position(pos) self.insert_before(data, node) #ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” dataì˜ ê°œìˆ˜ë¥¼ ì¹´ìš´íŠ¸ #ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©´ì„œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ cnt ë³€ìˆ˜ë¥¼ 1ì”© ì¦ê°€ì‹œí‚´ def count(self, data): cnt = 0 cur = self.head.next while cur is not self.tail: if cur.data == data: cnt += 1 cur = cur.next return cnt #ì¸ì dataê°€ ìœ„ì¹˜í•œ ì¸ë±ìŠ¤ë¥¼(ì—¬ê¸°ì—ì„œëŠ” pos) ë°˜í™˜í•œë‹¤ #startëŠ” ë°ì´í„°ë¥¼ ì°¾ê¸° ì‹œì‘í•˜ëŠ” ìœ„ì¹˜ def index(self, data, start=0): cur = self.__find_position(start) index = start while cur: if cur.data == data: return index cur = cur.next index += 1 raise ValueError('&#123;&#125; is not in the list'.format(data)) #print(li[3]) ì²˜ëŸ¼ [] ì—°ì‚°ìë¥¼ í†µí•´ ê°’ì„ ê°€ì ¸ì˜¬ ë•Œ ë‚´ë¶€ì—ì„œ í˜¸ì¶œ #indexëŠ” posë¥¼ ì˜ë¯¸ #indexì— ìœ„ì¹˜í•œ ë…¸ë“œì˜ ë°ì´í„°ë¥¼ ë°˜í™˜ def __getitem__(self, index):#ì—°ì‚°ì ì˜¤ë²„ë¡œë”© node = self.__find_position(index) return node.data #li[3]=10 ì²˜ëŸ¼ [] ì—°ì‚°ìë¡œ ê°’ì„ ëŒ€ì…í•  ë•Œ ë‚´ë¶€ì—ì„œ í˜¸ì¶œ #indexëŠ” posë¥¼ ì˜ë¯¸ #indexì— ìœ„ì¹˜í•œ ë…¸ë“œì˜ ê°’ì„ dataë¡œ ë°”ê¿ˆ def __setitem__(self, index, data):#ì—°ì‚°ì ì˜¤ë²„ë¡œë”© node = self.__find_position(index) node.data = data #posì— ìˆëŠ” ë°ì´í„°ë¥¼ ì‚­ì œí•˜ë©´ì„œ ë°˜í™˜ #íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ì¸ìë¥¼ ì£¼ì§€ ì•Šìœ¼ë©´ #ë¦¬ìŠ¤íŠ¸ì˜ ë§¨ ë§ˆì§€ë§‰ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ë©´ì„œ ë°˜í™˜ def pop(self, pos=None): #ì¸ì posê°€ ìˆëŠ” ê²½ìš° if pos: node = self.__find_position(pos) #posê°€ ë¹„ì–´ ìˆëŠ” ê²½ìš° else: node = self.tail.before #delete_node í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ë…¸ë“œ ì‚­ì œ cur = node self.delete_node(node) return cur.data #ë¦¬ìŠ¤íŠ¸ì— ìˆëŠ” ë°ì´í„°ë¥¼ ì‚­ì œ #ë§Œì•½ ê°™ì€ ë°ì´í„°ê°€ ì—¬ëŸ¬ê°œë¼ë©´ #ë¦¬ìŠ¤íŠ¸ì—ì„œ ìœ„ì¹˜ìƒ ì²«ë²ˆì§¸ ë°ì´í„°ê°€ ì‚­ì œëœë‹¤ #ë°˜í™˜ì€ í•˜ì§€ ì•ŠëŠ”ë‹¤ def remove(self, data): #delete_nodeì™€ search_forward í•¨ìˆ˜ ì´ìš© self.delete_node(self.search_forward(data)) def __str__(self): string = '[' cur = self.head.next while cur is not self.tail: string+=str(cur.data) if cur.next is not self.tail: string+=', ' cur= cur.next string+=']' return string ê°ì²´ ìƒì„± 12345678initial = [1, 2, 3, 4]#pseudo_listli = PseudoList(*initial)#python listpy_li = initialprint(\"pseudo list : \"+ str(li))print(\"python list : \" + str(py_li)) ê²°ê³¼ 12pseudo list : [1, 2, 3, 4]python list : [1, 2, 3, 4] append 12345678910111213#pseudo_listli.append(2)li.append(1)li.append(2)li.append(7)#python_listpy_li.append(2)py_li.append(1)py_li.append(2)py_li.append(7)print(\"pseudo list : \"+ str(li))print(\"python list : \" + str(py_li)) ê²°ê³¼ 12pseudo list : [1, 2, 3, 4, 2, 1, 2, 7]python list : [1, 2, 3, 4, 2, 1, 2, 7] count 123target = 2print('count of &#123;&#125; : &#123;&#125; in pseudo_list'.format(target, li.count(target)))print('count of &#123;&#125; : &#123;&#125; in python_list'.format(target, py_li.count(target))) ê²°ê³¼ 12count of 2 : 3 in pseudo_listcount of 2 : 3 in python_list pop 1234567#pseudo_listli.pop(2)#python_listpy_li.pop(2)print(\"pseudo list : \"+ str(li))print(\"python list : \" + str(py_li)) ê²°ê³¼ 1234data of 7 is deleteddata of 2 is deletedpseudo list : [1, 2, 3, 4, 2, 1]python list : [1, 2, 3, 4, 2, 1] pop(index) 1234567#pseudo_listli.pop(2)#python_listpy_li.pop(2)print(\"pseudo list : \"+ str(li))print(\"python list : \" + str(py_li)) ê²°ê³¼ 123data of 3 is deletedpseudo list : [1, 2, 4, 2, 1]python list : [1, 2, 4, 2, 1] insert 1234567#pseudo_listli.insert(3, 9)#python_listpy_li.insert(3, 9)print(\"pseudo list : \"+ str(li))print(\"python list : \" + str(py_li)) ê²°ê³¼ 12pseudo list : [1, 2, 4, 9, 2, 1]python list : [1, 2, 4, 9, 2, 1] index 1234target = 9print(\"index of &#123;&#125; : &#123;&#125; in pseudo_list\".format(target, li.index(target)))print(\"index of &#123;&#125; : &#123;&#125; in python_list\".format(target, py_li.index(target))) ê²°ê³¼ 12index of 9 : 3 in pseudo_listindex of 9 : 3 in python_list indexing 1234567#pseudo_listli[3]=7#python_listpy_li[3]=7print(\"pseudo list : \"+ str(li))print(\"python list : \" + str(py_li)) ê²°ê³¼ 12pseudo list : [1, 2, 4, 7, 2, 1]python list : [1, 2, 4, 7, 2, 1] remove 1234567#pseudo_listli.remove(9)#python_listpy_li.remove(9)print(\"pseudo list : \"+ str(li))print(\"python list : \" + str(py_li)) ê²°ê³¼ 123data of 9 is deletedpseudo list : [1, 2, 4, 2, 1]python list : [1, 2, 4, 2, 1]","categories":[{"name":"C/C++/ìë£Œêµ¬ì¡°","slug":"C-C-ìë£Œêµ¬ì¡°","permalink":"https://heung-bae-lee.github.io/categories/C-C-ìë£Œêµ¬ì¡°/"}],"tags":[]},{"title":"ë‚´ê°€ ì •ë¦¬í•˜ëŠ” ìë£Œêµ¬ì¡° 01 Stack","slug":"data_structure_02","date":"2020-04-27T12:38:39.000Z","updated":"2020-04-27T19:41:16.324Z","comments":true,"path":"2020/04/27/data_structure_02/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/27/data_structure_02/","excerpt":"","text":"Stackê¼­ ì•Œì•„ë‘¬ì•¼ í•  ìë£Œ êµ¬ì¡°: ìŠ¤íƒ (Stack) ë°ì´í„°ë¥¼ ì œí•œì ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” êµ¬ì¡° í•œìª½ ëì—ì„œë§Œ ìë£Œë¥¼ ë„£ê±°ë‚˜ ëº„ ìˆ˜ ìˆëŠ” êµ¬ì¡° ê°€ì¥ ë‚˜ì¤‘ì— ìŒ“ì€ ë°ì´í„°ë¥¼ ê°€ì¥ ë¨¼ì € ë¹¼ë‚¼ ìˆ˜ ìˆëŠ” ë°ì´í„° êµ¬ì¡° í: FIFO ì •ì±… -&gt; ì¤„ ì„¸ìš°ê¸° ìŠ¤íƒ: LIFO ì •ì±… -&gt; ì±… ìŒ“ê¸° 1. ìŠ¤íƒ êµ¬ì¡° ìŠ¤íƒì€ LIFO(Last In, Fisrt Out) ë˜ëŠ” FILO(First In, Last Out) ë°ì´í„° ê´€ë¦¬ ë°©ì‹ì„ ë”°ë¦„ LIFO: ë§ˆì§€ë§‰ì— ë„£ì€ ë°ì´í„°ë¥¼ ê°€ì¥ ë¨¼ì € ì¶”ì¶œí•˜ëŠ” ë°ì´í„° ê´€ë¦¬ ì •ì±… FILO: ì²˜ìŒì— ë„£ì€ ë°ì´í„°ë¥¼ ê°€ì¥ ë§ˆì§€ë§‰ì— ì¶”ì¶œí•˜ëŠ” ë°ì´í„° ê´€ë¦¬ ì •ì±… ì°¸ê³ ë¡œ QueueëŠ” FIFOë¼ê³  ë§ì´ ì–˜ê¸°í•˜ì§€ë§Œ, Stackì€ ê·¸ëƒ¥ Stackì´ë¼ê³  í•œë‹¤. ëŒ€í‘œì ì¸ ìŠ¤íƒì˜ í™œìš© ì»´í“¨í„° ë‚´ë¶€ì˜ í”„ë¡œì„¸ìŠ¤ êµ¬ì¡°ì˜ í•¨ìˆ˜ ë™ì‘ ë°©ì‹ ì£¼ìš” ê¸°ëŠ¥ push(): ë°ì´í„°ë¥¼ ìŠ¤íƒì— ë„£ê¸° pop(): ë°ì´í„°ë¥¼ ìŠ¤íƒì—ì„œ êº¼ë‚´ê¸° Visualgo ì‚¬ì´íŠ¸ì—ì„œ ì‹œì—°í•´ë³´ë©° ì´í•´í•˜ê¸° (push/pop ë§Œ í´ë¦­í•´ë³´ë©°): https://visualgo.net/en/list ê·¸ë¦¼ìœ¼ë¡œ ì´í•´í•´ë³´ê¸° 2. ìŠ¤íƒ êµ¬ì¡°ì™€ í”„ë¡œì„¸ìŠ¤ ìŠ¤íƒ ìŠ¤íƒ êµ¬ì¡°ëŠ” í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ êµ¬ì¡°ì˜ ê°€ì¥ ê¸°ë³¸ í•¨ìˆ˜ í˜¸ì¶œì‹œ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ êµ¬ì¡°ë¥¼ ìŠ¤íƒê³¼ ë¹„êµí•´ì„œ ì´í•´ í•„ìš” stackì´ë‘ queueëŠ” ì¼ì‹œì ìœ¼ë¡œ ìë£Œë¥¼ ë³´ê´€í• ë•Œ ì‚¬ìš©!!! ì¬ê·€ í•¨ìˆ˜1234567def recursive(data): if data &lt; 0: print (\"ended\") else: print(data) recursive(data - 1) print(\"returned\", data) ì‹¤í–‰1recursive(4) ê²°ê³¼123456789101143210endedreturned 0returned 1returned 2returned 3returned 4 Processì—ì„œ í•¨ìˆ˜ê°€ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ ê·¸ë¦¬ê³  ê·¸ê²ƒì´ ì–´ë–»ê²Œ Stackì´ë¼ëŠ” ìë£Œêµ¬ì¡°ì™€ ì—°ê²°ì´ ë˜ëŠ”ì§€ ì¤‘ì ì ìœ¼ë¡œ ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. Programì´ ì‹¤í–‰ë˜ëŠ” ìƒíƒœë¥¼ Processë¼ê³  í•˜ëŠ”ë° ê·¸ Processì•ˆì—ì„œ í•¨ìˆ˜ê°€ í˜¸ì¶œì´ ëœ ê²ƒì´ë¯€ë¡œ Process Stackì— ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ìŒ“ì´ê²Œ ëœë‹¤. recursive í•¨ìˆ˜ê°€ ìµœì¢…ì ìœ¼ë¡œ ëë‚˜ê²Œ ë˜ë©´, Stack êµ¬ì¡°ì™€ ê°™ì´ ì œì¼ ë§ˆì§ë§‰ ì‹¤í–‰ëœ í•¨ìˆ˜ë¶€í„° ì‹¤í–‰ì„ ë§ˆì¹˜ê²Œ ë˜ì–´ ìœ„ì™€ ê°™ì€ ê²°ê³¼ê°€ ì¶œë ¥ì´ ë˜ëŠ” ê²ƒì´ë‹¤. 3. ìë£Œ êµ¬ì¡° ìŠ¤íƒì˜ ì¥ë‹¨ì  ì¥ì  êµ¬ì¡°ê°€ ë‹¨ìˆœí•´ì„œ, êµ¬í˜„ì´ ì‰½ë‹¤. ë°ì´í„° ì €ì¥/ì½ê¸° ì†ë„ê°€ ë¹ ë¥´ë‹¤. ë‹¨ì  (ì¼ë°˜ì ì¸ ìŠ¤íƒ êµ¬í˜„ì‹œ) ë°ì´í„° ìµœëŒ€ ê°¯ìˆ˜ë¥¼ ë¯¸ë¦¬ ì •í•´ì•¼ í•œë‹¤. íŒŒì´ì¬ì˜ ê²½ìš° ì¬ê·€ í•¨ìˆ˜ëŠ” 1000ë²ˆê¹Œì§€ë§Œ í˜¸ì¶œì´ ê°€ëŠ¥í•¨ ì €ì¥ ê³µê°„ì˜ ë‚­ë¹„ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ ë¯¸ë¦¬ ìµœëŒ€ ê°¯ìˆ˜ë§Œí¼ ì €ì¥ ê³µê°„ì„ í™•ë³´í•´ì•¼ í•¨ ìŠ¤íƒì€ ë‹¨ìˆœí•˜ê³  ë¹ ë¥¸ ì„±ëŠ¥ì„ ìœ„í•´ ì‚¬ìš©ë˜ë¯€ë¡œ, ë³´í†µ ë°°ì—´ êµ¬ì¡°ë¥¼ í™œìš©í•´ì„œ êµ¬í˜„í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì„.ì´ ê²½ìš°, ìœ„ì—ì„œ ì—´ê±°í•œ ë‹¨ì ì´ ìˆì„ ìˆ˜ ìˆìŒ 4. íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ ê¸°ëŠ¥ì—ì„œ ì œê³µí•˜ëŠ” ë©”ì„œë“œë¡œ ìŠ¤íƒ ì‚¬ìš©í•´ë³´ê¸° append(push), pop ë©”ì„œë“œ ì œê³µ 1234data_stack = list()data_stack.append(1)data_stack.append(2) 1data_stack ê²°ê³¼1[1, 2] 1data_stack.pop() ê²°ê³¼12 5. í”„ë¡œê·¸ë˜ë° ì—°ìŠµ ì—°ìŠµ1: ë¦¬ìŠ¤íŠ¸ ë³€ìˆ˜ë¡œ ìŠ¤íƒì„ ë‹¤ë£¨ëŠ” pop, push ê¸°ëŠ¥ êµ¬í˜„í•´ë³´ê¸° (pop, push í•¨ìˆ˜ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì§ì ‘ êµ¬í˜„í•´ë³´ê¸°) 123456789stack_list = list()def push(data): stack_list.append(data)def pop(): data = stack_list[-1] del stack_list[-1] return data 12for index in range(10): push(index) 1pop() ê²°ê³¼19 ë˜ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” Stack êµ¬ì¡°12345678910111213141516171819# ë§ì€ ì–¸ì–´ë“¤ì´ ì‹¤ì œ êµ¬í˜„ë˜ì–´ ìˆëŠ” ê²ƒì„ ë³´ë©´ ì‹¤ì œ ìë£ŒëŠ” containerë¼ëŠ” stackì•ˆì˜ ê³µê°„ì— ìˆê³ , stackì€ ì¸í„°í˜ì´ìŠ¤ë§Œ ì œê³µclass Stack: def __init__(self): # ì‹¤ì œ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆëŠ” ìë£Œêµ¬ì¡° self.container=list() # ë¹ˆ list ê°ì²´ê°€ ë§Œë“¤ì–´ì ¸ì„œ containerì— ì €ì¥. def empty(self): if not self.container: # ë¹„ì–´ìˆìœ¼ë©´ trueê°€ë˜ë‹ˆê¹Œ return True return False def push(self, data): self.container.append(data) def pop(self): return self.container.pop() def peek(self): return self.container[-1] 12345678910s=Stack()s.push(1)s.push(2)s.push(3)s.push(4)s.push(5)while not s.empty(): print(s.pop(), end=' ') ê²°ê³¼15 4 3 2 1 Nodeë¥¼ í™œìš©í•œ Stack êµ¬ì¡° êµ¬í˜„í•˜ê¸°1234567891011121314151617181920class Node: def __init__(self, data=None): self.__data=data self.__next=None @property def data(self): return self.__data @data.setter def data(self, data): self.__data=data @property def next(self): return self.__next @next.setter def next(self, n): self.__next=n 123456789101112131415161718192021222324252627282930313233class LStack: def __init__(self): #ì¸ìŠ¤í„´ìŠ¤ ë©¤ë²„ self.top=None def empty(self): if self.top is None: return True return False def push(self, data): new_node = Node(data)# new_node.__data=data# if self.empty():# self.top = new_node# return new_node.next = self.top self.top = new_node def pop(self): if self.empty(): return None cur = self.top self.top = self.top.next return cur.data def peek(self): if self.empty(): return None cur = self.top return cur.data 12345678910s = LStack()s.push(1)s.push(2)s.push(3)s.push(4)s.push(5)while not s.empty(): print(s.pop(), end=\" \") ê²°ê³¼15 4 3 2 1 12345s = LStack()s.push(1)s.push(2)s.empty() ê²°ê³¼1False","categories":[{"name":"C/C++/ìë£Œêµ¬ì¡°","slug":"C-C-ìë£Œêµ¬ì¡°","permalink":"https://heung-bae-lee.github.io/categories/C-C-ìë£Œêµ¬ì¡°/"}],"tags":[]},{"title":"ì˜ì‚¬ê²°ì •ë‚˜ë¬´","slug":"machine_learning_13","date":"2020-04-25T18:27:27.000Z","updated":"2020-04-30T14:32:46.338Z","comments":true,"path":"2020/04/26/machine_learning_13/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/26/machine_learning_13/","excerpt":"","text":"Decision tree ë°°ê²½ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì˜ ì¥ì ì€ í•´ì„ë ¥ì´ ì¢‹ë‹¤. ìš°ë¦¬ê°€ ëª¨ë¸ì„ ë§Œë“¤ë•Œ ì„±ëŠ¥ì´ ì¢‹ì€ ê²ƒë„ ì¤‘ìš”í•˜ì§€ë§Œ, ì–´ë–»ê²Œ ì‚¬ëŒë“¤í•œí…Œ ë©”ì„¸ì§€ë¥¼ ì¤„ ìˆ˜ ìˆëŠ”ê°€ì²˜ëŸ¼ ì–´ë–»ê²Œ í™œìš©í•  ìˆ˜ ìˆëŠ”ê°€ê°€ ë” ì¤‘ìš”í•œ ê²½ìš°ë„ ìˆë‹¤. ì˜ˆì¸¡ë ¥ì´ ì¡°ê¸ˆ ë–¨ì–´ì§€ë”ë¼ë„ ì´ì•¼ê¸°ë¡œ í’€ì–´ì„œ ì–´ë– í•œ ê·¼ê±°ë¡œ ì¸í•´ YëŠ” ì´ë ‡ê²Œ ëœë‹¤ëŠ” ì‹ìœ¼ë¡œ í’€ì–´ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ê²°ì •íŠ¸ë¦¬ëŠ” ë§¤ìš° ì‰½ê³  ìœ ì—°í•˜ê²Œ ì ìš©ë  ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ë˜í•œ ë°ì´í„°ì˜ Scalingì´ë‚˜ ì •ê·œí™”(normalize) ë“±ì˜ ì‚¬ì „ ê°€ê³µì˜ ì˜í–¥ì´ ë§¤ìš° ì ë‹¤. í•˜ì§€ë§Œ, ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë³µì¡í•œ ê·œì¹™ êµ¬ì¡°ë¥¼ ê±°ì³ì•¼ í•˜ë©°, ì´ë¡œ ì¸í•œ ê³¼ì í•©(overfitting)ì´ ë°œìƒí•´ ë°˜ëŒ€ë¡œ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ë„ ìˆë‹¤ëŠ” ë‹¨ì ì´ìˆë‹¤. ì´ëŸ¬í•œ ë‹¨ì ì´ ì•™ìƒë¸” ê¸°ë²•ì—ì„œëŠ” ì˜¤íˆë ¤ ì¥ì ìœ¼ë¡œ ì‘ìš©í•œë‹¤. ì•™ìƒë¸”ì€ ë§¤ìš° ë§ì€ ì—¬ëŸ¬ê°œì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ìƒëŒ€ì ìœ¼ë¡œ ë–¨ì–´ì§€ëŠ” í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ê²°í•©í•´ í™•ë¥ ì  ë³´ì™„ê³¼ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ë¶€ë¶„ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ë¥¼ ê³„ì† ì—…ë°ì´íŠ¸í•˜ë©´ì„œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ”ë°, ê²°ì •íŠ¸ë¦¬ê°€ ì¢‹ì€ ì•½í•œ í•™ìŠµê¸°ê°€ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ê²°ì • íŠ¸ë¦¬(Decision Tree)ëŠ” ML ì•Œê³ ë¦¬ì¦˜ ì¤‘ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ê¸° ì‰¬ìš´ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ë°ì´í„°ì— ìˆëŠ” ê·œì¹™ì„ í•™ìŠµì„ í†µí•´ ìë™ìœ¼ë¡œ ì°¾ì•„ë‚´ëŠ” íŠ¸ë¦¬(Tree) ê¸°ë°˜ì˜ ë¶„ë¥˜ ê·œì¹™ì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ, ë°ì´í„°ì˜ ì–´ë–¤ ê¸°ì¤€ì„ ë°”íƒ•ìœ¼ë¡œ ê·œì¹™ì„ ë§Œë“¤ì–´ì•¼ ê°€ì¥ íš¨ìœ¨ì ì¸ ë¶„ë¥˜ê°€ ë  ê²ƒì¸ê°ê°€ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ì„ í¬ê²Œ ì¢Œìš°í•œë‹¤. ì˜ì‚¬ê²°ì •ë‚˜ë¬´(decision tree)ëŠ” ì—¬ëŸ¬ ê°€ì§€ ê·œì¹™ì„ ìˆœì°¨ì ìœ¼ë¡œ ì ìš©í•˜ë©´ì„œ ë…ë¦½ ë³€ìˆ˜ ê³µê°„ì„ ë¶„í• í•˜ëŠ” ë¶„ë¥˜ ëª¨í˜•ì´ë‹¤. ë¶„ë¥˜(classification)ì™€ íšŒê·€ ë¶„ì„(regression)ì— ëª¨ë‘ ì‚¬ìš©ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— CART(Classification And Regression Tree)ë¼ê³ ë„ í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ ê²°ì • íŠ¸ë¦¬ì˜ êµ¬ì¡°ë¥¼ ê°„ëµí•˜ê²Œ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤. ë°ì´í„° ì„¸íŠ¸ì— featureê°€ ìˆê³  ì´ëŸ¬í•œ featureê°€ ê²°í•©í•´ ê·œì¹™ ì¡°ê±´ì„ ë§Œë“¤ ë•Œë§ˆë‹¤ ê·œì¹™ ë…¸ë“œê°€ ë§Œë“¤ì–´ì§€ë©° ìƒˆë¡œìš´ ê·œì¹™ ì¡°ê²€ë§ˆë‹¤ ì„œë¸Œ íŠ¸ë¦¬(Sub tree)ê°€ ìƒì„±ëœë‹¤. í•˜ì§€ë§Œ ë§ì€ ê·œì¹™ì´ ìˆë‹¤ëŠ” ê²ƒì€ ê³§ ë¶„ë¥˜ë¥¼ ê²°ì •í•˜ëŠ” ë°©ì‹ì´ ë”ìš± ë³µì¡í•´ì§„ë‹¤ëŠ” ì–˜ê¸°ì´ê³ , ì´ëŠ” ê³§ ê³¼ì í•©(overfitting)ìœ¼ë¡œ ì´ì–´ì§€ê¸° ì‰½ë‹¤. ì¦‰, íŠ¸ë¦¬ì˜ ê¹Šì´(Depth)ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ê²°ì • íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì €í• ë  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§„ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ê²°ì •íŠ¸ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì¢…ì†ë³€ìˆ˜(ë°˜ì‘ ë³€ìˆ˜, target ê°’)ì˜ ìë£Œí˜•ì— ì˜í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ë¥˜ë  ìˆ˜ ìˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì´ ë¶„ë¥˜íŠ¸ë¦¬ì´ê³  ì™¼ìª½ ê·¸ë¦¼ì´ íšŒê·€ íŠ¸ë¦¬ì´ë‹¤. Entropy ê·¸ë ‡ë‹¤ë©´, ê°€ëŠ¥í•œ ì ì€ ê²°ì • ë…¸ë“œë¡œ ë†’ì€ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ê°€ì§€ë ¤ë©´ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•  ë•Œ ìµœëŒ€í•œ ë§ì€ ë°ì´í„° ì„¸íŠ¸ê°€ í•´ë‹¹ ë¶„ë¥˜ì— ì†í•  ìˆ˜ ìˆë„ë¡ ê²°ì • ë…¸ë“œì˜ ê·œì¹™ì´ ì •í•´ì ¸ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ íŠ¸ë¦¬ë¥¼ ë¶„í• (Split)í•  ê²ƒì¸ê°€ê°€ ì¤‘ìš”í•œë° ìµœëŒ€í•œ ê· ì¼í•œ ë°ì´í„° ì„¸íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆë„ë¡ ë¶„í• í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. ì—”íŠ¸ë¡œí”¼ëŠ” ì„ì—¬ìˆëŠ” ìƒíƒœë¥¼ ì˜ë¯¸í•œë‹¤ê³  ìƒê°í•˜ë©´ ì´í•´í•˜ê¸° ì‰½ë‹¤. ì„ì—¬ìˆëŠ” ìƒíƒœë©´ ì—”íŠ¸ë¡œí”¼ê°€ ë†’ì€ ê²ƒì´ê³  ë¬¼ë¦¬ì ì¸ í˜ì„ ì¨ì„œ ë¶„ë¦¬ëŠ” í•´ë†“ì€ ê²½ìš°ëŠ” ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ì€ ìƒíƒœì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ $x$ì¶•ì˜ $P+$ê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒì´ ë…¸ë€ ê³¡ë¬¼ì´ ë‚˜ì˜¬ í™•ë¥ ì´ë¼ê³  ê°€ì •í•´ ë³´ì. $P+$ê°€ 0ì¸ ìƒí™©ì€ ë…¸ë€ ê³¡ë¬¼ì´ ì—†ëŠ” ìƒíƒœë¥¼ ì˜ë¯¸í•˜ê³ , 1ì¸ ê²½ìš°ëŠ” ë…¸ë€ ê³¡ë¬¼ë§Œ ìˆëŠ” ìƒíƒœì¼ ê²ƒì´ë‹¤. ì´ ë•Œì˜ ì—”íŠ¸ë¡œí”¼ëŠ” ì˜ ë¶„ë¦¬ë˜ì–´ìˆê¸° ë•Œë¬¸ì— 0ì˜ ê°’ì„ ê°–ê²Œëœë‹¤. í—ˆë‚˜ $P+$ê°€ 0.5ì¼ ê²½ìš°ëŠ” ë…¸ë€ê³¡ë¬¼ì´ ì¡´ì¬í•˜ê±°ë‚˜ í•˜ì§€ ì•Šì„ í™•ë¥ ì´ ê°ê° ì ˆë°˜ì´ê¸° ë•Œë¬¸ì— ì—”íŠ¸ë¡œí”¼ê°€ ê°€ì¥ ë†’ê²Œ ëœë‹¤. í™•ë¥ ë¡ ì—ì„œì˜ ì—”íŠ¸ë¡œí”¼ ê°œë…ì€ í™•ë¥ ë¶„í¬ì˜ ëª¨ì–‘ì„ ì„¤ëª…í•˜ëŠ” íŠ¹ì§•ê°’ì´ë©° í™•ë¥ ë¶„í¬ê°€ ê°€ì§€ê³  ìˆëŠ” ì •ë³´ì˜ ì–‘ì„ ë‚˜íƒ€ë‚´ëŠ” ê°’ì´ê¸°ë„ í•˜ë‹¤. ì—”íŠ¸ë¡œí”¼ëŠ” ë‘ í™•ë¥ ë¶„í¬ì˜ ëª¨ì–‘ì´ ì–´ë–¤ ê´€ê³„ë¥¼ ê°€ì§€ëŠ”ì§€ í˜¹ì€ ìœ ì‚¬í•œì§€ë¥¼ í‘œí˜„í•˜ëŠ” ë°ë„ ì“°ì¸ë‹¤. ì¡°ê±´ë¶€ì—”íŠ¸ë¡œí”¼ëŠ” í•œ í™•ë¥ ë¶„í¬ì— ì˜í•´ ë‹¤ë¥¸ í™•ë¥ ë¶„í¬ê°€ ë°›ëŠ” ì˜í–¥ì„ ì„¤ëª…í•œë‹¤. êµì°¨ì—”íŠ¸ë¡œí”¼ì™€ ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°ì€ ë‘ í™•ë¥ ë¶„í¬ê°€ ì–¼ë§ˆë‚˜ ë‹®ì•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë‘ í™•ë¥ ë¶„í¬ì˜ ë…ë¦½ ë° ìƒê´€ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìƒí˜¸ì •ë³´ëŸ‰ì— ëŒ€í•´ì„œ ì„¤ëª…í•  ê²ƒì´ë‹¤. $Y\\;=\\;0$ ë˜ëŠ” $Y\\;=\\;1$ì¸ ë‘ ê°€ì§€ ê°’ì„ ê°€ì§€ëŠ” í™•ë¥ ë³€ìˆ˜ì˜ í™•ë¥ ë¶„í¬ê°€ ë‹¤ìŒê³¼ ê°™ì´ ì„¸ ì¢…ë¥˜ê°€ ìˆë‹¤ê³  í•˜ì. í™•ë¥ ë¶„í¬ $Y_{1}$ : $P(Y=0)=0.5, P(Y=1)=0.5$ í™•ë¥ ë¶„í¬ $Y_{2}$ : $P(Y=0)=0.8, P(Y=1)=0.2$ í™•ë¥ ë¶„í¬ $Y_{3}$ : $P(Y=0)=1.0, P(Y=0)=0.0$ 123456789101112131415161718plt.figure(figsize=(9, 3))plt.subplot(131)plt.bar([0, 1], [0.5, 0.5])plt.xticks([0, 1], [\"Y=0\", \"Y=1\"])plt.ylim(0, 1.1)plt.title(\"$Y_1$\")plt.subplot(132)plt.bar([0, 1], [0.8, 0.2])plt.xticks([0, 1], [\"Y=0\", \"Y=1\"])plt.ylim(0, 1.1)plt.title(\"$Y_2$\")plt.subplot(133)plt.bar([0, 1], [1.0, 0.0])plt.xticks([0, 1], [\"Y=0\", \"Y=1\"])plt.ylim(0, 1.1)plt.title(\"$Y_3$\")plt.tight_layout()plt.show() ë² ì´ì§€ì•ˆ ê´€ì ì—ì„œ ìœ„ í™•ë¥ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. í™•ë¥ ë¶„í¬ $Y_{1}$ì€ $y$ê°’ì— ëŒ€í•´ ì•„ë¬´ê²ƒë„ ëª¨ë¥´ëŠ” ìƒíƒœ í™•ë¥ ë¶„í¬ $Y_{2}$ì€ $y$ê°’ì´ 0ì´ë¼ê³  ë¯¿ì§€ë§Œ ì•„ë‹ ê°€ëŠ¥ì„±ë„ ìˆë‹¤ëŠ” ê²ƒì„ ì•„ëŠ” ìƒíƒœ í™•ë¥ ë¶„í¬ $Y_{2}$ì€ $y$ê°’ì´ 0ì´ë¼ê³  100% í™•ì‹ í•˜ëŠ” ìƒíƒœ í™•ë¥  ë¶„í¬ê°€ ê°€ì§€ëŠ” ì´ëŸ¬í•œ ì°¨ì´ë¥¼ í•˜ë‚˜ì˜ ìˆ«ìë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì´ ë°”ë¡œ ì—”íŠ¸ë¡œí”¼ì´ë‹¤. Entropy ì •ì˜ ì—”íŠ¸ë¡œí”¼(Entropy)ëŠ” í™•ë¥ ë¶„í¬ê°€ ê°€ì§€ëŠ” ì •ë³´ì˜ í™•ì‹ ë„ í˜¹ì€ ì •ë³´ëŸ‰ì„ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•œ ê²ƒì´ë‹¤. í™•ë¥ ë¶„í¬ì—ì„œ íŠ¹ì •í•œ ê°’ì´ ë‚˜ì˜¬ í™•ë¥ ì´ ë†’ì•„ì§€ê³  ë‚˜ë¨¸ì§€ ê°’ì˜ í™•ë¥ ì€ ë‚®ì•„ì§„ë‹¤ë©´ ì—”íŠ¸ë¡œí”¼ê°€ ì‘ì•„ì§„ë‹¤. ë°˜ëŒ€ë¡œ ì—¬ëŸ¬ê°€ì§€ ê°’ì´ ë‚˜ì˜¬ í™•ë¥ ì´ ëŒ€ë¶€ë¶„ ë¹„ìŠ·í•œ ê²½ìš°ì—ëŠ” ì—”íŠ¸ë¡œí”¼ê°€ ë†’ì•„ì§„ë‹¤. ì—”íŠ¸ë¡œí”¼ëŠ” í™•ë¥ ë¶„í¬ì˜ ëª¨ì–‘ì´ ì–´ë–¤ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” íŠ¹ì„±ê°’ ì¤‘ í•˜ë‚˜ë¡œ ë³¼ ìˆ˜ë„ ìˆë‹¤. í™•ë¥  ë˜ëŠ” í™•ë¥ ë°€ë„ê°€ íŠ¹ì •ê°’ì— ëª°ë ¤ìˆìœ¼ë©´ ì—”íŠ¸ë¡œí”¼ê°€ ì‘ë‹¤ê³  í•˜ê³  ë°˜ëŒ€ë¡œ ì—¬ëŸ¬ê°€ì§€ ê°’ì— ê³¨ê³ ë£¨ í¼ì ¸ ìˆë‹¤ë©´ ì—”íŠ¸ë¡œí”¼ê°€ í¬ë‹¤ê³  í•œë‹¤. í™•ë¥ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ëŠ” ë¬¼ë¦¬í•™ì˜ ì—”íŠ¸ë¡œí”¼ ìš©ì–´ë¥¼ ë¹Œë ¤ì˜¨ ê²ƒì´ë‹¤. ë¬¼ë¦¬í•™ì—ì„œëŠ” ë¬¼ì§ˆì˜ ìƒíƒœê°€ ë¶„ì‚°ë˜ëŠ” ì •ë„ë¥¼ ì—”íŠ¸ë¡œí”¼ë¡œ ì •ì˜í•œë‹¤. ë¬¼ì²´ì˜ ìƒíƒœê°€ ì—¬ëŸ¬ê°€ì§€ë¡œ ê³ ë£¨ ë¶„ì‚°ë˜ì–´ ìˆìœ¼ë©´ ì—”íŠ¸ë¡œí”¼ê°€ ë†’ê³  íŠ¹ì •í•œ í•˜ë‚˜ì˜ ìƒíƒœë¡œ ëª°ë ¤ìˆìœ¼ë©´ ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ë‹¤. ìˆ˜í•™ì ìœ¼ë¡œ ì—”íŠ¸ë¡œí”¼ëŠ” í™•ë¥ ë¶„í¬í•¨ìˆ˜ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìˆ«ìë¥¼ ì¶œë ¥í•˜ëŠ” ë²”í•¨ìˆ˜(functional)ë¡œ ì •ì˜í•œë‹¤. í™•ë¥ ë³€ìˆ˜ $Y$ê°€ ì¹´í…Œê³ ë¦¬ë¶„í¬ì™€ ê°™ì€ ì´ì‚°í™•ë¥ ë³€ìˆ˜ì´ë©´ ë‹¤ìŒì²˜ëŸ¼ ì •ì˜í•œë‹¤. ì´ ì‹ì—ì„œ $K$ëŠ” $X$ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” í´ë˜ìŠ¤ì˜ ìˆ˜ì´ê³  p(y)ëŠ” í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ì´ë‹¤. í™•ë¥ ì˜ ë¡œê·¸ê°’ì´ í•­ìƒ ìŒìˆ˜ì´ë¯€ë¡œ ìŒìˆ˜ ê¸°í˜¸ë¥¼ ë¶™ì—¬ì„œ ì–‘ìˆ˜ë¡œ ë§Œë“¤ì—ˆë‹¤. \\begin{align} H[Y] = -\\sum_{k=1}^K p(y_k) \\log_2 p(y_k) \\end{align} í™•ë¥ ë³€ìˆ˜ $Y$ê°€ ì—°ì†í™•ë¥ ë³€ìˆ˜ì´ë©´ ë‹¤ìŒì²˜ëŸ¼ ì •ì˜í•œë‹¤. ì•„ë˜ ìˆ˜ì‹ì—ì„œ $p(y)$ëŠ” pdfì´ë‹¤. \\begin{align} H[Y] = -\\int_{-\\infty}^{\\infty} p(y) \\log_2 p(y) \\; dy \\end{align} ë¡œê·¸ì˜ ë°‘(base)ì´ 2ë¡œ ì •ì˜ëœ ê²ƒì€ ì •ë³´í†µì‹ ê³¼ ê´€ë ¨ì„ ê°€ì§€ëŠ” ì—­ì‚¬ì ì¸ ì´ìœ  ë•Œë¬¸ì´ë‹¤. ì—”íŠ¸ë¡œí”¼ ê³„ì‚°ì—ì„œ $p(y)\\;=\\;0$ì¸ ê²½ìš°ì—ëŠ” ë¡œê·¸ê°’ì´ ì •ì˜ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê·¹í•œê°’ì„ ì‚¬ìš©í•œë‹¤. \\begin{align} \\lim_{p\\rightarrow 0} \\; p\\log_2{p} = 0 \\end{align} ì´ ê°’ì€ ë¡œí”¼íƒˆì˜ ì •ë¦¬ì—ì„œ êµ¬í•  ìˆ˜ ìˆë‹¤. ìœ„ì—ì„œ ì˜ˆë¥¼ ë“  $Y_{1}, Y_{2}, Y_{3}$ 3ê°œì˜ ì´ì‚°í™•ë¥ ë¶„í¬ì— ëŒ€í•´ ì—”íŠ¸ë¡œí”¼ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} H[Y_1] = -\\dfrac{1}{2} \\log_2 \\dfrac{1}{2} -\\dfrac{1}{2} \\log_2 \\dfrac{1}{2} = 1 \\end{align}\\begin{align} H[Y_2] = -\\dfrac{8}{10} \\log_2 \\dfrac{8}{10} -\\dfrac{2}{10} \\log_2 \\dfrac{2}{10} \\approx 0.72 \\end{align}\\begin{align} H[Y_3] = -1 \\log_2 1 -0 \\log_2 0 = 0 \\end{align} ë‹¤ìŒì€ Numpyë¡œ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•œ ê²°ê³¼ë‹¤. í™•ë¥ ê°’ì´ 0ì¼ ë•ŒëŠ” ê°€ì¥ ì‘ì€ ê°’ì¸ epsë¥¼ ëŒ€ì‹  ì‚¬ìš©í•œë‹¤. 1-0.5 * np.log2(0.5) - 0.5 * np.log2(0.5) ê²°ê³¼11.0 1-0.8 * np.log2(0.8) - 0.2 * np.log2(0.2) ê²°ê³¼10.7219280948873623 12eps = np.finfo(float).eps-1 * np.log2(1) - eps * np.log2(eps) ê²°ê³¼11.1546319456101628e-14 ì—°ìŠµë¬¸ì œ) ë² ë¥´ëˆ„ì´ë¶„í¬ì—ì„œ í™•ë¥ ê°’ $P(Y=1)$ì€ 0ë¶€í„° 1ê¹Œì§€ì˜ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ê°ê°ì˜ ê°’ì— ëŒ€í•´ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ë¡œì¶•ì´ P(Y=1)ì´ê³  ì„¸ë¡œì¶•ì´ H(Y)ì¸ ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë¼. 123456789101112131415161718192021222324import matplotlib.font_manager as fmpath = '/Library/Fonts/NanumGothic.ttf'font_name = fm.FontProperties(fname=path, size=50).get_name()plt.rc('font', family=font_name)P_Y = np.linspace(0, 1, 100)ls=[]for p_y in P_Y: if (p_y != 0) and (1-p_y != 0): ls.append(- p_y * np.log2(p_y) -(1 - p_y) * np.log2(1-p_y)) if (p_y == 0) and (1 - p_y == 1): p_y = np.finfo(float).eps ls.append(- p_y * np.log2(p_y) -(1 - p_y) * np.log2(1-p_y)) if (p_y == 1) and (1 - p_y == 0): ls.append(- p_y * np.log2(p_y) -(np.finfo(float).eps) * np.log2(np.finfo(float).eps))plt.plot(P_Y, ls, \"-\", label=\"ì—”íŠ¸ë¡œí”¼\")plt.ylabel(\"ì—”íŠ¸ë¡œí”¼\")plt.xlabel(\"ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ ëª¨ìˆ˜\")plt.show() ë‹¤ìŒ í™•ë¥ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•˜ë¼. \\begin{align} H[Y] = -\\sum_{k=1}^K p(y_k) \\log_2 p(y_k) \\end{align} ì—”íŠ¸ë¡œí”¼ì˜ ì •ì˜ì—ë”°ë¼ í’€ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. (1) P(Y=0)=\\dfrac{1}{8}, P(Y=1)=\\dfrac{1}{8}, P(Y=2)=\\dfrac{1}{4}, P(Y=3)=\\dfrac{1}{2}1- 1/8 * np.log2(1/8) -(1/8) * np.log2(1/8) -(2/8) * np.log2(2/8) -(4/8) * np.log2(4/8) ê²°ê³¼11.75 (2) P(Y=0)=1, P(Y=1)=0, P(Y=2)=0, P(Y=3)=01- 1 * np.log2(1) -(np.finfo(float).eps) * np.log2(np.finfo(float).eps) -(np.finfo(float).eps) * np.log2(np.finfo(float).eps) -(np.finfo(float).eps) * np.log2(np.finfo(float).eps) ê²°ê³¼13.4638958368304884e-14 (3) P(Y=0)=\\dfrac{1}{4}, P(Y=1)=\\dfrac{1}{4}, P(Y=2)=\\dfrac{1}{4}, P(Y=3)=\\dfrac{1}{4}1- 1/4 * np.log2(1/4) -(1/4) * np.log2(1/4) -(1/4) * np.log2(1/4) -(1/4) * np.log2(1/4) ê²°ê³¼12.0 ì—”íŠ¸ë¡œí”¼ì˜ ì„±ì§ˆ í™•ë¥ ë³€ìˆ˜ê°€ ê²°ì •ë¡ ì ì´ë©´ í™•ë¥ ë¶„í¬ì—ì„œ íŠ¹ì •í•œ í•˜ë‚˜ì˜ ê°’ì´ ë‚˜ì˜¬ í™•ë¥ ì´ 1ì´ë‹¤. ì´ ë•Œ ì—”íŠ¸ë¡œí”¼ëŠ” 0ì´ ë˜ê³  ì´ ê°’ì€ ì—”íŠ¸ë¡œí”¼ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ìµœì†Ÿê°’ì´ë‹¤. ë°˜ëŒ€ë¡œ ì—”íŠ¸ë¡œí”¼ì˜ ìµœëŒ€ê°’ì€ ì´ì‚° í™•ë¥ ë³€ìˆ˜ì˜ í´ë˜ìŠ¤ì˜ ê°¯ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤. ë§Œì•½ ì´ì‚°í™•ë¥ ë¶„í¬ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ì´ $2^{K}$ê°œë©´ ì—”íŠ¸ë¡œí”¼ì˜ ìµœëŒ€ê°’ì€ ê° ê°’ì— ëŒ€í•œ í™•ë¥ ì´ ëª¨ë‘ ê°™ì€ ê°’ì¸ $\\frac{1}/{2^{K}}$ì´ë‹¤. ì—”íŠ¸ë¡œí”¼ì˜ ê°’ì€ ì•„ë˜ì˜ ìˆ˜ì‹ê³¼ ê°™ì„ ê²ƒì´ë‹¤. \\begin{align} H = -2^K \\cdot \\frac{1}{2^K}\\log_2\\dfrac{1}{2^K} = K \\end{align}ì—”íŠ¸ë¡œí”¼ì˜ ì¶”ì • ì´ë¡ ì ì¸ í™•ë¥ ë°€ë„í•¨ìˆ˜ê°€ ì—†ê³  ì‹¤ì œ ë°ì´í„°ê°€ ì£¼ì–´ì§„ ê²½ìš°ì—ëŠ” ë°ì´í„°ì—ì„œ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ ì¶”ì •í•œ í›„, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë°ì´í„°ê°€ ëª¨ë‘ 80ê°œê°€ ìˆê³  ê·¸ ì¤‘ $Y=0$ì¸ ë°ì´í„°ê°€ 40ê°œ, $Y=1$ ë°ì´í„°ê°€ 40ê°œ ìˆëŠ” ê²½ìš°ëŠ” ì—”íŠ¸ë¡œí”¼ê°€ 1ì´ë‹¤. \\begin{align} P(y=0) = \\dfrac{40}{80} = \\dfrac{1}{2} \\end{align}\\begin{align} P(y=1) = \\dfrac{40}{80} = \\dfrac{1}{2} \\end{align}\\begin{align} H[Y] = -\\dfrac{1}{2}\\log_2\\left(\\dfrac{1}{2}\\right) -\\dfrac{1}{2}\\log_2\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{2} + \\dfrac{1}{2} = 1 \\end{align} Scipyì˜ stats ì„œë¸ŒíŒ¨í‚¤ì¦ˆëŠ” ì—”íŠ¸ë¡œí”¼ë¥¼ êµ¬í•˜ëŠ” entropyí•¨ìˆ˜ë¥¼ ì œê³µí•œë‹¤. baseì¸ìˆ˜ê°’ì€ 2ê°€ ë˜ì–´ì•¼ í•œë‹¤. 12p = [0.5, 0.5]sp.stats.entropy(p, base=2) ê²°ê³¼11.0 ì—°ìŠµ ë¬¸ì œ) (1) ë°ì´í„°ê°€ ëª¨ë‘ 60ê°œê°€ ìˆê³  ê·¸ ì¤‘ $Y=0$ì¸ ë°ì´í„°ê°€ 20ê°œ, $Y=1$ì¸ ë°ì´í„°ê°€ 40ê°œ ìˆëŠ” ê²½ìš°ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•˜ë¼. 12p = [1/3, 2/3]sp.stats.entropy(p, base=2) ê²°ê³¼10.9182958340544894 (2) ë°ì´í„°ê°€ ëª¨ë‘ 40ê°œê°€ ìˆê³  ê·¸ ì¤‘ $Y=0$ì¸ ë°ì´í„°ê°€ 30ê°œ, $Y=1$ì¸ ë°ì´í„°ê°€ 10ê°œ ìˆëŠ” ê²½ìš°ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•˜ë¼. 12p = [3/4, 1/4]sp.stats.entropy(p, base=2) ê²°ê³¼10.8112781244591328 12p = [1, 0]sp.stats.entropy(p, base=2) ê²°ê³¼10.0 ê°€ë³€ê¸¸ì´ ì¸ì½”ë”© ì—”íŠ¸ë¡œí”¼ëŠ” ì›ë˜ í†µì‹  ë¶„ì•¼ì—ì„œ ë°ì´í„°ê°€ ê°€ì§€ê³  ìˆëŠ” ì •ë³´ëŸ‰ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ê³ ì•ˆë˜ì—ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 4ê°œì˜ ê¸€ì A, B, C, Dë¡œ ì”Œì—¬ì§„ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì„œê°€ ìˆë‹¤ê³  í•˜ì. 123456N = 200p = [1/2, 1/4, 1/8, 1/8]doc0 = list(\"\".join([int(N * p[i]) * c for i, c in enumerate(\"ABCD\")]))np.random.shuffle(doc0)doc = \"\".join(doc0)doc ê²°ê³¼1'BDABABACBABBAACADAADAAADAAAAAABBAABADAAAAABBACAABACBBACDBAAACBCABBAABAAAAADDBABCBDBBDDBAABBBADCACAADAADCABADCAAAAACADBAABABCBAACAAABCDAADDCCCAAABABBDACACAAAAAABABBADABBABDBADBACAABDCAAABAAABACCDABAABA' ì´ ë¬¸ì„œë¥¼ 0ê³¼ 1ë¡œ ì´ë£¨ì–´ì§„ ì´ì§„ìˆ˜ë¡œ ë³€í™˜í•´ì•¼ í•˜ë©´ ë³´í†µ ë‹¤ìŒì²˜ëŸ¼ ì¸ì½”ë”©í•œë‹¤. A=â€00â€ A=â€01â€ A=â€10â€ A=â€11â€ ì´ë ‡ê²Œ ì¸ì½”ë”©ì„ í•˜ë©´ 200ê¸€ìë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì„œëŠ” ì´ì§„ìˆ˜ 400ê°œê°€ ëœë‹¤. 123encoder = &#123;\"A\": \"00\", \"B\": \"01\", \"C\": \"10\", \"D\": \"11\"&#125;encoded_doc = \"\".join([encoder[c] for c in doc])encoded_doc ê²°ê³¼1'0111000100010010010001010000100011000011000000110000000000000101000001001100000000000101001000000100100101001011010000001001100001010000010000000000111101000110011101011111010000010101001110001000001100001110000100111000000000001000110100000100011001000010000000011011000011111010100000000100010111001000100000000000000100010100110001010001110100110100100000011110000000010000000100101011000100000100' 1len(encoded_doc) ê²°ê³¼1400 ê·¸ëŸ°ë° ì´ì§„ìˆ˜ë¡œ ë³€í™˜í•  ë•Œ ë” ê¸€ììˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ìˆë‹¤. ìš°ì„  ìœ„ ê¸€ìì˜ ë¶„í¬ë¥¼ ì¡°ì‚¬í•˜ì. 123sns.countplot(list(doc), order=\"ABCD\")plt.title(\"ê¸€ììˆ˜ì˜ ë¶„í¬\")plt.show() ê¸€ììˆ˜ì˜ ë¶„í¬ê°€ ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} P(Y=A)=\\dfrac{1}{2}, P(Y=B)=\\dfrac{1}{4}, P(Y=C)=\\dfrac{1}{8}, P(Y=D)=\\dfrac{1}{8} \\end{align} ì§€í”„ì˜ ë²•ì¹™(Zipfâ€™s law)ì— ë”°ë¥´ë©´ ì´ëŸ¬í•œ ë¶„í¬ëŠ” í˜„ì‹¤ì˜ ê¸€ì ë¹ˆë„ìˆ˜ì—ì„œë„ í”íˆ ë‚˜íƒ€ë‚œë‹¤. í™•ë¥ ë¶„í¬ê°€ ìœ„ì™€ ê°™ì„ ë•ŒëŠ” ë‹¤ìŒì²˜ëŸ¼ ì¸ì½”ë”©í•˜ë©´ ì¸ì½”ë”©ëœ í›„ì˜ ì´ì§„ìˆ˜ ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. A=â€0â€ B=â€10â€ C=â€110â€ D=â€111â€ ì´ ë°©ë²•ì€ ê¸€ìë§ˆë‹¤ ì¸ì½”ë”©í•˜ëŠ” ì´ì§„ìˆ˜ì˜ ìˆ«ìê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ê°€ë³€ê¸¸ì´ ì¸ì½”ë”©(variable length encoding)ì´ë¼ê³  í•œë‹¤. ê°€ì¥ ë§ì´ ì¶œí˜„í•˜ëŠ” â€˜Aâ€™ëŠ” ë‘ ê¸€ìê°€ ì•„ë‹Œ í•œ ê¸€ìì´ë¯€ë¡œ ì¸ì½”ë”© í›„ì˜ ì´ì§„ìˆ˜ ìˆ˜ê°€ ê°ì†Œí•œë‹¤. ë°˜ëŒ€ë¡œ â€˜Câ€™, â€˜Dâ€™ëŠ” ì´ì§„ìˆ˜ì˜ ìˆ˜ê°€ 3ê°œë¡œ ë§ì§€ë§Œ ê¸€ìì˜ ë¹ˆë„ê°€ ì ì–´ì„œ ì˜í–¥ì´ ì ë‹¤. ë§Œì•½ ë¬¸ì„œì˜ ë¶„í¬ê°€ ìœ„ì—ì„œ ê°€ì •í•œ ë¶„í¬ì™€ ì •í™•í•˜ê²Œ ê°™ë‹¤ë©´ ì¸ì½”ë”©ëœ ì´ì§„ìˆ˜ì˜ ìˆ«ìëŠ” ë‹¤ìŒ ê³„ì‚°ì—ì„œ 350ê°œê°€ ë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤. \\begin{align} \\left(200 \\times \\dfrac{1}{2}\\right) \\cdot 1 + \\left(200 \\times \\dfrac{1}{4}\\right) \\cdot 2 + \\left(200 \\times \\dfrac{1}{8}\\right) \\cdot 3 + \\left(200 \\times \\dfrac{1}{8}\\right) \\cdot 3 = 350 \\end{align} ë”°ë¼ì„œ ì•ŒíŒŒë²³ í•œ ê¸€ìë¥¼ ì¸ì½”ë”©í•˜ëŠ”ë° í•„ìš”í•œ í‰ê·  ë¹„íŠ¸(bit)ìˆ˜ëŠ” $350 \\div 200 = 1.75$ì´ê³  ì´ ê°’ì€ í™•ë¥ ë³€ìˆ˜ì˜ ì—”íŠ¸ë¡œí”¼ ê°’ê³¼ ê°™ë‹¤. \\begin{align} H = -\\dfrac{1}{2}\\log_2\\dfrac{1}{2} -\\dfrac{1}{4}\\log_2\\dfrac{1}{4} -\\dfrac{2}{8}\\log_2\\dfrac{1}{8} = 1.75 \\end{align}123vl_encoder = &#123;\"A\": \"0\", \"B\": \"10\", \"C\": \"110\", \"D\": \"111\"&#125;vl_encoded_doc = \"\".join([vl_encoder[c] for c in doc])vl_encoded_doc ê²°ê³¼1'10111010010011010010100011001110011100011100000010100010011100000101001100010011010100110111100001101011001010001000000111111100101101011110101111111000101010011111001100011100111110010011111000000110011110001001011010001100001011011100111111110110110000100101011101100110000000100101001110101001011110011110011000101111100001000010011011011101000100' 1len(vl_encoded_doc) ê²°ê³¼1350 1sp.stats.entropy([1/2, 1/4, 1/8, 1/8], base=2) ê²°ê³¼11.75 ì§€ë‹ˆ ë¶ˆìˆœë„ ì—”íŠ¸ë¡œí”¼ì™€ ìœ ì‚¬í•œ ê°œë…ìœ¼ë¡œ ì§€ë‹ˆë¶ˆìˆœë„(Gini impurity)ë¼ëŠ” ê²ƒì´ ìˆë‹¤. ì§€ë‹ˆë¶ˆìˆœë„ëŠ” ì—”íŠ¸ë¡œí”¼ì²˜ëŸ¼ í™•ë¥ ë¶„í¬ê°€ ì–´ëŠìª½ì— ì¹˜ìš°ì³ì ¸ìˆëŠ”ê°€ë¥¼ ì¬ëŠ” ì²™ë„ì§€ë§Œ ë¡œê·¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê³„ì‚°ëŸ‰ì´ ë” ì ì–´ ì—”íŠ¸ë¡œí”¼ ëŒ€ìš©ìœ¼ë¡œ ë§ì´ ì‚¬ìš©ëœë‹¤. ê²½ì  í•™ì—ì„œë„ ì‚¬ìš©ë˜ì§€ë§Œ ì§€ë‹ˆê³„ìˆ˜(Gini coefficient)ì™€ëŠ” ë‹¤ë¥¸ ê°œë…ì´ë¼ëŠ” ì ì— ì£¼ì˜í•´ì•¼ í•œë‹¤. \\begin{align} G[Y] = \\sum_{k=1}^K P(y_k) (1 - P(y_k)) \\end{align} ë‹¤ìŒ ê·¸ë¦¼ì€ ê°’ì´ ë‘ ê°œì¸ ì´ì‚°í™•ë¥ ë¶„í¬ì—ì„œ ì§€ë‹ˆë¶ˆìˆœë„ì™€ ì—”íŠ¸ë¡œí”¼ë¥¼ ë¹„êµí•œ ê²°ê³¼ì´ë‹¤. 12345678910P0 = np.linspace(0.001, 1 - 0.001, 1000)P1 = 1 - P0H = - P0 * np.log2(P0) - P1 * np.log2(P1)G = 2 * (P0 * (1 - P0) + P1 * (1 - P1))plt.plot(P1, H, \"-\", label=\"ì—”íŠ¸ë¡œí”¼\")plt.plot(P1, G, \"--\", label=\"ì§€ë‹ˆë¶ˆìˆœë„\")plt.legend()plt.xlabel(\"P(Y=1)\")plt.show() ì—”íŠ¸ë¡œí”¼ ìµœëŒ€í™” ê¸°ëŒ€ê°’ $0$, ë¶„ì‚° $\\sigma^{2}$ì´ ì£¼ì–´ì¡Œì„ ë•Œ ì—”íŠ¸ë¡œí”¼ $\\text{H}[p(x)]$ë¥¼ ê°€ì¥ í¬ê²Œ ë§Œë“œëŠ” í™•ë¥ ë°€ë„í•¨ìˆ˜ $p(x)$ëŠ” ì •ê·œë¶„í¬ê°€ ëœë‹¤. ì´ëŠ” ì•„ë˜ì™€ ê°™ì´ ì¦ëª…í•œë‹¤. ìš°ì„  í™•ë¥  ë°€ë„í•¨ìˆ˜ê°€ ì§€ì¼œì•¼ í•  ì œí•œì¡°ê±´ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. (1) í™•ë¥ ë°€ë„í•¨ìˆ˜ì˜ ì´ë©´ì ì€ 1 \\begin{align} \\int_{-\\infty}^{\\infty} p(x) dx = 1 \\end{align} (2) ê¸°ëŒ“ê°’(í‰ê· )ì€ 0 \\begin{align} \\int_{-\\infty}^{\\infty} xp(x) dx = 0 \\end{align} (3) ë¶„ì‚°ì€ $\\sigma^{2}$ \\begin{align} \\int_{-\\infty}^{\\infty} x^2 p(x) dx = \\sigma^2 \\end{align} ìµœëŒ€í™”í•  ëª©ì ë²”í•¨ìˆ˜(objective functional)ì€ ì—”íŠ¸ë¡œí”¼ì´ë‹¤. \\begin{align} \\text{H}[p(x)] = -\\int_{-\\infty}^{\\infty} p(x)\\log p(x) dx \\end{align} ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ìœ¼ë¡œ ì œí•œì¡°ê±´ì„ ì¶”ê°€í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤. \\begin{align} \\begin{aligned} \\text{H}[p(x)] &= -\\int_{-\\infty}^{\\infty} p(x)\\log p(x) dx + \\lambda_1 \\left( \\int_{-\\infty}^{\\infty} p(x) dx - 1 \\right) \\\\ & + \\lambda_2 \\left( \\int_{-\\infty}^{\\infty} xp(x) dx\\right) + \\lambda_3 \\left( \\int_{-\\infty}^{\\infty} x^2 p(x) dx - \\sigma^2 \\right) \\\\ &= \\int_{-\\infty}^{\\infty} \\left(-p(x)\\log p(x) + \\lambda_1 p(x) + \\lambda_2 xp(x) + \\lambda_3 x^2p(x) - \\lambda_1 - \\lambda_3 \\sigma^2 \\right) dx \\end{aligned} \\end{align} ë³€ë¶„ë²•ì—ì„œ ë„í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤. ì°¸ê³  : ë³€ë¶„ë²• \\begin{align} \\dfrac{\\delta H}{\\delta p(x)} = -\\log p(x) - 1 + \\lambda_1 + \\lambda_2 x + \\lambda_3 x^2 = 0 \\end{align} ë”°ë¼ì„œ í™•ë¥ ë°€ë„í•¨ìˆ˜ì˜ í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} p(x) = \\exp \\left( - 1 + \\lambda_1 + \\lambda_2 x + \\lambda_3 x^2 \\right) \\end{align} ì ë¶„ì„ í†µí•´ ìœ„ í˜•íƒœì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ì˜ ë©´ì , ê¸°ëŒ€ê°’, ë¶„ì‚°ì„ ê³„ì‚°í•˜ê³  ì£¼ì–´ì§„ ì œí•œì¡°ê±´ì„ ë§Œì¡±í•˜ë„ë¡ ì—°ë¦½ë°©ì •ì‹ì„ í’€ë©´ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë¥¼ ë‹¤ìŒì²˜ëŸ¼ êµ¬í•  ìˆ˜ ìˆë‹¤. \\begin{align} \\begin{aligned} \\lambda_1 &= 1-\\dfrac{1}{2} \\log{2\\pi\\sigma^2} \\\\ \\lambda_2 &= 0 \\\\ \\lambda_3 &= -\\dfrac{1}{2\\sigma^2} \\\\ \\end{aligned} \\end{align} ì´ ê°’ì„ ëŒ€ì…í•˜ë©´ ì •ê·œë¶„í¬ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. \\begin{align} p(x) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left( -\\dfrac{x^2}{2\\sigma^2} \\right) \\end{align} ë”°ë¼ì„œ ì •ê·œë¶„í¬ëŠ” ê¸°ëŒ“ê°’ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì•Œê³ ìˆëŠ” í™•ë¥ ë¶„í¬ ì¤‘ì—ì„œ ê°€ì¥ ì—”íŠ¸ë¡œí”¼ê°€ í¬ê³  ë”°ë¼ì„œ ê°€ì¥ ì •ë³´ê°€ ì ì€ í™•ë¥ ë¶„í¬ì´ë‹¤. ì •ê·œë¶„í¬ëŠ” ë² ì´ì¦ˆ ì¶”ì •ì— ìˆì–´ì„œ ì‚¬ì‹¤ìƒì˜ ë¬´ì •ë³´ ì‚¬ì „í™•ë¥ ë¶„í¬ë¡œ ì‚¬ìš©ë˜ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¥¼ ì‚¬ìš©í•œ ë¶„ë¥˜ì˜ˆì¸¡ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì— ì „ì²´ íŠ¸ë ˆì´ë‹ ë°ì´í„°ë¥¼ ëª¨ë‘ ì ìš©í•´ ë³´ë©´ ê° ë°ì´í„°ëŠ” íŠ¹ì •í•œ ë…¸ë“œë¥¼ íƒ€ê³  ë‚´ë ¤ê°€ê²Œ ëœë‹¤. ê° ë…¸ë“œëŠ” ê·¸ ë…¸ë“œë¥¼ ì„ íƒí•œ ë°ì´í„° ì§‘í•©ì„ ê°–ëŠ”ë‹¤. ì´ ë•Œ ë…¸ë“œì— ì†í•œ ë°ì´í„°ì˜ í´ë˜ìŠ¤ì˜ ë¹„ìœ¨ì„ êµ¬í•˜ì—¬ ì´ë¥¼ ê·¸ ë…¸ë“œì˜ ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬ $P(Y\\;=\\;k|X)_{node}$ë¼ê³  ì •ì˜í•œë‹¤. P(Y=k|X)_{\\text{node}} \\approx \\dfrac{N_{\\text{node},k}}{N_{\\text{node}}} í…ŒìŠ¤íŠ¸ ë°ì´í„° $X_{test}$ì˜ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•  ë•ŒëŠ” ê°€ì¥ ìƒìœ„ì˜ ë…¸ë“œë¶€í„° ë¶„ë¥˜ ê·œì¹™ì„ ì°¨ë¡€ëŒ€ë¡œ ì ìš©í•˜ì—¬ ë§ˆì§€ë§‰ì— ë„ë‹¬í•˜ëŠ” ë…¸ë“œì˜ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ë¥¼ ì´ìš©í•˜ì—¬ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•œë‹¤. \\hat{Y} = \\text{arg}\\max_k P(Y=k|X_{\\text{test}})_{\\text{last node}}ë¶„ë¥˜ ê·œì¹™ì„ ì •í•˜ëŠ” ë°©ë²• ë¶„ë¥˜ ê·œì¹™ì„ ì •í•˜ëŠ” ë°©ë²•ì€ ë¶€ëª¨ ë…¸ë“œì™€ ìì‹ ë…¸ë“œê°„ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ê°€ì¥ ë‚®ê²Œ ë§Œë“œëŠ” ìµœìƒì˜ ë…ë¦½ ë³€ìˆ˜ì™€ ê¸°ì¤€ê°’ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ê¸°ì¤€ì„ ì •ëŸ‰í™”í•œ ê²ƒì´ ì •ë³´íšë“ëŸ‰(Information Gain)ì´ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ëª¨ë“  ë…ë¦½ë³€ìˆ˜ì™€ ëª¨ë“  ê°€ëŠ¥í•œ ê¸°ì¤€ê°’ì— ëŒ€í•´ ì •ë³´íšë“ëŸ‰ì„ êµ¬í•˜ì—¬ ê°€ì¥ ì •ë³´íšë“¤ëŸ‰ì´ í° ë…ë¦½ ë³€ìˆ˜ì™€ ê¸°ì¤€ê°’ì„ ì„ íƒí•œë‹¤. Information Gain ë°ì´í„° ì„¸íŠ¸ì˜ ê· ì¼ë„ëŠ” ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ëŠ” ë° í•„ìš”í•œ ì •ë³´ì˜ ì–‘ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤. ê²°ì • ë…¸ë“œëŠ” ì •ë³´ ê· ì¼ë„ê°€ ë†’ì€ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¨¼ì € ì„ íƒí•  ìˆ˜ ìˆë„ë¡ ê·œì¹™ ì¡°ê±´ì„ ë§Œë“ ë‹¤. ì¦‰, ì •ë³´ ê· ì¼ë„ê°€ ë°ì´í„° ì„¸íŠ¸ë¡œ ìª¼ê°œì§ˆ ìˆ˜ ìˆë„ë¡ ì¡°ê±´ì„ ì°¾ì•„ ì„œë¸Œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë§Œë“¤ê³ , ë‹¤ì‹œ ì´ ì„œë¸Œ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ê· ì¼ë„ê°€ ë†’ì€ ìì‹ ë°ì´í„° ì„¸íŠ¸ë¡œ ìª¼ê°œëŠ” ë°©ì‹ì„ ìì‹ íŠ¸ë¡œ ë‚´ë ¤ê°€ë©´ì„œ ë°˜ë³µí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ê°’ì„ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤. ì´ëŸ¬í•œ ì •ë³´ì˜ ê· ì¼ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ëŒ€í‘œì ì¸ ë°©ë²•ì€ ì—”íŠ¸ë¡œí”¼ë¥¼ ì´ìš©í•œ Information Gainê³¼ Gini ê³„ìˆ˜ê°€ ìˆë‹¤. ì¦‰, (ì´ì „ ì—”íŠ¸ë¡œí”¼ - ì´í›„ ì—”íŠ¸ë¡œí”¼)ì˜ ì°¨ê°€ ë§ì´ ë‚˜ëŠ” ê·œì¹™ë¶€í„° ì‹¤í–‰í•˜ì—¬ ê· ì¼ë„ë¥¼ ë†’ì´ëŠ” ë°©ì‹ì´ë‹¤. ê·œì¹™ë…¸ë“œë¥¼ í†µí•´ ì´ì „ ì—”íŠ¸ë¡œí”¼ì™€ì˜ ì°¨ì´ê°€ í´ìˆ˜ë¡ ìœ ì˜ë¯¸í•œ ê·œì¹™ì´ ë  ê²ƒì´ë‹¤. ì •ë³´íšë“ëŸ‰(Information Gain)ëŠ” $X$ë¼ëŠ” ì¡°ê±´ì— ì˜í•´ í™•ë¥  ë³€ìˆ˜ $Y$ì˜ ì—”íŠ¸ë¡œí”¼ê°€ ì–¼ë§ˆë‚˜ ê°ì†Œí•˜ì˜€ëŠ”ê°€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì´ë‹¤. ë‹¤ìŒì²˜ëŸ¼ $Y$ì˜ ì—”íŠ¸ë¡œí”¼ì—ì„œ $X$ì— ëŒ€í•œ $Y$ì˜ ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼ë¥¼ ëº€ ê°’ìœ¼ë¡œ ì •ì˜ëœë‹¤. IG[Y,X] = H[Y] - H[Y|X] ê²°í•© ì—”íŠ¸ë¡œí”¼ ê²°í•©ì—”íŠ¸ë¡œí”¼(joint entropy)ëŠ” ê²°í•©í™•ë¥ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ì˜í•œ ì—”íŠ¸ë¡œí”¼ë¥¼ ë§í•œë‹¤. ì´ì‚° í™•ë¥ ë³€ìˆ˜ $X,\\;Y$ì— ëŒ€í•´ ê²°í•©ì—”íŠ¸ë¡œí”¼ëŠ” ë‹¤ìŒ ì²˜ëŸ¼ ì •ì˜í•œë‹¤. ì•„ë˜ ì‹ì—ì„œ $K_{X}, K_{Y}$ëŠ” ê°ê° $X$ì™€ $Y$ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ì˜ ê°œìˆ˜ì´ê³ , $p$ëŠ” ê²°í•© í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ì´ë‹¤. \\begin{align} H[X, Y] = - \\sum_{i=1}^{K_X} \\sum_{j=1}^{K_Y} \\,p(x_i, y_j) \\log_2 p(x_i, y_j) \\end{align} ì—°ì†í™•ë¥ ë³€ìˆ˜ $X,\\;Y$ì— ëŒ€í•œ ê²°í•©ì—”íŠ¸ë¡œí”¼ëŠ” ë‹¤ìŒì²˜ëŸ¼ ì •ì˜í•œë‹¤. ì•„ë˜ ì‹ì—ì„œ $p$ëŠ” ê²°í•© í™•ë¥ ë°€ë„í•¨ìˆ˜ì´ë‹¤. \\begin{align} H[X, Y] = - \\int_{x} \\int_{y} \\,p(x, y) \\log_2 p(x, y) \\; dxdy \\end{align} ê²°í•©ì—”íŠ¸ë¡œí”¼ë„ ê²°í•©í™•ë¥ ë¶„í¬ë¼ëŠ” ì ë§Œ ì œì™¸í•˜ë©´ ì¼ë°˜ì ì¸ ì—”íŠ¸ë¡œí”¼ì™€ ê°™ë‹¤. ëª¨ë“  ê²½ìš°ì— ëŒ€í•´ ê³¨ê³ ë£¨ í™•ë¥ ì´ ë¶„í¬ë˜ì–´ ìˆìœ¼ë©´ ì—”íŠ¸ë¡œí”¼ê°’ì´ ì»¤ì§€ê³  íŠ¹ì •í•œ í•œ ê°€ì§€ ê²½ìš°ì— ëŒ€í•´ í™•ë¥ ì´ ëª¨ì—¬ìˆìœ¼ë©´ ì—”íŠ¸ë¡œí”¼ê°€ 0ì— ê°€ê¹Œì›Œì§„ë‹¤. ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼ ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼(conditional entropy)ëŠ” ì–´ë–¤ í™•ë¥  ë³€ìˆ˜ $X$ê°€ ë‹¤ë¥¸ í™•ë¥ ë³€ìˆ˜ $Y$ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²• ì¤‘ì˜ í•˜ë‚˜ì´ë‹¤. ë§Œì•½ í™•ë¥ ë³€ìˆ˜ Xì˜ ê°’ì´ ì–´ë–¤ íŠ¹ì •í•œ í•˜ë‚˜ì˜ ê°’ì„ ê°€ì§ˆ ë•Œ í™•ë¥ ë³€ìˆ˜ $Y$ë„ ë§ˆì°¬ê°€ì§€ë¡œ íŠ¹ì •í•œ ê°’ì´ ëœë‹¤ë©´ $X$ë¡œ $Y$ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. ë°˜ëŒ€ë¡œ í™•ë¥ ë³€ìˆ˜ $X$ì˜ ê°’ì´ ì–´ë–¤ íŠ¹ì •í•œ í•˜ë‚˜ì˜ ê°’ì„ ê°€ì ¸ë„ í™•ë¥ ë³€ìˆ˜ $Y$ê°€ ì—¬ëŸ¬ ê°’ìœ¼ë¡œ ê³¨ê³ ë£¨ ë¶„í¬ë˜ì–´ ìˆë‹¤ë©´ $X$ëŠ” $Y$ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ”ë° ë„ì›€ì´ ì•ˆëœë‹¤. ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼ì˜ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìœ ë„í•œë‹¤. í™•ë¥ ë³€ìˆ˜ $X,\\;Y$ê°€ ëª¨ë‘ ì´ì‚°í™•ë¥ ë³€ìˆ˜ë¼ê³  ê°€ì •í•˜ê³  $X$ê°€ íŠ¹ì •í•œ ê°’ $x_{i]}$ë¥¼ ê°€ì§ˆ ë–„ì˜ $Y$ì˜ ì—”íŠ¸ë¡œí”¼ $H[Y \\mid X=x_i]$ëŠ” ë‹¤ìŒì²˜ëŸ¼ ì¡°ê±´ë¶€í™•ë¥ ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ë¡œ ì •ì˜í•œë‹¤. \\begin{align} H[Y \\mid X=x_i] = - \\sum_{j=1}^{K_Y} p(y_j \\mid x_i) \\log_2 p(y_j \\mid x_i) \\end{align} ì¡°ê±´ë¶€ ì—”íŠ¸ë¡œí”¼ëŠ” í™•ë¥ ë³€ìˆ˜ $X$ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ëª¨ë“  ê²½ìš°ì— ëŒ€í•´ $H[Y \\mid X=x_i]$ë¥¼ ê°€ì¤‘í‰ê· í•œ ê°’ìœ¼ë¡œ ì •ì˜í•œë‹¤. \\begin{align} H[Y \\mid X] &= \\sum_{i=1}^{K_X} \\,p(x_i)\\,H[Y \\mid X=x_i] \\\\ &= - \\sum_{i=1}^{K_X} \\sum_{j=1}^{K_Y} p(y_j \\mid x_i)p(x_i) \\log_2 p(y_j \\mid x_i) \\\\ &= - \\sum_{i=1}^{K_X} \\sum_{j=1}^{K_Y} p(x_i, y_j) \\log_2 p(y_j \\mid x_i) \\\\ \\end{align} ì—°ì†í™•ë¥ ë³€ìˆ˜ì˜ ê²½ìš°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} H[Y \\mid X=x] = - \\int_{y} p(y \\mid x) \\log_2 p(y \\mid x)\\; dy \\end{align}\\begin{align} H[Y \\mid X] &= - \\int_{x} \\,p(x) \\,H[Y \\mid X=x] \\; dx \\\\ &= - \\int_{x} p(x) \\left( \\int_{y} p(y \\mid x) \\log_2 p(y \\mid x)\\; dy \\right) \\; dx \\\\ &= - \\int_{x} \\int_{y} p(y \\mid x_i) p(x) \\log_2 p(y \\mid x) \\; dxdy \\\\ &= - \\int_{x} \\int_{y} \\,p(x, y) \\log_2 p(y \\mid x) \\; dxdy \\\\ \\end{align} ë”°ë¼ì„œ ì¡°ê±´ë¶€ì—”íŠ¸ë¡œí”¼ì˜ ìµœì¢…ì ì¸ ìˆ˜í•™ì  ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ì´ì‚°í™•ë¥ ë³€ìˆ˜ì˜ ê²½ìš°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤. \\begin{align} H[Y \\mid X] = - \\sum_{i=1}^{K_X} \\sum_{j=1}^{K_Y} \\,p(x_i, y_j) \\log_2 p(y_j \\mid x_i) \\end{align} ì—°ì†í™•ë¥ ë³€ìˆ˜ì˜ ê²½ìš°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤. \\begin{align} H[Y \\mid X] = - \\int_{x} \\int_{y} \\,p(x, y) \\log_2 p(y \\mid x) \\; dxdy \\end{align} ë‹¤ì‹œ ëŒì•„ì™€ Decision Treeë¥¼ ì´ì•¼ê¸°í•˜ìë©´ Decision Treeì˜ ì¼ë°˜ì ì¸ ì•Œê³ ë¦¬ì¦˜ì€ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¶„í• í•˜ëŠ” ë° ê°€ì¥ ì¢‹ì€ ì¡°ê±´, ì¦‰ Information Gainì´ë‚˜ Gini coefficientê°€ ë†’ì€ ì¡°ê±´ì„ ì°¾ì•„ì„œ ìì‹ íŠ¸ë¦¬ ë…¸ë“±ì— ê±¸ì³ ë°˜ë³µì ìœ¼ë¡œ ë¶„í• í•œ ë’¤, ë°ì´í„°ê°€ ëª¨ë‘ íŠ¹ì • ë¶„ë¥˜ì— ì†í•˜ê²Œ ë˜ë©´ ë¶„í• ì„ ë©ˆì¶”ê³  ë¶„ë¥˜ë¥¼ ê²°ì •í•œë‹¤. Information Gain ì˜ˆì‹œ ì˜ˆë¥¼ ë“¤ì–´ A,B ë‘ ê°€ì§€ì˜ ë‹¤ë¥¸ ë¶„ë¥˜ ê·œì¹™ì„ ì ìš©í–ˆë”ë‹ˆ ìœ„ì—ì„œ ì²˜ëŸ¼ ì„œë¡œ ë‹¤ë¥´ê²Œ ë°ì´í„°ê°€ ë‚˜ë‰˜ì–´ ì¡Œë‹¤ê³  ê°€ì •í•˜ì. Aë°©ë²•ê³¼ Bë°©ë²• ëª¨ë‘ ë…¸ë“œ ë¶„ë¦¬ì „ì—ëŠ” Y=0ì¸ ë°ì´í„°ì˜ ìˆ˜ì™€ Y=1ì¸ ë°ì´í„°ì˜ ìˆ˜ê°€ ëª¨ë‘ 40ê°œì˜€ë‹¤. Aë°©ë²•ìœ¼ë¡œ ë…¸ë“œë¥¼ ë¶„ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë‘ ê°œì˜ ìì‹ ë…¸ë“œê°€ ìƒê¸´ë‹¤. ìì‹ ë…¸ë“œ A1ì€ Y=0ì¸ ë°ì´í„°ê°€ 30ê°œ, Y=1ì¸ ë°ì´í„°ê°€ 10ê°œ ìì‹ ë…¸ë“œ A2ì€ Y=0ì¸ ë°ì´í„°ê°€ 10ê°œ, Y=1ì¸ ë°ì´í„°ê°€ 30ê°œ Bë°©ë²•ìœ¼ë¡œ ë…¸ë“œë¥¼ ë¶„ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë‘ ê°œì˜ ìì‹ ë…¸ë“œê°€ ìƒê¸´ë‹¤. ìì‹ ë…¸ë“œ B1ì€ Y=0ì¸ ë°ì´í„°ê°€ 20ê°œ, Y=1ì¸ ë°ì´í„°ê°€ 40ê°œ ìì‹ ë…¸ë“œ B2ì€ Y=0ì¸ ë°ì´í„°ê°€ 20ê°œ, Y=1ì¸ ë°ì´í„°ê°€ 30ê°œ ìš°ì„  ë¶€ëª¨ ë…¸ë“œì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. H[Y] = -\\dfrac{1}{2}\\log_2\\left(\\dfrac{1}{2}\\right) -\\dfrac{1}{2}\\log_2\\left(\\dfrac{1}{2}\\right) = \\dfrac{1}{2} + \\dfrac{1}{2} = 1 A ë°©ë²•ì— ëŒ€í•´ IGë¥¼ ê³„ì‚°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. H[Y|X=X_1] = -\\dfrac{3}{4}\\log_2\\left(\\dfrac{3}{4}\\right) -\\dfrac{1}{4}\\log_2\\left(\\dfrac{1}{4}\\right) = 0.81H[Y|X=X_2] = -\\dfrac{1}{4}\\log_2\\left(\\dfrac{1}{4}\\right) -\\dfrac{3}{4}\\log_2\\left(\\dfrac{3}{4}\\right) = 0.81H[Y|X] = \\dfrac{1}{2} H[Y|X=X_1] + \\dfrac{1}{2} H[Y|X=X_2] = 0.81IG = H[Y] - H[Y|X] = 0.19 B ë°©ë²•ì— ëŒ€í•´ IGë¥¼ ê³„ì‚°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. H[Y|X=X_1] = -\\dfrac{1}{3}\\log_2\\left(\\dfrac{1}{3}\\right) - \\dfrac{2}{3}\\log_2\\left(\\dfrac{2}{3}\\right) = 0.92H[Y|X=X_2] = 0H[Y|X] = \\dfrac{3}{4} H[Y|X=X_1] + \\dfrac{1}{4} H[Y|X=X_2] = 0.69IG = H[D] - H[Y|X] = 0.31 ë”°ë¼ì„œ B ë°©ë²•ì´ ë” ë‚˜ì€ ë°©ë²•ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ìœ„ì™€ ê°™ì´ modelì„ fittingí•˜ì˜€ìœ¼ë©´, ì´ì œ ì–´ë–»ê²Œ í•´ë‹¹ ì˜ì—­ì— ì˜ˆì¸¡í•  ë°ì´í„°ê°€ í¬í•¨ëœë‹¤ë©´ ì–´ë–»ê²Œ í´ë˜ìŠ¤ë¥¼ ì •í•˜ëŠ”ì§€ì— ëŒ€í•´ì„œ ì•Œì•„ë³¼ ê²ƒì´ë‹¤. ê°„ë‹¨íˆ ë§í•˜ìë©´ í•´ë‹¹ ì˜ì—­ì— í¬í•¨ëœ í´ë˜ìŠ¤ì˜ ê°œìˆ˜ê°€ ë§ì€ ê²ƒìœ¼ë¡œ ì˜ˆì¸¡í•œë‹¤. Decision Treeì˜ íŠ¹ì§• ê²°ì • íŠ¸ë¦¬ì˜ ê°€ì¥ í° ì¥ì ì€ ì •ë³´ì˜ ê· ì¼ë„ë¼ëŠ” ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ê³  ìˆì–´ì„œ ì•Œê³ ë¦¬ì¦˜ì´ ì‰½ê³  ì§ê´€ì ì´ë¼ëŠ” ì ì´ë‹¤. ê²°ì • íŠ¸ë¦¬ê°€ ë£°ì´ ë§¤ìš° ëª…í™•í•˜ê³ , ì´ì— ê¸°ë°˜í•´ ì–´ë–»ê²Œ ê·œì¹™ ë…¸ë“œì™€ ë¦¬í”„ ë…¸ë“œê°€ ë§Œë“¤ì–´ì§€ëŠ”ì§€ ì•Œ ìˆ˜ ìˆê³ , ì‹œê°í™”ë¡œ í‘œí˜„ê¹Œì§€ í•  ìˆ˜ ìˆë‹¤. ë˜í•œ ì •ë³´ì˜ ê· ì¼ë„ë§Œ ì‹ ê²½ì“°ë©´ ë˜ë¯€ë¡œ íŠ¹ë³„í•œ ê²½ìš°ë¥¼ ì œì™¸í•˜ê³ ëŠ” ê° featureì˜ ìŠ¤ì¼€ì¼ë§ê³¼ normalizationê³¼ ê°™ì€ ì‘ì—…ì´ í¬ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤. ë°˜ë©´ì— ê²°ì • íŠ¸ë¦¬ ëª¨ë¸ì˜ ê°€ì¥ í° ë‹¨ì ì€ ê³¼ì í•©(overfitting)ìœ¼ë¡œ ì •í™•ë„ê°€ ë–¨ì–´ì§„ë‹¤ëŠ” ì ì´ë‹¤. ë³µì¡í•œ í•™ìŠµ ëª¨ë¸ì€ ê²°êµ­ì—ëŠ” ì‹¤ì œ ìƒí™©(í…ŒìŠ¤íŠ¸ ë°ì´í„°)ì— ìœ ì—°í•˜ê²Œ ëŒ€ì²˜í•  ìˆ˜ ì—†ì–´ì„œ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ë°–ì— ì—†ë‹¤. íŠ¸ë¦¬ì˜ í¬ê¸°ë¥¼ ì‚¬ì „ì— ì œí•œí•˜ëŠ” ê²ƒì´ ì˜¤íˆë ¤ ì„±ëŠ¥ íŠœë‹ì— ë” ë„ì›€ì´ ë  ê²ƒì´ë‹¤. Scikit-Learnì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ í´ë˜ìŠ¤- Scikit-Learnì—ì„œ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ëŠ” DecisionTreeClassifierí´ë˜ìŠ¤ë¡œ êµ¬í˜„ë˜ì–´ìˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” ë¶“ê½ƒ ë¶„ë¥˜ ë¬¸ì œë¥¼ ì˜ˆë¥¼ ë“¤ì–´ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¥¼ ì„¤ëª…í•œë‹¤. ì´ ì˜ˆì œì—ì„œëŠ” ë…ë¦½ë³€ìˆ˜ ê³µê°„ì„ ê³µê°„ìƒì— í‘œì‹œí•˜ê¸° ìœ„í•´ ê½ƒì˜ ê¸¸ì´ì™€ í­ë§Œì„ ë…ë¦½ë³€ìˆ˜ë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤.12345678910from sklearn.datasets import load_irisdata = load_iris()y = data.targetX = data.data[:, 2:]feature_names = data.feature_names[2:]from sklearn.tree import DecisionTreeClassifiertree1 = DecisionTreeClassifier(criterion='entropy', max_depth=1, random_state=0).fit(X, y) ë‹¤ìŒì€ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë¥¼ ì‹œê°í™”í•˜ê¸° ìœ„í•œ ì½”ë“œì´ë‹¤. draw_decision_treeí•¨ìˆ˜ëŠ” ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì˜ ì˜ì‚¬ ê²°ì • ê³¼ì •ì˜ ì„¸ë¶€ì ì¸ ë‚´ì—­ì„ ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ë³´ì—¬ì£¼ê³  plot_decision_regionsí•¨ìˆ˜ëŠ” ì´ëŸ¬í•œ ì˜ì‚¬ ê²°ì •ì— ì˜í•´ ë°ì´í„°ì˜ ì˜ì—­ì´ ì–´ë–»ê²Œ ë‚˜ë‰˜ì–´ì¡ŒëŠ”ì§€ë¥¼ ì‹œê°í™”í•˜ì—¬ ë³´ì—¬ì¤€ë‹¤. ì•„ë˜ ë°©ë²•ìœ¼ë¡œ draw_decision_treeí•¨ìˆ˜ë¥¼ ë™ì¼í•˜ê²Œ ì¶œë ¥í•  ìˆ˜ ìˆë‹¤. filled: ê·¸ë˜í”„ì— ê° í´ë˜ìŠ¤ë³„ë¡œ ìƒ‰ìƒì„ ì…í˜. rounded: ë°˜ì˜¬ë¦¼ ì‹œì¼œì£¼ëŠ” ì—­í•  special_characters: íŠ¹ìˆ˜ë¬¸ìê°€ ìˆì„ ê²½ìš° ì œì™¸ì‹œì¼œì¤Œ. 1234dot_data=tree.export_graphviz(out_file=None, decision_tree=tree1,feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True)graphviz.Source(dot_data) 123456789101112131415161718192021222324252627282930313233343536373839404142import ioimport pydotfrom IPython.core.display import Imagefrom sklearn.tree import export_graphvizdef draw_decision_tree(model): dot_buf = io.StringIO() export_graphviz(model, out_file=dot_buf, feature_names=feature_names) graph = pydot.graph_from_dot_data(dot_buf.getvalue())[0] image = graph.create_png() return Image(image)def plot_decision_regions(X, y, model, title): resolution = 0.01 markers = ('s', '^', 'o') colors = ('red', 'blue', 'lightgreen') cmap = mpl.colors.ListedColormap(colors) x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1 x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution)) Z = model.predict( np.array([xx1.ravel(), xx2.ravel()]).T).reshape(xx1.shape) plt.contour(xx1, xx2, Z, cmap=mpl.colors.ListedColormap(['k'])) plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) plt.xlim(xx1.min(), xx1.max()) plt.ylim(xx2.min(), xx2.max()) for idx, cl in enumerate(np.unique(y)): plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8, c=[cmap(idx)], marker=markers[idx], s=80, label=cl) plt.xlabel(data.feature_names[2]) plt.ylabel(data.feature_names[3]) plt.legend(loc='upper left') plt.title(title) return Z 1draw_decision_tree(tree1) 12plot_decision_regions(X, y, tree1, \"Depth 1\")plt.show() 123from sklearn.metrics import confusion_matrixconfusion_matrix(y, tree1.predict(X)) ê²°ê³¼123array([[50, 0, 0], [ 0, 50, 0], [ 0, 50, 0]]) - depth=2ë¡œ ë³€ê²½í•œ í›„ì˜ ê²°ê³¼12tree2 = DecisionTreeClassifier( criterion='entropy', max_depth=2, random_state=0).fit(X, y) 1draw_decision_tree(tree2) 12plot_decision_regions(X, y, tree2, \"Depth 2\")plt.show() 1confusion_matrix(y, tree2.predict(X)) ê²°ê³¼123array([[50, 0, 0], [ 0, 49, 1], [ 0, 5, 45]]) max_depth=3ìœ¼ë¡œ ë³€ê²½í•œ í›„ì˜ ê²°ê³¼ 12tree3 = DecisionTreeClassifier( criterion='entropy', max_depth=3, random_state=0).fit(X, y) 1draw_decision_tree(tree3) 12plot_decision_regions(X, y, tree3, \"Depth 3\")plt.show() 1confusion_matrix(y, tree3.predict(X)) ê²°ê³¼123array([[50, 0, 0], [ 0, 47, 3], [ 0, 1, 49]]) max_depth=5ë¡œ ë³€ê²½í•œ í›„ ê²°ê³¼ 12tree5 = DecisionTreeClassifier( criterion='entropy', max_depth=5, random_state=0).fit(X, y) 1draw_decision_tree(tree5) 12plot_decision_regions(X, y, tree5, \"Depth 5\")plt.show() 1confusion_matrix(y, tree5.predict(X)) ê²°ê³¼123array([[50, 0, 0], [ 0, 49, 1], [ 0, 0, 50]]) 123from sklearn.metrics import classification_reportprint(classification_report(y, tree5.predict(X))) ê²°ê³¼123456789 precision recall f1-score support 0 1.00 1.00 1.00 50 1 1.00 0.98 0.99 50 2 0.98 1.00 0.99 50 accuracy 0.99 150 macro avg 0.99 0.99 0.99 150weighted avg 0.99 0.99 0.99 150 êµì°¨ê²€ì¦ì„ í†µí•´ ìµœì¢…ëª¨í˜•ì˜ ì„±ëŠ¥ì„ ì‚´í´ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤. 123456from sklearn.model_selection import KFold, cross_val_scorecv = KFold(5, shuffle=True, random_state=0)model = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=0)cross_val_score(model, X, y, scoring=\"accuracy\", cv=cv).mean() ê²°ê³¼10.9466666666666667 Regression tree regression treeëŠ” ì¢…ì†ë³€ìˆ˜ê°€ ì—°ì†í˜•ì¸ ê²ƒë§Œ ì œì™¸í–ˆì„ë• ì•„ë˜ì—ì„œ ë³´ëŠ” ê²ƒê³¼ ê°™ì´ ë™ì¼í•œ ê°œë…ì„ ê°€ì§„ë‹¤. Regression TreeëŠ” ì˜ˆì¸¡ì‹œì— ê°ê°ì˜ ì˜ì—­ì— ëŒ€í•´ íŠ¹ì • ì‹¤ìˆ˜ê°’ì„ ì£¼ëŠ” ë°©ì‹ì´ë‹¤. ì•„ë˜ ìˆ˜ì‹ì—ì„œ $c_{m}$ì€ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ í•´ë‹¹ ì˜ì—­ì˜ ë†’ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. classificationì˜ ê²½ìš°ì—ëŠ” entropyë¡œ ì •í–ˆì§€ë§Œ, Regressionì˜ ê²½ìš°ì—ëŠ” ì•„ë˜ 2ë²ˆì§¸ ìˆ˜ì‹ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ê¸°ë³¸ì ì¸ íšŒê·€ì˜ ì„±ëŠ¥ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ê²Œ ëœë‹¤. ì´ë ‡ê²Œ ê²°ì •ëœ ì˜ì—­ì— ì†í•˜ëŠ” ì˜ˆì¸¡ê°’ì€ í•´ë‹¹ ì˜ì—­ì˜ í‰ê· ê°’ì„ ì¶œë ¥í•´ì£¼ë©°, ì˜ì—­ë³„ë¡œ(ì¸µë³„ë¡œ) ì¡´ì¬í•œë‹¤. 12345678910111213141516171819202122from sklearn.tree import DecisionTreeRegressorrng = np.random.RandomState(1)X = np.sort(5 * rng.rand(80, 1), axis=0)y = np.sin(X).ravel()y[::5] += 3 * (0.5 - rng.rand(16))regtree = DecisionTreeRegressor(max_depth=3)regtree.fit(X, y)X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]y_hat = regtree.predict(X_test)# Plot the resultsplt.figure()plt.scatter(X, y, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"ë°ì´í„°\")plt.plot(X_test, y_hat, color=\"cornflowerblue\", linewidth=2, label=\"ì˜ˆì¸¡\")plt.xlabel(\"x\")plt.ylabel(r\"$y$ &amp; $\\hat&#123;y&#125;$\")plt.title(\"íšŒê·€ ë‚˜ë¬´\")plt.legend()plt.show()","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"Support Vector Machine(SVM) - 02","slug":"machine_learning_12","date":"2020-04-25T11:03:51.000Z","updated":"2020-05-14T13:40:10.836Z","comments":true,"path":"2020/04/25/machine_learning_12/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/25/machine_learning_12/","excerpt":"","text":"ì»¤ë„ ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  - SVMì˜ ì‹¬í™”ì  ì´í•´ 12345678910111213np.random.seed(0)X_xor = np.random.randn(200, 2)y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &gt; 0)y_xor = np.where(y_xor, 1, 0)plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1], c='b', marker='o', label='í´ë˜ìŠ¤ 1', s=50)plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1], c='r', marker='s', label='í´ë˜ìŠ¤ 0', s=50)plt.legend()plt.xlabel(\"x1\")plt.ylabel(\"x2\")plt.title(\"XOR ë¬¸ì œ\")plt.show() XOR ë¬¸ì œ ìœ„ì˜ ê·¸ë¦¼ê³¼ê°™ì´ í¼ì…‰íŠ¸ë¡ ì´ë‚˜ ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ê³¼ ê°™ì€ ì„ í˜•íŒë³„í•¨ìˆ˜ ë¶„ë¥˜ ëª¨í˜•ì€ ë‹¤ìŒê³¼ ê°™ì€ XOR(exclusive OR) ë¬¸ì œë¥¼ í’€ì§€ ëª»í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ì´ëŸ¬í•œ ê²½ìš°ì—ëŠ” ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ì„ í˜• íŒë³„í‰ë©´(decision hyperplane)ìœ¼ë¡œ ì˜ì—­ì„ ë‚˜ëˆŒ ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ì¼ë°˜ì ì¸ SVMì„ ì‚¬ìš©í•˜ë©´ XORë¬¸ì œë¥¼ í’€ ìˆ˜ ì—†ë‹¤. 123456789101112131415161718192021def plot_xor(X, y, model, title, xmin=-3, xmax=3, ymin=-3, ymax=3): XX, YY = np.meshgrid(np.arange(xmin, xmax, (xmax-xmin)/1000), np.arange(ymin, ymax, (ymax-ymin)/1000)) ZZ = np.reshape(model.predict( np.array([XX.ravel(), YY.ravel()]).T), XX.shape) plt.contourf(XX, YY, ZZ, cmap=mpl.cm.Paired_r, alpha=0.5) plt.scatter(X[y == 1, 0], X[y == 1, 1], c='b', marker='o', label='í´ë˜ìŠ¤ 1', s=50) plt.scatter(X[y == 0, 0], X[y == 0, 1], c='r', marker='s', label='í´ë˜ìŠ¤ 0', s=50) plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) plt.title(title) plt.xlabel(\"x1\") plt.ylabel(\"x2\")from sklearn.svm import SVCsvc = SVC(kernel=\"linear\").fit(X_xor, y_xor)plot_xor(X_xor, y_xor, svc, \"ì„ í˜• SVC ëª¨í˜•ì„ ì‚¬ìš©í•œ XOR ë¶„ë¥˜ ê²°ê³¼\")plt.show() ë³€í™˜í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ ë¹„ì„ í˜• íŒë³„ ëª¨í˜• ì´ëŸ¬í•œ ê²½ìš° ë„ì›€ì´ ë˜ëŠ” ê²ƒì´ ì›ë˜ì˜ $D$ì°¨ì› ë…ë¦½ ë³€ìˆ˜ ë²¡í„° $x$ ëŒ€ì‹  ë¹„ì„ í˜• í•¨ìˆ˜ë¡œ ë³€í™˜í•œ $M$ì°¨ì› ë²¡í„° $\\phi(x)$ë¥¼ ë…ë¦½ ë³€ìˆ˜ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. \\phi(\\cdot): {R}^D \\rightarrow {R}^Mx=(x_1, x_2, \\cdots, x_D) \\;\\;\\; \\rightarrow \\;\\;\\; \\phi(x) = (\\phi_1(x), \\phi_2(x), \\cdots, \\phi_M(x)) ì•ì„œ XOR ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ìƒí˜¸ ê³± (cross-multiplication) í•­ì„ ì¶”ê°€í•œ ë³€í™˜í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë³´ì. (x_1, x_2) \\;\\;\\; \\rightarrow \\;\\;\\; \\phi(x) = (x_1^2, \\sqrt{2}x_1x_2, x_2^2)12X = np.arange(6).reshape(3, 2)X ê²°ê³¼123array([[0, 1], [2, 3], [4, 5]]) FunctionTransformer ì „ì²˜ë¦¬ í´ë˜ìŠ¤ë¡œ ìœ„ì™€ ë³€í™˜í•¨ìˆ˜ë¥¼ ì´ìš©í•œ ë³€í™˜ì„ í•  ìˆ˜ ìˆë‹¤. 123456from sklearn.preprocessing import FunctionTransformerdef basis(X): return np.vstack([X[:, 0]**2, np.sqrt(2)*X[:, 0]*X[:, 1], X[:, 1]**2]).TFunctionTransformer(basis).fit_transform(X) ê²°ê³¼123array([[ 0. , 0. , 1. ], [ 4. , 8.48528137, 9. ], [16. , 28.28427125, 25. ]]) ìœ„ì™€ ê°™ì€ ë³€í™˜í•¨ìˆ˜ë¥¼ ì¨ì„œ XOR ë¬¸ì œì˜ ë°ì´í„°ë¥¼ ë³€í™˜í•˜ë©´ íŠ¹ì„± $\\phi_2$ë¥¼ ì‚¬ìš©í•˜ì—¬ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 12345678X_xor2 = FunctionTransformer(basis).fit_transform(X_xor)plt.scatter(X_xor2[y_xor == 1, 0], X_xor2[y_xor == 1, 1], c=\"b\", marker='o', s=50)plt.scatter(X_xor2[y_xor == 0, 0], X_xor2[y_xor == 0, 1], c=\"r\", marker='s', s=50)plt.ylim(-6, 6)plt.title(\"ë³€í™˜ ê³µê°„ì—ì„œì˜ ë°ì´í„° ë¶„í¬\")plt.xlabel(r\"$\\phi_1$\")plt.ylabel(r\"$\\phi_2$\")plt.show() ë‹¤ìŒ ì½”ë“œëŠ” Pipelineí´ë˜ìŠ¤ë¡œ ë³€í™˜í•¨ìˆ˜ ì „ì²˜ë¦¬ê¸°ì™€ SVC í´ë˜ìŠ¤ë¥¼ í•©ì¹œ ëª¨í˜•ì˜ ë¶„ë¥˜ ê²°ê³¼ì´ë‹¤. 123456from sklearn.pipeline import Pipelinebasismodel = Pipeline([(\"basis\", FunctionTransformer(basis)), (\"svc\", SVC(kernel=\"linear\"))]).fit(X_xor, y_xor)plot_xor(X_xor, y_xor, basismodel, \"ë³€í™˜í•¨ìˆ˜ SVC ëª¨í˜•ì„ ì‚¬ìš©í•œ XOR ë¶„ë¥˜ ê²°ê³¼\")plt.show() ì»¤ë„ íŠ¸ë¦­ ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ì˜ ê²½ìš° ëª©ì  í•¨ìˆ˜(ë¹„ìš© í•¨ìˆ˜)ì™€ ì˜ˆì¸¡ ëª¨í˜•ì€ ë‹¤ìŒê³¼ ê°™ì´ dual formìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. L = \\sum_{n=1}^N a_n - \\dfrac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^N a_n a_m y_n y_m x_n^T x_my = w^T x - w_0 = \\sum_{n=1}^N a_n y_n x_n^T x - w_0 ì´ ìˆ˜ì‹ì—ì„œ $x$ë¥¼ ë³€í™˜í•¨ìˆ˜ ë³€í™˜ìœ¼ë¡œ $\\phi(x)$ë¡œ ë°”ê¾¸ë©´ ì•„ë˜ì™€ ê°™ì´ ëœë‹¤. ì¦‰, ëª¨ë“  ë³€í™˜í•¨ìˆ˜ëŠ” $\\phi(x_i)^T\\phi(x_j)$ì˜ í˜•íƒœë¡œë§Œ ì‚¬ìš©ë˜ë©° ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ë‘ ê°œì˜ ë³€í™˜ëœ ë…ë¦½ ë³€ìˆ˜ ë²¡í„°ë¥¼ ë‚´ì (inner product)í•œ ê°’ $\\phi(x_i)^T\\phi(x_j)$ë¥¼ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. L = \\sum_{n=1}^N a_n - \\dfrac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^N a_n a_m y_n y_m \\phi(x_n)^T \\phi(x_m)y = w^T x - w_0 = \\sum_{n=1}^N a_n y_n \\phi(x_n)^T \\phi(x) - w_0 ì´ë ‡ê²Œ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¸ ê²ƒì„ ì»¤ë„(kernel)ì´ë¼ê³  í•œë‹¤. ëŒ€ì‘í•˜ëŠ” ë³€í™˜í•¨ìˆ˜ê°€ ì¡´ì¬í•  ìˆ˜ë§Œ ìˆë‹¤ë©´ ë³€í™˜í•¨ìˆ˜ë¥¼ ë¨¼ì € ì •ì˜í•˜ê³  ì»¤ë„ì„ ì •ì˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì»¤ë„ì„ ë¨¼ì € ì •ì˜í•´ë„ ìƒê´€ì—†ë‹¤. ì»¤ë„ì´ ì œ ì—­í• ì„ í•˜ë ¤ë©´ $x_i$ì™€ $x_j$ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ì—¬ì•¼í•œë‹¤. ë˜í•œ ì»¤ë„í•¨ìˆ˜ì—ì„œ ë„ë¡œ ê¸°ì €í•¨ìˆ˜ í¬ë§·ìœ¼ë¡œ ë§Œë“¤ì–´ì§ˆìˆ˜ë„ ìˆì–´ì•¼í•œë‹¤. ì»¤ë„ì˜ ì˜ë¯¸ ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ì˜ ëª©ì  í•¨ìˆ˜ì™€ ì˜ˆì¸¡ ëª¨í˜•ì€ ì»¤ë„ì„ ì‚¬ìš©í•˜ì—¬ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. L = \\sum_{n=1}^N a_n - \\dfrac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^N a_n a_m y_n y_m k(x_n, x_m)y = w^T x - w_0 = \\sum_{n=1}^N a_n y_n k(x_n, x) - w_0 ì»¤ë„ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš° $k(x,y) = x^{T}y$ë¼ëŠ” ì ì„ ê³ ë ¤í•˜ë©´ ì»¤ë„ì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ë³´ì¸ë‹¤. $x$ì™€ $y$ê°€ ë™ì¼í•œ ë²¡í„°ì¼ ë•Œ ê°€ì¥ í¬ê³  ë‘ ë²¡í„°ê°„ì˜ ê±°ë¦¬ê°€ ë©€ì–´ì§ˆìˆ˜ë¡ ì‘ì•„ì§„ë‹¤. ì¦‰, ë‘ í‘œë³¸ ë°ì´í„°ê°„ì˜ ìœ ì‚¬ë„(similarity)ë¥¼ ì¸¡ì •í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ ë³¼ ìˆ˜ë„ ìˆë‹¤. ì»¤ë„ ì‚¬ìš©ì˜ ì¥ì  ì»¤ë„ì„ ì‚¬ìš©í•˜ë©´ basis í•¨ìˆ˜ë¥¼ í•˜ë‚˜ì”© ì •ì˜í•˜ëŠ” ìˆ˜ê³ ë¥¼ ëœ ìˆ˜ ìˆì„ë¿ë”ëŸ¬ ë³€í™˜ê³¼ ë‚´ì ì— ë“¤ì–´ê°€ëŠ” ê³„ì‚°ëŸ‰ì´ ì¤„ì–´ë“ ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì€ ë³€í™˜í•¨ìˆ˜ì˜ ê²½ìš° ì»¤ë„ë°©ë²•ì„ ì“°ì§€ ì•Šì„ ê²½ìš°ì— $\\phi(x_i)^T \\phi(x_j)$ë¥¼ ê³„ì‚°í•˜ë ¤ë©´ $4\\;+\\;4\\;+\\;3 \\;=\\;11$ë²ˆì˜ ê³±ì…ˆì„ í•´ì•¼ í•œë‹¤. $\\phi(x_1)$ ê³„ì‚° : ê³±ì…ˆ 4íšŒ $\\phi(x_2)$ ê³„ì‚° : ê³±ì…ˆ 4íšŒ ë‚´ì  ê³„ì‚° : ê³±ì…ˆ 3íšŒ \\phi(x_i) = \\phi([x_{i,1}, x_{i,2}]) = (x_{i,1}^2, \\sqrt{2}x_{i,1}x_{i,2}, x_{i,2}^2) ê·¸ëŸ°ë° ì´ ë³€í™˜í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì»¤ë„ë¡œ ëŒ€ì²´ê°€ëŠ¥í•˜ë‹¤. \\begin{eqnarray} k(x_1, x_2) &=& (x_1^Tx_2)^2 \\\\ &=& (x_{1,1}x_{2,1} + x_{1,2}x_{2,2})^2 \\\\ &=& x_{1,1}^2x_{2,1}^2 + 2x_{1,1}x_{2,1}x_{1,2}x_{2,2} + x_{1,2}^2y_{2,2}^2 \\\\ &=& (x_{1,1}^2, \\sqrt{2}x_{1,1}x_{1,2}, x_{1,2}^2) (x_{2,1}^2, \\sqrt{2}x_{2,1}x_{2,2}, x_{2,2}^2)^T \\\\ &=& \\phi(x_1)^T \\phi(x_2) \\end{eqnarray} ì»¤ë„ì„ ì‚¬ìš©í•˜ë©´ $\\phi(x_1)^T \\phi(x_2)$ì„ ê³„ì‚°í•˜ëŠ”ë° $2\\;+\\;1\\;=\\;3$ ë²ˆì˜ ê³±ì…ˆì´ë©´ ëœë‹¤. $x_1^Tx_2$: ê³±ì…ˆ 2íšŒ ì œê³± : ê³±ì…ˆ 1íšŒ ì»¤ë„ì˜ í™•ì¥ ìƒì„± ì–´ë–¤ í•¨ìˆ˜ê°€ ì»¤ë„í•¨ìˆ˜ê°€ ëœë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•˜ê¸° ìœ„í•´ì„œëŠ” ë³€í™˜í•¨ìˆ˜ë¥¼ í•˜ë‚˜ í•˜ë‚˜ ì •ì˜í•  í•„ìš”ì—†ì´ ë³€í™˜í•¨ìˆ˜ì˜ ë‚´ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒë§Œ ì¦ëª…í•˜ë©´ ëœë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë‹¤ìŒ ê·œì¹™ì„ ì´ìš©í•˜ë©´ ì´ë¯¸ ë§Œë“¤ì–´ì§„ ì»¤ë„ $k_1(x_1, x_2), k_2(x_1, x_2)$ë¡œ ë¶€í„° ìƒˆë¡œìš´ ì»¤ë„ì„ ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì»¤ë„í•¨ìˆ˜ë¥¼ ì–‘ìˆ˜ë°°í•œ í•¨ìˆ˜ëŠ” ì»¤ë„í•¨ìˆ˜ì´ë‹¤. k(x_1, x_2) = ck_1(x_1, x_2)\\;\\;(c > 0) ì»¤ë„í•¨ìˆ˜ì— ì–‘ìˆ˜ì¸ ìƒìˆ˜ë¥¼ ë”í•œ í•¨ìˆ˜ëŠ” ì»¤ë„ í•¨ìˆ˜ì´ë‹¤. k(x_1, x_2) = k_1(x_1, x_2) + c\\;\\;(c > 0) ë‘ ì»¤ë„í•¨ìˆ˜ë¥¼ ë”í•œ í•¨ìˆ˜ëŠ” ì»¤ë„í•¨ìˆ˜ì´ë‹¤. k(x_1, x_2) = k_1(x_1, x_2) + k_2(x_1, x_2) ë‘ ì»¤ë„í•¨ìˆ˜ë¥¼ ê³±í•œ í•¨ìˆ˜ëŠ” ì»¤ë„í•¨ìˆ˜ì´ë‹¤. k(x_1, x_2) = k_1(x_1, x_2)k_2(x_1, x_2) ì»¤ë„í•¨ìˆ˜ë¥¼ $x\\geq0$ì—ì„œ ë‹¨ì¡°ì¦ê°€(monotonically increasing)í•˜ëŠ” í•¨ìˆ˜ì— ì ìš©í•˜ë©´ ì»¤ë„í•¨ìˆ˜ì´ë‹¤. k(x_1, x_2) = (k_1(x_1, x_2))^n \\;\\; (n=1, 2, \\cdots)k(x_1, x_2) = \\exp(k_1(x_1, x_2))k(x_1, x_2) = \\text{sigmoid}(k_1(x_1, x_2)) $x_{1}, x_{2}$ ê°ê°ì˜ ì»¤ë„í•¨ìˆ˜ê°’ì˜ ê³±ë„ ì»¤ë„í•¨ìˆ˜ì´ë‹¤. k(x_1, x_2) = k_1(x_1, x_1)k_2(x_2, x_2)ë§ì´ ì‚¬ìš©ë˜ëŠ” ì»¤ë„ ë‹¤ìŒê³¼ ê°™ì€ ì»¤ë„ë“¤ì´ ë§ì´ ì‚¬ìš©ë˜ëŠ” ì»¤ë„ë“¤ì´ë‹¤. ì´ ì»¤ë„ë“¤ì€ ëŒ€ë¶€ë¶„ ë³€í™˜í•¨ìˆ˜ë¡œ ë³€í™˜í•˜ì˜€ì„ ë•Œ ë¬´í•œëŒ€ì˜ ì°¨ì›ì„ ê°€ì§€ëŠ” ë³€í™˜í•¨ìˆ˜ê°€ ëœë‹¤. ë”°ë¼ì„œ ëŒ€ë¶€ë¶„ì˜ ë¹„ì„ í˜•ì„±ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. ë¹„êµë¥¼ ìœ„í•´ ì„ í˜• ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ì˜ ê²½ìš°ë„ ì¶”ê°€í•˜ì˜€ë‹¤. ì„ í˜• ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  k(x_1, x_2) = x_1^Tx_2 ë‹¤í•­ ì»¤ë„ (Polynomial Kernel) k(x_1, x_2) = (\\gamma (x_1^Tx_2) + \\theta)^d RBF(Radial Basis Function) ë˜ëŠ” ê°€ìš°ì‹œì•ˆ ì»¤ë„(Gaussian Kernel) - $\\gamma = \\frac{1}{2\\sigma^{2}}$ì¸ ê²½ìš° ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ë”°ë¥´ê²Œ ëœë‹¤. k(x_1, x_2) = \\exp \\left( -\\gamma ||x_1-x_2||^2 \\right) ì‹œê·¸ëª¨ì´ë“œ ì»¤ë„ (Sigmoid Kernel) k(x_1, x_2) = \\tanh(\\gamma (x_1^Tx_2) + \\theta) ì•ì—ì„œ ì‚¬ìš©í•œ ë³€í™˜í•¨ìˆ˜ëŠ” $\\gamma \\;=\\;1,,\\; \\theta\\;=\\;0, \\;d\\;=\\;2$ì¸ ë‹¤í•­ ì»¤ë„ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ë‹¤í•­ ì»¤ë„ ë‹¤í•­ ì»¤ë„ì€ ë²¡í„°ì˜ ë‚´ì ìœ¼ë¡œ ì •ì˜ëœ ì»¤ë„ì„ í™•ì¥í•˜ì—¬ ë§Œë“  ì»¤ë„ì´ë‹¤. ì•„ë˜ì—ì„œ ë‹¤í•­ ì»¤ë„ì´ ì–´ë–¤ ë³€í™˜í•¨ìˆ˜ë¡œ ë˜ì–´ ìˆëŠ”ì§€ ì•Œì•„ë³¼ ê²ƒì´ë‹¤. ê°„ë‹¨í•œ ê²½ìš°ë¡œ $\\gamma \\;=\\;1,,\\; \\theta\\;=\\;1, \\;d\\;=\\;4$ì´ê³  $x$ê°€ ìŠ¤ì¹¼ë¼ì¸ ê²½ìš°ì—ëŠ” ì•„ë˜ì™€ ê°™ìœ¼ë©°, ë§ˆì§€ë§‰ ìˆ˜ì‹ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ ë³€í™˜í•¨ìˆ˜ì˜ ë‚´ì ì´ ëœë‹¤. \\begin{eqnarray} k(x_1, x_2) &=& (x_1^Tx_2 + 1)^4 \\\\ &=& x_1^4x_2^4 + 4x_1^3x_2^3 + 6x_1^2x_2^2 + 4x_1x_2 + 1 \\\\ &=& (x_1^4, 2x_1^3, \\sqrt{6}x_1, 2x_1, 1)^T (x_2^4, 2x_2^3, \\sqrt{6}x_2, 2x_2, 1) \\ \\\\ \\end{eqnarray} ì¦‰, ë³€í™˜í•¨ìˆ˜ëŠ” ë‹¤ìŒ 5ê°œê°€ ëœë‹¤. \\begin{eqnarray} \\phi_1(x) &=& x^4 \\\\ \\phi_2(x) &=& 2x^3 \\\\ \\phi_3(x) &=& \\sqrt{6}x^2 \\\\ \\phi_4(x) &=& 2x \\\\ \\phi_5(x) &=& 1 \\\\ \\end{eqnarray}12345678910111213141516171819202122x1 = 1x2 = np.linspace(-3, 3, 100)def poly4(x1, x2): return (x1 * x2 + 1) ** 4plt.figure(figsize=(8, 4))plt.subplot(121)plt.plot(x2, poly4(x1, x2), ls=\"-\")plt.xlabel(\"x2\")plt.title(\"4ì°¨ ë‹¤í•­ì»¤ë„ì˜ ì˜ˆ\")plt.subplot(122)plt.plot(x2, x2 ** 4)plt.plot(x2, 2 * x2 ** 3)plt.plot(x2, np.sqrt(6) * x2 ** 2)plt.plot(x2, 2 * x2)plt.plot(x2, np.ones_like(x2))plt.xlabel(\"x2\")plt.title(\"4ì°¨ ë‹¤í•­ì»¤ë„ì˜ ë³€í™˜í•¨ìˆ˜ë“¤\")plt.show() RBF ì»¤ë„ RBF ì»¤ë„ì€ ê°€ìš°ì‹œì•ˆ ì»¤ë„ì´ë¼ê³ ë„ í•œë‹¤. ë¬¸ì œë¥¼ ê°„ë‹¨í•˜ê²Œ í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ê°€ì •í•  ê²ƒì´ë‹¤. \\gamma=\\frac{1}{2}\\|x_1\\| = \\|x_2\\| = 1 ê·¸ëŸ¬ë©´ RBF ì»¤ë„ì€ ì•„ë˜ì™€ ê°™ì€ ì°¨ìˆ˜ê°€ ë¬´í•œëŒ€ì¸ ë‹¤í•­ì»¤ë„ê³¼ ê°™ì•„ì§„ë‹¤. \\begin{eqnarray} k(x_1, x_2) &=& \\exp{\\left(-\\frac{||x_1 - x_2||^2}{2}\\right)} \\\\ &=& \\exp{\\left(-\\frac{x_1^Tx_1}{2} - \\frac{x_2^Tx_2}{2} + 2x_1^Tx_2 \\right)} \\\\ &=& \\exp{\\left(-\\frac{x_1^Tx_1}{2}\\right)}\\exp{\\left(-\\frac{x_2^Tx_2}{2}\\right)}\\exp{(x_1^Tx_2)} \\\\ &=& C \\exp{(x_1^Tx_2)} \\\\ &\\approx& C \\left( 1 + (x_1^Tx_2) + \\dfrac{1}{2!}(x_1^Tx_2)^2 + \\dfrac{1}{3!}(x_1^Tx_2)^3 + \\cdots \\right) \\\\ \\end{eqnarray}1234567891011121314151617181920212223242526x1 = 0.0x2 = np.linspace(-7, 7, 100)def rbf(x1, x2, gamma): return np.exp(-gamma * np.abs(x2 - x1) ** 2)plt.figure(figsize=(8, 4))plt.subplot(121)plt.plot(x2, rbf(x1, x2, 1), ls=\"-\", label=\"gamma = 1\")plt.plot(x2, rbf(x1, x2, 0.5), ls=\":\", label=\"gamma = 0.5\")plt.plot(x2, rbf(x1, x2, 5), ls=\"--\", label=\"gamma = 5\")plt.xlabel(\"x2 - x1\")plt.xlim(-3, 7)plt.legend(loc=1)plt.title(\"RBF ì»¤ë„\")plt.subplot(122)plt.plot(x2, rbf(-4, x2, 1))plt.plot(x2, rbf(-2, x2, 1))plt.plot(x2, rbf(0, x2, 1))plt.plot(x2, rbf(2, x2, 1))plt.plot(x2, rbf(4, x2, 1))plt.xlabel(\"x2\")plt.title(\"RBF ì»¤ë„ì˜ ë³€í™˜í•¨ìˆ˜ë“¤\")plt.show() scikit-learnì˜ ì»¤ë„ SVM scikit-learnì˜ SVM í´ë˜ìŠ¤ëŠ” kernelì¸ìˆ˜ë¥¼ ì§€ì •í•˜ì—¬ ì»¤ë„ì„ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. kernel = &quot;linear&quot; : ì„ í˜• SVM. $k(x_{1},\\;x_{2})\\;=\\;x_{1}^{T}x_{2}$ kernel = &quot;poly&quot; : ë‹¤í•­ ì»¤ë„. $k(x_{1},\\;x_{2})\\;=\\;(\\gamma \\;(x_{1}^{T} x_{2})\\; +\\; \\theta)^{d}$ gamma: $\\gamma$ coef0: $\\theta$ degree: $d$ kernel = &quot;rbf&quot; ë˜ëŠ” kernel = None: RBF ì»¤ë„. $k(x_1, x_2) = \\exp \\left( -\\gamma ||x_1-x_2||^2 \\right)$ \\gamma kernel = &quot;sigmoid&quot; ì‹œê·¸ëª¨ì´ë“œ ì»¤ë„. $k(x_1, x_2) = \\tanh(\\gamma (x_1^Tx_2) + \\theta)$ gamma : $\\gamma$ coef0 : $\\theta$ 12345678910111213polysvc = SVC(kernel=\"poly\", degree=2, gamma=1, coef0=0).fit(X_xor, y_xor)rbfsvc = SVC(kernel=\"rbf\").fit(X_xor, y_xor)sigmoidsvc = SVC(kernel=\"sigmoid\", gamma=2, coef0=2).fit(X_xor, y_xor)plt.figure(figsize=(8, 12))plt.subplot(311)plot_xor(X_xor, y_xor, polysvc, \"ë‹¤í•­ì»¤ë„ SVCë¥¼ ì‚¬ìš©í•œ ë¶„ë¥˜ ê²°ê³¼\")plt.subplot(312)plot_xor(X_xor, y_xor, rbfsvc, \"RBFì»¤ë„ SVCë¥¼ ì‚¬ìš©í•œ ë¶„ë¥˜ ê²°ê³¼\")plt.subplot(313)plot_xor(X_xor, y_xor, sigmoidsvc, \"ì‹œê·¸ëª¨ì´ë“œì»¤ë„ SVCë¥¼ ì‚¬ìš©í•œ ë¶„ë¥˜ ê²°ê³¼\")plt.tight_layout()plt.show() ì»¤ë„ íŒŒë¼ë¯¸í„°ì˜ ì˜í–¥1234567891011plt.figure(figsize=(8, 8))plt.subplot(221)plot_xor(X_xor, y_xor, SVC(kernel=\"rbf\", gamma=2).fit(X_xor, y_xor), \"RBF SVM (gamma=2)\")plt.subplot(222)plot_xor(X_xor, y_xor, SVC(kernel=\"rbf\", gamma=10).fit(X_xor, y_xor), \"RBF SVM (gamma=10)\")plt.subplot(223)plot_xor(X_xor, y_xor, SVC(kernel=\"rbf\", gamma=50).fit(X_xor, y_xor), \"RBF SVM (gamma=50)\")plt.subplot(224)plot_xor(X_xor, y_xor, SVC(kernel=\"rbf\", gamma=100).fit(X_xor, y_xor), \"RBF SVM (gamma=100)\")plt.tight_layout()plt.show() $\\gamma$ì˜ ê°’ì´ ì»¤ì§ˆìˆ˜ë¡ hyperplaneì˜ ê²½ê³„ê°€ ë” ë‘¥ê¸€ì–´ì§€ë©´ì„œ, ê·¸ ê°’ì´ ë„ˆë¬´ ë†’ì•„ì§€ë©´ overfittingì´ ë˜ê²Œ ëœë‹¤. iris ë°ì´í„°ì— ì ìš©12345678910111213141516171819202122232425262728293031323334353637383940414243444546from sklearn.datasets import load_irisfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaleriris = load_iris()X = iris.data[:, [2, 3]]y = iris.targetX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)sc = StandardScaler()sc.fit(X_train)X_train_std = sc.transform(X_train)X_test_std = sc.transform(X_test)X_combined_std = np.vstack((X_train_std, X_test_std))y_combined = np.hstack((y_train, y_test))def plot_iris(X, y, model, title, xmin=-2.5, xmax=2.5, ymin=-2.5, ymax=2.5): XX, YY = np.meshgrid(np.arange(xmin, xmax, (xmax-xmin)/1000), np.arange(ymin, ymax, (ymax-ymin)/1000)) ZZ = np.reshape(model.predict(np.array([XX.ravel(), YY.ravel()]).T), XX.shape) plt.contourf(XX, YY, ZZ, cmap=mpl.cm.Paired_r, alpha=0.5) plt.scatter(X[y == 0, 0], X[y == 0, 1], c='r', marker='^', label='0', s=100) plt.scatter(X[y == 1, 0], X[y == 1, 1], c='g', marker='o', label='1', s=100) plt.scatter(X[y == 2, 0], X[y == 2, 1], c='b', marker='s', label='2', s=100) plt.xlim(xmin, xmax) plt.ylim(ymin, ymax) plt.xlabel(\"ê½ƒìì˜ ê¸¸ì´\") plt.ylabel(\"ê½ƒìì˜ í­\") plt.title(title)model1 = SVC(kernel='linear').fit(X_test_std, y_test)model2 = SVC(kernel='poly', random_state=0, gamma=10, C=1.0).fit(X_test_std, y_test)model3 = SVC(kernel='rbf', random_state=0, gamma=1, C=1.0).fit(X_test_std, y_test)plt.figure(figsize=(8, 12))plt.subplot(311)plot_iris(X_test_std, y_test, model1, \"ì„ í˜• SVC\")plt.subplot(312)plot_iris(X_test_std, y_test, model2, \"ë‹¤í•­ì»¤ë„ SVC\")plt.subplot(313)plot_iris(X_test_std, y_test, model3, \"RBFì»¤ë„ SVM\")plt.tight_layout()plt.show() One Class Support Vector Machine ì¢…ì†ë³€ìˆ˜ê°€ ì—†ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ê³ ë¡œ ë‹¤ ê°™ì€ ë²”ì£¼ì´ë¯€ë¡œ classifying ì •ë³´ê°€ ì—†ë‹¤ëŠ” ê²ƒê³¼ ë™ì¼í•œ ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤. One Class SVMì€ ìš°ë¦¬ê°€ ê°€ì§„ ìë£Œë“¤ì„ ìš”ì•½í•˜ëŠ”ë° ì‚¬ìš©í•œë‹¤. ë§ˆì¹˜ Unsupervised learningì˜ clustering ì²˜ëŸ¼ í™œìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ì›ì„ í™œìš©í•œ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤. ê°„ë‹¨íˆ ë§í•´ì„œ ì›ì•ˆì—ëŠ” ìë£Œê°€ ìˆê³ , ì› ë°”ê¹¥ì€ ìë£Œê°€ ì—†ë‹¤ëŠ” ì‹ìœ¼ë¡œ í™œìš©í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. SVMì—ì„œ ì„œí¬íŠ¸ ë²¡í„°ë¥¼ í†µí•´ $\\beta$ë¥¼ êµ¬í•´ hyperplaneì„ êµ¬í–ˆë“¯ì´ í•´ë‹¹ ì›ì— ê°€ì¥ ê°€ê¹Œìš´ $x_{k}$ ì„œí¬íŠ¸ ë²¡í„°ë¥¼ í†µí•´ $R^{2}$ (ë°˜ì§€ë¥¼)ì„ ê³„ì‚°í•œë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ë°”ë‚˜ë‚˜ëª¨ì–‘ìœ¼ë¡œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì— ì„ì˜ì˜ ë°˜ì§€ë¦„ì„ ê°–ëŠ” ì›ì•ˆì— ë°ì´í„°ë¥¼ ëª¨ë‘ ë„£ê³  ì‹¶ì€ë°, ìµœì†Œí•œì˜ ë°˜ì§€ë¦„ ê°’ì„ ê°–ëŠ” ì›ì„ ì°¾ëŠ” ë¬¸ì œê°€ ëœë‹¤. ë‚´ì ì„ kernelë¡œ ë°”ê¾¼ í›„ polynomial kernelì„ ì‚¬ìš©í•œ ê²½ìš° ì°¨ì›ì´ ë†’ì•„ ì§ˆìˆ˜ë¡ ììœ ë„ë„ ë†’ì•„ì§(ì›ë³´ë‹¤ëŠ” ë°ì´í„° í•˜ë‚˜í•˜ë‚˜ì— ì˜í–¥ì„ ë” ë§ì´ ë°›ìŒ)ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, RBF ì»¤ë„ì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ Cê°’ì´ ë†’ì•„ì§ˆìˆ˜ë¡ í—ˆìš©í•˜ëŠ” errorê°€ ë‚®ì•„ì ¸ support vectorì•ˆì— ë°ì´í„°ë“¤ì´ ì¡´ì¬í•˜ê²Œë˜ë©°, ê°ë§ˆë¥¼ ë‚®ì¶œìˆ˜ë¡($\\sigma$ë¥¼ ë†’ì¼ìˆ˜ë¡) ë‘ ë²¡í„° $x_{i}$ì™€ $x_{j}$ê°„ì˜ ì°¨ì´ì˜ ì •ë„ì— ëœ ë¯¼ê°í•´ ì§€ê¸° ë•Œë¬¸ì— ëª¨ì–‘ì´ ë‹¨ìˆœí•´ ì§€ê²Œ ëœë‹¤. SVR ì¢…ì†ë³€ìˆ˜ê°€ ë²”ì£¼í˜•ì´ ì•„ë‹Œ ì—°ì†í˜•ì¸ ê²½ìš°ì—ëŠ” SVRì„ ì‚¬ìš©í•´ì•¼ í•œë‹¤. ì•„ë˜ ìˆ˜ì‹ê³¼ ê°™ì´ marginê³¼ hyperplaneì„ ì •ì˜í•˜ëŠ” ê²ƒì€ ë™ì¼í•˜ë‹¤. ì´ë²ˆì—ëŠ” margin ë°–ì— ì¡´ì¬í•˜ëŠ” ë°ì´í„°ë“¤ê³¼ decision boundaryì™€ì˜ ì°¨ì´(error)ë¥¼ ìµœì†Œí™” í•˜ë„ë¡í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë™ì‘í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ margin ì•ˆì— ë§ì€ ë°ì´í„°ë¥¼ í¬í•¨í•˜ê³  ìˆì„ ìˆ˜ë¡ errorê°€ ìµœì†Œí™” ë  ê²ƒì´ë‹¤. ê²°ê³¼ì ìœ¼ë¡  ì•„ë˜ì™€ ê°™\u001f\u001dì€ ëª©ì í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ”ë° ë§ˆì§€ë§‰ ìˆ˜ì‹ê³¼ ê°™ì´ ì§ê´€ì ìœ¼ë¡œ marginê³¼ì˜ ì°¨ì´ë§Œì„ errorë¡œ ë³´ëŠ” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. í—ˆë‚˜, ì´ì „ ê·¸ë¦¼ì—ì„œì˜ errorë¥¼ ì •ì˜í•˜ëŠ”ë° ìˆ˜í•™ì ì¸ ê³„ì‚°ì— ìš©ì´ì„±ì„ ë”í•˜ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ errorë¥¼ ê³„ì‚°í•œë‹¤. ì•ì—ì„œì™€ ê°™ì´ ë¯¸ë¶„ë¶ˆê°€ëŠ¥í•œ ì ì´ ì—†ê³  ê³„ì‚°í•˜ê¸° í¸í•˜ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. SVMê³¼ ë§ˆì°¬ê°€ì§€ë¡œ SVRë„ ì»¤ë„ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ê³¡ì„ ì˜ í˜•íƒœë¡œ fittingí•  ìˆ˜ ìˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ linear modelì„ ì‚¬ìš©í–ˆì„ ë•ŒëŠ” ì „ë°˜ì ì¸ íŒ¨í„´ì„ ì¡ì•„ì£¼ê³  ë‹¤ë¥¸ ì»¤ë„ë“¤ì— ëŒ€í•´ì„œëŠ” ê° ì»¤ë„ì˜ íŠ¹ì„±ì— ë§ê²Œ ì í•©ì‹œì¼œ ì¤€ë‹¤.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"Support Vector Machine(SVM) - 01","slug":"machine_learning_11","date":"2020-04-22T09:04:12.000Z","updated":"2020-04-28T19:34:21.024Z","comments":true,"path":"2020/04/22/machine_learning_11/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/22/machine_learning_11/","excerpt":"","text":"Support Vector Machine(SVM) ë°ì´í„°ì˜ ë¶„í¬ê°€ ì •ê·œë¶„í¬ë¥¼ ëˆë‹¤ê³  ë³´ì´ì§€ ì•ŠëŠ”ë‹¤ë©´ ì´ì „ì— ë§í–ˆë˜ LDAë‚˜ QDAë¥¼ ì‚¬ìš©í•˜ê¸° í˜ë“¤ë‹¤. ì´ëŸ° ê²½ìš° í´ë˜ìŠ¤ê°„ ë¶„ë¥˜ë¥¼ í•˜ë ¤ê³  í•  ë•Œ ì‚¬ìš©ë  ìˆ˜ ìˆëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ê°€ SVMì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ í´ë˜ìŠ¤ ì§‘ë‹¨ì„ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ” ë²¡í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœëŒ€í•œì˜ ë§ˆì§„ì„ ì£¼ì–´ ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‘ í´ë˜ìŠ¤ ì§‘ë‹¨ê°„ì˜ ë°ì´í„° ë¶„í¬ê°€ í˜¼ìš©ë˜ì–´ìˆë‹¤ë©´ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•´ì•¼ í• ê¹Œ? ë‘ í´ë˜ìŠ¤ ì§‘ë‹¨ê°„ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í™”í•˜ë©´ì„œ í˜¼ìš©ë˜ì–´ìˆëŠ” ë°ì´í„°ë“¤ì— ëŒ€í•œ errorë¥¼ ì ë‹¹íˆ í—ˆìš©í•˜ëŠ” ì„ ì—ì„œ decision boundaryë¥¼ ê²°ì •í•´ì•¼ í•  ê²ƒì´ë‹¤. SVMì€ regression ë¬¸ì œë„ ì‚¬ìš©í•˜ì§€ë§Œ ë³´í†µì€ ë²”ì£¼í˜• ë³€ìˆ˜ì— ëŒ€í•œ classification ë¬¸ì œì— ë§ì´ ì‚¬ìš©í•œë‹¤. Support vector regression(SVR)ì€ ìµœëŒ€í•œ ë§ì€ ë°ì´í„°ë¥¼ margin ì•ˆì— í¬í•¨í•˜ê³ ì í•˜ëŠ” ê²ƒì´ë‹¤. ì´ì— ëŒ€í•´ margin ë°”ê¹¥ì— ì¡´í•´í•˜ëŠ” ë°ì´í„°ì— ëŒ€í•´ errorë¥¼ ì¤„ì–´ì„œ ê·¸ errorë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ íšŒê·€ë¥¼ ì§„í–‰í•œë‹¤. ì¼ë°˜ì ì¸ Regressionì€ í•´ë‹¹ ë°ì´í„°ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì„ ì— ëŒ€í•´ errorë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹ì´ë¼ë©´, SVRì€ margin ë°”ê¹¥ì— ì¡´ì¬í•˜ëŠ” ë°ì´í„°ë“¤ì— ëŒ€í•´ì„œë§Œ errorë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ˆí‰ë©´ì— ë¶€ë“±í˜¸ë¥¼ ë„ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì˜ì—­ìœ¼ë¡œ ë°ì´í„°ë¥¼ êµ¬ë¶„ì§€ì„ ìˆ˜ ìˆê²Œ ëœë‹¤. ë‚˜ê·¸ë‘ì£¼ ìŠ¹ìˆ˜(Lagrange multiplier) ìµœì í™” ë¬¸ì œ(ì˜ˆë¥¼ ë“¤ì–´ì„œ ê·¹ëŒ€ê°’ì´ë‚˜ ê·¹ì†Œê°’)ë¥¼ í‘¸ëŠ”ë° íŠ¹ì •ì¡°ê±´í•˜ì—ì„œ ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë³´ë©´, ì•„ë¬´ëŸ° ì œí•œì´ ì—†ì—ˆë‹¤ë©´ xì™€ yì˜ ê°’ì— ë”°ë¼ $-\\infty$ì—ì„œ $\\infty$ë¡œ ì›€ì§ì¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. í—ˆë‚˜, $g(x,y)=c$ë¼ëŠ” í•¨ìˆ˜ ë²”ìœ„ ë‚´ì—ì„œë§Œ ì›€ì§ì¼ ìˆ˜ ìˆë‹¤ê³  ì œí•œì„ ì£¼ë©´ í•´ë‹¹ ì œí•œì˜ì—­í•˜ì—ì„œì˜ ìµœì í™”ë¥¼ í’€ì–´ì•¼í•  ê²ƒì´ë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í•˜ëŠ” ìµœì í™” ë¬¸ì œë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì‚´í´ë³´ë ¤ë©´, ì•„ë˜ì™€ ê°™ì´ ìµœì í™”ë¬¸ì œì— ëŒ€í•œ ì„¤ëª…ì´ í•„ìš”í•˜ë‹¤. ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œ ì œí•œì¡°ê±´(constraint)ì„ ê°€ì§€ëŠ” ìµœì í™” ë¬¸ì œë¥¼ í’€ì–´ë³¸ë‹¤. ì œí•œ ì¡°ê±´ì€ ì—°ë¦½ë°©ì ì‹ ë˜ëŠ” ì—°ë¦½ë¶€ë“±ì‹ì´ë‹¤. ì—°ë¦½ë°©ì •ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ìµœì í™” ë¬¸ì œë¥¼ í’€ì–´ì•¼ í•œë‹¤. ì—°ë¦½ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì˜ ê²½ìš°ì—ëŠ” KKTì¡°ê±´ì´ë¼ëŠ” ê²ƒì„ ë§Œì¡±í•˜ë„ë¡ í•˜ëŠ” ë³µì¡í•œ ê³¼ì •ì„ ê±°ì³ì•¼ í•œë‹¤. ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œ í˜„ì‹¤ì˜ ìµœì í™” ë¬¸ì œì—ì„œëŠ” ì—¬ëŸ¬ê°€ì§€ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™”(constrained optimization) ë¬¸ì œê°€ ë§ë‹¤. ê°€ì¥ ê°„ë‹¨í•œ ê²½ìš°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì—°ë¦½ë°©ì •ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ê²½ìš°ë‹¤. ë“±ì‹(equality)ì œí•œ ì¡°ê±´ì´ë¼ê³ ë„ í•œë‹¤. \\begin{align} x^{\\ast} = \\text{arg} \\min_x f(x) \\end{align}\\begin{align} x \\in \\mathbf{R}^N \\end{align}\\begin{align} g_j(x) = 0 \\;\\; (j=1, \\ldots, M) \\end{align} ì²« ë²ˆì§¸ ì‹ë§Œ ë³´ë©´ ë‹¨ìˆœíˆ ëª©ì í•¨ìˆ˜ $f(x)$ë¥¼ ê°€ì¥ ì‘ê²Œ í•˜ëŠ” Nì°¨ì› ë²¡í„° xê°’ì„ ì°¾ëŠ” ë¬¸ì œë‹¤. í•˜ì§€ë§Œ ë§ˆì§€ë§‰ ì‹ì— ìˆëŠ” Mê°œì˜ ë“±ì‹ ì œí•œ ì¡°ê±´ì´ ìˆìœ¼ë©´ Mê°œ ì—°ë¦½ ë°©ì •ì‹ì„ ë™ì‹œì— ëª¨ë‘ ë§Œì¡±ì‹œí‚¤ë©´ì„œ ëª©ì í•¨ìˆ˜ $f(x)$ë¥¼ ê°€ì¥ ì‘ê²Œí•˜ëŠ” $x$ê°’ì„ ì°¾ì•„ì•¼ í•œë‹¤. \\begin{align} \\begin{aligned} g_1(x) &= 0 \\\\ g_2(x) &= 0 \\\\ &\\vdots \\\\ g_M(x) &= 0 \\\\ \\end{aligned} \\end{align}ì˜ˆì œ ëª©ì  í•¨ìˆ˜ $f$\u001cì™€ ë“±ì‹ ì œí•œì¡°ê±´ $g$ê°€ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ë¥¼ ìƒê°í•˜ì. ì´ ë¬¸ì œëŠ” ë‹¤ìŒ ê·¸ë¦¼ ì²˜ëŸ¼ $g(x_{1}, x_{2}) = 0$ìœ¼ë¡œ ì •ì˜ë˜ëŠ” ì§ì„ ìƒì—ì„œ ê°€ì¥ $f(x_{1},x_{2})$ê°’ì´ ì‘ì•„ì§€ëŠ” ì  $(x_1^{\\ast}, x_2^{\\ast})$ì„ ì°¾ëŠ” ë¬¸ì œê°€ ëœë‹¤. \\begin{align} f(x_1, x_2) = x_1^2 + x_2^2 \\end{align}\\begin{align} g(x_1, x_2) = x_1 + x_2 - 1 = 0 \\end{align}12345678910111213141516171819202122232425# ëª©ì í•¨ìˆ˜ f(x) = x1^2 + x2^2def f1(x1, x2): return x1 ** 2 + x2 ** 2x1 = np.linspace(-5, 5, 100)x2 = np.linspace(-3, 3, 100)X1, X2 = np.meshgrid(x1, x2)Y = f1(X1, X2)# ë“±ì‹ ì œí•œì¡°ê±´ ë°©ì •ì‹ g(x) = x1 + x2 - 1 = 0x2_g = 1 - x1plt.contour(X1, X2, Y, colors=\"gray\", levels=[0.5, 2, 8, 32])plt.plot(x1, x2_g, 'g-')plt.plot([0], [0], 'rP')plt.plot([0.5], [0.5], 'ro', ms=10)plt.xlim(-5, 5)plt.ylim(-3, 3)plt.xticks(np.linspace(-4, 4, 9))plt.xlabel(\"$x_1$\")plt.ylabel(\"$x_2$\")plt.title(\"ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œ\")plt.show() ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²• ì´ë ‡ê²Œ ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œëŠ” ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•(Lagrange multiplier)ì„ ì‚¬ìš©í•˜ì—¬ ìµœì í™”í•  ìˆ˜ ìˆë‹¤. ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ ë°©ë²•ì—ì„œëŠ” ëª©ì í•¨ìˆ˜ë¥¼ ì›ë˜ì˜ ëª©ì í•¨ìˆ˜ $f(x)$ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ëŒ€ì‹  ì œí•œì¡°ê±´ ë“±ì‹ì— $\\lambda$ë¼ëŠ” ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ê³±í•´ì„œ ë”í•œ í•¨ìˆ˜ë¥¼ ëª©ì í•¨ìˆ˜ë¡œ ê°„ì£¼í•˜ì—¬ ìµœì í™”í•œë‹¤. \\begin{align} \\begin{aligned} h(x, \\lambda) &= h(x_1, x_2, \\ldots , x_N, \\lambda_1, \\ldots , \\lambda_M) \\\\ &= f(x) + \\sum_{j=1}^M \\lambda_j g_j(x) \\end{aligned} \\end{align} ì´ë•Œ ì œí•œì¡°ê±´ ë“±ì‹ í•˜ë‚˜ë§ˆë‹¤ ìƒˆë¡œìš´ $\\lambda_{i}$ë¥¼ ì¶”ê°€í•´ì£¼ì–´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ë§Œì•½ ì œí•œì¡°ê±´ì´ $M$ê°œì´ë©´ $\\lambda_{1}, \\cdots, \\lambda_{M}$ê°œì˜ ë³€ìˆ˜ê°€ ìƒˆë¡œ ìƒê¸´ ê²ƒê³¼ ê°™ë‹¤. ì´ë ‡ê²Œ í™•ì¥ëœ ëª©ì í•¨ìˆ˜ $h$ëŠ” ì…ë ¥ë³€ìˆ˜ê°€ ë” ëŠ˜ì–´ë‚¬ê¸° ë•Œë¬¸ì— ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ë¥¼ ì˜ë²¡í„°ë¡œ ë§Œë“œëŠ” ìµœì í™” í•„ìš” ì¡°ê±´ì´ ë‹¤ìŒì²˜ëŸ¼ $N\\;+\\;M$ê°œê°€ ëœë‹¤. \\begin{align} \\begin{aligned} \\dfrac{\\partial h}{\\partial x_1} &= \\dfrac{\\partial f}{\\partial x_1} + \\sum_{j=1}^M \\lambda_j\\dfrac{\\partial g_j}{\\partial x_1} = 0 \\\\ \\dfrac{\\partial h}{\\partial x_2} &= \\dfrac{\\partial f}{\\partial x_2} + \\sum_{j=1}^M \\lambda_j\\dfrac{\\partial g_j}{\\partial x_2} = 0 \\\\ & \\vdots \\\\ \\dfrac{\\partial h}{\\partial x_N} &= \\dfrac{\\partial f}{\\partial x_N} + \\sum_{j=1}^M \\lambda_j\\dfrac{\\partial g_j}{\\partial x_N} = 0 \\\\ \\dfrac{\\partial h}{\\partial \\lambda_1} &= g_1 = 0 \\\\ & \\vdots \\\\ \\dfrac{\\partial h}{\\partial \\lambda_M} &= g_M = 0 \\end{aligned} \\end{align} ì´ $N\\;+\\;M$ê°œì˜ ì—°ë¦½ ë°©ì •ì‹ì„ í’€ë©´ $N\\;+\\;M$ê°œì˜ ë¯¸ì§€ìˆ˜ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. \\begin{align} x_1, x_2, \\ldots, x_N, , \\lambda_1, \\ldots , \\lambda_M \\end{align} êµ¬í•œ ê²°ê³¼ì—ì„œ ì°¾ëŠ” ìµœì†Œê°’ $x$ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ê°’ì€ í•„ìš”ì—†ë‹¤. ì˜ˆì œ) ìœ„ì—ì„œ ì œì‹œí•œ ì˜ˆì œë¥¼ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ìœ¼ë¡œ í’€ì–´ë³´ì. ìƒˆë¡œìš´ ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} h(x_1, x_2, \\lambda) = f(x_1, x_2) + \\lambda g(x_1, x_2) = x_1^2 + x_2^2 + \\lambda ( x_1 + x_2 - 1 ) \\end{align} ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì„ ì ìš©í•˜ì—¬ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ê°€ ì˜ë²¡í„°ì¸ ìœ„ì¹˜ë¥¼ êµ¬í•œë‹¤. \\begin{align} \\begin{aligned} \\dfrac{\\partial h}{\\partial x_1} &= 2{x_1} + \\lambda = 0 \\\\ \\dfrac{\\partial h}{\\partial x_2} &= 2{x_2} + \\lambda = 0 \\\\ \\dfrac{\\partial h}{\\partial \\lambda} &= x_1 + x_2 - 1 = 0 \\end{aligned} \\end{align} ë°©ì •ì‹ì˜ í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} x_1 = x_2 = \\dfrac{1}{2}, \\;\\;\\; \\lambda = -1 \\end{align} ì—°ìŠµë¬¸ì œ ì œí•œì¡°ê±´ì´ $x_{1}+x_{2}\\; = \\; 1$ì¼ ë•Œ ëª©ì  í•¨ìˆ˜ $f(x) = - log x_{1} - log x_{2} x_{1},x_{2} &gt; 0$ ì„ ìµœì†Œí™”í•˜ëŠ” x_{1}, x_{2}ê°’ì„ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ìœ¼ë¡œ ê³„ì‚°í•˜ë¼. ìœ„ì˜ ë¬¸ì œì—ì„œ ëª©ì €í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. \\begin{align} h(x_1, x_2, \\lambda) = f(x_1, x_2) + \\lambda g(x_1, x_2) = - log x_1 - log x_2 + \\lambda ( x_1 + x_2 - 1 ) \\end{align} ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì€ ì ìš©í•˜ì—¬ ê·¸ë ˆë””ì–¸íŠ¸ ë²¡í„°ê°€ ì˜ë²¡í„°ì¸ ìœ„ì¹˜ë¥¼ êµ¬í•œë‹¤. \\begin{align} \\begin{aligned} \\dfrac{\\partial h}{\\partial x_1} &= \\lambda x_1 -1 = 0 \\\\ \\dfrac{\\partial h}{\\partial x_2} &= \\lambda x_2 -1 = 0 \\\\ \\dfrac{\\partial h}{\\partial \\lambda} &= x_1 + x_2 - 1 = 0 \\end{aligned} \\end{align} ìœ„ ë°©ì •ì‹ì„ í’€ë©´ í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} x_1 = x_2 = \\dfrac{1}{2}, \\;\\;\\; \\lambda = 2 \\end{align}scipyë¥¼ ì‚¬ìš©í•˜ì—¬ ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œ ê³„ì‚°í•˜ê¸° scipyì˜ optimize ì„œë¸ŒíŒ¨í‚¤ì§€ëŠ” ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ” fmin_slsqp()ëª…ë ¹ì„ ì œê³µí•œë‹¤. ëª©ì í•¨ìˆ˜ì™€ ì´ˆê¸°ê°’, ê·¸ë¦¬ê³  ì œí•œì¡°ê±´ í•¨ìˆ˜ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¸ìˆ˜ë¡œ ë°›ëŠ”ë‹¤. ëª©ì í•¨ìˆ˜ëŠ” ë°°ì—´ì¸ ì¸ìˆ˜ë¥¼ ë°›ë„ë¡ êµ¬í˜„ë˜ì–´ì•¼ í•˜ê³  ì œí•œì¡°ê±´ í•¨ìˆ˜ì˜ ê²½ìš°ì—ëŠ” í•­ìƒ eqconsì¸ìˆ˜ë¥¼ ëª…ì‹œí•´ì•¼ í•œë‹¤. 1fmin_slsqp(func_objective, x0, eqcons=[func_constraint1, func_constraint2]) ìœ„ì—ì„œì˜ ë‘ ë¬¸ì œë¥¼ scipyë¥¼ í†µí•´ì„œ í’€ì–´ë³´ê² ë‹¤. 1234567def f1array(x): return x[0] ** 2 + x[1] ** 2def eq_constraint(x): return x[0] + x[1] - 1scipy.optimize.fmin_slsqp(f1array, np.array([1, 1]), eqcons=[eq_constraint]) ê²°ê³¼123456Optimization terminated successfully. (Exit mode 0) Current function value: 0.5000000000000002 Iterations: 2 Function evaluations: 8 Gradient evaluations: 2array([0.5, 0.5]) 1234567def f1array(x): return -np.log(x[0]) -np.log(x[1])def eq_constraint(x): return x[0] + x[1] - 1scipy.optimize.fmin_slsqp(f1array, np.array([1, 1]), eqcons=[eq_constraint]) ê²°ê³¼123456Optimization terminated successfully. (Exit mode 0) Current function value: 1.3862943611198901 Iterations: 2 Function evaluations: 8 Gradient evaluations: 2array([0.5, 0.5]) ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ì˜ ì˜ë¯¸ ë§Œì•½ ìµœì í™” ë¬¸ì œì—ì„œ ë“±ì‹ ì œí•œì¡°ê±´ $g_{i}ê°€ ìˆëŠ”ê°€ ì—†ëŠ”ê°€ì— ë”°ë¼ í•´ì˜ ê°’ì´ ë‹¬ë¼ì§„ë‹¤ë©´ ì´ ë“±ì‹ ì œí•œì¡°ê±´ì— ëŒ€ì‘í•˜ëŠ” ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ $\\lambda_{i}$ëŠ” 0ì´ ì•„ë‹Œ ê°’ì´ì–´ì•¼ í•œë‹¤. $\\lambda_{i} = 0$ì¼ ë•Œë§Œ ì›ë˜ì˜ ë¬¸ì œì™€ ì œí•œì¡°ê±´ì´ ìˆëŠ” ë¬¸ì œì˜ ìµœì í™” ì¡°ê±´ì´ ê°™ì•„ì§€ë¯€ë¡œ ìµœì í™” í•´ì˜ ìœ„ì¹˜ë„ ê°™ê²Œ ë‚˜ì˜¤ê¸° ë•Œë¬¸ì´ë‹¤. \\begin{align} \\lambda_i \\neq 0 \\end{align}ì˜ˆì œ ëª©ì í•¨ìˆ˜ê°€ ì•„ë˜ì™€ ê°™ì€ ìµœì†Œí™” ë¬¸ì œì˜ ë‹µì€ $x_{1} = x_{2} = 0$ì´ë‹¤. \\begin{align} f(x) = x_1^2 + x_2^2 \\end{align} ì—¬ê¸°ì— ë‹¤ìŒ ì œí•œ ì¡°ê±´ì´ ìˆë‹¤ê³  í•˜ì. \\begin{align} g(x_1, x_2) = x_1 + x_2 = 0 \\end{align} ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì—ì„œ ìƒˆë¡œìš´ ëª©ì í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. \\begin{align} h(x_1, x_2, \\lambda) = f(x_1, x_2) + \\lambda g(x_1, x_2) = x_1^2 + x_2^2 + \\lambda ( x_1 + x_2) \\end{align} ì´ì— ë”°ë¥¸ ìµœì í™” ì¡°ê±´ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} \\begin{aligned} \\dfrac{\\partial h}{\\partial x_1} &= 2{x_1} + \\lambda = 0 \\\\ \\dfrac{\\partial h}{\\partial x_2} &= 2{x_2} + \\lambda = 0 \\\\ \\dfrac{\\partial h}{\\partial \\lambda} &= x_1 + x_2 = 0 \\end{aligned} \\end{align} ì´ì— ëŒ€í•œ í•´ëŠ” $x_{1} = x_{2} = \\lambda = 0$ìœ¼ë¡œ ì œí•œì¡°ê±´ì´ ìˆìœ¼ë‚˜ ì—†ìœ¼ë‚˜ í•´ëŠ” ë™ì¼í•˜ë©°, ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ëŠ” 0ì´ ëœë‹¤. ì¦‰, ì œí•œì¡°ê±´ì´ ì˜ë¯¸ê°€ ì—†ëŠ” ê²½ìš°ëŠ” ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ê°€ 0ì´ëœë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œ ì´ë²ˆì—ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¶€ë“±ì‹(inequality) ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œë¥¼ ìƒê°í•˜ì. \\begin{align} x^{\\ast} = \\text{arg} \\min_x f(x) \\end{align}\\begin{align} x \\in \\mathbf{R}^N \\end{align}\\begin{align} g_j(x) \\leq 0 \\;\\; (j=1, \\ldots, M) \\end{align} ë§Œì•½ ë¶€ë“±ì‹ì´ $g_j(x) \\geq 0$ê³¼ ê°™ë‹¤ë©´ ì–‘ë³€ì— $-1$ì„ ê³±í•˜ì—¬ ë¶€ë“±í˜¸ì˜ ë°©í–¥ì„ ë°”ê¾¼ë‹¤. ì´ë ‡ê²Œ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œë„ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ ë°©ë²•ê³¼ ëª©ì í•¨ìˆ˜ë¥¼ ë‹¤ìŒì²˜ëŸ¼ ë°”ê¾¸ì–´ í‘¼ë‹¤. ì´ë ‡ê²Œ ë¶€ë“±ì‹ ì œí•œ ì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œë„ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ ë°©ë²•ê³¼ ëª©ì í•¨ìˆ˜ë¥¼ ë‹¤ìŒì²˜ëŸ¼ ë°”ê¾¸ì–´ í‘¼ë‹¤. \\begin{align} h(x, \\lambda) = f(x) + \\sum_{j=1}^M \\lambda_j g_j(x) \\end{align}ë‹¤ë§Œ, ì´ ê²½ìš° ìµœì í™” í•´ì˜ í•„ìš”ì¡°ê±´ì€ ë°©ì •ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œì™€ ë‹¤ë¥´ê²Œ KKT(Karush-Kuhn-Tucker)ì¡°ê±´ì´ë¼ê³  í•˜ë©° ë‹¤ìŒì²˜ëŸ¼ 3ê°œì˜ ì¡°ê±´ìœ¼ë¡œ ì´ë£¨ì–´ì§„ë‹¤. 1) ëª¨ë“  ë…ë¦½ë³€ìˆ˜ $x_{1}, x_{2}, \\ldots, \\x_{N}$ì— ëŒ€í•œ ë¯¸ë¶„ê°’ì´ 0ì´ë‹¤. ì²« ë²ˆì§¸ ì¡°ê±´ì€ ë°©ì •ì‹ ì œí•œì¡°ê±´ì˜ ê²½ìš°ì™€ ê°™ë‹¤. ë‹¤ë§Œ ë³€ìˆ˜ $x$ë“¤ì— ëŒ€í•œ ë¯¸ë¶„ê°’ë§Œ 0ì´ì–´ì•¼ í•œë‹¤. ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ $\\lambda$ì— ëŒ€í•œ ë¯¸ë¶„ì€ 0ì´ ì•„ë‹ˆì–´ë„ ëœë‹¤. \\begin{align} \\dfrac{\\partial h(x, \\lambda)}{\\partial x_i} = 0 \\end{align} 2) ëª¨ë“  ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ $\\lambda_{1}, \\lambda_{2}, \\ldots, \\lambda_{M}$ê³¼ ì œí•œì¡°ê±´ ë¶€ë“±ì‹($\\lambda$ì— ëŒ€í•œ ë¯¸ë¶„ê°’)ì˜ ê³±ì´ 0ì´ë‹¤. ë‘ ë²ˆì§¸ ì¡°ê±´ì„ ë³´ë©´ í™•ì¥ëœ ëª©ì í•¨ìˆ˜ë¥¼ ë‚˜ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë¡œ ë¯¸ë¶„í•œ ê°’ì€ ë³€ìˆ˜ $x$ë“¤ì— ëŒ€í•œ ë¯¸ë¶„ê°’ê³¼ëŠ” ë‹¬ë¦¬ ë°˜ë“œì‹œ 0ì´ ë  í•„ìš”ëŠ” ì—†ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ í•˜ë ¤ë©´ ë‘ ê²½ìš°ê°€ ê°€ëŠ¥í•œë° ë“±ì‹ ì œí•œì¡°ê±´ì˜ ê²½ìš°ì²˜ëŸ¼ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ $\\lambda$ì— ëŒ€í•œ ë¯¸ë¶„ê°’ì´ 0ì´ì–´ë„ ë˜ê³  ì•„ë‹ˆë©´ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ $\\lambda$ê°’ ìì²´ê°€ 0ì´ ë˜ì–´ë„ ëœë‹¤. \\begin{align} \\lambda_j \\cdot \\dfrac{\\partial h(x, \\lambda)}{\\partial \\lambda_j} = \\lambda_j \\cdot g_j = 0 \\end{align} 3) ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ëŠ” ìŒìˆ˜ê°€ ì•„ë‹ˆì–´ì•¼ í•œë‹¤. ë§ˆì§€ë§‰ ì¡°ê±´ì€ KKT ì¡°ê±´ì´ ì‹¤ì œë¡œ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œì™€ ê°™ì€ ë¬¸ì œì„ì„ ë³´ì¥í•˜ëŠ” ì¡°ê±´ì´ë‹¤. \\begin{align} \\lambda_j \\geq 0 \\end{align}ì˜ˆì œ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì„ ê°€ì§€ëŠ” ìµœì í™”ì˜ ì˜ˆë¥¼ í’€ì–´ë³´ì. ëª©ì í•¨ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. \\begin{align} f(x_1, x_2) = x_1^2 + x_2^2 \\end{align} ì´ ì˜ˆì œì—ì„œ ë‘ ê°€ì§€ ì œí•œ ì¡°ê±´ì„ ê³ ë ¤í•´ ë³¼ í…ë° í•˜ë‚˜ëŠ” ë‹¤ìŒ ê·¸ë¦¼ ì¤‘ ì™¼ìª½ ê·¸ë¦¼ì²˜ëŸ¼ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ì•„ë˜ì™€ ê°™ì€ ê²½ìš°ì´ë‹¤. \\begin{align} g(x_1, x_2) = x_1 + x_2 - 1 \\leq 0 \\end{align} ë‹¤ë¥¸ í•˜ë‚˜ì˜ ì œí•œì¡°ê±´ì€ ì•„ë˜ì™€ ê°™ê³  ì´ì— ëŒ€í•œ ê·¸ë¦¼ì€ ì˜¤ë¥¸ìª½ì— í•´ë‹¹í•œë‹¤. \\begin{align} g(x_1, x_2) = -x_1 - x_2 + 1 \\leq 0 \\end{align} ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì œí•œì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì˜ì—­ì„ ì–´ë‘¡ê²Œ í‘œì‹œí–ˆë‹¤. ìµœì ì ì˜ ìœ„ì¹˜ëŠ” ì ìœ¼ë¡œ í‘œì‹œí–ˆë‹¤. ì²« ë²ˆì§¸ ì œí•œì¡°ê±´ì˜ ê²½ìš°ì—ëŠ” ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆê¸°ëŠ” í•˜ì§€ë§Œ ì›ë˜ì˜ ìµœì í™” ë¬¸ì œì˜ í•´ê°€ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ì œì‹œí•˜ëŠ” ì˜ì—­ ì•ˆì— ìˆê¸° ë•Œë¬¸ì— ìµœì ì ì˜ ìœ„ì¹˜ê°€ ë‹¬ë¼ì§€ì§€ ì•ŠëŠ”ë‹¤. ë‘ ë²ˆì§¸ ì œí•œì¡°ê±´ì˜ ê²½ìš°ì—ëŠ” ì›ë˜ì˜ ìµœì í™” ë¬¸ì œì˜ í•´ê°€ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ì œì‹œí•˜ëŠ” ì˜ì—­ ë°”ê¹¥ì— ìˆê¸° ë•Œë¬¸ì— ìµœì ì ì˜ ìœ„ì¹˜ê°€ ë‹¬ë¼ì¡Œë‹¤. í•˜ì§€ë§Œ ìµœì ì ì˜ ìœ„ì¹˜ê°€ ì˜ì—­ì˜ ê²½ê³„ì„ (boundary line)ì— ìˆë‹¤ëŠ” ì ì— ì£¼ì˜í•˜ë¼. 12345678910111213141516171819202122232425262728plt.figure(figsize=(13, 7))ax1 = plt.subplot(121)plt.contour(X1, X2, Y, colors=\"gray\", levels=[0.5, 2, 8])plt.plot(x1, x2_g, 'g-')ax1.fill_between(x1, -20, x2_g, alpha=0.5)plt.plot([0], [0], 'ro', ms=10)plt.xlim(-3, 3)plt.ylim(-5, 5)plt.xticks(np.linspace(-4, 4, 9))plt.yticks(np.linspace(-5, 5, 11))plt.xlabel(\"$x_1$\")plt.ylabel(\"$x_2$\")plt.title(\"ìµœì í•´ê°€ ë¶€ë“±ì‹ê³¼ ê´€ê³„ì—†ëŠ” ê²½ìš°\")ax2 = plt.subplot(122)plt.contour(X1, X2, Y, colors=\"gray\", levels=[0.5, 2, 8])plt.plot(x1, x2_g, 'g-')ax2.fill_between(x1, 20, x2_g, alpha=0.5)plt.plot([0.5], [0.5], 'ro', ms=10)plt.xlabel(\"x_1\")plt.xlim(-3, 3)plt.ylim(-5, 5)plt.xticks(np.linspace(-4, 4, 9))plt.yticks(np.linspace(-5, 5, 11))plt.xlabel(\"$x_1$\")plt.ylabel(\"$x_2$\")plt.title(\"ìµœì í•´ê°€ ë¶€ë“±ì‹ì— ì˜í•´ ê²°ì •ë˜ëŠ” ê²½ìš°\")plt.suptitle(\"ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œ\")plt.show() ê·¸ë¦¼ì—ì„œ ë³´ë“¯ì´ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œë¥¼ í’€ë©´ ê·¸ ì œí•œì¡°ê±´ì€ ë‹¤ìŒ ë‘ ê°€ì§€ ê²½ìš°ì˜ í•˜ë‚˜ê°€ ë˜ì–´ ë²„ë¦°ë‹¤. ìµœì í™” ê²°ê³¼ì— ì „í˜€ ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ì“¸ëª¨ì—†ëŠ” ì œí•œì¡°ê±´ ìµœì í™” ê²°ê³¼ì— ì˜í–¥ì„ ì£¼ëŠ” ë“±ì‹(equality)ì¸ ì œí•œì¡°ê±´ ì–´ëŠ ê²½ìš°ì´ë“  ë¶€ë“±ì‹ ì œí•œì¡°ê±´ ë¬¸ì œë¡œ ì‹œì‘í–ˆì§€ë§Œ ê²°ê³¼ëŠ” ì œí•œì¡°ê±´ì´ ì—†ê±°ë‚˜ ë“±ì‹ ì œí•œì¡°ê±´ ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒê³¼ ê°™ì•„ì§„ë‹¤. KKTì¡°ê±´ ì¤‘ ë‘ ë²ˆì§¸ ì¡°ê±´ì´ ëœ»í•˜ëŠ” ë°”ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ë‹¤ìŒ ì‹ì—ì„œ $x^{\\ast}, \\lambda^{\\ast}$ëŠ” KKT ì¡°ê±´ì„ í’€ì–´ì„œ êµ¬í•œ ìµœì í•´ì˜ ê°’ì´ë‹¤. \\begin{align} \\lambda^{\\ast} = 0 \\;\\; \\text{or} \\;\\; g(x^{\\ast}) = 0 \\end{align} ë§Œì•½ $g_{i} = 0$ì´ë©´ ì´ ì¡°ê±´ì€ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ì•„ë‹Œ ë“±ì‹ ì œí•œì¡°ê±´ì´ ëœë‹¤. ê·¸ë¦¬ê³  ë“±ì‹ ì œí•œì¡°ê±´ì—ì„œ ë§í•œ ë°”ì™€ ê°™ì´ (ì´ ì œí•œì¡°ê±´ì´ ìˆìœ¼ë‚˜ ì—†ìœ¼ë‚˜ í•´ê°€ ë°”ë€Œì§€ ì•ŠëŠ” íŠ¹ìˆ˜í•œ ê²½ìš°ë¥¼ ì œì™¸í•˜ë©´) ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ëŠ” 0ì´ ì•„ë‹Œê°’ì„ ê°€ì§„ë‹¤. \\begin{align} g_i = 0 \\;\\; \\rightarrow \\;\\; \\lambda_i \\neq 0 \\; (\\lambda_i > 0) \\end{align} ë°˜ëŒ€ë¡œ $g_i \\neq 0 \\; (g_i &lt; 0)$ì´ë©´ í•´ê°€ $g_{i}$ê°€ í‘œí˜„í•˜ëŠ” ê³¡ì„ ìœ¼ë¡œë¶€í„° ë–¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ì•„ë¬´ëŸ° ì˜ë¯¸ê°€ ì—†ì–´ì§„ë‹¤. ì¦‰, ì œí•œì¡°ê±´ì´ ìˆì„ ë•Œì™€ ì—†ì„ ë•Œì˜ í•´ê°€ ê°™ë‹¤. ë”°ë¼ì„œ ëª©ì í•¨ìˆ˜ $h(x,\\lambda)$ëŠ” $\\lambda_{i}g_{i}(g_{i} \\neq 0)$í•­ì´ ìˆìœ¼ë‚˜ ì—†ìœ¼ë‚˜ ìƒê´€ì—†ì´ ê°™ì€ í•´ë¥¼ ê°€ì§„ë‹¤. ë”°ë¼ì„œ $\\lambda_{i} = 0$ì´ ëœë‹¤. \\begin{align} g_i \\neq 0 \\;\\; \\rightarrow \\;\\; \\lambda_i = 0 \\end{align} ë”°ë¼ì„œ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œëŠ” ê° ì œí•œì¡°ê±´ì— ëŒ€í•´ ìœ„ì˜ ë‘ ê°€ì§€ ê²½ìš°ë¥¼ ê°€ì •í•˜ì—¬ ê°ê° í’€ì–´ë³´ë©´ì„œ ìµœì ì˜ ë‹µì„ ì°¾ëŠ”ë‹¤. ì˜ˆì œ) ë‹¤ìŒì€ ë³µìˆ˜ì˜ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ë˜ë‹¤ë¥¸ 2ì°¨ì› ìµœì í™” ë¬¸ì œì˜ ì˜ˆì´ë‹¤. \\begin{align} \\text{arg} \\min_x \\; (x_1-4)^2 + (x_2-2)^2 \\end{align}\\begin{align} g_1(x) = x_1 + x_2 - 1\\leq 0 \\end{align}\\begin{align} g_2(x) = -x_1 + x_2 - 1\\leq 0 \\end{align}\\begin{align} g_3(x) = -x_1 - x_2 - 1\\leq 0 \\end{align}\\begin{align} g_4(x) = x_1 - x_2 - 1\\leq 0 \\end{align} ìœ„ì˜ 4ê°€ì§€ ì œí•œì¡°ê±´ì€ ë‹¤ìŒê³¼ ê°™ì€ í•˜ë‚˜ì˜ ë¶€ë“±ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ë„ ìˆë‹¤. \\begin{align} g(x) = \\left\\vert\\, x_1 \\right\\vert + \\left\\vert\\, x_2 \\right\\vert - 1 = \\sum_{i=1}^{2} \\left\\vert\\, x_i \\right\\vert - 1 \\leq 0 \\end{align} ì•„ë˜ ì˜ˆì œì—ì„œ ìµœì í•´ê°€ $x_{1}\\;=\\;1, x_{2}\\; = \\;0$ì´ë¼ëŠ” ì‚¬ì‹¤ì„ ì´ìš©í•˜ì—¬ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ $\\lambda_{1}, \\lambda_{2}, \\lambda_{3}, \\lambda_{4}$ ì¤‘ ì–´ëŠ ê°’ì´ 0ì´ ë˜ëŠ”ì§€ ë§í•´ë³´ì. ì´ì— ëŒ€í•œ ë‹µì€ $\\lambda_{2} = \\lambda_{3} = 0$ì´ ë  ê²ƒì´ë‹¤. í•´ë¥¼ ì°¾ëŠ”ë° ì•„ë¬´ëŸ° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. 123456789101112131415161718192021222324252627282930313233def f2plt(x1, x2): return np.sqrt((x1 - 4) ** 2 + (x2 - 2) ** 2)x1 = np.linspace(-2, 5, 100)x2 = np.linspace(-1.5, 3, 100)X1, X2 = np.meshgrid(x1, x2)Y = f2plt(X1, X2)plt.contour(X1, X2, Y, colors=\"gray\", levels=np.arange(0.5, 5, 0.5) * np.sqrt(2))# ì œí•œ ì¡°ê±´ì˜ ìƒìˆ˜k = 1ax = plt.gca()x12 = np.linspace(-k, 0, 10)x13 = np.linspace(0, k, 10)ax.fill_between(x12, x12 + k, -k - x12, color='g', alpha=0.5)ax.fill_between(x13, x13 - k, k - x13, color='g', alpha=0.5)# ìµœì ì  ìœ„ì¹˜x1_sol = 1x2_sol = 0plt.plot(x1_sol, x2_sol, 'ro', ms=20)plt.xlim(-2, 5)plt.ylim(-1.5, 3)plt.xticks(np.linspace(-2, 5, 8))plt.yticks(np.linspace(-1, 3, 5))plt.xlabel(\"$x_1$\")plt.ylabel(\"$x_2$\")plt.title(\"$|x_1| + |x_2| \\leq &#123;&#125;$ ì œí•œì¡°ê±´ì„ ê°€ì§€ëŠ” ìµœì í™” ë¬¸ì œ\".format(k))plt.show() Scipyë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œ ê³„ì‚°í•˜ê¸° fmin_slsqp()ëª…ë ¹ì€ ì´ë ‡ê²Œ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì´ ìˆëŠ” ê²½ìš°ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì œí•œì¡°ê±´ ì¸ìˆ˜ì˜ ì´ë¦„ì´ ieqconsë¡œ ë‹¬ë¼ì¡Œë‹¤. ë‹¨, ieqcons ì¸ìˆ˜ì— ë“¤ì–´ê°€ëŠ” ë¶€ë“±í˜¸ëŠ” ìš°ë¦¬ê°€ ì§€ê¸ˆê¹Œì§€ ì‚¬ìš©í•œ ë°©ì‹ê³¼ ë‹¬ë¦¬ 0 ë˜ëŠ” ì–‘ìˆ˜ì´ì–´ì•¼ í•œë‹¤. \\begin{align} g \\geq 0 \\end{align}1fmin_slsqp(func_objective, x0, ieqcons=[func_constraint1, func_constraint2]) ì´ë ‡ë“¯, fmin_slsqp() ëª…ë ¹ì€ ë“±ì‹ ì œí•œì¡°ê±´ê³¼ ë¶€ë“±ì‹ ì œí•œì¡°ê±´ì„ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 12345678910def f2(x): return np.sqrt((x[0] - 4) ** 2 + (x[1] - 2) ** 2)# ì œí•œ ì¡°ê±´ ìƒìˆ˜k = 1def ieq_constraint(x): return np.atleast_1d(k - np.sum(np.abs(x)))sp.optimize.fmin_slsqp(f2, np.array([0, 0]), ieqcons=[ieq_constraint]) ê²°ê³¼1234567Optimization terminated successfully. (Exit mode 0) Current function value: 3.6055512804550336 Iterations: 11 Function evaluations: 77 Gradient evaluations: 11array([9.99999982e-01, 1.79954011e-08]) ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  ë³€ìˆ˜ê°€ 1ê°œìˆë‹¤ë©´, $x^T$ëŠ” Nì°¨ì›ì„ ê°–ëŠ” í—¹ë²¡í„°ë¼ë©´, $\\beta$ë„ ê°™ì€ ì°¨ì›ì„ ê°–ëŠ” ì—´ë²¡í„°ë¡œì„œ ë‚´ì ì„ í†µí•´ ê·¸ì— ë”°ë¥¸ ë²¡í„°ì˜ ì˜ì—­ì´ ì´ˆí‰ë©´(hyperplane)ì„ ì´ë£¨ê²Œ ë  ê²ƒì´ë‹¤. í¼ì…‰íŠ¸ë¡ ì€ ê°€ì¥ ë‹¨ìˆœí•˜ê³  ë¹ ë¥¸ íŒë³„ í•¨ìˆ˜ ê¸°ë°˜ ë¶„ë¥˜ ëª¨í˜•ì´ì§€ë§Œ íŒë³„ ê²½ê³„ì„ (decision hyperplane)ì´ ìœ ë‹ˆí¬í•˜ê²Œ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” íŠ¹ì§•ì´ ìˆë‹¤. ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (SVM: Support vector machine)ì€ í¼ì…‰íŠ¸ë¡  ê¸°ë°˜ì˜ ëª¨í˜•ì— ê°€ì¥ ì•ˆì •ì ì¸ íŒë³„ ê²½ê³„ì„ ì„ ì°¾ê¸° ìœ„í•œ ì œí•œ ì¡°ê±´ì„ ì¶”ê°€í•œ ëª¨í˜•ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì„œí¬íŠ¸ì™€ ë§ˆì§„ ë‹¤ìŒê³¼ ê°™ì´ $N$ê°œì˜ í•™ìŠµìš© ë°ì´í„°ê°€ ìˆë‹¤ê³  í•˜ì. (x_{1}, y_{1}), (x_{2}, y_{2}), \\ldots, (x_{i}, y_{i}), \\ldots,(x_{N}, y_{N}) íŒë³„í•¨ìˆ˜ ëª¨í˜•ì—ì„œ $y$ëŠ” $+1,\\; -1$ ë‘ ê°œì˜ ê°’ì„ ê°€ì§„ë‹¤. y = \\begin{cases} +1 \\\\ -1 \\end{cases} $x$ ë°ì´í„° ì¤‘ì—ì„œ $y$ê°’ì´ $+1$ì¸ ë°ì´í„°ë¥¼ $x_{+}$, $y$ê°’ì´ $-1$ì¸ ë°ì´í„°ë¥¼ $x_{-}$ë¼ê³  í•˜ì. íŒë³„í•¨ìˆ˜ ëª¨í˜•ì—ì„œ ì§ì„ ì¸ íŒë³„ í•¨ìˆ˜ $f(x)$ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. f(x) = w^Tx-w_0 í˜¹ì‹œë¼ë„ íŒë³„í•¨ìˆ˜ì˜ ì§ì„ ì˜ ë°©ì •ì‹ì´ ì–´ë–»ê²Œ ë‚˜ì˜¨ê±´ì§€ ì´í•´ê°€ ì•ˆê°€ì‹œëŠ” ë¶„ë“¤ì—ê²Œ ì„¤ëª…ì„ ë“œë¦¬ê³ ì ì ê¹ ì„ í˜•ëŒ€ìˆ˜ì˜ ì§ì„ ì˜ ë°©ì •ì‹ì— ê´€í•œ ì„¤ëª…ì„ í•˜ë„ë¡ í•˜ê² ë‹¤. ì§ì„ ì˜ ë°©ì •ì‹ ì–´ë–¤ ë²¡í„° $w$ê°€ ìˆì„ ë•Œ ì›ì ì—ì„œ ì¶œë°œí•œ ë²¡í„° $w$ê°€ ê°€ë¦¬í‚¤ëŠ” ì ì„ ì§€ë‚˜ë©´ì„œ ë²¡í„° $w$ì— ìˆ˜ì§ì¸ ì§ì„ ì˜ ë°©ì •ì‹ì„ êµ¬í•´ë³´ì. ìœ„ ë‘ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì§ì„ ìƒì˜ ì„ì˜ì˜ ì ì„ ê°€ë¦¬í‚¤ëŠ” ë²¡í„°ë¥¼ $x$ë¼ê³  í•˜ë©´, ë²¡í„° $x$ê°€ ê°€ë¦¬í‚¤ëŠ” ì ê³¼ ë²¡í„° $w$ê°€ ê°€ë¦¬í‚¤ëŠ” ì ì„ ì´ì€ ë²¡í„° $x - w$ëŠ” ì¡°ê±´ì— ë”°ë¼ ë²¡í„° $w$ì™€ ì§êµí•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ë‹¤ìŒ ì‹ì´ ì„±ë¦½í•œë‹¤. \\begin{align} w^T(x - w) = 0 \\end{align} ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤. \\begin{align} w^T(x - w) = w^Tx - w^Tw = w^Tx - \\| w \\|^2 \\end{align}\\begin{align} w^Tx - \\| w \\|^2 = 0 \\end{align} ì´ ì§ì„ ê³¼ ì›ì  ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ë²¡í„° $w$ì˜ norm $|w|$ì´ë‹¤. ì—°ìŠµë¬¸ì œ) ë§Œì•½ $v$ê°€ ì›ì ì„ ì§€ë‚˜ëŠ” ì§ì„ ì˜ ë°©í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ìœ„ë²¡í„°ë¼ê³  í•˜ì. ì´ë•Œ ê·¸ ì§ì„  ìœ„ì— ìˆì§€ ì•ŠëŠ” ì–´ë–¤ ì  $x$ì™€ ê·¸ ì§ì„ ê³¼ì˜ ê±°ë¦¬ì˜ ì œê³±ì´ ë‹¤ìŒê³¼ ê°™ìŒì„ ì¦ëª…í•˜ë¼. \\begin{align} \\| x \\|^2 - (x^Tv)^2 \\end{align} $ x \\; - \\; v \\perp v$ì´ê¸° ë•Œë¬¸ì— $a^{\\Vert b} = | x | cos \\theta = \\frac{| v | | x | cos \\theta}{| v |} = \\frac{x^{T} v}{| v |}$ ë²¡í„° $v$ëŠ” ë‹¨ìœ„ë²¡í„°ì´ë¯€ë¡œ $a^{\\Vert b} = x^{T}v$ê°€ ëœë‹¤. ì—¬ê¸°ì„œ í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©´ ìš°ë¦¬ê°€ ì¦ëª…í•´ì•¼ í•˜ëŠ” ì‹ì„ êµ¬í•  ìˆ˜ ìˆë‹¤. 1234567891011121314v = np.array([2, 1]) / np.sqrt(5)x = np.array([1, 3])plt.plot(0, 0, 'kP', ms=10)plt.annotate('', xy=v, xytext=(0, 0), arrowprops=black)plt.plot([-2, 8], [-1, 4], 'b--', lw=2)plt.plot([1, 2], [3, 1], 'g:', lw=2)plt.plot(x[0], x[1], 'ro', ms=10)plt.text(0.1, 0.5, \"$v$\")plt.text(0.6, 3.2, \"$x$\")plt.xticks(np.arange(-3, 15))plt.yticks(np.arange(-1, 5))plt.xlim(-3, 7)plt.ylim(-1, 5)plt.show() ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì™€ ê°™ì„ ë•Œ \\begin{align} w = \\begin{bmatrix}1 \\\\ 2\\end{bmatrix} \\tag{3.1.49} \\end{align}\\begin{align} \\| w \\|^2 = 5 \\end{align}\\begin{align} \\begin{bmatrix}1 & 2\\end{bmatrix} \\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix} - 5 = x_1 + 2x_2 - 5 = 0 \\end{align}\\begin{align} x_1 + 2x_2 = 5 \\end{align} ì´ ë°©ì •ì‹ì€ ë²¡í„° $w$ê°€ ê°€ë¦¬í‚¤ëŠ” ì  (1,2)ë¥¼ ì§€ë‚˜ë©´ì„œ ë²¡í„° $w$ì— ìˆ˜ì§ì¸ ì§ì„ ì„ ëœ»í•œë‹¤. ì´ ì§ì„ ê³¼ ì›ì  ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” $ |w|=\\sqrt{5} $ì´ë‹¤. 12345678910111213141516171819202122w = np.array([1, 2])x1 = np.array([3, 1])x2 = np.array([-1, 3])plt.annotate('', xy=w, xytext=(0, 0), arrowprops=black)plt.annotate('', xy=x1, xytext=(0, 0), arrowprops=green)plt.annotate('', xy=x2, xytext=(0, 0), arrowprops=green)plt.plot(0, 0, 'kP', ms=10)plt.plot(w[0], w[1], 'ro', ms=10)plt.plot(x1[0], x1[1], 'ro', ms=10)plt.plot(x2[0], x2[1], 'ro', ms=10)plt.plot([-3, 5], [4, 0], 'r-', lw=5)plt.text(-0.2, 1.5, \"ë²¡í„° $w$\")plt.text(1.55, 0.25, \"$x_1$\")plt.text(-0.9, 1.40, \"$x_2$\")plt.text(1.8, 1.8, \"$x_1 - w$\")plt.text(-0.2, 2.8, \"$x_2 - w$\")plt.text(3.6, 0.8, \"ì§ì„  $x$\")plt.xticks(np.arange(-2, 5))plt.yticks(np.arange(-1, 5))plt.xlim(-2, 5)plt.ylim(-0.6, 3.6)plt.show() ì´ë²ˆì—ëŠ” ë²¡í„° $w$ê°€ ê°€ë¦¬í‚¤ëŠ” ì ì„ ì§€ë‚˜ì•¼ í•œë‹¤ëŠ” ì¡°ê±´ì„ ì—†ì• ê³  ë‹¨ìˆœíˆ ë²¡í„° $w$ì— ìˆ˜ì§ì¸ ì§ì„  $x$ì˜ ë°©ì •ì‹ì„ êµ¬í•˜ë©´ ì´ë•ŒëŠ” ì§ì„ ì´ $w$ê°€ ì•„ë‹ˆë¼ $w$ì™€ ë°©í–¥ì´ ê°™ê³  ê¸¸ì´ê°€ ë‹¤ë¥¸ ë²¡í„° $wâ€™=cw$ì„ ì§€ë‚  ê²ƒì´ë‹¤. cëŠ” ì–‘ì˜ ì‹¤ìˆ˜ì´ë‹¤. ìœ„ì—ì„œ í–ˆë˜ ë°©ë²•ìœ¼ë¡œ ë‹¤ì‹œ ì§ì„ ì˜ ë°©ì •ì‹ì„ ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} w'^Tx - \\| w' \\|^2 = cw^Tx - c^2 \\| w \\|^2 = 0 \\end{align}\\begin{align} w^Tx - c \\| w \\|^2 = 0 \\end{align} ì—¬ê¸°ì—ì„œ $c | w |^2$ëŠ” ì„ì˜ì˜ ìˆ˜ê°€ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¨ìˆœíˆ ë²¡í„° $w$ì— ìˆ˜ì§ì¸ ì§ì„ ì˜ ë°©ì •ì‹ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. \\begin{align} w^Tx - w_0 = 0 \\end{align} ì´ ì§ì„ ê³¼ ì›ì  ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} c \\| w \\| = \\dfrac{w_0}{\\|w\\|} \\end{align}1234567891011121314w = np.array([1, 2])plt.annotate('', xy=w, xytext=(0, 0), arrowprops=gray)plt.annotate('', xy=0.5 * w, xytext=(0, 0), arrowprops=black)plt.plot(0, 0, 'kP', ms=10)plt.plot(0.5 * w[0], 0.5 * w[1], 'ro', ms=10)plt.plot([-2, 5], [2.25, -1.25], 'r-', lw=5)plt.text(-0.7, 0.8, \"ë²¡í„° $cw$\")plt.text(-0.1, 1.6, \"ë²¡í„° $w$\")plt.text(1, 1, \"ì§ì„  $x$\")plt.xticks(np.arange(-2, 5))plt.yticks(np.arange(-1, 5))plt.xlim(-2, 5)plt.ylim(-0.6, 3.6)plt.show() ì˜ˆë¥¼ ë“¤ì–´ $c=0.5$ì´ë©´ ë²¡í„° $w=[1, 2]^T$ì— ìˆ˜ì§ì´ê³  ì›ì ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬ê°€ $\\frac{\\sqrt{5}}{2}$ì¸ ì§ì„ ì´ ëœë‹¤. \\begin{align} x_1 + 2x_2 - 2.5 = 0 \\end{align}ì§ì„ ê³¼ ì ì˜ ê±°ë¦¬ ì´ë²ˆì—ëŠ” ì§ì„  $w^Tx - |w|^2 = 0$ê³¼ ì´ ì§ì„  ìœ„ì— ìˆì§€ ì•Šì€ ì  $xâ€™$ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ êµ¬í•´ë³¼ ê²ƒì´ë‹¤. ë²¡í„° $w$ì— ëŒ€í•œ ë²¡í„° $xâ€™$ì˜ íˆ¬ì˜ì„±ë¶„ $xâ€™^{\\Vert w}$ì˜ ê¸¸ì´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} \\|x'^{\\Vert w}\\| = \\dfrac{w^Tx'}{\\|w\\|} \\end{align} ì§ì„ ê³¼ ì  $xâ€™$ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ì´ ê¸¸ì´ì—ì„œ ì›ì ì—ì„œ ì§ì„ ê¹Œì§€ì˜ ê±°ë¦¬ $|w|$ë¥¼ ëº€ ê°’ì˜ ì ˆëŒ€ê°’ì´ë‹¤. \\begin{align} \\left| \\|x'^{\\Vert w}\\| - \\|w\\| \\right| = \\left| \\dfrac{w^Tx'}{\\|w\\|} - \\|w\\| \\right| = \\dfrac{\\left|w^Tx' - \\|w\\|^2 \\right|}{\\|w\\|} \\end{align} ì§ì„ ì˜ ë°©ì •ì‹ì´ $w^Tx - w_0 = 0$ì´ë©´ ì§ì„ ê³¼ ì ì˜ ê±°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{align} \\dfrac{\\left|w^Tx' - w_0 \\right|}{\\|w\\|} \\end{align} ì´ ê³µì‹ì€ ì•„ë˜ ë‚´ìš©ì¤‘ SVMì˜ íŒë³„í•¨ìˆ˜ì˜ ì§ì„ ê³¼ ì„œí¬íŠ¸ë²¡í„°ê°„ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ëŠ”ë°ì—ì„œ ì‚¬ìš©ëœë‹¤. 1234567891011121314151617181920w = np.array([1, 2])x1 = np.array([4, 3])x2 = np.array([1, 2]) * 2plt.annotate('', xy=x1, xytext=(0, 0), arrowprops=gray)plt.annotate('', xy=x2, xytext=(0, 0), arrowprops=gray)plt.annotate('', xy=w, xytext=(0, 0), arrowprops=red)plt.plot(0, 0, 'kP', ms=10)plt.plot(w[0], w[1], 'ro', ms=10)plt.plot(x1[0], x1[1], 'ro', ms=10)plt.plot([-3, 7], [4, -1], 'r-', lw=5)plt.plot([2, 4], [4, 3], 'k:', lw=2)plt.plot([3, 4], [1, 3], 'k:', lw=2)plt.text(0.1, 0.9, \"$w$\")plt.text(4.2, 3.1, \"$x'$\")plt.text(1.5, 2.4, \"$x'^&#123;\\Vert w&#125;$\")plt.xticks(np.arange(-3, 15))plt.yticks(np.arange(-1, 5))plt.xlim(-3, 7)plt.ylim(-1, 5)plt.show() ë‹¤ì‹œ SVM íŒë³„í•¨ìˆ˜ë¥¼ ì‚´í´ë³´ë©´ ì •ì˜ì— ë”°ë¼ $y$ê°’ì´ $+1$ì¸ ê·¸ ë°ì´í„° $x_{+}$ì— ëŒ€í•œ íŒë³„í•¨ìˆ˜ ê°’ì€ ì–‘ìˆ˜ê°€ ëœë‹¤. f(x_+) = w^Tx_+ - w_0 > 0 ë°˜ëŒ€ë¡œ yê°’ì´ -1ì¸ ê·¸ ë°ì´í„° $x_{-}$ì— ëŒ€í•œ íŒë³„í•¨ìˆ˜ ê°’ì€ ìŒìˆ˜ê°€ ëœë‹¤. f(x_-) = w^Tx_- - w_0 < 0 $y$ ê°’ì´ $+1$ì¸ ë°ì´í„° ì¤‘ì—ì„œ íŒë³„ í•¨ìˆ˜ì˜ ê°’ì´ ê°€ì¥ ì‘ì€ ë°ì´í„°ë¥¼ $x^{+}$ë¼ê³  í•˜ê³  $y$ê°’ì´ $-1$ì¸ ë°ì´í„° ì¤‘ì—ì„œ íŒë³„í•¨ìˆ˜ì˜ ê°’ì´ ê°€ì¥ í° ë°ì´í„°ë¥¼ $x^{-}$ë¼ê³  í•˜ì. ì´ ë°ì´í„°ë“¤ì€ ê°ê°ì˜ í´ë˜ìŠ¤ì— ì†í•œ ë°ì´í„° ì¤‘ì—ì„œ ê°€ì¥ ê²½ê³„ì„ ì— ê°€ê¹Œì´ ë¶™ì–´ìˆëŠ” ìµœì „ë°©(most front)ì˜ ë°ì´í„°ë“¤ì´ë‹¤. ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ ì„œí¬íŠ¸(support) í˜¹ì€ ì„œí¬íŠ¸ ë²¡í„°(support vector)ë¼ê³  í•œë‹¤. ë¬¼ë¡  ì´ ì„œí¬íŠ¸ì— ëŒ€í•´ì„œë„ ë¶€í˜¸ ì¡°ê±´ì€ ë§Œì¡±ë˜ì–´ì•¼ í•œë‹¤. f(x^+) = w^Tx^+ - w_0 > 0f(x^-) = w^Tx^- - w_0 < 0 ì„œí¬íŠ¸ì— ëŒ€í•œ íŒë³„ í•¨ìˆ˜ì˜ ê°’ $f(x^{+}), f(x^{-})$ê°’ì€ ë¶€í˜¸ ì¡°ê±´ë§Œ ì§€í‚¤ë©´ ì–´ë–¤ ê°’ì´ ë˜ì–´ë„ ê´œì°®ë‹¤, ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì¡°ê±´ì„ ë§Œì¡±í•˜ë„ë¡ íŒë³„ í•¨ìˆ˜ë¥¼ êµ¬í•œë‹¤. f(x^+) = w^T x^{+} - w_0 = +1f(x^-) = w^T x^{-} - w_0 = -1 ì´ë ‡ê²Œ ë˜ë©´ ëª¨ë“  Support vector $(x_{+}, x_{-})$ ë°ì´í„°ë“¤ì— ëŒ€í•œ íŒë³„í•¨ìˆ˜ì˜ ê°’ì˜ ì ˆëŒ€ê°’ì´ 1ë³´ë‹¤ ì»¤ì§€ë¯€ë¡œ ë‹¤ìŒ ë¶€ë“±ì‹ì´ ì„±ë¦½í•œë‹¤. w^Tx_+ - w_o \\geq 1w^Tx_- - w_o \\leq -1 íŒë³„ ê²½ê³„ì„  $w^{T}x - w_{0} = 0$ê³¼ ì  $x^{+}, x^{-}$ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. \\dfrac{w^T x^{+} - w_0}{\\| w \\|} = \\dfrac{1}{\\| w \\|}-\\dfrac{w^T x^{-} - w_0}{\\| w \\|} = \\dfrac{1}{\\| w \\|} ì´ ê±°ë¦¬ì˜ í•©ì„ ë§ˆì§„(margin)ì´ë¼ê³  í•˜ë©° ë§ˆì§„ê°’ì´ í´ ìˆ˜ë¡ ë” ê²½ê³„ì„ ì´ ì•ˆì •ì ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ°ë° ìœ„ì—ì„œ ì •í•œ ìŠ¤ì¼€ì¼ë§ì— ì˜í•´ ë§ˆì§„ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬ëœë‹¤. \\dfrac{w^T x^{+} - w_0}{\\| w \\|} -\\dfrac{w^T x^{-} - w_0}{\\| w \\|} = \\dfrac{2}{\\| w \\|} ë§ˆì§„ ê°’ì´ ìµœëŒ€ê°€ ë˜ëŠ” ê²½ìš°ëŠ” $| w |$ ì¦‰, $| w |^{2}$ê°€ ìµœì†Œê°€ ë˜ëŠ” ê²½ìš°ì™€ ê°™ë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ëª©ì í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ë©´ ëœë‹¤. L = \\dfrac{1}{2} ||w||^2 = \\dfrac{1}{2} w^T w ë˜í•œ ëª¨ë“  í‘œë³¸ ë°ì´í„°ì— ëŒ€í•´ ë¶„ë¥˜ëŠ” ì œëŒ€ë¡œ ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ëª¨ë“  ë°ì´í„° $x_{i}, y_{i} (i = 1,\\ldots, N)$ì— ëŒ€í•´ ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•œë‹¤. ìœ„ì—ì„œ ìŠ¤ì¼€ì¼ë§ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ë°ì´í„°ì— ëŒ€í•´ $f(x_i) = w^Tx_i - w_o$ê°€ $1$ë³´ë‹¤ í¬ê±°ë‚˜ $-1$ë³´ë‹¤ ì‘ê²Œ ë§Œë“¤ì—ˆë‹¤ëŠ” ì ì„ ì´ìš©í•œë‹¤. y_i \\cdot f(x_i) = y_i \\cdot( w^Tx_i - w_o) \\geq 1 \\;\\;\\; ( i = 1, \\ldots, N )y_i \\cdot ( w^Tx_i - w_o) - 1 \\geq 0 \\;\\;\\; ( i = 1, \\ldots, N ) ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë²•ì„ ì‚¬ìš©í•˜ë©´ ìµœì†Œí™” ëª©ì í•¨ìˆ˜ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê³ ì¹˜ë©´ ëœë‹¤. ì¦‰, ìœ„ì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” wì˜ ìµœì†Œí™” ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒê³¼ ê°™ê²Œ ëœë‹¤. $a_{i}$ì€ ê°ê°ì˜ ë¶€ë“±ì‹ì— ëŒ€í•œ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ì´ë‹¤. ì´ ìµœì í™” ë¬¸ì œë¥¼ í’€ì–´ $w, w_{0}, a$ë¥¼ êµ¬í•˜ë©´ íŒë³„í•¨ìˆ˜ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. L = \\dfrac{1}{2} w^T w - \\sum_{i=1}^N a_i \\{ y_i \\cdot ( w^Tx_i - w_o) - 1 \\} KKT(Karush-Kuhn-Tucker) ì¡°ê±´ì— ë”°ë¥´ë©´ ë¶€ë“±ì‹ ì œí•œ ì¡°ê±´ì´ ìˆëŠ” ê²½ìš°ì—ëŠ” ë“±ì‹ ì œí•œì¡°ê±´ì„ ê°€ì§€ëŠ” ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ ë°©ë²•ê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ $i$ë²ˆì§¸ ë¶€ë“±ì‹ì´ ìˆìœ¼ë‚˜ ì—†ìœ¼ë‚˜ ë‹µì´ ê°™ì€ ê²½ìš°ì—ëŠ” í•´ë‹¹ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ì˜ ê°’ì´ $a_{i}=0$ì´ ëœë‹¤. ì´ ê²½ìš°ëŠ” íŒë³„í•¨ìˆ˜ì˜ ê°’ $w^Tx_i - w_o$ì´ $-1$ë³´ë‹¤ ì‘ê±°ë‚˜ 1ë³´ë‹¤ í° ê²½ìš°ì´ë‹¤.ì¦‰, ë§ˆì§„ì•ˆì— í¬í•¨ë˜ì§€ ì•Šê³  ë°”ê¹¥ì— ìˆëŠ” ë°ì´í„°ë“¤ ê°™ì€ ê²½ìš°ëŠ” í•´ë‹¹ ì¡°ê±´ì‹ì´ í•´ë¥¼ ì°¾ëŠ”ë° ì˜í–¥ì„ ì£¼ì§€ ì•Šì•„ ë“±ì‹ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒê³¼ ê°™ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. y_i(w^Tx_i - w_o) - 1 > 0 í•™ìŠµ ë°ì´í„° ì¤‘ì—ì„œ ìµœì „ë°© ë°ì´í„°ì¸ ì„œí¬íŠ¸ ë²¡í„°ê°€ ì•„ë‹Œ ëª¨ë“  ë°ì´í„°ë“¤ì— ëŒ€í•´ì„œëŠ” ì´ ì¡°ê±´ì´ ë§Œì¡±ë˜ë¯€ë¡œ ì„œí¬íŠ¸ ë²¡í„°ê°€ ì•„ë‹Œ ë°ì´í„°ëŠ” ë¼ê·¸ë‘ì§€ ìŠ¹ìˆ˜ê°€ $0$ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. a_i = 0 \\;\\; \\text{if} \\;\\; x_i \\notin \\{ x^{+}, x^{-} \\}ë“€ì–¼ í˜•ì‹ ìµœì í™” ì¡°ê±´ì€ ëª©ì í•¨ìˆ˜ $L$ì„ $w, w_{0}$ë¡œ ë¯¸ë¶„í•œ ê°’ì´ 0ì´ ë˜ì–´ì•¼ í•˜ëŠ” ê²ƒì´ë‹¤. \\dfrac{\\partial L}{\\partial w} = 0\\dfrac{\\partial L}{\\partial w_0} = 0 ì´ ì‹ì„ í’€ì–´ì„œ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤. \\begin{eqnarray} \\dfrac{\\partial L}{\\partial w} &=& \\dfrac{\\partial}{\\partial w} \\left( \\dfrac{1}{2} w^T w \\right) - \\dfrac{\\partial}{\\partial w} \\sum_{i=1}^N \\left( a_i y_i w^Tx_i - a_i y_i w_o - a_i \\right) \\\\ &=& w - \\sum_{i=1}^N a_i y_i x_i \\\\ &=& 0 \\end{eqnarray}\\begin{eqnarray} \\dfrac{\\partial L}{\\partial w_0} &=& \\dfrac{\\partial}{\\partial w_0} \\left( \\dfrac{1}{2} w^T w \\right) - \\dfrac{\\partial}{\\partial w_0} \\sum_{i=1}^N \\left( a_i y_i w^Tx_i - a_i y_i w_o - a_i \\right) \\\\ &=& \\sum_{i=1}^N a_i y_i \\\\ &=& 0 \\end{eqnarray} ì •ë¦¬í•´ë³´ë©´, ë‹¤ìŒê³¼ ê°™ë‹¤. w = \\sum_{i=1}^N a_i y_i x_i0 = \\sum_{i=1}^N a_i y_i ì´ ë‘ ìˆ˜ì‹ì„ ì›ë˜ì˜ ëª©ì í•¨ìˆ˜ì— ëŒ€ì…í•˜ì—¬ $w, w_{0}$ì„ ì—†ì• ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\begin{eqnarray} L &=& \\dfrac{1}{2} w^T w - \\sum_{i=1}^N a_i \\{ y_i \\cdot ( w^Tx_i - w_o) - 1 \\} \\\\ &=& \\dfrac{1}{2} \\left( \\sum_{i=1}^N a_i y_i x_i \\right)^T \\left( \\sum_{j=1}^N a_j y_j x_j \\right) - \\sum_{i=1}^N a_i \\left\\{ y_i \\cdot \\left( \\left( \\sum_{j=1}^N a_j y_j x_j \\right)^Tx_i - w_o \\right) - 1 \\right\\} \\\\ &=& \\dfrac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j - \\sum_{i=1}^N \\sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j + w_0 \\sum_{i=1}^N a_i y_i + \\sum_{i=1}^N a_i \\\\ &=& \\sum_{i=1}^N a_i - \\dfrac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j \\end{eqnarray}L = \\sum_{i=1}^N a_i - \\dfrac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j ì´ ë–„ $a$ëŠ” ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤. \\sum_{i=1}^N a_i y_i = 0a_i \\geq 0 \\;\\;\\; ( i = 1, \\ldots, N ) ì´ ë¬¸ì œëŠ” $w$ë¥¼ êµ¬í•˜ëŠ” ë¬¸ì œê°€ ì•„ë‹ˆë¼ $a$ë§Œì„ êµ¬í•˜ëŠ” ë¬¸ì œë¡œ ë°”ë€Œì—ˆìœ¼ë¯€ë¡œ ë“€ì–¼í˜•ì‹(dual form)ì´ë¼ê³  í•œë‹¤. ë“€ì–¼í˜•ì‹ìœ¼ë¡œ ë°”ê¾¸ë©´ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë°•ìŠ¤(Box)ì œí•œ ì¡°ê±´ì´ ìˆëŠ” ì´ì°¨í”„ë¡œê·¸ë˜ë°(QP: Quadratic programming)ë¬¸ì œê°€ ë˜ë¯€ë¡œ ì›ë˜ì˜ ë¬¸ì œë³´ë‹¤ëŠ” íš¨ìœ¨ì ìœ¼ë¡œ í’€ ìˆ˜ ìˆë‹¤. ì„ í˜•ê³„íšë²• ë¬¸ì œì™€ ì´ì°¨ê³„íšë²• ë¬¸ì œ ë“€ì–¼ í˜•ì‹ ë¬¸ì œë¥¼ í’€ì–´ í•¨ìˆ˜ $L$ì„ ìµœì†Œí™”í•˜ëŠ” $a$ë¥¼ êµ¬í•˜ë©´ ì˜ˆì¸¡ ëª¨í˜•ì„ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ ìˆ˜ ìˆë‹¤. f(x) = w^T x - w_0 = \\sum_{i=1}^N a_i y_i x_i^T x - w_0 $w_{0}$ëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬í•œë‹¤. w_0 = w^T x^{+} - 1 ë˜ëŠ” w_0 = w^T x^{-} + 1 ë˜ëŠ” w_0 = \\dfrac{1}{2} w^T (x^+ + x^{-}) ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ ê°’ì´ 0 ì¦‰, $a_{i} = 0$ì´ë©´ í•´ë‹¹ ë°ì´í„°ëŠ” ì˜ˆì¸¡ ëª¨í˜•, ì¦‰ $w$ ê³„ì‚°ì— ì•„ë¬´ëŸ° ê¸°ì—¬ë¥¼ í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìœ„ì˜ ì‹ì€ ì‹¤ì œë¡œëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. f(x) = a^+ x^T x^+ - a^- x^T x^- - w_0 ì—¬ê¸°ì—ì„œ $x^{T}x^{+}$ëŠ” $x$ì™€ $x^{+}$ ì‚¬ì´ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„, $x^{T}x^{-}$ëŠ” $x$ì™€ $x^{-}$ ì‚¬ì´ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì´ë¯€ë¡œ ê²°êµ­ ë‘ ì„œí¬íŠ¸ ë²¡í„°ì™€ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•´ì„œ ê°’ì´ í°ìª½ìœ¼ë¡œ íŒë³„í•˜ê²Œ ëœë‹¤. Scikit-Learnì˜ ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  Scikit-Learnì˜ svm ì„œë¸ŒíŒ¨í‚¤ì§€ëŠ” ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  ëª¨í˜•ì¸ SVC(Support Vector Classifier) í´ë˜ìŠ¤ë¥¼ ì œê³µí•œë‹¤. ì´ì™€ ë™ì‹œì— SVR(Support Vector Regressor)ë„ ì œê³µì„ í•˜ì§€ë§Œ SVRì€ ì¶”í›„ì— ì„¤ëª…í•˜ê³  ë¨¼ì €, SVCì— ëŒ€í•´ ë‹¤ë£¨ì–´ ë³¼ ê²ƒì´ë‹¤. 1234567891011from sklearn.datasets import make_blobsX, y = make_blobs(n_samples=50, centers=2, cluster_std=0.5, random_state=4)y = 2 * y - 1plt.scatter(X[y == -1, 0], X[y == -1, 1], marker='o', label=\"-1 í´ë˜ìŠ¤\")plt.scatter(X[y == +1, 0], X[y == +1, 1], marker='x', label=\"+1 í´ë˜ìŠ¤\")plt.xlabel(\"x1\")plt.ylabel(\"x2\")plt.legend()plt.title(\"í•™ìŠµìš© ë°ì´í„°\")plt.show() SVC í´ë˜ìŠ¤ëŠ” ì»¤ë„(Kernel)ì„ ì„ íƒí•˜ëŠ” ì¸ìˆ˜ kernelê³¼ ìŠ¬ë™ë³€ìˆ˜ ê°€ì¤‘ì¹˜(slack variable weight)ë¥¼ ì„ íƒí•˜ëŠ” ì¸ìˆ˜ Cë¥¼ ë°›ëŠ”ë° ì§€ê¸ˆê¹Œì§€ ê³µë¶€í•œ ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ì„ ì‚¬ìš©í•˜ë ¤ë©´ ì¸ìˆ˜ë¥¼ ë‹¤ìŒì²˜ëŸ¼ ë„£ì–´ì¤€ë‹¤. 12from sklearn.svm import SVCmodel = SVC(kernel='linear', C=1e10).fit(X, y) SVCë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨í˜•ì„ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì†ì„±ê°’ì„ ê°€ì§„ë‹¤. n_support : ê° í´ë˜ìŠ¤ì˜ ì„œí¬íŠ¸ ë²¡í„°ì˜ ê°œìˆ˜ support : ê° í´ë˜ìŠ¤ì˜ ì„œí¬íŠ¸ ë²¡í„°ì˜ ì¸ë±ìŠ¤ support_vectors_ : ê° í´ë˜ìŠ¤ì˜ ì„œí¬íŠ¸ì˜ $x$ê°’.$(x^{T}, x^{-})$ coef : $w$ë²¡í„° intercept : $- w_{0}$ dual_coef : ê° ì›ì†Œê°€ $a_{i} \\dot y_{i}$ë¡œ ì´ë£¨ì–´ì§„ ë²¡í„° 1model.n_support_ ê²°ê³¼1array([1, 1], dtype=int32) 1model.support_ ê²°ê³¼1array([42, 1], dtype=int32) 1model.support_vectors_ ê²°ê³¼12array([[9.03715314, 1.71813465], [9.17124955, 3.52485535]]) 1y[model.support_] ê²°ê³¼1array([-1, 1]) 12345678910111213141516171819202122232425262728293031xmin = X[:, 0].min()xmax = X[:, 0].max()ymin = X[:, 1].min()ymax = X[:, 1].max()xx = np.linspace(xmin, xmax, 10)yy = np.linspace(ymin, ymax, 10)X1, X2 = np.meshgrid(xx, yy)Z = np.empty(X1.shape)for (i, j), val in np.ndenumerate(X1): x1 = val x2 = X2[i, j] p = model.decision_function([[x1, x2]]) Z[i, j] = p[0]levels = [-1, 0, 1]linestyles = ['dashed', 'solid', 'dashed']plt.scatter(X[y == -1, 0], X[y == -1, 1], marker='o', label=\"-1 í´ë˜ìŠ¤\")plt.scatter(X[y == +1, 0], X[y == +1, 1], marker='x', label=\"+1 í´ë˜ìŠ¤\")plt.contour(X1, X2, Z, levels, colors='k', linestyles=linestyles)plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, alpha=0.3)x_new = [10, 2]plt.scatter(x_new[0], x_new[1], marker='^', s=100)plt.text(x_new[0] + 0.03, x_new[1] + 0.08, \"í…ŒìŠ¤íŠ¸ ë°ì´í„°\")plt.xlabel(\"x1\")plt.ylabel(\"x2\")plt.legend()plt.title(\"SVM ì˜ˆì¸¡ ê²°ê³¼\")plt.show() 12x_new = [10, 2]model.decision_function([x_new]) ê²°ê³¼1array([-0.61101582]) 1model.coef_.dot(x_new) + model.intercept_ ê²°ê³¼1array([-0.61101582]) 12# dual_coef_ = a_i * y_imodel.dual_coef_ ê²°ê³¼1array([[-0.60934379, 0.60934379]]) 123model.dual_coef_[0][0] * model.support_vectors_[0].dot(x_new) + \\ model.dual_coef_[0][1] * model.support_vectors_[1].dot(x_new) + \\ model.intercept_ ê²°ê³¼1array([-0.61101582]) iris ë¬¸ì œë¥¼ ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ ìœ¼ë¡œ í’€ì–´ë³´ì. ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ë§Œ ì‚¬ìš©í•œ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ë°”ê¾¸ì–´ í’€ì–´ë³¸ë‹¤. ìœ„ì˜ ì˜ˆì œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì»¤ë„ ì¸ìˆ˜ kernelê³¼ ìŠ¬ë™ë³€ìˆ˜ ê°€ì¤‘ì¹˜ ì¸ìˆ˜ CëŠ” ê°ê° linear, 1e10ìœ¼ë¡œ í•œë‹¤. - íŠ¹ì§• ë³€ìˆ˜ë¥¼ ê½ƒë°›ì¹¨ì˜ ê¸¸ì´ì™€ í­ë§Œ ì‚¬ìš©í•œë‹¤. - ë¶“ê½† ì¢…ì„ Setosaì™€ Versicolourë§Œ ëŒ€ìƒìœ¼ë¡œ í•œë‹¤. 1234567891011from sklearn.metrics import confusion_matrix, classification_reportfrom sklearn.datasets import load_irisfrom sklearn.svm import SVCfrom sklearn.model_selection import train_test_splitiris = load_iris()X_data = iris.data[(iris.target == 0) | (iris.target == 1), :2]y = iris.target[(iris.target == 0) | (iris.target == 1)]X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.3)svm = SVC(kernel=\"linear\", C=1e10)svm.fit(X_train, y_train) ê²°ê³¼1234SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3, gamma='auto_deprecated', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) 12pred_y = svm.predict(X_test)confusion_matrix(pred_y, y_test) ê²°ê³¼12array([[14, 0], [ 0, 16]]) ìœ„ì˜ ì¡°ê±´ì—ì„œ kernel=&quot;linear&quot;ë¡œ ìœ ì§€í•œì±„ Cê°’ë§Œ [0.01, 0.1, 1, 10, 100]ìœ¼ë¡œ ë³€í™”ë¥¼ ì£¼ë©° ê²°ê³¼ë¥¼ ì‚´í´ë³´ë‹ˆ Cê°’ì´ ë†’ì•„ì§€ë©´ Slack ë³€ìˆ˜ë¡œ ì¤„ ìˆ˜ ìˆëŠ” ê°’ì´ ì¤„ì–´ë“¤ì–´ ì„œí¬íŠ¸ ë²¡í„°ì˜ ìˆ˜ê°€ ì¤„ì–´ë“ ë‹¤. ë°˜ëŒ€ë¡œ Cê°’ì„ ë‚®ì¶”ì–´ ì¤„ìˆ˜ë¡ Slack ë³€ìˆ˜ê°€ ê°–ëŠ” ê°’ì´ í¬ê²Œ ë˜ì–´ ì„œí¬íŠ¸ ë²¡í„°ëŠ” ë§ì•„ì§€ë©° ë§ˆì§„ì´ ì¤„ì–´ë“ ë‹¤. ìŠ¬ë™ë³€ìˆ˜ ë§Œì•½ ë°ì´í„°ê°€ ì§ì„ ì¸ íŒë³„ ê²½ê³„ì„ ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì§€ì§€ ì•ŠëŠ” ì¦‰, ì„ í˜•ë¶„ì´(linear seperable)ê°€ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìŠ¬ë™ë³€ìˆ˜(slack variable)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°œë³„ì ì¸ ì˜¤ì°¨ë¥¼ í—ˆìš©í•  ìˆ˜ ìˆë‹¤. ì›ë˜ íŒë³„ í•¨ìˆ˜ì˜ ê°’ì€ í´ë˜ìŠ¤ $x^{T}$ ì˜ì—­ì˜ ìƒ˜í”Œ $x_{+}$ì— ëŒ€í•´ì„  ì²«ë²ˆì§¸ ìˆ˜ì‹ê³¼ ê°™ê³ , í´ë˜ìŠ¤ -1 ì˜ì—­ì˜ ìƒ˜í”Œ $x_{-}$ì— ëŒ€í•´ì„œëŠ” ë‘ ë²ˆì§¸ ìˆ˜ì‹ê³¼ ê°™ì•„ì•¼í•œë‹¤. w^Tx_+ - w_0 \\geq 1w^Tx_- - w_0 \\leq -1 ì–‘ìˆ˜ì¸ ìŠ¬ë™ë³€ìˆ˜ $\\xi \\geq 0$ë¥¼ ì‚¬ìš©í•˜ë©´ ì´ ì¡°ê±´ì„ ë‹¤ìŒê³¼ ê°™ì´ ì™„í™”í•  ìˆ˜ ìˆë‹¤. w^Tx_+ - w_0 \\geq +1-\\xi_iw^Tx_- - w_0 \\leq -1+\\xi_i ëª¨ë“  ìŠ¬ë™ë³€ìˆ˜ëŠ” 0ë³´ë‹¤ ê°™ê±°ë‚˜ í¬ë‹¤. \\xi_i \\geq 0 \\;\\;\\; (i=1, \\ldots, N) ìœ„ì˜ ë¶€ë“±ì‹ ì¡°ê±´ì„ ëª¨ë‘ ê³ ë ¤í•œ ìµœì í™” ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì•„ì§„ë‹¤. ì•„ë˜ ì‹ì—ì„œ $C \\sum_{i=1}^N \\xi_i$ í•­ì€ ìŠ¬ë™ë³€ìˆ˜ì˜ í•©ì´ ë„ˆë¬´ ì»¤ì§€ì§€ ì•Šë„ë¡ ì œí•œí•˜ëŠ” ì—­í• ì„ í•œë‹¤. L = \\dfrac{1}{2} ||w||^2 - \\sum_{i=1}^N a_i (y_i \\cdot ( w^Tx_i - w_o) - 1 + \\xi_i ) - \\sum_{i=1}^N \\mu_i \\xi_i + C \\sum_{i=1}^N \\xi_i 123456789101112131415161718192021222324252627282930313233np.random.seed(0)X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]Y = [-1] * 20 + [1] * 20plotnum = 1for name, penalty in (('C=10', 10), ('C=0.1', 0.1)): clf = SVC(kernel='linear', C=penalty).fit(X, Y) xx = np.linspace(-5, 5) x_jin = -5 x_jax = 5 y_jin = -9 y_jax = 9 XX, YY = np.mgrid[x_jin:x_jax:200j, y_jin:y_jax:200j] levels = [-1, 0, 1] linestyles = ['dashed', 'solid', 'dashed'] Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()]) Z = Z.reshape(XX.shape) plt.subplot(1, 2, plotnum) plt.contour(XX, YY, Z, levels, colors='k', linestyles=linestyles) plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=120, linewidth=4) plt.scatter(X[:, 0], X[:, 1], c=Y, s=60, linewidth=1, cmap=plt.cm.Paired) plt.xlim(x_jin, x_jax) plt.ylim(y_jin, y_jax) plt.title(name) plotnum += 1plt.suptitle(\"ìŠ¬ë™ë³€ìˆ˜ ê°€ì¤‘ì¹˜ Cì˜ ì˜í–¥\")plt.tight_layout()plt.show() ë‹¤ì‹œ í•œë²ˆ ì •ë¦¬í•˜ìë©´, ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì‚´í´ë³´ë©´ ì§ì„ ì˜ ë°©ì •ì‹ $x^{T} \\beta + \\beta_{0}$ì™€ ê° ì„œí¬íŠ¸ ë²¡í„° $x(x^{+}, x^{-})$ì™€ì˜ ê±°ë¦¬ê°€ $\\frac{1}{\\beta$}ì´ë¯€ë¡œ ê²°êµ´ ë§ˆì§„ì„ í¬ê²Œ í•˜ëŠ” ê²ƒì€ $\\beta$ë¥¼ ì‘ê²Œ í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•œ ì˜ë¯¸ì´ë‹¤. ê·¸ëŸ¬í•œ ì¸¡ë©´ì—ì„œë„ ìœ„ì—ì„œ ìì„¸íˆ ì–¸ê¸‰í–ˆë“¯ì´ Cost function(ëª©ì í•¨ìˆ˜, ë¹„ìš©í•¨ìˆ˜)ê°€ ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜¨ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ë™ì‹œì— errorë¥¼ ìµœì†Œí™”í•˜ê³  ì‹¶ìœ¼ë¯€ë¡œ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ $C$ë¥¼ í¬ê²Œê°€ì ¸ê°€ë©´ì„œ errorë¥¼ í—ˆìš©í•˜ëŠ” slackë³€ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë™ì‹œì— $\\beta$ \u001fì˜ ê°’ë„ ìµœì†Œí™”í•˜ëŠ” optimization ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆë‹¤. ìœ„ì—ì„œì˜ ì¡°ê±´í•˜ì— ìµœì í™”ë¥¼ í•˜ëŠ” ê²ƒì´ë¯€ë¡œ ë¼ê·¸ë‘ì£¼ ìŠ¹ìˆ˜ë¥¼ ë„ì…í•˜ì—¬ ë¶€ë“±ì‹ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œë¥¼ í’€ê²Œ ëœë‹¤. KKTì¡°ê±´ì„ ë§Œì¡±í•¨ìœ¼ë¡œì„œ, global minimumì„ ë³´ì¥ë°›ì„ ìˆ˜ ìˆë‹¤. ì¦‰, KKTì˜ 2ë²ˆì§¸ ì¡°ê±´ì— ì˜í•´ì„œ ì„œí¬íŠ¸ë²¡í„°ì¸ ê²½ìš°ëŠ” ì¡°ê±´ì‹ì´ ì˜ë¯¸ê°€ ìˆê¸° ë•Œë¬¸ì— ë¼ê·¸ë‘ì§€ ìŠ¹ìˆ˜ $\\alpha_{i} \\neq 0$ì´ ëœë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì–¼êµ´ ì´ë¯¸ì§€ ì¸ì‹ ì´ 40ëª…ì´ ê°ê° 10ì¥ì˜ ì¡°ê¸ˆì”© ë‹¤ë¥¸ í‘œì •ì´ë‚˜ ëª¨ìŠµìœ¼ë¡œ ì°ì€ ì´ë¯¸ì§€ ë°ì´í„°ì´ë‹¤. 1234567891011121314151617181920from sklearn.datasets import fetch_olivetti_facesfaces = fetch_olivetti_faces()N = 2M = 5np.random.seed(0)fig = plt.figure(figsize=(9, 5))plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05)klist = np.random.choice(range(len(faces.data)), N * M)for i in range(N): for j in range(M): k = klist[i * M + j] ax = fig.add_subplot(N, M, i * M + j + 1) ax.imshow(faces.images[k], cmap=plt.cm.bone) ax.grid(False) ax.xaxis.set_ticks([]) ax.yaxis.set_ticks([]) plt.title(faces.target[k])plt.tight_layout()plt.show() 123456789101112131415161718192021222324from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(faces.data, faces.target, test_size=0.4, random_state=0)from sklearn.svm import SVCsvc = SVC(kernel='linear').fit(X_train, y_train)N = 2M = 5np.random.seed(4)fig = plt.figure(figsize=(9, 5))plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05)klist = np.random.choice(range(len(y_test)), N * M)for i in range(N): for j in range(M): k = klist[i * M + j] ax = fig.add_subplot(N, M, i * M + j + 1) ax.imshow(X_test[k:(k + 1), :].reshape(64, 64), cmap=plt.cm.bone) ax.grid(False) ax.xaxis.set_ticks([]) ax.yaxis.set_ticks([]) plt.title(\"%d =&gt; %d\" % (y_test[k], svc.predict(X_test[k:(k + 1), :])[0]))plt.tight_layout()plt.show() 1234from sklearn.metrics import classification_report, accuracy_scorey_pred_train = svc.predict(X_train)y_pred_test = svc.predict(X_test) 1print(classification_report(y_train, y_pred_train)) ê²°ê³¼12345678910111213141516171819202122232425262728293031323334353637383940414243444546 precision recall f1-score support 0 1.00 1.00 1.00 4 1 1.00 1.00 1.00 5 2 1.00 1.00 1.00 6 3 1.00 1.00 1.00 8 4 1.00 1.00 1.00 8 5 1.00 1.00 1.00 5 6 1.00 1.00 1.00 4 7 1.00 1.00 1.00 7 8 1.00 1.00 1.00 8 9 1.00 1.00 1.00 7 10 1.00 1.00 1.00 4 11 1.00 1.00 1.00 6 12 1.00 1.00 1.00 6 13 1.00 1.00 1.00 6 14 1.00 1.00 1.00 4 15 1.00 1.00 1.00 4 16 1.00 1.00 1.00 8 17 1.00 1.00 1.00 4 18 1.00 1.00 1.00 9 19 1.00 1.00 1.00 4 20 1.00 1.00 1.00 9 21 1.00 1.00 1.00 6 22 1.00 1.00 1.00 7 23 1.00 1.00 1.00 5 24 1.00 1.00 1.00 6 25 1.00 1.00 1.00 5 26 1.00 1.00 1.00 5 27 1.00 1.00 1.00 8 28 1.00 1.00 1.00 6 29 1.00 1.00 1.00 4 30 1.00 1.00 1.00 6 31 1.00 1.00 1.00 5 32 1.00 1.00 1.00 6 33 1.00 1.00 1.00 7 34 1.00 1.00 1.00 4 35 1.00 1.00 1.00 7 36 1.00 1.00 1.00 6 37 1.00 1.00 1.00 6 38 1.00 1.00 1.00 9 39 1.00 1.00 1.00 6 accuracy 1.00 240 macro avg 1.00 1.00 1.00 240weighted avg 1.00 1.00 1.00 240 1print(classification_report(y_test, y_pred_test)) ê²°ê³¼12345678910111213141516171819202122232425262728293031323334353637383940414243444546 precision recall f1-score support 0 0.86 1.00 0.92 6 1 1.00 1.00 1.00 5 2 1.00 1.00 1.00 4 3 0.50 1.00 0.67 2 4 1.00 0.50 0.67 2 5 1.00 1.00 1.00 5 6 0.83 0.83 0.83 6 7 1.00 0.67 0.80 3 8 0.67 1.00 0.80 2 9 1.00 1.00 1.00 3 10 1.00 1.00 1.00 6 11 1.00 1.00 1.00 4 12 0.67 1.00 0.80 4 13 1.00 1.00 1.00 4 14 1.00 1.00 1.00 6 15 1.00 0.33 0.50 6 16 0.67 1.00 0.80 2 17 1.00 1.00 1.00 6 18 1.00 1.00 1.00 1 19 1.00 1.00 1.00 6 20 1.00 1.00 1.00 1 21 1.00 0.75 0.86 4 22 1.00 1.00 1.00 3 23 0.71 1.00 0.83 5 24 1.00 1.00 1.00 4 25 1.00 1.00 1.00 5 26 1.00 1.00 1.00 5 27 1.00 1.00 1.00 2 28 1.00 1.00 1.00 4 29 1.00 1.00 1.00 6 30 1.00 1.00 1.00 4 31 1.00 1.00 1.00 5 32 1.00 1.00 1.00 4 33 1.00 1.00 1.00 3 34 1.00 0.83 0.91 6 35 1.00 0.67 0.80 3 36 1.00 1.00 1.00 4 37 1.00 1.00 1.00 4 38 0.50 1.00 0.67 1 39 0.67 0.50 0.57 4 accuracy 0.93 160 macro avg 0.93 0.93 0.91 160weighted avg 0.95 0.93 0.92 160","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"í¼ì…‰íŠ¸ë¡ ","slug":"machine_learning_10","date":"2020-04-21T06:21:17.000Z","updated":"2020-04-26T09:35:27.428Z","comments":true,"path":"2020/04/21/machine_learning_10/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/21/machine_learning_10/","excerpt":"","text":"í¼ì…‰íŠ¸ë¡  í¼ì…‰íŠ¸ë¡ (perceptron)ì€ ê°€ì¥ ì˜¤ë˜ë˜ê³  ë‹¨ìˆœí•œ í˜•íƒœì˜ íŒë³„í•¨ìˆ˜ê¸°ë°˜ ë¶„ë¥˜ëª¨í˜• ì¤‘ í•˜ë‚˜ì´ë‹¤. í¼ì…‰íŠ¸ë¡ ì€ ì…ë ¥ $x = (1, x_{1}, \\cdots, x_{m})$ì— ëŒ€í•´ $1$ ë˜ëŠ” $-1$ì˜ ê°’ì„ ê°€ì§€ëŠ” $y$ë¥¼ ì¶œë ¥í•˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜ì´ë‹¤. 1ì„ í¬í•¨í•˜ëŠ” ì…ë ¥ ìš”ì†Œ $x_{i}$ì— ëŒ€í•´ ê°€ì¤‘ì¹˜ $w_{i}$ë¥¼ ê³±í•œ ê°’ $a = w^{T}x$ì„ í™œì„±í™”ê°’(activations)ì´ë¼ê³  í•˜ë©° ì´ ê°’ì´ íŒë³„í•¨ìˆ˜ì˜ ì—­í• ì„ í•œë‹¤. a = w^{T}x íŒë³„ í•¨ ê°’ì´ í™œì„±í™”í•¨ìˆ˜(activation function) $h(a)$ë¥¼ ì§€ë‚˜ë©´ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì¶œë ¥ $\\hat{y}$ê°€ ìƒì„±ëœë‹¤. \\hat{y} = h(w^{T}x) í¼ì…‰íŠ¸ë¡ ì˜ í™œì„±í™” í•¨ìˆ˜ëŠ” ë¶€í˜¸ í•¨ìˆ˜(sign function) ë˜ëŠ” ë‹¨ìœ„ ê³„ë‹¨ í•¨ìˆ˜(Heaviside step function)ë¼ê³  ë¶€ë¥´ëŠ” í•¨ìˆ˜ì´ë‹¤. h(a) = \\begin{cases} -1, & a < 0, \\\\ 1, & a \\ge 0 \\end{cases}í¼ì…‰íŠ¸ë¡  ì†ì‹¤í•¨ìˆ˜ë‹¤ìŒê³¼ ê°™ì´ Nê°œì˜ í•™ìŠµìš© ë°ì´í„°ê°€ ìˆë‹¤ê³  í•˜ì.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"LDA, QDA","slug":"machine_learning_09","date":"2020-04-19T13:26:35.000Z","updated":"2020-04-26T09:35:09.173Z","comments":true,"path":"2020/04/19/machine_learning_09/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/19/machine_learning_09/","excerpt":"","text":"LDA(ì„ í˜•íŒë³„ë¶„ì„ë²•), QDA(ì´ì°¨íŒë³„ë¶„ì„ë²•) ì„ í˜•íŒë³„ ë¶„ì„ë²•(Linear discriminant analysis, LDA)ê³¼ ì´ì°¨íŒë³„ ë¶„ì„ë²•(quadratic discriminant analysis, QDA)ëŠ” ëŒ€í‘œì ì¸ í™•ë¥ ë¡ ì  ìƒì„±ëª¨í˜•ì´ë‹¤. ê°€ëŠ¥ë„ yì˜ í´ë˜ìŠ¤ê°’ì— ë”°ë¥¸ xì˜ ë¶„í¬ì— ëŒ€í•œ ì •ë³´ë¥¼ ë¨¼ì € ì•Œì•„ë‚¸ í›„, ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ xì— ëŒ€í•œ yì˜ í™•ë¥ ë¶„í¬ë¥¼ ì°¾ì•„ë‚¸ë‹¤. ìƒì„±ëª¨í˜• ìƒì„±ëª¨í˜•ì—ì„œëŠ” ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¡°ê±´ë¶€ í™•ë¥  $p(y = k \\mid x)$ë¥¼ ê³„ì‚°í•œë‹¤. P(y = k \\mid x) = \\dfrac{P(x \\mid y = k)\\; P(y = k)}{P(x)} ë¶„ë¥˜ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ì„œëŠ” ê° í´ë˜ìŠ¤ $k$ì— ëŒ€í•œ í™•ë¥ ì„ ë¹„êµí•˜ì—¬ ê°€ì¥ í° ê°’ì„ ì„ íƒí•œë‹¤. ë”°ë¼ì„œ ëª¨ë“  í´ë˜ìŠ¤ì— ëŒ€í•´ ê°’ì´ ê°™ì€ ë¶„ëª¨ $P(x)$ì€ êµ³ì´ ê³„ì‚°í•˜ì§€ ì•Šì•„ë„ ê´œì°®ë‹¤. P(y = k \\mid x) \\;\\; \\propto \\;\\; P(x \\mid y = k) \\; P(y = k) ì—¬ê¸°ì—ì„œ ì‚¬ì „í™•ë¥  $P(y=k)$ëŠ” íŠ¹ë³„í•œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°, ë‹¤ìŒì²˜ëŸ¼ ê³„ì‚°í•œë‹¤. P(y = k) \\approx \\frac{ y = k \\;\\;ì¸\\;ë°ì´í„°\\;ìˆ˜\\;}{ëª¨ë“ \\;ë°ì´í„°ì˜\\;ìˆ˜} ë§Œì•½ ë‹¤ë¥¸ ì§€ì‹ì´ë‚˜ ì •ë³´ë¡œ ì•Œê³ ìˆëŠ” ì‚¬ì „í™•ë¥  ê°’ì´ ìˆë‹¤ë©´ ê·¸ ê°’ì„ ì‚¬ìš©í•˜ë©´ ëœë‹¤. $y$ì— ëŒ€í•œ $x$ì˜ ì¡°ê±´ë¶€í™•ë¥ ì¸ ê°€ëŠ¥ë„ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•œë‹¤. 1) $P(x \\mid y = k)$ê°€ íŠ¹ì •í•œ í™•ë¥ ë¶„í¬ ëª¨í˜•ì„ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•œë‹¤. ì¦‰, í™•ë¥ ë°€ë„ í•¨ìˆ˜ì˜ í˜•íƒœë¥¼ ê°€ì •í•œë‹¤. 2) $k$ë²ˆì§¸ í´ë˜ìŠ¤ì— ì†í•˜ëŠ” í•™ìŠµ ë°ì´í„° $\\{x_1, \\cdots, x_N\\}$ì„ ì‚¬ìš©í•˜ì—¬ ì´ ëª¨í˜•ì˜ ëª¨ìˆ˜ê°’ì„ êµ¬í•œë‹¤. 3) ëª¨ìˆ˜ê°’ì„ ì•Œê³  ìˆìœ¼ë¯€ë¡œ $P(x \\mid y = k)$ì˜ í™•ë¥  ë°€ë„ í•¨ìˆ˜ë¥¼ êµ¬í•œ ê²ƒì´ë‹¤. ì¦‰, ìƒˆë¡œìš´ ë…ë¦½ë³€ìˆ˜ ê°’ xì´ ì–´ë–¤ ê°’ì´ ë˜ë”ë¼ë„ $P(x \\mid y = k)$ì˜ ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. LDA(Linear Discriminant Analysis) 3ê°€ì§€(1,2,3) ë²”ì£¼ì˜ í´ë˜ìŠ¤ë¥¼ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ê°€ ì•„ë˜ì™€ ê°™ì„ ë•Œ, ê·¸ë¦¼ì„ ë³´ë©´ ì¤‘ì‹¬ì„ ê¸°ì ìœ¼ë¡œ ë§ˆì¹˜ ì •ê·œë¶„í¬ì™€ ë¹„ìŠ·í•˜ê²Œ ì¤‘ì‹¬ê³¼ì˜ ê±°ë¦¬ê°€ ë©€ì–´ì§ˆìˆ˜ë¡ ë¶„í¬ì˜ ë°€ë„ê°€ ë–¨ì–´ì§€ëŠ” ë“¯í•´ ë³´ì¸ë‹¤ë©´ LDAë¡œ ë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“¤ì–´ì•¼ í•œë‹¤. LDA(Linear Discriminant Analysis)ëŠ” ì„ í˜• íŒë³„ ë¶„ì„ë²•ìœ¼ë¡œ ë¶ˆë¦¬ë©°, PCAì™€ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤. LDAëŠ” PCAì™€ ìœ ì‚¬í•˜ê²Œ ì…ë ¥ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì €ì°¨ì› ê³µê°„ì— projectioní•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ê¸°ë²•ì´ì§€ë§Œ, ì¤‘ìš”í•œ ì°¨ì´ëŠ” LDAëŠ” ì§€ë„í•™ìŠµì˜ ë¶„ë¥˜(Classification)ì—ì„œ ì‚¬ìš©í•˜ê¸° ì‰½ë„ë¡ ê°œë³„ í´ë˜ìŠ¤ë¥¼ ë¶„ë³„í•  ìˆ˜ ìˆëŠ” ê¸°ì¤€ì„ ìµœëŒ€í•œ ìœ ì§€í•˜ë©´ì„œ ì°¨ì›ì„ ì¶•ì†Œí•œë‹¤. PCAëŠ” ì…ë ¥ ë°ì´í„°ì˜ ë³€ë™ì„±ì˜ ê°€ì¥ í° ì¶•ì„ ì°¾ì•˜ì§€ë§Œ LDAëŠ” ì…ë ¥ ë°ì´í„°ì˜ ê²°ì • ê°’ í´ë˜ìŠ¤ë¥¼ ìµœëŒ€í•œ ë¶„ë¦¬ í•  ìˆ˜ ì¶•ì„ ì°¾ëŠ”ë‹¤. ë¨¼ì €, LDAë„ ì´ì „ì˜ ë‚˜ì´ë¸Œ ë² ì´ì§€ì•ˆ ëª¨í˜•ê³¼ ê°™ì´ íŠ¹ì • ê°€ì •ì´ ì¡´ì¬í•œë‹¤. í´ë˜ìŠ¤ ì§‘ë‹¨ë³„ë¡œ ë™ì¼í•œ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ì§€ë…”ë‹¤ëŠ” ê°€ì • í´ë˜ìŠ¤ ì§‘ë‹¨ë³„ë¡œ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê°€ì • ì•„ë˜ ê·¸ë¦¼ì—ì„œ ê°€ì •ì„ ì ìš©í•œ í›„ì˜ ê·¸ë¦¼ì„ ë³´ë©´ 3ê°€ì§€ ì˜ì—­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì ¸ ìˆëŠ”ë°, ì´ëŸ¬í•œ ì˜ì—­ì€ ì–´ë–»ê²Œ ë‚˜ë‰ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì„¤ëª…í•  ê²ƒì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ 2ì°¨ì›(ë‘ê°€ì§€ ë…ë¦½ë³€ìˆ˜)ì˜ ë‘ ê°€ì§€ ë²”ì£¼ë¥¼ ê°–ëŠ” ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œì—ì„œ LDAëŠ” ë¨¼ì € í•˜ë‚˜ì˜ ì°¨ì›ì— projectionì„ í•˜ì—¬ ì°¨ì›ì„ ì¶•ì†Œì‹œí‚¨ë‹¤. ê·¸ í›„ì— í´ë˜ìŠ¤ë³„ ë¶„í¬ì˜ ë¶„ì‚° ëŒ€ë¹„ í‰ê· ì˜ ì°¨ì´ê°€ í¬ê²Œ ë‚˜ëŠ” ì§€ì (ì¦‰, ë‘ ì§‘ë‹¨ì˜ í‰ê· ì˜ í‰ê· ì )ì„ decision boundaryë¡œ ì„¤ì •í•œë‹¤. ì•„ë˜ ë‘ ê·¸ë¦¼ ì¤‘ ì–´ë– í•œ decision boundaryë¥¼ ê°–ëŠ” ê²ƒì´ ë” ë¶„ë¥˜ë¥¼ ì˜ í•œë‹¤ê³  ìƒê°ì´ë“œëŠ”ê°€? ì•„ë¬´ë˜ë„ ì˜¤ë¥¸ìª½ì„ ì„ íƒí•˜ëŠ” ë¶„ë“¤ì´ ë§ì„ ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ë°ì´í„°ê°€ ë™ì¼í•˜ì—¬ë„ projectionë˜ëŠ” ì¶•ì— ë”°ë¼ decision boundaryê°€ ë‹¬ë¼ì§„ë‹¤.ê·¸ëŸ¬ë¯€ë¡œ projectionë˜ëŠ” ì¶•ì„ ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ìš°ì„  ë¶„ë¥˜ë¥¼ í•˜ë ¤ë©´ ê° í´ë˜ìŠ¤ ì§‘ë‹¨ì˜ í‰ê· ì˜ ì°¨ì´ê°€ í° ì§€ì ì„ decision boundaryë¡œ ì°¾ëŠ”ë‹¤ë©´ ì‰½ê²Œ ë¶„ë¥˜ê°€ ê°€ëŠ¥í•  ê²ƒì´ë¯€ë¡œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë‘ í‰ê·  ë²¡í„°ì˜ ì°¨ì´ì— í‰í–‰í•œ ì¶•ì— projectionì„ í•˜ì—¬ ë‘ í´ë˜ìŠ¤ë¥¼ ë¹„êµí•˜ê²Œ ëœë‹¤. ê·¸ëŸ¬ë‚˜, ë‘ ì§‘ë‹¨ê°„ì˜ ë¶„ì‚°ì€ í¬ê¸° ë•Œë¬¸ì—(ë¶„ì‚°ì´ í¬ë‹¤ëŠ” ê²ƒì€ ë³€ë™ì„±ì´ í¬ë‹¤ëŠ” ì–˜ê¸°ì´ë¯€ë¡œ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ë‘ ì§‘ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ê¸°ì— ë¬´ë¦¬ê°€ ìˆë‹¤.) ì•„ì§ê¹Œì§„ ë¶„ë¥˜ì˜ ëª¨í˜•ìœ¼ë¡œ ë¶€ì¡±í•´ ë³´ì¸ë‹¤. ìœ„ì—ì„œì™€ ê°™ì´ ê° í´ë˜ìŠ¤ ì§‘ë‹¨ì˜ í‰ê· ì˜ ì°¨ì´ë§Œì„ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê° í´ë˜ìŠ¤ ì§‘ë‹¨ì˜ ë¶„ì‚°ì€ ì‘ê²Œë”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ projectionì„ ì‹œì¼œì•¼ í•  ê²ƒì´ë‹¤. ë¶„ì‚°ì´ ì‘ë‹¤ë©´ ê·¸ë§Œí¼ ë°ì´í„°ì˜ ë¶„í¬ê°€ ë°€ì§‘ë˜ì–´ìˆìœ¼ë¯€ë¡œ ë¶„ë¥˜ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ ë˜í•œ ë†’ì•„ì§ˆ ê°€ëŠ¥ì„±ì´ ì»¤ì§€ê¸° ë•Œë¬¸ì´ë‹¤. LDAì˜ ìˆ˜í•™ì  ê°œë… ì´í•´ - ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ ì´ë³€ëŸ‰ ì •ê·œ ë¶„í¬ëŠ” ë‘ ë³€ìˆ˜ê°„ì˜ ìƒê´€ê³„ìˆ˜ $\\rho$ê°€ 0ì´ë¼ë©´ ë‘ ë³€ìˆ˜ëŠ” ë…ë¦½ì´ë¯€ë¡œ ê²°êµ­ ìœ„ì˜ ì •ê·œë¶„í¬ì˜ ì‹ì„ ê³±í•œ ê°’ì´ ë  ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜, ìƒê´€ê³„ìˆ˜ $\\rho$ê°€ 0ì´ ì•„ë‹Œ ê°’ìœ¼ë¡œ ì¡´ì¬í•œë‹¤. ì•„ë˜ ìˆ˜ì‹ê³¼ ê°™ì€ jointed probability distributionì„ ê°–ëŠ”ë‹¤. 3D ì´ë¯¸ì§€ë¡œ ì‚´í´ë³¸ë‹¤ë©´ $\\rho$ì˜ ì ˆëŒ€ê°’ì´ ë†’ì•„ì§ˆìˆ˜ë¡ ë” ê°•í•œ ì„ í˜•ì„±ì„ ê°–ìœ¼ë©° ì „ì²´ feature ê³µê°„ì— ëŒ€í•´ì„œëŠ” ë¹„ëŒ€ì¹­ì ì´ê²Œ ë˜ì–´ì§„ë‹¤. ì¦‰, ê° ì¶•ì˜ ë°©í–¥ì—ì„œ ë³´ì•˜ì„ ê²½ìš° ë¶„í¬ì˜ ì´ë¯¸ì§€ê°€ ë‹¬ë¼ì§€ê²Œ ëœë‹¤. ë‹¤ë³€ëŸ‰ ì •ê·œ ë¶„í¬ë¡œ í™•ì¥í•˜ê¸°ì— ì•ì„œì„œ, ë¨¼ì € ì´ë³€ëŸ‰ ì •ê·œë¶„í¬ì— ëŒ€í•´ ì •ë¦¬í•œ í›„ ê·¸ ê°œë…ì„ í™•ì¥ì‹œí‚¬ ê²ƒì´ë‹¤. ì´ë³€ëŸ‰ ì •ê·œë¶„í¬ì˜ ìˆ˜ì‹ì€ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤. ë§Œì•½ $\\rho$ê°€ 0ì´ ë¼ë©´ ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ ê²ƒê³¼ ê°™ì´ ë‘ ì •ê·œë¶„í¬ ìˆ˜ì‹ì„ ë‹¨ìˆœí•œ ê³±í•œ ê²ƒê³¼ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ì˜ ì‹ì— $2X2$ ê³µë¶„ì‚°í–‰ë ¬ì„ ëŒ€ì…í•˜ì—¬ ê³„ì‚°í•œë‹¤ë©´, ì´ë³€ëŸ‰ ì •ê·œë¶„í¬ì‹ì´ ë‚˜ì˜¤ê²Œ ëœë‹¤. ë¨¼ì € LDAëŠ” í™•ë¥ ì  ìƒì„±ëª¨í˜•ì´ë¯€ë¡œ í™•ë¥ ê°’ì„ ê³„ì‚°í•´ë³¸ë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ kë²ˆì§¸ ë²”ì£¼ ì§‘ë‹¨ì˜ ë¶„í¬í•¨ìˆ˜ëŠ” ì •ì˜ ë  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´, ìƒˆë¡­ê²Œ ë“¤ì–´ì˜¤ëŠ” ê´€ì¸¡ì¹˜ì— ëŒ€í•´ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•  ê²½ìš° ì–´ë–»ê²Œ í™•ë¥ ê°’ì„ ê³„ì‚°í•´ì•¼ í• ê¹Œ? ë‹¤ìŒê³¼ ê°™ì´ ë²”ì£¼ì—ì„œ 2ê°œë¥¼ í•œ ìŒìœ¼ë¡œ ë½‘ëŠ” ê²½ìš°ì˜ ê°€ì§€ìˆ˜$_{4}C_{2}$ ë§Œí¼ì˜ ê³„ì‚°ì„ í†µí•´ ë¹„êµí•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ í™•ë¥ ê°’ì´ ê°€ì¥ ë†’ì€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë¥¼ í•  ê²ƒì´ë‹¤. ì•„ë˜ ìˆ˜ì‹ì—ì„œ ê°€ì¥ ë§ˆì§€ë§‰ ìˆ˜ì‹ì€ ì´ì°¨ì‹ ì²˜ëŸ¼ ëª¨ì–‘ì´ ë‚˜ì˜¤ê²Œ ë˜ëŠ”ë°, ê·¸ ì´ìœ ëŠ” ë‘ ë²¡í„°ì˜ ë‚´ì ì´ ë˜ê¸° ë•Œë¬¸ì— ì–´ë–¤ ë²¡í„°ë¥¼ ë¨¼ì € ê³±í•˜ëŠ”ì§€ëŠ” ìƒê´€ì´ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ì¦‰, $x^{T} \\sum^{-1} \\mu_{k} = \\mu_{k}^{T} \\sum^{-1} x $ì´ê¸° ë•Œë¬¸ì´ë‹¤. Linear Discriminant Analysisë¡œ ë¶ˆë¦¬ëŠ” ì´ìœ ëŠ” ì•„ë˜ì™€ ê°™ì´ feature xì— ëŒ€í•œ 1ì°¨ì‹ì˜ í˜•íƒœ(ì„ í˜•êµ¬ì¡°)ì´ê¸° ë•Œë¬¸ì´ë‹¤. ì—¬ê¸°ì„œ ìœ ì¶”í•  ìˆ˜ ìˆëŠ” ì ì€ ìœ„ì˜ ì‹ì„ ì–»ê¸° ìœ„í•´ì„  LDAì˜ ì¤‘ìš”í•œ ê°€ì • ì¤‘ í•˜ë‚˜ì¸ í´ë˜ìŠ¤ ì§‘ë‹¨ ë³„ ë™ì¼í•œ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°–ê³ ìˆëŠ”ë‹¤ëŠ” ê°€ì • ë•Œë¬¸ì´ì—ˆë‹¤. ë§Œì•½, í´ë˜ìŠ¤ ì§‘ë‹¨ ë³„ ë™ì¼í•˜ì§€ ì•Šì€ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°–ëŠ”ë‹¤ë©´ ìœ„ì˜ ì‹ì—ì„œ ê³µë¶„ì‚° í–‰ë ¬ì„ ê¸°ì¤€ìœ¼ë¡œ ê³±í•´ì§€ëŠ” 2ì°¨í˜•ì‹(Quadratic form)ì´ ì—†ì–´ì§€ì§€ ì•Šì„ ê²ƒì´ë‹¤. í´ë˜ìŠ¤ ì§‘ë‹¨ ë³„ ë‹¤ë¥¸ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°–ëŠ”ë‹¤ëŠ” ê°€ì •ì„ í•œë‹¤ë©´ QDAê°€ ëœë‹¤. LDA(Linear Discriminant Analysis)ì—ì„œëŠ” ê° $Y$ í´ë˜ìŠ¤ì— ëŒ€í•œ ë…ë¦½ë³€ìˆ˜ $X$ì˜ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ê°€ ê³µí†µëœ ê³µë¶„ì‚° í–‰ë ¬ì„ ê°€ì§€ëŠ” ë‹¤ë³€ìˆ˜ ì •ê·œë¶„í¬(multivariate Gaussian normal distribution)ì´ë¼ê³  ê°€ì •í•œë‹¤. \\Sigma_k = \\Sigma \\;\\;\\; \\text{ for all } k ì´ ë•ŒëŠ” ì¡°ê±´ë¶€í™•ë¥ ë¶„í¬ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤. \\begin{eqnarray} \\log p(x \\mid y = k) &=& \\log \\dfrac{1}{(2\\pi)^{D/2} |\\Sigma|^{1/2}} - \\dfrac{1}{2} (x-\\mu_k)^T \\Sigma^{-1} (x-\\mu_k) \\\\ &=& C_0 - \\dfrac{1}{2} (x-\\mu_k)^T \\Sigma^{-1} (x-\\mu_k) \\\\ &=& C_0 - \\dfrac{1}{2} \\left( x^T\\Sigma^{-1}x - 2\\mu_k^T \\Sigma^{-1}x + \\mu_k^T \\Sigma^{-1}\\mu_k \\right) \\\\ &=& C(x) + \\mu_k^T \\Sigma^{-1}x - \\dfrac{1}{2} \\mu_k^T \\Sigma^{-1}\\mu_k \\\\ \\end{eqnarray}\\begin{eqnarray} p(x \\mid y = k) &=& C'(x)\\exp(w_k^Tx + w_{k0}) \\\\ \\end{eqnarray} ì´ ì‹ì—ì„œ $Câ€™(x)=exp C(x)$ì´ë‹¤. \\begin{eqnarray} P(y=k \\mid x) &=& \\dfrac{p(x \\mid y = k)P(y=k)}{\\sum_l p(x \\mid y = l)P(y=l) } \\\\ &=& \\dfrac{C'(x)\\exp(w_k^Tx + w_{k0}) P(y=k)}{\\sum_l C'(x)\\exp(w_l^Tx + w_{l0})P(y=l) } \\\\ &=& \\dfrac{C'(x)\\exp(w_k^Tx + w_{k0}) P(y=k)}{C'(x)\\sum_l \\exp(w_l^Tx + w_{l0})P(y=l) } \\\\ &=& \\dfrac{P(y=k) \\exp(w_k^Tx + w_{k0}) }{\\sum_l P(y=l) \\exp(w_l^Tx + w_{k0})} \\\\ &=& \\dfrac{P(y=k) \\exp(w_k^Tx + w_{k0}) }{P(x)} \\\\ \\end{eqnarray} ì´ ì‹ì—ì„œ $P(x)$ëŠ” $y$í´ë˜ìŠ¤ê°’ì— ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ \\log P(y=k \\mid x) = \\log P(y=k) + w_k^Tx + w_{k0} - \\log{P(x)} = w_k^Tx + C''_k ëª¨ë“  í´ë˜ìŠ¤ kì— ëŒ€í•´ ìœ„ì™€ ê°™ì€ ì‹ì´ ì„±ë¦½í•˜ë¯€ë¡œ í´ë˜ìŠ¤ $k_{l}$ê³¼ í´ë˜ìŠ¤ $k_{m}$ì˜ ê²½ê³„ì„ , ì¦‰ ë‘ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ê°’ì´ ê°™ì•„ì§€ëŠ” $x$ì˜ ìœ„ì¹˜ë¥¼ ì°¾ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. w_{k_1}^Tx + C''_{k_1} = w_{k_2}^Tx + C''_{k_2}(w_{k_1} - w_{k_2})^Tx + (C''_{k_1} - C''_{k_2}) = 0w^Tx + C = 0 ì¦‰, íŒë³„í•¨ìˆ˜ê°€ xì— ëŒ€í•œ ì„ í˜•ë°©ì •ì‹ì´ ë˜ê³  ê²½ê³„ì„ ì˜ ëª¨ì–‘ì´ ì§ì„ ì´ ëœë‹¤. LDAì™€ eigen valueì™€ eigen vectorì™€ì˜ ì—°ê´€ì„± ë²¡í„° $a$ë¥¼ ë‹¤ë¥¸ ë²¡í„° $b$ì— ì§êµí•˜ëŠ” ì„±ë¶„ê³¼ ë²¡í„° $b$ì— í‰í–‰í•œ ì„±ë¶„ìœ¼ë¡œ ë¶„í•´í•  ìˆ˜ ìˆëŠ”ë°, í‰í–‰í•œ ì„±ë¶„ì„ ë²¡í„° $b$ì— ëŒ€í•œ íˆ¬ì˜ì„±ë¶„(projection), ë²¡í„° $b$ì— ëŒ€í•œ ì§êµì„±ë¶„(rejection)ì´ë¼ê³  í•˜ë©° ê°ê°ì„ ë‹¤ìŒê³¼ ê°™ì´ í‘œê¸°í•œë‹¤. \\begin{align} a^{\\Vert b} \\tag{3.1.37} \\end{align}\\begin{align} a^{\\perp b} \\tag{3.1.38} \\end{align} íˆ¬ì˜ì„±ë¶„ì˜ ê¸¸ì´ëŠ” ë‹¤ìŒì²˜ëŸ¼ êµ¬í•  ìˆ˜ ìˆë‹¤. \\begin{align} \\| a^{\\Vert b} \\| = \\|a\\|\\cos\\theta = \\dfrac{\\|a\\|\\|b\\|\\cos\\theta}{\\|b\\|} = \\dfrac{a^Tb}{\\|b\\|} = \\dfrac{b^Ta}{\\|b\\|} = a^T\\dfrac{b}{\\|b\\|} \\end{align} ë§Œì•½ ë²¡í„° $b$ìì²´ê°€ ì´ë¯¸ ë‹¨ìœ„ë²¡í„°(Unit vector)ì´ë©´ ë‹¨ìœ„ë²¡í„°ì— ëŒ€í•œ íˆ¬ì˜ê¸¸ì´ëŠ” ë‚´ì ì´ ëœë‹¤. \\begin{align} \\| a^{\\Vert b} \\| = a^Tb \\end{align} íˆ¬ì˜ì„±ë¶„ ì„±ë¶„ ë²¡í„°ëŠ” íˆ¬ì˜ì„±ë¶„ ê¸¸ì´ì™€ ë²¡í„° $b$ë°©í–¥ì˜ ë‹¨ìœ„ë²¡í„°ì˜ ê³±ì´ë‹¤. \\begin{align} a^{\\Vert b} = \\dfrac{a^Tb}{\\|b\\|} \\dfrac{b}{\\|b\\|}= \\dfrac{a^Tb}{\\|b\\|^2}b \\end{align} ì§êµì„±ë¶„ ë²¡í„°ëŠ” ì›ë˜ì˜ ë²¡í„°ì—ì„œ íˆ¬ì˜ì„±ë¶„ ì„±ë¶„ ë²¡í„°ë¥¼ ëº€ ë‚˜ë¨¸ì§€ì´ë‹¤. \\begin{align} a^{\\perp b} = a - a^{\\Vert b} \\end{align}ì°¸ê³  : projectionê³¼ rejectionì°¸ê³  : ì œ2 ì½”ì‚¬ì¸ë²•ì¹™ì„ ì‚¬ìš©í•œ ë‚´ì ê³¼ì˜ ì—°ê´€ì„± ì´ëŸ¬í•œ Projectionì´ LDAì™€ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ë©´ ë¨¼ì €, LDAì˜ ëª©í‘œì¸ í´ë˜ìŠ¤ ì§‘ë‹¨ ê°„ì˜ í‰ê· ì€ í¬ê²Œí•˜ê³  ë¶„ì‚°ì€ ì‘ê²Œí•˜ëŠ” decision boundaryê°€ ì¡´ì¬í•˜ëŠ” Projection ê³µê°„ì„ ë§ í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë§í•˜ëŠ” ì‚¬ì˜ì‹œí‚¬ ë²¡í„° $a$ë¼ëŠ” ê²ƒì€ decision boundaryë¥¼ ì°¾ê¸°ìœ„í•´ í•´ë‹¹ ë°ì´í„°ë“¤ì„ íˆ¬ì˜ì‹œí‚¨ ë²¡í„°ë¼ëŠ” ì˜ë¯¸ì´ë‹¤. ìœ„ì˜ ê·¸ë¦¼ì—ì„œëŠ” ë¶„í¬ë¡œ ì¡´ì¬í•˜ëŠ” ê³µê°„ì˜ ì¶•ì„ ì˜ë¯¸í•œë‹¤. ë‹¨ ì•„ë˜ ê·¸ë¦¼ì˜ ìˆ˜ì‹ë“¤ì´ ì •í™•í•˜ê¸° ìœ„í•´ì„  ë¨¼ì € ê° ë°ì´í„°ì˜ feature ë²¡í„°ë“¤ì´ Unit vectorì´ê³  ì‚¬ì˜ëœ ë²¡í„°ë„ Unit vectorì¸ ê²½ìš°ì— í•œí•´ì„œ ì•„ë˜ì˜ ìˆ˜ì‹ì´ ì •í™•í•˜ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. LDAì˜ ëª©í‘œì— ë§ê²Œ í´ë˜ìŠ¤ ì§‘ë‹¨ì˜ í‰ê· ì˜ ì°¨ì´ëŠ” í¬ê²Œí•˜ë©´ì„œ ë¶„ì‚°ì€ ìµœì†Œí™” ì‹œí‚¤ëŠ” ì‚¬ì˜ì„ ì°¾ëŠ”ê²ƒì´ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì€ ë¶„ìˆ˜ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ê° í‰ê· ê³¼ ë¶„ì‚°ì€ projectionëœ ì„±ë¶„ì„ ê°€ë¦¬í‚¨ë‹¤. í–‰ë ¬í‘œí˜„ìœ¼ë¡œ ë¬¶ì–´ì„œ ë‹¤ë¥´ê²Œ í‘œí˜„í•œë‹¤ë©´ ë§¨ ë§ˆì§ë§‰ì˜ ìˆ˜ì‹ë“¤ë¡œ í‘œí˜„ ê°€ëŠ¥í•˜ë‹¤. í–‰ë ¬ì‹ì— ëŒ€í•´ ê°€ì¥ í° aê°’ì„ ì°¾ê¸° ìœ„í•´ì„œëŠ” ë¯¸ë¶„ì„ í•´ì•¼ í•  ê²ƒì´ë‹¤. ë¯¸ë¶„ì„ í†µí•´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì€ í˜•íƒœë¡œ ë³€í˜•ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ë§¨ ë§ˆì§€ë§‰ ìˆ˜ì‹ì„ ì‚´í´ë³´ë©´ eigen valueì™€ eigen vectorì˜ ì •ì˜ì™€ ë™ì¼í•˜ê²Œ ë³¼ ìˆ˜ ìˆìŒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ìœ„ì˜ ì‹ì„ ë‹¤ì‹œí•œë²ˆ ì •ë¦¬í•˜ë©´ eigen vectorì™€ eigen valueë¥¼ êµ¬í•  ìˆ˜ ìˆëŠ”ë°, í•´ë‹¹ ë°ì´í„°ë¥¼ ì¶•ì†Œí•˜ëŠ”ë° ë°©í–¥ì€ ë³€í•˜ì§€ ì•Šê³  ê¸¸ì´ë§Œ ë³€í•˜ëŠ” ë²¡í„°ì¸ eigen vectorë¥¼ ì°¾ìœ¼ë©´ LDAì˜ decision boundaryê°€ ì¡´ì¬í•˜ëŠ” ê³µê°„ì¸ ë²¡í„°ì´ë‹¤. ì´ì „ì— decision boundaryë¥¼ ì°¾ëŠ” ê³µì‹ì—ì„œ ì‚´í´ë³´ë©´, ì°¾ì•„ë‚¸ ì¶•ì´ ê¸°ìš¸ê¸° ì—­í• ì„ í•˜ê³  í´ë˜ìŠ¤ ì§‘ë‹¨ì˜ í‰ê· ì„ ì§€ë‚˜ëŠ” ì´ˆí‰ë©´ì´ decision boundaryë¼ëŠ” ì‚¬ì‹¤ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. Scikit-Learnì€ ì„ í˜•íŒë³„ë¶„ì„ë²•ì„ ìœ„í•œ LinearDiscriminantAnalysis í´ë˜ìŠ¤ë¥¼ ì œê³µí•œë‹¤. ì°¸ê³ ë¡œ ë°ì´í„°ëŠ” ì•„ë˜ QDAì™€ ë™ì¼í•œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤. 123from sklearn.discriminant_analysis import LinearDiscriminantAnalysislda = LinearDiscriminantAnalysis(n_components=3, solver=\"svd\", store_covariance=True).fit(X, y) ì„ í˜•íŒë³„ ë¶„ì„ë²•ì—ì„œëŠ” ê¸°ëŒ€ê°’ ë²¡í„°ë§Œ í´ë˜ìŠ¤ì— ë”°ë¼ ë‹¬ë¼ì§€ê³  ê³µë¶„ì‚° í–‰ë ¬ì€ ê³µí†µìœ¼ë¡œ ì¶”ì •í•œë‹¤. 1lda.means_ ê²°ê³¼123array([[-8.01254084e-04, 1.19457204e-01], [ 1.16303727e+00, 1.03930605e+00], [-8.64060404e-01, 1.02295794e+00]]) 1lda.covariance_ ê²°ê³¼12array([[0.7718516 , 0.13942905], [0.13942905, 0.7620019 ]]) ê²°ê³¼ëŠ” ë‹¤ìŒì²˜ëŸ¼ ì§ì„ ì¸ ê²½ê³„ì„ ì„ ê°€ì§„ë‹¤. 1234567891011121314151617x1min, x1max = -5, 5x2min, x2max = -4, 5XX1, XX2 = np.meshgrid(np.arange(x1min, x1max, (x1max-x1min)/1000), np.arange(x2min, x2max, (x2max-x2min)/1000))YY = np.reshape(lda.predict(np.array([XX1.ravel(), XX2.ravel()]).T), XX1.shape)cmap = mpl.colors.ListedColormap(sns.color_palette([\"r\", \"g\", \"b\"]).as_hex())plt.contourf(XX1, XX2, YY, cmap=cmap, alpha=0.5)plt.scatter(X1[:, 0], X1[:, 1], alpha=0.8, s=50, marker=\"o\", color='r', label=\"í´ë˜ìŠ¤ 1\")plt.scatter(X2[:, 0], X2[:, 1], alpha=0.8, s=50, marker=\"s\", color='g', label=\"í´ë˜ìŠ¤ 2\")plt.scatter(X3[:, 0], X3[:, 1], alpha=0.8, s=50, marker=\"x\", color='b', label=\"í´ë˜ìŠ¤ 3\")plt.xlim(x1min, x1max)plt.ylim(x2min, x2max)plt.xlabel(\"$x_1$\")plt.ylabel(\"$x_2$\")plt.legend()plt.title(\"LDA ë¶„ì„ ê²°ê³¼\")plt.show() ì„ í˜•íŒë³„ë¶„ì„ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë¶“ê½ƒ ë¶„ë¥˜ë¬¸ì œë¥¼ í’€ê³  ì„±ëŠ¥ì„ ë¶„ë¥˜ê²°ê³¼í‘œì™€ ë¶„ë¥˜ë³´ê³ ì„œë¥¼ ì¶œë ¥í•˜ë¼. ê·¸ë¦¬ê³  ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ROC ì»¤ë¸Œë¥¼ ê·¸ë ¤ë¼.12from sklearn.datasets import load_irisfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score í´ë˜ìŠ¤ë³„ ê³ ë£¨ í•™ìŠµí•˜ê¸° ìœ„í•´ stratifyí•˜ê²Œ ì„¤ì •í•˜ì˜€ë‹¤. 1234iris = load_iris()X = iris.datay = iris.targettrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1234, stratify=y) 1234LDA = LinearDiscriminantAnalysis()LDA.fit(train_X, train_y)pred_LDA = LDA.predict(test_X)confusion_matrix(pred_LDA, test_y) ê²°ê³¼123array([[15, 0, 0], [ 0, 14, 0], [ 0, 1, 15]]) 1print(classification_report(pred_LDA, test_y)) ê²°ê³¼123456789 precision recall f1-score support 0 1.00 1.00 1.00 15 1 0.93 1.00 0.97 14 2 1.00 0.94 0.97 16 accuracy 0.98 45 macro avg 0.98 0.98 0.98 45weighted avg 0.98 0.98 0.98 45 1roc_auc_score(label_binarize(test_y, classes=[0,1,2]), LDA.predict_proba(test_X)) 12345678910111213label_test_y = label_binarize(test_y, [0, 1, 2])fpr = [None] * 3tpr = [None] * 3thr = [None] * 3for i in range(3): fpr[i], tpr[i], thr[i] = roc_curve(label_test_y[:, i], LDA.predict_proba(test_X)[:, i]) plt.plot(fpr[i], tpr[i])plt.xlabel('Fall-Out')plt.ylabel('Recall')plt.show() QDA(Quadratic Discriminant Analysis) ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ì€ ì¡°ê±´ë¶€ë…ë¦½ì´ë¼ëŠ” ê°€ì •ì´ ìˆì—ˆê¸° ë•Œë¬¸ì— ì‚¬ì‹¤ìƒ ê° ì„¤ëª…ë³€ìˆ˜ë“¤ê°„ì˜ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ë°˜ì˜í•˜ì§„ ì•ŠëŠ”ë‹¤. ê·¸ì— ë°˜í•´, LDAëŠ” ì„¤ëª…ë³€ìˆ˜ê°„ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°–ëŠ”ë‹¤ëŠ” ê°€ì •í•˜ì— ëª¨í˜•ì„ ì‹¤í–‰í•˜ëŠ”ë°, ê³µë¶„ì‚° êµ¬ì¡°ì˜ ì°¨ì´ê°€ ì‹¬í•˜ê²Œ ë‚œë‹¤ë©´, LDA ê°€ì •ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ë‹¤. ì´ëŸ° ê²½ìš°ì— ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë°”ë¡œ QDAì´ë‹¤. LDAëŠ” ê°€ì¥ ì‘ì€ ê·¸ë£¹ì˜ ìƒ˜í”Œ ìˆ˜ê°€ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ì˜ ê°œìˆ˜ë³´ë‹¤ ë§ì•„ì•¼ í•˜ëŠ”ë° ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ì€ ì¡°ê±´ë¶€ ë…ë¦½ì„ ê°€ì •í•˜ê¸°ì— í›¨ì”¬ ë” ì ì€ ìƒ˜í”Œì´ë”ë¼ë„ ì¶”ì •ì´ ê°€ëŠ¥í•˜ë‹¤. ì´ì°¨íŒë³„ë¶„ì„ë²•(QDA)ì—ì„œëŠ” ë…ë¦½ë³€ìˆ˜ xê°€ ì‹¤ìˆ˜ì´ê³  í™•ë¥ ë¶„í¬ê°€ ë‹¤ë³€ëŸ‰ ì •ê·œë¶„í¬ë¼ê³  ê°€ì •í•œë‹¤. ë‹¨ xë¶„í¬ì˜ ìœ„ì¹˜ì™€ í˜•íƒœëŠ” í´ë˜ìŠ¤ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.ì¦‰, í´ë˜ìŠ¤ë³„ë¡œ ë‹¤ë¥¸ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°–ëŠ”ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ê³µë¶„ì‚° êµ¬ì¡°ê°€ ë™ì¼í•˜ì§€ ì•Šì•„ ì´ì°¨í˜•ì‹(Quadratic form)ì´ ë‚¨ê²Œ ëœë‹¤. p(x \\mid y = k) = \\dfrac{1}{(2\\pi)^{D/2} |\\Sigma_k|^{1/2}} \\exp \\left( -\\dfrac{1}{2} (x-\\mu_k)^T \\Sigma_k^{-1} (x-\\mu_k) \\right) ì´ ë¶„í¬ë“¤ì„ ì•Œê³  ìˆìœ¼ë©´ ë…ë¦½ë³€ìˆ˜ $x$ì— ëŒ€í•œ $y$ í´ë˜ìŠ¤ì˜ ì¡°ê±´ë¶€í™•ë¥ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë² ì´ì¦ˆ ì •ë¦¬ì™€ ì „ì²´ í™•ë¥ ì˜ ë²•ì¹™ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤. P(y=k \\mid x) = \\dfrac{p(x \\mid y = k)P(y=k)}{p(x)} = \\dfrac{p(x \\mid y = k)P(y=k)}{\\sum_l p(x \\mid y = l)P(y=l) } LDAëŠ” Decision boundaryê°€ ì„ í˜•ìœ¼ë¡œ ë˜ê¸° ë•Œë¬¸ì— ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë³´ëŠ” ê²ƒê³¼ ê°™ì€ ë°ì´í„° ë¶„í¬ì´ë©´ êµ¬ë¶„í•˜ëŠ”ë° ì–´ë µë‹¤. í—ˆë‚˜ ë³€ìˆ˜ì˜ ì œê³±ì„í•œ ì¶”ê°€ì ì¸ ë³€ìˆ˜ë“¤ì„ í†µí•´ ì´ë¥¼ ë³´ì™„í•  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ì§€ ì•Šê³  í˜„ì¬ ì¡´ì¬í•˜ëŠ” ë³€ìˆ˜ë“¤ë§Œ ì‚¬ìš©í•´ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ì„  QDAë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ì¤‘ê°„ì— ìˆëŠ” ê·¸ë¦¼ì€ í´ë˜ìŠ¤ ë³„ ê°™ì€ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤ê³  ê°€ì •ì„ í•˜ê³  ë³€ìˆ˜ë¥¼ ì¶”ê°€í•´ ì£¼ì–´ ê³„ì‚°ë˜ëŠ” ë°˜ë©´ì— ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì€ ì²˜ìŒ ë°ì´í„° ë³€ìˆ˜ë“¤ë§Œ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. í—ˆë‚˜, QDAëŠ” í´ë˜ìŠ¤ë³„ ì„œë¡œ ë‹¤ë¥¸ ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ ê°€ì§„ë‹¤ëŠ” ê°€ì •ì´ ìˆê¸° ë•Œë¬¸ì— ëª¨ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” íšŸìˆ˜ê°€ ê·¸ ë§Œí¼ ë” ë§ì´ ëŠ˜ì–´ë‚˜ëŠ” ë¬¸ì œë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ $y$ê°€ 1,2,3 ì´ë¼ëŠ” 3ê°œì˜ í´ë˜ìŠ¤ë¥¼ ê°€ì§€ê³  ê° í´ë˜ìŠ¤ì—ì„œì˜ $x$ì˜ í™•ë¥ ë¶„í¬ê°€ ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŒ€ê°’ ë° ê³µë¶„ì‚° í–‰ë ¬ì„ ê°€ì§„ë‹¤ê³  ê°€ì •í•˜ì. \\mu_1 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}, \\;\\; \\mu_2 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\;\\; \\mu_3 = \\begin{bmatrix}-1 \\\\ 1 \\end{bmatrix}\\Sigma_1 = \\begin{bmatrix} 0.7 & 0 \\\\ 0 & 0.7 \\end{bmatrix}, \\;\\; \\Sigma_2 = \\begin{bmatrix} 0.8 & 0.2 \\\\ 0.2 & 0.8 \\end{bmatrix}, \\;\\; \\Sigma_3 = \\begin{bmatrix} 0.8 & 0.2 \\\\ 0.2 & 0.8 \\end{bmatrix} $y$ì˜ ì‚¬ì „ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ì´ ë™ì¼í•˜ë‹¤. P(Y=1) = P(Y=2) = P(Y=3) = \\dfrac{1}{3}1234567891011121314151617181920212223N = 100rv1 = sp.stats.multivariate_normal([ 0, 0], [[0.7, 0.0], [0.0, 0.7]])rv2 = sp.stats.multivariate_normal([ 1, 1], [[0.8, 0.2], [0.2, 0.8]])rv3 = sp.stats.multivariate_normal([-1, 1], [[0.8, 0.2], [0.2, 0.8]])np.random.seed(0)X1 = rv1.rvs(N)X2 = rv2.rvs(N)X3 = rv3.rvs(N)y1 = np.zeros(N)y2 = np.ones(N)y3 = 2 * np.ones(N)X = np.vstack([X1, X2, X3])y = np.hstack([y1, y2, y3])plt.scatter(X1[:, 0], X1[:, 1], alpha=0.8, s=50, marker=\"o\", color='r', label=\"class 1\")plt.scatter(X2[:, 0], X2[:, 1], alpha=0.8, s=50, marker=\"s\", color='g', label=\"class 2\")plt.scatter(X3[:, 0], X3[:, 1], alpha=0.8, s=50, marker=\"x\", color='b', label=\"class 3\")plt.xlim(-5, 5)plt.ylim(-4, 5)plt.xlabel(\"$x_1$\")plt.ylabel(\"$x_2$\")plt.legend()plt.show() Scikit-Learnì€ ì´ì°¨íŒë³„ ë¶„ì„ë²•ì„ ìœ„í•œ QuadraticDiscriminantAnalysisí´ë˜ìŠ¤ë¥¼ ì œê³µí•œë‹¤. 123from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysisqda = QuadraticDiscriminantAnalysis(store_covariance=True).fit(X, y) í•™ìŠµìš© ë°ì´í„°ì—ì„œ ê°€ëŠ¥ë„ë¥¼ ì¶”ì •í•œ í›„ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì†ì„±ì„ ê°€ì§€ê²Œ ëœë‹¤. priors_ : ê° í´ë˜ìŠ¤ $k$ì˜ ì‚¬ì „í™•ë¥  means_ : ê° í´ë˜ìŠ¤ $k$ì—ì„œ $x$ì˜ ê¸°ëŒ€ê°’ ë²¡í„° $\\mu_{k}$ì˜ ì¶”ì •ì¹˜ ë²¡í„° covariance_ : ê° í´ë˜ìŠ¤ $k$ì—ì„œ $x$ì˜ ê³µë¶„ì‚° í–‰ë ¬ $\\sum_{k}$ì˜ ì¶”ì •ì¹˜ í–‰ë ¬.(ìƒì„±ì ì¸ìˆ˜ store_covariance=Trueì¸ ê²½ìš°ì—ë§Œ ì œê³µ) 1qda.priors_ ê²°ê³¼1array([0.33333333, 0.33333333, 0.33333333]) 1qda.means_ ê²°ê³¼123array([[-8.01254084e-04, 1.19457204e-01], [ 1.16303727e+00, 1.03930605e+00], [-8.64060404e-01, 1.02295794e+00]]) 1qda.covariance_[0] ê²°ê³¼12array([[ 0.73846319, -0.01762041], [-0.01762041, 0.72961278]]) 1qda.covariance_[1] ê²°ê³¼12array([[0.66534246, 0.21132313], [0.21132313, 0.78806006]]) 1qda.covariance_[2] ê²°ê³¼12array([[0.9351386 , 0.22880955], [0.22880955, 0.79142383]]) ì´ í™•ë¥ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶„ë¥˜ë¥¼ í•œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. 1234567891011121314151617x1min, x1max = -5, 5x2min, x2max = -4, 5XX1, XX2 = np.meshgrid(np.arange(x1min, x1max, (x1max-x1min)/1000), np.arange(x2min, x2max, (x2max-x2min)/1000))YY = np.reshape(qda.predict(np.array([XX1.ravel(), XX2.ravel()]).T), XX1.shape)cmap = mpl.colors.ListedColormap(sns.color_palette([\"r\", \"g\", \"b\"]).as_hex())plt.contourf(XX1, XX2, YY, cmap=cmap, alpha=0.5)plt.scatter(X1[:, 0], X1[:, 1], alpha=0.8, s=50, marker=\"o\", color='r', label=\"í´ë˜ìŠ¤ 1\")plt.scatter(X2[:, 0], X2[:, 1], alpha=0.8, s=50, marker=\"s\", color='g', label=\"í´ë˜ìŠ¤ 2\")plt.scatter(X3[:, 0], X3[:, 1], alpha=0.8, s=50, marker=\"x\", color='b', label=\"í´ë˜ìŠ¤ 3\")plt.xlim(x1min, x1max)plt.ylim(x2min, x2max)plt.xlabel(\"$x_1$\")plt.ylabel(\"$x_2$\")plt.title(\"ì´ì°¨íŒë³„ë¶„ì„ë²• ê²°ê³¼\")plt.legend()plt.show() QDAë¥¼ ì‚¬ìš©í•˜ì—¬ iris classificationë¬¸ì œë¥¼ í’€ê³  ì„±ëŠ¥ì„ confusion_matrixì™€ classification_reportë¥¼ ì¶œë ¥í•˜ë¼. ê·¸ë¦¬ê³  ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ROC ì»¤ë¸Œë¥¼ ê·¸ë ¤ë¼.12from sklearn.datasets import load_irisfrom sklearn.metrics import confusion_matrix, classification_report ê° í´ë˜ìŠ¤ë¥¼ ë™ì¼í•˜ê²Œ(stratify) í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ì„œ trainê³¼ testë¥¼ ë‚˜ëˆŒë•Œ ì„¤ì •í•´ì£¼ì—ˆë‹¤. 1234iris = load_iris()X = iris.datay = iris.targettrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1234, stratify=y) 1234QDA = QuadraticDiscriminantAnalysis()QDA.fit(train_X, train_y)pred_QDA = QDA.predict(test_X)confusion_matrix(pred_QDA, test_y) ê²°ê³¼123array([[15, 0, 0], [ 0, 14, 0], [ 0, 1, 15]]) 1print(classification_report(pred_QDA, test_y)) ê²°ê³¼123456789 precision recall f1-score support 0 1.00 1.00 1.00 15 1 0.93 1.00 0.97 14 2 1.00 0.94 0.97 16 accuracy 0.98 45 macro avg 0.98 0.98 0.98 45weighted avg 0.98 0.98 0.98 45 1roc_auc_score(label_binarize(test_y, classes=[0,1,2]), QDA.decision_function(test_X)) 1roc_auc_score(label_binarize(test_y, classes=[0,1,2]), QDA.predict_proba(test_X)) 12345678910111213label_test_y = label_binarize(test_y, [0, 1, 2])fpr = [None] * 3tpr = [None] * 3thr = [None] * 3for i in range(3): fpr[i], tpr[i], thr[i] = roc_curve(label_test_y[:, i], QDA.predict_proba(test_X)[:, i]) plt.plot(fpr[i], tpr[i])plt.xlabel('ìœ„ì–‘ì„±ë¥ (Fall-Out)')plt.ylabel('ì¬í˜„ë¥ (Recall)')plt.show()","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"K-Nearest Neighbors(KNN)","slug":"machine_learning_08","date":"2020-04-17T09:11:42.000Z","updated":"2020-04-22T09:05:32.382Z","comments":true,"path":"2020/04/17/machine_learning_08/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/17/machine_learning_08/","excerpt":"","text":"K-Nearest Neighbors(KNN) kì˜ ê°œìˆ˜ë§Œí¼ ì£¼ë³€ì— ìˆëŠ” sampleë“¤ì˜ ì •ë³´ë¥¼ ì´ìš©í•´ì„œ ìƒˆë¡œìš´ ê´€ì¸¡ì¹˜ì˜ ì¢…ì†ë³€ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ê¸°ì¡´ì˜ íŒŒë€ìƒ‰ ë„¤ëª¨ì™€ ë¹¨ê°„ìƒ‰ ì„¸ëª¨ë¼ëŠ” 2ê°€ì§€ í´ë˜ìŠ¤ë¥¼ ê°–ëŠ” ë°ì´í„° ì§‘í•©ì´ ìˆë‹¤ê³  í•  ë•Œ, ì—¬ê¸°ì„œ ìƒˆë¡œìš´ ê´€ì¸¡ì¹˜ì¸ ë…¹ìƒ‰ì— ëŒ€í•´ ì–´ë–»ê²Œ ì˜ˆì¸¡í• ì§€ë¥¼ ìƒê°í•´ ë³´ì. k=3ì¸ ê²½ìš°(ì‹¤ì„ ): ë¹¨ê°„ìƒ‰ ì„¸ëª¨ë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. k=5ì¸ ê²½ìš°(ì ì„ ): íŒŒë€ìƒ‰ ë„¤ëª¨ë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. í—ˆë‚˜, k=5ì¸ ê²½ìš° ë¹¨ê°„ìƒ‰ ì„¸ëª¨ì™€ì˜ ê±°ë¦¬ê°€ ë” ê°€ê¹ê¸° ë•Œë¬¸ì— ê·¸ë§Œí¼ì˜ weightë¥¼ ì£¼ì–´\u001dì„œ ê°¯ìˆ˜ë¥¼ ë– ë‚˜ì„œ ë¹¨ê°„ìƒ‰ìœ¼ë¡œ ë¶„ë¥˜ë  ê°€ëŠ¥ì„±ë„ ì•Œê³ ë¦¬ì¦˜ì— ë”°ë¼ ì¡´ì¬í•œë‹¤. K-Nearest neighborhood ê·¸ë ‡ë‹¤ë©´ voting ë°©ì‹ì™¸ì—ë„ ê´€ì¸¡ì¹˜ë“¤ê°„ì˜ distanceì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ”ë° ì—¬ê¸°ì„œ ê±°ë¦¬ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì€ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì€ ë°©ë²•ë“¤ì´ ì¡´ì¬í•œë‹¤. ë¬¼ë¡ , ì´ ë°–ì—ë„ ë°ì´í„°ì˜ ê±°ë¦¬ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì€ ë” ë§ì´ ì¡´ì¬í•œë‹¤. ëŒ€í‘œì ì¸ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ì™€ ë§¨í•˜íƒ„ ê±°ë¦¬, ê·¸ë¦¬ê³  ë²”ì£¼í˜• ë³€ìˆ˜ì— ì‚¬ìš©ë˜ëŠ” Hamming distanceë¥¼ ê°„ë‹¨íˆ ë³´ì—¬ ì¤„ ê²ƒì´ë‹¤. ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ëŠ” ìš°ë¦¬ê°€ ì˜ ì•Œê³ ìˆëŠ” í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬ì— ì˜í•´ ë‘ ì§€ì  ì‚¬ì´ì˜ ìµœë‹¨ ê±°ë¦¬ë¥¼ êµ¬í•˜ëŠ” ê³µì‹ìœ¼ë¡œ ê³„ì‚°ë˜ë©°, ë§¨í•˜íƒ„ ê±°ë¦¬ëŠ” ë§¨í•˜íƒ„ ê°™ì´ ë¸”ë¡ë³„ë¡œ ë˜ì–´ìˆëŠ” ê³³ì—ì„œì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•  ê²½ìš° ì‚¬ìš©ëœë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ Hamming distanceë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹ì€ Indicatorí•¨ìˆ˜ì•ˆì˜ ì¡°ê±´ì´ ì°¸ì¸ ê²½ìš°ë§Œì„ 1ë¡œ ê°’ì„ ì‚°ì¶œí•˜ì—¬ í•©í•œ ê°’ì´ë‹¤. ì¦‰, ì‰½ê²Œ ë§í•˜ë©´ í•´ë‹¹ ì´ì§„ê°’ë“¤ì˜ ìë¦¬ì—ì„œ ë‹¤ë¥¸ ê³³ì´ ì¡´ì¬í•˜ëŠ” ê°œìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ë¨¼ì € ë…ë¦½ ë³€ìˆ˜ì™€ ìƒˆë¡œì´ ì˜ˆì¸¡í•˜ë ¤ëŠ” ê´€ì¸¡ê°’ê³¼ì˜ ê±°ë¦¬ë¥¼ ë”°ì ¸ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ë¶€í„° ìˆœì„œëŒ€ë¡œ ì •ë ¬í•´ ë†“ëŠ”ë‹¤. í†µê³„ì ìœ¼ë¡œ ìˆœì„œ í†µê³„ëŸ‰ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ê·¸ì— ë”°ë¥¸ ìˆœì„œë¡œ ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì† ë³€ìˆ˜ì˜ ìŒìœ¼ë¡œ ì •ë ¬í•œë‹¤. ì—¬ê¸°ì„œì˜ ê±°ë¦¬ëŠ” ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ ë°©ë²•ë“¤ì„ ì‚¬ìš©í•œë‹¤. ì¢…ì†ë³€ìˆ˜ê°€ ë²”ì£¼í˜•ì´ë¼ë©´ ì•„ë˜ì™€ ê°™ì´ ì†Œí”„íŠ¸ ë³´íŒ…ë°©ë²•ìœ¼ë¡œ í™•ë¥ ì„ êµ¬í•´ í™•ë¥ ê°’ì´ ê°€ì¥ í° í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì±„íƒí•œë‹¤. ë¬¼ë¡ , ë²”ì£¼í˜• ë³€ìˆ˜ë„ ì—°ì†í˜• ë³€ìˆ˜ì²˜ëŸ¼ ê±°ë¦¬ì— ì˜í•œ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì¢…ì†ë³€ìˆ˜ê°€ ì—°ì†í˜•ì¸ ê²½ìš°ëŠ” í‰ê· ì„ êµ¬í•˜ëŠ”ë°, ê±°ë¦¬ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ë¥¼ ë‘ì–´ ê°€ì¤‘í‰ê· ì„ êµ¬í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ê°€ì¤‘ì¹˜ëŠ” ê±°ë¦¬ì— ë°˜ë¹„ë¡€í•˜ê²Œ í•˜ì—¬ ê±°ë¦¬ê°€ ì§§ì„ìˆ˜ë¡ í° ê°€ì¤‘ì¹˜ë¥¼ ê°–ê²Œ í•œë‹¤. Cross validation êµì°¨ê²€ì¦(Cross validation)ì€ ê¸°ë³¸ì ìœ¼ë¡œ ê³¼ì í•©ì— ëŒ€í•œ ë°©ì§€ë¥¼ ìœ„í•´ì„œ ì‹¤í–‰í•˜ë©°, ë˜ ë‹¤ë¥¸ ì´ìœ ë¡œëŠ” sample lossì— ê´€í•œ ì¸¡ë©´ì´ ì¡´ì¬í•œë‹¤. ê³¼ì í•©(Overfitting)ë¬¸ì œëŠ” Training setì—ë§Œ ìµœì í™” ë˜ì–´ìˆì–´ ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ ë•Œ ê¸°ì¡´ì˜ Training setì— ì´ì§ˆì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ë³µì¡ë„ì™€ë„ ë°€ì ‘í•œ ì—°ê´€ì´ ìˆë‹¤. ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ë†’ì„ìˆ˜ë¡ Training setì€ ì˜ ë§ì¶”ì§€ë§Œ, Test setì— ëŒ€í•œ ì„±ëŠ¥ì€ ë‚®ì„ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ê·¸ë ‡ë‹¤ê³  ë„ˆë¬´ ê°„ë‹¨í•˜ê²Œ ë§Œë“¤ì–´ë²„ë ¤ë„ Training setê³¼ Test set ëª¨ë‘ì— ëŒ€í•´ ëŒ€í•œ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šì„ ê²ƒì´ë‹¤. Training errorëŠ” errorë¥¼ ê³¼ì†Œì¶”ì •í•˜ëŠ” ì„±í–¥ì´ ìˆë‹¤ëŠ” ë§ì€ ì•„ë˜ ê·¸ë˜í”„ë¥¼ ì‚´í´ë³´ë©´ì„œ ì„¤ëª…í•˜ê² ë‹¤. KNN ëª¨ë¸ì˜ ê²½ìš° Kê°€ ì‘ì„ìˆ˜ë¡ ëª¨ë¸ì˜ ë³µì¡ë„ëŠ” ë†’ì„ ê²ƒì´ë‹¤. ê°€ì¥ ë³µì¡í•œ k=1ì¸ ê²½ìš° Training errorëŠ” ê±°ì˜ 0ì— ìˆ˜ë ´í•œë‹¤. í—ˆë‚˜, Test errorëŠ” ìƒë‹¹íˆ ë†’ë‹¤. ì¦‰, ê³¼ì í•©ì´ ë°œìƒë˜ì—ˆë‹¤ëŠ” ì´ì•¼ê¸°ì´ë‹¤. ì—¬ê¸°ì„œ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒì€ Training errorë¡œ ëª¨ë¸ì„ ì„ íƒí•œë‹¤ë©´ ë„ˆë¬´ ë³µì¡í•œ ëª¨ë¸ì„ ì„ íƒí•˜ì—¬ ê³¼ì í•©ì„ ë°œìƒì‹œì¼œ Test errorê°€ ì»¤ì§€ê²Œ ë  ì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. ë˜í•œ, Test errorë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ì²˜ìŒì— Train set, Test setìœ¼ë¡œ ë‚˜ëˆ„ì–´ì•¼ í•˜ê¸°ì— Test setìœ¼ë¡œ ë‚˜ëˆˆë§Œí¼ì˜ ë°ì´í„° ì†Œì‹¤ë¡œ ì¸í•´ Test errorëŠ” ì¦ê°€í•˜ê²Œ ëœë‹¤. ì´ë ‡ê²Œ Errorë¥¼ ê³¼ì†Œì¶”ì •í•˜ì§€ ì•Šê¸° ìœ„í•´ì„œëŠ” êµì°¨ê²€ì¦ì„ í•´ì•¼ í•  ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³ ì CVë¥¼ ì‹¤í–‰í•˜ëŠ”ë° ê·¸ ì¤‘ K-hold Cross validationì€ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•œë‹¤. Loss functionì€ ì˜ˆë¡œ ë“¤ì–´ë†“ì€ ê²ƒì¸ë°, í•´ë‹¹ ë¬¸ì œì˜ ì„±ëŠ¥ì§€í‘œë¥¼ ì˜ë¯¸í•œë‹¤. KNNì˜ ì‹¬í™”ì  ì´í•´ ë°ì´í„°ì— ë”°ë¼ ì ì ˆí•œ Kê°€ ë‹¤ë¥¸ë° ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ ë„ˆë¬´ Kë¥¼ ì‘ê²Œ ì„¤ì •í•˜ë©´, Train setì€ êµ‰ì¥íˆ ì˜ ì„¤ëª…í•˜ì§€ë§Œ Test setì— ëŒ€í•´ì„  ì„±ëŠ¥ì´ ì•ˆì¢‹ê²Œ ë˜ëŠ” ê³¼ì í•©ì´ ë°œìƒë  ìˆ˜ ìˆë‹¤. ê·¸ ë‹¤ìŒìœ¼ë¡œëŠ” K=1ë¡œ ì„¤ì •í–ˆì„ ê²½ìš° ì´ìƒì¹˜ì™€ì˜ ê±°ë¦¬ê°€ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„°ì— ì˜í–¥ì„ ì¤„ ê²ƒì´ë‹¤. ì•„ë˜ì˜ ì˜ˆì‹œ ê·¸ë˜í”„ ì²˜ëŸ¼ ì˜ì—­ì´ ë¶ˆì—°ì†ì ì´ì–´ì„œ ì˜ì—­ì„ ë‚˜ëˆ„ëŠ” ì˜ë¯¸ê°€ ì—†ì–´ ë³´ì´ëŠ” ê²½ìš°ê°€ ë°œìƒ ë  ìˆ˜ ìˆë‹¤. ë°˜ëŒ€ë¡œ Kê°€ ë„ˆë¬´ í¬ë‹¤ë©´ ë¯¸ì„¸í•œ ê²½ê³„ì— ë¶„ë¥˜ê°€ ì•„ì‰¬ìš¸ ê²ƒì´ë‹¤. ë°ì´í„°ì— ë”°ë¼ ì ì ˆí•œ Kê°€ ë‹¤ë¥´ë¯€ë¡œ Test errorë¥¼ ì‘ê²Œí•˜ëŠ” kë¥¼ ì„ íƒí•´ì•¼ í•  ê²ƒì´ë‹¤. Cross-validationì„ ì´ìš©í•˜ì—¬ ë³´í†µ ëª¨í˜•ì˜ ëª¨ìˆ˜ë“¤ì„ ì¡°ì ˆí•˜ê²Œ ëœë‹¤. ê·¸ ì¤‘ ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì„±ëŠ¥ì´ ì¢‹ì€ ëª¨ìˆ˜ë¥¼ ì±„íƒí•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ test ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ì§€ë‚œ ë²ˆì— ì–¸ê¸‰í–ˆë˜ ì°¨ì›ì˜ ì €ì£¼ì™€ KNN ì•Œê³ ë¦¬ì¦˜ë„ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆë‹¤. ì°¨ì›ì˜ ì €ì£¼ëŠ” ì°¨ì›ì´ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ ìš°ë¦¬ê°€ ì„¤ëª…í•˜ê³  ì‹¶ì€ ê³µê°„ ëŒ€ë¹„ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ê³µê°„ì´ ì¤„ì–´ë“œëŠ” ë¬¸ì œì¸ë°, ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ ê°€ë¡œì¶• ë³€ìˆ˜ë§Œì„ ì‚¬ìš©í•œë‹¤ë©´ ì¶©ë¶„íˆ ë‘ í´ë˜ìŠ¤ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆëŠ” ê¸°ì¤€ì„ ì„¤ì •í•  ìˆ˜ ìˆì§€ë§Œ, ì˜¤íˆë ¤ ì„¸ë¡œì¶•ì— ì˜í•´ì„œëŠ” í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„í•˜ëŠ”ë° ì•„ë¬´ëŸ° ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë° ì„¸ë¡œì¶• ë³€ìˆ˜ë„ ê³ ë ¤í•˜ê²Œ ë˜ë©´ì„œ ë‹¤ë¥¸ ì˜ˆì¸¡ì„ í•˜ê²Œ ëœë‹¤. k-Nearest Neighborhood Algorithm ì‹¤ìŠµ1. ë°ì´í„°, ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° ë° kNN í”¼íŒ… ë°©ë²•123456import modin.pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapfrom sklearn import neighbors, datasetsfrom sklearn.metrics import confusion_matrix iris ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. 1234iris = datasets.load_iris()X = iris.data[:, :2]y = iris.target ëª¨ë¸ êµ¬ì¶• neighborsë¥¼ 5ë¡œ ì„¤ì • 123clf = neighbors.KNeighborsClassifier(n_neighbors=5)clf.fit(X,y)y_pred=clf.predict(X) 1confusion_matrix(y,y_pred) ê²°ê³¼123array([[49, 1, 0], [ 0, 38, 12], [ 0, 12, 38]]) 2.Cross-validationì„ í™œìš©í•œ ìµœì ì˜ kì°¾ê¸°1234567891011121314from sklearn.model_selection import cross_val_scorek_range = np.arange(1,100)k_scores = []for k in k_range: knn=neighbors.KNeighborsClassifier(k) scores= cross_val_score(knn, X, y, cv=10, scoring=\"accuracy\") k_scores.append(scores.mean())plt.plot(k_range, k_scores)plt.xlabel('Value of K for KNN')plt.ylabel('Cross-validated accuracy')plt.show() ì´ë ‡ê²Œ ì ì ˆí•œ ëª¨ìˆ˜ë¥¼ ì°¾ëŠ” ë°©ë²•ì€ ìœ„ì—ì„œì™€ ê°™ì´ ì§ì ‘ grid searchë¥¼ í•˜ëŠ” ë°©ë²•ì„ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤. 2.Weightë¥¼ ì¤€ kNN ê±°ë¦¬ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•œ ê²½ìš°ê°€ ì¢€ ë” decision boundaryê°€ ë§¤ë„ëŸ½ê²Œ ì—°ê²°ë˜ì–´ ìˆë‹¤. ì¢€ ë” ë³´í¸ì ì¸ ëª¨ë¸ì¸ ê²ƒì´ë‹¤. 123456789101112131415161718192021222324252627282930313233n_neighbors = 40h = .02 # step size in the meshcmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])for weights in ['uniform', 'distance']: clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights) clf.fit(X, y) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, x_max]x[y_min, y_max]. x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light) # Plot also the training points plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.title(\"3-Class classification (k = %i, weights = '%s')\" % (n_neighbors, weights))plt.show() 12345678np.random.seed(0)X = np.sort(5 * np.random.rand(40, 1), axis=0)T = np.linspace(0, 5, 500)[:, np.newaxis]y = np.sin(X).ravel()y[::5] += 1 * (0.5 - np.random.rand(8))knn = neighbors.KNeighborsRegressor(n_neighbors)y_ = knn.fit(X, y).predict(T) ì´ì›ƒì„ í•˜ë‚˜ë§Œ ì‚¬ìš©í•  ë•ŒëŠ” í›ˆë ¨ ì„¸íŠ¸ì˜ ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ ì˜ˆì¸¡ì— ì£¼ëŠ” ì˜í–¥ì´ ì»¤ì„œ ì˜ˆì¸¡ê°’ì´ í›ˆë ¨ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ì§€ë‚˜ê°„ë‹¤. ì´ëŠ” ë§¤ìš° ë¶ˆì•ˆì •í•œ ì˜ˆì¸¡ì„ ë§Œë“¤ì–´ ë‚¸ë‹¤. ì´ì›ƒì„ ë§ì´ ì‚¬ìš©í•˜ë©´ í›ˆë ¨ ë°ì´í„°ì—ëŠ” ì˜ ì•ˆ ë§ì„ ìˆ˜ ìˆì§€ë§Œ ë” ì•ˆì •ëœ ì˜ˆì¸¡ì„ ì–»ê²Œ ëœë‹¤. ë˜í•œ Uniformí•˜ê²Œ ëª¨ë‘ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•´ì„œ ê³„ì‚°í•˜ëŠ” ê²ƒë³´ë‹¤ ë•Œë¡  ê±°ë¦¬ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” í¸ì´ ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ ì¢€ ë” ì˜ˆì¸¡í•˜ëŠ” ë° íš¨ê³¼ê°€ ìˆì„ìˆ˜ë„ ìˆë‹¤. 12345678910111213141516n_neighbors = 5for i, weights in enumerate(['uniform', 'distance']): knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights) y_ = knn.fit(X, y).predict(T) plt.subplot(2, 1, i + 1) plt.scatter(X, y, c='k', label='data') plt.plot(T, y_, c='g', label='prediction') plt.axis('tight') plt.legend() plt.title(\"KNeighborsRegressor (k = %i, weights = '%s')\" % (n_neighbors, weights))plt.tight_layout()plt.show() ì¥ë‹¨ì ê³¼ ë§¤ê°œë³€ìˆ˜ ì¼ë°˜ì ìœ¼ë¡œ KNeighbors ë¶„ë¥˜ê¸°ì— ì¤‘ìš”í•œ ë§¤ê°œë³€ìˆ˜ëŠ” ë‘ ê°€ì§€ì´ë‹¤. ë°ì´í„° í¬ì¸íŠ¸ ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì¬ëŠ” ë°©ë²•ê³¼ ì´ì›ƒì˜ ìˆ˜ì´ë‹¤. ì‹¤ì œë¡œ ì´ì›ƒì˜ ìˆ˜ëŠ” 3ê°œë‚˜ 5ê°œ ì •ë„ë¡œ ì ì„ ë•Œ ì˜ ì‘ë™í•˜ì§€ë§Œ, ì´ ë§¤ê°œë³€ìˆ˜ëŠ” ì˜ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤. k-NNì˜ ì¥ì ì€ ì´í•´í•˜ê¸° ë§¤ìš° ì‰¬ìš´ ëª¨ë¸ì´ë¼ëŠ” ì ì´ë‹¤. ê·¸ë¦¬ê³  ë§ì´ ì¡°ì •í•˜ì§€ ì•Šì•„ë„ ìì£¼ ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•œë‹¤. ë” ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•´ë³´ê¸° ì „ì— ì‹œë„í•´ë³¼ ìˆ˜ ìˆëŠ” ì¢‹ì€ ì‹œì‘ì ì´ë‹¤. ë³´í†µ ìµœê·¼ì ‘ ì´ì›ƒ ëª¨ë¸ì€ ë§¤ìš° ë¹ ë¥´ê²Œ ë§Œë“¤ ìˆ˜ ìˆì§€ë§Œ, í›ˆë ¨ ì„¸íŠ¸ê°€ ë§¤ìš° í¬ë©´ (íŠ¹ì„±ì˜ ìˆ˜ë‚˜ ìƒ˜í”Œì˜ ìˆ˜ê°€ í´ ê²½ìš°) ì˜ˆì¸¡ì´ ëŠë ¤ì§„ë‹¤. k-NN ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•  ë• ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì´ ì¤‘ìš”í•˜ë‹¤. ê·¸ë¦¬ê³  (ìˆ˜ë°± ê°œ ì´ìƒì˜) ë§ì€ íŠ¹ì„±ì„ ê°€ì§„ ë°ì´í„°ì…‹ì—ëŠ” ì˜ ë™ì‘í•˜ì§€ ì•Šìœ¼ë©°, íŠ¹ì„± ê°’ ëŒ€ë¶€ë¶„ì´ 0ì¸ (ì¦‰ í¬ì†Œí•œ) ë°ì´í„°ì…‹ê³¼ëŠ” íŠ¹íˆ ì˜ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤. k-ìµœê·¼ì ‘ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ì´ ì´í•´í•˜ê¸´ ì‰½ì§€ë§Œ, ì˜ˆì¸¡ì´ ëŠë¦¬ê³  ë§ì€ íŠ¹ì„±ì„ ì²˜ë¦¬í•˜ëŠ” ëŠ¥ë ¥ì´ ë¶€ì¡±í•´ í˜„ì—…ì—ì„œëŠ” ì˜ ì“°ì§€ ì•ŠëŠ”ë‹¤.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜ëª¨í˜•","slug":"machine_learning_07","date":"2020-04-14T05:59:44.000Z","updated":"2020-04-17T12:47:37.106Z","comments":true,"path":"2020/04/14/machine_learning_07/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/","excerpt":"","text":"ë¶„ë¥˜ëª¨í˜• í˜„ì‹¤ì ì¸ ë¬¸ì œë¡œ ë°”ê¾¸ì–´ ë§í•˜ë©´ ì–´ë–¤ í‘œë³¸ì— ëŒ€í•œ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê·¸ í‘œë³¸ì´ ì–´ë–¤ ì¹´í…Œê³ ë¦¬ í˜¹ì€ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë‚´ëŠ” ë¬¸ì œì´ê¸°ë„ í•˜ë‹¤. í”íˆë“¤ ë§ì´ ì‚¬ìš©í•˜ëŠ” ëª¨í˜•ë“¤ì˜ ë¶„ë¥˜ëª¨í˜• ì¢…ë¥˜ë¥¼ ì•„ë˜ì˜ í‘œë¡œ ê¸°ì¬í•´ ë†“ì•˜ë‹¤. ì´ì „ì— ê¸°ì¬í–ˆë˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¶„ì„ë„ ì‹¤ì§ˆì ìœ¼ë¡  ë¶„ë¥˜ë¬¸ì œì— ë§ì´ ì‚¬ìš©ëœë‹¤. ëª¨í˜• ë°©ë²•ë¡  ë‚˜ì´ë¸Œ ë² ì´ì§€ì•ˆ í™•ë¥ ì  ìƒì„±ëª¨í˜• LDA/QDA í™•ë¥ ì  ìƒì„±ëª¨í˜• ë¡œì§€ìŠ¤í‹±íšŒê·€ í™•ë¥ ì  íŒë³„ëª¨í˜• ì˜ì‚¬ê²°ì •ë‚˜ë¬´ í™•ë¥ ì  íŒë³„ëª¨í˜• í¼ì…‰íŠ¸ë¡  íŒë³„í•¨ìˆ˜ ëª¨í˜• ì„œí¬íŠ¸ë²¡í„°ë¨¸ì‹  íŒë³„í•¨ìˆ˜ ëª¨í˜• ì¸ê³µì‹ ê²½ë§ íŒë³„í•¨ìˆ˜ ëª¨í˜• ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ë¶„ë¥˜ ëª¨í˜• ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™\bì€ ì§ˆë¬¸ì— ëŒ€í•œ í™•ë¥ ì„ ê³„ì‚°í•˜ë ¤ë©´, ì•„ë˜ ê²°í•©í™•ë¥ ì„ ê³„ì‚°í•´ì•¼ í•  ê²ƒì´ë‹¤. ë…ë¦½ë³€ìˆ˜ê°€ 2ê°œ ì¦‰, Featureì˜ ì°¨ì›ì´ 2ê°œì¸ ì§€ê¸ˆì€ í¬ê²Œ ì–´ë µì§„ ì•Šê² ì§€ë§Œ, í˜„ì‹¤ì—ì„œëŠ” Featureì˜ ê°œìˆ˜ê°€ í›¨ì”¬ ë” ë§ì„ ê²ƒì´ë‹¤. ì°¨ì›ì´ ì»¤ì§„ë‹¤ë©´ ì´ì—ë”°ë¼ í•´ë‹¹ ì¡°ê±´ë¶€ í•¨ìˆ˜ì˜ ê°€ëŠ¥ë„í•¨ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ”ë° ì–´ë ¤ì›€ì„ ê²ªì„ ê²ƒì´ë‹¤. \\begin{align} P( X = weather state, Humidity | Y = playing Tennis) \\\\ = \\frac{P( X_{1} = weather state, Humidity \\cap Y = playing Tennis)}{P(Y = playing Tennis)} \\end{align} ìœ„ì˜ ìˆ˜ì‹ì—ì„œ ë¶„ìë¶€ë¶„ì˜ í™•ë¥ ì¸ ê²°í•©í™•ë¥ (Joint probability)ì„ êµ¬í•˜ëŠ” ê²ƒì€ ì‰¬ìš´ ë¬¸ì œê°€ ì•„ë‹ˆë‹¤. ê²°í•©ë˜ì–´ìˆëŠ” ìƒí™©ì„ ë‚˜ëˆ„ì–´ì„œ í™•ë¥ ì„ ê³„ì‚°í•  ìˆœ ì—†ì„ê¹Œë¼ëŠ” ê°€ì •ì—ì„œ ë‚˜ì´ë¸Œ ë² ì´ì§€ì•ˆ ëª¨í˜•ì€ ì¶œë°œí•œë‹¤. ê·¸ë ‡ë‹¤ë©´, ì–´ë–»ê²Œ ë‚˜ëˆŒìˆ˜ ìˆì„ê¹Œ? í†µê³„ë¥¼ ì¡°ê¸ˆì´ë¼ë„ ì•„ì‹œëŠ” ë¶„ë“¤ì€ ë…ë¦½ì´ë¼ëŠ” ê°œë…ì„ ì•Œê³ ê³„ì‹¤ ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ A: ì½”ë¡œë‚˜ê°€ ì§‘ë‹¨ ë°œë³‘í•œë‹¤, B: 2ì£¼ê°„ ì¬íƒê·¼ë¬´ë¥¼ ì‹œí–‰í•œë‹¤. ì´ ë‘ê°€ì§€ ì‚¬ê±´ì€ ì„œë¡œ ë°œí˜„ë  í™•ë¥ ì— ì˜í–¥ì„ ë¯¸ì¹˜ê¸° ë•Œë¬¸ì— ë‘ ì‚¬ê±´ì€ ì¢…ì†(dependent) ê´€ê³„ë¼ê³  í•  ìˆ˜ ìˆë‹¤. í—ˆë‚˜, A: ë¹„ê°€ì˜¨ë‹¤, B: ëŒ€ì¶œì‹¬ì‚¬ì—ì„œ ìŠ¹ì¸ì„ ë°›ëŠ”ë‹¤ ë¼ëŠ” ë‘ ì‚¬ê±´ì€ ì„œë¡œ ë‘˜ ì¤‘ ì–´ë–¤ ì‚¬ê±´ì—ë„ ë°œí˜„ë  í™•ë¥ ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤. ì´ëŸ¬í•œ, ê²½ìš°ì— ìš°ë¦¬ëŠ” ë‘ ì‚¬ê±´ì´ í™•ë¥ ì ìœ¼ë¡œ ë…ë¦½ì´ë¼ê³  ì •ì˜í•œë‹¤. ë‘ ì‚¬ê±´ A,Bê°€ ë…ë¦½ì¼ í•„ìš”ì¶©ë¶„ì¡°ê±´ì€ ì•„ë˜ì™€ ê°™ë‹¤. P(A \\cap B) = P(A)P(B) ì´ë ‡ê²Œ ë‘ ì‚¬ê±´ì´ ë…ë¦½ì´ë¼ë©´ ì¢€ ë” ì¶”ì •í•˜ê¸° ì‰¬ìš¸ ê²ƒì´ë‹¤. í—ˆë‚˜, ìœ„ì˜ ìƒí™©ì—ì„  ì¡°ê±´ë¶€ì´ê¸° ë•Œë¬¸ì— ì¡°ê±´ë¶€ ë…ë¦½ì˜ ê°€ì •ì´ ë§ì„ ê²ƒì´ë‹¤. ì¡°ê±´ë¶€ ë…ë¦½(conditional independence)ì€ ì¼ë°˜ì ì¸ ë…ë¦½ê³¼ ë‹¬ë¦¬ ì¡°ê±´ì´ ë˜ëŠ” ë³„ê°œì˜ í™•ë¥ ë³€ìˆ˜ Cê°€ ì¡´ì¬í•´ì•¼í•œë‹¤. ì•„ë˜ ìˆ˜ì‹ì´ ì„±ë¦½ë˜ë©´ ì¡°ê±´ë¶€ ë…ë¦½ì´ë¼ê³  í•œë‹¤. A \\text{â««} B \\;\\vert\\; C \\Longrightarrow P(A, B | C) = P(A|C)P(B|C) ì¡°ê±´ë¶€ë…ë¦½ê³¼ ë¹„êµí•˜ì—¬ ì¼ë°˜ì ì¸ ë…ë¦½ì€ ë¬´ì¡°ê±´ë¶€ë…ë¦½ì´ë¼ê³  í•œë‹¤. ë¬´ì¡°ê±´ë¶€ ë…ë¦½ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œê¸°í•˜ê¸°ë„ í•œë‹¤. A \\text{â««} B \\;\\vert\\; \\emptyset A, Bê°€ Cì— ëŒ€í•´ ì¡°ê±´ë¶€ ë…ë¦½ì´ë©´ ë‹¤ìŒë„ ë§Œì¡±í•œë‹¤. P(A \\;\\vert\\; B, C) = P(A \\vert C)P(B \\;\\vert\\; A, C) = P(B \\vert C)\\begin{align} P(A|B,C) &= \\frac{P(A,B,C)}{P(B,C)} \\\\ &= \\frac{P(A,B|C)P(C)}{P(B|C)P(C)} \\\\ &= \\frac{P(A|C)P(B|C)}{P(B|C)} \\\\ &= P(A|C) \\end{align} ì£¼ì˜í•  ì ì€ ì¡°ê±´ë¶€ ë…ë¦½ê³¼ ë¬´ì¡°ê±´ë¶€ ë…ë¦½ì€ ê´€ê³„ê°€ ì—†ë‹¤ëŠ” ì ì´ë‹¤. ì¦‰, ë‘ í™•ë¥ ë³€ìˆ˜ê°€ ë…ë¦½ì´ë¼ê³  í•­ìƒ ì¡°ê±´ë¶€ ë…ë¦½ì´ ë˜ëŠ” ê²ƒë„ ì•„ë‹ˆê³  ì¡°ê±´ë¶€ ë…ë¦½ì´ë¼ê³  ê¼­ ë…ë¦½ì´ ë˜ëŠ” ê²ƒë„ ì•„ë‹ˆë‹¤. P(A,B) = P(A)P(B) \\;\\; \\nRightarrow \\;\\; P(A,B|C) = P(A|C)P(B|C)P(A,B|C) = P(A|C)P(B|C) \\;\\; \\nRightarrow \\;\\; P(A,B) = P(A)P(B)ë² ì´ì¦ˆ ì •ë¦¬(Bayesâ€™ rule) ë² ì´ì¦ˆ ì •ë¦¬ëŠ” ì‚¬ê±´ Bê°€ ë°œìƒí•¨ìœ¼ë¡œì¨ ì‚¬ê±´ Aì˜ í™•ë¥ ì´ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ë¥¼ í‘œí˜„í•œ ì •ë¦¬ì´ë‹¤. ë”°ë¼ì„œ ë² ì´ì¦ˆ ì •ë¦¬ëŠ” ìƒˆë¡œìš´ ì •ë³´ê°€ ê¸°ì¡´ì˜ ì¶”ë¡ ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. P(A|B) = \\frac{P(B|A) P(A)}{P(B)} $P(A|B)$ : ì‚¬í›„í™•ë¥ (Posterior). ì‚¬ê±´ Bê°€ ë°œìƒí•œ í›„ ê°±ì‹ ëœ ì‚¬ê±´ Aì˜ í™•ë¥  $P(A)$ : ì‚¬ì „í™•ë¥ (Prior). ì‚¬ê±´ Bê°€ ë°œìƒí•˜ê¸° ì „ì— ê°€ì§€ê³  ìˆë˜ ì‚¬ê±´ Aì˜ í™•ë¥  $P(B|A)$ : ê°€ëŠ¥ë„(likelihood). ì‚¬ê±´ Aê°€ ë°œìƒí•œ ê²½ìš° ì‚¬ê±´ Bì˜ í™•ë¥  $P(B)$ : ì •ê·œí™” ìƒìˆ˜(normalizing constant) ë˜ëŠ” ì¦ê±°(evidence). í™•ë¥ ì˜ í¬ê¸° ì¡°ì •ì—­í• ì„ í•¨. ì´ë ‡ê²Œ ë…ë¦½ë³€ìˆ˜(Feature)ë“¤ê°„ì— ì„œë¡œ ì¡°ê±´ë¶€ë…ë¦½ì„ì„ ê°€ì •í•˜ì—¬ ì¡°ê±´ì„ ë‚˜ì´ë¸Œí•˜ê²Œ í•˜ì˜€ê³ , ë² ì´ì¦ˆì •ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ MLEë¥¼ í†µí•´ ê°€ì¥ í° í™•ë¥ ê°’ì„ ê°–ëŠ” ëª¨ìˆ˜ë¥¼ ì¶”ì •í•´ë‚´ëŠ” ëª¨í˜•ì´ ë‚˜ì´ë¸Œ ë² ì´ì§€ì•ˆ ëª¨í˜•ì´ë‹¤. \\begin{align} P(y = k \\mid x) &= \\dfrac{ P(x_1, \\ldots, x_D \\mid y = k) P(y = k) }{P(x)} \\\\ &= \\dfrac{ \\left( \\prod_{d=1}^D P(x_{d} \\mid y = k) \\right) P(y = k) }{P(x)} \\end{align} ì œì¼ ì²˜ìŒ ì˜ˆì‹œë¡œ ë“¤ì—ˆë˜ ë‚ ì”¨ì˜ ìƒíƒœì™€ ìŠµë„ë¥¼ ë…ë¦½ë³€ìˆ˜ë¡œ í•˜ê³ , ì¢…ì†ë³€ìˆ˜ë¥¼ í…Œë‹ˆìŠ¤ë¥¼ ì¹˜ëŠ”ì§€ì— ëŒ€í•œ ì—¬ë¶€ì— ëŒ€í•œ ë¬¸ì œë¥¼ í‘¼ë‹¤ê³  ê°€ì •í•´ë³´ì. ë¨¼ì € í…Œë‹ˆìŠ¤ë¥¼ ì¹˜ëŠ” ê²½ìš°ì— ëŒ€í•œ í™•ë¥ ì„ ê³„ì‚°í•  ê²½ìš° í…Œë‹ˆìŠ¤ë¥¼ ì¹˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ì„ ë†’ê²Œ í•´ì£¼ëŠ” ë‘ê°€ì§€ ê²½ìš°ì˜ ìˆ˜ë¥¼ ì•„ë˜ì™€ ê°™ì´ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤. ì•„ë˜ ë‘ ê·¸ë¦¼ì„ ìˆ˜ì‹ìœ¼ë¡œ ì •ë¦¬í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì‰½ê²Œ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤. ìˆ˜ì‹ì˜ ë§ˆì§€ë§‰ì—ì„œ ë¶„ìë¶€ë¶„ì—ì„œ likelihoodì™€ Prior í™•ë¥ ê°’ì„ ì‚´í´ë³´ë©´ ê·¸ë¦¼ì—ì„œ ì–¸ê¸‰í•œ ë‘ ê°€ì§€ ê²½ìš°ì˜ ì‚¬ê±´ë“¤ì˜ í™•ë¥  ê°’ì˜ ê³±ìœ¼ë¡œ ë˜ì–´ìˆìŒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰, ì‚¬ì „í™•ë¥ ì— ìƒˆë¡­ê²Œ ì¶”ê°€ë˜ëŠ” likelihoodê°€ ì¶”ë¡ ì˜ ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë² ì´ì¦ˆ ì •ë¦¬ì˜ ì˜ë¯¸ë¥¼ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤. \\begin{align} P(weather state, Humidity | playing Tennis) \\\\ &= \\frac{P(weather state)}{P(playing Tennis)} \\frac{P(Humidity)}{P(playing Tennis)} \\\\ &= \\frac{P(playing Tennis|weather state)P(weather state)}{P(playing Tennis)} \\frac{P(playing Tennis|Humidity)P(Humidity)}{P(playing Tennis)} \\end{align} ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ ë•Œì˜ ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ì´ìš©í•´ ê³„ì‚°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ì˜ˆì‹œë¡œ ê³„ì‚°í•´ ë³¸ ê²ƒê³¼ ê°™ì´ ì•„ë˜ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ë…ë¦½ë³€ìˆ˜ë“¤ì´ ì¡°ê±´ë¶€ ë…ë¦½ì´ë¼ëŠ” ì¡°ê±´ì„ í†µí•´ ê³±ìœ¼ë¡œ í‘œí˜„ ë  ìˆ˜ ìˆë‹¤. ì˜ˆì¸¡ê°’ì€ ë§ˆì§€ë§‰ ìˆ˜ì‹ì—ì„œ ë³´ëŠ” ê²ƒê³¼ ê°™ì´ í•´ë‹¹ ê°€ëŠ¥ë„í•¨ìˆ˜ì˜ í™•ë¥ ê°’ì„ ìµœëŒ€ë¡œ í•´ì£¼ëŠ” classë¥¼ ì„ íƒí•˜ì—¬ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì¶œë ¥í•´ì¤€ë‹¤. ì°¸ì¡° : MLE ê°œë… ì´ì œ í•˜ë‚˜ì”© ì•ì—ì„œ ì–¸ê¸‰í–ˆë˜ ìˆœì„œëŒ€ë¡œ ê³„ì‚°í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ì¶œë ¥ì„ ë‚´ëŠ” ë‹¨ê³„ê¹Œì§€ ê·¸ë¦¼ì„ í†µí•´ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ë‚˜ì´ë¸Œ ë² ì´ì§€ì•ˆ ëª¨í˜•ì˜ ì¢…ë¥˜ ë‚˜ì´ë¸Œ ê°€ì • ë…ë¦½ë³€ìˆ˜ $x$ê°€ $D$ì°¨ì›ì´ë¼ê³  ê°€ì •í•˜ì. x = (x_1, \\ldots, x_D) ê°€ëŠ¥ë„ í•¨ìˆ˜ëŠ” $x_{1}, \\ldots, x_{D}$ì˜ ê²°í•©í™•ë¥ ì´ ëœë‹¤. P(x \\mid y = k) = P(x_1, \\ldots, x_D \\mid y = k) ì›ë¦¬ìƒìœ¼ë¡œëŠ” $y=k$ì¸ ë°ì´í„°ë§Œ ëª¨ì•„ì„œ ì´ ê°€ëŠ¥ë„í•¨ìˆ˜ì˜ ëª¨ì–‘ì„ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì°¨ì› $D$ê°€ ì»¤ì§€ë©´ ê°€ëŠ¥ë„í•¨ìˆ˜ì˜ ì¶”ì •ì´ í˜„ì‹¤ì ìœ¼ë¡œ ì–´ë ¤ì›Œì§„ë‹¤. ë”°ë¼ì„œ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ë¶„ë¥˜ëª¨í˜•(Naive Bayes classification model)ì—ì„œëŠ” ëª¨ë“  ì°¨ì›ì˜ ê°œë³„ ë…ë¦½ë³€ìˆ˜ê°€ ì„œë¡œ ì¡°ê±´ë¶€ ë…ë¦½(conditional independent)ì´ë¼ëŠ” ê°€ì •ì„ ì‚¬ìš©í•œë‹¤. ì´ëŸ¬í•œ ê°€ì •ì„ ë‚˜ì´ë¸Œ ê°€ì •(naive assumption)ì´ë¼ê³  í•œë‹¤. ë‚˜ì´ë¸Œ ê°€ì •ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ ë²¡í„° $x$ì˜ ê²°í•©í™•ë¥ ë¶„í¬í•¨ìˆ˜ëŠ” ê°œë³„ ìŠ¤ì¹¼ë¼ ì›ì†Œ $x_{d}$ì˜ í™•ë¥ ë¶„í¬í•¨ìˆ˜ì˜ ê³±ì´ ëœë‹¤. P(x_1, \\ldots, x_D \\mid y = k) = \\prod_{d=1}^D P(x_d \\mid y = k) ìŠ¤ì¹¼ë¼ ì›ì†Œ $x_{d}$ì˜ í™•ë¥ ë¶„í¬í•¨ìˆ˜ëŠ” ê²°í•©í™•ë¥ ë¶„í¬í•¨ìˆ˜ë³´ë‹¤ ì¶”ì •í•˜ê¸° í›¨ì”¬ ì‰½ë‹¤. ê°€ëŠ¥ë„í•¨ìˆ˜ë¥¼ ì¶”ì •í•œ í›„ì—ëŠ” ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©° ì¡°ê±´ë¶€í™•ë¥ ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. \\begin{align} P(y = k \\mid x) &= \\dfrac{ P(x_1, \\ldots, x_D \\mid y = k) P(y = k) }{P(x)} \\\\ &= \\dfrac{ \\left(\\prod_{d=1}^D P(x_{d} \\mid y = k) \\right) P(y = k) }{P(x)} \\end{align}ê°€ìš°ì‹œì•ˆ ë¶„í¬(ì •ê·œë¶„í¬) ê°€ëŠ¥ë„ ëª¨í˜• $x$ë²¡í„°ì˜ ì›ì†Œê°€ ëª¨ë‘ ì‹¤ìˆ˜ì´ê³  í´ë˜ìŠ¤ë§ˆë‹¤ íŠ¹ì •í•œ ê°’ ì£¼ë³€ì—ì„œ ë°œìƒí•œë‹¤ê³  í•˜ë©´ ê°€ëŠ¥ë„ ë¶„í¬ë¡œ ì •ê·œë¶„í¬ë¥¼ ì‚¬ìš©í•œë‹¤. ê° ë…ë¦½ë³€ìˆ˜ $x_{d}$ë§ˆë‹¤, ê·¸ë¦¬ê³  í´ë˜ìŠ¤ $k$ë§ˆë‹¤ ì •ê·œ ë¶„í¬ì˜ ê¸°ëŒ€ê°’ $\\mu_{d,k}$, í‘œì¤€í¸ì°¨ $\\sigma^{2}_{d,k}$ê°€ ë‹¬ë¼ì§„ë‹¤. QDAëª¨í˜•ê³¼ëŠ” ë‹¬ë¦¬ ëª¨ë“  ë…ë¦½ë³€ìˆ˜ë“¤ì´ ì„œë¡œ ì¡°ê±´ë¶€ë…ë¦½ì´ë¼ê³  ê°€ì •í•œë‹¤. P(x_d \\mid y = k) = \\dfrac{1}{\\sqrt{2\\pi\\sigma_{d,k}^2}} \\exp \\left(-\\dfrac{(x_d-\\mu_{d,k})^2}{2\\sigma_{d,k}^2}\\right) ë² ë¥´ëˆ„ì´ ë¶„í¬ ê°€ëŠ¥ë„ ëª¨í˜• ë² ì´ëˆ„ì´ë¶„í¬ ê°€ëŠ¥ë„ ëª¨í˜•ì—ì„œëŠ” ê°ê°ì˜ $x = (x_1,\\ldots, x_D)$ì˜ ê° ì›ì†Œ $x_{d}$ê°€ 0 ë˜ëŠ” 1ì´ë¼ëŠ” ê°’ë§Œì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ë…ë¦½ë³€ìˆ˜ëŠ” $D$ê°œì˜ ë…ë¦½ì ì¸ ë² ë¥´ëˆ„ì´ í™•ë¥ ë³€ìˆ˜, ë™ì „ìœ¼ë¡œ êµ¬ì„±ëœ ë™ì „ ì„¸íŠ¸ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ì´ ë™ì „ë“¤ì˜ ëª¨ìˆ˜ $\\mu_{d}$ëŠ” ë™ì „ $d$ë§ˆë‹¤ ë‹¤ë¥´ë‹¤. ê·¸ëŸ°ë° class $y=k (k = 1,\\ldots, K)$ë§ˆë‹¤ë„ $x_{d}$ê°€ 1ì´ ë  í™•ë¥ ì´ ë‹¤ë¥´ë‹¤. ì¦‰, ë™ì „ì˜ ëª¨ìˆ˜ $\\mu_{d,k}$ëŠ” ë™ì „ $d$ë§ˆë‹¤ ë‹¤ë¥´ê³  class $k$ë§ˆë‹¤ë„ ë‹¤ë¥´ë‹¤. ì¦‰, ì „ì²´ $ D \\times K $ì˜ ì¡°í•©ì˜ ë™ì „ì´ ì¡´ì¬í•˜ë©° ê°™ì€ classì— ì†í•˜ëŠ” Dê°œì˜ ë™ì „ì´ í•˜ë‚˜ì˜ ë™ì „ ì„¸íŠ¸ë¥¼ êµ¬ì„±í•˜ê³  ì´ëŸ¬í•œ ë™ì „ ì„¸íŠ¸ê°€ $K$ê°œ ìˆë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. P(x_d \\mid y = k) = \\mu_{d,k}^{x_d} (1-\\mu_{d,k})^{(1-x_d)}P(x_1, \\ldots, x_D \\mid y = k) = \\prod_{d=1}^D \\mu_{d,k}^{x_d} (1-\\mu_{d,k})^{(1-x_d)} ì´ëŸ¬í•œ ë™ì „ ì„¸íŠ¸ë§ˆë‹¤ í™•ë¥  íŠ¹ì„±ì´ ë‹¤ë¥´ë¯€ë¡œ ë² ë¥´ëˆ„ì´ë¶„í¬ ê°€ëŠ¥ë„ ëª¨í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì€ ë™ì „ ì„¸íŠ¸ë¥¼ $N$ë²ˆ ë˜ì§„ ê²°ê³¼ë¡œë¶€í„° $1, \\ldots, K$ ì¤‘ ì–´ëŠ ë™ì „ ì„¸íŠ¸ë¥¼ ë˜ì¡ŒëŠ”ì§€ë¥¼ ì°¾ì•„ë‚´ëŠ” ëª¨í˜•ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ë‹¤í•­ë¶„í¬(Multinomial Distribution) ê°€ëŠ¥ë„ ëª¨í˜• ë‹¤í•­ë¶„í¬ ëª¨í˜•ì—ì„œëŠ” $x$ ë²¡í„°ê°€ ë‹¤í•­ë¶„í¬ì˜ í‘œë³¸ì´ë¼ê³  ê°€ì •í•œë‹¤. ì¦‰, $D$ê°œì˜ ë©´ì„ ê°€ì§€ëŠ” ì£¼ì‚¬ìœ„ë¥¼ $\\sum_{d=1}^D x_d$ ë²ˆ ë˜ì ¸ì„œ ë‚˜ì˜¨ ê²°ê³¼ë¡œ ë³¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ $x$ê°€ ë‹¤ìŒê³¼ ê°™ë‹¤ë©´, $x=(1, 4, 0, 5)$ 4ë©´ì²´ ì£¼ì‚¬ìœ„ë¥¼ 10ë²ˆ ë˜ì ¸ì„œ 1ì¸ ë©´ì´ 1ë²ˆ, 2ì¸ ë©´ì´ 4ë²ˆ, 4ì¸ ë©´ì´ 5ë²ˆ ë‚˜ì˜¨ ê²°ê³¼ë¡œ í•´ì„í•œë‹¤. ê° class ë§ˆë‹¤ ì£¼ì‚¬ìœ„ê°€ ë‹¤ë¥´ë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ $K$ê°œì˜ classë¥¼ êµ¬ë¶„í•˜ëŠ” ë¬¸ì œì—ì„œëŠ” $D$ê°œì˜ ë©´ì„ ê°€ì§„ ì£¼ì‚¬ìœ„ê°€ $K$ê°œ ìˆë‹¤ê³  ë³¸ë‹¤. P(x_1, \\ldots, x_D \\mid y = k) \\;\\; \\propto \\;\\; \\prod_{d=1}^D \\mu_{d,k}^{x_{d,k}}\\sum_{d=1}^{D} \\mu_{d,k} = 1 ë”°ë¼ì„œ ë‹¤í•­ë¶„í¬ ê°€ëŠ¥ë„ ëª¨í˜•ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ì€ ì£¼ì‚¬ìœ„ë¥¼ ë˜ì§„ ê²°ê³¼ë¡œë¶€í„° $1, \\ldots, K$ ì¤‘ ì–´ëŠ ì£¼ì‚¬ìœ„ë¥¼ ë˜ì¡ŒëŠ”ì§€ë¥¼ ì°¾ì•„ë‚´ëŠ” ëª¨í˜•ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. Naive Bayes ì‹¤ìŠµ1. Gaussian Naive Bayes ë°ì´í„°, ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸° iris(ë¶“ê½ƒ)ë°ì´í„°ë¥¼ ê°€ì§€ê³  ì‹¤ìŠµì„ ì§„í–‰í•  ê²ƒì´ë‹¤. ë¶“ê½ƒì˜ ì¢…ë¥˜ëŠ” 3ê°€ì§€(â€˜setosaâ€™, â€˜versicolorâ€™, â€˜virginicaâ€™)ì´ë©°, ê° featureëŠ” sepal length (cm), sepal width (cm), petal length (cm), petal width (cm) ë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤. ëª¨ë“  í”¼ì²˜ê°€ ì‹¤ìˆ˜ì˜ ê°’ì„ ê°–ê¸° ë•Œë¬¸ì— ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤. 12345import pandas as pdimport numpy as npfrom sklearn import datasetsfrom sklearn.naive_bayes import GaussianNBfrom sklearn.model_selection import train_test_split 123iris = datasets.load_iris()df_X=pd.DataFrame(iris.data, columns=iris.feature_names)df_Y=pd.DataFrame(iris.target, columns=[\"target\"]) ì˜ˆì¸¡ì˜ ì„±ëŠ¥ì€ í•­ìƒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í†µí•´ ì¸¡ì •í•´ì•¼ í•˜ë¯€ë¡œ ë¶„ë¦¬í•´ì¤€ë‹¤. ì§€ê¸ˆì€ ì—°ìŠµì‚¼ì•„ í•´ë³´ëŠ” ê²ƒì´ë¯€ë¡œ í•„ìëŠ” ë”°ë¡œ validation setì„ êµ¬ì¶•í•˜ì§€ ì•Šê² ë‹¤. í—ˆë‚˜ ì‹¤ì œë¡œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ê·¸ì— ë”°ë¥¸ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ë ¤ë©´ ê¼­ validation setì„ ë”°ë¡œ ë‘ì–´ ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ì˜ parameterë“¤ì„ íŠœë‹ì„ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ë„ë¡ í•œ ë’¤ ìµœì¢…ì ìœ¼ë¡œ test setì„ ì˜ˆì¸¡í•˜ì—¬ ì„±ëŠ¥ì„ ê²€ì¦í•´ì•¼ í•  ê²ƒì´ë‹¤. 1train_X, test_X, train_Y, test_Y = train_test_split(df_X, df_Y, train_size=0.8, test_size=0.2, random_state=123) ëª¨ë¸ í”¼íŒ… 1234GNB = GaussianNB()fitted = GNB.fit(train_X, train_Y)y_pred = fitted.predict(test_X)y_pred í™•ë¥  êµ¬í•˜ê¸° ë¨¼ì € ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ì™€ í•´ë‹¹ ì˜ˆì¸¡ ë°ì´í„°ì˜ í´ë˜ìŠ¤ë³„ í™•ë¥ ì„ ì‚´í´ ë³¼ ê²ƒì´ë‹¤. 1fitted.predict(test_X)[:1] ê²°ê³¼11 1fitted.predict_proba(test_X)[:1] ê²°ê³¼1array([[7.24143720e-126, 9.23061979e-001, 7.69380215e-002]]) ìœ„ì˜ í™•ë¥  ì§ì ‘ êµ¬í•´ë³´ê¸° 1) ìœ„ì˜ í™•ë¥  ê°’ì´ ë‚˜ì˜¤ê²Œ ëœ ì¤‘ê°„ê³¼ì •ì„ ì‚´í´ë³´ì. ìš°ì„  ì¶”ì •ëœ ë…ë¦½ë³€ìˆ˜ì˜ ëª¨ìˆ˜ì™€ ì •ê·œ ë¶„í¬ì˜ í™•ë¥ ë°€ë„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ëŠ¥ë„ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. p(x_1, x_2, x_3, x_4 | y) \\propto p(x_1) p(x_2) p(x_3) p(x_4)- ìœ„ì—ì„œ í•™ìŠµí–ˆì„ ë•Œë„ ë§í–ˆë˜ ê²ƒ ì²˜ëŸ¼, (classê°œìˆ˜ * ë³€ìˆ˜ì˜ ê°œìˆ˜)ê°œì˜ ì¡°í•©ì˜ ëª¨ìˆ˜ë¥¼ ê°–ê³  ìˆìœ¼ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì´ 12ê°œì˜ ëª¨ìˆ˜ë¥¼ ê°–ëŠ”ë‹¤. ì•„ë˜ì—ì„œëŠ” ê° classë³„ë¡œ ì •ê·œë¶„í¬ì˜ ëª¨ìˆ˜ì¸ í‰ê· ê³¼ ë¶„ì‚°ì„ ë³´ì—¬ì¤€ë‹¤. 12predict_data = np.array(test_X.iloc[0])predict_data ê²°ê³¼1array([6.3, 2.5, 4.9, 1.5]) ì¶”ì •í•œ ëª¨ë¸ì˜ í´ë˜ìŠ¤ë³„ ëª¨ìˆ˜(í‰ê· ê³¼ ë¶„ì‚°)ì„ ë‹¤ìŒê³¼ ê°™ì´ ì•Œ ìˆ˜ ìˆë‹¤. 1fitted.theta_[0], fitted.sigma_[0] ê²°ê³¼12(array([5.01621622, 3.43243243, 1.46756757, 0.25945946]), array([0.10568298, 0.14975895, 0.02705625, 0.01214025])) 1fitted.theta_[1], fitted.sigma_[1] ê²°ê³¼12(array([5.95 , 2.78409091, 4.24090909, 1.32272727]), array([0.27068182, 0.10042872, 0.22741736, 0.04221075])) 1fitted.theta_[2], fitted.sigma_[2] ê²°ê³¼12(array([6.58717949, 2.95897436, 5.57948718, 2.02820513]), array([0.39752795, 0.11011177, 0.29188692, 0.0774096 ])) ìœ„ì˜ ëª¨ìˆ˜ë“¤ì„ í†µí•´ classë³„ ê°€ëŠ¥ë„ë¥¼ êµ¬í•˜ë©´ ì•„ë˜ì™€ ê°™ì„ ê²ƒì´ë‹¤. 123456789101112131415likelihood = [(sp.stats.norm(fitted.theta_[0][0], np.sqrt(fitted.sigma_[0][0])).pdf(predict_data[0]) * \\sp.stats.norm(fitted.theta_[0][1], np.sqrt(fitted.sigma_[0][1])).pdf(predict_data[1]) * \\sp.stats.norm(fitted.theta_[0][2], np.sqrt(fitted.sigma_[0][2])).pdf(predict_data[2]) * \\sp.stats.norm(fitted.theta_[0][3], np.sqrt(fitted.sigma_[0][3])).pdf(predict_data[3])),\\(sp.stats.norm(fitted.theta_[1][0], np.sqrt(fitted.sigma_[1][0])).pdf(predict_data[0]) * \\sp.stats.norm(fitted.theta_[1][1], np.sqrt(fitted.sigma_[1][1])).pdf(predict_data[1]) * \\sp.stats.norm(fitted.theta_[1][2], np.sqrt(fitted.sigma_[1][2])).pdf(predict_data[2]) * \\sp.stats.norm(fitted.theta_[1][3], np.sqrt(fitted.sigma_[1][3])).pdf(predict_data[3])),\\(sp.stats.norm(fitted.theta_[2][0], np.sqrt(fitted.sigma_[0][0])).pdf(predict_data[0]) * \\sp.stats.norm(fitted.theta_[2][1], np.sqrt(fitted.sigma_[0][1])).pdf(predict_data[1]) * \\sp.stats.norm(fitted.theta_[2][2], np.sqrt(fitted.sigma_[0][2])).pdf(predict_data[2]) * \\sp.stats.norm(fitted.theta_[2][3], np.sqrt(fitted.sigma_[0][3])).pdf(predict_data[3])) ]likelihood ê²°ê³¼1[2.0700298536453225e-126, 0.2218869448618605, 7.497361843154609e-09] ì—¬ê¸°ì— ì‚¬ì „í™•ë¥ ì„ ê³±í•˜ë©´ ì‚¬í›„ í™•ë¥ ì— ë¹„ë¡€í•˜ëŠ” ê°’ì´ ë‚˜ì˜¨ë‹¤. p(y|x_1, x_2) \\propto p(x_1, x_2|y) p(y) ì•„ì§ ì •ê·œí™” ìƒìˆ˜ $p(x)$ë¡œ ë‚˜ëˆ„ì–´ì£¼ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ë‘ ê°’ì˜ í•©ì´ 1ì´ ì•„ë‹ˆë‹¤. ì¦‰, í™•ë¥ ì´ë¼ê³  ë¶€ë¥¼ ìˆ˜ëŠ” ì—†ë‹¤. í•˜ì§€ë§Œ í¬ê¸°ë¥¼ ë¹„êµí•˜ë©´ ì´ ë°ì´í„°ëŠ” $y=1$ì¼ í™•ë¥ ì´ ê°€ì¥ ë†’ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 1fitted.class_prior_ ê²°ê³¼1array([0.30833333, 0.36666667, 0.325 ]) 12posterior = likelihood * fitted.class_prior_posterior ê²°ê³¼1array([6.38259205e-127, 8.13585464e-002, 2.43664260e-009]) ì´ ê°’ì„ ì •ê·œí™”í•˜ë©´ predict_proba ë©”ì„œë“œë¡œ êµ¬í•œ ê²ƒê³¼ ê°™ì€ ê°’ì´ ë‚˜ì˜¨ë‹¤. ë¬¼ë¡  ì™„ë²½íˆ ì¼ì¹˜í•˜ì§„ ì•Šì§€ë§Œ ê·¸ì— ê·¼ì‚¬í•˜ëŠ” ê°’ì„ ì¶”ì •ê°’ìœ¼ë¡œ ê³„ì‚°í•´ë‚´ì„œ ì–»ì„ ìˆ˜ ìˆë‹¤. ì´ëŠ” ê³„ì‚°ì‹œ ì†Œìˆ˜ì ì„ ì–´ëŠì •ë„ê¹Œì§€ ì‚¬ìš©í•˜ì˜€ëŠ”ì§€ì— ë”°ë¼ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì´ ì •ë„ì˜ ì˜¤ì°¨ëŠ” ë¬¸ì œê°€ ì—†ë‹¤. 1posterior / np.sum(posterior, axis=0) ê²°ê³¼1array([7.84501707e-126, 9.99999970e-001, 2.99494353e-008]) 1fitted.predict_proba(test_X)[:1] ê²°ê³¼1array([[7.24143720e-126, 9.23061979e-001, 7.69380215e-002]]) Confusion matrix êµ¬í•˜ê¸° 12from sklearn.metrics import confusion_matrixconfusion_matrix(y_pred, test_Y) ê²°ê³¼123array([[13, 0, 0], [ 0, 6, 1], [ 0, 0, 10]]) Prior ì„¤ì •í•˜ê¸° ì´ë²ˆì—ëŠ” classê°€ ë°œìƒë˜ëŠ” ì‚¬ì „í™•ë¥ ì„ ë¯¸ë¦¬ ì•Œê³  ìˆì—ˆë˜ ê²½ìš°ë¼ê³  ê°€ì •í•˜ê³  ë¬¸ì œë¥¼ í’€ì–´ë³¼ ê²ƒì´ë‹¤. 1234GNB2 = GaussianNB(priors=[0.01, 0.01, 0.98])set_prior_fitted_01 = GNB2.fit(train_X, train_Y)set_prior_pred_01 = set_prior_fitted_01.predict(test_X)confusion_matrix(set_prior_pred_01, test_Y) ê²°ê³¼123array([[13, 0, 0], [ 0, 4, 0], [ 0, 2, 11]]) 1234GNB3 = GaussianNB(priors=[0.01, 0.98, 0.01])set_prior_fitted_02 = GNB3.fit(train_X, train_Y)set_prior_pred_02 = set_prior_fitted_02.predict(test_X)confusion_matrix(set_prior_pred_02, test_Y) 123array([[13, 0, 0], [ 0, 6, 4], [ 0, 0, 7]]) 2. Bernoulli naive bayes e-mailê³¼ ê°™ì€ ë¬¸ì„œ ë‚´ì— íŠ¹ì •í•œ ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ì˜ ì—¬ë¶€ëŠ” ë² ë¥´ëˆ„ì´ í™•ë¥ ë³€ìˆ˜ë¡œ ëª¨í˜•í™”í•  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ë…ë¦½ë³€ìˆ˜ê°€ 0 ë˜ëŠ” 1ì˜ ê°’ì„ ê°€ì§€ë©´ ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì„ ì‚¬ìš©í•œë‹¤. pythonì˜ sklearnì˜ ë² ë¥´ëˆ„ì´ë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜• í´ë˜ìŠ¤ BernoulliNBëŠ” ê°€ëŠ¥ë„ ì¶”ì •ê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ì†ì„±ì„ ê°€ì§„ë‹¤. feature_count_ : ê° class kì— ëŒ€í•´ dë²ˆì§¸ ë™ì „ì´ ì•ë©´ì´ ë‚˜ì˜¨ íšŸìˆ˜ $N_{d,k}$ feature_log_prob_ : ë² ë¥´ëˆ„ì´ë¶„í¬ ëª¨ìˆ˜ì˜ ë¡œê·¸ê°’ \\log \\mu_k = (\\log \\mu_{1,k}, \\ldots, \\log \\mu_{D, k}) = \\left( \\log \\dfrac{N_{1,k}}{N_k}, \\ldots, \\log \\dfrac{N_{D,k}}{N_k} \\right) ì—¬ê¸°ì—ì„œ $N_{k}$ì€ í´ë˜ìŠ¤ kì— ëŒ€í•´ ë™ì „ì„ ë˜ì§„ ì´ íšŸìˆ˜ì´ë‹¤. í‘œë³¸ ë°ì´í„°ì˜ ìˆ˜ê°€ ì ì€ ê²½ìš°ì—ëŠ” ëª¨ìˆ˜ì— ëŒ€í•´ ë‹¤ìŒì²˜ëŸ¼ ìŠ¤ë¬´ë”©(smoothing)ì„ í•  ìˆ˜ë„ ìˆë‹¤. ìŠ¤ë¬´ë”©(Smoothing) í‘œë³¸ ë°ì´í„°ì˜ ìˆ˜ê°€ ì ì€ ê²½ìš°ì—ëŠ” ë² ë¥´ëˆ„ì´ ëª¨ìˆ˜ê°€ 0 ë˜ëŠ” 1ì´ë¼ëŠ” ê·¹ë‹¨ì ì¸ ëª¨ìˆ˜ ì¶”ì •ê°’ì´ ë‚˜ì˜¬ ìˆ˜ë„ ìˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì ìœ¼ë¡œëŠ” ì‹¤ì œ ëª¨ìˆ˜ê°’ì´ ì´ëŸ° ê·¹ë‹¨ì ì¸ ê°’ì´ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ì ë‹¤. ë”°ë¼ì„œ ë² ë¥´ëˆ„ì´ ëª¨ìˆ˜ê°€ 0.5ì¸ ê°€ì¥ ì¼ë°˜ì ì¸ ê²½ìš°ë¥¼ ê°€ì •í•˜ì—¬ 0ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ì™€ 1ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°, ë‘ ê°œì˜ ê°€ìƒ í‘œë³¸ ë°ì´í„°ë¥¼ ì¶”ê°€í•œë‹¤. ê·¸ëŸ¬ë©´ 0ì´ë‚˜ 1ê³¼ ê°™ì€ ê·¹ë‹¨ì ì¸ ì¶”ì •ê°’ì´ 0.5ì— ê°€ê¹Œìš´ ë‹¤ìŒê³¼ ê°™ì€ ê°’ìœ¼ë¡œ ë³€í•œë‹¤. ì´ë¥¼ ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©(Laplace smoothing) ë˜ëŠ” ì• ë“œì›(Add-One) ìŠ¤ë¬´ë”©ì´ë¼ê³  í•œë‹¤. \\hat{\\mu}_{d,k} = \\frac{ N_{d,k} + \\alpha}{N_k + 2 \\alpha} ê°€ì¤‘ì¹˜ $\\alpha$ë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤ë¬´ë”©ì˜ ì •ë„ë¥¼ ì¡°ì ˆí•  ìˆ˜ë„ ìˆë‹¤. ê°€ì¤‘ì¹˜ $\\alpha$ëŠ” ì •ìˆ˜ê°€ ì•„ë‹ˆë¼ë„ ê´œì°®ë‹¤. ê°€ì¤‘ì¹˜ê°€ 1ì¸ ê²½ìš°ëŠ” ë¬´ì •ë³´ ì‚¬ì „í™•ë¥ ì„ ì‚¬ìš©í•œ ë² ì´ì¦ˆ ëª¨ìˆ˜ì¶”ì •ì˜ ê²°ê³¼ì™€ ê°™ë‹¤. ì•„ë˜ì˜ ë°ì´í„°ëŠ” 4ê°œì˜ key wordë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ìƒ ë©”ì¼ 4ê°œì™€ spam ë©”ì¼ 6ê°œë¥¼ BOW ì¸ì½”ë”©í•œ í–‰ë ¬ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì²«ë²ˆì§¸ ë©”ì¼ì€ ì •ìƒ ë©”ì¼ì´ê³  1ë²ˆ, 4ë²ˆ key wordëŠ” í¬í•¨í•˜ì§€ ì•Šì§€ë§Œ 2ë²ˆ,3ë²ˆ key wordë¥¼ í¬í•¨í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. 12345678910111213141516from sklearn.naive_bayes import BernoulliNBX = np.array([ [0, 1, 1, 0], [1, 1, 1, 1], [1, 1, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1], [0, 1, 1, 0], [0, 1, 1, 1], [1, 0, 1, 0], [1, 0, 1, 1], [0, 1, 1, 0]])y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])BNB = BernoulliNB()fitted = BNB.fit(X, y) y í´ë˜ìŠ¤ì˜ ì¢…ë¥˜ì™€ ê° í´ë˜ìŠ¤ì— ì†í•˜ëŠ” í‘œë³¸ì˜ ìˆ˜, ê·¸ë¦¬ê³  ê·¸ ê°’ìœ¼ë¡œë¶€í„° êµ¬í•œ ì‚¬ì „ í™•ë¥ ì˜ ê°’ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 1fitted.classes_ ê²°ê³¼1array([0, 1]) 1fitted.class_count_ ê²°ê³¼1array([4., 6.]) 1np.exp(fitted.class_log_prior_) ê²°ê³¼1array([0.4, 0.6]) ê° í´ë˜ìŠ¤ k ë³„ë¡œ, ê·¸ë¦¬ê³  ê° ë…ë¦½ë³€ìˆ˜ d ë³„ë¡œ, ê°ê° ë‹¤ë¥¸ ë² ë¥´ëˆ„ì´ í™•ë¥ ë³€ìˆ˜ë¼ê³  ê°€ì •í•˜ì—¬ ëª¨ë‘ 8ê°œì˜ ë² ë¥´ëˆ„ì´ í™•ë¥ ë³€ìˆ˜ì˜ ëª¨ìˆ˜ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. 123# í–‰ì€ í´ë˜ìŠ¤ì˜ ê°œìˆ˜, ì—´ì€ ë³€ìˆ˜ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸count = fitted.feature_count_count ê²°ê³¼12array([[2., 4., 3., 1.], [2., 3., 5., 3.]]) ìœ„ì˜ ê° ë³€ìˆ˜ì˜ í´ë˜ìŠ¤ë³„ë¡œ ëª‡ë²ˆ ë‚˜ì˜¨ì§€ì— ëŒ€í•œ í–‰ë ¬ì— ê° í´ë˜ìŠ¤ì˜ ì „ì²´ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì£¼ì–´ì•¼ í•´ë‹¹ ë³€ìˆ˜ë“¤ì˜ ëª¨ìˆ˜ì¸ pê°’ì„ ì•Œ ìˆ˜ ìˆìœ¼ë¯€ë¡œ class_countì˜ ë°°ì—´ì˜ ëª¨ì–‘ì„ ë³€í˜•ì‹œì¼œì£¼ì–´ì•¼ í•œë‹¤. í˜„ì¬ëŠ” 1ì°¨ì›ì˜ ë²¡í„°(2,)ì´ë¯€ë¡œ 2ì°¨ì›ì˜ (2,1)ì˜ ëª¨ì–‘ì„ ê°–ë„ë¡ í•´ì£¼ì–´ì•¼ ë‚˜ëˆ„ì–´ ì¤„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. 1count / fitted.class_count_[:,np.newaxis] ê²°ê³¼12array([[0.5 , 1. , 0.75 , 0.25 ], [0.33333333, 0.5 , 0.83333333, 0.5 ]]) ìœ„ì—ì„œëŠ” í•„ìëŠ” Numpyì˜ brodcasting ì—°ì‚°ì„ ì‚¬ìš©í•˜ì—¬ êµ¬í•œê²ƒì¸ë°, í˜¹ì‹œ count í–‰ë ¬ì˜ ëª¨ì–‘ê³¼ ë™ì¼í•˜ê²Œ ë§Œë“¤ì–´ í™•ì‹¤í•˜ê²Œ ì—°ì‚°í•˜ê³  ì‹¶ë‹¤ë©´, ì•„ë˜ì™€ ê°™ì´ ì‹¤í–‰í•˜ë©´ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. 1count / np.repeat(fitted.class_count_[:,np.newaxis], 4, axis=1) ê²°ê³¼12array([[0.5 , 1. , 0.75 , 0.25 ], [0.33333333, 0.5 , 0.83333333, 0.5 ]]) ê·¸ëŸ°ë° ì´ ê°’ì€ ëª¨í˜• ë‚´ì—ì„œ êµ¬í•œ ê°’ê³¼ ë‹¤ë¥´ë‹¤. ëª¨í˜• ë‚´ì—ì„œ ìŠ¤ë¬´ë”©(smoothing)ì´ ì´ë£¨ì–´ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ìŠ¤ë¬´ë”©ì€ ë™ì „ì˜ ê° ë©´ ì¦‰, 0ê³¼ 1ì´ ë‚˜ì˜¤ëŠ” ê°€ìƒì˜ ë°ì´í„°ë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì„œ ì¶”ì •í•œ ëª¨ìˆ˜ì˜ ê°’ì´ ì¢€ ë” 0.5ì— ê°€ê¹Œì›Œì§€ë„ë¡ í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ ë•Œ ì‚¬ìš©í•œ ìŠ¤ë¬´ë”© ê°€ì¤‘ì¹˜ ê°’ì€ ë‹¤ìŒì²˜ëŸ¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 1fitted.alpha ê²°ê³¼11.0 12theta = np.exp(fitted.feature_log_prob_)theta ê²°ê³¼12array([[0.5 , 0.83333333, 0.66666667, 0.33333333], [0.375 , 0.5 , 0.75 , 0.5 ]]) ì´ì— ëª¨í˜•ì´ ì™„ì„±ë˜ì—ˆìœ¼ë‹ˆ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ í•´ ë³¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 1ë²ˆ, 2ë²ˆ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ë©”ì¼ì´ ì •ìƒ ë©”ì¼ì¸ì§€ ìŠ¤íŒ¸ ë©”ì¼ì¸ì§€ ì•Œì•„ë³´ì. 12x_new = np.array([0, 1, 1, 1])fitted.predict_proba([x_new]) ê²°ê³¼1array([[0.34501348, 0.65498652]]) ìœ„ ê²°ê³¼ì—ì„œ ì •ìƒ ë©”ì¼ì¼ ê°€ëŠ¥ì„±ì´ ì•½ 3ë°°ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ ê°’ì€ ë‹¤ìŒì²˜ëŸ¼ êµ¬í•  ìˆ˜ë„ ìˆë‹¤. 123# np.prod(axis=1)ë¡œ í•´ì¤€ ì´ìœ ëŠ” ì•„ë˜ ê° ë³€ìˆ˜ë³„ë¡œ ë…ë¦½ì´ë¯€ë¡œ ê°€ëŠ¥ë„ë¥¼ êµ¬í•˜ë ¤ë©´ ê³±ì„ í•´ì£¼ì–´ì•¼ í•˜ê¸° ë•Œë¬¸p = ((theta ** x_new) * (1 - theta) ** (1 - x_new)).prod(axis=1) * np.exp(fitted.class_log_prior_)p / p.sum() ê²°ê³¼1array([0.34501348, 0.65498652]) ë°˜ëŒ€ë¡œ 3ë²ˆ, 4ë²ˆ keywordê°€ í¬í•¨ëœ ë©”ì¼ì€ ìŠ¤íŒ¸ì¼ ê°€ëŠ¥ì„±ì´ ì•½ 90%ì´ë‹¤. 12x_new = np.array([0, 0, 1, 1])fitted.predict_proba([x_new]) ê²°ê³¼1array([[0.09530901, 0.90469099]]) 12p = ((theta ** x_new) * (1 - theta) ** (1 - x_new)).prod(axis=1) * np.exp(fitted.class_log_prior_)p / p.sum() ê²°ê³¼1array([[0.09530901, 0.90469099]]) MNIST ìˆ«ì ë¶„ë¥˜ë¬¸ì œì—ì„œ sklearn.preprocessing.Binarizerë¡œ xê°’ì„ 0, 1ë¡œ ë°”ê¾¼ë‹¤(ê°’ì´ 8 ì´ìƒì´ë©´ 1, 8 ë¯¸ë§Œì´ë©´ 0). ì¦‰ í°ìƒ‰ê³¼ ê²€ì€ìƒ‰ í”½ì…€ë¡œë§Œ êµ¬ì„±ëœ ì´ë¯¸ì§€ë¡œ ë§Œë“ ë‹¤(ë‹¤ìŒ ì½”ë“œ ì°¸ì¡°) 123456from sklearn.datasets import load_digitsdigits = load_digits()X = digits.datay = digits.targetfrom sklearn.preprocessing import BinarizerX = Binarizer(7).fit_transform(X) ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì„ ì ìš©í•˜ì. ë¶„ë¥˜ ê²°ê³¼ë¥¼ ë¶„ë¥˜ë³´ê³ ì„œ í˜•ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë¼. (1) BernoulliNB í´ë˜ìŠ¤ì˜ binarize ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°™ì€ ë¬¸ì œë¥¼ í’€ì–´ë³¸ë‹¤. (2) ê³„ì‚°ëœ ëª¨í˜•ì˜ ëª¨ìˆ˜ ë²¡í„° ê°’ì„ ê° í´ë˜ìŠ¤ë³„ë¡œ 8x8 ì´ë¯¸ì§€ì˜ í˜•íƒœë¡œ ë‚˜íƒ€ë‚´ë¼. ì´ ì´ë¯¸ì§€ëŠ” ë¬´ì—‡ì„ ëœ»í•˜ëŠ”ê°€? 1digits.images[0] ê²°ê³¼12345678array([[ 0., 0., 5., 13., 9., 1., 0., 0.], [ 0., 0., 13., 15., 10., 15., 5., 0.], [ 0., 3., 15., 2., 0., 11., 8., 0.], [ 0., 4., 12., 0., 0., 8., 8., 0.], [ 0., 5., 8., 0., 0., 9., 8., 0.], [ 0., 4., 11., 0., 1., 12., 7., 0.], [ 0., 2., 14., 5., 10., 12., 0., 0.], [ 0., 0., 6., 13., 10., 0., 0., 0.]]) 1X.shape ê²°ê³¼1(1797, 64) 1234BNB = BernoulliNB()fitted = BNB.fit(X, y)theta = np.exp(fitted.feature_log_prob_)theta = theta.reshape((10, 8, 8)) 12345678fig, axes = plt.subplots(2, 5, figsize=(12, 3), subplot_kw=&#123;'xticks': [], 'yticks': []&#125;)for i in range(5): axes[0][i].set_title(\"class &#123;&#125;\".format(i)) axes[0][i].imshow(theta[i], interpolation='nearest', cmap=plt.cm.Blues) axes[1][i].set_title(\"class &#123;&#125;\".format(i+5)) axes[1][i].imshow(theta[i+5], interpolation='nearest', cmap=plt.cm.Blues)plt.show() ìœ„ì˜ ì´ë¯¸ì§€ì—ì„œëŠ” ëª¨ìˆ˜ê°’ì´ ë†’ì€ ë³€ìˆ˜ê°€ ì§„í•œ íŒŒë€ìƒ‰ì„ ë„ê²Œ ëœë‹¤. sklearn.preprocessing.Binarizerë¥¼ í†µí•´ xê°’ì„ ê°’ì´ 8 ì´ìƒì´ë©´ 1, 8 ë¯¸ë§Œì´ë©´ 0ìœ¼ë¡œ ë°”ê¾¸ì–´ì£¼ì—ˆìœ¼ë¯€ë¡œ 8 ë¯¸ë§Œì¸ ë°ì´í„°ë³´ë‹¤ëŠ” 8ì´ìƒì¸ ë°ì´í„°ê°€ ê° í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„í•˜ëŠ”ë° ì¢€ ë” ì˜í–¥ì„ ì£¼ëŠ” ê³µê°„ì„ ì•Œ ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. 3. Multinomial naive bayes ë‹¤í•­ë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜• í´ë˜ìŠ¤ MultinomialNBëŠ” ê°€ëŠ¥ë„ ì¶”ì •ê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ì†ì„±ì„ ê°€ì§„ë‹¤. feature_count_ : ê° í´ë˜ìŠ¤ $k$ì—ì„œ $d$ë²ˆì§¸ ë©´ì´ ë‚˜ì˜¨ íšŸìˆ˜ $N_{d,k}$ feature_log_prob_ : ë‹¤í•­ë¶„í¬ì˜ ëª¨ìˆ˜ì˜ ë¡œê·¸ \\log \\mu_k = (\\log \\mu_{1,k}, \\ldots, \\log \\mu_{D, k}) = \\left( \\log \\dfrac{N_{1,k}}{N_k}, \\ldots, \\log \\dfrac{N_{D,k}}{N_k} \\right) ì—¬ê¸°ì—ì„œ $N_{k}$ì€ í´ë˜ìŠ¤ $k$ì— ëŒ€í•´ ì£¼ì‚¬ìœ„ë¥¼ ë˜ì§„ ì´ íšŒìˆ˜ë¥¼ ëœ»í•œë‹¤. ìŠ¤ë¬´ë”©ê³µì‹ì€ ì•„ë˜ì™€ ê°™ë‹¤. \\hat{\\mu}_{d,k} = \\frac{ N_{d,k} + \\alpha}{N_k + D \\alpha} , \\; (D=ë³€ìˆ˜ì˜ ê°œìˆ˜) ì´ë²ˆì—ë„ ìŠ¤íŒ¸ ë©”ì¼ í•„í„°ë§ì„ ì˜ˆë¡œ ë“¤ì–´ë³´ë‹¤. ë‹¤ë§Œ BOW ì¸ì½”ë”©ì„ í•  ë•Œ, ê° í‚¤ì›Œë“œê°€ ì¶œí˜„í•œ ë¹ˆë„ë¥¼ ì§ì ‘ ì…ë ¥ ë³€ìˆ˜ë¡œ ì‚¬ìš©í•œë‹¤. 123456789101112X = np.array([ [3, 4, 1, 2], [3, 5, 1, 1], [3, 3, 0, 4], [3, 4, 1, 2], [1, 2, 1, 4], [0, 0, 5, 3], [1, 2, 4, 1], [1, 1, 4, 2], [0, 1, 2, 5], [2, 1, 2, 3]])y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1]) 123from sklearn.naive_bayes import MultinomialNBMNB = MultinomialNB()fitted = MNB.fit(X, y) 1fitted.classes_ ê²°ê³¼1array([0, 1]) 1fitted.class_count_ ê²°ê³¼1array([4., 6.]) 1np.exp(fitted.class_log_prior_) ê²°ê³¼1array([0.4, 0.6]) ë‹¤ìŒìœ¼ë¡œ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ê°€ëŠ¥ë„ í™•ë¥ ë¶„í¬ë¥¼ êµ¬í•œë‹¤. ë‹¤í•­ë¶„í¬ ëª¨í˜•ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ê° í´ë˜ìŠ¤ë¥¼ 4ê°œì˜ ë©´ì„ ê°€ì§„ ì£¼ì‚¬ìœ„ë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ê° ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì€ ê° ë©´ì´ ë‚˜ì˜¨ íšŸìˆ˜ë¥¼ ì£¼ì‚¬ìœ„ë¥¼ ë˜ì§„ ì „ì²´ íšŸìˆ˜ë¡œ ë‚˜ëˆ„ë©´ ëœë‹¤. ìš°ì„  ê° í´ë˜ìŠ¤ ë³„ë¡œ ê°ê°ì˜ ë©´ì´ ë‚˜ì˜¨ íšŸìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. 12count = fitted.feature_count_count ê²°ê³¼12array([[12., 16., 3., 9.], [ 5., 7., 18., 18.]]) ì´ ë°ì´í„°ì—ì„œ í´ë˜ìŠ¤ $Y=0$ì¸ ì£¼ì‚¬ìœ„ë¥¼ ë˜ì§„ íšŸìˆ˜ëŠ” ì²«ë²ˆì§¸ í–‰ì˜ ê°’ì˜ í•©ì¸ 40ì´ë¯€ë¡œ í´ë˜ìŠ¤ $Y=0$ì¸ ì£¼ì‚¬ìœ„ë¥¼ ë˜ì ¸ 1ì´ë¼ëŠ” ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì€ ë‹¤ìŒì²˜ëŸ¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. \\mu_{1,Y=0} = \\dfrac{12}{40} = 0.31count / np.repeat(count.sum(axis=1)[:,np.newaxis], 4, axis=1) ê²°ê³¼12array([[0.3 , 0.4 , 0.075 , 0.225 ], [0.10416667, 0.14583333, 0.375 , 0.375 ]]) ì‹¤ì œë¡œëŠ” ê·¹ë‹¨ì ì¸ ì¶”ì •ì„ í”¼í•˜ê¸° ìœ„í•´ ì´ ê°’ì„ ê°€ì¤‘ì¹˜ 1ì¸ ìŠ¤ë¬´ë”©ì„ í•œ ì¶”ì •ê°’ì„ ì‚¬ìš©í•œë‹¤. 1fitted.alpha ê²°ê³¼11.0 12(count + fitted.alpha) / \\(np.repeat(count.sum(axis=1)[:,np.newaxis], 4, axis=1) + fitted.alpha * X.shape[1]) ê²°ê³¼12array([[0.29545455, 0.38636364, 0.09090909, 0.22727273], [0.11538462, 0.15384615, 0.36538462, 0.36538462]]) 12theta = np.exp(fitted.feature_log_prob_)theta ê²°ê³¼12array([[0.29545455, 0.38636364, 0.09090909, 0.22727273], [0.11538462, 0.15384615, 0.36538462, 0.36538462]]) ì´ì œ ì´ ê°’ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ì„ í•´ ë³´ì. ë§Œì•½ ì–´ë–¤ ë©”ì¼ì— 1ë²ˆë¶€í„° 4ë²ˆê¹Œì§€ì˜ í‚¤ì›Œë“œê°€ ê°ê° 10ë²ˆì”© ë‚˜ì™”ë‹¤ë©´ ë‹¤ìŒì²˜ëŸ¼ í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆë‹¤. êµ¬í•´ì§„ í™•ë¥ ë¡œë¶€í„° ì´ ë©”ì¼ì´ ìŠ¤íŒ¸ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. 12x_new = np.array([10, 10, 10, 10])fitted.predict_proba([x_new]) ê²°ê³¼1array([[0.38848858, 0.61151142]]) 12p = (theta ** x_new).prod(axis=1) * np.exp(fitted.class_log_prior_)p / p.sum(axis=0) ê²°ê³¼1array([0.38848858, 0.61151142]) MNIST ìˆ«ì ë¶„ë¥˜ë¬¸ì œë¥¼ ë‹¤í•­ë¶„í¬ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì„ ì‚¬ìš©í•˜ì—¬ í’€ê³  ì´ì§„í™”(Binarizing)ë¥¼ í•˜ì—¬ ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜•ì„ ì ìš©í–ˆì„ ê²½ìš°ì™€ ì„±ëŠ¥ì„ ë¹„êµí•˜ë¼. 12345678910111213from sklearn.datasets import load_digitsfrom sklearn.metrics import classification_reportdigits = load_digits()X = digits.datay = digits.targettrain_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.3, random_state=123)from sklearn.preprocessing import Binarizerbinary_train_X = Binarizer(7).fit_transform(train_X)binary_test_X = Binarizer(7).fit_transform(test_X)BNB = BernoulliNB().fit(binary_train_X, train_Y)bnb_pred = BNB.predict(binary_test_X)MNB = MultinomialNB().fit(train_X, train_Y)mnb_pred = MNB.predict(test_X) ì´ì§„í™” í•œ ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ëª¨í˜• ì„±ëŠ¥1print(classification_report(bnb_pred, test_Y)) ê²°ê³¼12345678910111213141516 precision recall f1-score support 0 0.98 0.98 0.98 59 1 0.82 0.84 0.83 55 2 0.87 0.90 0.88 51 3 0.80 0.93 0.86 40 4 0.95 0.97 0.96 60 5 0.84 0.94 0.89 51 6 0.96 1.00 0.98 55 7 1.00 0.89 0.94 56 8 0.85 0.80 0.83 51 9 0.87 0.74 0.80 62 accuracy 0.90 540 macro avg 0.90 0.90 0.90 540weighted avg 0.90 0.90 0.90 540 ë‹¤í•­ ë¶„í¬ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜• ì„±ëŠ¥1print(classification_report(mnb_pred, test_Y)) ê²°ê³¼12345678910111213141516 precision recall f1-score support 0 0.98 1.00 0.99 58 1 0.77 0.88 0.82 49 2 0.83 0.88 0.85 50 3 0.89 1.00 0.94 41 4 0.97 0.92 0.94 64 5 0.74 1.00 0.85 42 6 0.98 0.98 0.98 57 7 1.00 0.89 0.94 56 8 0.81 0.74 0.77 53 9 0.89 0.67 0.76 70 accuracy 0.89 540 macro avg 0.89 0.90 0.89 540weighted avg 0.89 0.89 0.89 540 í…ìŠ¤íŠ¸ ë¶„ì„ì—ì„œ TF-IDF ì¸ì½”ë”©ì„ í•˜ë©´ ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜ê°€ ì •ìˆ˜ê°€ ì•„ë‹Œ ì‹¤ìˆ˜ê°’ì´ ëœë‹¤. ì´ëŸ° ê²½ìš°ì—ë„ ë‹¤í•­ë¶„í¬ ëª¨í˜•ì„ ì ìš©í•  ìˆ˜ ìˆëŠ”ê°€? ì •ìˆ˜ê°€ ì•„ë‹ˆë”ë¼ë„ í•´ë‹¹ ì ìš© ê°€ëŠ¥í•˜ë‹¤! ë‹¤í•­ë¶„í¬ì˜ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•  ë•Œ í•´ë‹¹ ê´€ì¸¡ì¹˜ì˜ ë³€ìˆ˜ê°€ ê°–ëŠ” ê°’ì˜ í•©ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì£¼ì–´ ëª¨ìˆ˜ê°’ì„ êµ¬í–ˆëŠ”ë°, TF-idf í–‰ë ¬ì€ rowê°€ ë¬¸ì„œë¥¼ ì˜ë¯¸í•˜ê³  ì „ì²´ ë¬¸ì„œì—ì„œì˜ í† í°ë“¤ì´ ì—´ì„ ì´ë£¨ê²Œ ë˜ë¯€ë¡œ í•´ë‹¹ ë¬¸ì„œì—ì„œ ì–´ë– í•œ ë‹¨ì–´ê°€ ëª‡ë²ˆ ë‚˜ì˜¨ê²ƒì¸ì§€ì— ëŒ€í•´ ë‹¤í•­ë¶„í¬ë¥¼ í†µí•´ ê³„ì‚°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. MultinomialNBë¥¼ ì‚¬ìš©í•  ê²½ìš° ë²”ì£¼í˜•ìœ¼ë¡œ ì •ìˆ˜ì´ë©´ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•˜ì§€ë§ê³  ìœ„ì˜ ì˜ˆì‹œ ì²˜ëŸ¼ ë°ì´í„° ë‹¹ ê° í”¼ì²˜ê°€ ìœ ê¸°ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ì‚¬ê±´ì—ì„œ íŒŒìƒë˜ì–´ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•´ì„œ ë¨¼ì € ìƒê°í•´ë³´ì. í†µê³„ì ì¸ ë¶„í¬ë¥¼ ë‹¤í•­ë¶„í¬ë¡œ ìƒê°í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ í™•ì¸í•´ë³´ìëŠ” ì´ì•¼ê¸°ì´ë‹¤. ì•„ë˜ì˜ ë‰´ìŠ¤ê·¸ë£¹ ë¶„ë¥˜ ë¬¸ì œë¥¼ í†µí•´ ê²€ì¦í•´ë³´ì. ë‰´ìŠ¤ê·¸ë£¹ ë¶„ë¥˜ ë‹¤ìŒì€ ë‰´ìŠ¤ê·¸ë£¹ ë°ì´í„°ì— ëŒ€í•´ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ë¶„ë¥˜ëª¨í˜•ì„ ì ìš©í•œ ê²°ê³¼ì´ë‹¤. ë¬¸ì„œëŠ”18846ê±´ 123456789101112131415161718192021222324252627from sklearn.datasets import fetch_20newsgroupsnews = fetch_20newsgroups(subset=\"all\")X = news.datay = news.targetfrom sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizerfrom sklearn.naive_bayes import MultinomialNBfrom sklearn.pipeline import Pipelinemodel1 = Pipeline([ ('vect', CountVectorizer()), ('model', MultinomialNB()),])model2 = Pipeline([ ('vect', TfidfVectorizer()), ('model', MultinomialNB()),])model3 = Pipeline([ ('vect', TfidfVectorizer(stop_words=\"english\")), ('model', MultinomialNB()),])model4 = Pipeline([ ('vect', TfidfVectorizer(stop_words=\"english\", token_pattern=r\"\\b[a-z0-9_\\-\\.]+[a-z][a-z0-9_\\-\\.]+\\b\")), ('model', MultinomialNB()),]) 123456%%timefrom sklearn.model_selection import cross_val_score, KFoldfor i, model in enumerate([model1, model2, model3, model4]): scores = cross_val_score(model, X, y, cv=5) print((\"Model&#123;0:d&#125;: Mean score: &#123;1:.3f&#125;\").format(i + 1, np.mean(scores))) ê²°ê³¼123456Model1: Mean score: 0.855Model2: Mean score: 0.856Model3: Mean score: 0.883Model4: Mean score: 0.888CPU times: user 1min 35s, sys: 4.54 s, total: 1min 40sWall time: 1min 53s (1) ë§Œì•½ ë…ë¦½ë³€ìˆ˜ë¡œ ì‹¤ìˆ˜ ë³€ìˆ˜, 0 ë˜ëŠ” 1 ê°’ì„ ê°€ì§€ëŠ” ë³€ìˆ˜, ìì—°ìˆ˜ ê°’ì„ ê°€ì§€ëŠ” ë³€ìˆ˜ê°€ ì„ì—¬ìˆë‹¤ë©´ ì‚¬ì´í‚·ëŸ°ì—ì„œ ì œê³µí•˜ëŠ” ë‚˜ì´ë¸Œë² ì´ì¦ˆ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í’€ ìˆ˜ ìˆëŠ”ê°€? - ìœ„ì—ì„œ likelihoodë¥¼ ì§ì ‘ ê³„ì‚°í–ˆë˜ ê²ƒê³¼ ê°™ì´ `likelihoodë§Œì„ ê°ê° ê³„ì‚°í•˜ì—¬ ê° ë³€ìˆ˜ë“¤ì€ ë…ë¦½ì´ë¼ëŠ” ê°€ì •ì„ ì „ì œë¡œí•˜ê¸° ë•Œë¬¸ì— ì„œë¡œ ê³±í•œë’¤ì— ë² ì´ì¦ˆì •ë¦¬ì‹ì— ë”°ë¼ ìµœì¢…ì ìœ¼ë¡œ í™•ë¥ ê°’ì„ êµ¬í•´ í´ë˜ìŠ¤ë¥¼ êµ¬ë¶„`í•  ìˆ˜ ìˆë‹¤. (2) ì‚¬ì´í‚·ëŸ°ì—ì„œ ì œê³µí•˜ëŠ” ë¶„ë¥˜ë¬¸ì œ ì˜ˆì œ ì¤‘ ìˆ²ì˜ ìˆ˜ì¢…ì„ ì˜ˆì¸¡í•˜ëŠ” covtype ë¶„ë¥˜ë¬¸ì œëŠ” ì—°ì†í™•ë¥ ë¶„í¬ íŠ¹ì§•ê³¼ ë² ë¥´ëˆ„ì´í™•ë¥ ë¶„í¬ íŠ¹ì§•ì´ ì„ì—¬ìˆë‹¤. ì´ ë¬¸ì œë¥¼ ì‚¬ì´í‚·ëŸ°ì—ì„œ ì œê³µí•˜ëŠ” ë‚˜ì´ë¸Œë² ì´ì¦ˆ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í’€ì–´ë¼. ëŒ€í‘œ ìˆ˜ì¢… ë°ì´í„°(covtype) ëŒ€í‘œ ìˆ˜ì¢… ë°ì´í„°ëŠ” ë¯¸êµ­ ì‚¼ë¦¼ì„ 30Ã—30m ì˜ì—­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê° ì˜ì—­ì˜ íŠ¹ì§•ìœ¼ë¡œë¶€í„° ëŒ€í‘œì ì¸ ë‚˜ë¬´ì˜ ì¢…ë¥˜(species of tree)ì„ ì˜ˆì¸¡í•˜ê¸°ìœ„í•œ ë°ì´í„°ì´ë‹¤. ìˆ˜ì¢…ì€ 7ì¢…ë¥˜ì´ì§€ë§Œ íŠ¹ì§• ë°ì´í„°ê°€ 54ì¢…ë¥˜, í‘œë³¸ ë°ì´í„°ì˜ ê°¯ìˆ˜ê°€ 581,012ê°œì— ë‹¬í•˜ëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ì´ë‹¤. 123from sklearn.datasets import fetch_covtypecovtype = fetch_covtype()# print(covtype.DESCR) 12345df = pd.DataFrame(covtype.data, columns=[\"x&#123;:02d&#125;\".format(i + 1) for i in range(covtype.data.shape[1])], dtype=int)sy = pd.Series(covtype.target, dtype=\"category\")df['covtype'] = sy ê° íŠ¹ì§• ë°ì´í„°ê°€ ê°€ì§€ëŠ” ê°’ì˜ ì¢…ë¥˜ë¥¼ ë³´ë©´ 1ë²ˆë¶€í„° 10ë²ˆ íŠ¹ì§•ì€ ì‹¤ìˆ˜ê°’ì´ê³  11ë²ˆë¶€í„° 54ë²ˆ íŠ¹ì§•ì€ ì´ì§„ ì¹´í…Œê³ ë¦¬ê°’ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 1df.iloc[:, 10:54] = df.iloc[:, 10:54].astype('category') ë‹¤ìŒ í”Œë¡¯ì€ ì¹´í…Œê³ ë¦¬ê°’ì— ë”°ë¼ â€œx14â€ íŠ¹ì§•ì˜ ê°’ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤. â€œx14â€ íŠ¹ì§•ì´ 0ì¸ê°€ 1ì¸ê°€ë¥¼ ì‚¬ìš©í•˜ë©´ 1, 5, 7ë²ˆ í´ë˜ìŠ¤ì™€ 4ë²ˆ í´ë˜ìŠ¤ëŠ” ì™„ë²½í•˜ê²Œ ë¶„ë¥˜í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 1234plt.figure(figsize=(10,12))df_count = df.pivot_table(index=\"covtype\", columns=\"x14\", aggfunc=\"size\")sns.heatmap(df_count, cmap=sns.light_palette(\"gray\", as_cmap=True), annot=True, fmt=\"0\")plt.show() 123456789Gaussian_df_X = df.iloc[:, :10]Bern_df_X = df.iloc[:, 10:-1]df_Y = df.iloc[:, -1]GNB = GaussianNB()fitted_GNB = GNB.fit(Gaussian_df_X, df_Y)theta = fitted_GNB.theta_sigma = fitted_GNB.sigma_ ê°€ëŠ¥ë„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ì˜€ë‹¤. ì•„ë˜ í•¨ìˆ˜ëŠ” ë°˜ë³µë¬¸ì„ í†µí•´ ì‹¤í–‰í•˜ëŠ” ë°©ì‹ì¸ë° ë°ì´í„° ìˆ˜ê°€ ë§ë‹¤ë©´ ë„ˆë¬´ ë¹„íš¨ìœ¨ì ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, ë°ì´í„°(ê´€ì¸¡ì¹˜)ì˜ ìˆ˜ê°€ ì ì€ ê²½ìš°ì—ë§Œ ì´ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤. ê·¸ë˜ì„œ ë˜ ì§ì ‘ì ìœ¼ë¡œ forë¬¸ì„ ëŒë¦¬ì§€ ì•Šê³  ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°©ì‹ì˜ í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ êµ¬í˜„í•˜ì˜€ë‹¤. forë¬¸ìœ¼ë¡œ ë°˜ë³µë¬¸ì„ ì‘ì„±í•œê²ƒê³¼ ë¹„êµí–ˆì„ ë•ŒëŠ” ì§ê´€ì ìœ¼ë¡œ ì–´ë–»ê²Œ ê°€ëŠ¥ë„ë¥¼ êµ¬í•˜ëŠ” ì§€ ì•Œ ìˆ˜ ìˆë‹¤. 12345678def Gaussian_likelihood_cal(predict_data, theta, sigma, class_count, feature_count): likelihood = [] for c in np.arange(class_count): prod = 1 for f in np.arange(feature_count): prod = prod * sp.stats.norm(theta[c][f], np.sqrt(sigma[c][f])).pdf(predict_data[f]) likelihood.append(prod) return likelihood 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172def Gaussian_likelihood_cal(predict_data, theta, sigma): likelihood = [(sp.stats.norm(theta[0][0], np.sqrt(sigma[0][0])).pdf(predict_data[0]) *\\ sp.stats.norm(theta[0][1], np.sqrt(sigma[0][1])).pdf(predict_data[1]) *\\ sp.stats.norm(theta[0][2], np.sqrt(sigma[0][2])).pdf(predict_data[2]) *\\ sp.stats.norm(theta[0][3], np.sqrt(sigma[0][3])).pdf(predict_data[3]) *\\ sp.stats.norm(theta[0][4], np.sqrt(sigma[0][4])).pdf(predict_data[4]) *\\ sp.stats.norm(theta[0][5], np.sqrt(sigma[0][5])).pdf(predict_data[5]) *\\ sp.stats.norm(theta[0][6], np.sqrt(sigma[0][6])).pdf(predict_data[6]) *\\ sp.stats.norm(theta[0][7], np.sqrt(sigma[0][7])).pdf(predict_data[7]) *\\ sp.stats.norm(theta[0][8], np.sqrt(sigma[0][8])).pdf(predict_data[8]) *\\ sp.stats.norm(theta[0][9], np.sqrt(sigma[0][9])).pdf(predict_data[9])),\\ (sp.stats.norm(theta[1][0], np.sqrt(sigma[1][0])).pdf(predict_data[0]) *\\ sp.stats.norm(theta[1][1], np.sqrt(sigma[1][1])).pdf(predict_data[1]) *\\ sp.stats.norm(theta[1][2], np.sqrt(sigma[1][2])).pdf(predict_data[2]) *\\ sp.stats.norm(theta[1][3], np.sqrt(sigma[1][3])).pdf(predict_data[3]) *\\ sp.stats.norm(theta[1][4], np.sqrt(sigma[1][4])).pdf(predict_data[4]) *\\ sp.stats.norm(theta[1][5], np.sqrt(sigma[1][5])).pdf(predict_data[5]) *\\ sp.stats.norm(theta[1][6], np.sqrt(sigma[1][6])).pdf(predict_data[6]) *\\ sp.stats.norm(theta[1][7], np.sqrt(sigma[1][7])).pdf(predict_data[7]) *\\ sp.stats.norm(theta[1][8], np.sqrt(sigma[1][8])).pdf(predict_data[8]) *\\ sp.stats.norm(theta[1][9], np.sqrt(sigma[1][9])).pdf(predict_data[9])),\\ (sp.stats.norm(theta[2][0], np.sqrt(sigma[2][0])).pdf(predict_data[0]) *\\ sp.stats.norm(theta[2][1], np.sqrt(sigma[2][1])).pdf(predict_data[1]) *\\ sp.stats.norm(theta[2][2], np.sqrt(sigma[2][2])).pdf(predict_data[2]) *\\ sp.stats.norm(theta[2][3], np.sqrt(sigma[2][3])).pdf(predict_data[3]) *\\ sp.stats.norm(theta[2][4], np.sqrt(sigma[2][4])).pdf(predict_data[4]) *\\ sp.stats.norm(theta[2][5], np.sqrt(sigma[2][5])).pdf(predict_data[5]) *\\ sp.stats.norm(theta[2][6], np.sqrt(sigma[2][6])).pdf(predict_data[6]) *\\ sp.stats.norm(theta[2][7], np.sqrt(sigma[2][7])).pdf(predict_data[7]) *\\ sp.stats.norm(theta[2][8], np.sqrt(sigma[2][8])).pdf(predict_data[8]) *\\ sp.stats.norm(theta[2][9], np.sqrt(sigma[2][9])).pdf(predict_data[9])),\\ (sp.stats.norm(theta[3][0], np.sqrt(sigma[3][0])).pdf(predict_data[0]) *\\ sp.stats.norm(theta[3][1], np.sqrt(sigma[3][1])).pdf(predict_data[1]) *\\ sp.stats.norm(theta[3][2], np.sqrt(sigma[3][2])).pdf(predict_data[2]) *\\ sp.stats.norm(theta[3][3], np.sqrt(sigma[3][3])).pdf(predict_data[3]) *\\ sp.stats.norm(theta[3][4], np.sqrt(sigma[3][4])).pdf(predict_data[4]) *\\ sp.stats.norm(theta[3][5], np.sqrt(sigma[3][5])).pdf(predict_data[5]) *\\ sp.stats.norm(theta[3][6], np.sqrt(sigma[3][6])).pdf(predict_data[6]) *\\ sp.stats.norm(theta[3][7], np.sqrt(sigma[3][7])).pdf(predict_data[7]) *\\ sp.stats.norm(theta[3][8], np.sqrt(sigma[3][8])).pdf(predict_data[8]) *\\ sp.stats.norm(theta[3][9], np.sqrt(sigma[3][9])).pdf(predict_data[9])),\\ (sp.stats.norm(theta[4][0], np.sqrt(sigma[4][0])).pdf(predict_data[0]) *\\ sp.stats.norm(theta[4][1], np.sqrt(sigma[4][1])).pdf(predict_data[1]) *\\ sp.stats.norm(theta[4][2], np.sqrt(sigma[4][2])).pdf(predict_data[2]) *\\ sp.stats.norm(theta[4][3], np.sqrt(sigma[4][3])).pdf(predict_data[3]) *\\ sp.stats.norm(theta[4][4], np.sqrt(sigma[4][4])).pdf(predict_data[4]) *\\ sp.stats.norm(theta[4][5], np.sqrt(sigma[4][5])).pdf(predict_data[5]) *\\ sp.stats.norm(theta[4][6], np.sqrt(sigma[4][6])).pdf(predict_data[6]) *\\ sp.stats.norm(theta[4][7], np.sqrt(sigma[4][7])).pdf(predict_data[7]) *\\ sp.stats.norm(theta[4][8], np.sqrt(sigma[4][8])).pdf(predict_data[8]) *\\ sp.stats.norm(theta[4][9], np.sqrt(sigma[4][9])).pdf(predict_data[9])),\\ (sp.stats.norm(theta[5][0], np.sqrt(sigma[5][0])).pdf(predict_data[0]) *\\ sp.stats.norm(theta[5][1], np.sqrt(sigma[5][1])).pdf(predict_data[1]) *\\ sp.stats.norm(theta[5][2], np.sqrt(sigma[5][2])).pdf(predict_data[2]) *\\ sp.stats.norm(theta[5][3], np.sqrt(sigma[5][3])).pdf(predict_data[3]) *\\ sp.stats.norm(theta[5][4], np.sqrt(sigma[5][4])).pdf(predict_data[4]) *\\ sp.stats.norm(theta[5][5], np.sqrt(sigma[5][5])).pdf(predict_data[5]) *\\ sp.stats.norm(theta[5][6], np.sqrt(sigma[5][6])).pdf(predict_data[6]) *\\ sp.stats.norm(theta[5][7], np.sqrt(sigma[5][7])).pdf(predict_data[7]) *\\ sp.stats.norm(theta[5][8], np.sqrt(sigma[5][8])).pdf(predict_data[8]) *\\ sp.stats.norm(theta[5][9], np.sqrt(sigma[5][9])).pdf(predict_data[9])),\\ (sp.stats.norm(theta[6][0], np.sqrt(sigma[6][0])).pdf(predict_data[0]) *\\ sp.stats.norm(theta[6][1], np.sqrt(sigma[6][1])).pdf(predict_data[1]) *\\ sp.stats.norm(theta[6][2], np.sqrt(sigma[6][2])).pdf(predict_data[2]) *\\ sp.stats.norm(theta[6][3], np.sqrt(sigma[6][3])).pdf(predict_data[3]) *\\ sp.stats.norm(theta[6][4], np.sqrt(sigma[6][4])).pdf(predict_data[4]) *\\ sp.stats.norm(theta[6][5], np.sqrt(sigma[6][5])).pdf(predict_data[5]) *\\ sp.stats.norm(theta[6][6], np.sqrt(sigma[6][6])).pdf(predict_data[6]) *\\ sp.stats.norm(theta[6][7], np.sqrt(sigma[6][7])).pdf(predict_data[7]) *\\ sp.stats.norm(theta[6][8], np.sqrt(sigma[6][8])).pdf(predict_data[8]) *\\ sp.stats.norm(theta[6][9], np.sqrt(sigma[6][9])).pdf(predict_data[9]))] return likelihood ìœ„ì˜ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 10ê°œì˜ ì‹¤ìˆ˜ ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ëª¨ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ëŠ¥ë„ë¥¼ êµ¬í•˜ëŠ” ë°˜ë³µë¬¸ì„ ì‘ì„±í•˜ì˜€ë‹¤. 12345678910%%timetotal_num = np.array(Gaussian_df_X).shape[0]gaussian_likelihood_matrix = []percentage = 0for num, predict_data in enumerate(np.array(Gaussian_df_X)): if (percentage != int(num / total_num * 100)) and (int(num / total_num * 100) in list(np.arange(10,101,10))): percentage = int(num / total_num * 100) print(\"ì™„ì„±ë„ &#123;&#125; %\".format(percentage)) likelihood = Gaussian_likelihood_cal(predict_data, theta, sigma) gaussian_likelihood_matrix.append(likelihood) ìœ„ì—ì„œ ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ì˜ ê°€ëŠ¥ë„ë¥¼ êµ¬í–ˆìœ¼ë¯€ë¡œ ì´ì   ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ì˜ ê°€ëŠ¥ë„ë¥¼ êµ¬í•  ê²ƒì´ë‹¤. 12BNB = BernoulliNB()fitted_BNB = BNB.fit(Bern_df_X, df_Y) 12theta = np.exp(fitted_BNB.feature_log_prob_)theta.shape ìƒëŒ€ì ìœ¼ë¡œ ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ì˜ ê°€ëŠ¥ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒë³´ë‹¨ ë‹¨ìˆœ ì—°ì‚°ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆì–´ ì†ë„ê°€ í›¨ì”¬ ë¹ ë¥´ë‹¤. 1234%%timebern_likelihood_matrix = []for data in np.array(Bern_df_X): bern_likelihood_matrix.append(list(((theta ** data) * (1 - theta) ** (1 - data)).prod(axis=1))) ê²°ê³¼12CPU times: user 49.2 s, sys: 1.28 s, total: 50.5 sWall time: 50.4 s 1234likelihood = np.array(gaussian_likelihood_matrix) * np.array(bern_likelihood_matrix)posterior = likelihood * np.exp(BNB.class_log_prior_)prob = posterior / np.repeat(posterior.sum(axis=1)[:, np.newaxis], 7, axis=1)result = np.argmax(prob, axis=1) 12result_bern = fitted_BNB.predict(Bern_df_X)result_gaussian = fitted_GNB.predict(Gaussian_df_X) ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ì˜ ì„±ëŠ¥1print(classification_report(result_gaussian, df_Y)) ê²°ê³¼12345678910111213 precision recall f1-score support 1 0.67 0.63 0.65 225973 2 0.66 0.73 0.69 255934 3 0.65 0.50 0.56 46785 4 0.47 0.41 0.44 3099 5 0.22 0.18 0.20 11425 6 0.31 0.33 0.32 16424 7 0.28 0.27 0.28 21372 accuracy 0.63 581012 macro avg 0.47 0.44 0.45 581012weighted avg 0.63 0.63 0.63 581012 ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ì˜ ì„±ëŠ¥1print(classification_report(result_bern, df_Y)) ê²°ê³¼12345678910111213 precision recall f1-score support 1 0.67 0.63 0.65 225973 2 0.66 0.73 0.69 255934 3 0.65 0.50 0.56 46785 4 0.47 0.41 0.44 3099 5 0.22 0.18 0.20 11425 6 0.31 0.33 0.32 16424 7 0.28 0.27 0.28 21372 accuracy 0.63 581012 macro avg 0.47 0.44 0.45 581012weighted avg 0.63 0.63 0.63 581012 ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ê³¼ ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ì˜ ê°€ëŠ¥ë„ë¥¼ ê³±í•´ í™•ë¥ ì„ ê³„ì‚°í•œ ëª¨í˜•ì˜ ì„±ëŠ¥1print(classification_report(result, df_Y)) ê²°ê³¼12345678910111213 precision recall f1-score support 1 0.03 0.25 0.06 26481 2 0.93 0.48 0.63 554531 3 0.00 0.00 0.00 0 4 0.00 0.00 0.00 0 5 0.00 0.00 0.00 0 6 0.00 0.00 0.00 0 7 0.00 0.00 0.00 0 accuracy 0.47 581012 macro avg 0.14 0.10 0.10 581012weighted avg 0.89 0.47 0.60 581012 ìœ„ì˜ ì„±ëŠ¥ ë³´ë©´ 3ê°€ì§€ ëª¨í˜• ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ì ì ˆí•œ í”¼ì²˜ì˜ ì„ íƒì´ ì´ë£¨ì–´ì§€ì§€ ì•Šì€ ëª¨í˜•ì´ê¸° ë•Œë¬¸ì¼ ê²ƒì´ë©°, ë˜í•œ ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ í´ë˜ìŠ¤ê°„ì˜ ë¹„ìœ¨ì°¨ì´ê°€ ê·¹ì‹¬í•˜ê²Œ ì°¨ì´ê°€ ë‚˜ëŠ”ë°, íŠ¹íˆ 1,2 í´ë˜ìŠ¤ê°€ ëŒ€ë‹¤ìˆ˜ë¥¼ ì´ë£¨ê³  ìˆê¸° ë•Œë¬¸ì— 1, 2í´ë˜ìŠ¤ì— ëŒ€í•œ í•™ìŠµì´ ë§ì´ ëœ ê²°ê³¼ë¼ê³  í•´ì„ í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ì´ëŠ” ë§ˆì§€ë§‰ ë‘ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨í˜•ì˜ ì„±ëŠ¥ì„ ë³´ì•„ë„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ë§ˆì§€ë§‰ ëª¨í˜•ì˜ ì„±ëŠ¥ì€ ë‹¤ë¥¸ í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í•œ ë°ì´í„°ëŠ” ì¡´ì¬í•˜ì§€ ì•Šê³  ì˜¤ë¡œì§€ 1ê³¼ 2ë¡œ ì˜ˆì¸¡ì„ í–ˆë‹¤.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"PCAë¥¼ ì´í•´í•˜ê¸° ìœ„í•œ ê¸°ë³¸ì  ì„ í˜•ëŒ€ìˆ˜","slug":"machine_learning_06","date":"2020-04-03T06:40:52.000Z","updated":"2020-04-13T08:45:00.404Z","comments":true,"path":"2020/04/03/machine_learning_06/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/03/machine_learning_06/","excerpt":"","text":"ì°¨ì›ì˜ ì €ì£¼ ë¨¼ì € PCAë¥¼ í•˜ëŠ” ì´ìœ ì— ëŒ€í•´ ì„¤ëª…í•´ ë³¼ ê²ƒì´ë‹¤. ì „ì— ì–¸ê¸‰í–ˆë˜ ë³€ìˆ˜(ë˜ëŠ” í”¼ì²˜)ë“¤ì´ ë§ì•„ì§ˆìˆ˜ë¡ ë³€ìˆ˜ë“¤ì´ ìˆëŠ” ê³µê°„ì˜ ì°¨ì›ìˆ˜ ë˜í•œ ì ì°¨ì ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ê²Œ ëœë‹¤. ì°¨ì›ì˜ ì €ì£¼ëŠ”ì°¨ì›ì´ ëŠ˜ì–´ë‚¨ì— ë”°ë¼ì„œ ê°™ì€ ì˜ì—­ì˜ ìë£Œë¥¼ ê°–ê³  ìˆìŒì—ë„ ì „ì²´ ì˜ì—­ ëŒ€ë¹„ ëª¨ë¸ì„ í†µí•´ ë³€ìˆ˜ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì˜ íŒ¨í„´ì€ ì¤„ì–´ë“¤ê²Œ ë˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë‹¤ë¥¸ ê´€ì ì—ì„œ í•œë²ˆ ì‚´í´ë³´ë©´, ê´€ì¸¡ì¹˜ì˜ ìˆ˜ê°€ í•œì •ë˜ì–´ ìˆë‹¤ê³  í•  ê²½ìš°ì— ë¨¼ì €, 1ì°¨ì› ìƒì˜ ê³µê°„(ë³€ìˆ˜ê°€ 1ê°œì¸ ê²½ìš°)ì—ì„œëŠ” 10ê°œì˜ ë°ì´í„°ë§Œ ìˆì–´ë„ í•´ë‹¹ ì°¨ì›ì˜ ì ˆë°˜ì„ ì„¤ëª…í•  ìˆ˜ ìˆê¸°ì— ê³µê°„ì„ ì„¤ëª…í•¨ì— ìˆì–´ì„œ ë¶€ì¡±í•¨ì´ ì—†ë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤. í—ˆë‚˜ 2ì°¨ì›ìƒì˜ ê³µê°„ìœ¼ë¡œ ì‚´í´ë³´ì•˜ì„ ê²½ìš° 1ì°¨ì›ì˜ ê²½ìš°ë³´ë‹¤ ê´€ì¸¡ì¹˜ ê°„ì˜ ê°„ê²©ì´ ë©€ê²Œ ìˆì–´ ì‚¬ì´ì— ë¹ˆ ê³µê°„ì´ ë§ì´ ë‚˜íƒ€ë‚¨ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ë¹ˆ ê³µê°„ì´ ë‚˜íƒ€ë‚¨ì€ ì‚¬ì‹¤ìƒ ìš°ë¦¬ê°€ ëª¨ë¸ì„ í†µí•´ ì˜ˆì¸¡ì€ ê°€ëŠ¥í•˜ì§€ë§Œ ëª¨í˜•ì— ë”°ë¼ ë³€ë™ì´ ì‹¬í•´ì§€ëŠ” ì–´ë””ê¹Œì§€ë‚˜ ì˜ˆì¸¡í•˜ëŠ” ê°’ì„ ì˜ë¯¸í•œë‹¤. 3ì°¨ì› ìƒì—ì„œëŠ” 2ì°¨ì› ê³µê°„ìƒì—ì„œ ë³´ë‹¤ í›¨ì”¬ ë” ê´€ì¸¡ì¹˜ê°€ ë–¨ì–´ì ¸ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ëª¨ë¸ì˜ ë³€ë™ì„± ì¦‰, ëª¨ë¸ì˜ ë¶„ì‚°ì´ ë” ì»¤ì§€ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. ê°„ë‹¨íˆ ë§í•˜ìë©´, ì ê³¼ ì  ì‚¬ì´ì˜ ê³µê°„ì´ ì°¨ì›ì´ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ë©€ì–´ì§€ëŠ”ë°, ë¹ˆ ê³µê°„ì— ìœ„ì¹˜í•œ ì˜ˆì¸¡ê°’ì„ ì±„ì›Œë„£ê¸° ìœ„í•´ì„œëŠ” ì„ì˜ë¡œ ì±„ì›Œë„£ê¸° ë•Œë¬¸ì— ëª¨ë¸ì´ ì–´ë ¤ì›Œì§„ë‹¤ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë˜ì™€ ê°™ì´ KNN ì•Œê³ ë¦¬ì¦˜(ê°€ì¥ ê°€ê¹Œì´ ìˆëŠ” ë³€ìˆ˜ê°€ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ê²°ê³¼ë¥¼ ê°–ëŠ” ì•Œê³ ë¦¬ì¦˜)ì„ í†µí•œ ë¹„ì§€ë„í•™ìŠµì„ ì§„í–‰í•œë‹¤ê³  ê°€ì •í•´ë³´ì. x1 ë³€ìˆ˜ì˜ ê³µê°„ë§Œì„ ìƒê°í•˜ì—¬ 1ì°¨ì›ì ìœ¼ë¡œ ìƒê°í•˜ë©´, ì•„ë˜ ê·¸ë¦¼ì—ì„œ ê²€ì •ìƒ‰ ì ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ì ì€ ë…¸ë€ìƒ‰ í…Œë‘ë¦¬ì•ˆì— ìˆëŠ” 2ê°œì˜ ì ë“¤ ì¤‘ í•˜ë‚˜ì¼ ê²ƒì´ë‹¤. í—ˆë‚˜ x2 ë³€ìˆ˜ì˜ ê³µê°„ë„ ê°™ì´ ê³ ë ¤í•˜ì—¬ 2ì°¨ì›ì ìœ¼ë¡œ ì‚´í´ë³´ë©´, íŒŒë€ìƒ‰ ì› ì•ˆì— ë†“ì—¬ìˆëŠ” 1ê°œì˜ \u001dì ì´ ê°€ì¥ ê°€ê¹Œìš´ ì ì´ ë  ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ì£¼ëª©í•´ì•¼ í•  ì ì€ ë™ì¼í•œ ë°ì´í„°ë¥¼ í†µí•´ì„œ 2ì°¨ì›ìœ¼ë¡œ ì‚´í´ë³´ë©´ ìµœë‹¨ê±°ë¦¬ì˜ ê¸¸ì´ê°€ ëŠ˜ì–´ë‚¬ë‹¤ëŠ” ì ì´ë‹¤. KNN ì•Œê³ ë¦¬ì¦˜ì˜ ê²°ê³¼ë¡œ 1ì°¨ì›ìœ¼ë¡œ ì‚´í´ë³¸ ê²ƒ ë³´ë‹¤ 2ì°¨ì›ìœ¼ë¡œ ì‚´í´ë³´ëŠ” ê²ƒì´ ìµœë‹¨ê±°ë¦¬ì˜ ê¸¸ì´ê°€ ë” ê¸¸ê¸° ë•Œë¬¸ì— ê°’ì´ ì¢€ ë” ì´ì§ˆì ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìœ¼ë©°, ê²°ê³¼ì ìœ¼ë¡œ ê³¼ì—° 2ì°¨ì›ì ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê°’ì„ KNNì•Œê³ ë¦¬ì¦˜ì˜ ê²°ê³¼ê°’ìœ¼ë¡œ ì‚¬ìš©í•´ë„ ë¬¸ì œê°€ ì—†ì„ì§€ì— ì˜ë¬¸ì ì´ ìƒê¸°ê²Œ ëœë‹¤. x2ê°€ Yì™€ ê´€ë ¨ëœ ë³€ìˆ˜ë¼ë©´ í¬ê²Œ ë¬¸ì œê°€ ì—†ê² ì§€ë§Œ, x2ê°€ ì „í˜€ ê´€ë ¨ì—†ëŠ” ë³€ìˆ˜ë¼ë©´ êµ‰ì¥íˆ ëª¨ë¸ì´ ë‚­ë¹„ê°€ë˜ë©°, ì¢‹ì§€ ì•Šì€ ëª¨ë¸ì´ ë  ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ ê²ƒì²˜ëŸ¼ ì°¨ì›ì´ ì¦ê°€í•˜ê²Œ ë˜ë©´ ì¢‹ì§€ ì•Šì€ ì¸¡ë©´ì´ ìˆê³ , ì´ê²ƒì„ ì°¨ì›ì˜ ì €ì£¼ë¼ê³  ë¶€ë¥´ë©° ì°¨ì›ì„ ë  ìˆ˜ ìˆëŠ” í•œ ì¶•ì†Œí•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ ê²ƒë“¤ì„ ì˜ˆë¡œ ë“¤ ìˆ˜ ìˆë‹¤. ë¨¼ì €, ìƒê´€ê³„ìˆ˜ê°€ ë†’ì€ ë³€\u001d\u001dìˆ˜ë“¤ì´ë¼ë©´ ë™ì¼í•œ ì›€ì§ì„ì˜ ë°©í–¥ì„ ê°–ê¸° ë•Œë¬¸ì— ê·¸ë“¤ ì¤‘ í•˜ë‚˜ì˜ ë³€ìˆ˜ë§Œì„ ì„ íƒí•´ë„ í° ë¬¸ì œê°€ ì—†ë‹¤ê³  ë³´ëŠ” ì¸¡ë©´ì´ë‹¤. ì´ ë°©ë²•ì—ì„œì˜ ë¬¸ì œì ì€ ì˜ˆë¥¼ ë“¤ì–´ ë‘ ë³€ìˆ˜ì˜ ìƒê´€ê³„ìˆ˜ê°€ 0.8ì´ë¼ë©´, ë‚˜ë¨¸ì§€ 0.2ì— í•´ë‹¹í•˜ëŠ” ì •ë³´ëŠ” ë²„ë ¤ì§€ê²Œ ë˜ëŠ” ì ì´ë‹¤. ì´ëŸ° ë¶€ë¶„ì„ ë³´ì™„í•œ ê²ƒì´ PCAì´ë‹¤. PCAëŠ” ì°¨ì›ì„ ì¤„ì´ë©´ì„œë„ ì •ë³´ì˜ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤. ê³µë¶„ì‚° í–‰ë ¬ì˜ ì´í•´ ê³µë¶„ì‚° í–‰ë ¬ì€ ì‰½ê²Œ ë§í•´ í–‰ë ¬ì˜ ë‚´ì ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ëŒ€ê° ìš”ì†Œë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¹„ëŒ€ê°ìš”ì†Œë“¤ì€ ë‘ ë³€ìˆ˜ê°„ì˜ ì›€ì§ì„ì˜ ë°©í–¥ì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ìƒê´€ê´€ê³„ë¥¼ ë¯¸ë¦¬ ì•Œê³  ìˆë‹¤ë©´ ë‘ ë³€ìˆ˜ê°„ì˜ ê³µë¶„ì‚°ì˜ ë¶€í˜¸ë„ ì•Œ ìˆ˜ ìˆë‹¤. ê³µë¶„ì‚° í–‰ë ¬ì˜ í™œìš©ì€ ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ ê²ƒ ì²˜ëŸ¼ ë‘ ë³€ìˆ˜ê°„ì˜ ìƒê´€ê³„ìˆ˜ì˜ ë¶€í˜¸ë¥¼ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒ ë§ê³ ë„ ì¡´ì¬í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ íŠ¹ì • ì ë“¤ê³¼ì˜ ë‚´ì ì—°ì‚°ì„ ì‹œí–‰í•˜ë©´, ì ì˜ ìœ„ì¹˜ë¥¼ ì´ë™ì‹œí‚¤ëŠ”ë° ì´ ë•Œ ì´ë™ëœ ì ë“¤ì˜ ë¶„í¬ê°€ ê³µë¶„ì‚° êµ¬ì¡°ì™€ ë¹„ìŠ·í•œ í˜•íƒœë¥¼ ê°€ì§€ê²Œ ëœë‹¤. ì¦‰ ê³µë¶„ì‚° í–‰ë ¬ê³¼ ì (ë²¡í„° ë˜ëŠ” í–‰ë ¬)ë“¤ì˜ ì—°ì‚°ì€ ê³µë¶„ì‚° êµ¬ì¡°ë¡œ ì ë“¤ì„ ë¶„í¬ì‹œí‚¤ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤. ê³µë¶„ì‚° í–‰ë ¬ì´ ëŒ€ì¹­í–‰ë ¬ì´ì§€ë§Œ, positive definiteê°€ ì•„ë‹Œ í–‰ë ¬ì¸ ê²½ìš°ë¥¼ ë‹¤ìŒ ê·¸ë¦¼ì—ì„œ ë³´ì—¬ì£¼ê³  ìˆë‹¤. ì•„ë˜ ìˆ˜ì‹ì´ ì„±ë¦½í•œë‹¤ë©´ í–‰ë ¬ Aê°€ positive definiteí•˜ë‹¤ê³  í•œë‹¤. x^{T} A x > 0ì°¸ê³  - í–‰ë ¬ì˜ ì„±ì§ˆ ì•„ë˜ì˜ ê³µë¶„ì‚° í–‰ë ¬ì€ í–‰ë ¬ì‹(determinant)ê°€ 0ì¸ ê²½ìš°ì´ë‹¤. ì´ë ‡ê²Œ í–‰ë ¬ì‹ì´ 0ì¸ ê³µë¶„ì‚°í–‰ë ¬ê³¼ì˜ ë‚´ì ì€ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ê³¼ ê°™ì´ ì›ì ì„ ê¸°ì¤€ìœ¼ë¡œ ì¼ì§ì„ ì˜ í˜•íƒœë¥¼ ì´ë£¨ê²Œ ëœë‹¤. Principal Componentsì˜ ì´í•´ ì•„ë˜ ê·¸ë¦¼ì„ ì‚´í´ë³´ë©´ x1ê³¼ x2ëŠ” ìŒì˜ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ê°–ëŠ”ë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. x1ì´ ì¦ê°€í•˜ë©´ x2ê°€ ê°ì†Œí•˜ëŠ” ì›€ì§ì„ ê°–ëŠ”ë‹¤. ê²°ë¡ ì ìœ¼ë¡œëŠ” ì •ë³´ê°€ ì¤‘ë³µë˜ì–´ìˆë‹¤. ì´ë ‡ê²Œ ì¤‘ë³µë˜ì–´ìˆëŠ” ì •ë³´ë¥¼ ìš”ì•½í•´ì„œ ê°–ê³ ìˆì„ìˆ˜ ìˆì„ ìˆœ ì—†ì„ê¹Œì— ëŒ€í•œ ë°©ë²•ì„ ê³ ì•ˆí•œ ê²ƒì´ ë°”ë¡œ PCAì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™\u001dì´ ë‘ ë³€ìˆ˜ê°„ì˜ ê°€ì¥ í° ë¶„ì‚°ì„ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ì— ëŒ€í•´ ê° ë°ì´í„°ë¥¼ projectionì„ ì·¨í•œë‹¤ë©´ í•´ë‹¹ projection ëœ ë°ì´í„°ë“¤ì˜ ë²¡í„°ë§Œì„ ê°€ì§€ê³ ë„ ê¸°ì¡´ì˜ ë°ì´í„°ì— ëŒ€í•œ ë¶„ì‚°ì„ í‘œí˜„í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ì°¨ì›ì„ ì¤„ì´ë©´ì„œ ì •ë³´ì˜ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ë°©ë²•ì€ ìë£Œì˜ ë³€ë™ì„ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ì–´ë–¤ ì¶•ì„ ì°¾ëŠ” ê²ƒì´ë‹¤. Principal Componentsë¥¼ ì°¾ì•„ë‚´ëŠ” ë°©ë²•ì„ ê°„ë‹¨íˆ ë§í•˜ìë©´ ìë£Œì˜ ë¶„ì‚°ì„ ê°€ì¥ ë§ì´ ì„¤ëª…í•˜ëŠ” ì¶•ì¸ ì¥ì¶•ê³¼ ìë£Œì˜ ë¶„ì‚°ì„ ê°€ì¥ ì ê²Œ ì„¤ëª…í•˜ëŠ” ë‹¨ì¶•ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤. PCA ìˆ˜í•™ì  ê°œë…ì´í•´ - í–‰ë ¬ì—°ì‚°, í–‰ë ¬ì‹, íŠ¹ì„±ë°©ì •ì‹ í–‰ë ¬ì‹(determinant)ì€ ì‰½ê²Œ ë§í•´ í•´ë‹¹ í–‰ë ¬ì´ ê°–ëŠ” Volume(2ì°¨ì›ì¸ ê²½ìš°ëŠ” ë©´ì )ì„ ì˜ë¯¸í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ 2ì°¨ì› ê³µê°„ìƒì—ì„œ ë©´ì ì´ 1ì„ ê°–ëŠ” ì¢Œí‘œë“¤(íŒŒë€ìƒ‰)ì„ Dë¼ëŠ” í–‰ë ¬ë¡œ ëª…ëª…í–ˆì„ ê²½ìš°, Aí–‰ë ¬ë¡œ ì„ í˜•ë³€í™˜ì‹œì¼œì£¼ë©´ ì•„ë˜ ê·¸ë¦¼ì˜ ë¹¨ê°„ìƒ‰ ì§ì‚¬ê°í˜•ìœ¼ë¡œ ë³€í™˜ë˜ë©°, ë¶€í”¼ëŠ” ê° í–‰ë ¬ì‹ì˜ ê³± í˜•íƒœë¡œ ê³„ì‚° ë˜ì–´ 5ë¥¼ ê°–ëŠ”ë‹¤. ë§Œì•½ í–‰ë ¬ì‹ì´ 0ì„ ê°–ëŠ”ë‹¤ë©´ í–‰ë ¬ì˜ ê³±ì€ 1ì§ì„ ì„ ì´ë£° ê²ƒì´ê³ , ì„ ë¶„ì˜ í˜•íƒœë¡œ í‘œí˜„ë  ê²ƒì´ë‹¤. ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„° ì„ì˜ì˜ ì •ë°©í–‰ë ¬ Aë¡œ ì„ í˜•ë³€í™˜í•˜ë”ë¼ë„ ë°©í–¥ì€ ìœ ì§€ë˜ë©´ í•´ë‹¹ ë²¡í„°ë¥¼ ê³ ìœ ë²¡í„°ë¼ê³  í•˜ë©°, í¬ê¸°(ëŠ˜ì–´ë‚œ ì •ë„)ë¥¼ ê³ ìœ ê°’ì´ë¼ê³  í•œë‹¤. ê¸°í•˜í•™ì ìœ¼ë¡œ ì„¤ëª…í•´ë³´ìë©´, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ê³ ìœ ë²¡í„°ëŠ” ë¹¨ê°„ìƒ‰ ì„ ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì„ í˜•ë³€í™˜ì„ í•´ë„ ë²¡í„°ì˜ ë°©í–¥ì´ ë°”ë€Œì§€ ì•ŠëŠ” íŒŒë€ìƒ‰, ë¶„í™ìƒ‰ ë²¡í„°ë“¤ì´ ê³ ìœ ë²¡í„°ë¥¼ ì˜ë¯¸í•œë‹¤. ê·¸ë¦¬ê³  ê³ ìœ ê°’ì€ ìŠ¤ì¼€ì¼(í¬ê¸°)ë¥¼ ì˜ë¯¸í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì€ ì‹ìœ¼ë¡œ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ë¨¼ì € ê³ ìœ ê°’ì„ êµ¬í•˜ëŠ” ë°©ë²•ì€ $det(A - \\lambda I)=0$ì„ ë§Œì¡±í•˜ëŠ” ê³ ìœ ê°’ì„ ì°¾ìœ¼ë©´ ë˜ëŠ”ë°, ì´ìœ ëŠ” ìš°ë¦¬ê°€ êµ¬í•˜ê³  ì‹¶ì€ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ëŠ” 0ë²¡í„°ë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ ë²¡í„°ë¥¼ êµ¬í•˜ê³  ì‹¶ê¸° ë•Œë¬¸ì— í–‰ë ¬ì‹ì´ 0ì´ì•„ë‹Œ ë‹¤ë¥¸ ê°’ì„ ê°–ëŠ”ë‹¤ë©´ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ê²Œ ë˜ì–´ 0ë²¡í„°ê°€ í•´ë‹¹ ì„ í˜•ì‹œìŠ¤í…œì˜ ìœ ì¼í•œ í•´ê°€ ë˜ì§€ ì•Šë„ë¡í•˜ë ¤ë©´ non-trivial solution(í•´ê°€ ë¬´ìˆ˜íˆ ë§ì•„ì•¼)ì„ ê°–ì–´ì•¼ í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ í•´ë‹¹ í–‰ë ¬ì‹ì´ 0ì˜ ê°’ì„ ê°–ê²Œë˜ì–´ ì—­í–‰ë ¬ì´ ì¡´ì¬í•˜ì§€ ì•Šë„ë¡ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ì„ í†µí•´ ê³ ìœ ê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤. ê³ ìœ ê°’ì€ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ(í° ê²ƒë¶€í„° ì‘ì€ ê²ƒ ìˆœìœ¼ë¡œ)ì ì–´ì£¼ë©´, ê·¸ì—ë”°ë¥¸ ê³ ìœ ë²¡í„°ë„ ë™ì¼í•œ ì¸ë±ìŠ¤ë¥¼ ë§¤ì¹­í•´ì¤€ë‹¤. ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì€ ê³ ìœ ë²¡í„°ëŠ” í•´ë‹¹ ê³ ìœ ë²¡í„°ë¥¼ normalizationì„ í†µí•´ í¬ê¸°ê°€ 1ì¸ ë²¡í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìŠ¤ì¹¼ë¼ë°°ë¥¼ í•´ë„ ë™ì¼í•œ ê³ ìœ ë²¡í„°ë¼ê³  ì—¬ê¸´ë‹¤ëŠ” ì ì´ë‹¤. PCA ìˆ˜í•™ì  ê°œë…ì´í•´ - Singular Value Decomposition(SVD) PCAëŠ” ë‹¤ë¥´ê²Œ í–‰ë ¬ì‹ì„ ê³„ì‚°í•˜ê¸° ë•Œë¬¸ì— ì •ë°©í–‰ë ¬ì¸ ê²½ìš°ì—ë§Œ ê³„ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ì´ëŸ¬í•œ ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•œ Decomposition ë°©ë²•ì´ ë°”ë¡œ SVD(Singular Value Decomposition)ì´ë‹¤. SVDëŠ” ì„ì˜ì˜ í–‰ë ¬ Xì— ê´€í•´ 3ê°œì˜ í–‰ë ¬ë¡œ ë¶„í•´ê°€ ê°€ëŠ¥í•˜ë©°, ê°ê°ì˜ í–‰ë ¬ì˜ í¬ê¸°ëŠ” ì•„ë˜ì™€ ê°™ê³ , Vì™€ UëŠ” columnë“¤ì´ Orthogonal(ì§êµ)í•œ í–‰ë ¬(ì„œë¡œ ì§êµí•˜ëŠ” ì„±ë¶„ë“¤ë¼ë¦¬ì˜ ë‚´ì ì€ 0)ì´ë¯€ë¡œ ë™ì¼í–‰ë ¬ì˜ ë‚´ì ì€ Identity matrixë¥¼ ê°–ëŠ”ë‹¤. Vì™€ Dê°€ ê°ê° ì„ì˜ì˜í–‰ë ¬ Xì— ê³µë¶„ì‚°í–‰ë ¬ì— í•´ë‹¹í•˜ëŠ” ê³ ìœ ë²¡í„° í–‰ë ¬ê³¼ ê³ ìœ ê°’ í–‰ë ¬ì„ ì˜ë¯¸í•œë‹¤. ê³µë¶„ì‚° í–‰ë ¬ì¸ $X^{T}X$ë¥¼ SVD í•œ í›„, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ eigen vectorë“¤ì˜ í–‰ë ¬ë¡œ ê³±í•´ì£¼ë©´ eigen vectorë“¤ì˜ í–‰ë ¬ê³¼ eigen valueë“¤ì˜ ë‚´ì  ê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤. PCA ìˆ˜í–‰ê³¼ì • ë° ìˆ˜í•™ì  ê°œë… ì ìš© ì œì¼ ë¨¼ì €, ê° ë°ì´í„°ì— ëŒ€í•´ standardizationì„ í†µí•´ í‘œì¤€ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ë„ë¡ scalingì„ í•´ì¤€ë‹¤. ì´ëŸ° normalizationì€ ê° featureì— ëŒ€í•œ ê³µê°„ì˜ ë²”ìœ„ê°€ ë‹¤ë¥´ë‹¤ë©´ ë¹„êµí•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë˜í•œ, ì´ ê³¼ì •ì—ì„œ feature ë²¡í„°ì˜ í¬ê¸°ë¥¼ 1ë¡œ ê°–ê²Œë” Unit vectorë¡œ ë§Œë“¤ì–´ ì£¼ëŠ” ê²ƒì´ ì¶”í›„ì— SVDë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì—ì„œ Orthogonalí•œ eigen vectorê°€ Orthonormal vectorê°€ ë˜ì–´ ê³„ì‚°í•¨ì— ìˆì–´ì„œ í¸ë¦¬í•˜ê²Œ ëœë‹¤. ê·¸ í›„ SVDëŠ” ìš°ì„  ìš°ë¦¬ëŠ” í”„ë¡œê·¸ë¨ì„ ì‚¬ìš©í•˜ì—¬ ë°”ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤ëŠ” ê°€ì •ì„ í•´ë³´ì. ì—¬ê¸°ì„œ eigen vectorê°™ì€ ê²½ìš°ì—ëŠ” ê³µë¶„ì‚° êµ¬ì¡°ë‚˜ ê³µë¶„ì‚° í–‰ë ¬ì´ë‚˜ ë™ì¼í•œ ê°’ì„ ê°–ì§€ë§Œ, eigen valueëŠ” ë‹¤ìŒ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ ê´€ì¸¡ì¹˜ì˜ ê°œìˆ˜ì—ì„œ í•˜ë‚˜ë¥¼ ë¹¼ì£¼ì–´ì•¼ í•œë‹¤. PC ScoreëŠ” ìœ„ì—ì„œì™€ ê°™ì´ í° ìˆœì„œëŒ€ë¡œ ì •ë ¬ëœ eigen value í–‰ë ¬ Dì™€ ì•ì— Uí–‰ë ¬ê°’ì˜ ë‚´ì ì´ ê³§ í–‰ë ¬ Xì™€ Vì˜ ë‚´ì ì„ í•˜ì—¬ ìƒìˆ˜ì¸ $\\lambda V$ë¥¼ ë§Œë“œëŠ” ê³¼ì •ê³¼ ë™ì¼í•˜ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ PC Scoreë¥¼ êµ¬í•´ ì¤‘ìš”ë„ë¥¼ ë”°ì ¸ ìƒìœ„ qê°œë§Œì„ í†µí•´ íšŒê·€ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ìˆë‹¤. PCAì˜ ì‹¬í™”ì  ì´í•´ í•œ ë§ˆë””ë¡œ PCAëŠ” ë°ì´í„°ì˜ ë³€ë™ì„ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” í•œ ì¶•ì„ ì°¾ì•„ì„œ ê·¸ ì¶•ì— ëŒ€í•´ ê° ê´€ì¸¡ì¹˜ë“¤ì„ projectioní•œ ì„±ë¶„ì„ ì˜ë¯¸í•œë‹¤. ê·¸ ë‹¤ìŒì—ëŠ” ì´ëŸ¬í•œ í•´ë‹¹ ë³€ë™ì¶•ì— ì§ê°ì´ ë˜ëŠ” ì¶•ì— ê´€ì¸¡ì¹˜ë“¤ì„ projectioní•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•˜ì—¬ ì§„í–‰í•˜ê²Œ ë˜ëŠ”ë° ì§ê°ì´ ë˜ëŠ” ì¶•ì„ ì°¾ëŠ” ì´ìœ ëŠ” ì´ì „ì˜ ì¶•ê³¼ ë™ì¼í•œ ì •ë³´ë¥¼ ê°–ì§€ì•ŠëŠ” ìƒˆë¡œìš´ í”¼ì²˜ë¥¼ ë½‘ê¸° ìœ„í•¨ì´ë¼ê³  í•„ìëŠ” ìƒê°í•œë‹¤. PCAì˜ ë‹¨ì  ì¤‘ í•˜ë‚˜ë¡œëŠ” í•´ë‹¹ ì„±ë¶„ì´ ì˜ë¯¸í•˜ëŠ” ë°”ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì„¤ëª…í•  ìˆ˜ ì—†ë‹¤ëŠ” ì ì´ë‹¤. ë˜í•œ ì„ í˜•íšŒê·€ì— ì‚¬ìš©ë˜ëŠ” ê²ƒê³¼ ê°™ì´ í•´ë‹¹ ë³€ìˆ˜ë“¤ì´ ë¹„ì„ í˜•ê´€ê³„ë¥¼ ê°–ëŠ”ë‹¤ë©´ ì¡ì•„ë‚¼ ìˆ˜ ì—†ë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í•œ ì²«ë²ˆì§¸ ë‹¨ì (ì§ê´€ì ìœ¼ë¡œ PCì„±ë¶„ì„ í†µí•œ í”¼ì²˜ë¥¼ ì„¤ëª…í•˜ê¸° ì–´ë µë‹¤.)ì„ ì¡°ê¸ˆì´ë‚˜ë§ˆ ë³´ì™„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ gridë¥¼ í†µí•´ í•´ë‹¹í•˜ëŠ” ê³³ì˜ ë²”ìœ„ë“¤ì˜ ë°ì´í„°ë¥¼ ì§ì ‘ ë½‘ì•„ ê° ì„±ë¶„ì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ì™€ ê°™ì´ ì´ë¯¸ì§€ ë°ì´í„°ì¸ ê²½ìš°ëŠ” ì¢€ ë” êµ¬ë³„í•˜ê¸° ì‰¬ìš¸ ê²ƒì´ë‹¤. Kernel PCA ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ ë‘ ë²ˆì§¸ ë‹¨ì ì¸ ë¹„ì„ í˜•ê´€ê³„ë¥¼ ê°–ëŠ”ë‹¤ë©´ ì¡ì•„ë‚´ê¸° í˜ë“¤ë‹¤ëŠ” ì ì„ ë³´ì™„í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œì„œ, ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì€ ìƒí™©ì—ì„œ Kernel PCAë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ì•„ë˜ ê´€ì¸¡ì¹˜ë“¤ì˜ ì§‘í•©ì¸ í–‰ë ¬ Xì˜ ê³µë¶„ì‚° êµ¬ì¡°ëŠ” 0ì— ê°€ê¹Œìš´ ê°’ì„ ê°–ì„ ê²ƒì´ë‹¤. ì¦‰, ì„œë¡œ ì„ í˜•ê´€ê³„ê°€ ì•„ë‹Œ ìƒí™©ì´ë‹¤. ê¸°ì¡´ì˜ PCAëŠ” featureë“¤ ì‚¬ì´ì˜ íŒ¨í„´ì´ ì¡´ì¬í•˜ëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ë¹„êµ(ê° featureë“¤ê°„ì˜ ì›€ì§ì´ëŠ” ë°©í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ê³µë¶„ì‚° êµ¬ì¡°ë¥¼ í†µí•´ ê°’ì„ êµ¬í–ˆê¸° ë•Œë¬¸)ë¥¼ í–ˆì§€ë§Œ, ì´ë²ˆì—ëŠ” ê´€ì¸¡ì¹˜ì™€ ê´€ì¸¡ì¹˜ ì‚¬ì´ì˜ íŒ¨í„´ì„ ìˆ˜ì¹˜í™”í•˜ì—¬ PCë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ K(Kernel matrix)ì˜ ì˜ˆì‹œë¥¼ ë³´ë“¯ì´ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì´ ì¡´ì¬í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ Kernel PCAë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì›í˜•ì˜ ë ë¡œ êµ¬ë¶„ì§€ì–´ì ¸ ìˆëŠ” í˜•ìƒì¸ë°ë„ ë¶ˆêµ¬í•˜êµ¬ Kernel PCAë¥¼ ì‚¬ìš©í•˜ë©´ ì˜ êµ¬ë¶„ë˜ëŠ”ì§€ì— ëŒ€í•œ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ë¨¼ì € ê° ê´€ì¸¡ì¹˜ì—ëŒ€í•´ pair-wiseí•˜ê²Œ Kernel matrixë¥¼ êµ¬í•´ ë¹„ìŠ·í•œ ê°’ë“¤ì„ ê°–ëŠ” ê´€ì¸¡ì¹˜ë“¤ë¡œ ì •ë ¬í•˜ì—¬ ë³´ë©´ ë¹„ìŠ·í•œ ê°’ì„ ê°–ëŠ” ê´€ì¸¡ì¹˜ë“¤ ë¼ë¦¬ ëª¨ì—¬ block matrixë¥¼ í˜•ì„±í•˜ëŠ” ëª¨ìŠµì„ ë³¼ ìˆ˜ ìˆë‹¤. Kernel matrixë¥¼ êµ¬í•œ ê°’ìœ¼ë¡œ eigen vectorì˜ ì—­í• ì„ í•˜ëŠ” í–‰ë ¬ Uì™€ ê° eigen valueì˜ $d_{i}$ì˜ ê³±ì¸ PC Scoreë¥¼ í†µí•´ ì–»ì€ ê²°ê³¼ë¥¼ ê·¸ë˜í”„ë¡œ ê·¸ë ¤ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì•„ë˜ì˜ ê²½ìš°ì—ì„œëŠ” ì²«ë²ˆì§¸ ì„±ë¶„ì„ ì„ íƒí•˜ì—¬ ì‚¬ìš©í•˜ë©´ 3ê°€ì§€ êµ°ì§‘ì„ ì˜ íŒë³„í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„","slug":"machine_learning_05","date":"2020-04-03T04:58:10.000Z","updated":"2020-04-03T06:40:57.940Z","comments":true,"path":"2020/04/03/machine_learning_05/","link":"","permalink":"https://heung-bae-lee.github.io/2020/04/03/machine_learning_05/","excerpt":"","text":"ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ìœ„ì˜ ì¶”ì •ì‹ì—ì„œ ê°€ì¥ ì˜¤ë¥¸ìª½ì— ìˆëŠ” ë¡œê·¸ ì˜¤ì¦ˆë¹„(log odds ratio)ë¥¼ ë¨¼ì € ì„¤ëª…í•˜ìë©´, ë¡œê·¸ ì•ˆì— ë“¤ì–´ê°„ ì˜¤ì¦ˆë¹„(odds ratio)ëŠ” ë² ë¥´ëˆ„ì´ ì‹œë„ì—ì„œ 1ì´ ë‚˜ì˜¬ í™•ë¥  $\\theta (x)$ì™€ 0ì´ ë‚˜ì˜¬ í™•ë¥  $1-\\theta (x)$ì˜ ë¹„ìœ¨(ratio)ì„ ì˜ë¯¸í•œë‹¤. odds ratio = \\frac{\\theta (x)}{1 - \\theta (x)}0ë¶€í„° 1ì‚¬ì´ì˜ ê°’ë§Œ ê°€ì§€ëŠ” í™•ë¥ ê°’ì¸ $\\theta (x)$ë¥¼ ìŠ¹ì‚°ë¹„ë¡œ ë³€í™˜í•˜ë©´ 0ë¶€í„° ì–‘ì˜ ë¬´í•œëŒ€ê¹Œì§€ì˜ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ìŠ¹ì‚°ë¹„ë¥¼ ë¡œê·¸ë³€í™˜í•œ ê²ƒì´ ìœ„ì—ì„œ ì–¸ê¸‰í•œ Logit functionì´ë‹¤. y = logit(odds ratio) = log\\( \\frac{theta (x)}{} \\)ì´ë¡œì¨ ë¡œì§€íŠ¸ í•¨ìˆ˜ì˜ ê°’ì€ ë¡œê·¸ë³€í™˜ì— ì˜í•´ ìŒì˜ ë¬´í•œëŒ€ $(- \\infty)$ë¶€í„° ì–‘ì˜ ë¬´í•œëŒ€$(\\infty)$ê¹Œì§€ì˜ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. ë¡œì§€ìŠ¤í‹±í•¨ìˆ˜(Logistic function)ì€ ë¡œì§€íŠ¸ í•¨ìˆ˜ì˜ ì—­í•¨ìˆ˜ì´ë‹¤. ì¦‰ ìŒì˜ ë¬´í•œëŒ€ë¶€í„° ì–‘ì˜ ë¬´í•œëŒ€ ê¹Œì§€ì˜ ê°’ì„ ê°€ì§€ëŠ” ì…ë ¥ë³€ìˆ˜ë¥¼ 0ë¶€í„° 1ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ëŠ” ì¶œë ¥ë³€ìˆ˜ë¡œ ë³€í™˜í•œ ê²ƒì´ë‹¤. logistic(z) = \\theta (z) = \\frac{1}{1 + exp(-z)} ë˜í•œ, ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì—ì„œëŠ” íŒë³„í•¨ìˆ˜ ìˆ˜ì‹ìœ¼ë¡œ ì„ í˜•í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ íŒë³„ ê²½ê³„ë©´ ë˜í•œ ì„ í˜•ì´ ë¨ì„ ìœ ì˜í•´ì•¼í•œë‹¤. z = w^{T} x ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´, Xì˜ ë²”ìœ„ê°€ [$-\\infty$, $+\\infty$]ì¸ ê²½ìš°ì— Yì˜ ë²”ìœ„ë¥¼ [0,1]ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤. ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ì¢Œì¸¡ê·¸ë¦¼ì˜ ì í•©ì‹œí‚¨ íšŒê·€ì„ (íŒŒë€ì„ )ì„ ë³´ë©´ ì˜ˆì¸¡í•  í™•ë¥ ê°’ì´ 500ë¯¸ë§Œì´ë©´ ìŒìˆ˜ë¥¼ ê°–ì„ ìˆ˜ ìˆê²Œ ë˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” í™•ë¥ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“ ë‹¤ëŠ” ê°€ì • ìì²´ì— ìœ„ë°˜í•˜ëŠ” ê²ƒì´ë¯€ë¡œ ì´ì „ì— í•™ìŠµí–ˆì—ˆë˜ íšŒê·€ëª¨í˜•ë“¤ê³¼ëŠ” ë‹¤ë¥¸ ë°©ë²•ì˜ íšŒê·€ëª¨í˜•ì„ ë§Œë“¤ì–´ ì í•©ì‹œì¼œì•¼ í•  ê²ƒì„ì´ ë¶„ëª…í•´ì¡Œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ decision boundary(threshold)ë¥¼ 0.5ë¡œ í•˜ì§€ë§Œ classê°€ imbalanced í•œ ê²½ìš°ëŠ” ì¡°ì ˆí•˜ì—¬ë³´ë©´ì„œ ì í•©ì‹œí‚¬ ìˆ˜ ìˆë‹¤. ìœ„ì˜ MLE ë°©ì‹ì„ í†µí•´ parameterì˜ ê°’ì„ ì—…ë°ì´íŠ¸í•˜ëŠ”ë°, ì•„ë˜ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì¹˜ì  ìµœì í™”ë¥¼ ì§„í–‰í•œë‹¤. ìœ„ì˜ ë¡œê·¸ ê°€ëŠ¥ë„í•¨ìˆ˜ $log(l(\\beta_{0}, \\beta_{1}))$ì„ í¸ì˜ìƒ LLì´ë¼í•˜ì. ë¡œê·¸ê°€ëŠ¥ë„í•¨ìˆ˜ LLì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì€ ë‹¤ìŒ ëª©ì í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. J = - LL SGD(Steepest Gradient Descent)ë°©ì‹ì„ ì‚¬ìš©(Stochastic Gradient Descentì•„ë‹˜!!)í•˜ì—¬ ì•„ë˜ì™€ ê°™ì€ ê·¸ë ˆë””ì–¸ ë²¡í„°ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. g_{k} = \\frac{d}{dw} (-LL) ìœ„ì—ì„œ êµ¬í•œ ê·¸ë ˆë””ì–¸íŠ¸ ë°©í–¥ìœ¼ë¡œ step size $(n_{k})$ë§Œí¼ ì´ë™í•œë‹¤. w_{k+1} = w_{k} - n_{k} g_{k}= w_{k} + n_{k} \\sum_{i=1}^{N} (y_{i} - \\theta_{i} (x_{i} ; w)) x_{i} ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ìš”ì•½ëœ ë‹¤ì¤‘ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ì˜ ê²°ê³¼ë¥¼ ì‚´í´ë³´ë©´, ë§¨ ë§ˆì§€ë§‰ ì„¤ëª…ê³¼ ê°™ì´ í•´ì„í•  ìˆ˜ ìˆì§€ë§Œ, ì¢€ ë” ìì—°ìŠ¤ëŸ¬ìš´ í•´ì„í•˜ê³  ì‹¶ì„ ê²½ìš° ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ Logit ë³´ë‹¤ expë¥¼ ì·¨í•´ì¤˜ Oddsë¡œ ë³€í™˜í•˜ì—¬ í•´ì„í•˜ëŠ” ê²ƒì„ ê¶Œí•œë‹¤.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"Python - 00 (Pythonì˜ ì¥ì  ë° ìë£Œí˜•)","slug":"Python_00","date":"2020-03-20T12:02:48.000Z","updated":"2020-03-21T08:02:15.297Z","comments":true,"path":"2020/03/20/Python_00/","link":"","permalink":"https://heung-bae-lee.github.io/2020/03/20/Python_00/","excerpt":"","text":"Pythonì™œ Pythonì„ ë°°ì›Œì•¼ í• ê¹Œ? í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ë°°ìš°ê³  ì‹¶ì€ë° ì–´ë–¤ ì–¸ì–´ë¥¼ ë°°ìš°ë©´ ë ê¹Œ? C, C++, C#, Java, Javascript, Python, Ruby, C#, Go, Rust, Scala Perl, Obj-C, PHP, R, Julia ë“± ì—¬ëŸ¬ê°€ì§€ ì–¸ì–´ê°€ ì¡´ì¬í•˜ì§€ë§Œ ê°€ì¥ ì§„ì…ì¥ë²½ì´ ë‚®ë‹¤. Python ì–¸ì–´ì˜ ì¥ì  ë¬¸ë²•ì´ ê°„ê²° ë‹¤ì–‘í•œ ìš´ì˜ì²´ì œ ì§€ì› GUI Application ê°œë°œ(PyQT) ë²”ìš© ì–¸ì–´(ë„¤íŠ¸ì›Œí¬, ì›¹, ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹ ë“±) ë°©ëŒ€í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§€ì› data science : Numpy, Pandas, Matplotlib, scikit-learn, statsmodels, tensorflow, keras, Pytorch web : Django, Flask ë“± crawling : scrapy, BeautifulSoap, requests ë“± Pythonì€ ì–´ë–¤ ì–¸ì–´ì¸ê°€? Python is an interpreted high-level programming language for general-purpose programming.wikipedia) Pythonì˜ íŠ¹ì§• í”Œë«í¼ì— ë…ë¦½ì  ì¸í„°í”„ë¦¬í„° ì–¸ì–´ ê°ì²´ì§€í–¥ì  ë™ì íƒ€ì´í•‘ ìœ„ì˜ ìš©ì–´ì—ëŒ€í•´ ì˜ ëª¨ë¥´ê² ë‹¤ë©´, ë¨¼ì € ì»´í“¨í„°ì— ëŒ€í•´ ì•Œì•„ë³´ì. ì»´í“¨í„°ë¥¼ ì´í•´í•´ ë³´ì! ì»´í“¨í„°ëŠ” ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ê³„ì´ë‹¤. ì»´í“¨í„°ë¥¼ êµ¬ì„±í•˜ëŠ” ê¸°ë³¸ ìš”ì†Œ CPU(Central Processing Unit) RAM(Random Access Memory) ROM(Read-Only Memory) OS(Operating System : ìš´ì˜ì²´ì œ) Kernel í•˜ë“œì›¨ì–´ë¥¼ ì»¨íŠ¸ë¡¤í•˜ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ (ìš´ì˜ì²´ì œì˜ í•µì‹¬) CPU, RAM, ROM ìì›ì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì •ì˜ App(Application : ì–´í”Œë¦¬ì¼€ì´ì…˜) OS ê¸°ë°˜ ì‘ìš© í”„ë¡œê·¸ë¨ ëŒ€ë¶€ë¶„ì˜ í”„ë¡œê·¸ë˜ë°ì˜ ì˜ì—­ ì»´í“¨í„°ì—ì„œ í”„ë¡œê·¸ë¨ì˜ ë™ì‘ì›ë¦¬ëŠ”? CPU, RAM, ROMì€ 0ê³¼ 1ë°–ì— ëª¨ë¥¸ë‹¤ëŠ” ì‚¬ì‹¤ì€ ìƒì‹ì ìœ¼ë¡œ ë‹¤ë“¤ ì•Œê³  ìˆì„ ê²ƒì´ë‹¤. ê·¸ëŸ°ë°, í”„ë¡œê·¸ë¨ ì–¸ì–´ëŠ” ìˆ«ìì™€ ì•ŒíŒŒë²¡ê³¼ íŠ¹ìˆ˜ê¸°í˜¸ë¥¼ ì‚¬ìš©í•œë‹¤. ì´ëŠ” ê° ì–¸ì–´ì˜ Compilerê°€ ì½”ë“œë¥¼ 0ê³¼ 1ì˜ ì´ì§„ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ì£¼ëŠ” ì»´íŒŒì¼ëŸ¬ ì–¸ì–´ì´ê¸° ë•Œë¬¸ì´ë‹¤. ì»´íŒŒì¼ëŸ¬ ì–¸ì–´ ëª¨ë“  ì½”ë“œë¥¼ ì»´íŒŒì¼ë§ í›„ì— ì»´í“¨í„°ì—ì„œ ì²˜ë¦¬ -&gt; ì²˜ë¦¬ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ í”„ë¡œê·¸ë¨ ì‹¤í–‰ì„ ìœ„í•´ ì»´íŒŒì¼ë§ ì‹œê°„ì„ ê¸°ë‹¤ë ¤ì•¼í•œë‹¤. ì¸í„°í”„ë¦¬í„° ì–¸ì–´ í•œì¤„ì”© ì½”ë“œë¥¼ ì»´íŒŒì¼ë§ í•˜ë©´ì„œ ì»´í“¨í„°ì—ì„œ ì²˜ë¦¬ -&gt; ì²˜ë¦¬ì†ë„ê°€ ëŠë¦¬ì§€ë§Œ ì»´íŒŒì¼ë§ ì‹œê°„ ì—†ì´ ë°”ë¡œë°”ë¡œ í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•œë‹¤. ê²°ë¡ ì ìœ¼ë¡ , ì½”ë“œë¥¼ ì˜ ë§Œë“¤ë©´ ì»´í“¨í„°ê°€ íš¨ìœ¨ì ìœ¼ë¡œ ì¼í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤!!! Pythonì€ ì–´ë–¤ ì–¸ì–´ì¸ê°€? í”Œë«í¼ì— ë…ë¦½ì  ì–´ë– í•œ ì¢…ë¥˜ì˜ OSì—ë„ ê°™ì€ ë¬¸ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì¦‰, Window, Mac OS, Linuxë“± ì—¬ëŸ¬ OSì—ì„œ ì‚¬ìš©ê°€ëŠ¥í•˜ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì¸í„°í”„ë¦¬í„° ì–¸ì–´ í•œì¤„ì”© ì»´íŒŒì¼ë§ í•˜ë©´ì„œ ì½”ë“œë¥¼ ìˆ˜í–‰í•œë‹¤. ê°ì²´ì§€í–¥ì  ì‹¤ì œ ì„¸ê³„ë¥¼ ëª¨ë¸ë§í•˜ì—¬ ê³µí†µì ì¸ ê¸°ëŠ¥ì„ ë¬¶ì–´ì„œ ê°œë°œí•˜ëŠ” ë°©ì‹ ì¶”ìƒí™”(abstraction), ìº¡ìŠí™”(encapsulation), ìƒì†(inheritance), ë‹¤í˜•ì„±(polymorphism)ì˜ íŠ¹ì§•ì„ ê°–ëŠ”ë‹¤. ì°¸ê³ ë¡œ, ë°˜ëŒ€ì˜ ê°œë…ì€ ì ˆì°¨ì§€í–¥ì´ë‹¤. ë™ì íƒ€ì´í•‘ ë³€ìˆ˜ ì„ ì–¸ì‹œ ë°ì´í„° íƒ€ì…ì„ ì§€ì •í•´ ì£¼ì§€ ì•Šì•„ë„ ë°ì´í„°ì— ë”°ë¼ì„œ ìë™ìœ¼ë¡œ íƒ€ì´í•‘ëœë‹¤. Pythonì˜ ì¢…ë¥˜ëŠ”? Cpython Cë¡œ ë§Œë“¤ì–´ì§„ íŒŒì´ì¬ ìš°ë¦¬ê°€ ì½”ë”©í•˜ëŠ” ë¶€ë¶„ì€ ì¸í„°í”„ë¦¬í„°ì´ì§€ë§Œ ì•ˆì—ì„œëŠ” ë‹¤ ì»´íŒŒì¼ëŸ¬ì–¸ì–´ì¸ Cë¡œ ë™ì‘ë˜ì–´ ì†ë„ê°€ ë¹ ë¥´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì¸í„°í”„ë¦¬í„° ì–¸ì–´ê°€ ê°–ëŠ” ì†ë„ê°€ ëŠë¦¬ë‹¤ëŠ” ì ì„ ë³´ì™„í•  ìˆ˜ ìˆì–´ ë°ì´í„° ë¶„ì„ì— ìš©ì´í•˜ë‹¤. Jython Javaë¡œ ë§Œë“¤ì–´ì§„ íŒŒì´ì¬ IronPython C#ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ íŒŒì´ì¬ Pypy Pythonìœ¼ë¡œ ë§Œë“¤ì–´ì§„ íŒŒì´ì¬ Cpython ë³´ë‹¤ ë¹ ë¥´ê²Œ ìˆ˜í–‰ë˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. http://pypy.org/ í”„ë¡œê·¸ë˜ë°ì„ í•œë‹¤ëŠ” ê²ƒì€? ì»´í“¨í„°ì™€ì˜ íš¨ìœ¨ì ì¸ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ìœ¼ë¡œ ìì‹ ì´ ìƒê°í•˜ëŠ” ëª©ì ì„ ì»´í“¨í„°ê°€ ì˜ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì‹œìŠ¤í…œì˜ êµ¬ì¡°ë¥¼ ì˜ ì„¤ê³„í•˜ê³  ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒì´ë‹¤. 1. PEP(Python Enhance Proposal) pythonì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì œì•ˆ Zen of Python : PEP20 : https://www.python.org/dev/peps/pep-0020/ Style Guide for Python Code : PEP8 : https://www.python.org/dev/peps/pep-0008/ Python ê¸°ë³¸ í•µì‹­ ì´í•´í•˜ê¸°ê°’(value) ì²˜ë¦¬ Pythonì€ ì‹¤í–‰ë˜ëŠ” ëª¨ë“ ê²ƒì„ ê°ì²´ë¡œ ê´€ë¦¬í•˜ë¯€ë¡œ ê°ì²´ë¥¼ ì¼ê´€ì„±ì„ ê°€ì§€ê³  í‰ê°€í• ìˆ˜ ìˆëŠ” ê·œì¹™ì„ ë„ì…í–ˆê¸° ë•Œë¬¸ì— ëª¨ë“  ê²ƒì„ ê°’(value)ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤. ê°’ì„ ì¬ì‚¬ìš©ì„ í•˜ê¸° ìœ„í•´ì„  ë³€ìˆ˜ì— ì €ì¥í•´ì•¼í•œë‹¤. ìœ„ì˜ ì„¤ëª…ì´ ì•„ì§ ê¹Œì§„ ì˜ ì´í•´ê°€ ê°€ì§€ ì•Šì„ ê²ƒì´ë‹¤. literal(ë¦¬í„°ëŸ´) literalì´ë€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¡œ ì‘ì„±ëœ ì½”ë“œì—ì„œ ê°’ì„ ëŒ€í‘œí•˜ëŠ” ìš©ì–´ì´ë‹¤. ì˜ˆë¥¼ ë“¤ë©´, Pythonì—ì„œëŠ” ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì •ìˆ˜, ë¶€ë™ ì†Œìˆ˜ì  ìˆ«ì, ë¬¸ìì—´, Boolean ë“±ì˜ ê°ì²´ë¡œ í‰ê°€ë˜ë©° ëª¨ë“  ê²ƒì„ ê°’ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ì´ë¥¼ í†µí•´ ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤. ì •ìˆ˜í˜• ë¦¬í„°ëŸ´ê°’ ì •ì˜11 ì‹¤í–‰ê²°ê³¼11 Expression(í‘œí˜„ì‹) literalì€ ë‹¨ìˆœíˆ í•˜ë‚˜ì˜ ê°’ì„ ì²˜ë¦¬í•œë‹¤. ì—¬ëŸ¬ ê°œì˜ ê°’ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ì„œ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ í‘œí˜„ì‹ì´ë¼ê³  í•œë‹¤. í‘œí˜„ì‹ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ê°€ í•´ì„í•˜ëŠ” í•˜ë‚˜ ì´ìƒì˜ ëª…ì‹œì  ê°’, ìƒìˆ˜, ë³€ìˆ˜, ì—°ì‚°ì ì¡°í•©ì´ê³  í•¨ìˆ˜ë„ ì‹¤í–‰ë˜ë©´ í•˜ë‚˜ì˜ ê°’ì´ ë˜ì–´ ì´ë¥¼ ì¡°í•©í•´ë„ í‘œí˜„ì‹ìœ¼ë¡œ ì¸ì‹í•œë‹¤. í‘œí˜„ì‹ì„ í‰ê°€í•´ì„œ ì‹¤í–‰ë  ë•Œì—ëŠ” ìš°ì„ ìˆœìœ„ ë° ì—°ê´€ ê·œì¹™ì— ë”°ë¼ í•´ì„í•˜ì—¬ ì‹¤í–‰ë˜ë©° í‰ê°€ëœ ê²°ê³¼ëŠ” í•˜ë‚˜ì˜ ê°’ì¸ literalë¡œ í‘œí˜„ëœë‹¤. í‘œí˜„ì‹ ì²˜ë¦¬ìˆœì„œëŠ” ì¢Œì¸¡ë¶€í„° ìš°ì¸¡ìœ¼ë¡œ ê°€ë©° í‰ê°€í•˜ê³  () ì—°ì‚°ìë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì²˜ë¦¬ëœë‹¤. í‘œí˜„ì‹ í‰ê°€13 + 4 ìœ„ì˜ í‘œí˜„ì‹ ê²°ê³¼17 í‘œí˜„ì‹ í‰ê°€1\"ë¬¸ì\" + \"ì—´\" ìœ„ì˜ í‘œí˜„ì‹ ê²°ê³¼1\"ë¬¸ìì—´\" condition expression(ì¡°ê±´ì‹) í‘œí˜„ì‹ì—ì„œ íŠ¹ì • ì¡°ê±´ë¬¸ ë“±ì— ì œí•œì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê²ƒì„ ì¡°ê±´ì‹ì´ë¼ê³  í•œë‹¤. ì£¼ë¡œ íŠ¹ì • ë¬¸ì¥ì¸ ifë¬¸ì´ë‚˜ while ë¬¸ì—ì„œ ì²˜ë¦¬ëœë‹¤. ì²˜ë¦¬ëœ ê²°ê³¼ ê°’ì€ Boolean ê°’ì„ ê°–ëŠ”ë‹¤. ì¡°ê±´ì‹ í‰ê°€1bool(\"\") ì¡°ê±´ì‹ í‰ê°€ ê²°ê³¼1False ì´ë¦„(name) ì²˜ë¦¬ í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•œë‹¤ëŠ” ê²ƒì€ ê°’ë“¤ì„ ì €ì¥í•˜ê³ , ë‹¤ìŒì— í•„ìš”í•  ê²½ìš° ì´ë¥¼ ì½ì–´ì™€ì„œ ê³„ì‚°í•˜ê³  ë‹¤ì‹œ ì €ì¥í•´ì„œ ì²˜ë¦¬í•œë‹¤ëŠ” ëœ»ì´ë‹¤. í”„ë¡œê·¸ë¨ì—ì„œ ì´ëŸ° ê°’ë“¤ì˜ ë³€í™”ë¥¼ ì €ì¥í•´ì„œ ê´€ë¦¬í•˜ëŠ” ê¸°ì¤€ì´ í•„ìš”í•˜ê³ , íŒŒì´ì¬ì—ì„œëŠ” ì´ë¥¼ ìœ„í•´ Name spaceë¥¼ ë§Œë“¤ì–´ì„œ ê´€ë¦¬í•œë‹¤. ì´ë¦„ìœ¼ë¡œ ì§€ì •í•  ìš”ì†Œë“¤ì€ ë³€ìˆ˜(variable), í•¨ìˆ˜(function), í´ë˜ìŠ¤(class), ëª¨ë“ˆ(module), íŒ¨í‚¤ì§€(package)ë“± ì´ë‹¤. ì´ ì¤‘ì— ë³€ìˆ˜ë¥¼ ë¹¼ë©´ ë‹¤ ê°ì²´ë¡œ ì‚¬ìš©ë˜ë¯€ë¡œ ê°’ìœ¼ë¡œ ê´€ë¦¬ë˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. íŒŒì´ì¬ì€ ë³€ìˆ˜, í•¨ìˆ˜, í´ë˜ìŠ¤ ë“±ì— ëŒ€í•œ ì´ë¦„ì„ êµ¬ë³„í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë³€ìˆ˜, í•¨ìˆ˜, í´ë˜ìŠ¤ ë“±ì˜ ëª…ëª… ê·œì¹™ì„ ëª…í™•íˆ í•´ì„œ ê°ê°ì„ ì´ë¦„ìœ¼ë¡œ ì‹ë³„í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. ì˜ˆì•½ì–´(keyword) íŒŒì´ì¬ì—ì„œ ë³€ìˆ˜, í•¨ìˆ˜, í´ë˜ìŠ¤ ë“±ì€ ì˜ˆì•½ì–´ì™€ ë™ì¼í•œ ì´ë¦„ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ì—†ê³  íŒŒì´ì¬ ë‚´ë¶€ ë¬¸ë²• ê·œì¹™ì—ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì •ì˜í•œ ê²ƒì„ ì˜ˆì•½ì–´ë¼ê³  í•œë‹¤. íŒŒì´ì¬ì—ì„œ ëª¨ë“ˆ(module)ì€ í”„ë¡œê·¸ë¨ì„ ê´€ë¦¬í•˜ëŠ” í•˜ë‚˜ì˜ ë‹¨ìœ„ì´ê³  ì´ ë‚´ë¶€ì— ë³€ìˆ˜, í•¨ìˆ˜, í´ë˜ìŠ¤ ë“±ì„ ì§€ì •í•´ì„œ ê´€ë¦¬í•œë‹¤. íŒŒì´ì¬ ë‚´ë¶€ì˜ ì˜ˆì•½ì–´ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ í•˜ë‚˜ì˜ ëª¨ë“ˆë¡œ ê´€ë¦¬í•˜ë©° ê·¸ ëª¨ë“ˆì˜ ì´ë¦„ì´ keywordì´ë‹¤. Keyword ëª¨ë“ˆ ì•Œì•„ë³´ê¸°1234from pprint import pprintimport keywordpprint(keyword.kwlist, width=60, compact=True) Keyword ëª¨ë“ˆ ë¦¬ìŠ¤íŠ¸123456['False', 'None', 'True', 'and', 'as', 'assert', 'async', 'await', 'break', 'class', 'continue', 'def', 'del', 'elif', 'else', 'except', 'finally', 'for', 'from', 'global', 'if', 'import', 'in', 'is', 'lambda', 'nonlocal', 'not', 'or', 'pass', 'raise', 'return', 'try', 'while', 'with', 'yield'] ëª…ëª… ê·œì¹™(naming convention) íŒŒì´ì¬ì€ í•˜ë‚˜ì˜ í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ëŠ” ê¸°ì¤€ì´ ëª¨ë“ˆì´ë¯€ë¡œ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ ì´ë¦„ì„ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” í•˜ë‚˜ì˜ ì˜ì—­ì´ ì¡´ì¬í•œë‹¤. ì´ë ‡ê²Œ ëª¨ë“ˆ ë‹¨ìœ„ë¡œ ê´€ë¦¬í•˜ëŠ” í•˜ë‚˜ì˜ ì´ë¦„ ê´€ë¦¬ ì˜ì—­ì„ ì „ì—­ ë„¤ì„ìŠ¤í˜ì´ìŠ¤(global name space)ë¼ê³  í•œë‹¤. ë³€ìˆ˜ì™€ í•¨ìˆ˜ ë“±ì´ ë™ì¼í•œ ì´ë¦„ìœ¼ë¡œ ì •ì˜ë  ê²½ìš° ìµœì¢…ì ìœ¼ë¡œ í• ë‹¹ëœ ê°’ì„ ê°–ìœ¼ë¯€ë¡œ ìµœëŒ€í•œ ëª…ëª… ê·œì¹™ì„ ì¤€ìˆ˜í•´ì•¼ ë²„ì „ì´ ë³€ê²½ë  ê²½ìš°ì—ë„ ì¼ê´€ì„± ìˆëŠ” ê·œì¹™ì„ ì¤€ìˆ˜í•  ìˆ˜ ìˆë‹¤. ì‹ë³„ì : Identifiersë³€ìˆ˜, í•¨ìˆ˜, í´ë˜ìŠ¤, ëª¨ë“ˆ ë“±ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì´ë¦„ì„ ì‹ë³„ìë¼ê³  í•©ë‹ˆë‹¤. ì´ ì‹ë³„ìëŠ” ëª‡ê°€ì§€ì˜ ê·œì¹™ì´ ìˆìŠµë‹ˆë‹¤. ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„ ì†Œë¬¸ì(a-z), ëŒ€ë¬¸ì(A-Z), ìˆ«ì(0-9), ì–¸ë”ìŠ¤ì½”ì–´( _ ) ì‚¬ìš© ê°€ëŠ¥ ( _ )ë¥¼ ì œì™¸í•œ íŠ¹ìˆ˜ë¬¸ìëŠ” ì‚¬ìš© ë¶ˆê°€ ê°€ì¥ ì•ì— ( __ ) ì‚¬ìš© ì§€ì–‘ (reserved global variable) ê°€ì¥ì•ì—ìˆ«ìì‚¬ìš©ë¶ˆê°€ ì˜ˆì•½ì–´ì‚¬ìš©ë¶ˆê°€ ì˜ˆì•½ì–´ Fasle, class, finally, is, return, None, continue, for, lambda, try, True, def, from, nonlocal, while, and, del, global, not, with, as, elif, if, or, yield, assert, else, import, pass, break, except, in, raise íŒŒì´ì¬ ê¶Œì¥ ëª…ëª… ê·œì¹™: ëª¨ë“ˆ, íŒ¨í‚¤ì§€ ëª¨ë“ˆ ì´ë¦„ì€ ì§§ì•„ì•¼ í•˜ê³ , ì „ë¶€ ì†Œë¬¸ìë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤. ê°€ë…ì„±ì„ ìœ„í•´ ì–¸ë”ìŠ¤ì½”ì–´( _ )ë¥¼ ì“¸ ìˆ˜ ìˆë‹¤. íŒ¨í‚¤ì§€ ì´ë¦„ ë˜í•œ ì§§ì•„ì•¼ í•˜ê³ , ì „ë¶€ ì†Œë¬¸ìë¡œ ì‘ì„±í•˜ì§€ë§Œ, ì–¸ë”ìŠ¤ì½”ì–´( _ )ëŠ” ê¶Œì¥í•˜ì§€ ì•ŠëŠ”ë‹¤. íŒŒì´ì¬ ê¶Œì¥ ëª…ëª… ê·œì¹™: í´ë˜ìŠ¤ í´ë˜ìŠ¤ ì´ë¦„ì€ Capitalized words í˜•ì‹(ë‹¨ì–´ë¥¼ ëŒ€ë¬¸ìë¡œ ì‹œì‘)ì„ ë”°ë¥´ë©°, ì¹´ë©œ í‘œê¸°ë²•ë„ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤. íŒŒì´ì¬ ê¶Œì¥ ëª…ëª… ê·œì¹™: ìƒìˆ˜, ë³€ìˆ˜, í•¨ìˆ˜, ë©”ì„œë“œ ë³€ìˆ˜, í•¨ìˆ˜ì™€ ë©”ì„œë“œì˜ ì´ë¦„ì€ ì›ì¹™ì ìœ¼ë¡œ ì†Œë¬¸ìì—¬ì•¼í•˜ê³ , ê°€ì¡±ì„±ì„ ìœ„í•´ì„œ ì–¸ë”ìŠ¤ì½”ì–´( _ )ë‹¨ì–´ë¡œ ì“°ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤. ë³´í˜¸ ì†ì„±ì¼ ë•ŒëŠ” ë§¨ì•ì— _ ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë¶™ì¸ë‹¤. í‚¤ì›Œë“œì™€ ë™ì¼ ë³€ìˆ˜ì¼ ë•ŒëŠ” ë§¨ ë’¤ì— _ ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë¶™ì¸ë‹¤. ë¹„ê³µê°œ ì†ì„±ì¼ ë•ŒëŠ” ë§¨ ì•ì— __ í•˜ë‚˜ë¥¼ ë¶™ì¸ë‹¤. ìŠ¤í˜ì…œ ì†ì„±ì¼ë•ŒëŠ” ì•ê³¼ ë’¤ì— __ í•˜ë‚˜ì”© ë¶™ì¸ë‹¤. Variable(ë³€ìˆ˜) íŒŒì´ì¬ì—ì„œ ë³€ìˆ˜ëŠ” ë‹¤ë¥¸ ì–¸ì–´ì˜ ë³€ìˆ˜ì™€ ì°¨ì´ê°€ í¬ë‹¤. ë™ì íƒ€ì´í•‘ì˜ íŠ¹ì„±ì„ ì§€ë‹ˆë¯€ë¡œ ë³€ìˆ˜ì— íŠ¹ì •í•œ ìë£Œí˜• ë“±ì„ ë°°ì •í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì´ë‹¤. ë³€ìˆ˜ëŠ” ë‹¨ìˆœíˆ ì´ë¦„ë§Œ ì§€ì •í•˜ê³  ì´ ë³€ìˆ˜ì— ê°’ì„ í• ë‹¹í•´ì„œ ì‚¬ìš©í•˜ë¯€ë¡œ ë‹¤ì–‘í•œ ìë£Œí˜•ì´ í• ë‹¹ëœë‹¤. íŒŒì´ì¬ì—ì„œ ë³€ìˆ˜ëŠ” ë‹¨ìˆœíˆ ê°’ì„ ë³´ê´€í•˜ëŠ” ì¥ì†Œê°€ ì•„ë‹ˆë¼ ê°’ë“¤ì˜ ì„ì‹œ ì €ì¥ ì¥ì†Œë¡œë§Œ ì‚¬ìš©í•œë‹¤. í”„ë¡œê·¸ë¨ ë‚´ì—ì„œ ë³€ìˆ˜ëŠ” ë‹¨ìˆœíˆ ì´ë¦„ìœ¼ë¡œ êµ¬ë³„í•˜ëŠ” ê²ƒì´ê³  ë³€ìˆ˜ì—ëŠ” ê°’ì¸ ê°ì²´ê°€ ì–´ë””ì— ìˆëŠ”ì§€ì— ëŒ€í•œ ì£¼ì†Œì¸ referenceë§Œ ë³´ê´€í•œë‹¤. ì¦‰, ë³€ìˆ˜ëŠ” ë‹¨ìˆœíˆ ìë£Œí˜•ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ê°ì²´ì— ëŒ€í•œ ì£¼ì†Œë§Œì„ ê´€ë¦¬í•˜ëŠ” ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. 12 12","categories":[{"name":"Python","slug":"Python","permalink":"https://heung-bae-lee.github.io/categories/Python/"}],"tags":[]},{"title":"ë‚´ê°€ ì •ë¦¬í•˜ëŠ” ìë£Œêµ¬ì¡° 00 (Node, List, Queue)","slug":"data_structure_01","date":"2020-03-19T09:46:30.000Z","updated":"2020-03-24T10:49:19.957Z","comments":true,"path":"2020/03/19/data_structure_01/","link":"","permalink":"https://heung-bae-lee.github.io/2020/03/19/data_structure_01/","excerpt":"","text":"ëª©í‘œ ê¸°ë³¸ ìë£Œ êµ¬ì¡°/ì•Œê³ ë¦¬ì¦˜ ìµíˆê¸° ì•Œê³ ë¦¬ì¦˜ í’€ì´ë¥¼ ìœ„í•´, ê¸°ë³¸ì ìœ¼ë¡œ ì•Œê³  ìˆì–´ì•¼ í•˜ëŠ” ìë£Œêµ¬ì¡°ì™€ ì•Œê³ ë¦¬ì¦˜ ì •ë¦¬ ìë£Œêµ¬ì¡°ë€? ìš©ì–´: ìë£Œêµ¬ì¡° = ë°ì´í„° êµ¬ì¡° = data structure ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì˜ë¯¸ ì½”ë“œìƒì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´, ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼, ì²´ê³„ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”í•´ì•¼ í•¨. ì–´ë–¤ ë°ì´í„° êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ëŠëƒì— ë”°ë¼, ì½”ë“œ íš¨ìœ¨ì´ ë‹¬ë¼ì§„ë‹¤. íš¨ìœ¨ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê´€ë¦¬í•˜ëŠ” ì˜ˆ ìš°í¸ë²ˆí˜¸ 5ìë¦¬ ìš°í¸ë²ˆí˜¸ë¡œ êµ­ê°€ì˜ ê¸°ì´ˆêµ¬ì—­ì„ ì‹ë³„ 5ìë¦¬ ìš°í¸ë²ˆí˜¸ì—ì„œ ì• 3ìë¦¬ëŠ” ì‹œ,êµ°,ìì¹˜êµ¬ë¥¼ ì˜ë¯¸, ë’¤ 2ìë¦¬ëŠ” ì¼ë ¨ë³€í˜¸ë¡œ êµ¬ì„± í•™ìƒê´€ë¦¬ í•™ë…„, ë°˜, ë²ˆí˜¸ë¥¼ í•™ìƒì—ê²Œ ë¶€ì—¬í•´ì„œ, í•™ìƒë¶€ë¥¼ ê´€ë¦¬ ìœ„ê°™ì€ ê¸°ë²•ì´ ì—†ì—ˆë‹¤ë©´, ë§ì€ í•™ìƒ ì¤‘ íŠ¹ì • í•™ìƒì„ ì°¾ê¸° ìœ„í•´, ì „ì²´ í•™ìƒë¶€ë¥¼ ëª¨ë‘ í›‘ì–´ì•¼ í•˜ëŠ” ë¶ˆí¸í•¨ì´ ìƒê¸´ë‹¤. ëŒ€í‘œì ì¸ ìë£Œ êµ¬ì¡° array(ë°°ì—´), stack(ìŠ¤íƒ), queue(í), linked list(ë§í¬ë“œ ë¦¬ìŠ¤íŠ¸), hash table(í•´ì‰¬ í…Œì´ë¸”), heap(í™) ë“± ì•Œê³ ë¦¬ì¦˜ì´ë€? ìš©ì–´: ì•Œê³ ë¦¬ì¦˜(algorithm) ì–´ë–¤ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ ì ˆì°¨/ë°©ë²• ì–´ë–¤ ë¬¸ì œì— ëŒ€í•´, íŠ¹ì •í•œ â€˜ì…ë ¥â€™ì„ ë„£ìœ¼ë©´, ì›í•˜ëŠ” â€˜ì¶œë ¥â€™ì„ ì–»ì„ ìˆ˜ ìˆë„ë¡ ë§Œë“œëŠ” í”„ë¡œê·¸ë˜ë° ìë£Œêµ¬ì¡°ì™€ ì•Œê³ ë¦¬ì¦˜ì´ ì¤‘ìš”í•œ ì´ìœ  ì–´ë–¤ ìë£Œêµ¬ì¡°ì™€ ì•Œê³ ë¦¬ì¦˜ì„ ì“°ëŠëƒì— ë”°ë¼, ì„±ëŠ¥ì´ ë§ì´ ì°¨ì´ë‚˜ê¸° ë•Œë¬¸ì´ë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ìë£Œêµ¬ì¡°ì™€ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ í”„ë¡œê·¸ë˜ë°ì„ ì˜ í•  ìˆ˜ ìˆëŠ” ê¸°ìˆ ê³¼ ì—­ëŸ‰ì„ ê²€ì¦í•  ìˆ˜ ìˆë‹¤. ìë£Œêµ¬ì¡°/ì•Œê³ ë¦¬ì¦˜, ê·¸ë¦¬ê³  Python ì–´ë–¤ ì–¸ì–´ë¡œë“  ìë£Œêµ¬ì¡°/ì•Œê³ ë¦¬ì¦˜ì„ ìµí ìˆ˜ ìˆë‹¤. ì´ì „ì—ëŠ” ë¬´ì¡°ê±´ C ë˜ëŠ” C++ë¡œë§Œ ì‘ì„±í•˜ë„ë¡ í•˜ëŠ” ê²½í–¥ì´ ë§ì•˜ë‹¤. ìµœê·¼ì—ëŠ” ì–¸ì–´ë¡œ ì¸í•œ ì œì•½/í‰ê°€ëŠ” ì—†ì–´ì¡Œë‹¤ê³  ë´ë„ ë¬´ë°©í•  ê²ƒ ê°™ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, ê°€ì¥ ì‰½ê³  ì§„ì…ì¥ë²½ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ Pythonì„ í†µí•´ í•„ìëŠ” ì •ë¦¬í•´ ë³¼ ê²ƒì´ë‹¤. ADT(Abstract Data Type)ì¶”ìƒ ìë£Œí˜•ì‹¤ì œ ì •ì˜) - ë‚´ë¶€ êµ¬ì¡°(object) - ê¸°ëŠ¥(operation) ==&gt; í•¨ìˆ˜ë¡œ êµ¬í˜„ ê°„ë‹¨í•œ ë§ë¡œëŠ” ë‚´ê°€ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ìë£Œêµ¬ì¡°ì˜ í•¨ìˆ˜ì˜ ì‚¬ìš©ë²•ì´ë‹¤. í•„ìëŠ” Pythonì„ í†µí•´ ê° ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ ë³¼ ê²ƒì´ë‹¤. ë¬¼ë¡  Pythonì—ëŠ” ì´ë¯¸ ë§Œë“¤ì–´ì ¸ ë†“ì€ library ì¤‘ ëŒ€ë¶€ë¶„ì˜ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•´ ë†“ì€ ê²ƒë“¤ì´ ì¡´ì¬í•˜ì§€ë§Œ, ê·¸ë ‡ê²Œ ì‚¬ìš©í•˜ë‹¤ë³´ë©´ ë‚´ë¶€ì ìœ¼ë¡œ ëœ¯ì–´ë³´ì§€ ì•ŠëŠ” ì´ìƒ ì–´ë– í•œ ë°©ì‹ìœ¼ë¡œ ì‘ë™ë˜ëŠ”ì§€ ì•Œì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ê° ì•Œê³ ë¦¬ì¦˜ì˜ êµ¬ì¡°ë¥¼ ì„¤ëª…í•œ í›„ ADTë¥¼ ì‘ì„±í•´ ë³´ê³  classë¥¼ ë§Œë“¤ì–´ functionì„ ì‘ì„±í•˜ì—¬ êµ¬í˜„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê¸€ì„ ì¨ ê°ˆ ê²ƒì´ë‹¤. ë¬¼ë¡ , Python ë‚´ë¶€ì— ì¡´ì¬í•˜ëŠ” libraryì˜ ì‚¬ìš©ë²•ë„ ê°™ì´ ì†Œê°œí•  ê²ƒì´ë‹¤.ì„ í˜•êµ¬ì¡° ë°°ì—´ ì—°ê²°ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ í ë¹„ì„ í˜•êµ¬ì¡° íŠ¸ë¦¬ (BTS â€”&gt; B-tree, red-black tree) ê·¸ë˜í”„ í…Œì´ë¸” etc Array(ë°°ì—´) ë°ì´í„°ë¥¼ ë‚˜ì—´í•˜ê³ , ê° ë°ì´í„°ë¥¼ ì¸ë±ìŠ¤ì— ëŒ€ì‘í•˜ë„ë¡ êµ¬ì„±í•œ ë°ì´í„° êµ¬ì¡° Pythonì—ì„œëŠ” ë¦¬ìŠ¤íŠ¸ íƒ€ì…ì´ ë°°ì—´ ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤. 1. ë°°ì—´ì€ ì™œ í•„ìš”í• ê¹Œ? ê°™ì€ ì¢…ë¥˜ì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•´ ì‚¬ìš© ê°™ì€ ì¢…ë¥˜ì˜ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì €ì¥ ì¥ì : ë¹ ë¥¸ ì ‘ê·¼ ê°€ëŠ¥ ì²« ë°ì´í„°ì˜ ìœ„ì¹˜ì—ì„œ ìƒëŒ€ì ì¸ ìœ„ì¹˜ë¡œ ë°ì´í„° ì ‘ê·¼(ì¸ë±ìŠ¤ ë²ˆí˜¸ë¡œ ì ‘ê·¼) ë‹¨ì : ë°ì´í„° ì¶”ê°€/ì‚­ì œì˜ ì–´ë ¤ì›€ ë¯¸ë¦¬ ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •í•´ì•¼ í•¨ C ì–¸ì–´ ì˜ˆ: ì˜ì–´ ë‹¨ì–´ ì €ì¥ ë°°ì—´ì„ ìƒì„±í•  ë•Œ ë¨¼ì € ê¸¸ì´ë¥¼ ì •í•´ì£¼ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ì•„ë˜ì™€ ê°™ì´ []ì•ˆì— ìµœëŒ€ ê¸¸ì´ë¥¼ ì§€ì •í•´ì¤€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 123456789#include &lt;stdio.h&gt;int main(int argc, char * argv[])&#123; char country[3] = \"US\"; printf (\"%c%c\\n\", country[0], country[1]); printf (\"%s\\n\", country); return 0;&#125; íŒŒì´ì¬ ì–¸ì–´ ì˜ˆ: ì˜ì–´ ë‹¨ì–´ ì €ì¥12country = 'US'print (country) 2. íŒŒì´ì¬ê³¼ ë°°ì—´ íŒŒì´ì¬ì—ì„œëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ë°°ì—´ êµ¬í˜„ ê°€ëŠ¥ 1ì°¨ì› ë°°ì—´: ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬í˜„ì‹œ12data_list = [1, 2, 3, 4, 5]data_list 2ì°¨ì› ë°°ì—´: ë¦¬ìŠ¤íŠ¸ë¡œ êµ¬í˜„ì‹œ12data_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]data_list ê²°ê³¼123456789101112131415print (data_list[0])# [1, 2, 3]print (data_list[0][0])# 1print (data_list[0][1])# 2print (data_list[0][2])# 3print (data_list[1][0])# 4print (data_list[1][1])# 5print (data_list[1][2])# 6 3. í”„ë¡œê·¸ë˜ë° ì—°ìŠµì—°ìŠµ1: ìœ„ì˜ 2ì°¨ì› ë°°ì—´ì—ì„œ 9, 8, 7 ì„ ìˆœì„œëŒ€ë¡œ ì¶œë ¥í•´ë³´ê¸°1print(data_list[2][2], data_list[2][1], data_list[2][0]) ì—°ìŠµ2: ì•„ë˜ì˜ dataset ë¦¬ìŠ¤íŠ¸ì—ì„œ ì „ì²´ ì´ë¦„ ì•ˆì— M ì€ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ë¹ˆë„ìˆ˜ ì¶œë ¥í•˜ê¸°123456789101112131415161718192021222324252627282930dataset = ['Braund, Mr. Owen Harris','Cumings, Mrs. John Bradley (Florence Briggs Thayer)','Heikkinen, Miss. Laina','Futrelle, Mrs. Jacques Heath (Lily May Peel)','Allen, Mr. William Henry','Moran, Mr. James','McCarthy, Mr. Timothy J','Palsson, Master. Gosta Leonard','Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)','Nasser, Mrs. Nicholas (Adele Achem)','Sandstrom, Miss. Marguerite Rut','Bonnell, Miss. Elizabeth','Saundercock, Mr. William Henry','Andersson, Mr. Anders Johan','Vestrom, Miss. Hulda Amanda Adolfina','Hewlett, Mrs. (Mary D Kingcome) ','Rice, Master. Eugene','Williams, Mr. Charles Eugene','Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)','Masselmani, Mrs. Fatima','Fynney, Mr. Joseph J','Beesley, Mr. Lawrence','McGowan, Miss. Anna \"Annie\"','Sloper, Mr. William Thompson','Palsson, Miss. Torborg Danira','Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)','Emir, Mr. Farred Chehab','Fortune, Mr. Charles Alexander','Dwyer, Miss. Ellen \"Nellie\"','Todoroff, Mr. Lalio'] 123456m_count = 0for data in dataset: for index in range(len(data)): if data[index] == 'M': m_count += 1print (m_count) ìœ„ì—ì„œì™€ ê°™ì´ ê°„ë‹¨í•˜ê²Œ ì´ë¯¸ ë§Œë“¤ì–´ì ¸ ìˆëŠ” Pythonì˜ ìë£Œí˜•ì¸ listë¥¼ ì‚¬ìš©í•˜ì—¬ Arrayë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒ ë§ê³  Node ê°œë…ì„ ë„ì…í•˜ì—¬ Arrayë¥¼ ë§Œë“¤ì–´ ë³¼ ê²ƒì´ë‹¤. Node ì•„ë˜ì™€ ê°™ì´ í•˜ë‚˜ì˜ ë…¸ë“œëŠ” ì‹¤ì œ ë°ì´í„°ì™€ ë‹¤ìŒë°ì´í„°ë¥¼ ê°€ë¦¬í‚¤ê³  ìˆëŠ” ì°¸ì¡°ë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. Node Class êµ¬í˜„ ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì‹¤ì œ ë°ì´í„°ë¥¼ ê°€ë¦¬í‚¤ëŠ” ë¶€ë¶„ê³¼ ë‹¤ìŒ ë°ì´í„°ë¥¼ ì°¸ì¡°í•˜ëŠ” ë¶€ë¶„ì„ ë‚˜ëˆ„ì–´ì„œ ì •ì˜í•´ ì¤€ë‹¤. í˜¼ì ë…¸ë“œë¥¼ ë§Œë“¤ë•Œ í•„ìš”í•œ ê³¼ì •ì„ ì •ë¦¬í•˜ë©´ì„œ ë§Œë“¤ì–´ë³´ê¸°1) Nodeì˜ ì •ì˜ë¥¼ ë¨¼ì € ì‚´í´ë³´ë©´ ë…¸ë“œëŠ” ë°ì´í„°ë¥¼ í¬í•¨í•˜ê³ ìˆëŠ” ë¶€ë¶„ê³¼ ë‹¤ìŒë°ì´í„°ë¥¼ ê°€ë¦¬í‚¤ëŠ” ì°¸ì¡°ë¶€ë¶„ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤! 2) ê·¸ëŸ¬ë¯€ë¡œ Node classì—ëŠ” ë¨¼ì €, ë°ì´í„° ë¶€ë¶„ê³¼ ì°¸ì¡°ë¶€ë¶„ì„ ë§Œë“¤ì–´ì£¼ì–´ì•¼ í•˜ëŠ”ë°, ìš°ì„  ìš°ë¦¬ì˜ ëª©í‘œëŠ” ì–´ë””ê¹Œì§€ë‚˜ Nodeì˜ ê¸°ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ê²ƒì´ë‹¤. 3) Nodeì˜ ë°ì´í„° ì¡°íšŒ, ë°ì´í„° ë°”ê¾¸ê¸°ì™€ ê°™ì€ ê¸°ëŠ¥, ê·¸ë¦¬ê³  ì°¸ì¡°ë¥¼ ì–´ë””ë¡œ í•˜ëŠ”ì§€ ë“±ì´í•„ìš”í•  ê²ƒì´ë‹¤! 123456789101112131415161718192021222324252627282930class Node_mine(): # ìƒì„±ìì—ëŠ” ìš°ì„ ì ìœ¼ë¡œ ë…¸ë“œì˜ componentë“¤ì„ í•„ìˆ˜ìš”ì†Œë¡œ ë„£ì–´ì¤˜ì•¼í•  ê²ƒì´ë‹¤. # ì—¬ê¸°ì„œ ì²˜ìŒ ë¶€í„° nextë¥¼ argumentì—ì„œ ë¹¼ ë†“ì€ ì´ìœ ëŠ”? # ì´ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— ë‹¤ìŒ ë…¸ë“œë¥¼ ë°”ë¡œ ì°¸ì¡°í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì— argumentë¡œ ë„£ì–´ì¤„ í•„ìš”ëŠ” ì—†ë‹¤. # ì¦‰, ìš°ë¦¬ê°€ ë§ˆìŒëŒ€ë¡œ 4ë²ˆì§¸ ë…¸ë“œë¥¼ 1ë²ˆì§¸ë…¸ë“œë¡œ ì°¸ì¡°í•˜ê²Œë”í•˜ëŠ” ê²ƒì€ linked listì˜ ì •ì˜ë¥¼ ë²—ì–´ë‚˜ë¯€ë¡œ # ì°¸ì¡°ë¶€ë¶„ì„ Nodeë¥¼ ìƒì„±í•˜ë©´ì„œë¶€í„° ì§€ì •í•  í•„ìš”ëŠ” ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. def __init__(self, data): self.__data=data self.__next=None # ì†Œë©¸ì: ê°ì²´ê°€ ë©”ëª¨ë¦¬ì—ì„œ ì‚¬ë¼ì§ˆë•Œ ë°˜ë“œì‹œ í•œë²ˆì€ í˜¸ì¶œí•´ ì£¼ëŠ” ê²ƒ def __delete__(self): print(\"&#123;&#125; is deleted\".format(self.__data)) @property def get_data(self): return self.__data @get_data.setter def get_data(self, data): self.__data=data @property def get_next(self): return self.__next @get_next.setter def get_next(self, nt): self.__next=nt node ì •ì˜ ë° ìƒì„± 1n1=Node_mine(5) nodeì˜ value ì¶œë ¥ 1234n1.get_data### ê²°ê³¼### 5 nodeì— ìƒˆë¡œìš´ ê°’ í• ë‹¹ 1n1.get_data=3 ë³€ê²½ëœ ê°’ì„ ì œëŒ€ë¡œ í• ë‹¹ ë°›ì•˜ëŠ”ì§€ í™•ì¸123n1.get_data### ê²°ê³¼### 3 í˜„ì—…ì—ì„œ ì‚¬ìš©í•˜ëŠ” linked listë¼ í•¨ì€ dummy duable linked listë¥¼ ì˜ë¯¸í•˜ê³ , ì•„ë˜ì—ì„œì˜ single linked listì—ì„œ singleì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ì°¸ì¡°ê°€ í•˜ë‚˜ ì¦‰ í•œê°œì˜ ë°©í–¥ë§Œì„ ì°¸ì¡°í•˜ê³  duableì€ ì–‘ë°©í–¥ì„ ì°¸ì¡°í•œë‹¤.Singel Linked listInstance Member head ë¦¬ìŠ¤íŠ¸ì˜ ì²«ë²ˆì§¸ ë…¸ë“œë¥¼ ê°€ë¦¬í‚¨ë‹¤. d_size ë¦¬ìŠ¤íŠ¸ì˜ ìš”ì†Œ ê°œìˆ˜ 123456789101112131415161718192021222324252627282930313233343536373839404142class SingleLinkedList: def __init__(self): self.head = None self.d_size = 0 def empty(self): if self.d_size==0: return True else : return False def size(self): return self.d_size def add(self, data): new_node=Node_mine(data) # ì•„ë˜ì—ì„œì™€ ê°™ì´ ë‹¨ë°©í–¥ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ , ê¸°ì¡´ì— ìˆë˜ Nodeë¥¼ ìƒˆë¡œìš´ Nodeì˜ ë‹¤ìŒìœ¼ë¡œ ì°¸ì¡°ì‹œì¼œì¤€ ë’¤ì—, SingleLinkedList í´ë˜ìŠ¤ì˜ headê°’ì—ëŠ” ìƒˆë¡œìš´ Nodeë¥¼ ì¶”ê°€í•´ì¤€ë‹¤. new_node.next=self.head self.head=new_node self.d_size+=1 def search(self, target): cur=self.head while cur : if cur.data == target : return cur else : cur=cur.next return cur def delete(self): # headì— ìˆëŠ” ê°’ì„ ì§€ìš¸ê²ƒì´ë¯€ë¡œ headê°’ì„ ê¸°ì¡´ì˜ headì˜ ë‹¤ìŒ ê°’ìœ¼ë¡œ ë°”ê¾¸ì–´ì£¼ë©´ garbage collectorì— ì˜í•´ì„œ ì‚¬ë¼ì§€ê²Œ ëœë‹¤. self.head=self.head.next self.d_size-=1 def traverse(self): cur=self.head while cur: yield cur cur=cur.next Queue(í) arrayì™€ í•¨ê»˜ ê°€ì¥ ì‰¬ìš´ ìë£Œêµ¬ì¡° ì¤‘ í•˜ë‚˜ ì»´í“¨í„°ì—ì„œ ê°€ì¥ í•µì‹¬ì ì¸ OS(ìš´ì˜ì²´ì œ)ì—ì„œë„ ë§ì´ ì‚¬ìš©ë˜ë©°, ì¸í„°ë„·ì—ì„œë„ ë„¤íŠ¸ì›Œí¬ ê¸°ëŠ¥ì—ì„œë„ ë§ì´ ì‚¬ìš©ëœë‹¤. 1. Queue êµ¬ì¡° ì¤„ì„ ì„œëŠ” í–‰ìœ„ì™€ ìœ ì‚¬ ê°€ì¥ ë¨¼ì € ë„£ì€ ë°ì´í„°ë¥¼ ê°€ì¥ ë¨¼ì € êº¼ë‚¼ ìˆ˜ ìˆëŠ” êµ¬ì¡° ìŒì‹ì ì—ì„œ ê°€ì¥ ë¨¼ì € ì¤„ì„ ì„  ì‚¬ëŒì´ ì œì¼ ë¨¼ì € ìŒì‹ì ì— ì…ì¥í•˜ëŠ” ê²ƒê³¼ ë™ì¼ FIFO(First-In, First-Out) ë˜ëŠ” LILO(Last-In, Last-Out) ë°©ì‹ìœ¼ë¡œ ìŠ¤íƒê³¼ êº¼ë‚´ëŠ” ìˆœì„œê°€ ë°˜ëŒ€ ì¶œì²˜: http://www.stoimen.com/blog/2012/06/05/computer-algorithms-stack-and-queue-data-structure/ 2. ì•Œì•„ë‘˜ ìš©ì–´ Enqueue: íì— ë°ì´í„°ë¥¼ ë„£ëŠ” ê¸°ëŠ¥ Dequeue: íì—ì„œ ë°ì´í„°ë¥¼ êº¼ë‚´ëŠ” ê¸°ëŠ¥ Visualgo ì‚¬ì´íŠ¸ì—ì„œ ì‹œì—°í•´ë³´ë©° ì´í•´í•˜ê¸° (enqueue/dequeue ë§Œ í´ë¦­í•´ë³´ë©°): https://visualgo.net/en/list 3. íŒŒì´ì¬ queue ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©í•´ì„œ í ìë£Œ êµ¬ì¡° ì‚¬ìš©í•˜ê¸° queue ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ëŠ” ë‹¤ì–‘í•œ í êµ¬ì¡°ë¡œ Queue(), LifoQueue(), PriorityQueue() ì œê³µ í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•  ë•Œ í”„ë¡œê·¸ë¨ì— ë”°ë¼ ì í•©í•œ ìë£Œ êµ¬ì¡°ë¥¼ ì‚¬ìš© Queue(): ê°€ì¥ ì¼ë°˜ì ì¸ í ìë£Œ êµ¬ì¡° LifoQueue(): ë‚˜ì¤‘ì— ì…ë ¥ëœ ë°ì´í„°ê°€ ë¨¼ì € ì¶œë ¥ë˜ëŠ” êµ¬ì¡° (ìŠ¤íƒ êµ¬ì¡°ë¼ê³  ë³´ë©´ ë¨) PriorityQueue(): ë°ì´í„°ë§ˆë‹¤ ìš°ì„ ìˆœìœ„ë¥¼ ë„£ì–´ì„œ, ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ë°ì´í„° ì¶œë ¥ ì¼ë°˜ì ì¸ í ì™¸ì— ë‹¤ì–‘í•œ ì •ì±…ì´ ì ìš©ëœ íë“¤ì´ ìˆìŒ 3.1. Queue()ë¡œ í ë§Œë“¤ê¸° (ê°€ì¥ ì¼ë°˜ì ì¸ í, FIFO(First-In, First-Out))12345import queuedata_queue = queue.Queue()data_queue.put(\"funcoding\")data_queue.put(1) 1data_queue.qsize() ê²°ê³¼12 1data_queue.get() ê²°ê³¼1'funcoding' 1data_queue.qsize() ê²°ê³¼11 1data_queue.get() ê²°ê³¼1'funcoding' ê²°ê³¼11 1data_queue.qsize() ê²°ê³¼10 3.2. LifoQueue()ë¡œ í ë§Œë“¤ê¸° (LIFO(Last-In, First-Out))12import queuedata_queue = queue.LifoQueue() 12data_queue.put(\"funcoding\")data_queue.put(1) 1data_queue.qsize() ê²°ê³¼12 ê·¸ëƒ¥ Queue êµ¬ì¡°ê°€ ì•„ë‹Œ Lifo Queueì´ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ì— ì¶”ê°€í•´ ì¤€ 1ì´ ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¨ë‹¤.1data_queue.get() ê²°ê³¼11 3.3. PriorityQueue()ë¡œ í ë§Œë“¤ê¸°123import queuedata_queue = queue.PriorityQueue() ì•„ë˜ì—ì„œ ë³´ëŠ” ê²ƒê³¼ ê°™ì´ ìš°ì„  ìˆœìœ„ì™€ ê°’ì„ íŠœí”Œí˜•íƒœë¡œ ê°™ì´ ë„£ì–´ì¤€ë‹¤. ìš°ì„ ìˆœìœ„ëŠ” ìˆ«ìê°€ ë‚®ì„ ìˆ˜ë¡ ë†’ì€ ìš°ì„  ìˆœìœ„ë¥¼ ì§€ë‹Œë‹¤.123data_queue.put((10, \"korea\"))data_queue.put((5, 1))data_queue.put((15, \"china\")) 1data_queue.qsize() ê²°ê³¼13 1data_queue.get() ìš°ì„ ìˆœìœ„ê°€ ì œì¼ ë†’ì€ ì œì¼ ë‚®ì€ ìˆ«ìì¸ 5ë¥¼ ì§€ë‹Œ ê°’ 1ì´ ì¶œë ¥ëœë‹¤.1(5, 1) 1data_queue.get() ê·¸ ë‹¤ìŒ ìš°ì„  ìˆœìœ„ì¸ 10ì„ ì§€ë‹Œ koreaê°€ ì¶œë ¥ëœë‹¤.1(10, \"korea\") ì°¸ê³ : ì–´ë””ì— Queueê°€ ë§ì´ ì“°ì¼ê¹Œ? ë©€í‹° íƒœìŠ¤í‚¹ì„ ìœ„í•œ í”„ë¡œì„¸ìŠ¤ ìŠ¤ì¼€ì¥´ë§ ë°©ì‹ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ ë§ì´ ì‚¬ìš©ëœë‹¤. (ìš´ì˜ì²´ì œ ì°¸ì¡°) Queueì˜ ê²½ìš°ì—ëŠ” ì¥ë‹¨ì  ë³´ë‹¤ëŠ” (íŠ¹ë³„íˆ ì–¸ê¸‰ë˜ëŠ” ì¥ë‹¨ì ì´ ì—†ìŒ), íì˜ í™œìš© ì˜ˆë¡œ í”„ë¡œì„¸ìŠ¤ ìŠ¤ì¼€ì¥´ë§ ë°©ì‹ì„ í•¨ê»˜ ì´í•´í•´ë‘ëŠ” ê²ƒì´ ì¢‹ìŒ 4. í”„ë¡œê·¸ë˜ë° ì—°ìŠµ ì—°ìŠµ1: ë¦¬ìŠ¤íŠ¸ ë³€ìˆ˜ë¡œ íë¥¼ ë‹¤ë£¨ëŠ” enqueue, dequeue ê¸°ëŠ¥ êµ¬í˜„í•´ë³´ê¸° 123456789queue_list = list()def enqueue(data): queue_list.append(data)def dequeue(): data = queue_list[0] del queue_list[0] return data ìœ„ì—ì„œ ë§Œë“  queue_list ì•ˆì— 0~9ê¹Œì§€ì˜ ìˆ˜ë¥¼ ë„£ëŠ”ë‹¤. 12for index in range(10): enqueue(index) 1len(queue_list) ê²°ê³¼110 1dequeue() ê²°ê³¼ FIFO êµ¬ì¡°ì´ë¯€ë¡œ ì œì¼ ë¨¼ì € ë“¤ì–´ê°„ 0ì´ ì¶œë ¥ëœë‹¤. 10 íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì´ìš©í•œ í êµ¬í˜„ ìœ„ì—ì„œ ë§Œë“¤ì—ˆë˜ functionë“¤ì„ ì´ìš©í•˜ì—¬ ì¢€ë” ë§ì€ ê¸°ëŠ¥ì´ í¬í•¨ë˜ì–´ ìˆëŠ” í•˜ë‚˜ì˜ classë¡œ Queueë¥¼ êµ¬í˜„í•´ ë³¼ ê²ƒì´ë‹¤. Python list ìë£Œí˜•ì˜ pop ë©”ì„œë“œë¥¼ í†µí•´ dequeueë¥¼ êµ¬í˜„í•  ê²ƒì´ë‹¤. Enqueue12345678910111213141516171819class Queue: def __init__(self): self.container=list() def empty(self):# if self.container is None: if not self.container: return True return False def enqueue(self, data): self.container.append(data) def dequeue(self): return self.container.pop(0) def peek(self): return self.container[0] 123456789q=Queue()q.enqueue(1)q.enqueue(2)q.enqueue(3)q.enqueue(4)q.enqueue(5)while not q.empty(): print(q.dequeue(), end=' ') ê²°ê³¼11 2 3 4 5 single linked listë¥¼ ì´ìš©í•´ Queueë¥¼ êµ¬í˜„ADT Queue.empty() -&gt; Boolean Queueê°€ ë¹„ì–´ìˆìœ¼ë©´ True, ì•„ë‹ˆë©´ False Queue.enqueue(data) -&gt; None Queueì˜ ë§¨ ë’¤ì— ë°ì´í„°ë¥¼ ìŒ“ëŠ”ë‹¤. ë§¨ ì²˜ìŒ enqueue í•œ ë°ì´í„°ëŠ” frontì— ìœ„ì¹˜í•˜ê³  ìµœê·¼ì— ì¶”ê°€ë¡œ ì‚½ì…í•´ì¤€ ë°ì´í„°ëŠ” rearë¡œ ìœ„ì¹˜ì‹œí‚¨ë‹¤. ê°ê°ì˜ NodeëŠ” nextë¡œ ì—°ê²°ì„ ì‹œì¼œì¤€ë‹¤. ë¨¼ì € ìƒˆë¡œìš´ Nodeë¥¼ ì¶”ê°€í•´ì£¼ê¸° ìœ„í•´ì„œëŠ” í˜„ì¬ self.rear.nextë¥¼ new_nodeë¥¼ ê°€ë¦¬í‚¤ë„ë¡í•˜ê³ , self.rearë¥¼ new_nodeë¡œ ì„¤ì •í•´ì£¼ë©´ ëœë‹¤. Queue.dequeue() -&gt; data Queue ë§¨ ì•ì˜ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ë©´ì„œ ë°˜í™˜ ìš°ì„  Dequeueë¥¼ í•˜ë©´ ì œì¼ ë¨¼ì € ì¶”ê°€í•´ë†“ì•˜ë˜ ë°ì´í„°ì˜ ê°’ì„ ì¶œë ¥í•´ì£¼ë©´ í•´ë‹¹ Nodeë¥¼ ì œê±°í•´ ì£¼ì–´ì•¼í•˜ë¯€ë¡œ, self.frontì˜ ë°ì´í„°ë¥¼ curë¼ëŠ” ë³€ìˆ˜ì— ë”°ë¡œ ì €ìí•´ ì£¼ëŠ”ë°, Nodeê°€ í•˜ë‚˜ì¼ë•Œë¥¼ ìƒê°í•´ ë³´ë©´ self.frontì™€ self.rearê°€ ë™ì¼í•˜ë¯€ë¡œ ë¨¼ì € self.rearë¥¼ Noneìœ¼ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ curì— self.frontë¥¼ í• ë‹¹í•´ì£¼ê³ , self.frontëŠ” self.front.nextë¥¼ í• ë‹¹í•´ì£¼ë©°, cur.dataë¥¼ ì¶œë ¥í•´ì£¼ë©´ ëœë‹¤. Queue.peek() -&gt; data Queue ë§¨ ì• ë°ì´í„°ë¥¼ ë°˜í™˜ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Node: def __init__(self, data): self.__data = data self.__next = None @property def data(self): return self.__data @data.setter def data(self, data): self.__data=data @property def next(self): return self.__next @next.setter def next(self, n): self.__next=nclass LQueue: def __init__(self): self.front=None self.rear=None def empty(self): if self.front is None: return True return False def enqueue(self, data): new_node=Node(data) if self.empty(): self.front=new_node self.rear=new_node self.rear.next = new_node self.rear = new_node def dequeue(self): if self.empty(): return None if self.front is self.rear: self.rear = None cur = self.front self.front = self.front.next return cur.data def peek(self): return self.front.data 123456789q = LQueue()q.enqueue(1)q.enqueue(2)q.enqueue(3)q.enqueue(4)q.enqueue(5)while not q.empty(): print(q.dequeue(), end=' ') ê²°ê³¼11 2 3 4 5 12","categories":[{"name":"C/C++/ìë£Œêµ¬ì¡°","slug":"C-C-ìë£Œêµ¬ì¡°","permalink":"https://heung-bae-lee.github.io/categories/C-C-ìë£Œêµ¬ì¡°/"}],"tags":[]},{"title":"data engineering (ë°ì´í„° ëª¨ë¸ë§ ë° ì±—ë´‡ ë§Œë“¤ê¸°)","slug":"data_engineering_10","date":"2020-03-03T14:29:20.000Z","updated":"2020-03-09T14:06:21.058Z","comments":true,"path":"2020/03/03/data_engineering_10/","link":"","permalink":"https://heung-bae-lee.github.io/2020/03/03/data_engineering_10/","excerpt":"","text":"Spotify ë°ì´í„° ìœ ì‚¬ë„ ëª¨ë¸ë§ ëª¨ë“  trackì„ ë‹¤ ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ì„œ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ê¸°ì—ëŠ” ë§ì€ ì–‘ì´ê¸° ë•Œë¬¸ì— í•´ë‹¹ Artistì˜ trackë“¤ì˜ audio feature ë°ì´í„°ì— ëŒ€í•´ í‰ê· ì„ ë‚¸ ê°’ì„ ì‚¬ìš©í•˜ì—¬ Artist ë¼ë¦¬ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ê²ƒì´ë‹¤. í•´ë‹¹ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì´ ë¨¼ì € RDSì— ì ‘ì†í•˜ì—¬ tableì„ ìƒì„±í•´ ì¤€ë‹¤. 123mysql -h spotify.cgaj5rvtgf25.ap-northeast-2.rds.amazonaws.com -P 3306 -u hb0619 -pCREATE TABLE related_artists (artist_id VARCHAR(255), y_artist VARCHAR(255), distance FLOAT, PRIMARY KEY(artist_id, y_artist)) ENGINE=InnoDB DEFAULT CHARSET=utf8; ê·¸ ë‹¤ìŒì€ ìš°ë¦¬ê°€ ë¯¸ë¦¬ ë§Œë“¤ì–´ë†“ì•˜ë˜ Athenaì˜ DataBaseëŠ” ë¬´ì—‡ì´ì—ˆëŠ”ì§€ë¥¼ í™•ì¸í•˜ì. í•„ìëŠ” ì•„ë˜ì™€ ê°™ì´ ë”°ë¡œ Databaseë¥¼ ë§Œë“¤ì§€ ì•Šê³  defaultë¡œ ì‚¬ìš©í–ˆë‹¤. ë˜í•œ, ì…ë ¥í–ˆì—ˆë˜ ë‚ ì§œë¥¼ í™•ì¸í•´ë†“ì•„ì•¼ ì¶”í›„ì— ì½”ë“œ ì‘ì„±ì‹œ Athenaë¡œ ì ‘ì†í•˜ì—¬ ë§Œë“¤ì–´ì§„ í…Œì´ë¸”ë“¤ì„ ì°¸ì¡°í•  ìˆ˜ ìˆë‹¤. í•„ìëŠ” Athenaì— ë¯¸ë¦¬ ë§Œë“¤ì–´ë†“ì•˜ë˜ ë‘ê°€ì§€ top_tracksì™€ audio_features í…Œì´ë¸”ì„ ì´ìš©í•˜ì—¬ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ê³  í•´ë‹¹ ìœ ì‚¬ë„ë¥¼ MySQL DBì— insertí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ì—…ì„ ì§„í–‰ í•  ê²ƒì´ë‹¤. data_modeling.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import sysimport osimport loggingimport pymysqlimport boto3import timeimport mathhost = \"end-point url\"port = you port numberusername = \"your MYSQL DB ID\"database = \"your MYSQL DB Name\"password = \"your MYSQL DB Password\"def main(): try: conn = pymysql.connect(host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") sys.exit(1) athena = boto3.client('athena') query = \"\"\" SELECT artist_id, AVG(danceability) AS danceability, AVG(energy) AS energy, AVG(loudness) AS loudness, AVG(speechiness) AS speechiness, AVG(acousticness) AS acousticness, AVG(instrumentalness) AS instrumentalness FROM top_tracks t1 JOIN audio_features t2 ON t2.id = t1.id AND CAST(t1.dt AS DATE) = DATE('2020-02-24') AND CAST(t2.dt AS DATE) = DATE('2020-02-24') GROUP BY t1.artist_id \"\"\" # ìœ„ì—ì„œ DATEë¥¼ ì •í•˜ëŠ” ë¶€ë¶„ì—ì„œ CAST(t1.dt AS DATE) = CURRENT_DATE - INTERVAL '1' DAY ì´ë ‡ê²Œ í˜„ì¬ë‚ ì§œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì°¨ì´ë‚˜ëŠ” ê¸°ê°„ì„ í†µí•´ ì •í•´ì¤„ ìˆ˜ ìˆë‹¤. # í•„ìëŠ” ì—¬ëŸ¬ë²ˆ Athenaì— ì‹¤í–‰í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ìµœê·¼ì— Athenaì— ë§Œë“¤ì–´ ë†“ì€ ìœ„ì˜ ë‘ í…Œì´ë¸”ì˜ ë°ì´í„°ë¥¼ ì§ì ‘ ë³´ê³  ë‚ ì§œë¥¼ ì§€ì •í–ˆë‹¤. r = query_athena(query, athena) results = get_query_result(r['QueryExecutionId'], athena) artists = process_data(results) query = \"\"\" SELECT MIN(danceability) AS danceability_min, MAX(danceability) AS danceability_max, MIN(energy) AS energy_min, MAX(energy) AS energy_max, MIN(loudness) AS loudness_min, MAX(loudness) AS loudness_max, MIN(speechiness) AS speechiness_min, MAX(speechiness) AS speechiness_max, ROUND(MIN(acousticness),4) AS acousticness_min, MAX(acousticness) AS acousticness_max, MIN(instrumentalness) AS instrumentalness_min, MAX(instrumentalness) AS instrumentalness_max FROM audio_features \"\"\" r = query_athena(query, athena) results = get_query_result(r['QueryExecutionId'], athena) avgs = process_data(results)[0] metrics = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness'] for i in artists: for j in artists: dist = 0 for k in metrics: x = float(i[k]) x_norm = normalize(x, float(avgs[k+'_min']), float(avgs[k+'_max'])) y = float(j[k]) y_norm = normalize(y, float(avgs[k+'_min']), float(avgs[k+'_max'])) dist += (x_norm-y_norm)**2 dist = math.sqrt(dist) ## euclidean distance data = &#123; 'artist_id': i['artist_id'], 'y_artist': j['artist_id'], 'distance': dist &#125; insert_row(cursor, data, 'related_artists') conn.commit() cursor.close()def normalize(x, x_min, x_max): normalized = (x-x_min) / (x_max-x_min) return normalizeddef query_athena(query, athena): response = athena.start_query_execution( QueryString=query, QueryExecutionContext=&#123; 'Database': 'default' &#125;, ResultConfiguration=&#123; 'OutputLocation': \"s3://spotify-chatbot-project/athena-panomix-tables/\", 'EncryptionConfiguration': &#123; 'EncryptionOption': 'SSE_S3' &#125; &#125; ) return response# ì•„ë˜ì™€ ê°™ì´ Athena APIëŠ” responseë¥¼ ë°›ì•˜ë‹¤ê³  í•´ì„œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì‹¤í–‰ì„ ì‹œí‚¨ í›„ì—# í•´ë‹¹ query idë¥¼ í†µí•´ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” í˜•ì‹ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.def get_query_result(query_id, athena): response = athena.get_query_execution( QueryExecutionId=str(query_id) ) while response['QueryExecution']['Status']['State'] != 'SUCCEEDED': if response['QueryExecution']['Status']['State'] == 'FAILED': logging.error('QUERY FAILED') break time.sleep(5) response = athena.get_query_execution( QueryExecutionId=str(query_id) ) # ì¤‘ìš”í•œ ì ì€ MaxResultsê°€ 1000ì´ Maxë¼ëŠ” ì ì´ë‹¤. response = athena.get_query_results( QueryExecutionId=str(query_id) ) return responsedef process_data(results): columns = [col['Label'] for col in results['ResultSet']['ResultSetMetadata']['ColumnInfo']] listed_results = [] for res in results['ResultSet']['Rows'][1:]: values = [] for field in res['Data']: try: values.append(list(field.values())[0]) except: values.append(list(' ')) listed_results.append(dict(zip(columns, values))) return listed_resultsdef insert_row(cursor, data, table): placeholders = ', '.join(['%s'] * len(data)) columns = ', '.join(data.keys()) key_placeholders = ', '.join(['&#123;0&#125;=%s'.format(k) for k in data.keys()]) sql = \"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s\" % (table, columns, placeholders, key_placeholders) cursor.execute(sql, list(data.values())*2)if __name__=='__main__': main() ìœ„ì˜ íŒŒì¼ì„ ì‹¤í–‰ì‹œì¼œë³´ì. ìœ„ì˜ python script íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” pathë¡œ ì´ë™í•˜ì—¬ ì•„ë˜ ëª…ë ¹ë¬¸ì„ ì‹¤í–‰ì‹œí‚¤ë©´ ì‹¤í–‰ì— ì™„ë£Œë ë•Œê¹Œì§€ ê±¸ë¦° ì‹œê°„ ë˜í•œ ì•Œ ìˆ˜ ìˆë‹¤. 12345time python3 data_modeling.pyreal 28m11.013suser 1m36.141ssys 0m24.518s ì´ì œ MySQLì— ì ‘ì†í•´ì„œ ë°ì´í„°ê°€ ì œëŒ€ë¡œ insert ë¬ëŠ”ì§€ í™•ì¸í•´ ë³´ì. 12345678910111213141516171819202122232425262728SELECT * FROM related_artists LIMIT 20;# ê²°ê³¼+------------------------+------------------------+-----------+| artist_id | y_artist | distance |+------------------------+------------------------+-----------+| 00FQb4jTyendYWaN8pK0wa | 00FQb4jTyendYWaN8pK0wa | 0 || 00FQb4jTyendYWaN8pK0wa | 01C9OoXDvCKkGcf735Tcfo | 0.366558 || 00FQb4jTyendYWaN8pK0wa | 02rd0anEWfMtF7iMku9uor | 0.327869 || 00FQb4jTyendYWaN8pK0wa | 02uYdhMhCgdB49hZlYRm9o | 0.595705 || 00FQb4jTyendYWaN8pK0wa | 03r4iKL2g2442PT9n2UKsx | 0.632109 || 00FQb4jTyendYWaN8pK0wa | 03YhcM6fxypfwckPCQV8pQ | 0.812604 || 00FQb4jTyendYWaN8pK0wa | 04gDigrS5kc9YWfZHwBETP | 0.498764 || 00FQb4jTyendYWaN8pK0wa | 04tBaW21jyUfeP5iqiKBVq | 0.322017 || 00FQb4jTyendYWaN8pK0wa | 0543y7yrvny4KymoaneT4W | 0.365608 || 00FQb4jTyendYWaN8pK0wa | 05E3NBxNMdnrPtxF9oraJm | 0.958604 || 00FQb4jTyendYWaN8pK0wa | 06HL4z0CvFAxyc27GXpf02 | 0.483454 || 00FQb4jTyendYWaN8pK0wa | 06nevPmNVfWUXyZkccahL8 | 0.0592581 || 00FQb4jTyendYWaN8pK0wa | 06nsZ3qSOYZ2hPVIMcr1IN | 0.39567 || 00FQb4jTyendYWaN8pK0wa | 085pc2PYOi8bGKj0PNjekA | 0.608243 || 00FQb4jTyendYWaN8pK0wa | 08avsqaGIlK2x3i2Cu7rKH | 0.328059 || 00FQb4jTyendYWaN8pK0wa | 09C0xjtosNAIXP36wTnWxd | 0.210568 || 00FQb4jTyendYWaN8pK0wa | 0BvkDsjIUla7X0k6CSWh1I | 0.606556 || 00FQb4jTyendYWaN8pK0wa | 0bvRYuXRvd14RYEE7c0PRW | 0.670187 || 00FQb4jTyendYWaN8pK0wa | 0C0XlULifJtAgn6ZNCW2eu | 0.70478 || 00FQb4jTyendYWaN8pK0wa | 0cc6vw3VN8YlIcvr1v7tBL | 0.716507 |+------------------------+------------------------+-----------+20 rows in set (0.01 sec) JOINì„ í†µí•´ ê°ê° ì´ë¦„ì„ ë³¼ìˆ˜ìˆê²Œ í•´ì£¼ë©´ì„œ ê°€ì¥ distanceê°€ ì‘ì€ ì¦‰ ìœ ì‚¬ì„±ì´ í° ë°ì´í„° ìˆœì„œë¡œ ë³´ì—¬ì£¼ê¸¸ ì›í•´ ì•„ë˜ì™€ ê°™ì€ queryë¥¼ ì‘ì„±í•˜ì—¬ ì‹¤í–‰ì‹œì¼°ë‹¤. ê·¸ ê²°ê³¼, audio_featuresë¡œë§Œ ëª¨ë¸ë§ì„ í–ˆìŒì—ë„ ë¹„ìŠ·í•œ ì¥ë¥´ì˜ ì•„í‹°ìŠ¤íŠ¸ê°€ ë¬¶ì—¬ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 123456789101112131415161718192021222324252627SELECT p1.name, p2.name, p1.url, p2.url, p2.distance FROM artists p1 JOIN (SELECT t1.name, t1.url, t2.y_artist, t2.distance FROM artists t1 JOIN related_artists t2 ON t2.artist_id = t1.id) p2 ON p2.y_artist=p1.id WHERE distance != 0 ORDER BY p2.distance ASC LIMIT 20;+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+| name | name | url | url | distance |+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+| Four Tops | Alan Jackson | https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4 | https://open.spotify.com/artist/4mxWe1mtYIYfP040G38yvS | 0.0241995 || Alan Jackson | Four Tops | https://open.spotify.com/artist/4mxWe1mtYIYfP040G38yvS | https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4 | 0.0241995 || Martha Reeves &amp; The Vandellas | Jimmy Ruffin | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | https://open.spotify.com/artist/0hF0PwB04hnXfYMiZWfJzy | 0.0258624 || Jimmy Ruffin | Martha Reeves &amp; The Vandellas | https://open.spotify.com/artist/0hF0PwB04hnXfYMiZWfJzy | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | 0.0258624 || George Harrison | Martha Reeves &amp; The Vandellas | https://open.spotify.com/artist/7FIoB5PHdrMZVC3q2HE5MS | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | 0.0272287 || Martha Reeves &amp; The Vandellas | George Harrison | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | https://open.spotify.com/artist/7FIoB5PHdrMZVC3q2HE5MS | 0.0272287 || Nik Kershaw | Elton John | https://open.spotify.com/artist/7kCL98rPFsNKjAHDmWrMac | https://open.spotify.com/artist/3PhoLpVuITZKcymswpck5b | 0.0272474 || Elton John | Nik Kershaw | https://open.spotify.com/artist/3PhoLpVuITZKcymswpck5b | https://open.spotify.com/artist/7kCL98rPFsNKjAHDmWrMac | 0.0272474 || Tammi Terrell | Kim Carnes | https://open.spotify.com/artist/75jNCko3SnEMI5gwGqrbb8 | https://open.spotify.com/artist/5PN2aHIvLEM98XIorsPMhE | 0.0279891 || Kim Carnes | Tammi Terrell | https://open.spotify.com/artist/5PN2aHIvLEM98XIorsPMhE | https://open.spotify.com/artist/75jNCko3SnEMI5gwGqrbb8 | 0.0279891 || Roger Daltrey | Arcade Fire | https://open.spotify.com/artist/5odf7hjI7hyvAw66tmxhGF | https://open.spotify.com/artist/3kjuyTCjPG1WMFCiyc5IuB | 0.0291541 || Arcade Fire | Roger Daltrey | https://open.spotify.com/artist/3kjuyTCjPG1WMFCiyc5IuB | https://open.spotify.com/artist/5odf7hjI7hyvAw66tmxhGF | 0.0291541 || Billy Fury | Otis Redding | https://open.spotify.com/artist/7rtLZcKWGV4eaZsBwSKimf | https://open.spotify.com/artist/60df5JBRRPcnSpsIMxxwQm | 0.0292248 || Otis Redding | Billy Fury | https://open.spotify.com/artist/60df5JBRRPcnSpsIMxxwQm | https://open.spotify.com/artist/7rtLZcKWGV4eaZsBwSKimf | 0.0292248 || Katy Perry | John Fogerty | https://open.spotify.com/artist/6jJ0s89eD6GaHleKKya26X | https://open.spotify.com/artist/5ujCegv1BRbEPTCwQqFk6t | 0.0302168 || John Fogerty | Katy Perry | https://open.spotify.com/artist/5ujCegv1BRbEPTCwQqFk6t | https://open.spotify.com/artist/6jJ0s89eD6GaHleKKya26X | 0.0302168 || Dierks Bentley | The Cadillac Three | https://open.spotify.com/artist/7x8nK0m0cP2ksQf0mjWdPS | https://open.spotify.com/artist/1nivFfWu6oXBFDNyVfFU5x | 0.0313435 || The Cadillac Three | Dierks Bentley | https://open.spotify.com/artist/1nivFfWu6oXBFDNyVfFU5x | https://open.spotify.com/artist/7x8nK0m0cP2ksQf0mjWdPS | 0.0313435 || Sheryl Crow | Phil Collins | https://open.spotify.com/artist/4TKTii6gnOnUXQHyuo9JaD | https://open.spotify.com/artist/4lxfqrEsLX6N1N4OCSkILp | 0.0317203 || Phil Collins | Sheryl Crow | https://open.spotify.com/artist/4lxfqrEsLX6N1N4OCSkILp | https://open.spotify.com/artist/4TKTii6gnOnUXQHyuo9JaD | 0.0317203 |+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+20 rows in set (1.18 sec) ì´ì œ Facebook messenger APIë¥¼ í†µí•´ ì±—ë´‡ì„ ì„œë¹„ìŠ¤ë¥¼ ë§Œë“¤ì–´ ë³¼ ê²ƒì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™\u001dì´ êµ¬ê¸€ì—ì„œ Facebook messenger APIë¥¼ ê²€ìƒ‰í•˜ì—¬ í˜ì´ì§€ë¡œ ì ‘ì†í•œë‹¤. Facebook messenger API ì›¹í˜ì´ì§€ë¥¼ ì ‘ì†í•˜ë©´ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ë³´ì´ë©°, ê·¸ì— ëŒ€í•œ ì‘ë™ì›ë¦¬ì— ëŒ€í•œ ì„¤ëª…ì€ Introductionì˜ learn moreë¥¼ í´ë¦­í•˜ë©´ ë‘ë²ˆì§¸ ê·¸ë¦¼ê³¼ ê°™ì´ ì‘ë™ì›ë¦¬ë¥¼ ë³´ì—¬ì¤€ë‹¤. ê°„ë‹¨íˆ ë§í•˜ìë©´, Userê°€ Facebook messenger APIë¥¼ í†µí•´ ì§ˆì˜í•˜ë©´ ê·¸ ì •ë³´ë¥¼ Business Serverì— ë³´ë‚´ì„œ í•´ë‹¹ ì§ˆë¬¸ì— ë”°ë¥¸ ë‹µë³€ì„ ê°€ì ¸ì™€ ë³´ì—¬ì£¼ëŠ” í˜•ì‹ì´ë‹¤. Facebook messenger APIë¥¼ í†µí•´ì„œëŠ” ë‹¤ì–‘í•œ ë°©ì‹ì˜ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆë‹¤. ì´ë¯¸ ë§Œë“¤ì–´ì ¸ìˆëŠ” UI/UX í…œí”Œë¦¿ë“¤ì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— ì›í•˜ëŠ” í˜•ì‹ì— ë§ì¶° ë‹¤ì–‘í•˜ê²Œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆë‹¤. Facebook messenger APIë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ê°€ì¥ ë¨¼ì € í˜ì´ì§€ê°€ ë§Œë“¤ì–´ì ¸ ìˆì–´ì•¼ í•œë‹¤. ì•„ë˜ì™€ ê°™ì´ ìƒˆë¡œìš´ í˜ì´ì§€ë¥¼ ë§Œë“¤ê±°ë‚˜ ì´ë¯¸ ë§Œë“¤ì–´ì ¸ ìˆëŠ” ìì‹ ì˜ í˜ì´ì§€ë¥¼ ë¨¼ì € ë“±ë¡ì‹œí‚¨ë‹¤. Lambdaë¥¼ í†µí•´ì„œ AWSì™€ Facebook messenger APIë¥¼ ì—°ê²°í•´ ë³¼ ê²ƒì´ë‹¤. Lambdaë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ì§€ë‚œë²ˆì— ì–¸ê¸‰í–ˆë˜ ê²ƒê³¼ ê°™ì´ EC2ì™€ ê°™ì´ ì„œë²„ë¥¼ í•­ìƒ ë„ì–´ë†“ê³  ì •í•´ì§„ resourceë¥¼ í†µí•´ ì„œë¹„ìŠ¤ë¥¼ ê´€ë¦¬í•˜ë©´ ëŠ˜ì–´ë‚˜ê±°ë‚˜ ì¤„ì–´ë“œëŠ” Userì— ëŒ€í•´ì„œ ìœ ì—°í•˜ê²Œ ì²˜ë¦¬í•˜ê¸° í˜ë“¤ê¸° ë•Œë¬¸ì´ë‹¤. LambdaëŠ” ì˜ˆë¥¼ ë“¤ì–´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ Userê°€ ëŠ˜ë”ë¼ë„ ê·¸ì—ë”°ë¼ ë³‘ë ¬ì ìœ¼ë¡œ ì‘ì—…í•˜ê¸° ë•Œë¬¸ì— Trafficì˜ í¬ê¸°ì— í¬ê²Œ ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë‹¤. ë°˜ëŒ€ë¡œ EC2ì˜ ê²½ìš°ì—ëŠ” í•´ë‹¹ Trafficì´ ì¦ê°€í•¨ì— ë”°ë¼ ì—¬ëŸ¬ê°€ì§€ ì¥ì¹˜ë¥¼ êµ¬í˜„í•´ ë†“ì•„ì•¼í•œë‹¤. AWSì— ë¡œê·¸ì¸í•œ í›„ Lambdaë¥¼ ë“¤ì–´ê°€ì„œ, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ìƒˆë¡œìš´ Lambda Functionì„ ìƒì„±í•´ ì¤€ë‹¤. ì´ì œ ìœ„ì—ì„œ ë§Œë“  Lambda Functionê³¼ Facebook messenger APIë¥¼ ì—°ê²°í•˜ê¸° ìœ„í•´ì„œ AWSì—ì„œ API ê´€ë¦¬ ë° ì„¤ì •ì„ ë‹´ë‹¹í•˜ëŠ” API Gateway í˜ì´ì§€ë¡œ ì´ë™í•´ì„œ ìƒˆë¡­ê²Œ API Gateë¥¼ ë§Œë“¤ ê²ƒì´ë‹¤. í•„ìëŠ” REST API ë°©ì‹ìœ¼ë¡œ ìƒì„±í•  ê²ƒì´ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í•˜ì˜€ìœ¼ë©°, APIì´ë¦„ë„ ì„¤ì •í•´ ì£¼ì—ˆë‹¤. ê·¸ ë‹¤ìŒì€ Crawlingí•  ë•Œ í•œë²ˆì”© ì ‘í•´ë´¤ì„ ë²•í•œ GET, POST Methodë¥¼ ë§Œë“¤ì–´ ì£¼ëŠ” ê³¼ì •ì„ ê±°ì¹œë‹¤. ë¨¼ì € GETì€ Integration typeì„ Lambda Functionìœ¼ë¡œ ì„¤ì •í•´ ì£¼ê³ , Lambda Functionì€ ë°©ê¸ˆ ë§Œë“¤ì–´ë†“ì€ ê²ƒì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ì„¤ì •í•œ ë’¤ì— Integration Request íƒ­ìœ¼ë¡œ ì´ë™í•˜ì—¬ Mapping Templatesì˜ Request body passthrough ì•„ë˜ì™€ê°™ì´ ì„¤ì •í•˜ë©°, mapping Templatesì— application/jsonì„ ì¶”ê°€í•´ì¤€ë‹¤. Generate templateì„ Method Request passthroughë¡œ ì„¤ì •í•œ í›„ ìµœì¢…ì ìœ¼ë¡œ saveë¥¼ í•˜ì—¬ GET methodì˜ ì„¤ì •ì„ ë§ˆì¹œë‹¤. í•„ìê°€ ì‚¬ìš©í•  ì„œë¹„ìŠ¤ì—ì„œëŠ” POST methodëŠ” Facebook APIì— ë°ì´í„°ë¥¼ ì£¼ëŠ” ì—­í•  ë°–ì— ì—†ê¸° ë•Œë¬¸ì— í¬ê²Œ ì„¤ì •í•  ê²ƒì´ ì—†ë‹¤. GET, POST methodë¥¼ ë‹¤ ì„¤ì •í–ˆë‹¤ë©´, ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë°°í¬ë¥¼ í•´ì•¼ í•  ê²ƒì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ actionë²„íŠ¼ì„ ëˆŒëŸ¬ deploy apië¥¼ ì„ íƒí•˜ì—¬ stageë¥¼ ìƒˆë¡­ê²Œ ë§Œë“¤ì–´ì£¼ë©°, ì´ë¦„ì„ ì„¤ì •í•œë‹¤. deployë¥¼ ë‹¤ ì™„ë£Œí•˜ê²Œ ë˜ë©´, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì€ í™”ë©´ì´ ë‚˜íƒ€ë‚  ê²ƒì´ë‹¤. ê·¸ ì¤‘ ì•„ë˜ ë¹¨ê°„ìƒ‰ ìƒì ì•ˆì— ìˆëŠ” invoke URLì€ ìš°ë¦¬ê°€ Facebookì— ì—°ê²°í•´ ì¤„ endpoint ì—­í• ì„ í•œë‹¤. ì¶”í›„ì— invoke URL ì£¼ì†Œë¥¼ ë³µì‚¬í•œ í›„ì— ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ Facebookì—ì„œ ë§Œë“¤ì–´ ë†“ì€ appì˜ ì½œë°± url ì¶”ê°€ë¥¼ ëˆŒëŸ¬ ì¶”ê°€í•´ ì¤„ ê²ƒì´ë‹¤. ìœ„ì˜ ê·¸ë¦¼ ì²˜ëŸ¼ webhook urlì„ ì¶”ê°€ í•´ì£¼ë ¤ë©´ Lambda Functionì„ ë§Œë“¤ì–´ ì£¼ì–´ì•¼ í•˜ëŠ”ë° ë¨¼ì € ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ í† í°ì„ ìƒì„±í•´ì„œ ë³µì‚¬í•œ í›„ Lambda Functionì„ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì‘ì„±í•´ ì¤€ë’¤ì— webhook urlì„ ì¶”ê°€í•´ ì¤„ ìˆ˜ ìˆë‹¤. ì°¸ê³ ë¡œ í˜ì´ì§€ í† í°ì€ Facebook appì—ì„œ page tokenì„ ìƒì„±í•˜ì—¬ í•´ë‹¹ ê°’ì„ ì ì–´ì£¼ê³ , verify tokenì€ ì„ìœ¼ë¡œ ì§€ì •í•´ì£¼ë©´ ëœë‹¤. ì•„ë˜ì™€ ê°™ì´ Lambda Functionì„ ìˆ˜ì •í•˜ëŠ” ì´ìœ ëŠ” Facebookì˜ Webhook ì‚¬ìš©ë²•ì„ ì‚´í´ë³´ë©´ ì•Œ ìˆ˜ ìˆë‹¤. ìœ„ì—ì„œ ì§€ì •í•œ verify tokenê³¼ ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ API Gatewayë¥¼ í´ë¦­í•˜ì—¬ ì´ì „ì— invoke URLì˜ ì£¼ì†Œë¥¼ ë³µì‚¬í•˜ì—¬ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ webhook urlì„ ì¶”ê°€í•´ì¤€ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ connectionì€ ì™„ë£Œí•œ ìƒíƒœì´ë‹¤. ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ ì±—ë´‡ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ Lambda Functionì˜ else ë°‘ì˜ ë¶€ë¶„ì„ ìˆ˜ì •í•´ ë³¼ ê²ƒì´ë‹¤. ë¨¼ì € ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ Lambda Functionì€ S3ì— ì˜¬ë ¤ ê·¸ íŒŒì¼ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ requirements.txtì™€ shell scriptë¥¼ í¬í•¨í•˜ëŠ” í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤. ë˜í•œ, AWSì—ì„œ S3ì— ìƒˆë¡œìš´ bucketì„ ìƒì„±í•´ì¤€ë‹¤. í•„ìëŠ” ì•„ë˜ì™€ ê°™ì´ spotify-chat-botì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ìƒˆë¡­ê²Œ bucketì„ ë§Œë“¤ì–´ ì£¼ì—ˆë‹¤. ì „ì²´ì ì¸ êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. 1234chatbotâ”œâ”€â”€ deploy.shâ”œâ”€â”€ lambda_handler.pyâ””â”€â”€ requirements.txt deploy.sh 12345678910#!/bin/bashrm -rf ./libspip install -r requirements.txt -t ./libsrm *.zipzip spotify.zip -r *aws s3 rm s3://spotify-chat-bot/spotify.zipaws s3 cp ./spotify.zip s3://spotify-chat-bot/spotify.zipaws lambda update-function-code --function-name spotify-lambda --s3-bucket spotify-chat-bot --s3-key spotify.zip ìœ„ì™€ ê°™ì´ ì‘ì„±í–ˆë‹¤ë©´ ë¨¼ì € deploy.shì˜ íŒŒì¼ ê¶Œí•œì„ ë°”ê¿”ì¤€ë‹¤. ëª¨ë“  ì‚¬ìš©ì(a)ì˜ ì‹¤í–‰(x) ê¶Œí•œ ì¶”ê°€(+)í•˜ì—¬ ì¤€ë‹¤. 1chmod +x deploy.sh requirements.txt 12requestspymysql lambda_hendler.py 12345678910111213141516171819202122232425262728293031# -*- coding: utf-8 -*-import syssys.path.append('./libs')import loggingimport requestsimport pymysqlimport fb_botimport jsonimport base64import boto3logger = logging.getLogger()logger.setLevel(logging.INFO)PAGE_TOKEN = \"your page token\"VERIFY_TOKEN = \"your verify code\"def lambda_handler(event, context): # event['params'] only exists for HTTPS GET if 'params' in event.keys(): if event['params']['querystring']['hub.verify_token'] == VERIFY_TOKEN: return int(event['params']['querystring']['hub.challenge']) else: logging.error('wrong validation token') raise SystemExit else: logger.info(event) ìœ„ì™€ ê°™ì´ ì‘ì„±í•œ ìƒíƒœí•œ í›„ í•´ë‹¹ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” pathì—ì„œ ì•„ë˜ shell scriptë¥¼ ì‘ë™ì‹œí‚¨ë‹¤. 1./deploy.sh AWS S3ì—ì„œ í•´ë‹¹ bucketì„ í™•ì¸í•´ ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ spotify.zip íŒŒì¼ì´ ì¡´ì¬í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë‹¤ì‹œ AWS Lambda Functionìœ¼ë¡œ ëŒì•„ê°€ í•´ë‹¹ bucketê³¼ ì—°ê²°ì„ ì‹œì¼œ ë³¼ ê²ƒì´ë‹¤. Facebook Appìœ¼ë¡œ ëŒì•„ê°€ì„œ ì•„ë˜ í™”ë©´ê³¼ ê°™ì´ í•„ë“œë¥¼ ì¶”ê°€í•´ ì£¼ì–´ì•¼ í•œë‹¤. ì´ì œ webhookìœ¼ë¡œ ì—°ê²°í•´ ë†“ì€ í˜ì´ì§€ë¡œ ì ‘ì†í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ë²„íŠ¼ì„ ë§Œë“¤ì–´ ë†“ëŠ”ë‹¤. ê·¸ ì´ìœ ëŠ” Lambda Functionì˜ ë‚˜ë¨¸ì§€ ë¶€ë¶„ì„ ì‘ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ event êµ¬ì¡°ê°€ êµ¬ì„±ë˜ì–´ì ¸ ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ì œ AWS Lambda Functionì„ í†µí•´ ì–´ë–»ê²Œ messageê°€ ë“¤ì–´ì˜¤ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ pageì—ì„œ ë²„íŠ¼ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•˜ê³ , ë©”ì„¸ì§€ëŠ” ê°„ë‹¨í•˜ê²Œ helloë¥¼ ì…ë ¥í•´ë³´ì•˜ë‹¤. AWS CloudWatchì—ì„œ logë¥¼ ì‚´í´ë³´ë©´, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë°›ì•„ì˜¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ë¹¨ê°„ìƒ‰ ìƒìì•ˆì˜ key ê°’ ì¤‘ recipientëŠ” í•´ë‹¹ í˜ì´ì§€ì˜ idì´ë©°, senderì˜ idëŠ” Facebook Userì˜ idì´ë‹¤. ê³ ìœ ì˜ ê°’ì€ ì•„ë‹ˆê³  ê° í˜ì´ì§€ì— ê° Userì— ëŒ€í•œ idì´ë¯€ë¡œ ë™ì¼í•œ Userê°€ ë‹¤ë¥¸ í˜ì´ì§€ì—ì„œ ìš”ì²­ì„ í–ˆë‹¤ë©´, ë‹¤ë¥¸ idë¥¼ ê°–ëŠ”ë‹¤. appì„ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” python script íŒŒì¼ì„ fb_bot.pyë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì—ˆë‹¤. ì´ íŒŒì¼ ë˜í•œ ìœ„ì˜ lambda functionë‚´ì— ì¡´ì¬í•  ìˆ˜ ìˆë„ë¡ pathë¥¼ ì¡ì•„ì£¼ì–´ì•¼ í•œë‹¤. Facebook appì€ graph Facebook APIë¥¼ í†µí•´ì„œ controlí•  ìˆ˜ ìˆë‹¤. ì•„ë˜ íŒ¨í‚¤ì§€ ì¤‘ Enumì€ ê³ ìœ í•œ ì´ë¦„ ì§‘í•©ê³¼ ê°’ì„ ì •ì˜í•˜ëŠ” ë° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë„¤ ê°€ì§€ ì—´ê±°í˜• í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ëŠ”ë° ì‚¬ìš©ë˜ì–´ ì§„ë‹¤. ì•„ë˜í•¨ìˆ˜ì—ì„œ forë¬¸ì„ í†µí•´ NotificationTypeì„ ì‘ë™ì‹œí‚¨ë‹¤ë©´, NotificationType.REGULAR, NotificationType.SILENT_PUSH, NotificationType.no_push ì‹ìœ¼ë¡œ ê°’ì´ í”„ë¦°íŠ¸ ëœë‹¤. ì•„ë˜ íƒ¬í”Œë¦¿ì— ë§ëŠ” í˜•ì‹ì€ Facebook messenger APIì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. fb_bot.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213#!/usr/bin/env pythonimport syssys.path.append(\"./libs\")import osimport requestsimport base64import jsonimport loggingfrom enum import EnumDEFAULT_API_VERSION = 6.0## messaging types: \"RESPONSE\", \"UPDATE\", \"MESSAGE_TAG\"class NotificationType(Enum): regular = \"REGULAR\" silent_push = \"SILENT_PUSH\" no_push = \"no_push\"class Bot: def __init__(self, access_token, **kwargs): self.access_token = access_token self.api_version = kwargs.get('api_version') or DEFAULT_API_VERSION self.graph_url = 'https://graph.facebook.com/v&#123;0&#125;'.format(self.api_version) @property def auth_args(self): if not hasattr(self, '_auth_args'): auth = &#123; 'access_token': self.access_token &#125; self._auth_args = auth return self._auth_args def send_message(self, recipient_id, payload, notification_type, messaging_type, tag): payload['recipient'] = &#123; 'id': recipient_id &#125; #payload['notification_type'] = notification_type payload['messaging_type'] = messaging_type if tag is not None: payload['tag'] = tag request_endpoint = '&#123;0&#125;/me/messages'.format(self.graph_url) response = requests.post( request_endpoint, params = self.auth_args, json = payload ) logging.info(payload) return response.json() def send_text(self, recipient_id, text, notification_type = NotificationType.regular, messaging_type = 'RESPONSE', tag = None): return self.send_message( recipient_id, &#123; \"message\": &#123; \"text\": text &#125; &#125;, notification_type, messaging_type, tag ) def send_quick_replies(self, recipient_id, text, quick_replies, notification_type = NotificationType.regular, messaging_type = 'RESPONSE', tag = None): return self.send_message( recipient_id, &#123; \"message\":&#123; \"text\": text, \"quick_replies\": quick_replies &#125; &#125;, notification_type, messaging_type, tag ) def send_attachment(self, recipient_id, attachment_type, payload, notification_type = NotificationType.regular, messaging_type = 'RESPONSE', tag = None): return self.send_message( recipient_id, &#123; \"message\": &#123; \"attachment\":&#123; \"type\": attachment_type, \"payload\": payload &#125; &#125; &#125;, notification_type, messaging_type, tag ) def send_action(self, recipient_id, action, notification_type = NotificationType.regular, messaging_type = 'RESPONSE', tag = None): return self.send_message( recipient_id, &#123; \"sender_action\": action &#125;, notification_type, messaging_type, tag ) def whitelist_domain(self, domain_list, domain_action_type): payload = &#123; \"setting_type\": \"domain_whitelisting\", \"whitelisted_domains\": domain_list, \"domain_action_type\": domain_action_type &#125; request_endpoint = '&#123;0&#125;/me/thread_settings'.format(self.graph_url) response = requests.post( request_endpoint, params = self.auth_args, json = payload ) return response.json() def set_greeting(self, template): request_endpoint = '&#123;0&#125;/me/thread_settings'.format(self.graph_url) response = requests.post( request_endpoint, params = self.auth_args, json = &#123; \"setting_type\": \"greeting\", \"greeting\": &#123; \"text\": template &#125; &#125; ) return response def set_get_started(self, text): request_endpoint = '&#123;0&#125;/me/messenger_profile'.format(self.graph_url) response = requests.post( request_endpoint, params = self.auth_args, json = &#123; \"get_started\":&#123; \"payload\": text &#125; &#125; ) return response def get_get_started(self): request_endpoint = '&#123;0&#125;/me/messenger_profile?fields=get_started'.format(self.graph_url) response = requests.get( request_endpoint, params = self.auth_args ) return response def get_messenger_profile(self, field): request_endpoint = '&#123;0&#125;/me/messenger_profile?fields=&#123;1&#125;'.format(self.graph_url, field) response = requests.get( request_endpoint, params = self.auth_args ) return response def upload_attachment(self, url): request_endpoint = '&#123;0&#125;/me/message_attachments'.format(self.graph_url) response = requests.post( request_endpoint, params = self.auth_args, json = &#123; \"message\":&#123; \"attachment\":&#123; \"type\": \"image\", \"payload\": &#123; \"is_reusable\": True, \"url\": url &#125; &#125; &#125; &#125; ) return response ì´ì œ ìœ„ì˜ fb_bot.pyë¥¼ importí•˜ì—¬ lambda_hendler.py íŒŒì¼ì„ ì•„ë˜ì™€ ê°™ì´ ìˆ˜ì •í•´ ì£¼ì—ˆë‹¤. lambda_hendler.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199# -*- coding: utf-8 -*-import syssys.path.append('./libs')import loggingimport requestsimport pymysqlimport fb_botimport jsonimport base64import boto3logger = logging.getLogger()logger.setLevel(logging.INFO)client_id = \"Your Spotify ID\"client_secret = \"Your Spotify PW\"PAGE_TOKEN = \"Your Page Token\"VERIFY_TOKEN = \"Your verify token\"host = \"Your RDS End point\"port = 3306username = \"Your RDS ID\"database = \"Using RDS table name\"password = \"Your RDS PW\"try: conn = pymysql.connect(host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor()except: logging.error(\"could not connect to rds\") sys.exit(1)bot = fb_bot.Bot(PAGE_TOKEN)def lambda_handler(event, context): # event['params'] only exists for HTTPS GET if 'params' in event.keys(): if event['params']['querystring']['hub.verify_token'] == VERIFY_TOKEN: return int(event['params']['querystring']['hub.challenge']) else: logging.error('wrong validation token') raise SystemExit else: messaging = event['entry'][0]['messaging'][0] user_id = messaging['sender']['id'] logger.info(messaging) artist_name = messaging['message']['text'] query = \"SELECT image_url, url FROM artists WHERE name = '&#123;&#125;'\".format(artist_name) cursor.execute(query) raw = cursor.fetchall() # rawê°€ 0ì¸ ê²½ìš°ëŠ” DBì•ˆì—ëŠ” ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ Spotify APIì—ì„œ ì§ì ‘ searchí•œë‹¤. if len(raw) == 0: text = search_artist(cursor, artist_name) bot.send_text(user_id, text) sys.exit(0) image_url, url = raw[0] payload = &#123; 'template_type': 'generic', 'elements': [ &#123; 'title': \"Artist Info: '&#123;&#125;'\".format(artist_name), 'image_url': image_url, 'subtitle': 'information', 'default_action': &#123; 'type': 'web_url', 'url': url, 'webview_height_ratio': 'full' &#125; &#125; ] &#125; bot.send_attachment(user_id, \"template\", payload) query = \"SELECT t2.genre FROM artists t1 JOIN artist_genres t2 ON t2.artist_id = t1.id WHERE t1.name = '&#123;&#125;'\".format(artist_name) cursor.execute(query) genres = [] for (genre, ) in cursor.fetchall(): genres.append(genre) text = \"Here are genres of &#123;&#125;\".format(artist_name) bot.send_text(user_id, text) bot.send_text(user_id, ', '.join(genres)) ## ë§Œì•½ì— ì•„í‹°ìŠ¤íŠ¸ê°€ ì—†ì„ì‹œì—ëŠ” ì•„í‹°ìŠ¤íŠ¸ ì¶”ê°€ ## Spotify API hit --&gt; Artist Search ## Database Upload ## One second ## ì˜¤íƒ€ ë° ì•„í‹°ìŠ¤íŠ¸ê°€ ì•„ë‹ ê²½ìš°def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersdef insert_row(cursor, data, table): placeholders = ', '.join(['%s'] * len(data)) columns = ', '.join(data.keys()) key_placeholders = ', '.join(['&#123;0&#125;=%s'.format(k) for k in data.keys()]) sql = \"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s\" % (table, columns, placeholders, key_placeholders) cursor.execute(sql, list(data.values())*2)# ì¶”ê°€ì ìœ¼ë¡œ S3ì˜ top_tracksì—ë„ ì—…ë°ì´íŠ¸í•´ì£¼ê¸° ìœ„í•´ì„œ top-tracks lambda functionì„ ì‹¤í–‰ì‹œì¼œì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤.# payload ë¶€ë¶„ì€ lambda_handler í•¨ìˆ˜ ì•ˆì—ì„œ ë“¤ì–´ì˜¤ëŠ” eventì— ê´€í•œ ë¶€ë¶„ì´ë‹¤.def invoke_lambda(fxn_name, payload, invocation_type='Event'): lambda_client = boto3.client('lambda') invoke_response = lambda_client.invoke( FunctionName = fxn_name, InvocationType = invocation_type, Payload = json.dumps(payload) ) if invoke_response['StatusCode'] not in [200, 202, 204]: logging.error(\"ERROR: Invoking lmabda function: '&#123;0&#125;' failed\".format(fxn_name)) return invoke_responsedef search_artist(cursor, artist_name): headers = get_headers(client_id, client_secret) ## Spotify Search API params = &#123; \"q\": artist_name, \"type\": \"artist\", \"limit\": \"1\" &#125; r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) raw = json.loads(r.text) if raw['artists']['items'] == []: return \"Could not find artist. Please Try Again!\" artist = &#123;&#125; artist_raw = raw['artists']['items'][0] if artist_raw['name'] == params['q']: artist.update( &#123; 'id': artist_raw['id'], 'name': artist_raw['name'], 'followers': artist_raw['followers']['total'], 'popularity': artist_raw['popularity'], 'url': artist_raw['external_urls']['spotify'], 'image_url': artist_raw['images'][0]['url'] &#125; ) for i in artist_raw['genres']: if len(artist_raw['genres']) != 0: insert_row(cursor, &#123;'artist_id': artist_raw['id'], 'genre': i&#125;, 'artist_genres') insert_row(cursor, artist, 'artists') conn.commit() r = invoke_lambda('top-tracks', payload=&#123;'artist_id': artist_raw['id']&#125;) print(r) return \"We added artist. Please try again in a second!\" return \"Could not find artist. Please Try Again!\" ìœ„ì—ì„œ ë‹¤ë¥¸ trigger lambda functionì„ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •í•´ ì£¼ì—ˆê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì´ roleì„ ë‹¤ë¥¸ lambda functionì„ invoke í•  ìˆ˜ ìˆë„ë¡ IAM í˜ì´ì§€ì—ì„œ permissionì„ ì¶”ê°€í•´ ì£¼ì–´ì•¼ ì •ìƒì ìœ¼ë¡œ ì‘ë™ëœë‹¤. ì´ì œ í•´ë‹¹ í˜ì´ì§€ì˜ testë¥¼ ì§„í–‰í•´ ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ ìœ„ì—ì„œ ì„¤ì •í•œ ê²ƒ ì²˜ëŸ¼ í•´ë‹¹ì•„í‹°ìŠ¤íŠ¸ì˜ urlê³¼ ì¥ë¥´ë¥¼ ë³´ë‚´ì£¼ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¼ì€ ì—†ì§€ë§Œ í•„ìëŠ” 2PMë„ ê²€ìƒ‰í•´ë³´ì•˜ëŠ”ë°, ì˜ ê²€ìƒ‰ë˜ì–´ ë‚˜ì™”ë‹¤. ì´ì œê» data engineeringì— ê´€í•œ ëª‡ê°€ì§€ ê¸°ì´ˆì ì¸ ë¶€ë¶„ë“¤ì„ ì‹¤ìŠµí•´ ë³´ë©°, data engineerëŠ” ìƒí™©ì— ë§ì¶° resourceë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„ íƒê³¼ ì§‘ì¤‘ì„ í•´ì•¼ í•œë‹¤ê³  ìƒê°í–ˆë‹¤. ê·¸ëŸ¬í•œ, ìƒí™©ì— ë§ëŠ” ì„ íƒê³¼ ì§‘ì¤‘ì„ ìœ„í•´ í•´ë‹¹ ë¹„ì¦ˆë‹ˆìŠ¤ê°€ ì²˜í•´ìˆëŠ” ìƒí™©ê³¼ ë‹¨ê³„ë¥¼ ì˜ ì§„ë‹¨í•˜ê³  ê¹Šê²Œ ì•Œê³ ìˆì–´ì•¼ í•  ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì„ í•˜ê²Œ ë˜ëŠ” í”„ë¡œì íŠ¸ ì˜€ë‹¤.","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering (ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìë™í™”)","slug":"data_engineering_09","date":"2020-03-01T04:49:06.000Z","updated":"2020-03-02T18:05:24.919Z","comments":true,"path":"2020/03/01/data_engineering_09/","link":"","permalink":"https://heung-bae-lee.github.io/2020/03/01/data_engineering_09/","excerpt":"","text":"ë°ì´í„° ì›Œí¬ í”Œë¡œìš° ì´ì „ì—ë„ ì–¸ê¸‰í–ˆì—ˆë“¯ì´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì€ ì•„ë˜ì™€ ê°™ì€ ì„œë¹„ìŠ¤ë“¤ì„ S3ì— ëª¨ì•„ Athenaê°™ì€ ì„œë¹„ìŠ¤ë¡œ ë¶„ì„í•´ì¤€ ë’¤ ê·¸ ê²°ê³¼ë¥¼ ì €ì¥í•´ë†“ì€ ì¼ë ¨ì˜ ë°ì´í„° ì‘ì—…ì˜ íë¦„ì„ ì¼ì»«ëŠ”ë‹¤. í•˜ë‚˜ì˜ jobì´ ì‹œì‘ë˜ê±°ë‚˜ ì–´ë– í•œ eventì— triggerê°€ ë¬ì„ë•Œ, ë˜ ë‹¤ë¥¸ jobìœ¼ë¡œ ì—°ê²°ì´ ë˜ëŠ” ì´ëŸ° ì •ë³´ë“¤ì„ DAGs(Directed Acyclic Graphs)ë¼ê³  ë¶€ë¥¸ë‹¤. ETL ë³´í†µì€ Extract -&gt; Transform -&gt; Loadìˆœìœ¼ë¡œ ì‘ì—…ì„ í•´ ì™”ì§€ë§Œ, ìµœê·¼ì—ëŠ” Extract -&gt; Load -&gt; Transform ìˆœìœ¼ë¡œ ì‘ì—…ì„ í•˜ê¸°ë„ í•œë‹¤. ë°ì´í„° íŒŒì´í”„ ë¼ì¸ì˜ ì—°ì¥ì„ ì´ë‹¤. í•˜ë‚˜ì˜ ì˜ˆì‹œë¥¼ ë“¤ìë©´, í•˜ë£¨ì˜ ì •í•´ì§„ ì‹œê°„ì— ì˜í•œ ìŠ¤ì¼€ì¥´ë§ì¸ Amazon CloudWatch Eventì™€ Automation Scriptë¥¼ í†µí•´ì„œ machineì´ ì‹œì‘í•˜ë©´, AWSì•ˆì— AWS Step FunctionsëŠ” ê° ê³¼ì •ì—ì„œ ê·¸ ë‹¤ìŒê³¼ì •ìœ¼ë¡œì˜ ì—°ê²°ì— ëŒ€í•œ ì—¬ëŸ¬ê°€ì§€ ê²½ìš°ì˜ ìˆ˜ì— ëŒ€í•œ ë£°ì„ ì •í•´ë†“ëŠ” ì„œë¹„ìŠ¤ë¡œ ì‰½ê²Œ ë§í•˜ë©´, ì„ì˜ì˜ ë‹¨ê³„ì—ì„œ failì´ ì¼ì–´ë‚˜ë©´ ì–´ë–¤ eventë¥¼ ë°œìƒì‹œì¼œì•¼ í•˜ê³ , successë¥¼ í•˜ë©´ ì–´ë–¤ eventë¥¼ ë°œìƒì‹œì¼œì•¼ í•˜ëŠ”ì§€ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ì„œë¹„ìŠ¤ì´ë‹¤. ì´ëŸ° Step Functionì•ˆì˜ ETL Flow state machineì´ ì‹œì‘í•˜ê³ , ì´í›„ì—ëŠ” ë‹¤ì–‘í•œ jobë“¤ì´ ì‘ë™í•˜ê²Œ ëœë‹¤. ì´ëŸ¬í•œ ETL jobë“¤ì˜ logë¥¼ CloudWatchì— ì €ì¥ì„ í•˜ê³ , ì•„ë˜ì™€ ê°™ì€ Flowë¥¼ ê°–ê²Œëœë‹¤. AWSì˜ Step functionì— ê´€í•´ ì¡°ê¸ˆ ë” ë§í•˜ìë©´, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. startê°€ ë˜ë©´ jobì´ submitì´ ë˜ê³ , jobì´ finishë ë•Œ ê¹Œì§€ ê¸°ë‹¤ë ¤ ì¤„ ìˆ˜ ìˆê²Œë” Wait Secondsë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. ì™œëƒí•˜ë©´, ì˜ˆë¥¼ ë“¤ì–´ AthenaëŠ” ì–´ëŠ ì •ë„ ë¹…ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‹œìŠ¤í…œì´ê¸° ë•Œë¬¸ì— MySQLì´ë‚˜ PostgreSQLë³´ë‹¤ëŠ” ëŠë¦° ë¶€ë¶„ì´ ìˆë‹¤. ì´ëŸ° ê²½ìš° ìœ„ì™€ ê°™ì´ time sleepì„ í†µí•´ python scriptë¥¼ ì ê¹ ë©ˆì¶°ë‘ê³  ê·¸ ë‹¤ìŒì— í•´ë‹¹ ì‹œê°„ì´ ì§€ë‚¬ì„ë•Œ ê·¸ queryì— ëŒ€í•œ ê²°ê³¼ë“¤ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤. ì´í›„ì—ëŠ” ë‹¤ì‹œ job statusë¥¼ ë°›ê³  jobì´ ëë‚¬ëŠ”ì§€ ì•„ë‹Œì§€ì— ë”°ë¼ ì‘ì—…ì„ ì§„í–‰í•˜ëŠ” flowë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. ì´ëŸ° serviceë“¤ì´ ì—†ì—ˆì„ ë•ŒëŠ” í•˜ë‚˜í•˜ë‚˜ monitoringì„ í†µí•´ì„œ ìˆ˜ë™ìœ¼ë¡œ ê´€ë¦¬ë¥¼ í•´ì•¼ í–ˆë‹¤. AWS Glueê°€ ê°€ì¥ ì¢‹ì€ ë¶€ë¶„ì€ ì´ì „ì—ëŠ” MySQL ê°™ì€ ê²½ìš°ì—ëŠ” ë§Œë“¤ì–´ ë†“ì€ Schemaì— ë§ì¶°ì„œ dataë¥¼ insertí•˜ì˜€ëŠ”ë°, ì´ì œëŠ” dataê°€ ë„ˆë¬´ë‚˜ ë°©ëŒ€í•´ì§€ê³  í˜•ì‹ë„ ë‹¤ ë‹¤ë¥¸ë°, ì´ëŸ° ê²ƒë“¤ì„ í†µí•©í•˜ëŠ”ëŠ” ë‹¤ Glueí•œë‹¤ëŠ” ì˜ë¯¸ì˜ ì„œë¹„ìŠ¤ë¼ëŠ” ì ì´ë‹¤. ê°€ì¥ ë§ì´ ì“°ì—¬ì§€ëŠ” ë¶€ë¶„ ì¤‘ì— í•˜ë‚˜ê°€ Crawlerì¸ë° Crawlerë¥¼ ì‚¬ìš©í•˜ë©´ ìë™ìœ¼ë¡œ í•´ë‹¹ dataë¥¼ Crwalingí•´ì„œ dataê°€ ì–´ë–¤ í˜•ì‹ì¸\bì§€ì— ëŒ€í•´ì„œ ì§€ì†ì ìœ¼ë¡œ Schema ê´€ë¦¬ê°€ ë“¤ì–´ê°€ëŠ” ë¶€ë¶„ì´ ìˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ data ì–‘ë„ ë„ˆë¬´ë‚˜ ë§ê³  columnë„ ë„ˆë¬´ë‚˜ ë§ì€ë° columnì´ ë³€í•˜ëŠ” ê²½ìš°ë„ ìˆì„ ê²½ìš°ì— ì‚¬ìš©í•˜ë©´ ì¢‹ë‹¤. AWS Glue í˜ì´ì§€ë¥¼ ë³´ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ tableê³¼ ETL, Triggerë“± ë‹¤ì–‘í•œ ì‘ì—…ì„ í•  ìˆ˜ ìˆë‹¤. í•œ ê°€ì§€ ì˜ˆì‹œë¡œ S3ì— ì €ì¥í•´ë†“ì€ python Scriptë¥¼ Jobs íƒ­ì—ì„œ ë°”ë¡œ ìˆ˜ì •ê°€ëŠ¥í•˜ë©°, Triggerë“¤ë„ ë“±ë¡í•´ì„œ ê´€ë¦¬ í•  ìˆ˜ ìˆë‹¤. í•´ë‹¹ jobë“¤ì€ step functionì´ë‚˜ Glueë¥¼ í†µí•´ ê´€ë¦¬ë¥¼ í•˜ê±°ë‚˜, EC2ì—ì„œ Crontabìœ¼ë¡œ ìŠ¤ì¼€ì¥´ë§ì˜ ë³€í™”ë¥¼ í†µí•´ì„œ ê´€ë¦¬ë¥¼ í•˜ëŠ” ë“± ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ê´€ë¦¬ë¥¼ í•˜ì§€ë§Œ ì•„ë˜ì™€ ê°™ì´ ì„œë¹„ìŠ¤ë“¤ì˜ ì§€ì†ì ì¸ monitoringì„ í†µí•´ costë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ì„ íƒê³¼ ì§‘ì¤‘ì„ í•´ì•¼ í•  ê²ƒì´ë‹¤. ì–´ë–¤ ë¶€ë¶„ê¹Œì§€ monitoringì„ í•  ê²ƒì¸ì§€ì— ëŒ€í•´ ì„ íƒí•˜ì—¬ ì§‘ì¤‘í•˜ëŠ” ê²ƒì´ë‹¤. Crontabì´ë€? ì—¬ëŸ¬ê°€ì§€ ë§Œë“¤ì–´ì§„ data jobë“¤ì„ ìš°ë¦¬ê°€ ì‘ì„±í•œ í•´ë‹¹ ì½”ë“œë¥¼ íŠ¹ì •í•œ ì‹œê°„ì´ë‚˜ í•˜ë£¨ì— í•œë²ˆ ì¼ì£¼ì¼ì— í•œë²ˆì´ ëê±´ ì–´ë– í•œ ìŠ¤ì¼€ì¥´ë§ ë°©ë²•ì„ í†µí•´ì„œ ì§€ì†ì ìœ¼ë¡œ ì‘ë™ì‹œí‚¤ë ¤ê³  í• ë•Œ ì‚¬ìš©í•œë‹¤. Crontab Quick reference ì´ëŸ¬í•œ ìŠ¤ì¼€ì¥´ë§ì„ í•˜ë ¤ë©´ ê·œì¹™ì—ì˜í•œ ëª…ë ¹ì´ ì¡´ì¬í•  ê²ƒì„ì„ ëˆˆì¹˜ì±˜ì„ ê²ƒì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ëª¨ë‘ â€˜*â€˜ì´ë©´ ê³„ì† 1ë¶„ë§ˆë‹¤ ì‘ë™í•˜ë¼ëŠ” ì˜ë¯¸ì´ë©°, ìˆœì„œëŒ€ë¡œ ë¶„, ì‹œê°„, ì¼, ì›”, ìš”ì¼ ìˆœìœ¼ë¡œ ì§€ì •í•  ìˆ˜ ìˆë‹¤. í•œê°€ì§€ ê°„ë‹¨í•œ ì˜ˆë¡œëŠ” ì•„ë˜ì™€ ê°™ì´ Crontabì„ ì‹¤í–‰í•˜ë©´ ë§¤ì¼ ì˜¤í›„ 6ì‹œ 30ë¶„ì— /home/someuser/tmpì•ˆì˜ ëª¨ë“  íŒŒì¼ì„ ì œê±°í•˜ë¼ëŠ” jobì„ ìŠ¤ì¼€ì¥´ë§í•  ìˆ˜ ìˆë‹¤. 130 18 * * * rm /home/someuser/tmp/* ì´ì œ Crontabì„ ì‹¤ìŠµí•´ ë³´ê¸° ìœ„í•´ EC2 ì„œë²„ë¥¼ í•˜ë‚˜ AWSì—ì„œ ìƒì„±í•´ ë³¼ ê²ƒì´ë‹¤. ë¨¼ì € service íƒ­ì—ì„œ EC2ë¥¼ í´ë¦­í•˜ì—¬ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ EC2 service í˜ì´ì§€ë¡œ ì´ë™í•œ í›„ ë‹¤ì‹œ instance íƒ­ìœ¼ë¡œ ì´ë™í•œë‹¤. Launch instance ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ìƒì„±í•  instanceì˜ í™˜ê²½ì„ ì„¤ì •í•˜ëŠ” í˜ì´ì§€ë¡œ ì´ë™í•˜ê²Œëœë‹¤. Crontab ì‹¤ìŠµì„ ìœ„í•œ ì„œë²„ì´ê¸° ë•Œë¬¸ì— ì‚¬ì–‘ì´ ì¢‹ì€ ê²ƒì„ ê³ ë¥´ì§„ ì•Šì„ ê²ƒì´ë‹¤. í•„ìëŠ” Free tierë§Œ ì‚¬ìš©ê°€ëŠ¥í•œ 2ë²ˆì§¸ ì‚¬ì–‘ì„ ê³ ë¥¼ ê²ƒì´ë‹¤. ë‹¤ë§Œ, ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•  ê²½ìš°ëŠ” ê°ìì˜ ìƒí™©ì— ë§ëŠ” ì„œë²„ì˜ í¬ê¸°ì™€ ì„±ëŠ¥ì„ ê³¨ë¼ì„œ ì‚¬ìš©í•´ì•¼ í•  ê²ƒì´ë‹¤. í•„ìëŠ” Free Tierë§Œ ê°€ëŠ¥í•œ t2.micro typeì„ ì„ íƒí•˜ì˜€ë‹¤. Configure instance íƒ­ì—ì„œëŠ” ê°œìˆ˜ë¥¼ ì§€ì •í•˜ëŠ” ë“± instanceì— ëŒ€í•œ ì„¤ì •ì„ í•˜ëŠ” íƒ­ì¸ë°, í•„ìëŠ” defaultê°’ì„ ì‚¬ìš©í•˜ê¸°ë¡œ ìƒê°í•´ì„œ ì•„ë¬´ëŸ° ì„¤ì •ë„ í•˜ì§€ì•Šê³  ë„˜ì–´ê°”ë‹¤. Add storage íƒ­ì€ ë§ ê·¸ëŒ€ë¡œ ì €ì¥ ì„±ëŠ¥ì„ ì„¤ì •í•˜ëŠ” íƒ­ìœ¼ë¡œ í•„ìëŠ” ì´ë¶€ë¶„ë„ ë³„ë‹¤ë¥¸ ì„¤ì •ì—†ì´ ê¸°ë³¸ê°’ìœ¼ë¡œ í•˜ê³  ë„˜ì–´ê°”ë‹¤. tagë¥¼ ì„¤ì •í•˜ëŠ” íƒ­ì´ë©°, í•„ìëŠ” ìƒëµí•˜ì˜€ë‹¤. Configure Security Group íƒ­ì€ Security Groupì˜ ì„¤ì •ì— ê´€í•œ íƒ­ì´ë©°, ìƒˆë¡œ ë§Œë“¤ìˆ˜ë„ ìˆê³ , ì´ì „ì— ìƒì„±ë˜ì–´ìˆëŠ” ê·¸ë£¹ì„ ì‚¬ìš©í•´ë„ ë¬´ë°©í•˜ë‹¤. í—ˆë‚˜, ë™ì¼í•œ inbound ê·œì¹™ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ìƒˆë¡­ê²Œ ìƒì„±í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. í•„ìëŠ” ìƒˆë¡­ê²Œ ë§Œë“¤ì–´ì„œ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ë˜í•œ, ì ‘ê·¼ì„ sshë¡œ í•  ê²ƒì´ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í•œë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ Review íƒ­ì€ ì´ì œê¹Œì§€ ì„¤ì •í•œ ëª¨ë“  ì‚¬í•­ì„ ì ê²€í•˜ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ ì ‘ì†ì‹œ ì‚¬ìš©í•  key pairë¥¼ ì–´ë–¤ê²ƒìœ¼ë¡œ í• ì§€ ì •í•´ì£¼ê³  ë‚˜ë©´ ëª¨ë“  ê³¼ì •ì´ ëì´ ë‚œë‹¤. ì´ì œ ë‹¤ì‹œ EC2ì˜ instance íƒ­ì„ ì‚´í´ë³´ë©´, ìƒˆë¡­ê²Œ EC2 ì„œë²„ê°€ ìƒì„±ëœ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ì´ì œ ì ‘ì†ì„ í•´ë³¼ ê²ƒì¸ë°, ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ ìì‹ ì˜ pem íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” pathì—ì„œ public DNS ì„œë²„ ì£¼ì†Œë¥¼ ê°™ì´ ì…ë ¥í•˜ê³  ì ‘ì†í•˜ë©´ ëœë‹¤. ì•„ë˜ì™€ ê°™ì´ commandë¥¼ ì‹¤í–‰í•˜ë©´ ê³„ì†ì§„í–‰í•  ê²ƒì´ëƒëŠ” ë¬¼ìŒì´ ë‚˜ì˜¬í…ë° yesë¼ê³  í•˜ë©´ ëœë‹¤. ì´ëŠ” ì•ìœ¼ë¡œ ê³„ì†í•´ì„œ ì ‘ì†í•˜ê¸°ìœ„í•´ ì´ DNSë¥¼ ì¶”ê°€í•  ê²ƒì´ëƒëŠ” ë¬¼ìŒì´ë‹¤. 1ssh -i pem_file_name.pem ec2-user@Public_DNS ì´ì œ Crontabì„ ì‹¤í–‰í•  script íŒŒì¼ì„ ì •í•´ì•¼ í•˜ëŠ”ë°, í•„ìëŠ” ì´ì „ì— ë§Œë“¤ì–´ ë‘ì—ˆë˜ script íŒŒì¼ ì¤‘ì— top_tracksì™€ audio featureë“¤ì— ëŒ€í•œ ë°ì´í„°ë¥¼ S3ì— parquetí™”í•˜ì—¬ ì €ì¥í•˜ê²Œë” ì½”ë“œë¥¼ ì‘ì„±í•œ íŒŒì¼ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ì•„ë˜ì™€ ê°™ì´ í•„ìì˜ pem íŒŒì¼ì€ data_engineeringì´ë¼ëŠ” íŒŒì¼ ì•ˆì— ì¡´ì¬í•œë‹¤. 123456789101112131415161718192021data_engineeringâ”œâ”€â”€ Codeâ”‚ â”œâ”€â”€ Chatbot\\ Projectâ”‚ â”‚ â”œâ”€â”€ deploy.shâ”‚ â”‚ â”œâ”€â”€ fb_bot.pyâ”‚ â”‚ â”œâ”€â”€ lambda_handler.pyâ”‚ â”‚ â””â”€â”€ requirements.txtâ”‚ â”œâ”€â”€ artist_list.csvâ”‚ â”œâ”€â”€ audio_features.parquetâ”‚ â”œâ”€â”€ create_artist_genres_table.pyâ”‚ â”œâ”€â”€ create_artist_table.pyâ”‚ â”œâ”€â”€ dynamodb_insert.pyâ”‚ â”œâ”€â”€ select_dynamodb.pyâ”‚ â”œâ”€â”€ spotify_S3.pyâ”‚ â”œâ”€â”€ spotify_s3_artist.pyâ”‚ â”œâ”€â”€ spotify_s3_make.pyâ”‚ â”œâ”€â”€ top-tracks.parquetâ”‚ â”œâ”€â”€ Slideâ”œâ”€â”€ foxyproxy-settings.xmlâ””â”€â”€ spotift_chatbot.pem ìœ„ì™€ ê°™ì€ êµ¬ì¡°ë¡œ ë˜ì–´ìˆìœ¼ë¯€ë¡œ ì‚¬ìš©í•  spotify_s3_make.pyì„ ìƒì„±í•œ EC2 Serverë¡œ ì˜®ê²¨ ì¤„ ê²ƒì´ë‹¤. scpì€ ì„œë²„ë¡œ íŒŒì¼ì„ copyí•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. ë‹¨,ì•„ë˜ ëª…ë ¹ì–´ëŠ” ì´ë¯¸ EC2ì— ì ‘ì†í•œ ìƒíƒœê°€ ì•„ë‹Œ ë¡œì»¬ì—ì„œ ì§„í–‰í•˜ì—¬ì•¼í•œë‹¤. ì•„ë˜ì™€ ê°™ì´ ì˜®ê²¨ì§„ê²ƒì„ ë³¼ ìˆ˜ ìˆê³ , EC2 ì„œë²„ë¡œ ì ‘ì†í•˜ì—¬ í™•ì¸ê°€ëŠ¥í•˜ë‹¤. 12345scp -i spotift_chatbot.pem ./Code/spotify_s3_make.py ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.com:~/spotify_s3_make.py 100% 5411 536.2KB/s 00:00ssh -i spotift_chatbot.pem ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.comls ë§Œì¼ EC2 ì„œë²„ì˜ ì–´ë–¤ í´ë”ë¥¼ ë§Œë“¤ì–´ ë†“ì•˜ëŠ”ë° í•´ë‹¹ í´ë”ì•ˆìœ¼ë¡œ ì´ë™ì‹œí‚¤ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì´ í•˜ë©´ëœë‹¤. 1scp -i spotift_chatbot.pem ./Code/spotify_s3_make.py ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.com:~/íŒŒì¼ì´ë¦„ ì´ì œ scriptë¥¼ ì‹¤í–‰í•˜ê¸°ì— ì•ì„œì„œ ì‘ë™í•  ìˆ˜ ìˆê²Œë” ë¨¼ì € í™˜ê²½ì„ ë§Œë“¤ì–´ì£¼ì–´ì•¼ í•  ê²ƒì´ë‹¤. ê°€ì¥ ë¨¼ì € python3ê°€ ì„¤ì¹˜ë˜ì–´ìˆëŠ”ì§€ë¥¼ í™•ì¸í•´ ë³´ì. 123456789101112131415161718sudo yum list | grep python3# ìœ„ì˜ ëª…ë ¹ë¬¸ì„ ì‹¤í–‰í•œ í›„ python3xì— ê´€í•œ ë¦¬ìŠ¤íŠ¸ê°€ ë‚˜ì˜¨ë‹¤ë©´ python3ê°€ ì„¤ì¹˜ë˜ì–´ìˆëŠ” ìƒíƒœì´ë‹¤.# í—ˆë‚˜ ì˜ ëª¨ë¥´ê² ë‹¤ë©´ ì•„ë˜ ëª…ë ¹ì–´ë¥¼ í†µí•´ ì„¤ì¹˜í•˜ë„ë¡í•˜ì.sudo yum install python36 -y# ê·¸ ë‹¤ìŒì€ pipë¥¼ ì„¤ì¹˜í•´ì•¼í•œë‹¤curl -O https://bootstrap.pypa.io/get-pip.pysudo python3 get-pip.py#ì´ì œ scriptíŒŒì¼ì„ runí•´ë³¸ í›„ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ì„ ì¶”ê°€ë¡œ ì„¤ì¹˜í•´ì¤€ë‹¤.python3 spotify_s3_make.pypip install boto3 --userpip install requests --userpip install pymysql --userpip install pandas --userpip install jsonpath --user ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ crontabì„ ì‹¤ìŠµí•´ ë³¼ ê²ƒì´ë‹¤. ë³´í†µ crontabì€ ì•„ë˜ ë‘ ê°€ì§€ë¥¼ ë§ì´ ì‚¬ìš©í•œë‹¤. crontab íŒŒì¼ì€ vim editorë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, í•´ë‹¹ ìŠ¤ì¼€ì¥´ì„ ì‹œì‘í• ë•Œ emailì„ ë³´ë‚´ì£¼ëŠ” ê¸°ëŠ¥ë„ ìˆë‹¤. ìš°ì„  ì‘ë™ì‹œí‚¤ê³  ì‹¶ì€ íŒŒì¼ê³¼ íŒŒì´ì¬ì˜ ìœ„ì¹˜ë¥¼ ì•Œê³ ìˆì–´ì•¼ í•œë‹¤. UTC ì‹œê°„ ë³€ê²½ 123456789pwd# ê²°ê³¼/home/ec2-userwhich python3# ê²°ê³¼/usr/bin/python3 12345678910111213141516sudo service crond startcrontab -l# crontab íŒŒì¼ ì“°ëŠ” vimì´ ì¼œì§„ í›„# ë§¤ì¼ 18ì‹œ 30ë¶„ì—30 18 * * * /usr/bin/python3 /home/ec2-user/spotify_s3_make.py# vimìœ¼ë¡œ ì €ì¥í•œ í›„ ë‚˜ì˜¤ë©´ ì•„ë˜ì™€ ê°™ì€ ë©”ì„¸ì§€ê°€ ë‚˜ì™€ì•¼ ì •ìƒì ìœ¼ë¡œ crontabì„ ì„¤ì •í•œ ê²ƒì´ë‹¤.crontab: installing new crontab# ì–´ë– í•œ ì‚¬í•­ì„ crontabìœ¼ë¡œ ìŠ¤ì¼€ì¥´ë§í•˜ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ê¸°crontab -l# ì„œë²„ëŠ” UTCë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— date ëª…ë ¹ì–´ë¥¼ í†µí•´ í˜„ì¬ ì„œë²„ê°€ ëª‡ì‹œì¸ì§€ë¥¼ í™•ì¸í•˜ê³  ìš°ë¦¬ë‚˜ë¼ ì‹œê°„ê³¼ ë§ì¶”ë„ë¡ crontabì„ ì„¤ì •í•´ ì£¼ì–´ì•¼ í•œë‹¤.date ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ì— ëŒ€í•œ ì´í•´ ì „ì²´ì ì¸ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì„¤ëª…ì„ ë‹¤ì‹œ í•œë²ˆ ì§šê³ ê°€ìë©´, Unknown Artistê°€ ì±—ë´‡ ë©”ì„¸ì§€ë¡œ ë“¤ì–´ì™”ì„ ê²½ìš° AWS Serverless Lamda ì„œë¹„ìŠ¤ë¥¼ í†µí•´ Spotify APIì— Accessë¥¼ í•˜ê³  ê·¸ë¦¬ê³  í•´ë‹¹ ë°ì´í„°ê°€ Top Tracks, Artist Tableì— ê°€ì•¼ë˜ëŠ”ì§€ ë˜ëŠ” S3ì— ê°€ì•¼ë˜ëŠ”ì§€ë¥¼ ê´€ë¦¬ë¥¼ í•˜ê²Œ ë  ê²ƒì´ë‹¤. ë˜í•œ, Ad Hoc Data Jobì„ í†µí•´ í•˜ë£¨ì— í•œë²ˆì´ë¼ë˜ì§€, ì§ì ‘ ë¡œì»¬ì—ì„œ command lineì„ í†µí•´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆê²Œëœë‹¤. Lambdaê°€ í•„ìš”í•œ ì´ìœ ëŠ” ìš°ë¦¬ê°€ Unknown Artistê°€ ì±—ë´‡ ë©”ì„¸ì§€ë¡œ ë“¤ì–´ì™”ì„ë•Œ ë‚´ìš©ì„ ì—…ë°ì´íŠ¸ë¥¼ í•´ì•¼ë˜ëŠ”ë°, ë³´í†µ ì‚¬ëŒë“¤ì´ ê¸°ëŒ€í•˜ëŠ” ì±—ë´‡ì€ ì—…ë°ì´íŠ¸ë¥¼ ë°”ë¡œ í•´ì£¼ì–´ì„œ ì›í•˜ëŠ” ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆê²Œë” í•´ì£¼ì–´ì•¼ í•˜ê¸°ì— ì´ë ‡ê²Œ ë°”ë¡œ ì—…ë°ì´íŠ¸ë¥¼ í•  ìˆ˜ ìˆê²Œ\u001dë” Lambdaë¼ëŠ” ì„œë¹„ìŠ¤ë¥¼ í†µí•´ì„œ í•´ê²° í•  ìˆ˜ ìˆë‹¤. ì´ëŸ° LambdaëŠ” ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ì˜ ê°œë…ì¸ë°, monolithic ì´ë¼ëŠ” ê°œë…ì˜ ë°˜ëŒ€ì´ë‹¤. monolithicì€ í•˜ë‚˜ì˜ ì„œë¹„ìŠ¤ë¥¼ ë§Œë“¤ë•Œ í¬ê²Œ í”„ë¡œì íŠ¸ ë‹¨ìœ„ë¡œ ë§Œë“¤ì–´ ë†“ê³  ê´€ë¦¬ë¥¼ í•´ì£¼ëŠ” ê°œë…ìœ¼ë¡œì¨ ê´€ë¦¬ì— ìˆì–´ì„œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ê°€ ë³´ì´ê¸° ë•Œë¬¸ì— ì»¨íŠ¸ë¡¤í•˜ê¸° ì‰¬ìš´ ë¶€ë¶„ì´ ìˆë‹¤. ì´ì— ë°˜í•´ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ëŠ” ì„¸ì„¸í•œ ì‘ì—…í•˜ë‚˜ì”©ì„ ë‹¨ìœ„ë¡œ ê´€ë¦¬ë¥¼ í•˜ëŠ” ê²ƒì´ë‹¤. ì±—ë´‡ì„ Lambdaë¡œ êµ¬í˜„í•˜ëŠ” ì´ìœ ëŠ” ServerlessëŠ” í•˜ë‚˜ì˜ Functionì´ê¸° ë•Œë¬¸ì— Statelessë¼ê³ ë„ í•˜ëŠ”ë° ì§€ê¸ˆ ìƒíƒœê°€ ì–´ë–¤ì§€ ëª¨ë¥´ê² ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì˜ˆë¥¼ë“¤ì–´, ì–´ë– í•œ User\u001c ì–´ë–¤ ë©”ì‹œì§€ë¥¼ ë³´ëƒˆë‹¤ê³  ê°€ì •í•˜ë©´, Lambda Functionì—ëŠ” ì´ì „ì˜ ì–´ë– í•œ ë©”ì‹œì§€ë¥¼ ê°–ê³  ìˆì—ˆëŠ”ì§€ë¥¼ ë‹´ê³ \u001cìˆì„ ìˆ˜ ì—†ë‹¤. ìƒíƒœê°€ ì—†ëŠ” Functionì´ë¼ê³  ìƒê°í•˜ë©´ ë  ê²ƒ ê°™ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì´ëŸ° Stateë¥¼ ê´€ë¦¬í• ë§Œí•œ ë°ì´í„°ë² ì´ìŠ¤ê°€ í•„ìš”í•  ê²ƒì´ë‹¤. ì£¼ë¡œ ë©”ì‹œì§€ì— íŠ¹í™”ëœ DynamoDBë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ë˜í•œ Lambdaì˜ ê²½ìš°ì—ëŠ” í•´ë‹¹ ì„œë¹„ìŠ¤ì˜ Userê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚¬ì„ ë•Œ ë³‘ë ¬ë¡œ ëŠ˜ì–´ë‚˜ê¸° ë•Œë¬¸ì— ì œí•œì ì´ ì„œë²„ë¡œ êµ¬í˜„í•˜ëŠ”ê²ƒë³´ë‹¨ ëœí•˜ë‹¤ëŠ” ì¥ì ë„ ìˆë‹¤. ì„œë²„ì˜ ê²½ìš°ì—ëŠ” ë©”ëª¨ë¦¬ë‚˜ CPUì˜ ì œí•œëœ ì„±ëŠ¥ìœ¼ë¡œ êµ¬ì¶•ëœ ë™ì¼í•œ ì„œë²„ë¥¼ í†µí•´ 1ëª…ì—ê²Œ ì„œë¹„ìŠ¤í•˜ëŠ” ê²ƒê³¼ ë°±ë§Œëª…ì—ê²Œ ì„œë¹„ìŠ¤í•˜ëŠ” ê²ƒì€ ì™„ì „ ë‹¤ë¥¼ê²ƒì´ë‹¤. ë˜ í•œê°€ì§€ ì¢‹ì€ ì ì€ ì§€ì†ì ìœ¼ë¡œ ë„ì›Œì ¸ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•„ìš”í•  ë•Œ ë„ì›Œì„œ ì‚¬ìš©í•œ ë§Œí¼ë§Œ ë¹„ìš©ì„ ì§€ë¶ˆí•œë‹¤ëŠ” ì ì´ë‹¤. Crontabì„ í†µí•˜ì—¬ Lambdaë¥¼ í˜¸ì¶œí•  ìˆ˜ë„ ìˆê³ , Lambdaê°€ Lambdaë¥¼ í˜¸ì¶œí•  ìˆ˜ë„ ìˆë‹¤. ë³¸ê²©ì ìœ¼ë¡œ Lambda Functionì„ ë§Œë“¤ ê²ƒì´ë‹¤. ë¨¼ì € AWS Lambda Function í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ Create Function ë²„íŠ¼ì„ í´ë¦­í•œë‹¤. ì´ë²ˆ Lambda Functionì€ ì´ì „ì— DynamoDBì— top trackì •ë³´ë¥¼ DynamoDBì— ì €ì¥í–ˆì—ˆëŠ”ë°, Artistê°€ ì¶”ê°€ëœë‹¤ë©´ DynamoDBì—ë„ ì €ì¥ë˜ì–´ì•¼í•˜ë¯€ë¡œ ì´ ì‘ì—…ì„ ì‘ì„±í•´ ë³¼ ê²ƒì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ Author from scratchëŠ” ê¸°ë³¸ì ì¸ ì˜ˆì œë¥¼ í†µí•´ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ì´ê³ , Use a blueprintëŠ” í”í•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ê²½ìš°ë“¤ì„ ì½”ë“œë¡œ ì œê³µí•˜ëŠ” ë¶€ë¶„ì´ë‹¤. ë§ˆì§€ë§‰ì€ App repositoryì—ì„œ ë°”ë¡œ ì—°ê²°í•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. í•„ìëŠ” Scratchë¡œ ì§„í–‰í•  ê²ƒì´ë‹¤. LambdaëŠ” í•˜ë‚˜ì˜ Functiondì´ë¯€ë¡œ ì œí•œì ë„ ìˆì„ ê²ƒì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ Functionì˜ ì´ë¦„ì„ ì •í•˜ê³  functionì˜ languageë¥¼ ì •í•œë‹¤. ë˜í•œ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì¸ Permissionë¶€ë¶„ì´ ë‚¨ì•˜ëŠ”ë°, ì´ ë¶€ë¶„ì€ í˜„ì¬ í•„ìê°€ ì§„í–‰í•  Functionì˜ ëª©í‘œëŠ” DynamoDBì— ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ëŠ” ê²ƒì´ë¯€ë¡œ DynamoDBì— ëŒ€í•œ permissionì„ ê°–ê³  ìˆì–´ì•¼ ì˜¤ë¥˜ê°€ ì—†ì„ ê²ƒì´ë‹¤. ê·¸ë ‡ê¸°ì— 2ë²ˆì§¸ ë¶€ë¶„ì¸ Use an exsiting roleì„ í´ë¦­í•˜ì—¬ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ë°, ìƒˆë¡­ê²Œ ê·œì¹™ì„ ì¶”ê°€í•´ì„œ ì‚¬ìš©í•˜ë©´ errorê°€ ì–´ë–»ê²Œ ë°œìƒí•˜ëŠ” ì§€ë¥¼ ë³´ê¸°ìœ„í•´ ìš°ì„  ì²«ë²ˆì§¸ë¥¼ ì„ íƒí•˜ì˜€ë‹¤. ë¬¼ë¡  ë¬´ì¡°ê±´ì ìœ¼ë¡œ ìˆë˜ ê·œì¹™ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ìƒí™©ì— ë§ì¶° ì‚¬ìš©í•´ì•¼í•œë‹¤. functionì„ ìƒì„±í•˜ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì—¬ëŸ¬ê°€ì§€ ì„¤ì • ë° ì‘ì—…ì„ í•  ìˆ˜ ìˆëŠ” í˜ì´ì§€ê°€ ë‚˜ì˜¨ë‹¤. ì•„ë˜ ë¶€ë¶„ìœ¼ë¡œ ë‚´ë ¤ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ Functionì˜ ì½”ë“œë¥¼ ì‘ì„±í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì´ ì¡´ì¬í•œë‹¤. ì´ ê³³ì—ì„œ Functionì„ ì •ì˜í•  ê²ƒì´ë‹¤. í—ˆë‚˜, Edit code inlineì€ ê±°ì˜ ì‚¬ìš©í•˜ì§€ ëª»í•˜ëŠ” ì„¤ì •ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. í•´ë‹¹ Lambda Functionì€ Linux ê¸°ë°˜ì˜ AMI compute systemì— ìˆëŠ”ë°, í•¨ìˆ˜ë¥¼ ë™ì‘í•  ìˆ˜ ìˆëŠ” íŒ¨í‚¤ì§€ë“¤ì´ ì•„ë¬´ëŸ° ì„¤ì¹˜ë‚˜ ì„¤ì •ì´ ë˜ì–´ìˆì§€ ì•Šì€ ìƒíƒœì´ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë˜ì„œ zip fileì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ S3ì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹ì„ ë³´í†µ ì±„íƒí•œë‹¤. í•„ìëŠ” S3ì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ìœ„ì—ì„œ í•¨ìˆ˜ì— ì‚¬ìš©ë  ë³€ìˆ˜ë“¤ì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, Spotify APIì— ì ‘ì†í•˜ê¸° ìœ„í•´ì„œëŠ” IDì™€ Secret Keyê°€ í•„ìš”í–ˆëŠ”ë° ì´ ë¶€ë¶„ì„ ì½”ë“œì— ì ê¸° ë³´ë‹¤ëŠ” ë³´ì•ˆì˜ ë¬¸ì œë¡œ ë”°ë¡œ ë³€ìˆ˜ë¡œ ì²˜ë¦¬í•´ ë‘” ë’¤ í•¨ìˆ˜ì—ëŠ” ê·¸ ê°’ì„ ë°›ì•„ ì‚¬ìš©í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ì•„ë˜ ê·¸ë¦¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ì˜ Editì„ í´ë¦­í•˜ë©´ ë³€ìˆ˜ì˜ Keyì™€ Valueë¥¼ ì…ë ¥í•˜ì—¬ ì¶”ê°€í•˜ê²Œë” ë˜ì–´ìˆëŠ”ë° ì¶”ê°€í–ˆë‹¤ê³  ê°€ì •í•˜ë©´ í•¨ìˆ˜ì—ì„œëŠ” os.environ.get(key)ë¡œ ê°’ì„ ë°›ìœ¼ë©´ ëœë‹¤. ë˜í•œ Basic settingsë¶€ë¶„ì€ Memory(Max : 3GB)ì™€ timeout(Max : 15ë¶„)ì„ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì¸ë°, ê°€ì¥ ê°„ë‹¨í•˜ê³  ëª…ë£Œí•˜ê²Œ ë³‘ë ¬ì ìœ¼ë¡œ ë¶„ì‚°ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆë„ë¡ ì½”ë“œë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ëŠ” Lambdaì˜ íŠ¹ì§•ì„ ì˜ ë³´ì—¬ì£¼ëŠ” ë¶€ë¶„ì´ë‹¤. ì—¬ê¸°ì„œ MemoryëŠ” í¬ê²Œ í•´ ë†“ì•„ë„ ì‚¬ìš©í•œ ë§Œí¼ë§Œ ë¹„ìš©ì„ ì§€ë¶ˆí•˜ëŠ” ê²ƒì´ë¯€ë¡œ ìƒê´€ì—†ë‹¤ëŠ” ê²ƒì— ìœ ì˜í•˜ì. ì´ë¯¸ í•´ë‹¹ Artistê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì€ í™•ì¸í•œ ìƒíƒœì—¬ì„œ í•´ë‹¹ Artist IDì— ëŒ€í•œ top trackë§Œ í™•ë³´í•˜ê³  ì‹¶ì€ ê²½ìš°ë¼ê³  ê°€ì •í•  ê²ƒì´ë‹¤. ê·¸ ID ê°’ì„ ì´ Lambda Functionì— ë³´ë‚´ ì¤Œìœ¼ë¡œì¨ ë‹¤ì‹œ í•œë²ˆ Spotify APIì— hitì„ í•˜ì—¬ top track ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê³  ë‹¤ì‹œ DynamoDBì— ì €ì¥í•˜ë„ë¡ í•  ê²ƒì´ë‹¤. ë³¸ê²©ì ìœ¼ë¡œ Lambda Functionì„ ë§Œë“¤ì–´ ë³¼ ê²ƒì´ë‹¤. ìš°ì„  ìƒˆë¡œìš´ í´ë”ë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤. ê·¸ ì•ˆì—ëŠ” Lambda Functionì— ê´€í•œ ê²ƒë“¤ë§Œ ë‹´ê¸° ìœ„í•´ì„œì´ë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰ í•œ ê²ƒì²˜ëŸ¼ ìš°ì„  ì•„ë˜ íŒ¨í‚¤ì§€ë“¤ì„ shell scriptë¥¼ í†µí•´ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œ requirements.txtë¥¼ ì‘ì„±í•  ê²ƒì´ë‹¤. í—Œë° AWSì—ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ boto3ê°€ ì„¤ì¹˜ë˜ì–´ì ¸ ìˆê³  ë‚˜ë¨¸ì§€ë“¤ì€ ê¸°ë³¸ ë‚´ì¥ íŒ¨í‚¤ì§€ë“¤ì´ë¯€ë¡œ requestsë§Œ ì„¤ì¹˜í•´ ì£¼ë©´ ë  ê²ƒ ê°™ë‹¤. requirements.txt 1requests ìœ„ì—ì„œ ë§Œë“¤ì–´ ë†“ì€ requirements.txtì•ˆì˜ íŒ¨í‚¤ì§€ë¥¼ -tì˜µì…˜ì˜ target íŒŒì¼ì— ì €ì¥í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. í—Œë° ì´ë ‡ê²Œ homeì´ ì•„ë‹Œ íŒŒì¼ì— ì €ì¥í•˜ê²Œ ë˜ë©´ pointer issue ë•Œë¬¸ì— errorê°€ ë°œìƒí•˜ëŠ”ë° ì´ëŠ” ìƒˆë¡­ê²Œ setup.cfgë¼ëŠ” íŒŒì¼ì•ˆì— ì•„ë˜ì™€ ê°™ì´ ì‘ì„±í•œ í›„ì— ì €ì¥í•´ì£¼ë©´ í•´ê²°ëœë‹¤. ë‹¤ì‹œ ì•„ë˜ì˜ ëª…ë ¹ë¬¸ì„ ì‹¤í–‰í•˜ë©´ error ì—†ì´ libsì•ˆì— ì„¤ì¹˜ê°€ ë  ê²ƒì´ë‹¤. 1pip install -r requirements.txt -t ./libs setup.cfg 12[install]prefix= ë§¤ë²ˆ ìš°ë¦¬ê°€ AWS CLIë¥¼ í†µí•´ ëª…ë ¹ë¬¸ì„ ì¹˜ê³  ì‹¤í–‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ shell scriptë¡œ ì‘ì„±í•´ ì¤€ë‹¤. deploy.sh 1234567891011#!/bin/bashrm -rf ./libspip3 install -r requirements.txt -t ./libsrm *.zipzip top_tracks.zip -r *aws s3 rm s3://top-tracks-lambda/top_tracks.zipaws s3 cp ./top_tracks.zip s3://top-tracks-lambda/top_tracks.zipaws lambda update-function-code --function-name top-tracks --s3-bucket top-tracks-lambda --s3-key top_tracks.zip ë˜í•œ, ìœ„ì—ì„œ lambda functionì„ updateí•  s3 bucketì„ ì§€ì •í–ˆìœ¼ë¯€ë¡œ ìƒˆë¡­ê²Œ ìœ„ì˜ ì´ë¦„ìœ¼ë¡œ ìƒì„±í•œë‹¤. ì´ ë‹¨ê³„ëŠ” ë¨¼ì € bucketì„ ë§Œë“ ë’¤ì— ì•ì˜ shell scriptë¥¼ ë§Œë“¤ì–´ë„ ë¬´ê´€í•˜ë‹¤. lambda_function.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import syssys.path.append('./libs')import osimport boto3import requestsimport base64import jsonimport loggingclient_id = \"Your Spotify Developer ID\"client_secret = \"Your Spotify Developer PW\"try: dynamodb = boto3.resource('dynamodb', region_name='ap-northeast-2', endpoint_url='http://dynamodb.ap-northeast-2.amazonaws.com')except: logging.error('could not connect to dynamodb') sys.exit(1)def lambda_handler(event, context): headers = get_headers(client_id, client_secret) table = dynamodb.Table('top_tracks') artist_id = event['artist_id'] URL = \"https://api.spotify.com/v1/artists/&#123;&#125;/top-tracks\".format(artist_id) params = &#123; 'country': 'US' &#125; r = requests.get(URL, params=params, headers=headers) raw = json.loads(r.text) for track in raw['tracks']: data = &#123; 'artist_id': artist_id &#125; data.update(track) table.put_item( Item=data ) # AWS CloudWatchì—ì„œ Logê¸°ë¡ì„ ì‚´í´ë³¼ë•Œ í™•ì¸í•˜ê¸° ìœ„í•´ return ê°’ì„ Successë¡œ ì£¼ì—ˆë‹¤. return \"SUCCESS\"def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersif __name__=='__main__': main() ìœ„ì™€ ê°™ì´ íŒŒì¼ì„ ëª¨ë‘ ì‘ì„±í–ˆë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ë¡œ ë§Œë“¤ì–´ì ¸ ìˆì„ ê²ƒì´ë‹¤. 12345top_tracksâ”œâ”€â”€ deploy.shâ”œâ”€â”€ lambda_function.pyâ”œâ”€â”€ requirements.txtâ””â”€â”€ setup.cfg ì´ì œ top_tracksì˜ pathì—ì„œ ì•„ë˜ì™€ ê°™ì´ shell scriptë¥¼ ì‹¤í–‰ì‹œì¼œì¤€ë‹¤. 1234./deploy.sh# ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ì´ permission deniedê°€ ë°œìƒí•  ê²ƒì´ë‹¤.-bash: ./deploy.sh: Permission denied Permission ê¶Œí•œì„ ë°”ê¿”ì£¼ê¸° ìœ„í•´ ë‹¤ìŒì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•œë’¤ ë‹¤ì‹œ shell scriptë¥¼ ì‹¤í–‰ì‹œí‚¨ë‹¤. 123chmod +x deploy.sh./deploy.sh ìœ„ì˜ ì½”ë“œë“¤ì„ ì‹¤í–‰í•œ ë’¤ ë‹¤ì‹œ AWS Lambda Functionì˜ í˜ì´ì§€ë¡œ ëŒì•„ê°€ ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ í•´ë‹¹ íŒŒì¼ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ì„¤ì •ì´ ë˜ì–´ì ¸ ìˆëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ë³´í†µì€ ì•„ë˜ì™€ ê°™ì´ sensitiveí•œ ì •ë³´ë“¤(client_id, client_secret) ê°™ì€ ì •ë³´ë“¤ì€ ë”°ë¡œ config.py íŒŒì¼ì„ ë§Œë“¤ì–´ì„œ ëª¨ì•„ë†“ëŠ”ë‹¤. ì´ëŸ° config.pyíŒŒì¼ì€ í•´ë‹¹ ê´€ë¦¬ìë§Œ ê°€ì§€ê³  ìˆë„ë¡ í•˜ì—¬ ë³´ì•ˆì— ìœ ì§€í•˜ë‚˜, í•„ìëŠ” í˜¼ì ì‚¬ìš©í•˜ë¯€ë¡œ ë”°ë¡œ ë§Œë“¤ì§€ ì•Šì•˜ë‹¤. ë§Œì•½ ìœ„ì—ì„œ Function Codeë‚´ì—ì„œëŠ” sensitiveí•œ ì •ë³´ë“¤ì´ ì•ˆë³´ì´ë„ë¡ í•˜ë ¤ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ environment variableì„ ì¶”ê°€í•˜ê³ , ì½”ë“œë¥¼ ì‘ì„±í•˜ë©´ëœë‹¤. keyê°’ê³¼ valueê°’ì´ ë¬¸ìë¼ë„ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë„£ìœ¼ë©´ ëœë‹¤. ì´ì œ Lambda Functionì„ Testí•´ ë³¼ ì°¨ë¡€ì´ë‹¤. Test ë²„íŠ¼ì„ í´ë¦­í•´ ì¤€ ë’¤, eventì˜ artist_idê°’ì„ ì£¼ê¸° ìœ„í•´ RDSì— ì ‘ì†í•´ì„œ ì„ì˜ì˜ ID í•˜ë‚˜ë¥¼ í•„ìëŠ” ê°€ì ¸ì™”ë‹¤. Test ë°ì´í„°ë¥¼ ì €ì¥í•˜ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ í…ŒìŠ¤íŠ¸í•  íŒŒì¼ë¡œ ë°”ë€Œì—ˆë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ì´ì œ Test ë²„íŠ¼ì„ í´ë¦­í•œë‹¤. Logë¥¼ ë³´ë‹ˆ Errorê°€ ë°œìƒí•œ ê²ƒ ê°™ìœ¼ë¯€ë¡œ ë¨¼ì € í™•ì¸í•´ ë³¼ ê²ƒì´ë‹¤. LogëŠ” AWS CloudWatch ì„œë¹„ìŠ¤ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ë¡œê·¸ ì¤‘ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤ ë¶€ë¶„ì„ ì‚´í´ë³´ë©´ í•´ë‹¹ roleì— ëŒ€í•œ permissionì´ ì—†ê¸° ë•Œë¬¸ì— ë°œìƒí•œ errorì„ì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ì²˜ìŒì— Lambda Functionì„ ë§Œë“¤ë•Œ ë§í–ˆë“¯ì´ permissionì´ ì—†ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” errorì´ë¯€ë¡œ excution roleì„ ë³€ê²½í•´ì£¼ê¸° ìœ„í•´ ì•„ë˜ ê·¸ë¦¼ì˜ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤ ì•ˆì˜ ë²„íŠ¼ì„ í´ë¦­í•´ ì¤€ë‹¤. roleì„ ë³€ê²½í•˜ê¸° ìœ„í•´ IAM í˜ì´ì§€ë¡œ ì´ë™ë˜ë©°, Attachë¥¼ í†µí•´ DynamoDBì— access í•  ìˆ˜ ìˆëŠ” ê¶Œí•œì„ ë¶€ì—¬í•  ê²ƒì´ë‹¤. ìƒˆë¡­ê²Œ roleì´ ì¶”ê°€ë˜ì–´ ê¸°ì¡´ì˜ 1ê°œì—ì„œ 2ê°œë¡œ ëŠ˜ì–´ë‚¬ìœ¼ë©°, DynamoDBFullAccess ê¶Œí•œì„ ë¶€ì—¬ë°›ìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ì œ ë‹¤ì‹œ Lambda Function í˜ì´ì§€ë¡œ ëŒì•„ì™€ì„œ ì‹¤í–‰ì‹œì¼œ ë³´ë©´ ì œëŒ€ë¡œ ì‹¤í–‰ë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤. LambdaëŠ” Event trigger ë¿ë§Œ ì•„ë‹ˆë¼, Crontabê³¼ ê°™ì´ ìŠ¤ì¼€ì¥´ë§ì— ì˜í•œ jobì„ ì‘ë™ì‹œí‚¬ìˆ˜ë„ ìˆìœ¼ë©°, ì ìš©ê°€ëŠ¥í•œ ì—¬ëŸ¬ê°€ì§€ eventë“¤ì´ ì¡´ì¬í•œë‹¤.","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering (Prestoë€?)","slug":"data_engineering_08","date":"2020-02-24T13:11:09.000Z","updated":"2020-02-28T23:27:23.231Z","comments":true,"path":"2020/02/24/data_engineering_08/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/24/data_engineering_08/","excerpt":"","text":"Prestoë€? Sparkì˜ ë‹¨ì ì´ë¼ í•˜ë©´, ë¬¼ë¡  Spark SQLë„ ìˆì§€ë§Œ, ì–´ëŠ ì •ë„ Scriptingì´ í•„ìš”í•œ ë¶€ë¶„ì´ ìˆë‹¤. MySQL ê°™ì´ RDSë¡œ ë°ì´í„° êµ¬ì¶•ì„ í–ˆì„ë•Œì—ëŠ” SQLì„ í†µí•´ì„œ ì‰½ê²Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆì—ˆì§€ë§Œ, Big dataë¡œ ë„˜ì–´ ì˜¤ë©´ì„œ ì´ì „ í•„ìì˜ ê¸€ì„ ë³´ì•˜ì„ ë•Œ S3ì—ì„œ ë‘ ê³³ì— ë‚˜ëˆ„ì–´ ì €ì¥ì„ í–ˆëŠ”ë°, ì´ëŸ° ê²½ìš° ê·¸ëŸ¼ RDSì™€ ë‹¤ë¥´ê²Œ ì–´ë–»ê²Œ í•©ì¹  ìˆ˜ ìˆëŠ”ì§€ë¥¼ Prestoë¥¼ í†µí•´ í•˜ë‚˜ì˜ queryë¡œ í•´ê²°í•  ìˆ˜ ìˆë‹¤. syntaxëŠ” SQLê³¼ ë¹„ìŠ·í•˜ë‹¤. ë‹¤ì–‘í•œ multiple data sourceë¥¼ single queryë¥¼ í†µí•´ì„œ ì§„í–‰ í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. Hadoopì˜ ê²½ìš°ëŠ” performanceë‚˜ ì—¬ëŸ¬ê°€ì§€ data analytics í• ë•Œ ì—¬ëŸ¬ê°€ì§€ issueë“¤ì´ìˆìœ¼ë©° ì´ì „ ë°©ì‹ì´ê¸° ë•Œë¬¸ì— ìµœê·¼ì—ëŠ” Sparkì™€ Prestoë¡œ ë„˜ì–´ì˜¤ëŠ” ì¶”ì„¸ì´ë‹¤. AWSëŠ” Prestoê¸°ë°˜ì¸ Athenaë¥¼ í†µí•´ì„œ S3ì˜ ë°ì´í„°ë¥¼ ì‘ì—…í•  ìˆ˜ ìˆë‹¤. Serverlessë€? ë§ ê·¸ëŒ€ë¡œ serverê°€ ì—†ë‹¤ë¼ê³  í•  ìˆ˜ ìˆìœ¼ë©°, Severlessë¼ê³  í•˜ëŠ” ë¶€ë¶„ì€ ë³´í†µ ì–´ë– í•œ ì„œë¹„ìŠ¤ë¥¼ ë§Œë“¤ ë•Œ, ìš°ë¦¬ì˜ Desktop PCë¥¼ ê³„ì†í•´ì„œ ì¼œë‘ëŠ” ê²½ìš°ê°€ ì•„ë‹ˆë¯€ë¡œ EC2ë¼ê³  í•˜ëŠ” ê³„ì† ì§€ì†ì ìœ¼ë¡œ ë„ì–´ì ¸ìˆëŠ” ê°€ìƒì˜ ì„œë²„ë¥¼ ë§Œë“¤ê²Œ ëœë‹¤. ì´ë•Œ ì„œë²„ì˜ ìš©ëŸ‰ì„ ê²°ì •í•´ì•¼í•˜ëŠ”ë°, ì˜ˆë¥¼ë“¤ì–´ chat botì„ í†µí•´ Userì™€ ì†Œí†µì„í• ë•Œ ì–´ë–¤ ë‚ ì€ Userê°€ 1ëª… ë‹¤ë¥¸ë‚ ì€ 100ëª… ì–´ë–¤ë‚ ì€ 10,000ëª…ìœ¼ë¡œ ëŠ˜ì–´ë‚ ìˆ˜ìˆì–´ì„œ ë¬´ì‘ì • í° ì„œë²„ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ê±°ë‚˜ ìˆœì°¨ì ìœ¼ë¡œ Dockerë¥¼ í†µí•´ ë³‘ë ¬ì ìœ¼ë¡œ ì–´ë–¤ ê¸°ì¤€ì´ìƒì´ ë˜ë©´ ìš©ëŸ‰ì„ ëŠ˜ë¦¬ëŠ” ì´ëŸ¬í•œ ì‘ì—…ë„ ë‹¤ ë¹„ìš©ì´ ë˜ë¯€ë¡œ ì´ëŸ¬í•œ ë¬¸ì œë“¤ì„ ë³´ì™„í•˜ê³ ì Serverlessë¼ëŠ” ê°œë…ì´ ë„ì…ëœë‹¤. ì–´ë– í•œ ìš”ì²­ì´ ë“¤ì–´ì˜¬ë•Œ serverë¥¼ ë„ìš°ëŠ”ë° ì§€ì†ì ìœ¼ë¡œ ìš”ì²­ì´ ë“¤ì–´ì˜¨ë‹¤ë©´ ê³„ì†ì ìœ¼ë¡œ ë³‘ë ¬ì ì¸ serverë¥¼ ë„ìš´ë‹¤ëŠ” ê²ƒì´ë‹¤. serverì•ˆì—ì„œ ìš©ëŸ‰ì„ ì •í•˜ëŠ” ê²ƒì„ ì•Œì•„ì„œ ìë™ì ìœ¼ë¡œ í•´ê²°í•´ ì£¼ë¯€ë¡œ ë¹„ìš©ì ì¸ ë¬¸ì œë¥¼ ë³´ì™„í•´ì¤€ë‹¤. AWSì—ì„œ EC2ê°™ì€ ê²½ìš°ëŠ” server í•˜ë‚˜ë¥¼ ë„ìš°ëŠ” ê²ƒì´ê³ , Lambdaê°€ Serverlessì˜ ê°œë…ì„ ê°–ëŠ” ì„œë¹„ìŠ¤ì´ë‹¤. ë˜í•œ Athenaë„ Serverlessì˜ ê°œë…ì„ ê°–ëŠ” ì„œë¹„ìŠ¤ì´ë‹¤. AWS Athenaì˜ ê°œìš” AWS Athenaì—ì„œë„ data lakeì˜ ì‹œìŠ¤í…œ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ì‘ì—…í•˜ë”ë¼ë„ queryë¥¼ í†µí•´ ì‘ì—…ì„ í•˜ë ¤ë©´ data warehouse ì²˜ëŸ¼ tableì˜ í˜•ì‹ì„ ì•ˆë§Œë“¤ ìˆ˜ëŠ” ì—†ë‹¤. ë¨¼ì €, AWS Athena í•„ìì™€ ê°™ì´ ì²˜ìŒ Athenaë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë¼ë©´, create table ë²„íŠ¼ì„ í´ë¦­í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ í˜•íƒœë¡œ queryë¬¸ì„ ì…ë ¥í•˜ëŠ” í˜ì´ì§€ ë³´ì¼ ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ queryë¬¸ì˜ ê²°ê³¼ë¥¼ ì €ì¥í•˜ëŠ” ê³³ì„ ë¨¼ì € ì„¤ì •í•´ ì£¼ì–´ì•¼ í•˜ëŠ”ë°, ì´ì „ì˜ S3ì˜ spotify-chatbot-project bucketì— ìƒˆë¡œìš´ í´ë”ë¥¼ ì¶”ê°€í•´ ì£¼ê³  ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ pathë¥¼ ì„¤ì •í•´ì¤€ë‹¤. ì´ì œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì´ì „ì— ë§Œë“¤ì–´ ë†“ì€ parquetí˜•íƒœì˜ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” folderì˜ pathë¡œ queryë¬¸ì„ ë‚ ë ¤ì¤€ë‹¤. ì´ì „ì— ì¡´ì¬í•˜ì§€ ì•Šì•˜ë˜ í…Œì´ë¸”ì´ ì™¼ìª½ì˜ tabì— ìƒì„±ë˜ë©° ì•„ë˜ ë¶€ë¶„ì—ëŠ” ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ëŠ” ë¶€ë¶„ì´ ë³´ì—¬ì§ˆ ê²ƒì´ë‹¤. ê·¸ ë¶€ë¶„ì—ëŠ” partitionë˜ì–´ì§„ ë¶€ë¶„ì€ ì§ì ‘ì ìœ¼ë¡œ loadë¥¼ í•´ì£¼ì–´ì•¼ í•œë‹¤ëŠ” ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ê³ ìˆë‹¤. ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ë§¨ ì•„ë˜ ë¶€ë¶„ì˜ ëª¨ë“  partitionì„ ë³´ë ¤ë©´, MSCK REPAIR TABLE) command ì‚¬ìš©í•˜ë¼ê³ í•œ ê²ƒ ì²˜ëŸ¼, queryë¬¸ì„ ì¶”ê°€í•´ì„œ ì‚¬ìš©í•˜ì˜€ë‹¤. ì¼ë°˜ì ì¸ queryë¬¸ê³¼ ê°™ì´ ì‘ì„±í•´ì„œ ë³¼ ìˆ˜ ìˆë‹¤. ë¨¼ì € top_tracks í…Œì´ë¸”ì˜ ìƒìœ„ 10ê°œë§Œ ë¶ˆëŸ¬ì™€ ë³¼ ê²ƒì´ë‹¤. ê·¸ë¦¬ë„ partitionì„ dtë¡œ í–ˆê¸° ë•Œë¬¸ì— dtë„ ê°™ì´ ë¶ˆëŸ¬ì™€ ì§€ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. audio_featuresë„ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ ë³¼ ê²ƒì´ë‹¤. ì´ë°–ìœ¼ ì‚¬ìš©ë²•ì€ ì•„ë˜ ë¬¸ì„œë¥¼ í†µí•´ ì‚¬ìš©ë²•ì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©°, ìœ ì˜í• ì ì€ ìš°ë¦¬ê°€ partitionìœ¼ë¡œ ë‚˜ëˆ„ì–´ ë†“ì€ ê²ƒì„ ì „ì²´ë¡œ ë¶ˆëŸ¬ ì™”ê¸°ì— ìµœê·¼ì˜ partitionë§Œì„ ì‚´í´ë³´ê¸° ìœ„í•´ dateë¥¼ ì–´ë–¤ ê°’ìœ¼ë¡œ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì–´ì•¼ í•œë‹¤. prestodb documents Apache Spark ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í•˜ë‚˜ì˜ ì‹œìŠ¤í…œì´ë‹¤. ë°ì´í„°ëŠ” í•­ìƒ ëŠ˜ì–´ë‚˜ê³  ê·¸ë¦¬ê³  ë„ˆë¬´ë‚˜ í° ë°©ëŒ€í•œ ì–‘ì„ ì²˜ë¦¬ë¥¼ í•´ì•¼í•˜ê¸° ë°ì´í„°ê°€ ëŠ˜ì–´ë‚˜ë©´ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ì†ë„,ì‹œê°„,ë¹„ìš© ì—¬ëŸ¬ë©´ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì•¼í•œë‹¤. í•„ìëŠ” ì œí”Œë¦°ì„ ì‚¬ìš©í•  ê²ƒì¸ë°, ì œí”Œë¦°ì€ Sparkë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ Web UIì´ë‹¤. ê·¸ë˜ì„œ Sparkë¥¼ í†µí•´ì„œ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì‹œê°í™”í•œë‹¤ë˜ì§€ ì–´ë–¤ì‹ìœ¼ë¡œ outputì´ ë‚˜ì˜¤ëŠ”ì§€ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. Jupyter notebookê³¼ ë¹„ìŠ·í•˜ë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. Map Reduce Map ReduceëŠ” ë°ì´í„°ê°€ ë°©ëŒ€í•œ ì–‘ìœ¼ë¡œ ëŠ˜ì–´ë‚ ë•Œ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì— issueê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤. ì´ëŸ° issueë“¤ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ì„œ ë°ì´í„°ê°€ ì—¬ëŸ¬êµ°ë° ë¶„ì‚°ì²˜ë¦¬ ë˜ì–´ìˆëŠ” í˜•íƒœë¡œ ì €ì¥ë˜ì–´ìˆëŠ”ë°, S3 bucketì— ì €ì¥í•œ ë°©ì‹ì²˜ëŸ¼ partitionìœ¼ë¡œ êµ¬ë¶„ëœ ë°ì´í„°ë¥¼ functionì´ë‚˜ ì–´ë– í•œ ë°©ì‹ì— ì˜í•´ì„œ mappingì„ í•´ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œì„ ì¤„ì´ëŠ” Reduce ê³¼ì •ì„ ê±°ì¹˜ê²Œ ëœë‹¤. ì´ ë°©ì‹ì€ ì²˜ìŒ Google File Systemìœ¼ë¡œ ì‚¬ìš©ë˜ì–´ì§€ë‹¤ê°€ ê·¸ ë’¤ì— Map-Reduceë°©ì‹ìœ¼ë¡œ Hadoopì„ ì‚¬ìš©í–ˆìœ¼ë©°, ì†ë„ì— íŠ¹í™”ëœ Sparkë¥¼ í˜„ì¬ëŠ” ì£¼ë¡œ ì‚¬ìš©í•˜ê³  ìˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ì˜ˆë¥¼ë“¤ë©´, êµ¬ê¸€ê°™ì´ ë‹¤ì–‘í•œ web pageë¥¼ í¬ë¡¤ë§í•´ì„œ ê° í˜ì´ì§€ë“¤ì˜ ë…¸ì¶œ ë­í‚¹ì„ ë¶„ì„í•´ì•¼ í•˜ëŠ” Page Rankë¼ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í• ë•Œ htmlì•ˆì— ë“¤ì–´ê°€ëŠ” tagë¼ë˜ì§€ ì´ëŸ° ë¬¸ë²•ì ì¸ ìš”ì†Œë“¤ê³¼ contentsë“¤ì„ í•œ ê³³ì— ëª°ì•„ì„œ ë¶„ì„í•˜ê¸° ë³´ë‹¤ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ Inputì„ ë³‘ë ¬ì ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì§„í–‰í•˜ê³  ê·¸ ë‹¤ìŒ ì–´ë– í•œ Suffling processë¥¼ í†µí•´ì„œ Reduceí•˜ì—¬ ê²°ê³¼ë¥¼ ë‚¸ë‹¤. í•„ìëŠ” AWSì˜ EMR(Elastic Map Reduce)ì„œë¹„ìŠ¤ë¥¼ í†µí•´ Sparkì™€ ì œí”Œë¦°ì„ ì„¤ì¹˜ í•´ ë³¼ ê²ƒì´ë‹¤. EMRì€ Sparkë‚˜ Hadoop ê°™ì€ ì‹œìŠ¤í…œì— clusterë¥¼ ë§Œë“œëŠ” ê³³ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. clusterëŠ” ì„œë²„ë“¤ì´ë¼ê³  ë§ í•  ìˆ˜ ìˆë‹¤. EC2ë¥¼ baseë¡œí•œ EMRì„ clusterí™”í•´ì„œ í•˜ë‚˜ì˜ instance ECS serverê°€ ì•„ë‹ˆë¼ master ì•„ë‹ˆë©´ ë‹¤ì–‘í•œ core Nodeë“¤ì´ ì—¬ëŸ¬ê°œê°€ ìƒì„±ì´ ë˜ì–´ì„œ ë°©ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œ í•„ìš”í•œ settingì„ êµ¬ì„±í•  ìˆ˜ ìˆëŠ” ê³³ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. ì„œë²„ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ì–‘í•œ ê¶Œí•œì´ë¼ë˜ì§€ key fileë“¤ì´ í•„ìš”í•˜ë¯€ë¡œ securityì™€ ê°™ì€ ë¶€ë¶„ì„ ë‹¤ë£¨ì–´ì•¼ í•˜ëŠ”ë° ì´ëŸ° ë¶€ë¶€ì€ AWSì—ì„œ IAM ì„œë¹„ìŠ¤ì—ì„œ ì‘ì—…í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë¨¼ì € AWSì—ì„œ EMR í˜ì´ì§€ë¡œ ì´ë™í•œë‹¤. cluster ì´ë¦„ì„ ì •í•˜ê³ , Applicationì€ Sparkë¥¼ ì‚¬ìš©í•  ê²ƒì´ë¯€ë¡œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì„¤ì •í•´ ì£¼ì—ˆë‹¤. ë˜í•œ, hardware ë¶€ë¶„ì€ memory optimizationë“± ì—¬ëŸ¬ê°€ì§€ ì˜µì…˜ì´ ì¡´ì¬í•˜ì§€ë§Œ ëª¨ë“  ê²ƒì€ ë‹¤ ë¹„ìš©ì´ë¯€ë¡œ ìš°ì„  ê°„ë‹¨í•˜ê²Œ c4 largeë¥¼ ì‚¬ìš©í•´ ë³´ë ¤ê³  í•œë‹¤. EC2 ì„œë²„ ì•ˆì—ì„œ ë‹¤ì–‘í•œ ì‘ì—…ë“¤ì„ í•˜ê¸° ìœ„í•´ì„œ key pair í•„ìš”í•˜ë‹¤. í•„ìì™€ ê°™ì´ í•œë²ˆë„ key pairë¥¼ ìƒì„±í•œ ì ì´ ì—†ë‹¤ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ Learn how to key pair ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ EC2 í˜ì´ì§€ë¡œ ë„˜ì–´ê°€ key pairë¥¼ ìƒì„±í•œë‹¤. key pairê°€ ìƒì„±ë˜ë©´ ë™ì‹œì— pem íŒŒì¼ì´ ë‹¤ìš´ë¡œë“œ ë˜ì–´ì§ˆ ê²ƒì´ë‹¤. ê·¸ íŒŒì¼ì„ í˜„ì¬ projectë¥¼ ì§„í–‰í•˜ëŠ” í´ë”ë¡œ ì˜®ê²¨ ë†“ëŠ”ê²ƒì„ ì¶”ì²œí•œë‹¤. ì˜®ê²¨ ë†“ì•˜ë‹¤ë©´, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ íŒŒì¼ì˜ ëª¨ë“œë¥¼ ë³€ê²½í•´ ì£¼ì–´ì•¼ í•œë‹¤. í•´ë‹¹ pem íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” pathì—ì„œ ì•„ë˜ì™€ ê°™ì´ ë³€ê²½í•œë‹¤. 1chmod og-rwx spotift_chatbot.pem key pairë¥¼ ìƒì„±í•˜ì˜€ìœ¼ë¯€ë¡œ ë‹¤ì‹œ clusterë¥¼ ìƒì„±í•˜ëŠ” í˜ì´ì§€ë¡œ ëŒì•„ê°€ key pairë¥¼ ì„¤ì •í•œ ë’¤ì— ì•„ë˜ ìƒì„± ë²„íŠ¼ì„ í´ë¦­í•œë‹¤. clusterê°€ ìƒì„±ë˜ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ clusterê°€ ì†ì„±ë“¤ì— ëŒ€í•œ í˜ì´ì§€ê°€ ë³´ì¼ ê²ƒì´ë‹¤. ë˜í•œ, Master ê¶Œí•œìœ¼ë¡œ ì ‘ì†ì„ í•˜ë ¤ë©´ SSH ë°©ì‹ìœ¼ë¡œ ì¸ì¦ í›„ ì ‘ì†í•´ì•¼í•˜ê¸° ë•Œë¬¸ì— security groupì„ ê´€ë¦¬ í•´ ì£¼ì–´ì•¼ í•œë‹¤. Security groups for Masterì˜ ê°’ì„ í´ë¦­í•˜ì—¬ EC2ì˜ security group í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ Inbound ê·œì¹™ì— Masterì™€ slave ë‘˜ë‹¤ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ssh ê·œì¹™ì„ ì¶”ê°€í•´ ì£¼ì–´ì•¼ í•œë‹¤. sshë¡œ ì ‘ì†í•˜ëŠ” ë°©ë²•ì€ ì•„ë˜ê·¸ë¦¼ê³¼ ê°™ì´ enable web connectionì„ í´ë¦­í•˜ë©´ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. pem íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” pathë¡œ ê°€ì„œ ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë¹¨ê°„ìƒ‰ ì¤„ì´ ì³ì ¸ìˆëŠ” commandë¥¼ ì‹¤í–‰í•´ ë†“ì€ ë’¤, ë™ì¼í•œ íŒŒì¼ pathì— ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ ìƒì„±í•˜ë¼ëŠ” foxyproxy-settings.xmlë¥¼ íšŒìƒ‰ ë„¤ëª¨ì¹¸ì˜ ë‚´ìš©ë“¤ì„ ë³µì‚¬í•˜ì—¬ ìƒì„±í•œë‹¤. ë™ì¼í•œ íŒŒì¼ pathì—ì„œ í•˜ë¼ê³  ì¶”ì²œí•˜ëŠ” ê²ƒì€ ì´ pathì—ì„œ ê³„ì† ì ‘ì†í•  ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ ë‹¤ìŒì€ chrome web storeì— ê°€ì„œ foxy proxyë¥¼ ê²€ìƒ‰í•œ í›„ standard ë²„ì ¼ì„ ì„¤ì¹˜í•´ì¤€ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ chrome ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•´ì•¼ í•  ê²ƒì„ì€ ë‹¹ì—°íˆ ì•Œ ê²ƒì´ë¼ê³  ìƒê°í•œë‹¤. foxy proxyë¥¼ chrome browserì— ì¶”ê°€í–ˆë‹¤ë©´, ì˜¤ë¥¸ìª½ ìƒë‹¨ì— ì—¬ìš°ëª¨ì–‘ì˜ ì•„ì´ì½˜ì´ ìƒì„±ë˜ì—ˆì„ ê²ƒì´ë‹¤. í´ë¦­í•œ í›„ option ë²„íŠ¼ì„ ëˆŒëŸ¬ì¤€ë‹¤. ê·¸ ë‹¤ìŒì€ ì™¼ìª½ tabì—ì„œ import/exportë¥¼ ëˆŒëŸ¬ ì¤€ í›„ì— ì´ì „ì— ë§Œë“¤ì–´ ì¤€ xmlíŒŒì¼ì„ ëˆŒëŸ¬ replaceí•´ ì£¼ë©´ ëœë‹¤. ê·¸ëŸ° ë‹¤ìŒ public dnsë¡œ ì ‘ì†í•´ ë³´ë©´ ë§¨ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ test pageë¥¼ ë³¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ë˜í•œ dnsìœ„ì— connectionë“¤ì´ í™œì„±í™” ëœ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ connectionë“¤ ì¤‘ ì œí”Œë¦°ì„ í´ë¦­í•˜ë©´ ì œí”Œë¦° í˜ì´ì§€ë¡œ ì´ë™í•œë‹¤. ëª¨ë“  ëª…ë ¹ë¬¸ì´ %pysparkë¡œ ì‹œì‘í•´ì•¼ í•œë‹¤. í•„ìëŠ” ê¸°ì¡´ì˜ c4.largeë¡œ ì§„í–‰ì‹œì—ëŠ” connection errorê°€ ìƒê²¨ ì›ì¸ì„ ì°¾ë‹¤ê°€ í•´ê²°ì¹˜ ëª»í•˜ê³  ìš°ì„  r3.xlargeë¥¼ ì„ íƒí•˜ì—¬ ë‹¤ì‹œ clusterë¥¼ ë§Œë“  ê²°ê³¼ connection errorê°€ ë°œìƒí•˜ì§€ ì•Šì•˜ë‹¤. 2ë²ˆì§¸ cellê¹Œì§€ëŠ” pythonì—ì„œ ì§„í–‰í•˜ë˜ ë°©ì‹ì´ê³  3ë²ˆì§¸ cellì—ì„œ êµ¬í˜„í•˜ëŠ” ë°©ì‹ì´ sparkì˜ ë°©ì‹ì¸ë° sparkëŠ” rddë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°©ëŒ€í•œ ë°ì´í„°ë¥¼ ë¶„ì‚°ì‹œì¼œì„œ mappingí•œ í›„ applyí•´ì„œ ì–»ì€ ê°’ì„ í†µí•©ì„ í•´ì„œ ë°›ëŠ” êµ¬ì¡°ì´ë‹¤. sc(spark context)ë¥¼ í†µí•´ parallelizeí•˜ê²Œ 3ê°œì˜ ë°ì´í„°ë¥¼ ìª¼ê°œì„œ rddë¡œ ë‚˜ëˆ„ì–´ ì¤€ ê²°ê³¼ì´ë‹¤. ì•„ë˜ì™€ ê°™ì´ rddë¡œ ìª¼ê°œì„œ mapí•¨ìˆ˜ë¥¼ í†µí•´ ìª¼ê°œë†“ì€ ë°ì´í„°ì— ê°ê° mapping ì‹œì¼œì£¼ê³  ì„ íƒí•œë‹¤. ë˜í•œ, sqlContextë¥¼ í†µí•´ S3ì— ì´ì „ì— ì €ì¥í•´ ë†“ì•˜ë˜ ë°ì´í„°ë¥¼ dataFrameí˜•íƒœë¡œ ë¶ˆëŸ¬ì™€ ì‘ì—…í•  ìˆ˜ ìˆë‹¤. printSchema()í•¨ìˆ˜ë¥¼ í†µí•´ ê°ê°ì˜ ê°’ë“¤ì´ ì–´ë–¤ í˜•íƒœë¡œ ë“¤ì–´ìˆëŠ”ì§€ì— ì£¼ì˜ë¥¼ ê°–ê³  ì‚´í´ë´ì•¼ ì¶”í›„ì— ì‘ì—…ì— ì–´ë ¤ì›€ì´ ì—†ë‹¤. ì´ë ‡ê²Œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ ë‚´ì¥ í•¨ìˆ˜ë¥¼ í™œìš©í•´ ê¸°ë³¸ì ì¸ í†µê³„ëŸ‰ê°’ë“¤ì„ ê³„ì‚°í•  ìˆ˜ ìˆìœ¼ë©°, í•„ìš”ì— ë”°ë¼ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜(UDF : User Definition Function)ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆë‹¤. UDFë¥¼ í†µí•´ ì •ì˜í•œ í•¨ìˆ˜ë¥¼ Booleanê°’ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ filterí•¨ìˆ˜ì— ì ìš©ì‹œì¼œ conditionì„ ì¤„ ìˆ˜ë„ ìˆë‹¤. ë³¸ê²©ì ìœ¼ë¡œ S3ì— ì €ì¥í•´ ë†“ì€ ëª¨ë“  ë°ì´í„°ë“¤ì„ joiní•œ master tableì„ ë§Œë“¤ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ ì‘ì—…ì„ í•œë‹¤. ê°€ì¥ ë¨¼ì €, artists parquet ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ì•„ë˜ì™€ ê°™ì´ DataFrameìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆë‹¤. í—ˆë‚˜, artists ë°ì´í„°ëŠ” ì˜ ë³€í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— zeppelinì—ì„œ pythonì„ í†µí•´ RDSì—ì„œ ë°”ë¡œ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì´ ì¢‹ì„ ìˆ˜ë„ ìˆë‹¤. ì•„ë˜ì—ì„œ í•¨ìˆ˜ ì•ˆì˜ argumentê°’ë“¤ì€ ìì‹ ì˜ ê°’ì— ë§ëŠ” ê²ƒë“¤ë¡œ ë¨¼ì € ì •í•´ë†“ê³  ì‹¤í–‰í•´ì•¼ í•œë‹¤. ìµœì¢…ì ìœ¼ë¡œ artists, top-tracks, ê·¸ë¦¬ê³  audio_features ëª¨ë‘ joiní•œ tableì„ ë§Œë“¤ ê²ƒ ì´ë‹¤. ì°¸ê³ ë¡œ ì•„ë˜ cellì„ ì‹¤í–‰í•˜ê¸° ì´ì „ì— master nodeì— ì ‘ì†í•´ì„œ ë¨¼ì € sudo pip install pandasì™€ sudo pip install pyspark ëª…ë ¹ì–´ë¥¼ í†µí•´ ì„¤ì¹˜í•´ ì£¼ì–´ì•¼ í•œë‹¤. zeppelinì˜ ì¥ì  ì¤‘ í•˜ë‚˜ë¡œ ë°”ë¡œ sql tableë¡œ ì§€ì •í•˜ì—¬ sql queryë¬¸ìœ¼ë¡œ ì‘ì—…í•  ìˆ˜ ìˆë‹¤. í•´ë‹¹ ê°’ì„ ë°”ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ì ë„ ì¥ì  ì¤‘ì˜ í•˜ë‚˜ì´ë‹¤. ì˜µì…˜ì—ì„œ ì—†ëŠ” ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ëŠ” ê²ƒì€ pythonì´ë‚˜ ë‹¤ë¥¸ libraryë¥¼ í™œìš©í•˜ì—¬ ë³´ì™„í•  ìˆ˜ ìˆë‹¤. ë°ì´í„°ì˜ audio featureì˜ ë¶„í¬ë¥¼ í†µí•´ ì˜ˆë¥¼ ë“¤ì–´ ê°€ìˆ˜ì˜ ì¸ê¸°ë„ì™€ íŠ¸ë™ì˜ ì¸ê¸°ë„ì˜ ì°¨ì´ê°€ ê±°ì˜ ì—†ëŠ” í•´ë‹¹ ê°€ìˆ˜ì˜ ëŒ€í‘œì ì¸ íŠ¸ë™ì„ ì•Œê³  ì‹¶ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ EDAë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ audio featureë“¤ì˜ íŠ¹ì§•ì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ acousticnessëŠ” ì „ì²´ì ìœ¼ë¡œ 0ìª½ìœ¼ë¡œ ì¹˜ìš°ì³ìˆì–´ ì¤‘ì‹¬ì„ ëŒ€í‘œí•˜ëŠ” ê°’ìœ¼ë¡œëŠ” medianì„ ì‚¬ìš©í•´ì•¼ ë  ê²ƒì´ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆìœ¼ë©°, danceabilityëŠ” ì •ê·œë¶„í¬ ê¼´ì„ ë„ê³  ìˆì–´ meanì„ ì‚¬ìš©í•´ë„ ë¬´ë°©í•  ê²ƒìœ¼ë¡œ íŒë‹¨ í•  ìˆ˜ ìˆë‹¤. ì°¸ê³ í•˜ë©´ ì¢‹ì„ ë¬¸ì„œ - 01 ì°¸ê³ í•˜ë©´ ì¢‹ì„ ë¬¸ì„œ - 02 ì°¸ê³ í•˜ë©´ ì¢‹ì„ ë¬¸ì„œ - 03","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering (ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ vs ë°ì´í„° ë ˆì´í¬)","slug":"data_engineering_07","date":"2020-02-21T16:14:31.000Z","updated":"2020-02-28T22:57:23.162Z","comments":true,"path":"2020/02/22/data_engineering_07/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/22/data_engineering_07/","excerpt":"","text":"ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ vs ë°ì´í„° ë ˆì´í¬ ë°ì´í„° ë ˆì´í¬ë¼ëŠ” ê°œë…ì€ ë¹„êµì  ìµœì‹ ì˜ ê°œë…ì´ë‹¤. ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¼ê³  í•˜ëŠ” MySQL, PostgreSQL ê°™ì€ RDBMS í”„ë¡œê·¸ë¨ë“¤ì„ ë„˜ì–´ì„œ ë°ì´í„°ë“¤ì´ ë„ˆë¬´ë‚˜ ë°©ëŒ€í•´ì¡Œê¸° ë•Œë¬¸ì— ë‚˜ì˜¨ ì‹œìŠ¤í…œì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ì „ì˜ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ëŠ” ë¯¸ë¦¬ ì§œì—¬ì§„ êµ¬ì¡°ë¥¼ í†µí•´ ê°€ê³µí•´ì„œ ì €ì¥í–ˆê¸°ì— ì¢€ ë” ì ‘ê·¼í•˜ê¸° ì‰¬ì—ˆë‹¤. ë°˜ë©´ì— ë°ì´í„° ë ˆì´í¬ëŠ” ë°ì´í„°ê°€ ë„ˆë¬´ ë°©ëŒ€í•˜ê¸° ë•Œë¬¸ì— ì–´ë–¤ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì‚¬ìš©í• ì§€ ëª¨ë¥´ë¯€ë¡œ Raw ë°ì´í„°ë¥¼ ì €ì¥í•œë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë°ì´í„°ì— ëŒ€í•œ ì ‘ê·¼ì„±ì´ ì¡°ê¸ˆ ë–¨ì–´ì§€ëŠ” ë©´ì´ ìˆì—ˆì§€ë§Œ Hadoopì´ë‚˜ Spark ê°™ì€ ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ë“¤ì„ í†µí•´ ê·¸ëŸ° ë‹¨ì ì„ ë³´ì™„í•˜ì—¬ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. ê´€ê³„í˜• DBë¥¼ ì‚¬ìš©í•  ê²½ìš°ì—ëŠ” ë°ì´í„°ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚  ìˆ˜ë¡ ë¹„ìš©ì´ë‚˜ ê´€ë¦¬ë¹„ìš©ë„ ë§ì´ ì†Œìš”ë  ê²ƒì´ë‹¤. ETL, ì¦‰ Extractí•˜ê³ , Transform í•œ í›„ì— Loadí•˜ëŠ” ê³¼ì •ì„ ì¼ì»«ëŠ” ìš©ì–´ì´ë©°, ì´ì „ì˜ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì—ì„œëŠ” ì´ëŸ° ìˆœì„œë¥¼ ê±°ì³ ì‘ì—…í–ˆì§€ë§Œ ìµœê·¼ì—ëŠ” ELT, ìš°ì„  Extractí•˜ê³ , Loadí•œë’¤ì— ë°ì´í„° ë ˆì´í¬ì— ë„£ì€ ë‹¤ìŒì— Transformer\bí•˜ìë¼ê³  ë§ì´ë“¤ ì´ì•¼ê¸° í•˜ê³  ìˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ í•˜ë‚˜ì˜ ì˜ˆë¡œì„œ, ë¨¼ì € ì—¬ëŸ¬ ê³³ì— ì‚°ì¬í•´ ìˆëŠ” ì •í˜•/ë¹„ì •í˜• ë°ì´í„°ë“¤ì„ ë°ì´í„° ë ˆì´í¬ì— í•œê³³ìœ¼ë¡œ ëª¨\bì•„ ê·¸ ë‹¤\bìŒ Sparkê°€ ëë˜ ë‹¤ë¥¸ ë¹…ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œì„ í†µí•´ ì¬ê°€ê³µì„ í•œ í›„ ë‹¤ë¥¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì´ë‚˜ BI TooLë“¤ì— í™œìš©í•  ìˆ˜ ìˆë‹¤. ë°ì´í„° ë ˆì´í¬ ì•„í‚¤í…ì³ ì—¬ëŸ¬ ì–´í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë‚˜ì˜¤ëŠ” ë°ì´í„°ë‚˜ í•´ë‹¹ APIë¥¼ í†µí•´ì„œ ì–»ê²Œë˜ëŠ” ë°ì´í„°ë“¤ì„ ëª¨ì•„ ì–´ë–»ê²Œ ì¬ê°€ê³µí•´ì•¼í• ì§€ë¥¼ ê³ ë¯¼í•˜ëŠ” ê²ƒì´ ê°€ì¥ í° ë¬¸ì œì¼ ê²ƒì´ë‹¤. ë˜í•œ \bì´ëŸ° ê²ƒë“¤ í†µí•´ì„œ redashê°™ì€ ì‹œê°í™”ë¥¼ í†µí•´ insightë¥¼ ì–»ì–´ì•¼ í•  ê²ƒì´ë‹¤. ë˜í•œ, errorê°€ ë‚˜ì˜¨ë‹¤ë©´, ê·¸ ë‚ ì— ë°ì´í„°ë§Œ ì–´ë–»\bê²Œ backfillì„ í†µí•´ì„œ í™•ë³´ë¥¼ í•  ê²ƒì¸ì§€ë¥¼ ê³ ë¯¼í•´ì•¼í•œë‹¤. ì±—ë´‡ì„ í†µí•´ì„œ êµ¬ì¶•ì„ í•  ë•Œ, ì•„í‹°ìŠ¤íŠ¸ê°€ ëŠ˜ì–´ë‚¬ì„ë•Œ, ìƒˆë¡œìš´ artistê°€ ì…ë ¥ì„ ë“¤ì–´ì™”ì„ ë•Œ, ì €ì¥ë˜ì–´ìˆë˜ ì •ë³´ì—ëŠ” ì—†ë˜ Unknown artistì´ë©´, Trigger baseì˜ Lambdaì— ì˜í•´ì„œ Unknown artistì— ëŒ€í•œ ë°ì´í„°ë¥¼ í™•ë³´ë¥¼ í•˜ê³ , ë°ì´í„° ë ˆì´í¬ëŠ” Latency(ë°ì´í„°ë¥¼ ì£¼ê³ ë°›ëŠ” ì†ë„ì˜ ê°œë…)ê°€ ëŠë¦¬ê¸° ë•Œë¬¸ì— ë‹¤ì–‘í•œ DBì— RDBMSë¥¼ êµ¬ì¶•í•˜ì—¬ ìƒí™©ì— ë§ê²Œ ì €ì¥í•œë‹¤. ê·¸ëŸ¬ë‚˜ ì´ ëª¨ë“  ë°ì´í„°ë“¤ì´ ì¡´ì¬í•˜ëŠ” DBë¥¼ í•œê³³ìœ¼ë¡œ ë¬¶ì–´ì•¼ ë˜ëŠ” ë¶€ë¶„ì´ ë°ì´í„° ë ˆì´í¬ì´ë‹¤. ê·¸ ëª¨ë“ ê±¸ í•„ìëŠ” AWS S3ì— ì˜®ê¸¸ ê²ƒì´ë‹¤. ì˜®ê¸°ëŠ” ê³¼ì •ì—ì„œ ìŠ¤ì¼€ì¥´ë§ì€ ì–´ë–»ê²Œ í•  ê²ƒì¸ì§€ ê·¸ë¦¬ê³ , ì–´ë–¤ ì´ë²¤íŠ¸ê°€ ìˆì„ë•Œ ì—…ë°ì´íŠ¸ë¥¼ í•  ê²ƒì¸ì§€ë¥¼ ì •í•´ì£¼ì–´ì•¼ í•  ê²ƒì´ë‹¤. S3(Simple Storage System) ë°ì´í„° ë ˆì´í¬ ì—­í• ì„ í•  S3 bucketì„ ë§Œë“¤ ê²ƒì´ë‹¤. ì¼ì¢…ì˜ í´ë”ë¼ê³  ìƒê°í•˜ì—¬ë„ ë  ê²ƒ ê°™ë‹¤. ì•„ë˜ì™€ ê°™ì€ ë‹¨ê³„ë¡œ bucketì„ ë§Œë“ ë‹¤. create bucket ë²„íŠ¼ì„ í´ë¦­í•œë‹¤. ìƒì„±í•  bucket ì´ë¦„ì„ ì„¤ì •í•œë‹¤. bucketì— ê´€ë ¨ëœ ì„¤ì • ì¤‘ tag ë¶€ë¶„ì„ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì¸ë°, ìƒì„±í›„ì—ë„ ì„¤ì • ê°€ëŠ¥í•˜ë¯€ë¡œ ë‹¤ìŒë‹¨ê³„ë¡œ ë„˜ì–´ê°„ë‹¤. configure optionì„ ì„¤ì •í•˜ëŠ” ë‹¨ê³„ì´ë©°, ì•„ë˜ì™€ ê°™ì´ ëª¨ë‘ public access í•˜ê²Œë” default ì„¤ì •ê°’ìœ¼ë¡œ ì„ íƒí•˜ì˜€ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì•ì˜ ê³¼ì •ì—ì„œ ì„¤ì •í•œ ë¶€ë¶„ë“¤ì„ ìš”ì•½í•´ì„œ ë³´ì—¬ì¤€ë‹¤. ë§¨ ì•„ë˜ Create bucket ë²„íŠ¼ì„ ëˆ„ë¥´ë©´ bucket ìƒì„±ì´ ì™„ë£Œëœë‹¤. ìœ„ì˜ ë‹¨ê³„ë¥¼ ë‹¤ ê±°ì¹˜ë©´ ì²˜ìŒê³¼ ë‹¤ë¥´ê²Œ í•˜ë‚˜ì˜ bucketì´ ìƒì„±ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë¬¼ë¡  ì´ì „ì— ì´ë¯¸ bucketì„ ìƒì„±í•˜ì‹  ë¶„ë“¤ì€ ì—¬ëŸ¬ê°œì˜ bucket ë¦¬ìŠ¤íŠ¸ê°€ ë³´ì¼ ê²ƒì´ë‹¤. AWS GlueëŠ” ì–´ë–¨ë•ŒëŠ” ë°ì´í„°ê°€ í˜•ì‹ì´ ì—†ìœ¼ë‹ˆê¹Œ ì›ë˜ëŠ” í‚¤ê°’ì´ 3ê°œì˜€ëŠ”ë°, ê·¸ í›„ ì ì  í‚¤ê°’ì´ ëŠ˜ì–´ë‚  ìˆ˜ ë„ ìˆë‹¤. ì´ëŸ° ê²½ìš° ë‹¤ì–‘í•œ Tableì˜ ìŠ¤í‚¤ë§ˆë¥¼ ê´€ë¦¬ í•  ìˆ˜ ìˆë‹¤. ë°ì´í„° ë ˆì´í¬ ê²½ìš°ì—ëŠ” ì§€ì†ì ìœ¼ë¡œ ë³€í• ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì´ë¼ëŠ” ê²ƒì„ ì¸ì§€í•˜ê³  ìˆì–´ì•¼í•˜ê¸° ë•Œë¬¸ì—, ìœ„ì™€ ê°™ì€ ê²½ìš°ë“¤ì„ ìë™í™” í•  ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ì´ë‹¤. ê°€ì¥ í° ë¶€ë¶„ì´ Crwalerì´ë‹¤. ì–´ë– í•œ Tableì— ë³€í˜•ì´ ì¼ì–´ë‚¬ì„ ë•Œ, ì´ Crwalerê°€ ê°ì§€ë¥¼ í•´ì„œ, ê·¸ê²ƒì„ ë°˜ì˜ì„ í•´ì¤€ë‹¤. S3ì— ì˜¬ë¦´ íŒŒì¼ì‘ì—… Python Script ì‘ì„± bucketì˜ keyê°’ì´ë¼ê³  í•˜ëŠ” dt(data type)ì„ ì •í•´ì•¼í•œë‹¤. top tracksì™€ ê°™ì´ ì§€ì†ì ìœ¼ë¡œ ë³€í™”í•˜ëŠ” ë°ì´í„°ëŠ” ë°©ëŒ€í•œì–‘ìœ¼ë¡œ ëŠ˜ì–´ë‚¬ì„ë•ŒëŠ” ê²°êµ­ì—” ìª¼ê°œì„œ scanì„ í•´ì•¼í•˜ë¯€ë¡œ ì–´ë– í•œ í˜•ì‹ì„ í†µí•´ì„œ Sparkë‚˜ Hadoopì´ readableí•œ í˜•ì‹ìœ¼ë¡œ partitionì„ ë§Œë“¤ì–´ë†”ì•¼ Sparkë‚˜ Haddepì—ì„œ ìµœê·¼ì˜ ë°ì´í„°ë¥¼ ê°–ê³ ìˆëŠ” ë§ˆì§€ë§‰ partitionë§Œ í™•ì¸í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. í•„ìëŠ” ë‚ ì§œë¥¼ í†µí•´ ì‹œì ì´ ì–¸ì œì¼ì§€ ì•Œ ìˆ˜ ìˆë„ë¡ partitionì„ êµ¬ë¶„ì§€ì–´ ì¤„ ê²ƒì´ë‹¤. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import sysimport osimport base64import boto3import requestsimport loggingimport jsonimport pymysqlimport sys, os, argparsefrom datetime import datetimeimport pandas as pddef main(host, user, passwd, db, port, client_id, client_secret): try: # use_unicode=Trueë¥¼ ì¨ì•¼ í•œê¸€ê°™ì€ ê²½ìš°ëŠ” ê¹¨ì§€ì§€ ì•ŠëŠ”ë‹¤. conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") # ë³´í†µ ë¬¸ì œê°€ ì—†ìœ¼ë©´ 0 # ë¬¸ì œê°€ ìˆìœ¼ë©´ 1ì„ ë¦¬í„´í•˜ë„ë¡ ì•ˆì— ìˆ«ìë¥¼ ë„£ì–´ì¤€ë‹¤. sys.exit(1) headers = get_headers(client_id, client_secret) # RDS - ì•„í‹°ìŠ¤íŠ¸ IDë¥¼ ê°€ì ¸ì˜¤ê³  cursor.execute(\"SELECT id FROM artists\") dt = datetime.utcnow().strftime(\"%Y-%m-%d\") print(dt) for (id, ) in cursor.fetchall(): # Spotify APIë¥¼ í†µí•´ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  # .jsoní˜•íƒœë¡œ ì €ì¥í•œë’¤ì— with open('top_tracks.json', 'w') as f: for top_track in top_tracks: json.dump(top_track, f) f.write(os.linesep) # S3ì— importë¥¼ ì‹œí‚¨ë‹¤. s3 = boto3.resource('s3') object = s3.Object('spotify-chatbot-project', 'dt=&#123;&#125;/topt-racks.json'.format(dt))def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersif __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) í•„ìê°€ ì‚¬ìš©í•  SparkëŠ” Parquetì´ë¼ëŠ” formatì„ ë” ì„ í˜¸í•˜ê¸°ì—, Parquetìœ¼ë¡œ ë³€í˜•ì„ í•œí›„, compression(ì••ì¶•)ì„ í†µí•´ì„œ ë°ì´í„° Volumeë„ ì¤„ì´ë©´ì„œ ë” Performanceë„ ì¢‹ê²Œë” í•  ê²ƒì´ë‹¤. ìœ„ì—ì„œ ë§Œë“  top_tracks.json ë¡œì»¬ íŒŒì¼ì„ S3ì— ì €ì¥ì„ í•  ê²ƒì´ë‹¤. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import sysimport osimport base64import boto3import requestsimport loggingimport jsonimport pymysqlimport sys, os, argparsefrom datetime import datetimeimport pandas as pddef main(host, user, passwd, db, port, client_id, client_secret): try: # use_unicode=Trueë¥¼ ì¨ì•¼ í•œê¸€ê°™ì€ ê²½ìš°ëŠ” ê¹¨ì§€ì§€ ì•ŠëŠ”ë‹¤. conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") # ë³´í†µ ë¬¸ì œê°€ ì—†ìœ¼ë©´ 0 # ë¬¸ì œê°€ ìˆìœ¼ë©´ 1ì„ ë¦¬í„´í•˜ë„ë¡ ì•ˆì— ìˆ«ìë¥¼ ë„£ì–´ì¤€ë‹¤. sys.exit(1) headers = get_headers(client_id, client_secret) # RDS - ì•„í‹°ìŠ¤íŠ¸ IDë¥¼ ê°€ì ¸ì˜¤ê³  cursor.execute(\"SELECT id FROM artists LIMIT 10\") # Top tracksë¥¼ Spotifyì—ì„œ ê°€ì ¸ì˜¤ê³  top_tracks = [] for (id, ) in cursor.fetchall(): URL = 'https://api.spotify.com/v1/artists/&#123;id&#125;/top-tracks'.format(id) params = &#123; 'country' : 'US' &#125; r = requests.get(URL, params=params, headers=headers) raw = json.loads(r.text) top_tracks.extend(raw['tracks']) top_tracks = pd.DataFrame(top_tracks) top_tracks.to_parquet('top-tracks.parquet', engine='pyarrow', compression='snappy') sys.exit(0) dt = datetime.utcnow().strftime(\"%Y-%m-%d\") # S3ì— importë¥¼ ì‹œí‚¨ë‹¤. s3 = boto3.resource('s3') object = s3.Object('spotify-chatbot-project', 'dt=&#123;&#125;/top-tracks.parquet'.format(dt)) data = open('top-tracks.parquet', 'rb') object.put(Body=data)def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersif __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) ìœ„ì˜ ë°©ë²•ì²˜ëŸ¼ í•œë‹¤ë©´ errorê°€ ë°œìƒë˜ëŠ”ë°, ê·¸ ì´ìœ ëŠ” nested columnì´ë¼ í•´ì„œ ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ ì–´ë– í•œ keyê°’ì˜ ì•ˆì— listí˜•ì‹ìœ¼ë¡œ ì¡´ì¬í•˜ëŠ” structí˜•ì‹ì´ê¸° ë•Œë¬¸ì— ë°œìƒëœë‹¤. parquetí™” í•´ì„œ ì‚¬ìš©í•˜ë©´ ë°ì´í„°ë¥¼ í†µí•´ì„œ performanceê°€ ë¹¨ë¼ì§€ì§€ë§Œ, ê·¸ë ‡ê²Œ performanceë¥¼ ì¢‹ê²Œí•˜ë ¤ë©´ ì •í™•í•˜ê²Œ defineì„ í•´ ì£¼ì–´ì•¼ í•œë‹¤. ë³´í†µ json í˜•ì‹ìœ¼ë¡œ ê°€ì¥ raw í˜•íƒœë¡œ ì €ì¥í•œë‹¤ìŒ, processing jobì´ í•œ ë²ˆ ëŒì€í›„ì—, ìƒˆë¡œìš´ dataê°€ ê°€ì¥ raw dataê°€ S3ì— ë“¤ì–´ì™”ì„ë•Œ, triggerê°€ ë˜ì–´ì„œ í•´ë‹¹ parquetí™”ë¥¼ í•˜ê³  ì‹¶ì€ ëª‡ê°œì˜ ë°ì´í„°ë§Œ ë½‘ì€ í›„ ë‹¤ì‹œ ëŒì•„ì„œ ë‹¤ë¥¸ S3 bucketì•ˆì— ì €ì¥í•˜ëŠ” ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ê±°ì¹œë‹¤. ì•ìœ¼ë¡œì˜ ì‘ì—…ì€ jsonpathë¼ëŠ” packageê°€ í•„ìš”í•˜ë¯€ë¡œ ì•„ë˜ì˜ ì½”ë“œì²˜ëŸ¼ ì„¤ì¹˜ë¥¼ í•´ì£¼ì–´ì•¼ í•œë‹¤. 1pip install jsonpath --user ì´ì œ top_tracks ë¿ë§Œ ì•„ë‹ˆë¼ Audio Featureë“¤ë„ ì¶”ê°€í•´ì„œ parquet í˜•íƒœë¡œ S3ì— ì €ì¥í•´ ì¤„ ê²ƒì´ë‹¤. í•„ìì˜ ê²½ìš° êµ­ê°€ì½”ë“œëŠ” USì— ëŒ€í•´ì„œë§Œ ìš°ì„  ì‹¤í–‰í–ˆìœ¼ë©°, artist_idë“¤ ì¤‘ Audio featureê°€ USì—ì„œëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ” artistë“¤ì´ ìˆì–´ ì´ ë¶€ë¶„ì€ ë‚˜ì¤‘ì— ë‹¤ë¥¸ êµ­ê°€ë‚˜ artistë“¤ì— ëŒ€í•´ ë™ì¼í•œ í˜„ìƒìœ¼ë¡œ errorê°€ ë°œìƒë˜ëŠ” ê²½ìš°ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ifë¬¸ìœ¼ë¡œ Nullê°’ì´ í¬í•¨ë˜ì–´ìˆëŠ”ì§€ ì•„ë‹Œì§€ë¥¼ checkí•´ë³¸ë’¤ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í–ˆë‹¤. spotify_make_s3.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151import sysimport osimport base64import boto3import requestsimport loggingimport jsonimport pymysqlimport sys, os, argparsefrom datetime import datetimeimport pandas as pdimport jsonpathfrom pandas.io.json import json_normalizedef main(host, user, passwd, db, port, client_id, client_secret): try: # use_unicode=Trueë¥¼ ì¨ì•¼ í•œê¸€ê°™ì€ ê²½ìš°ëŠ” ê¹¨ì§€ì§€ ì•ŠëŠ”ë‹¤. conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") # ë³´í†µ ë¬¸ì œê°€ ì—†ìœ¼ë©´ 0 # ë¬¸ì œê°€ ìˆìœ¼ë©´ 1ì„ ë¦¬í„´í•˜ë„ë¡ ì•ˆì— ìˆ«ìë¥¼ ë„£ì–´ì¤€ë‹¤. sys.exit(1) headers = get_headers(client_id, client_secret) # RDS - ì•„í‹°ìŠ¤íŠ¸ IDë¥¼ ê°€ì ¸ì˜¤ê³  cursor.execute(\"SELECT id FROM artists\") # jsonpathë¼ëŠ” packageë¥¼ í†µí•´ì„œ í•´ë‹¹ pathì•ˆì— ì–´ë–¤ ë°ì´í„°ë¥¼ insertí–ˆì„ë•Œ, # key ê°’ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ê·¸ì— í•´ë‹¹í•˜ëŠ” valueê°’ì„ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸ì´ë‹¤. top_track_keys = &#123; 'id' : 'id', 'name' : 'name', 'popularity' : 'popularity', 'external_url' : 'external_urls.spotify' &#125; # Top tracksë¥¼ Spotifyì—ì„œ ê°€ì ¸ì˜¤ê³  top_tracks = [] for (id, ) in cursor.fetchall(): URL = 'https://api.spotify.com/v1/artists/&#123;&#125;/top-tracks'.format(id) params = &#123; 'country' : 'US' &#125; r = requests.get(URL, params=params, headers=headers) raw = json.loads(r.text) for i in raw['tracks']: top_track = &#123;&#125; for k, v in top_track_keys.items(): top_track.update(&#123;k: jsonpath.jsonpath(i, v)&#125;) # ë°ì´í„°ë¥¼ mappingí•˜ê¸° ìœ„í•´ì„œ artist_idë¥¼ ì¶”ê°€í•œë‹¤. top_track.update(&#123;'artist_id': id&#125;) top_tracks.append(top_track) # track_id track_ids = [i['id'][0] for i in top_tracks] # parquetí™” í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ packageê°€ ìˆì§€ë§Œ pandasë¥¼ ì‚¬ìš©í•  ê²ƒ ì´ë‹¤. # í•„ìê°€ ì‚¬ìš©í•  SparkëŠ” Parquetì´ë¼ëŠ” formatì„ ë” ì„ í˜¸í•˜ê¸°ì—, Parquetìœ¼ë¡œ ë³€í˜•ì„ í•œí›„, # compression(ì••ì¶•)ì„ í†µí•´ì„œ ë°ì´í„° Volumeë„ ì¤„ì´ë©´ì„œ ë” Performanceë„ ì¢‹ê²Œë” í•  ê²ƒì´ë‹¤. # ìœ„ì—ì„œ ë§Œë“  top_tracks.json local íŒŒì¼ì„ S3ì— ì €ì¥ì„ í•  ê²ƒì´ë‹¤. top_tracks = pd.DataFrame(top_tracks) top_tracks.to_parquet('top-tracks.parquet', engine='pyarrow', compression='snappy') dt = datetime.utcnow().strftime(\"%Y-%m-%d\") # S3ì— importë¥¼ ì‹œí‚¨ë‹¤. s3 = boto3.resource('s3') # bucketì˜ keyê°’ì´ë¼ê³  í•˜ëŠ” data typeì„ ì •í•´ì•¼í•œë‹¤. # top tracksì™€ ê°™ì´ ì§€ì†ì ìœ¼ë¡œ ë³€í™”í•˜ëŠ” ë°ì´í„°ëŠ” ë°©ëŒ€í•œì–‘ìœ¼ë¡œ ëŠ˜ì–´ë‚¬ì„ë•ŒëŠ” # ê²°êµ­ì—” ìª¼ê°œì„œ scanì„ í•´ì•¼í•˜ë¯€ë¡œ ì–´ë– í•œ í˜•ì‹ì„ í†µí•´ì„œ Sparkë‚˜ Hadoopì´ readableí•œ í˜•ì‹ìœ¼ë¡œ # partitionì„ ë§Œë“¤ì–´ë†”ì•¼ Sparkë‚˜ Haddepì—ì„œ ìµœê·¼ì˜ ë°ì´í„°ë¥¼ ê°–ê³ ìˆëŠ” ë§ˆì§€ë§‰ partitionë§Œ í™•ì¸í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. # í•„ìëŠ” ë‚ ì§œë¥¼ í†µí•´ ì‹œì ì´ ì–¸ì œì¼ì§€ ì•Œ ìˆ˜ ìˆë„ë¡ partitionì„ êµ¬ë¶„ì§€ì–´ ì¤„ ê²ƒì´ë‹¤. object = s3.Object('spotify-chatbot-project', 'top-tracks/dt=&#123;&#125;/top-tracks.parquet'.format(dt)) data = open('top-tracks.parquet', 'rb') object.put(Body=data) tracks_batch = [track_ids[i: i+100] for i in range(0, len(track_ids), 100)] audio_features = [] null_features = [] for batch in tracks_batch: ids = ','.join(batch) URL = 'https://api.spotify.com/v1/audio-features/?ids=&#123;&#125;'.format(ids) r = requests.get(URL, headers=headers) if 'null' in r.text: raw = json.loads(r.text) for i in raw['audio_features']: if pd.isnull(i) == False: # audio_featuresëŠ” dictionary keyê°’ ì•ˆì— ë˜ ë‹¤ë¥¸ listí˜•ì‹ìœ¼ë¡œ ë˜ì–´ìˆì§€ ì•Šìœ¼ë¯€ë¡œ # ê·¸ëƒ¥ ì‚¬ìš©í•´ë„ ëœë‹¤. null_features.append(i) else: raw = json.loads(r.text) audio_features.extend(raw['audio_features']) audio_features.extend(null_features) audio_features = json_normalize(audio_features) audio_features.to_parquet('audio_features.parquet', engine='pyarrow', compression='snappy') s3 = boto3.resource('s3') object = s3.Object('spotify-chatbot-project', 'audio_features/dt=&#123;&#125;/top-tracks.parquet'.format(dt)) data = open('audio_features.parquet', 'rb') object.put(Body=data)def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersif __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) ìœ„ì˜ ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ S3ì— ê°ê°ì˜ íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ì €ì¥ë˜ì–´ ì—…ë°ì´íŠ¸ì‹œì— í•´ë‹¹ ì‹œê°„ê³¼ ë‚ ì§œì— ì˜í•´ partitionë˜ì–´ì§€ëŠ” ë°ì´í„° ì €ì¥ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì— top-tracks ë°ì´í„°ì™€ audio feature ë°ì´í„° ë¿ë§Œì•„ë‹ˆë¼ artist ë°ì´í„°ë„ S3ì— parquetí˜•ì‹ìœ¼ë¡œ ì €ì¥í•´ ì¤„ ê²ƒì´ë‹¤. spotify_s3_artist.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import sysimport osimport base64import boto3import requestsimport loggingimport jsonimport pymysqlimport sys, os, argparsefrom datetime import datetimeimport pandas as pdimport jsonpathfrom pandas.io.json import json_normalizedef main(host, user, passwd, db, port, client_id, client_secret): try: # use_unicode=Trueë¥¼ ì¨ì•¼ í•œê¸€ê°™ì€ ê²½ìš°ëŠ” ê¹¨ì§€ì§€ ì•ŠëŠ”ë‹¤. conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") # ë³´í†µ ë¬¸ì œê°€ ì—†ìœ¼ë©´ 0 # ë¬¸ì œê°€ ìˆìœ¼ë©´ 1ì„ ë¦¬í„´í•˜ë„ë¡ ì•ˆì— ìˆ«ìë¥¼ ë„£ì–´ì¤€ë‹¤. sys.exit(1) # RDS - ì•„í‹°ìŠ¤íŠ¸ IDë¥¼ ê°€ì ¸ì˜¤ê³  cursor.execute(\"SELECT * FROM artists\") colnames = [d[0] for d in cursor.description] artists = [dict(zip(colnames, row)) for row in cursor.fetchall()] artists = pd.DataFrame(artists) artists.to_parquet('artists.parquet', engine='pyarrow', compression='snappy') dt = datetime.utcnow().strftime(\"%Y-%m-%d\") s3 = boto3.resource('s3') object = s3.Object('spotify-chatbot-project', 'artists/dt=&#123;&#125;/artists.parquet'.format(dt)) data = open('artists.parquet', 'rb') object.put(Body=data)if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret)","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering (AWS DynamoDB ì‚¬ìš©í•´ì„œ ì˜¤ë””ì˜¤ feature í™œìš©í•˜ê¸°)","slug":"data_engineering_06","date":"2020-02-20T09:25:18.000Z","updated":"2020-02-24T13:13:09.501Z","comments":true,"path":"2020/02/20/data_engineering_06/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/20/data_engineering_06/","excerpt":"","text":"NoSQL RDBMSê°€ ì œí•œë˜ëŠ” ì ë“¤ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ í™œìš©í•˜ê¸° ì‹œì‘í–ˆë‹¤. NoSQL ê°™ì€ ê²½ìš°ëŠ” RDB ì²˜ëŸ¼ íŠ¹ì • Structureê°€ ì •í•´ì ¸ ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ë°ì´í„°ì˜ ì¶”ê°€ê°€ ìš©ì´í•˜ë‹¤. Scalability SQL Databases are vertically scalable - CPU, RAM or SSD ì—¬ëŸ¬ê°€ì§€ ì‚¬ì–‘ë“¤ì„ ì •í•´ë†“ì•„ì„œ ì •í•´ì§„ resourceë¥¼ ì´ˆê³¼í•  ê²½ìš° ë˜ë‹¤ë¥¸ resourceë¥¼ ì¶”ê°€ ì‹œì¼œì£¼ì–´ì•¼ í•œë‹¤. NoSQL Databases are horizontally scalable - Sharding / Partitioning NoSQLì€ Partitioningì„ í†µí•´ ë‹¤ë¥¸ resourceì—ì„œ ë‚¨ëŠ” ë¶€ë¶„ì„ ë¶€ì¡±í•œ ë¶€ë¶„ì— ë„˜ê²¨ì¤„ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ partitionì´ ë¬´ì—‡ì´ëƒ ì¡°ê¸ˆ ë” ìì„¸í•˜ê²Œ ì•Œì•„ë³´ìë©´, ìš°ì„  í•´ì„ ê·¸ëŒ€ë¡œ ë‚˜ëˆˆë‹¤ëŠ” ì˜ë¯¸ê°€ ìˆë‹¤. partitionì„ í•´ì•¼í•˜ëŠ” ì´ìœ  ì¤‘ ê°€ì¥ ì¤‘ìš”í•œ ì´ìœ ëŠ” ë°ì´í„°ê°€ ëŠ˜ì–´ë‚˜ë©´ ëŠ˜ì–´ë‚ ìˆ˜ë¡ SQLì„ í†µí•˜\u001dì—¬ queryë¬¸ì˜ performanceê°€ ëŠë ¤ì§ˆìˆ˜ ë°–ì— ì—†ëŠ”ë°, ì´ëŸ° ë¶€ë¶„ì„ ë°©ì§€í•˜ê³ ì ë‚˜ëˆ„ì–´ ì£¼ëŠ” ê²ƒì´ë‹¤. ë°ì´í„°ì˜ ì–‘ì´ ëŠ˜ì–´ë‚˜ëŠ”ë° ë§¤ë²ˆ ëª¨ë“  ë°ì´í„°ë¥¼ ë‹¤ ì½ì–´ì™€ì„œ ì‘ì—…ì„ í•˜ëŠ” ê²ƒì€ ë„ˆë¬´ ë¹„íš¨ìœ¨ì ì´ê¸° ë•Œë¬¸ì´ë‹¤. ë¬¼ë¡  ë¬´ì¡°ê±´ì ìœ¼ë¡œ NoSQLì´ ì¢‹ë‹¤ëŠ” ì˜ë¯¸ëŠ” ì•„ë‹ˆë‹¤. NoSQLì˜ ê²½ìš°ì—ëŠ” ì•„ë˜ì˜ ê·¸ë¦¼ì—ì„œ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì²˜ëŸ¼ ì• ì´ˆì— ìš°ë¦¬ê°€ lookup(ì°¸ì¡°)í•´ì•¼ í•˜ëŠ” ë°ì´í„°ì˜ ì–‘ì„ ì¤„ì—¬ì„œ ë‚˜ëˆ„ì–´ ë†“ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” Sparkì˜ ê²½ìš°ì—ë„ ë™ì¼í•˜ë‹¤. Vertical partitioningì„ í•˜ëŠ” ì´ìœ ëŠ” ì¤‘ë³µì ì¸ ë°ì´í„°ëŠ” ERDë¥¼ Tableì„ ë¶„ë¦¬í•´ì„œ ê´€ë¦¬ë¥¼ í•˜ê²Œë” í•´ì„œ Normalizationì„ í•˜ì˜€ëŠ”ë°, Normalizationì„ ì§„í–‰í•˜ê³ ë„ column ìˆ˜ê°€ ëŠ˜ì–´ë‚˜ë©´, í•´ë‹¹ Tableì„ ì½ëŠ”ë° ì†ë„ê°€ ëŠë ¤ì§ˆìˆ˜ê°€ ìˆë‹¤. ë˜í•œ, ì–´ë–¤ columnë“¤ì€ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ê°€ ë  ìˆ˜ë„ ìˆì§€ë§Œ, ì–´ë–¤ columnë“¤ì€ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ê°€ ë˜ì§€ ì•Šì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì‚¬ìš©í•œë‹¤. RDBMì—ì„œëŠ” ë‚˜ëˆŒìˆ˜ëŠ” ìˆì§€ë§Œ, ë°ì´í„°ê°€ ì—„ì²­ë‚˜ê²Œ ë§ì€ ì–‘ì´ì–´ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ”ë° ì†ë„ì˜ í–¥ìƒì„ ê¸°ëŒ€í•œë‹¤ë©´ ì‚¬ìš©í•˜ì§€ë§Œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ëŠ” ë§ì´ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. Horizontal Partitionì€ NoSQLì—ì„œ ë¬´ì¡°ê±´ ì‚¬ìš©ëœë‹¤. ì–´ë–¤ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•  ë•Œ Sharded Keyë¡œ ë¹ ë¥´ê²Œ Accessí•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ë‹¤. ë§ˆì¹˜ RDBì—ì„œ primary keyë¥¼ í†µí•œ ë¹ ë¥¸ searchì™€ ìœ ì‚¬í•œ ëŠë‚Œì´ë‹¤. ë™ì¼í•œ ì»¬ëŸ¼ì´ì§€ë§Œ ë°ì´í„°ì˜ ì–‘ì„ ë‚˜ëˆ„ì–´ì„œ ê°ê° ì €ì¥í•˜ëŠ” ë°©ì‹ì´ë‹¤. Partition keyë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ ì£¼ëŠ”\u001dë°, AWSì•ˆì˜ DynamoDBë¥¼ ë§Œë“¤ë©´ì„œ í™•ì¸í•´ ë³¼ ê²ƒì´ë‹¤. spotify Developer artistâ€™s top tracksë¥¼ ë“¤ì–´ê°€ë³´ë©´, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ path patameterë¡œ í•´ë‹¹ artist idë¥¼ ë°›ì•„ì„œ ê²°ê³¼ë¥¼ ì¶œë ¥ í•´ì¤€ë‹¤. NoSQLì´ë¯€ë¡œ track ì „ì²´ë¥¼ ê°€ì ¸ì™€ë„ ë˜ì§€ë§Œ, ê·¸ ì•ˆì—ì„œ í•„ìš”í•œ ë°ì´í„°ë¥¼ ë˜ ì‘ì—…ì„ í•´ì•¼í•˜ëŠ”ë°, artist í•œëª…ì´ ì—¬ëŸ¬ê°œì˜ trackì„ ê°€ì§€ê³  ìˆëŠ” ê²½ìš°ë„ ìˆìœ¼ë¯€ë¡œ artist idë§Œì„ partition keyë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. ì™œëƒí•˜ë©´ partition keyë¡œ ì‚¬ìš©ë˜ëŠ” ê²½ìš° í•´ë‹¹ columnì— ë™ì¼í•œ ê°’ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ artist_idë¥¼ partition keyë¡œ í•´ë†“ê³ , track idë¥¼ sort keyë¡œ ì‚¬ìš©í•  ê²ƒì´ë‹¤. AWS DynamoDB ì‚¬ìš©í•˜ê¸° ë¨¼ì € AWSì— ë¡œê·¸ì¸ì„ ë§ˆì¹œ í›„, ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ service íƒ­ì„ ëˆŒëŸ¬ ë‚´ë ¤ë³´ë©´ ì´ì „ì˜ RDSë¥¼ ë§Œë“¤ì—ˆì„ ë•Œ ë³´ì•˜ë˜, Databaseë€ì—ì„œ DynamoDBë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. í´ë¦­í•˜ë©´ í•˜ê²Œ ë˜ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì€ í˜ì´ì§€ê°€ ë‚˜ì˜¬ ê²ƒì´ë‹¤. ê·¸ ë‹¤ìŒì€, ìœ„ì˜ í˜ì´ì§€ì—ì„œ ë³´ì´ëŠ” ê²ƒì²˜ëŸ¼ create tableì„ í´ë¦­í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ í˜ì´ì§€ë¡œ ì´ë™ ë  ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ Table nameì€ ë§ ê·¸ëŒ€ë¡œ Tableì˜ ì´ë¦„ì„ ì§€ì •í•´ì£¼ëŠ” ë¶€ë¶„ì´ê³ , Partition key ë¶€ë¶„ì€ RDSì—ì„œ Primary Keyì™€ ë™ì¼í•œ ì—­í• (ë¹ ë¥´ê²Œ ì°¸ì¡°í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ”)ì„ í•˜ëŠ” ë¶€ë¶„ì´ë¼ê³  ìƒê°í•˜ë©´ëœë‹¤. ê·¸ëŸ¬ë¯€ë¡œ í•´ë‹¹ artist_idì— í•´ë‹¹í•˜ëŠ” rowëŠ” ìœ ì¼í•´ì•¼ í•œë‹¤. í•˜ì§€ë§Œ sort keyë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ë°, ì´ ë¶€ë¶„ì€ í•´ë‹¹ ì„¸ì…˜ì´ ì–¸ì œ ìƒì„±ë˜ì—ˆëŠ”ì§€ë¥¼ ì•Œë ¤ì£¼ê¸° ìœ„í•œ ì—­í• ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ìœ„ì—ì„œ ë§í–ˆë˜ ê²ƒ ì²˜ëŸ¼ partition keyëŠ” artist_idë¡œ sort keyëŠ” track idë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ProvisionedëŠ” serverë¥¼ ë„ìš°ê¸° ë•Œë¬¸ì— ê·¸ ìƒí™© ì•ˆì—ì„œ free tierë¥¼ ì‚¬ìš©ê°€ëŠ¥í•œ ì‚¬ëŒë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ë°, Auto-Scaleì´ ê°€ëŠ¥í•œ ë¶€ë¶„ì´ ìˆë‹¤. í•˜ì§€ë§Œ Auto-Scaleì´ ì‹¤ì§ˆì ìœ¼ë¡œ ì ìš©ì´ ë˜ëŠ” ì‹œê°„ì°¨ê°€ ìˆì–´ í•´ë‹¹ Scalingì´ ì§„í–‰ë˜ëŠ” ë™ì•ˆì€ trafficì´ ë§ì•„ì§„ë‹¤ë©´ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆì„ ìˆ˜ ìˆë‹¤. On-demandëŠ” ì–´ëŠ ì •ë„ í•„ìš”í•œì§€ë¥¼ ëª¨ë¥¼ ê²½ìš°ì— awsì—ì„œ ì•Œì•„ì„œ scalingì„ í•´ì£¼ê³  ì“°ì´ëŠ” ë§Œí¼ë§Œ ëˆì„ ì§€ë¶ˆí•  ìƒí™©ì¼ ë•Œ ì‚¬ìš©í•œë‹¤. í•„ìëŠ” ì•„ë˜ì™€ ê°™ì´ ì„¤ì •í•œ í›„ì—, DynamoDBë¥¼ ìƒì„±í•˜ì˜€ë‹¤. ìƒì„±í•œ í›„ì—ëŠ” ìƒì„±ëœ DynamoDBì˜ í˜ì´ì§€ê°€ ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜¨ë‹¤. Read capacity units(ì½ê¸° ìš”ì²­ ë‹¨ìœ„ 1)ì€ ê°•ë ¥íˆ ì¼ê´€ëœ ì½ê¸° ìš”ì²­ 1 ë˜ëŠ” ìµœì¢…ì  ì¼ê´€ëœ ì½ê¸° ìš”ì²­ 2(ìµœëŒ€ 4 KB í¬ê¸° í•­ëª©ì˜ ê²½ìš°)ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. íŠ¸ëœì­ì…˜ ì½ê¸° ìš”ì²­ì€ ìµœëŒ€ 4 KB í¬ê¸° í•­ëª©ì˜ 1íšŒ ì½ê¸°ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° 2ê°œì˜ ì½ê¸° ìš”ì²­ ë‹¨ìœ„ê°€ í•„ìš”í•©ë‹ˆë‹¤. Write capacity units(ì“°ê¸° ìš”ì²­ ë‹¨ìœ„ 1)ì€ ìµœëŒ€ 1 KB í¬ê¸°ì˜ í•­ëª©ì— ëŒ€í•´ 1íšŒ ì“°ê¸°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì°¸ê³  DBë¥¼ ì‚¬ìš©í•˜ë‹¤ê°€ primary keyê°€ Tableë‚´ì—ì„œ ìœ ì¼í•œ RDBMSì™€ëŠ” ë‹¤ë¥´ê²Œ partition keyë¥¼ ì¶”ê°€í•  ìˆ˜ë„ ìˆë‹¤. í•„ìì˜ ê²½ìš° ì¶”ê°€ë¥¼ í•  í•„ìš”ê°€ ì—†ì—ˆê¸°ì— í•˜ì§„ ì•Šì•˜ì§€ë§Œ, ì¶”ê°€í•˜ì—¬ ì‚¬ìš©í•  ê²½ìš°ëŠ” ì•„ë˜ì™€ ê°™ì´ ìƒì„±í•´ì£¼ë©´ëœë‹¤. Audio Feature DynamoDBì— insert í•˜ê¸° AWS serviceë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•œ íŒ¨í‚¤ì§€ì¸ boto3ë¥¼ ì„¤ì¹˜í•œë‹¤. boto3ëŠ” AWSê°€ ì œê³µí•˜ëŠ” Python SDKì˜ ì´ë¦„ì´ë‹¤. DynamoDBì— insertë¥¼ í•˜ê±°ë‚˜ selectí•´ì˜¤ê±°ë‚˜ í•  ë•Œ í•„ìš”í•˜ë‹¤ê³  ê°„ë‹¨í•˜ê²Œ ì´í•´í•´ë„ ì¢‹ì„ ê²ƒ ê°™ë‹¤. consoleì—ì„œëŠ” ë¡œê·¸ì¸ì„ í†µí•´ ì¸ì¦ì ˆì°¨ë¥¼ ê±°ì¹˜ì§€ë§Œ boto3ë¥¼ í†µí•œ ì¸ì¦ë°©ì‹ì€ ì´ì „ì˜ ìš°ë¦¬ê°€ ì„¤ì •í•´ ë†“ì•˜ë˜ aws configureë¥¼ í†µí•´ ì„¤ì •ë˜ìˆëŠ” ìƒíƒœë¥¼ í†µí•´ ì¸ì¦í•˜ë¯€ë¡œ AWSë¥¼ consoleì— ë¡œê·¸ì¸í•˜ì§€ ì•Šê³ ë„ ì»¨íŠ¸ë¡¤í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. boto3 documents 12#pip3 install boto3 --userpip install boto3 --user ì˜¤ë””ì˜¤ í”¼ì²˜ ì‚´í´ë³´ê¸° ì „ì²´ì ì¸ flowëŠ” ì´ë¯¸ RDSë°›ì•„ë†“ì€ artists Tableì—ì„œ artist_idë“¤ì„ ë°›ì•„ì˜¨ í›„, Spotify APIë¥¼ í†µí•´ í•´ë‹¹ artist_idì˜ top track ì •ë³´ë¥¼ ë°›ì•„ì„œ AWSì˜ DynamoDBì— ì €ì¥í•  ê²ƒì´ë‹¤. Spotify APIì—ì„œëŠ” í•´ë‹¹ artist_id ë¿ë§Œì•„ë‹ˆë¼ êµ­ê°€ì—ëŒ€í•œ parameterë„ ë°›ëŠ”ë°, ì´ëŠ” êµ­ê°€ë§ˆë‹¤ í•´ë‹¹ artistì˜ top trackì´ ë‹¤ë¥¼ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì´ë‹¤. í•„ìëŠ” ìš°ì„  US(ë¯¸êµ­)ê³¼ CA(ìºë‚˜ë‹¤)ì—ëŒ€í•´ì„œë§Œ ì‚´í´ ë³¼ ê²ƒì´ë‹¤. boto3ì—ì„œ DynamoDB ì‚¬ìš©ë²• documents 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import sysimport osimport boto3import requestsimport base64import jsonimport loggingimport pymysqldef main(host, user, passwd, db, port, client_id, client_secret): try: dynamodb = boto3.resource('dynamodb', region_name='ap-northeast-2', endpoint_url='http://dynamodb.ap-northeast-2.amazonaws.com') except: logging.error('could not connect to dynamodb') sys.exit(1) try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") sys.exit(1) headers = get_headers(client_id, client_secret) # ì‚¬ìš©í•  DynamoDBë¡œ ë§Œë“  Table ì´ë¦„ table = dynamodb.Table('top_tracks') cursor.execute('SELECT id FROM artists') # êµ­ê°€ëŠ” ë¯¸êµ­ê³¼ ìºë‚˜ë‹¤ë§Œì„ ì„ íƒí•  ê²ƒì´ë‹¤. countries = ['US', 'CA'] for country in countries: for (artist_id, ) in cursor.fetchall(): URL = \"https://api.spotify.com/v1/artists/&#123;&#125;/top-tracks\".format(artist_id) params = &#123; 'country': country &#125; r = requests.get(URL, params=params, headers=headers) raw = json.loads(r.text) for track in raw['tracks']: # ìœ„ì—ì„œ ê°€ì ¸ì˜¨ trackì—ì„œëŠ” artist_idê°€ ì—†ê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì´ ë¨¼ì € dictionaryë¡œ ë§Œë“¤ì–´ì¤Œ. data = &#123; 'artist_id': artist_id, 'country': country &#125; data.update(track) table.put_item( Item=data )def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersif __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) ì•„ë˜ì™€ ê°™ì´ AWS console ì°½ì—ì„œë„ ë°ì´í„°ê°€ insertëœ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. DynamoDB ì €ì¥ëœ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ boto3ë¥¼ ì´ìš©í•˜ì—¬ DynamoDBì—ì„œ ë°ì´í„°ë¥¼ selectí•˜ì—¬ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•ì€ get_item í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. ë‹¨, í•´ë‹¹ Tableì˜ Partition key ë‚˜ sort keyë¡œ ì§€ì •í•œ columnì˜ ê°’ì„ ì „ë¶€ ì…ë ¥í•´ ì£¼ì–´ì•¼ error ì—†ì´ ì‘ë™ëœë‹¤. 123456789101112131415161718192021222324import sysimport osimport boto3def main(): try: dynamodb = boto3.resource('dynamodb', region_name='ap-northeast-2', endpoint_url='http://dynamodb.ap-northeast-2.amazonaws.com') except: logging.error('could not connect to dynamodb') sys.exit(1) table = dynamodb.Table('top_tracks') response = table.get_item( Key=&#123; 'artist_id' : '0L8ExT028jH3ddEcZwqJJ5', 'id' : '0uppYCG86ajpV2hSR3dJJ0' &#125; ) print(response)if __name__=='__main__': main() ê²°ê³¼1&#123;'Item': &#123;'is_playable': True, 'duration_ms': Decimal('282906'), 'external_ids': &#123;'isrc': 'USWB19901574'&#125;, 'uri': 'spotify:track:0uppYCG86ajpV2hSR3dJJ0', 'country': 'US', 'name': 'Give It Away', 'album': &#123;'total_tracks': Decimal('19'), 'images': [&#123;'width': Decimal('640'), 'url': 'https://i.scdn.co/image/ab67616d0000b273153d79816d853f2694b2cc70', 'height': Decimal('640')&#125;, &#123;'width': Decimal('300'), 'url': 'https://i.scdn.co/image/ab67616d00001e02153d79816d853f2694b2cc70', 'height': Decimal('300')&#125;, &#123;'width': Decimal('64'), 'url': 'https://i.scdn.co/image/ab67616d00004851153d79816d853f2694b2cc70', 'height': Decimal('64')&#125;], 'artists': [&#123;'name': 'Red Hot Chili Peppers', 'href': 'https://api.spotify.com/v1/artists/0L8ExT028jH3ddEcZwqJJ5', 'id': '0L8ExT028jH3ddEcZwqJJ5', 'type': 'artist', 'external_urls': &#123;'spotify': 'https://open.spotify.com/artist/0L8ExT028jH3ddEcZwqJJ5'&#125;, 'uri': 'spotify:artist:0L8ExT028jH3ddEcZwqJJ5'&#125;], 'release_date': '1991-09-24', 'name': 'Blood Sugar Sex Magik (Deluxe Edition)', 'album_type': 'album', 'release_date_precision': 'day', 'href': 'https://api.spotify.com/v1/albums/30Perjew8HyGkdSmqguYyg', 'id': '30Perjew8HyGkdSmqguYyg', 'type': 'album', 'external_urls': &#123;'spotify': 'https://open.spotify.com/album/30Perjew8HyGkdSmqguYyg'&#125;, 'uri': 'spotify:album:30Perjew8HyGkdSmqguYyg'&#125;, 'popularity': Decimal('72'), 'artists': [&#123;'name': 'Red Hot Chili Peppers', 'href': 'https://api.spotify.com/v1/artists/0L8ExT028jH3ddEcZwqJJ5', 'id': '0L8ExT028jH3ddEcZwqJJ5', 'type': 'artist', 'external_urls': &#123;'spotify': 'https://open.spotify.com/artist/0L8ExT028jH3ddEcZwqJJ5'&#125;, 'uri': 'spotify:artist:0L8ExT028jH3ddEcZwqJJ5'&#125;], 'disc_number': Decimal('1'), 'href': 'https://api.spotify.com/v1/tracks/0uppYCG86ajpV2hSR3dJJ0', 'track_number': Decimal('9'), 'external_urls': &#123;'spotify': 'https://open.spotify.com/track/0uppYCG86ajpV2hSR3dJJ0'&#125;, 'artist_id': '0L8ExT028jH3ddEcZwqJJ5', 'preview_url': 'https://p.scdn.co/mp3-preview/fcdf3224d230b26b637418c2d1028bc482db7fce?cid=93b8cdc701294ab7992eaf370c7ba1cd', 'is_local': False, 'id': '0uppYCG86ajpV2hSR3dJJ0', 'explicit': False, 'type': 'track'&#125;, 'ResponseMetadata': &#123;'RequestId': 'KKGRB8AQD7G3H83UAA4J40TPE7VV4KQNSO5AEMVJF66Q9ASUAAJG', 'HTTPStatusCode': 200, 'HTTPHeaders': &#123;'x-amzn-requestid': 'KKGRB8AQD7G3H83UAA4J40TPE7VV4KQNSO5AEMVJF66Q9ASUAAJG', 'x-amz-crc32': '1465669044', 'content-type': 'application/x-amz-json-1.0', 'content-length': '2296', 'date': 'Fri, 21 Feb 2020 12:56:26 GMT'&#125;, 'RetryAttempts': 0&#125;&#125; ê·¸ëŸ°ë°, ì—¬ê¸°ì„œ ì–´ë– í•œ í˜•ì‹ìœ¼ë¡œë“  DBë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ queryë„ ì‚¬ìš©í•  í•„ìš”ê°€ ìˆì„ ê²ƒì´ë‹¤. ì´ëŸ° ê²½ìš° ì‚¬ìš©í•˜ëŠ” ê²ƒì´ Scanningê³¼ Queryingì´ë‹¤. Queryingì€ Partition key(Primary key)ë¥¼ ì•Œê³ ìˆì„ ê²½ìš°ì— ì‚¬ìš©í•˜ê³ , Scanningì€ ê·¸ ì´ì™¸ì˜ ë¶€ìˆ˜ì ì¸ ë‹¤ë¥¸ attribute(column)ì˜ valueë¥¼ ì•Œê³  ìˆì„ ë•Œ ì‚¬ìš©í•œë‹¤. partition keyì™€ ë‹¤ë¥¸ attributeë¥¼ ì‚¬ìš©í•œ queryë¬¸123456789101112131415161718192021222324import sysimport osimport boto3from boto3.dynamodb.conditions import Key, Attrdef main(): try: dynamodb = boto3.resource('dynamodb', region_name='ap-northeast-2', endpoint_url='http://dynamodb.ap-northeast-2.amazonaws.com') except: logging.error('could not connect to dynamodb') sys.exit(1) table = dynamodb.Table('top_tracks') response = table.query( KeyConditionExpression=Key('artist_id').eq('0L8ExT028jH3ddEcZwqJJ5') FilterExpression=Attr('popularity').gt(80) ) print(response['Items'])if __name__=='__main__': main() Attributeë§Œ ì‚¬ìš©í•˜ëŠ” scanning í—ˆë‚˜, ë˜ë„ë¡ì´ë©´ queryingì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤. ì™œëƒí•˜ë©´, scanì„ keyê°’ì´ ì—†ê¸° ë•Œë¬¸ì— ì „ì²´ ë°ì´í„°ë¥¼ í•œë²ˆ ë‹¤ ëŒì•„ì„œ ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ë“¤ì„ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ë¯€ë¡œ, ë°ì´í„°ì˜ ì–‘ì´ ë§ì•„ rowì˜ ìˆ˜ê°€ ë§ë‹¤ë©´, ì†ë„ê°€ ëŠë ¤ì§€ê¸° ë•Œë¬¸ì´ë‹¤. 1234567891011121314151617181920212223import sysimport osimport boto3from boto3.dynamodb.conditions import Key, Attrdef main(): try: dynamodb = boto3.resource('dynamodb', region_name='ap-northeast-2', endpoint_url='http://dynamodb.ap-northeast-2.amazonaws.com') except: logging.error('could not connect to dynamodb') sys.exit(1) table = dynamodb.Table('top_tracks') response = table.scan( FilterExpression=Attr('popularity').gt(80) ) print(response['Items'])if __name__=='__main__': main()","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"NLP ì‹¤ìŠµ Chat bot ë§Œë“¤ê¸°","slug":"NLP_13","date":"2020-02-20T07:01:42.000Z","updated":"2020-02-20T07:19:38.147Z","comments":true,"path":"2020/02/20/NLP_13/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/20/NLP_13/","excerpt":"","text":"ì§€ê¸ˆê¹Œì§€ ë‘ ê°€ì§€ ë¬¸ì œì— ëŒ€í•´ ì‹¤ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤. 1) í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•´ì„œ ê° í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œë¥¼ ì‹¤ìŠµí–ˆê³ , 2) ë‘ ê°œì˜ í…ìŠ¤íŠ¸ê°€ ìˆì„ ë•Œ ê° í…ìŠ¤íŠ¸ë¼ë¦¬ì˜ ìœ ì‚¬ë„ë¥¼ íŒë‹¨í•˜ëŠ” ë¬¸ì œë¥¼ ì‹¤ìŠµí–ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ë²ˆì—ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë‹¨ìˆœíˆ ë¶„ì„í•´ì„œ ë¶„ë¥˜ë‚˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ì§ì—… ë¬¸ì¥ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” text generation ë¬¸ì œë¥¼ ì‹¤ìŠµí•´ ë³¼ ê²ƒì´ë‹¤. text generationì—ë„ ë§ì€ ë¬¸ì œê°€ ìˆì§€ë§Œ â€˜ìì—°ì–´ì˜ ê½ƒâ€™ì´ë¼ê³  ë¶ˆë¦¬ëŠ” â€˜Chat botâ€™ì„ ì œì‘í•´ ë³¼ ê²ƒì´ë‹¤. Chat bot ë§Œë“¤ê¸° ì¼ë°˜ì ìœ¼ë¡œ chat botì„ ì œì‘í•˜ëŠ” ë°©ë²•ì€ ë§¤ìš° ë‹¤ì–‘í•˜ë‹¤. ë‹¨ìˆœí•˜ê²Œ rule based ê¸°ë°˜ìœ¼ë¡œ ì œì‘í•  ìˆ˜ë„ ìˆê³ , machine learningì„ ì„ì€ hybrid ê¸°ë°˜, íŠ¹ì • ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ë™ì‘ ê°€ëŠ¥í•´ì§€ëŠ” ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜ê¹Œì§€ ì •ì˜í•˜ëŠ” ì‚¬ëŒì— ë”°ë¼ ì œì‘ ë°©ë²•ì´ ë§¤ìš° ë‹¤ì–‘í•˜ë‹¤. ë¬¼ë¡ , ì •ì˜í•˜ëŠ” ê²ƒì€ ì–´ë””ê¹Œì§€ë‚˜ ê°€ìš©í•  ë°ì´í„°ì˜ ì„±ê²©ì— ë§¤ìš° ì˜ì¡´ì ì¼ ê²ƒì´ë‹¤. í•„ìëŠ” ìš°ì„  ì œì‘ë°©ë²• ì¤‘ì—ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í†µí•œ chat botì„ ë§Œë“¤ì–´ ë³¼ ê²ƒì´ë‹¤. ë˜í•œ chat botì„ ë§Œë“¤ê¸° ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì—ë„ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆì§€ë§Œ ê·¸ ì¤‘ì—ì„œ ë²ˆì—­ ë¬¸ì œì—ì„œ ì´ë¯¸ ì„±ëŠ¥ì´ ì…ì¦ ëœ Seq2seqë¥¼ í™œìš©í•˜ì—¬ ì œì‘í•  ê²ƒì´ë‹¤.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP ì‹¤ìŠµ ìœ ì‚¬ë„ë¥¼ ë°˜ì˜í•œ ê²€ìƒ‰ í‚¤ì›Œë“œ ìµœì í™”","slug":"NLP_12","date":"2020-02-11T08:16:56.000Z","updated":"2020-02-20T07:19:46.701Z","comments":true,"path":"2020/02/11/NLP_12/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/11/NLP_12/","excerpt":"","text":"ì´ë²ˆ ì‹¤ìŠµì˜ ì†Œê°œëŠ” í”„ë¡œì íŠ¸ì„±ìœ¼ë¡œ ì§„í–‰ í•  ê²ƒì´ë‹¤. í”„ë¡œì íŠ¸ ì†Œê°œë”ì¡´ ICT ì˜¨ë¼ì¸ ê³ ê°ì„¼í„° í‚¤ì›Œë“œ ê²€ìƒ‰ ìµœì í™” ë° ì±—ë´‡ êµ¬í˜„ í”„ë¡œì íŠ¸ë¥¼ í•˜ê²Œ ëœ ê³„ê¸° ë¨¼ì €, ë”ì¡´ ì˜¨ë¼ì¸ ê³ ê°ì„¼í„° í˜ì´ì§€ ì¤‘ smart Aì— ê´€í•œ í˜ì´ì§€ì—ì„œ ì „ì²´ íƒ­ì„ í´ë¦­í•œ í›„, ì‚´í´ë³¸ QnA í˜ì´ì§€ë¥¼ ì‚´í´ë³´ì•˜ë‹¤. í•„ìëŠ” ê³ ê°ë“¤ì˜ ì…ì¥ì—ì„œ ìƒê°í•´ë³´ì•˜ì„ë•Œ, ìì‹ ì´ ì‘ì„±í•˜ëŠ” ì§ˆë¬¸(ë¬¼ë¡ , ê·¸ë¦¼ìœ¼ë¡œ ì²¨ë¶€í•´ì•¼í•  ë§Œí¼ ê·¸ í™˜ê²½ì´ ì¤‘ìš”í•œ ì§ˆë¬¸ë“¤ì€ ì œì™¸í•˜ê³ )ê³¼ ë¹„ìŠ·í•œ ì§ˆë¬¸ë“¤ì´ ì¡´ì¬í•  ê±°ë¼ëŠ” ìƒê°ì„ ê°–ê³  í‚¤ì›Œë“œë¥¼ í†µí•´ ê²€ìƒ‰í•´ë³¼ ê²ƒì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ ì¬ì…ì‚¬ìë¼ëŠ” í‚¤ì›Œë“œë¥¼ smart Aí˜ì´ì§€ì—ì„œ ê²€ìƒ‰í–ˆì„ ë•Œ ì¶œë ¥ë˜ëŠ” ê²°ê³¼ì´ë‹¤. 11ê±´ì˜ ì´ ê²€ìƒ‰ ê²°ê³¼ ì¤‘ ì¬ì…ì‚¬ìì— ëŒ€í•œ ì—°ë§ì •ì‚°ê³¼ ê´€ë ¨ëœ ë¬¸ê±´ì´ 8ê±´ì´ ì¡´ì¬í•œë‹¤. ê·¸ë˜ì„œ í•„ìëŠ” ì¬ì…ì‚¬ì ì—°ë§ì •ì‚°ì´ë¼ëŠ” í‚¤ì›Œë“œë¥¼ í†µí•´ ê²€ìƒ‰ì„ í•´ë³´ì•˜ë‹¤. ìœ„ì—ì„œ ì¬ì…ì‚¬ìë¼ëŠ” í‚¤ì›Œë“œë¥¼ í†µí•´ ê²€ìƒ‰ í–ˆì„ ë•Œ, ì¬ì…ì‚¬ìì˜ ì—°ë§ì •ì‚°ì— ëŒ€í•œ ì§ˆë¬¸ì´ 8ê±´ì´ ì¡´ì¬í•œ ë°˜ë©´ì— ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ 8ê±´ ì¤‘ 5ê±´ ë§Œì„ ë³´ì—¬ì¤€ë‹¤. í•„ìëŠ” ì§ˆë¬¸ì˜ ë‚´ìš©ì´ ì•„ë‹Œ ì§ˆë¬¸ì˜ ì œëª©ì— ì¬ì…ì‚¬ì ì—°ë§ì •ì‚°ì´ë¼ëŠ” í‚¤ì›Œë“œê°€ 8ê±´ì´ ì¡´ì¬í•  ë¿ ë‚´ìš©ì€ ê·¸ì™€ëŠ” ë‹¤ë¥¼ ìˆ˜ë„ ìˆë‹¤ëŠ” ìƒê°ì´ ë“¤ì–´, ê²€ìƒ‰ê²°ê³¼ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠëŠ” ì§ˆë¬¸ë“¤ì„ ì‚´í´ë³´ì•˜ë‹¤. ë˜í•œ, ì§ˆë¬¸ ë‚´ìš© ìì²´ê°€ ë³¸ì§ˆì ìœ¼ë¡œ ë¬¼ì–´ë³´ëŠ” ì˜ë¯¸ê°€ ê²€ìƒ‰ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì€ ì§ˆë¬¸ë“¤ì€ ë‹¤ë¥¼ ìˆ˜ë„ ìˆê¸°ì— íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤„ ìˆ˜ ìˆë‹¤ëŠ” ìƒê°ì´ ë“¤ì–´ ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ëœ ì§ˆë¬¸ê³¼ë„ ë¹„êµí•´ ë³´ê¸°ë¡œ í–ˆë‹¤. ê²€ìƒ‰ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì€ ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ì™¼ìª½ ê·¸ë¦¼ì´ê³ , ê²€ìƒ‰ê²°ê³¼ì— í¬í•¨ëœ ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ì˜¤ë¥¸ìª½ì˜ ë¹¨ê°•ìƒ‰ ë„¤ëª¨ë¡œ ë˜ì–´ìˆëŠ” ê·¸ë¦¼ì´ë‹¤. ë‘ ì§ˆë¬¸ì€ ë¹„ìŠ·í•œ ì§ˆë¬¸ì´ë¼ê³  ë³´ì¸ë‹¤. ê·¸ëŸ°ë°ë„ ë¶ˆêµ¬í•˜ê³  ì¬ì…ì‚¬ì ì—°ë§ì •ì‚°ì´ë¼ëŠ” í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠëŠ” ì ì„ í†µí•´ í•„ìëŠ” ê°ê°ì˜ ì§ˆë¬¸ë“¤ê³¼ ê²€ìƒ‰ í‚¤ì›Œë“œ ê°„ì˜ ìœ ì‚¬ì„±ì„ ì ìˆ˜í™”í•´ ìœ ì‚¬ì„±ì´ ë†’ì€ ì§ˆë¬¸ë“¤ì„ ë³´ì—¬ì£¼ëŠ” ì‹œìŠ¤í…œë„ì…ì´ í•„ìš”í•  ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤. ë˜í•œ, ì±—ë´‡ì„ ë§Œë“œëŠ” ë¶€ë¶„ì— ìˆì–´ì„œ ì…ë ¥ê³¼ ì¶œë ¥ì˜ ë¬¸ì¥ì˜ sequence ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ì–´ì•¼ í•˜ëŠ”ë°, ê·¸ì— ë”°ë¼ì„œ ë‹µë³€ì´ íŠ¹ì • ë¶„ì•¼(ì˜ˆë¥¼ë“¤ë©´, ì—°ë§ì •ì‚°ì´ë‚˜ ì›ì²œì§•ìˆ˜ë“±)ì—ì„œëŠ” ê¸´ ë¬¸ì¥ìœ¼ë¡œ ì´ë£¨ì–´ì§ˆ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ, ì±—ë´‡ì„ êµ¬í˜„í•œë‹¤ë©´, ê° ë¶„ì•¼ì— ë”°ë¥¸ ë¬¸ì¥ê¸¸ì´ë¥¼ ë¶„ì„í•´ ë³´ê¸°ë„ í•´ì•¼ í•  ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ë“¤ì—ˆë‹¤. ì´ëŸ° ì œí•œ ìƒí™©ìœ¼ë¡œ ì¸í•´ ì±—ë´‡ êµ¬í˜„ì´ í˜ë“¤ë‹¤ë©´, ì§ˆë¬¸ê³¼ ê²€ìƒ‰ í‚¤ì›Œë“œ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë°˜ì˜í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•´ì„œë¼ë„ ë”ì¡´ ì˜¨ë¼ì¸ ê³ ê°ì„¼í„°ì˜ ì§ˆë¬¸ì„ í•˜ì‹œëŠ” ê³ ê° ë¶„ë“¤ì—ê²Œ ì¡°ê¸ˆì´ë‚˜ë§ˆ ë” í¸ì˜ì„±ì„ ë“œë¦´ìˆ˜ ìˆê²Œë” í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤. ë”ì¡´ ì‚¬ì´íŠ¸ë‚´ì—ì„œ ì˜ì—… ë¬¸ì˜ ì „í™”ë‚˜ êµ¬ë§¤ìì— ëŒ€í•œ ìƒë‹´ì€ ë”°ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³  ìˆì§€ë§Œ, ì˜¨ë¼ì¸ ê³ ê°ì„¼í„° tapë¶€ë¶„ì—ì„œë§Œ Q&amp;Aì— ê´€í•œ ì‚¬í•­ì„ ë‹¤ë£¨ëŠ”ë° ë‹µë³€ì„ í•´ì£¼ëŠ” ì‹œê°„ì€ ì—…ë¬´ ì‹œê°„ë‚´ë¡œë§Œ ì œí•œ ë˜ì–´ìˆë‹¤. ì´ì— ë”°ë¼ 24ì‹œê°„ ë˜ëŠ” ì—…ë¬´ ì´ì™¸ì˜ ì‹œê°„ì—ëŠ” ì±—ë´‡ ì„œë¹„ìŠ¤ë¥¼ ì‹œí–‰í•œë‹¤ë©´ ê³ ê°ë“¤ì˜ ì…ì¥ì—ì„œ ë³´ì•˜ì„ ë•Œ ì¡°ê¸ˆ ë” í¸ë¦¬í•˜ê²Œ ë”ì¡´ì˜ ì„œë¹„ìŠ¤ë‚˜ ì†”ë£¨ì…˜ì„ ì´ìš©í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ëŠ” ì·¨ì§€ì— ì˜í•´ì„œ ì±—ë´‡ êµ¬í˜„ì— ê´€ì‹¬ì„ ê°–ê²Œ ë˜ì—ˆë‹¤. ë°ì´í„° ì´ë¦„ : qna_smart_a.csv ë”ì¡´ì—ì„œëŠ” WEHAGO í”Œë«í¼ìƒì—ì„œ ì—¬ëŸ¬ê°€ì§€ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³  ìˆë‹¤. ê·¸ ì¤‘ ë”ì¡´ Smart AëŠ” ì¬ë¬´íšŒê³„, ì„¸ë¬´ì‹ ê³ , ì¸ì‚¬Â·ê¸‰ì—¬ê´€ë¦¬, ë¬¼ë¥˜ê´€ë¦¬ê¹Œì§€ ì¤‘ì†Œê¸°ì—…ì˜ ì—…ë¬´ë¥¼ í†µí•©ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” íšŒê³„í”„ë¡œê·¸ë¨ë¡œì„œ, ì´ í”„ë¡œê·¸ë¨ì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ì— ì˜í•´ì„œë§Œ ë¨¼ì € í•™ìŠµì„ í•´ ë³¼ ê²ƒì´ë‹¤. ê·¸ ì´ìœ ëŠ” ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ë“¤(ERPì™€ WEHAGO)ì€ ì‚¬ìš©ìë“¤ì˜ ì„±ê²©ì— ë”°ë¼ ë‹¤ì–‘í•œ ìš©ë„ë¡œ ê°œë°œ ë˜ì–´ìˆì§€ë§Œ, íšŒê³„í”„ë¡œê·¸ë¨ì¸ Smart AëŠ” ëª¨ë“  ê¸°ì—…ì´ ê³µìš©ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ìš°ì„ ì ìœ¼ë¡œ í•™ìŠµí•´ ë³¼ ê²ƒì´ë‹¤. ë˜í•œ, ê°€ì¥ ì£¼ìš”í•œ ì„ íƒ ì´ìœ ëŠ” Q&amp;A ê²Œì‹œíŒì˜ ë°ì´í„° ì¤‘ ê°€ì¥ ë§ì€ ë°ì´í„°ë¥¼ í¬í•¨í•˜ê³  ìˆì—ˆê¸° ë•Œë¬¸ì´ë‹¤. ë°ì´í„° ìš©ë„ : ë°ì´í„° ì¶œì²˜ : ë”ì¡´ ì˜¨ë¼ì¸ ê³ ê°ì„¼í„° Smart A ì „ì²´ tapì˜ ì „ì²´ ì§ˆë¬¸ê³¼ ë‹µë³€ë“¤ì„ í¬ë¡¤ë§ í•´ì„œ ì‚¬ìš©í•˜ì˜€ë‹¤. í¬ë¡¤ë§ ë°©ì‹ì€ Scrapyë¥¼ í†µí•´ í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ê²Œë” ì½”ë“œë¥¼ ì‘ì„±í•˜ì—¬ í¬ë¡¤ë§í•´ì„œ ì–»ì—ˆë‹¤. ë¨¼ì €, ë”ì¡´ ì˜¨ë¼ì¸ ê³ ê° ì„¼í„°í˜ì´ì§€ì—ì„œ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í¬ë¡¤ë§í•´ì™€ì„œ ë°ì´í„° ì…‹ì„ êµ¬ì„±í•  ê²ƒì´ë‹¤. Spider bot ë§Œë“¤ê¸° ì „ì²´ scrapy botì˜ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. items.pyì™€ settings.pyë¥¼ í™œìš©í–ˆìœ¼ë©°, ë§ˆì§€ë§‰ ê²°ê³¼ íŒŒì¼ì€ csvë¡œ ì €ì¥í–ˆë‹¤. í˜¹ì‹œ dbíŒŒì¼ë¡œ ì €ì¥í•˜ê³  ì‹¶ë‹¤ë©´ ì¶”ê°€ì ìœ¼ë¡œ pipelinesì—ì„œ ì‘ì—…ì„ í•˜ë©´ëœë‹¤. ë”ì¡´ ì˜¨ë¼ì¸ ê³ ê°ì„¼í„°ì˜ ê²Œì‹œíŒì—ì„œ ìµœê·¼ ê²Œì‹œíŒì—ì„œëŠ” ë‹µë³€ ì™„ë£Œìƒíƒœì¸ ë°ì´í„°ê°€ ì£¼ë¡œ ë§ì§€ë§Œ ì˜ˆì „ ë°ì´í„° ì¤‘ì—ëŠ” ê°„ê°„íˆ ë‹µë³€ ëŒ€ê¸° ìƒíƒœì¸ ë°ì´í„°ê°€ ì¡´ì¬í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ pipeline.pyì—ì„œ ì´ë¥¼ í†µí•´ ë‹µë³€ ì™„ë£Œì¸ ìƒíƒœì¸ ë°ì´í„°ë§Œì„ í¬ë¡¤ë§í•˜ì—¬ë„ ë˜ì§€ë§Œ, í•„ìëŠ” ì–´ë–¤ ë°ì´í„°ê°€ ë‹µë³€ ëŒ€ê¸° ìƒíƒœì¸ ë°ì´í„°ì¸ì§€ ëˆˆìœ¼ë¡œ ì‚´í´ë³´ê¸° ìœ„í•´ ê·¸ëƒ¥ ëª¨ë‘ í¬ë¡¤ë§í•˜ëŠ” ê²ƒìœ¼ë¡œ ì²˜ë¦¬í•˜ì˜€ë‹¤. 123456789101112131415161718192021thezoneâ”œâ”€â”€ scrapy.cfgâ””â”€â”€ thezone â”œâ”€â”€ __init__.py â”œâ”€â”€ __pycache__ â”‚ â”œâ”€â”€ __init__.cpython-37.pyc â”‚ â”œâ”€â”€ items.cpython-37.pyc â”‚ â”œâ”€â”€ pipelines.cpython-37.pyc â”‚ â””â”€â”€ settings.cpython-37.pyc â”œâ”€â”€ items.py â”œâ”€â”€ middlewares.py â”œâ”€â”€ pipelines.py â”œâ”€â”€ settings.py â””â”€â”€ spiders â”œâ”€â”€ __init__.py â”œâ”€â”€ __pycache__ â”‚ â”œâ”€â”€ __init__.cpython-37.pyc â”‚ â””â”€â”€ qnacrawler.cpython-37.pyc â”œâ”€â”€ last_qna_smart_a.csv â”œâ”€â”€ qna_smart_a.csv â””â”€â”€ qnacrawler.py qnacrawler.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Ruleimport syssys.path.insert(0, '/Users/heungbaelee/workspace/project/chat_bot_project/thezone/thezone')from items import ThezoneItemclass QnacrawlerSpider(CrawlSpider): name = 'qnacrawler' allowed_domains = ['help.douzone.com'] start_urls = ['http://help.douzone.com/pboard/index.jsp?code=qna10&amp;pid=10&amp;s_category_id=all&amp;type=all&amp;s_listnum=50&amp;s_field=&amp;s_keyword='] # rules = [ # Rule(LinkExtractor(allow=r'/pboard/index.jsp?code=qna10&amp;pid=10&amp;s_category_id=all&amp;type=all&amp;s_listnum=50&amp;s_field=&amp;s_keyword=&amp;page=\\d+', ), callback='parse_parent', follow=True), # ] rules = [ Rule(LinkExtractor(restrict_css='div.page_box &gt; ul &gt; li:nth-child(n+4)',attrs='href'), callback='parse_parent', follow=True), ] def parse_parent(self, response): # link = LinkExtractor(allow=r'/pboard/index.jsp?code=qna10&amp;pid=10&amp;s_category_id=all&amp;type=all&amp;s_listnum=50&amp;s_field=&amp;s_keyword=&amp;page=\\d+') # links = link.extract_links(response) # print(links) # print(response.status) for url in response.css('div.tab_cnt.mt30 &gt; table &gt; tbody &gt; tr'): article_num = url.css('td:nth-child(1)::text').extract_first().strip() self.logger.info('Article number : %s' % article_num) article_link = url.css('td:nth-child(3) &gt; a::attr(href)').extract_first().strip() self.logger.info('Article link : %s' % article_link) # print(article_num, response.urljoin(article_link)) yield scrapy.Request(response.urljoin(article_link), self.parse_child, meta=&#123;'article_num': article_num&#125;) def parse_child(self, response): # ë¶€ëª¨, ìì‹ ìˆ˜ì‹  ì •ë³´ ë¡œê¹… self.logger.info('----------------------------------------') self.logger.info('Child Response URL : %s' % response.url) self.logger.info('Child Response Status ; %s' % response.status) self.logger.info('----------------------------------------') # ì§ˆë¬¸ ë²ˆí˜¸ article_num = response.meta['article_num'] # ìœ í˜• category = response.css(\"div.qna_read.mt30 &gt; table:nth-child(1) &gt; tbody &gt; tr:nth-child(2) &gt; td &gt; dl &gt; dd:nth-child(2)::text\").extract_first().strip() # ì§ˆë¬¸ question = \"\".join(response.css(\"div.qna_read.mt30 &gt; table:nth-child(1) &gt; tbody &gt; tr:nth-child(2) &gt; td &gt; div.q &gt; div.q_cnt &gt; p::text\").extract()).strip() # ë“±ë¡ì¼ enrolled_date_time = response.css(\"div.qna_read.mt30 &gt; table:nth-child(1) &gt; tbody &gt; tr:nth-child(1) &gt; td:nth-child(4)::text\").extract_first().strip() # ì‘ì„±ì¼ answer_date_time = response.css(\"table.mt10 &gt; tbody &gt; tr:nth-child(1) &gt; td:nth-child(4)::text\").extract_first().strip() # ë‹µë³€ì—¬ë¶€ answer_yes = response.css(\"table.mt10 &gt; tbody &gt; tr:nth-child(1) &gt; td.ta_l &gt; span::text\").extract_first().strip() # ë‹µë³€ answering = \"\".join(response.css(\"table.mt10 &gt; tbody &gt; tr:nth-child(2) &gt; td.ta_l.pd20 &gt; div.a &gt; div &gt; p::text\").extract()).strip() yield ThezoneItem(article_num=article_num, category=category, enrolled_date_time=enrolled_date_time, question=question, answer_date_time=answer_date_time, answer_yes=answer_yes, answering=answering) items.py 123456789101112131415161718192021222324252627282930313233# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# https://docs.scrapy.org/en/latest/topics/items.htmlimport scrapyclass ThezoneItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # ë¬¸ì„œë²ˆí˜¸ article_num = scrapy.Field() # ìœ í˜• category = scrapy.Field() # ì§ˆë¬¸ question = scrapy.Field() # ë‹µë³€ answering = scrapy.Field() # ì‘ì„±ì¼ answer_date_time = scrapy.Field() # ë“±ë¡ì¼ enrolled_date_time = scrapy.Field() # ë‹µë³€ì—¬ë¶€ answer_yes = scrapy.Field() settings.py 1234567891011121314151617181920212223242526272829303132333435# -*- coding: utf-8 -*-BOT_NAME = 'thezone'SPIDER_MODULES = ['thezone.spiders']NEWSPIDER_MODULE = 'thezone.spiders'DEFAULT_REQUEST_HEADERS = &#123;'Referer' : 'http://help.douzone.com'&#125;# Obey robots.txt rulesROBOTSTXT_OBEY = False# ì¿ í‚¤ì‚¬ìš©COOKIES_ENABLED = TrueDOWNLOAD_DELAY = 3# User-Agent ë¯¸ë“¤ì›¨ì–´ ì‚¬ìš©DOWNLOADER_MIDDLEWARES = &#123; 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,&#125;# íŒŒì´í”„ ë¼ì¸ í™œì„±í™”# ìˆ«ìê°€ ì‘ì„ ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ìƒìœ„ITEM_PIPELINES = &#123; 'thezone.pipelines.ThezonePipeline': 300,&#125;# ì¬ì‹œë„ íšŸìˆ˜RETRY_ENABLED = TrueRETRY_TIMES = 2# í•œê¸€ ì“°ê¸°(ì¶œë ¥ ì¸ì½”ë”©)FEED_EXPORT_ENCODING = 'utf-8' ìœ„ì˜ scrapy íŒŒì¼ë“¤ì„ í†µí•´ì„œ ë°ì´í„°ë¥¼ ë¨¼ì € í™•ë³´ í–ˆë‹¤. í•„ìì˜ ë¡œì»¬í™˜ê²½ì„ í†µí•´ì„œëŠ” 10ì‹œê°„ ì •ë„ ê±¸ë ¸ë‹¤. 1scrapy runspider qnacrawler.py -o qna_smart_a.csv - t csv ë°ì´í„° ì†Œê°œ ìœ„ì˜ scrapy spider botì„ í†µí•´ì„œ ì–»ì€ ë°ì´í„°ë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ featureë“¤ì„ ì–»ì—ˆë‹¤. íšŒê³„í”„ë¡œê·¸ë¨ì¸ smart_aì— ëŒ€í•œ ì „ì²´ Q&amp;Aë¥¼ í¬ë¡¤ë§í•˜ì—¬ ì±—ë´‡ì„ ë§Œë“œëŠ” ê²ƒì´ í”„ë¡œì íŠ¸ì˜ ëª©í‘œì´ë‹¤. raw ë°ì´í„° êµ¬ì„± answer_date_time : ë‹µë³€ì™„ë£Œì¼ì answer_yes : ë‹µë³€ ì—¬ë¶€ answering : ë‹µë³€ ë‚´ìš© category : ì§ˆë¬¸ì˜ ìœ í˜• enrolled_date_time : ì§ˆë¬¸ë“±ë¡ì¼ì question : ì§ˆë¬¸ ë‚´ìš© 1raw_data.info() 1234567891011&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 12475 entries, 0 to 12474Data columns (total 6 columns):answer_date_time 12436 non-null objectanswer_yes 12436 non-null objectanswering 12420 non-null objectcategory 12475 non-null objectenrolled_date_time 12475 non-null objectquestion 12409 non-null objectdtypes: object(6)memory usage: 584.9+ KB ë°ì´í„°ì— null ê°’ì´ í¬í•¨ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— nullê°’ë“¤ì„ ì œê±°í•´ì£¼ê³ , ë‹µë³€ ëŒ€ê¸° ìƒíƒœì¸ ë°ì´í„°ë“¤ì€ ì´ 39ê±´ì´ ìˆì—ˆëŠ”ë° ë‹µë³€ì´ ì‘ì„±ë˜ì§€ ì•Šì€ ë°ì´í„° ì´ë¯€ë¡œ ë‹µë³€ ëŒ€ê¸° ìƒíƒœì¸ ë°ì´í„°ë“¤ë„ ê°™ì´ ì œê±°í•´ì¤€ë‹¤. 12345678raw_data = raw_data[raw_data[\"answer_yes\"]==\"ë‹µë³€ì™„ë£Œ\"]raw_data.reset_index(drop=True, inplace=True)raw_data = raw_data[pd.isnull(raw_data[\"question\"])!=True].reset_index(drop=True)print(sum(raw_data[\"question\"].apply(lambda x: pd.isnull(x))))raw_data = raw_data[pd.isnull(raw_data[\"answering\"])!=True].reset_index(drop=True)print(sum(raw_data[\"question\"].apply(lambda x: pd.isnull(x)))) ë‹µë³€ëŒ€ê¸° ìƒíƒœì¸ ë°ì´í„°ë“¤ì„ ì œê±°í•˜ê³  ì´ ì‚¬ìš©ê°€ëŠ¥í•œ ë°ì´í„°ëŠ” 12,354ê±´ì˜ ì§ˆë¬¸ê³¼ ë‹µë³€ ìŒì´ë‹¤. 1234567891011&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 12354 entries, 0 to 12353Data columns (total 6 columns):answer_date_time 12354 non-null objectanswer_yes 12354 non-null objectanswering 12354 non-null objectcategory 12354 non-null objectenrolled_date_time 12354 non-null objectquestion 12354 non-null objectdtypes: object(6)memory usage: 579.2+ KB ë¨¼ì €, ê°„ë‹¨í•˜ê²Œ ë°ì´í„°ë“¤ì˜ ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ì— ë”°ë¼ì„œ ì–´ë–¤ ë¶„í¬ë¥¼ ë„ê³  ìˆëŠ”ì§€ ê°„ëµí•˜ê²Œ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ì§ˆë¬¸ ë°ì´í„° ì „ì²˜ë¦¬ ì„¸ë¬´/íšŒê³„ê´€ë ¨ ì§ˆë¬¸ë“¤ì´ë¼ì„œ ê¸ˆì•¡ì— ê´€í•œ ì§ˆë¬¸ê³¼ ë‹µë³€ë“¤ì´ ë§ì´ ìˆê¸°ì— ìˆ«ìì— ëŒ€í•œ ë‚´ìš©ì„ ì œê±°í• ì§€ í•˜ì§€ ë§í•˜ì•¼ í• ì§€ë¥¼ ë‘ê³  í•„ìëŠ” ìƒê°ì´ ë§ì•˜ëŠ”ë°, ìš°ì„  í”„ë¡œì íŠ¸ì˜ ì²«ë²ˆì§¸ ëª©í‘œì¸ ê²€ìƒ‰ í‚¤ì›Œë“œì™€ ì§ˆë¬¸ì˜ ë‚´ìš©ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ë©´ì— ìˆì–´ì„œëŠ” ìˆ«ìë“¤ì´ í¬ê²Œ ì¤‘ìš”í•˜ì§€ ì•Šì„ ê²ƒì´ë¼ëŠ” íŒë‹¨í•˜ì— ìˆ«ìë¶€ë¶„ë“¤ê³¼ ë§ˆì¹¨í‘œê°™ì€ ë¶€í˜¸ë“¤ì„ ì œê±°í•˜ê¸°ë¡œ ê²°ì •í–ˆë‹¤. ë‹¤ë§Œ, []ì•ˆì˜ ë‚´ìš©ì€ ëŒ€ë¶€ë¶„ smart Aì˜ ë©”ë‰´ëª…ì„ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì— ì‚´ë ¤ë‘ì—ˆë‹¤. [ë©”ë‰´ëª…]ì„ í•˜ë‚˜ì˜ ëª…ì‚¬ë¡œ ì¸ì‹í•˜ê¸° ìœ„í•´ í˜•íƒœì†Œ ë¶„ì„ì„ í•  ê²½ìš°ì—ë„ ë¹„ì§€ë„ í•™ìŠµì„ í†µí•œ ë°©ì‹ì„ ì±„íƒí•˜ê¸° ìœ„í•´ soynlpë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. 123456789def pattern_match(x): pattern = \"\\d+\" reg = re.compile(pattern) sentence = re.sub(reg, \" \", x) pattern = \"[!|,|.|?|~|â€»|)|(|â– |+|=|-|/|*|-|&gt;|-|;|^|]|-|%|'|'|ã… +|ã…+]\" reg = re.compile(pattern) sentence = re.sub(reg, \" \", sentence) 1raw_data['question_after'] = raw_data['question_after'].apply(lambda x : pattern_match(str(x))) ê¸°ë³¸ì ì¸ ë¶€í˜¸ë“¤ê³¼ ìˆ«ìë“¤ì„ ì œê±°í•´ ì£¼ì—ˆìœ¼ë¯€ë¡œ ì´ì œ ê¸°ë³¸ì ì¸ ë„ì–´ì“°ê¸° ë‹¨ìœ„ ì–´ì ˆê³¼ ìŒì ˆ(ë¬¸ì í•˜ë‚˜í•˜ë‚˜ë¥¼ ì˜ë¯¸)ë‹¨ìœ„ë¡œ ì§ˆë¬¸ì˜ í‰ê· ì ì¸ ê¸¸ì´ì™€ í•œ ì§ˆë¬¸ë‹¹ ë‹¨ì–´ì˜ í‰ê· ì ì¸ ì‚¬ìš©ëŸ‰ì„ ëŒ€ëµì ìœ¼ë¡œ ì‚´í´ë³¼ ê²ƒì´ë‹¤. 12# ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ì–´ì ˆ ê¸°ì´ˆí†µê³„ëŸ‰raw_data['question_after'].apply(lambda x: len(str(x).split(\" \"))).describe() ìœ„ì˜ ë„ì–´ì“°ê¸° ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ì§ˆë¬¸ì˜ Tokenì˜ ê°œìˆ˜ëŠ” í‰ê· ì ìœ¼ë¡œ 33ê°œì˜ ì–´ì ˆê³¼ ì¤‘ì•™ê°’ì€ 27ê°œì˜ ì–´ì ˆì„ ê°–ëŠ”ë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. í‰ê· ì´ ì˜¬ë¼ê°„ê²ƒì€ 3ì‚¬ë¶„ìœ„ìˆ˜ê°€ 42ê°œì¸ ê²ƒê³¼ ìµœëŒ€ ì–´ì ˆì´ 791ê°œì¸ ê²ƒìœ¼ë¡œ ë¯¸ë£¨ì–´ ë³´ì•„ ì´ìƒì¹˜ì— ì˜í•œ ì˜í–¥ì„ ë°›ì•„ í‰ê· ì´ ë°ì´í„°ì˜ ì¤‘ì‹¬ì„ ì˜ ë°˜ì˜í•˜ê³  ìˆì§€ ì•Šë‹¤ê³  íŒë‹¨í•´ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì´ìƒì¹˜ë“¤ì˜ ë°ì´í„° í˜•íƒœë¥¼ ì‚´í´ë³´ê³  ë¬¸ì œì ì´ ë¬´ì—‡ì¸ì§€ íŒŒì•…í•´ ë³¼ ê²ƒì´ë‹¤. 123456789count 12290.000000mean 35.055411std 33.537140min 1.00000025% 17.00000050% 27.00000075% 42.000000max 791.000000Name: question_after, dtype: float64 ê°€ì¥ ë†’ì€ ìµœëŒ“ê°’ì„ ê°–ëŠ” ë°ì´í„°ë¥¼ ì‚´í´ë³´ë©´, ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ê³µë°±ìœ¼ë¡œ ì¼ì •í•œ í˜•ì‹ì„ ë§ì¶°ë³´ë ¤ê³  í•œ ê²ƒ ê°™ì´ ë˜ì–´ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ëŠ” ì´ ì§ˆë¬¸ì˜ ë‚´ìš©ì ì¸ ë©´ì´ë‚˜ í‚¤ì›Œë“œê°€ ì¤‘ìš”í•œ ê²ƒì´ë¯€ë¡œ í˜•ì‹ì´ ìš°ë¦¬ê°€ í‘¸ëŠ” ë¬¸ì œì—ëŠ” í° ì˜í–¥ì„ ì£¼ì§€ ëª»í•˜ë¯€ë¡œ ê³µë°±ì„ ì œê±°í•´ ì¤„ ê²ƒì´ë‹¤. 12345def pattern_match(x): pattern = \" +\" reg = re.compile(pattern) sentence = re.sub(reg, \" \", x) return sentence 123raw_data['question_after'] = raw_data['question_after'].apply(lambda x : pattern_match(str(x)))sent_len_by_token = raw_data['question_after'].apply(lambda x: len(str(x).split(\" \"))) ê³µë°±ì´ ë§ì€ ë°ì´í„°ë“¤ì„ ê³µë°±ì„ ì¤„ì—¬ì£¼ëŠ” í•¨ìˆ˜ë¥¼ í†µí•´ ì²˜ë¦¬ë¥¼ í•´ì¤€ í›„ì— ë‹¤ì‹œ ì§ˆë¬¸ ë‹¹ ë„ì–´ì“°ê¸° ë‹¨ìœ„ ì–´ì ˆì˜ ê¸¸ì´ì— ê´€í•œ ê¸°ì´ˆ í†µê³„ëŸ‰ì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ì—­ì‹œ í•¨ìˆ˜ë¥¼ í†µí•´ ê³µë°±ì„ ì¤„ì—¬ì¤€ í›„ì— ë‹¤ì‹œ ì¸¡ì •í•´ë³´ë‹ˆ í‰ê· ê³¼ ì¤‘ì•™ê°’ì˜ ì°¨ì´ê°€ ì´ì „ê³¼ ë‹¤ë¥´ê²Œ í™•ì—°íˆ ì¤„ì–´ë“  ê²ƒì„ ë³¼ ìˆ˜ ìˆìœ¼ë©°, í‰ê· ì ìœ¼ë¡œ 22~23ê°œì˜ ì–´ì ˆì„ ì‚¬ìš©í•¨ì„ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆë‹¤. 123456789count 12290.000000mean 28.416273std 22.040055min 1.00000025% 15.00000050% 23.00000075% 35.000000max 355.000000Name: question_after, dtype: float64 90%ì˜ ìœ„ì¹˜ì— ìœ„ì¹˜í•˜ê³  ìˆëŠ” ì–´ì ˆì˜ ê¸¸ì´ëŠ” 52ê°œ ì˜€ë‹¤. 1np.quantile(sent_len_by_token, 0.90) 152.0 ë˜í•œ, ìœ„ì—ì„œ 355ê°œì˜ ì–´ì ˆì„ ê°–ëŠ” ë°ì´í„°ì— ê´€í•´ì„œë„ ì´ìƒì¹˜ì´ë¯€ë¡œ ì‚´í´ë³´ì•˜ë‹¤. ì•„ë˜ì™€ ê°™ì´ ì˜¤ë¥˜ ì½”ë“œì— ê´€í•œ ì§ˆë¬¸ì´ì—ˆê¸° ë•Œë¬¸ì— ê³µë°±ì´ ë§ì´ í¬í•¨ë˜ì–´ìˆì„ ìˆ˜ ë°–ì— ì—†ë‹¤ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆì—ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì´ ë°ì´í„°ì˜ ê³µë°±ì€ ì§ˆë¬¸ì˜ ë‚´ìš©ì„ í‘œí˜„í•˜ëŠ”ë° ë¶ˆí•„ìš”í•œ ìš”ì†Œê°€ ì•„ë‹ˆë¯€ë¡œ ê·¸ëŒ€ë¡œ ìƒíƒœë¥¼ ìœ ì§€ í•  ê²ƒì´ë‹¤. ê·¸ ë‹¤ìŒì€ ìŒì ˆ ë‹¨ìœ„ ê¸¸ì´ë¥¼ ë¶„ì„í•´ ë³¼ ê²ƒì´ë‹¤. ìŒì ˆ ë‹¨ìœ„ì˜ ê¸°ì´ˆ í†µê³„ëŸ‰ì€ ì•„ë˜ì™€ ê°™ë‹¤. í‰ê· ì ìœ¼ë¡œ 136ìë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, ì¤‘ì•™ê°’ì€ 112ìì´ë‹¤. 123456789count 12279.000000mean 136.459647std 109.823703min 3.00000025% 74.00000050% 112.00000075% 167.000000max 2637.000000Name: question_after, dtype: float64 ìœ„ì—ì„œì˜ ê¸°ì´ˆ í†µê³„ëŸ‰ ê°’ì„ ì‹œê°í™”í•´ì„œ ê°„ë‹¨íˆ ì‚´í´ ë³´ê¸°ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì´ íˆìŠ¤í† ê·¸ë¨ì„ í™œìš©í•˜ì˜€ë‹¤. ìƒì‹ì ìœ¼ë¡œë„ ì•Œ ìˆ˜ ìˆë“¯ì´, ìŒì ˆì´ ì–´ì ˆë³´ë‹¤ í›¨ì”¬ ë‹¨ìœ„ê°€ í´ìˆ˜ë°–ì— ì—†ì„ ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ë³¼ ê²ƒì€ ê¼¬ë¦¬ ë¶„í¬ì´ë‹¤. ìŒì ˆê³¼ ì–´ì ˆ ë‹¨ìœ„ë¡œ ì‚´í´ë³¸ ì§ˆë¬¸ì˜ ê¸¸ì´ëŠ” ë‘˜ë‹¤ ì¼ì • ìˆ˜ì¤€ì´í•˜ì— ì£¼ë¡œ ë¶„í¬ë¼ìˆê³ , ì¼ì • ìˆ˜ì¤€ ì´ìƒì€ ì´ìƒì¹˜ê°€ ì¡´ì¬í•˜ê³  ìˆë‹¤. ëª¨ë¸ ì„¤ì • ì œì¼ ë¨¼ì €, TF-IDF í–‰ë ¬ì„ ì‚¬ìš©í•´ LSA ë¶„ì„ì˜ ì¼ì¢…ì¸ TruncatedSVD í–‰ë ¬ì„ ì´ìš©í•´ ë¬¸ì¥ ì„ë² ë”©ì„ ì‹¤í–‰í•œ í›„, í‚¤ì›Œë“œ ê²€ìƒ‰ì–´ì™€ì˜ ìœ ì‚¬í•œ ë¬¸ì„œë“¤ì„ ì‚´í´ ë³¼ ê²ƒì´ë‹¤. 12345678910111213141516171819202122232425262728293031323334353637383940import mathfrom sklearn.feature_extraction.text import TfidfVectorizerfrom soynlp.word import WordExtractorfrom soynlp.tokenizer import LTokenizerfrom sklearn.decomposition import TruncatedSVDfrom sklearn.preprocessing import normalizefrom sklearn.metrics.pairwise import cosine_similarityq_sentence = list(raw_data['question_after'])word_extractor = WordExtractor(min_frequency=1, min_cohesion_forward=0.05, min_right_branching_entropy=0.0)word_extractor.train(q_sentence)scores = word_extractor.word_scores()cohesion_scores = &#123;key:(scores[key].cohesion_forward * math.exp(scores[key].right_branching_entropy)) for key in scores.keys()&#125;tokenizer = LTokenizer(scores=cohesion_scores)tokens = []for q_s in q_sentence: tokens.append(tokenizer.tokenize(q_s))sentence_by_tokens = [' '.join(word) for word in tokens]## TfidfVectorizervectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,1), lowercase=True, tokenizer=lambda x : x.split(\" \"))input_matrix = vectorizer.fit_transform(sentence_by_tokens)vocab2id = &#123;token : vectorizer.vocabulary_[token] for token in vectorizer.vocabulary_.keys()&#125;id2vocab = &#123;vectorizer.vocabulary_[token]: token for token in vectorizer.vocabulary_.keys()&#125;## TruncatedSVDsvd = TruncatedSVD(n_components=100)vecs = svd.fit_transform(input_matrix)criterion_sentence = \"ì¬ì…ì‚¬ì ì—°ë§ì •ì‚°\"criterion_tokens = tokenizer.tokenize(criterion_sentence)criterion_tokens ì¬ì…ì‚¬ì ì—°ë§ì •ì‚°ì´ë¼ëŠ” í‚¤ì›Œë“œë¥¼ tokenizingí•œ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ë‹¤. ì¬ì…ì‚¬, ì, ì—°ë§ì •ì‚° ì´ë ‡ê²Œ 3ê°€ì§€ í˜•íƒœë¡œ í˜•íƒœì†Œë¥¼ ë¶„ë¦¬í–ˆë‹¤. 1['ì¬ì…ì‚¬', 'ì', 'ì—°ë§ì •ì‚°'] 1234567891011121314151617181920212223242526criterion_sentence_by_token = [\" \".join(criterion_tokens)]criterion_vec = vectorizer.transform(criterion_sentence_by_token)criterion_vec = svd.transform(criterion_vec)svd_l2norm_vectors = normalize(vecs, axis=1, norm='l2')svd_l2norm_criterion_vectors = normalize(criterion_vec, axis=1, norm='l2').reshape(100,1)cosine_similarity = np.dot(svd_l2norm_vectors, svd_l2norm_criterion_vectors)ls=[]for idx, cosine_similarity in enumerate(cosine_similarity.tolist()): ls.append((idx, cosine_similarity))sorted_list = sorted(ls, key= lambda x: x[1], reverse=True)criterion_tokens_list = []for i in np.arange(len(sorted_list)): criterion_tokens_list.append(criterion_tokens)show_list = []for sorted_lists, criterion_tokens in zip(sorted_list, criterion_tokens_list): idx=sorted_lists[0] similarity=sorted_lists[1] tf_list=[] for token in criterion_tokens: tf_list.append(token in raw_data['question'].loc[idx]) if (np.array(tf_list) == True).all(): show_list.append((idx, similarity))show_list ìœ„ì˜ show_listê²°ê³¼ ì¤‘ ëª‡ê°€ì§€ ì§ˆë¬¸ë“¤ì„ ì‚´í´ë³´ìë©´, ì•„ë˜ì™€ ê°™ë‹¤. ìœ„ì—ì„œ í˜•íƒœì†Œê°€ ì¬ì…ì‚¬, ì, ì—°ë§ì •ì‚° ì´ë ‡ê²Œ 3ê°€ì§€ë¡œ ë¶„ë¦¬í–ˆë˜ ê²ƒì„ ìš°ë¦¬ê°€ ì•Œ ê³  ìˆë“¯ì´ ì¬ì…ì‚¬ì, ì—°ë§ì •ì‚° 2ê°€ì§€ë¡œ ì˜ ë¶„ë¦¬í•˜ë„ë¡ ëª…ì‚¬ ì¶”ì¶œê¸° ì ìˆ˜ë¥¼ ë”í•œ ì ìˆ˜ë¥¼ í†µí•´ì„œ ë‹¤ì‹œ tokenizeí•  ê²ƒì´ë‹¤. ëª…ì‚¬ ì¶”ì¶œê¸°ë¥¼ í†µí•œ ëª…ì‚¬ ì ìˆ˜ë¥¼ í•©ì‚°í•œ scoreë¥¼ í†µí•œ tokenizer í™œìš©1234567891011121314151617181920212223242526272829303132333435363738394041import mathfrom soynlp.word import WordExtractorfrom soynlp.noun import LRNounExtractor_v2from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.decomposition import TruncatedSVDfrom sklearn.preprocessing import normalizefrom sklearn.metrics.pairwise import cosine_similaritynoun_extractor = LRNounExtractor_v2(verbose=True)nouns = noun_extractor.train_extract(q_sentence)noun_scores = &#123;noun:score.score for noun, score in nouns.items()&#125;combined_scores = &#123;noun:score + cohesion_scores.get(noun, 0) for noun, score in noun_scores.items()&#125;combined_scores = combined_scores.update( &#123;subword:cohesion for subword, cohesion in cohesion_scores.items() if not (subword in combined_scores)&#125;)tokenizer = LTokenizer(scores=combined_scores)tokens = []for q_s in q_sentence: tokens.append(tokenizer.tokenize(q_s))sentence_by_tokens = [' '.join(word) for word in tokens]vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,1), lowercase=True, tokenizer=lambda x : x.split(\" \"))input_matrix = vectorizer.fit_transform(sentence_by_tokens)vocab2id = &#123;token : vectorizer.vocabulary_[token] for token in vectorizer.vocabulary_.keys()&#125;id2vocab = &#123;vectorizer.vocabulary_[token]: token for token in vectorizer.vocabulary_.keys()&#125;svd = TruncatedSVD(n_components=100)vecs = svd.fit_transform(input_matrix)criterion_sentence = \"ì¬ì…ì‚¬ì ì—°ë§ì •ì‚°\"criterion_tokens = tokenizer.tokenize(criterion_sentence)criterion_tokens ì¬ì…ì‚¬ì ì—°ë§ì •ì‚°ì´ë¼ëŠ” ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ tokenizingí•œ ê²°ê³¼ ì•„ë˜ì™€ ê°™ì´ ì¬ì…ì‚¬ì, ì—°ë§ì •ì‚°ì´ë¼ê³  ë¶„ë¥˜í•´ëƒˆë‹¤. í—ˆë‚˜, ìœ„ì—ì„œì™€ ê°™ì´ 1['ì¬ì…ì‚¬ì', 'ì—°ë§ì •ì‚°'] sorted_listì— í¬í•¨ëœ ì§ˆë¬¸ë“¤ì„ ë³´ë©´ ëŒ€ë¶€ë¶„ ì—°ë§ì •ì‚°ì´ ë“¤ì–´ê°€ìˆëŠ” ì§ˆë¬¸ë“¤ì´ ìœ ì‚¬ë„ê°€ ë†’ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. ì´ëŸ° ë¬¸ì œì ì€ í•„ìì˜ ê°œì¸ì ì¸ ìƒê°ìœ¼ë¡œ input matrixë¡œ TF-IDF matrixë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì— ì „ì²´ ì§ˆë¬¸ ê±´ìˆ˜ì—ì„œ ì—°ë§ì •ì‚°ì´ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨ì´ ë†’ë‹¤ë³´ë‹ˆ ë‚˜íƒ€ë‚˜ëŠ” í˜„ìƒì´ë¼ê³  ìƒê°í–ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë¨¼ì € í•„ìëŠ” ê° ë¶„ì•¼ì˜ ì§ˆë¬¸ì˜ ìˆ˜ë¥¼ ë§ì¶”ê±°ë‚˜ ë‹¤ë¥¸ ë°©ë²•ì˜ input matrixë¥¼ ì‚¬ìš©í•´ì„œ ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•  ê²ƒì´ë¼ê³  ìƒê°í–ˆë‹¤. 123456789101112131415161718192021222324252627criterion_sentence_by_token = [\" \".join(criterion_tokens)]criterion_vec = vectorizer.transform(criterion_sentence_by_token)criterion_vec=svd.transform(criterion_vec)svd_l2norm_vectors = normalize(vecs, axis=1, norm='l2')svd_l2norm_criterion_vectors = normalize(criterion_vec, axis=1, norm='l2').reshape(100,1)cosine_similarity = np.dot(svd_l2norm_vectors, svd_l2norm_criterion_vectors)ls=[]for idx, cosine_similarity in enumerate(cosine_similarity.tolist()): ls.append((idx, cosine_similarity))sorted_list = sorted(ls, key= lambda x: x[1], reverse=True)criterion_tokens_list = []for i in np.arange(len(sorted_list)): criterion_tokens_list.append(criterion_tokens)show_list_noun = []for sorted_lists, criterion_tokens in zip(sorted_list, criterion_tokens_list): idx=sorted_lists[0] similarity=sorted_lists[1] tf_list=[] for token in criterion_tokens: tf_list.append(token in raw_data['question'].loc[idx]) if (np.array(tf_list) == True).all(): show_list_noun.append((idx, similarity))show_list_noun 123456789show_list_index = []for idx, score in show_list: show_list_index.append(idx)show_list_noun_index = []for idx, score in show_list_noun: show_list_noun_index.append(idx)set(show_list_noun_index) == set(show_list_index) ê²°ê³¼ëŠ” Falseë¡œ ì²˜ìŒ ëª…ì‚¬ì¶”ì¶œê¸° ì ìˆ˜ë¥¼ ë”í•´ì„œ tokenizingí•œ ê²°ê³¼ê°€ ë” ë§ê³  ì¢‹ì€ ì§ˆë¬¸ë“¤ì„ ê²€ìƒ‰í•´ ë‚´ì—ˆë‹¤. 1False ê²°ë¡  ì—°ë§ì •ì‚° ì¹´í…Œê³ ë¦¬ì˜ ë¬¸ê±´ì´ ì „ì²´ ë¬¸ê±´ ì¤‘ ë‹¤ìˆ˜ë¥¼ í¬í•¨í•˜ê³  ìˆê¸° ë•Œë¬¸ì—, ê·¸ì— ë”°ë¥¸ ì˜í–¥ìœ¼ë¡œ ê²€ìƒ‰ í‚¤ì›Œë“œì— ì—°ë§ì •ì‚°ì´ í¬í•¨ë˜ë©´ ìœ ì‚¬ë„ê°€ í° ë¬¸ê±´ë“¤ì€ ëŒ€ë¶€ë¶„ ì—°ë§ì •ì‚°ì˜ ë‚´ìš©ë§Œì„ ë‹´ê³  ìˆì—ˆê¸° ë•Œë¬¸ì— ì¶”í›„ì— tokenizingí•œ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ì „ë¶€ í¬í•¨í•˜ê³  ìˆëŠ” ë¬¸ê±´ë“¤ì„ ì¶œë ¥í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ë°”ê¾¸ì—ˆë‹¤. ì²˜ìŒì˜ modelì„ ìµœì¢…ì ìœ¼ë¡œ ì„ íƒí•  ê²ƒì´ë©°, í˜„ì¬ì˜ ê²€ìƒ‰ì–´ ì‹œìŠ¤í…œì—ì„œ ì¬ì…ì‚¬ìì™€ ì¬ì…ì‚¬ì ì—°ë§ì •ì‚°ì´ë¼ëŠ” ë‘ ê°€ì§€ í‚¤ì›Œë“œì— ëŒ€í•œ ê²€ìƒ‰ì´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì¬ì…ì‚¬ìëŠ” 11ê±´ ì¬ì…ì‚¬ì ì—°ë§ì •ì‚°ëŠ” 5ê±´ìœ¼ë¡œ ì¬ì…ì‚¬ì í‚¤ì›Œë“œì—ì„œ ëŒ€ë¶€ë¶„ì´ ì—°ë§ì •ì‚°ì— ê´€í•œ ë‚´ìš©ì„ì—ë„ ë¶ˆêµ¬í•˜ê³  ê²€ìƒ‰ì´ ë˜ì§€ ì•ŠëŠ” ë¬¸ì œì ì€ í•´ê²°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì— ë§Œì¡±í•  ê²ƒì´ë‹¤. ê²Œë‹¤ê°€ ê¸°ì¡´ì˜ ë„ì–´ì“°ê¸°ì— ì·¨ì•½í•œ ë¬¸ì œì ë„ ë³´ì™„í•  ìˆ˜ ìˆê¸°ì— ì´ì „ë³´ë‹¤ëŠ” ë” ë‚˜ì€ ê²€ìƒ‰ ì‹œìŠ¤í…œì´ë¼ê³  ì£¼ì¥í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ ë”ì¡´ ì˜¨ë¼ì¸ ê³ ê°ì„¼í„°ì˜ smart A ê²Œì‹œíŒì—ì„œ ë™ì¼í•œ ë‚´ìš©ì´ì§€ë§Œ ë„ì–´ì“°ê¸°ë§Œ ë‹¤ë¥¸ ì¬ì…ì‚¬ì ì—°ë§ì •ì‚°(ìœ„)ê³¼ ì¬ì…ì‚¬ì ì—°ë§ ì •ì‚°(ì•„ë˜)ì´ë¼ëŠ” ë‘ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•œ ê²°ê³¼ì´ë‹¤. ìœ„ ê°™ì´ ë„ì–´ì“°ê¸°ê°€ ë‹¬ë¼ë„ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í–ˆì„ ë•Œ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆë‹¤. ë³´ì™„ì  TF-IDFë¥¼ ì‚¬ìš©í•˜ì˜€ê¸° ë•Œë¬¸ì— ë‹¨ì–´ì™€ ë‹¨ì–´ê°€ ì‚¬ìš©ëœ ë¬¸ê±´ì˜ ìˆ˜ì— ì˜í•œ ê°€ì¤‘ì¹˜ì— ì˜í•´ ì˜í–¥ì„ ë°›ëŠ”ë‹¤ëŠ” ì ì„ ê³ ë ¤í–ˆì—ˆì•¼ í•œë‹¤ëŠ” íŒë‹¨ì„ ë‚´ë ¸ë‹¤. ë˜í•œ, ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ë‚˜ì¤‘ì— ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•œ ë¦¬ìŠ¤íŠ¸ ì¤‘ì— í•„í„°ë§ ì—­í• ë¡œ ì‚¬ìš©í•˜ê¸°ì— ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ë§Œë“¤ë©´ ë” ì¢‹ì€ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ êµ¬ì„±í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ ëœë‹¤. ë˜í•œ, ê²€ìƒ‰ ì‹œìŠ¤í…œ ë¿ë§Œ ì•„ë‹ˆë¼ ìì‹ ì´ ì§ˆë¬¸ì„ ì‘ì„±í•œ í›„ì— ìì‹ ì˜ ì§ˆë¬¸ê³¼ ìœ ì‚¬ë„ê°€ ë†’ì€ ì§ˆë¬¸ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³´ì—¬ì£¼ëŠ” í˜ì´ì§€ë¡œ ì „í™˜ì‹œì¼œ ì£¼ëŠ” ì„œë¹„ìŠ¤ë„ ì¢‹ì„ ê²ƒ ê°™ë‹¤. ì´ëŸ¬í•œ ìƒê°ì´ ë“¤ì—ˆë˜ ì´ìœ ëŠ” ë”ì¡´ ì˜¨ë¼ì¸ ê³ ê°ì„¼í„°ì˜ ë‹µë³€ë“¤ ì¤‘ ì—°ë§ì •ì‚° ê°™ì€ íšŒê³„ë¶„ì•¼ì˜ íŠ¹ì • ì‹œì¦Œ ë•Œ ì§ˆë¬¸ë“¤ì— ëŒ€í•œ ë‹µë³€ì´ ì¡°ê¸ˆ ëŠ¦ëŠ” ê²½ìš°(ë¬¼ë¡  í•˜ë£¨ì´ìƒì„ ë„˜ê¸°ì§€ ì•Šê³  ë‹µë³€ì„ ë‹¤ ë‹¬ì•„ì£¼ì‹ ë‹¤)ë¥¼ ë³´ì•˜ëŠ”ë°, ìœ„ì™€ ê°™ì€ ì‹œìŠ¤í…œì„ ë„ì…í•˜ë©´ ì˜¨ë¼ì¸ ê³ ê°ì„¼í„°ì˜ ì§ì› ë¶„ë“¤ë„ ëœ ê³ ìƒí•˜ì‹œê³ , ê³ ê°ë‹˜ë“¤ê»˜ì„œë„ ì¡°ê¸ˆ ë” í•´ê²°ë°©ì•ˆì„ ë¹¨ë¦¬ ì°¾ìœ¼ì‹¤ ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤ê³  ìƒê°í–ˆê¸° ë•Œë¬¸ì´ë‹¤. í† ì´ í”„ë¡œì íŠ¸ë¥¼ í•˜ë©´ì„œ ëŠë‚€ì  ì œì¼ ë§ì€ ê²ƒì„ ëŠë‚€ê²ƒì€ ì „ì²˜ë¦¬ë¶€ë¶„ì´ì—ˆë‹¤. ë°ì´í„° ë¶„ì„ì— ìˆì–´ì„œ ì „ì²˜ë¦¬ê°€ 80%ì´ìƒì´ë¼ëŠ” ë§ì€ ë§¤ë²ˆ ë˜ìƒˆê¸°ê²Œë˜ì§€ë§Œ, ì´ë²ˆì—ëŠ” íŠ¹íˆë‚˜ ë” ì™€ ë‹¿ì•˜ë˜ í† ì´ í”„ë¡œì íŠ¸ ì˜€ë˜ ê²ƒ ê°™ë‹¤. ë‹¤ìŒ í† ì´ í”„ë¡œì íŠ¸ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ëŠ” ì±—ë´‡ì„ êµ¬í˜„í•´ ë³´ë ¤ê³ í•˜ëŠ”ë°, ì§ˆë¬¸ì˜ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ëª¨ë¸ì„ ë”°ë¡œ ë§Œë“¤ì–´ì•¼ ë  ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆë‹¤. ì±—ë´‡ì„ êµ¬í˜„í•  ë•Œ ë¨¼ì € ì…,ì¶œë ¥ ë²¡í„°ì˜ í¬ê¸°ë¥¼ ì¼ì •í•˜ê²Œ ì •í•´ì„œ ë¶€ì¡±í•˜ë©´ íŒ¨ë”©ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ì•¼ í•˜ëŠ”ë°, ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë‹µë³€ì„ ì£¼ëŠ” í‰ê· ì ì¸ ë‹µë³€ê³¼ ì§ˆë¬¸ì˜ sequenceì˜ ê¸¸ì´ê°€ ë‹¤ë¥¼ ê²ƒì´ë¼ê³  ìƒê° í–ˆê¸° ë•Œë¬¸ì´ë‹¤.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP ì‹¤ìŠµ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ - 02 (XGBoost, 1D-CNN, MaLSTM)","slug":"NLP_11","date":"2020-02-10T16:36:58.000Z","updated":"2020-02-11T08:10:01.052Z","comments":true,"path":"2020/02/11/NLP_11/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/11/NLP_11/","excerpt":"","text":"ëª¨ë¸ì€ ì´ 3ê°€ì§€ë¥¼ ì¢…ë¥˜ë¥¼ ë§Œë“¤ì–´ ë³¼ ê²ƒì´ë‹¤. XGBoost CNN MaLSTM XGBoost ì•™ìƒë¸” ëª¨ë¸ ì¤‘ í•˜ë‚˜ì¸ XGBoost ëª¨ë¸ì€ â€˜eXtream Gradient Boostingâ€™ì˜ ì•½ìë¡œ ìºê¸€ ì‚¬ìš©ìì— í° ì¸ê¸°ë¥¼ ì–»ì€ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì´ë‹¤. ì•™ìƒë¸” ê¸°ë²•ì´ë€ ì—¬ëŸ¬ ê°œì˜ í•™ìŠµ ì•Œê³ ì¦˜ì„ ì‚¬ìš©í•´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ëŠ” ë°©ë²•ì„ ëœ»í•œë‹¤. ì•™ìƒë¸” ê¸°ë²•ì—ëŠ” Baggingê³¼ Boostingì´ë¼ëŠ” ë°©ë²•ì´ ìˆë‹¤. Baggingì€ ì—¬ëŸ¬ ê°œì˜ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜, ëª¨ë¸ì„ í†µí•´ ê°ê° ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ê³  ëª¨ë“  ê²°ê³¼ë¥¼ ë™ë“±í•˜ê²Œ ë³´ê³  ì·¨í•©í•´ì„œ ê²°ê³¼ë¥¼ ì–»ëŠ” ë°©ì‹ì´ë‹¤. Random Forestë„ ì—¬ëŸ¬ê°œì˜ decision tree ê²°ê³¼ê°’ì˜ í‰ê· ì„ í†µí•´ ê²°ê³¼ë¥¼ ì–»ëŠ” Baggingì˜ ì¼ì¢…ì´ë‹¤. Boostingì€ ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜, ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì·¨í•©í•˜ëŠ”ë°, ë‹¨ìˆœíˆ í•˜ë‚˜ì”© ì·¨í•˜ëŠ” ë°©ë²•ì´ ì•„ë‹ˆë¼ ì´ì „ ì•Œê³ ë¦¬ì¦˜, ëª¨ë¸ì´ í•™ìŠµ í›„ ì˜ëª» ì˜ˆì¸¡í•œ ë¶€ë¶„ì— ê°€ì¤‘ì¹˜ë¥¼ ì¤˜ì„œ ë‹¤ì‹œ ëª¨ë¸ë¡œ ê°€ì„œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ë‹¤. XGBoostëŠ” Boosting ê¸°ë²• ì¤‘ Tree Bossting ê¸°ë²•ì„ í™œìš©í•œ ëª¨ë¸ì´ë‹¤. ì‰½ê²Œ ë§í•´ Random Forestì™€ ë¹„ìŠ·í•œ ì›ë¦¬ì— Boosting ê¸°ë²•ì„ ì ìš©í–ˆë‹¤ê³  ìƒê°í•˜ë©´ëœë‹¤. ì—¬ëŸ¬ê°œì˜ Decision Treeë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ ë‹¨ìˆœíˆ ê²°ê³¼ë¥¼ í‰ê· ë‚´ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê²°ê³¼ë¥¼ ë³´ê³  ì˜¤ë‹µì— ëŒ€í•´ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œë‹¤. ê·¸ë¦¬ê³  ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì˜¤ë‹µì— ëŒ€í•´ì„œëŠ” ê´€ì‹¬ì„ ê°€ì§€ê³  ì •ë‹µì´ ë  ìˆ˜ ìˆë„ë¡ ê²°ê³¼ë¥¼ ë§Œë“¤ê³  í•´ë‹¹ ê²°ê³¼ì— ëŒ€í•œ ë‹¤ë¥¸ ì˜¤ë‹µì„ ì°¾ì•„ ë‹¤ì‹œ ë˜‘ê°™ì€ ì‘ì—…ì„ ë°˜ë³µì ìœ¼ë¡œ ì§„í–‰í•˜ëŠ” ê²ƒì´ë‹¤. ìµœì¢…ì ìœ¼ë¡œëŠ” XGBoostë€ ì´ëŸ¬í•œ Tree Boosting ë°©ì‹ì— ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•´ optimizationì„ í•˜ëŠ” ë°©ë²•ì´ë‹¤. ê·¸ë¦¬ê³  ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•´ Decision Treeë¥¼ êµ¬ì„±í•  ë•Œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•´ ë¹ ë¥¸ ì‹œê°„ì— í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤. 1234567TRAIN_Q1_DATA_FILE = 'q1_train.npy'TRAIN_Q2_DATA_FILE = 'q2_train.npy'TRAIN_LABEL_DATA_FILE = 'label_train.npy'train_q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, 'rb'))train_q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, 'rb'))train_labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, 'rb')) numpyì˜ stack í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë‘ ì§ˆë¬¸ì„ í•˜ë‚˜ì˜ ìŒìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì§ˆë¬¸ [A]ì™€ ì§ˆë¬¸ [B]ê°€ ìˆì„ ë•Œ ì´ ì§ˆë¬¸ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ [[A], [B]] í˜•íƒœë¡œ ë§Œë“¤ì—ˆë‹¤. ì´ì™€ ê°™ì€ í˜•íƒœëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. 12train_input_expand = np.concatenate((np.expand_dims(train_q1_data, 1), np.expand_dims(train_q2_data, 1)), axis=1)train_input_expand.shape ê²°ê³¼1(298526, 2, 31) 12train_input_concate = np.concatenate((train_q1_data[:,np.newaxis,:], train_q2_data[:,np.newaxis,:]), axis=1)train_input_concate.shape ê²°ê³¼1(298526, 2, 31) 12train_input_stack = np.stack((train_q1_data, train_q2_data), axis=1)train_input_stack.shape ê²°ê³¼1(298526, 2, 31) 1(train_input_concate == train_input_stack).all() and (train_input_stack == train_input_expand).all() and (train_input_concate == train_input_expand).all() ê²°ê³¼1True ì „ì²´ 29ë§Œê°œ ì •ë„ì˜ ë°ì´í„°ì— ëŒ€í•´ ë‘ ì§ˆë¬¸ì´ ê°ê° 31ê°œì˜ ì§ˆë¬¸ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆìŒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ë‘ ì§ˆë¬¸ ìŒì´ í•˜ë‚˜ë¡œ ë¬¶ì—¬ ìˆëŠ” ê²ƒë„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì´ì œ í•™ìŠµ ë°ì´í„°ì˜ 20%ë¥¼ ëª¨ë¸ ê²€ì¦ì„ ìœ„í•œ validation setìœ¼ë¡œ ë§Œë“¤ì–´ ë‘˜ ê²ƒì´ë‹¤. 123from sklearn.model_selection import train_test_splittrain_input, eval_input, train_label, eval_label = train_test_split(train_input_stack, train_labels, test_size=0.2, random_state=4242) XGBoostë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì…ë ¥ê°’ì„ xgb ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë°ì´í„° í˜•ì‹ì¸ DMatrix í˜•íƒœë¡œ ë§Œë“¤ì–´ì•¼ í•œë‹¤. í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„° ëª¨ë‘ ì ìš©í•´ì„œ í•´ë‹¹ ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë§Œë“ ë‹¤. ì ìš© ê³¼ì •ì—ì„œ ê° ë°ì´í„°ì— ëŒ€í•´ sum í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ”ë° ì´ëŠ” ê° ë°ì´í„°ì˜ ë‘ ì§ˆë¬¸ì„ í•˜ë‚˜ì˜ ê°’ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ê¸° ìœ„í•´ì„œì´ë‹¤. ê·¸ë¦¬ê³  ë‘ ê°œì˜ ë°ì´í„°ë¥¼ ë¬¶ì–´ì„œ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“ ë‹¤. ì´ë•Œ í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ëŠ” ê° ìƒíƒœì˜ ë¬¸ìì—´ê³¼ í•¨ê»˜ tupleí˜•íƒœë¡œ êµ¬ì„±í•œë‹¤. ì°¸ê³ ë¡œ XGBoostì™€ sklearnì˜ ensemble.GradientBoostingClassifierì€ ë™ì¼í•˜ê²Œ Tree Boosting ëª¨ë¸ì„ ê°€ì§€ê³  ìˆì§€ë§Œ ì†ë„ë©´ì—ì„œ XGBoostê°€ í›¨ì”¬ ë¹ ë¥´ë‹¤. (ì‚¬ìš©ë²•ë„ ì¡°ê¸ˆ ë‹¤ë¦„) 123456import xgboost as xgbtrain_data = xgb.DMatrix(train_input.sum(axis=1), label=train_label)eval_data = xgb.DMatrix(eval_input.sum(axis=1), label=eval_label)data_list = [(train_data, 'train'), (eval_data, 'valid')] ìš°ì„  ëª¨ë¸ì„ ë§Œë“¤ê³  í•™ìŠµí•˜ê¸° ìœ„í•´ ëª‡ ê°€ì§€ ì„ íƒí•´ì•¼ í•˜ëŠ” ì˜µì…˜ì€ dictionaryë¥¼ ë§Œë“¤ì–´ ë„£ìœ¼ë©´ ëœë‹¤. ì´ë•Œ dictionaryì—ëŠ” ëª¨ë¸ì˜ objective(loss) functionì™€ í‰ê°€ ì§€í‘œë¥¼ ì •í•´ì„œ ë„£ì–´ì•¼ í•˜ëŠ”ë° ì—¬ê¸°ì„œëŠ” ìš°ì„  objective(loss) functionì˜ ê²½ìš° ì´ì§„ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. í‰ê°€ ì§€í‘œì˜ ê²½ìš° RMSEë¥¼ ì‚¬ìš©í•œë‹¤. ì´ë ‡ê²Œ ë§Œë“  ì¸ìì™€ í•™ìŠµ ë°ì´í„°, ë°ì´í„°ë¥¼ ë°˜ë³µí•˜ëŠ” íšŸìˆ˜ì¸ num_boost_round, ëª¨ë¸ ê²€ì¦ ì‹œ ì‚¬ìš©í•  ì „ì²´ ë°ì´í„° ìŒ, ê·¸ë¦¬ê³  early stoppingì„ ìœ„í•œ íšŸìˆ˜ë¥¼ ì •í•œë‹¤. ë°ì´í„°ë¥¼ ë°˜ë³µí•˜ëŠ” íšŸìˆ˜, ì¦‰ Epochì„ ì˜ë¯¸í•˜ëŠ” ê°’ìœ¼ë¡œëŠ” 1000ì„ ì„¤ì •í–ˆë‹¤. ì „ì²´ ë°ì´í„°ë¥¼ 1000ë²ˆ ë°˜ë³µí•´ì•¼ ëë‚˜ë„ë¡ ì„¤ì •í•œ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  early stoppingì„ ìœ„í•œ íšŸìˆ˜ê°’ìœ¼ë¡œëŠ” 10ì„ ì„¤ì •í•´ì„œ ë§Œì•½ 10 epoch ë™ì•ˆ errorê°’ì´ í¬ê²Œ ì¤„ì§€ ì•ŠëŠ”ë‹¤ë©´ í•™ìŠµì„ ì¢…ë£Œì‹œí‚¤ë„ë¡ í•˜ì˜€ë‹¤. 12345params = &#123;&#125;params['objective'] = 'binary:logistic'params['eval_metric'] = 'rmse'bst = xgb.train(params, train_data, num_boost_round = 1000, evals = data_list, early_stopping_rounds=10) ì˜ˆì¸¡í•˜ê¸°1234567TEST_Q1_DATA_FILE = 'test_q1.npy'TEST_Q2_DATA_FILE = 'test_q2.npy'TEST_ID_DATA_FILE = 'test_id.npy'test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, 'rb'))test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, 'rb'))test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb')) 123test_input = np.stack((test_q1_data, test_q2_data), axis=1)test_data = xgb.DMatrix(test_input.sum(axis=1))test_predict = bst.predict(test_data) 1234567DATA_OUT_PATH = '/content/'if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_OUT_PATH)output = pd.DataFrame(&#123;'test_id': test_id_data, 'is_duplicate': test_predict&#125;)output.to_csv(DATA_OUT_PATH + 'sample_xgb.csv', index=False) kaggle APIë¥¼ í†µí•´ì„œ ë°”ë¡œ íŒŒì¼ ì˜¬ë ¤ì£¼ì—ˆë‹¤. 1!kaggle competitions submit quora-question-pairs -f \"sample_xgb.csv\" -m \"XGBoost Model\" 3294íŒ€ ì¤‘ 2714\u001dë“±ì´ë‹¤. ë¬¼ë¡ , ì„ë² ë”© ë²¡í„°ë¼ë“ ì§€ ì•„ë¬´ëŸ° ì¡°ì¹˜ë¥¼ ì·¨í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— scoreê°€ ì•ˆì¢‹ì„ ìˆ˜ ë°–ì— ì—†ë‹¤. ì¶”í›„ì— TF-IDF í–‰ë ¬ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ë” ì¢‹ì€ ì„ë² ë”© ê¸°ë²•ì„ ì‚¬ìš©í•´ì„œ ë‹¤ì‹œ ì˜¬ë ¤ë³¼ ê²ƒì´ë‹¤. ì§€ê¸ˆì€ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì´ë¯€ë¡œ ì´ ì •ë„ì—ì„œ ê·¸ì¹˜ê² ë‹¤. CNN í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¶„ì„ ëª¨ë¸ í•©ì„±ê³± ì‹ ê²½ë§ êµ¬ì¡°ë¥¼ í™œìš©í•´ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ ë³´ê² ë‹¤. ê¸°ë³¸ì ì¸ êµ¬ì¡°ëŠ” ì´ì „ ì¥ì˜ í•©ì„±ê³± ëª¨ë¸ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ ì´ë²ˆ ê²½ìš°ì—ëŠ” ê° ë°ì´í„°ê°€ ë‘ ê°œì˜ í…ìŠ¤íŠ¸ ë¬¸ì¥ìœ¼ë¡œ ë¼ ìˆê¸° ë•Œë¬¸ì— ë³‘ë ¬ì ì¸ êµ¬ì¡°ë¥¼ ê°€ì§„ ëª¨ë¸ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤. ëª¨ë¸ì— ì…ë ¥í•˜ê³ ì í•˜ëŠ” ë°ì´í„°ëŠ” ë¬¸ì¥ 2ê°œë‹¤. ë¬¸ì¥ì— ëŒ€í•œ ìœ ì‚¬ë„ë¥¼ ë³´ê¸° ìœ„í•´ì„œëŠ” ê¸°ì¤€ì´ ë˜ëŠ” ë¬¸ì¥ì´ í•„ìš”í•˜ë‹¤. ì´ë¥¼ â€˜ê¸°ì¤€ ë¬¸ì¥â€™ì´ë¼ ì •ì˜í•œë‹¤. ê·¸ë¦¬ê³  â€˜ê¸°ì¤€ ë¬¸ì¥â€™ì— ëŒ€í•´ ë¹„êµí•´ì•¼ í•˜ëŠ” ë¬¸ì¥ì´ ìˆëŠ”ë° ì´ë¥¼ â€˜ëŒ€ìƒë¬¸ì¥â€™ì´ë¼ í•œë‹¤. ë§Œì•½ ëª¨ë¸ì— ì…ë ¥í•˜ê³ ì í•˜ëŠ” ê¸°ì¤€ ë¬¸ì¥ì´ â€˜I love deep NLPâ€™ì´ê³  ì´ë¥¼ ë¹„êµí•  ëŒ€ìƒ ë¬¸ì¥ì´ â€˜Deep NLP is awesomeâ€™ì´ë¼ í•˜ì. ì´ ë‘ ë¬¸ì¥ì€ ì˜ë¯¸ê°€ ìƒë‹¹íˆ ìœ ì‚¬í•˜ë‹¤. ë§Œì•½ í•™ìŠµì´ ì§„í–‰ëœ í›„ì— ë‘ ë¬¸ì¥ì— ëŒ€í•œ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ê³ í•˜ í•œë‹¤ë§ˆë…€ ì•„ë§ˆë„ ë†’ì€ ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ë³´ì¼ ê²ƒì´ë‹¤. ì´ì²˜ëŸ¼ ë¬¸ì¥ì´ ì˜ë¯¸ì ìœ¼ë¡œ ê°€ê¹Œìš°ë©´ ìœ ì‚¬ë„ ì ìˆ˜ëŠ” ë†’ê²Œ í‘œí˜„ ë  ê²ƒì´ê³  ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš°ì—ëŠ” ë‚®ê²Œ í‘œí˜„ë  ê²ƒì´ë‹¤. ì „ë°˜ì ì¸ ìœ ì‚¬ë„ ë¶„ì„ ëª¨ë¸ êµ¬ì¡°ì— ëŒ€í•œ íë¦„ì„ ë³´ì. ëª¨ë¸ì— ë°ì´í„°ë¥¼ ì…ë ¥í•˜ê¸° ì „ì— ê¸°ì¤€ ë¬¸ì¥ê³¼ ëŒ€ìƒ ë¬¸ì¥ì— ëŒ€í•´ì„œ ì¸ë±ì‹±ì„ ê±°ì³ ë¬¸ìì—´ í˜•íƒœì˜ ë¬¸ì¥ì„ ì¸ë±ìŠ¤ ë²¡í„° í˜•íƒœë¡œ êµ¬ì„±í•œë‹¤. ì¸ë±ìŠ¤ ë²¡í„°ë¡œ êµ¬ì„±ëœ ë¬¸ì¥ ì •ë³´ëŠ” ì„ë² ë”© ê³¼ì •ì„ í†µí•´ ê° ë‹¨ì–´ë“¤ì´ ì„ë² ë”© ë²¡í„°ë¡œ ë°”ë€ í–‰ë ¬ë¡œ êµ¬ì„± ë  ê²ƒì´ë‹¤. ì„ë² ë”© ê³¼ì •ì„ í†µí•´ ë‚˜ì˜¨ ë¬¸ì¥ í–‰ë ¬ì€ ê¸°ì¤€ ë¬¸ì¥ê³¼ ëŒ€ìƒ ë¬¸ì¥ ê°ê°ì— í•´ë‹¹í•˜ëŠ” CNN ë¸”ë¡ì„ ê±°ì¹˜ê²Œ í•œë‹¤. CNN ë¸”ë¡ì€ Convolution ì¸µê³¼ Max Poolingì¸µì„ í•©ì¹œ í•˜ë‚˜ì˜ ì‹ ê²½ë§ì„ ì˜ë¯¸í•œë‹¤. ë‘ ë¸”ë¡ì„ ê±°ì³ ë‚˜ì˜¨ ë²¡í„°ëŠ” ë¬¸ì¥ì— ëŒ€í•œ ì˜ë¯¸ ë²¡í„°ê°€ ëœë‹¤. ë‘ ë¬¸ì¥ì— ëŒ€í•œ ì˜ë¯¸ ë²¡í„°ë¥¼ ê°€ì§€ê³  ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ ìœ ì‚¬ë„ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œëŠ” FC layerë¥¼ ê±°ì¹œ í›„ ìµœì¢…ì ìœ¼ë¡œ logistic regression ë°©ë²•ì„ í†µí•´ ë¬¸ì ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ì¸¡ì •í•  ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ì¸¡ì •í•œ ì ìˆ˜ì— ë”°ë¼ ë‘ ë¬¸ì¥ì˜ ìœ ì‚¬ ì—¬ë¶€ë¥¼ íŒë‹¨í•  ê²ƒì´ë‹¤. ëª¨ë¸ êµ¬í˜„ ì¤€ë¹„1234567import tensorflow as tfimport numpy as npimport osfrom sklearn.model_selection import train_test_splitimport json 12345678910DATA_IN_PATH = '/content/'DATA_OUT_PATH = '/content/'TRAIN_Q1_DATA_FILE = 'q1_train.npy'TRAIN_Q2_DATA_FILE = 'q2_train.npy'TRAIN_LABEL_DATA_FILE = 'label_train.npy'DATA_CONFIGS = 'data_configs.json'TEST_SPLIT = 0.1RNG_SEED = 13371447 ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì • 1234567891011121314151617EPOCH=10BATCH_SIZE=1024MAX_SEQUENCE_LENGTH = 26 # 31WORD_EMBEDDING_DIM = 100CONV_FEATURE_DIM = 300CONV_OUTPUT_DIM = 128CONV_WINDOW_SIZE = 3SIMILARITY_DENSE_FEATURE_DIM = 200prepro_configs = Nonewith open(DATA_IN_PATH + DATA_CONFIGS, 'r') as f: prepro_configs = json.load(f)VOCAB_SIZE = prepro_configs['vocab_size'] #76464ê°œ 123q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, 'rb'))q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, 'rb'))labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, 'rb')) 123456789X = np.stack((q1_data, q2_data), axis=1)y = labelstrain_X, eval_X, train_y, eval_y = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)train_Q1 = train_X[:, 0]train_Q2 = train_X[:, 1]eval_Q1 = eval_X[:,0]eval_Q2 = eval_X[:,1] estimatorì— í™œìš©í•  ë°ì´í„° ì…ë ¥ í•¨ìˆ˜ë¥¼ ë§Œë“¤ ê²ƒì´ë‹¤. map í•¨ìˆ˜ í•™ìŠµ ì…ë ¥ í•¨ìˆ˜ ê²€ì¦ ì…ë ¥ í•¨ìˆ˜ ìš°ì„  map í•¨ìˆ˜ë¡œ ì •ì˜í•œ rearrange í•¨ìˆ˜ë¶€í„° ì„¤ëª…í•˜ë©´ 3ê°œì˜ ê°’ì´ ì¸ìë¡œ ë“¤ì–´ì˜¤ëŠ”ë°, ê°ê° ê¸°ì¤€ ì§ˆë¬¸, ëŒ€ìƒ ì§ˆë¬¸, ë¼ë²¨ê°’ì´ë‹¤. ì´ë ‡ê²Œ ë“¤ì–´ì˜¨ ì¸ì ê°’ì„ í†µí•´ 2ê°œì˜ ì§ˆë¬¸ì„ í•˜ë‚˜ì˜ dictionary í˜•íƒœì˜ ì…ë ¥ê°’ìœ¼ë¡œ ë§Œë“ ë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ë§Œë“  dictionaryì™€ labelì„ returní•˜ëŠ” êµ¬ì¡°ë¡œ ë¼ ìˆë‹¤. ì´ í•¨ìˆ˜ë¥¼ í•™ìŠµ ì…ë ¥í•¨ìˆ˜ì™€ ê²€ì¦ ì…ë ¥ í•¨ìˆ˜ì— ì ìš©í•  ê²ƒì´ë‹¤. 12345678910111213141516171819202122def rearrange(base, hypothesis, label): features = &#123;'x1' : base, 'x2' : hypothesis&#125; return features, labeldef train_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y)) dataset = dataset.shuffle(buffer_size=100) dataset = dataset.batch(16) dataset = dataset.map(rearrange) dataset = dataset.repeat(EPOCH) iterator = dataset.make_one_shot_iterator() return iterator.get_next()def eval_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((eval_Q1, eval_Q2, eval_y)) dataset = dataset.shuffle(buffer_size=100) dataset = dataset.batch(16) dataset = dataset.map(rearrange) iterator = dataset.make_one_shot_iterator() return iterator.get_next() ëª¨ë¸ êµ¬í˜„ CNN ë¸”ë¡ í•¨ìˆ˜ë¥¼ ë¨¼ì € ì •ì˜í•  ê²ƒì´ë‹¤. CNN ë¸”ë¡ í•¨ìˆ˜ëŠ” convolution layerì™€ Pooling, Denseë¥¼ í•˜ë‚˜ë¡œ í•©ì¹œ í˜•íƒœë¡œ ì •ì˜í•  ê²ƒì´ë‹¤. ì´ í•¨ìˆ˜ëŠ” 2ê°œì˜ ì¸ìê°’ì„ ë°›ëŠ”ë°, ê°ê° ì…ë ¥ê°’ê³¼ ì´ë¦„ì„ ì˜ë¯¸í•œë‹¤. ì´ í•¨ìˆ˜ì—ì„œ í•©ì„±ê³±ì˜ ê²½ìš° ì´ì „ ì¥ì˜ CNN ëª¨ë¸ì„ êµ¬ì„±í•  ë•Œì™€ ë™ì¼í•˜ê²Œ Conv1Dë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. Max Poolingë„ ë§ˆì°¬ì‚¬ì§€ë¡œ MaxPooling1D ê°ì²´ë¥¼ í™œìš©í•œë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ í•©ì„±ê³±ê³¼ Max Poolingì„ ì ìš©í•œ ê°’ì— ëŒ€í•´ ì°¨ì›ì„ ë°”ê¾¸ê¸° ìœ„í•´ Dense ì¸µì„ í†µê³¼ ì‹œí‚¨ë‹¤. 1234567891011121314def basic_conv_sementic_network(inputs, name): conv_layer = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, CONV_WINDOW_SIZE, activation=tf.nn.relu, name=name + 'conv_1d', padding='same')(inputs) #1024 X 26 X 300 max_pool_layer = tf.keras.layers.MaxPool1D(MAX_SEQUENCE_LENGTH, 1)(conv_layer) # 1024 X 1 X 300 output_layer = tf.keras.layers.Dense(CONV_OUTPUT_DIM, activation=tf.nn.relu, name=name + 'dense')(max_pool_layer) #1024 X 1 X 128 output_layer = tf.squeeze(output_layer, 1) # 1024 X 128 return output_layer ì´ì œ ëª¨ë¸ í•¨ìˆ˜ë¥¼ ì„¤ëª… í•  ê²ƒì´ë‹¤. ë¨¼ì € í˜„ì¬ íŠœí† ë¦¬ì–¼ì€ ì„ë² ë”© ë²¡í„°ì— í¬ê²Œ ì‹ ê²½ì“°ì§€ ì•Šê³  ëª¨ë¸ì˜ êµ¬ì¡°ì— ëŒ€í•´ ì§‘ì¤‘í•˜ëŠ” íŠœí† ë¦¬ì–¼ì´ë¯€ë¡œ íŠ¹ë³„í•œ ê¸°ë²• ì—†ì´ tf.keras.layers.Embeddingìœ¼ë¡œ ì„ë² ë”© ë²¡í„°ë¥¼ ë§Œë“¤ì–´ì¤€ ë’¤ Conv1D êµ¬ì¡°ë¥¼ 3ë²ˆ ê±°ì³ ìµœì¢…ì ìœ¼ë¡œ dense layerë¥¼ í†µí•´ 1ê°œì˜ ë…¸ë“œë¡œ ë§ì¶°ì¤€ logit ê°’ì„ sigmoidí•¨ìˆ˜ë¥¼ í†µí•´ ë§ˆì¹˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ê°™ì€ êµ¬ì¡°ë¥¼ ë§Œë“¤ì–´ ì¤„ ê²ƒì´ë‹¤. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263def model_fn(features, labels, mode): TRAIN = mode == tf.estimator.ModeKeys.TRAIN EVAL = mode == tf.estimator.ModeKeys.EVAL PREDICT = mode == tf.estimator.ModeKeys.PREDICT embedding = tf.keras.layers.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM) base_embedded_matrix = embedding(features['x1']) # 1024 X 31 X 100 hypothesis_embedded_matrix = embedding(features['x2']) # 1024 X 31 X 100 base_embedded_matrix = tf.keras.layers.Dropout(0.2)(base_embedded_matrix) hypothesis_embedded_matrix = tf.keras.layers.Dropout(0.2)(hypothesis_embedded_matrix) conv_layer_base_first = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, CONV_WINDOW_SIZE, activation=tf.nn.relu, padding='same')(base_embedded_matrix) #1024 X 31 X 300 max_pool_layer_base_first = tf.keras.layers.MaxPool1D(2, 1)(conv_layer_base_first) # 1024 X 30 X 300 conv_layer_hypothesis_first = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, CONV_WINDOW_SIZE, activation=tf.nn.relu, padding='same')(hypothesis_embedded_matrix) #1024 X 31 X 300 max_pool_layer_hypothesis_first = tf.keras.layers.MaxPool1D(2, 1)(conv_layer_hypothesis_first) # 1024 X 30 X 300 conv_layer_base_second = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, 5, activation=tf.nn.relu, padding='same')(max_pool_layer_base_first) #1024 X 30 X 300 max_pool_layer_base_second = tf.keras.layers.MaxPool1D(5, 1)(conv_layer_base_second) # 1024 X 26 X 300 conv_layer_hypothesis_second = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, 5, activation=tf.nn.relu, padding='same')(max_pool_layer_hypothesis_first) #1024 X 30 X 300 max_pool_layer_hypothesis_second = tf.keras.layers.MaxPool1D(5, 1)(conv_layer_hypothesis_second) # 1024 X 26 X 300 base_sementic_matrix = basic_conv_sementic_network(max_pool_layer_base_second, 'base') # 1024 X 128 hypothesis_sementic_matrix = basic_conv_sementic_network(max_pool_layer_hypothesis_second, 'hypothesis') # 1024 X 128 merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1) # 1024 X 256 similarity_dense_layer = tf.keras.layers.Dense(SIMILARITY_DENSE_FEATURE_DIM, activation=tf.nn.relu)(merged_matrix) # 1024 X 200 similarity_dense_layer = tf.keras.layers.Dropout(0.2)(similarity_dense_layer) logit_layer = tf.keras.layers.Dense(1)(similarity_dense_layer) # 1024 X 1 logit_layer = tf.squeeze(logit_layer, 1) # (1024, ) similarity = tf.nn.sigmoid(logit_layer) if PREDICT: return tf.estimator.EstimatorSpec( mode=mode, predictions=&#123; 'is_duplicate':similarity &#125;) loss = tf.losses.sigmoid_cross_entropy(labels, logit_layer) if EVAL: accuracy = tf.metrics.accuracy(labels, tf.round(similarity)) return tf.estimator.EstimatorSpec( mode=mode, eval_metric_ops= &#123;'acc': accuracy&#125;, loss=loss) if TRAIN: global_step = tf.train.get_global_step() train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step) return tf.estimator.EstimatorSpec( mode=mode, train_op=train_op, loss=loss) ë¨¼ì €, ë³€ìˆ˜ê°’ ë“± ëª¨ë¸ê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ë‹´ì€ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì €ì¥í•  ê²½ë¡œë¥¼ ì„¤ì •í•´ì•¼í•œë‹¤. ê²½ë¡œë¥¼ ì§€ì •í•œ í›„ í•´ë‹¹ ê²½ë¡œê°€ ì—†ë‹¤ë©´ ìƒì„±í•˜ê³  Estimator ê°ì²´ë¥¼ ìƒì„±í•  ë•Œ í•´ë‹¹ ê²½ë¡œë¥¼ ì„¤ì •í•œë‹¤. 12345DATA_OUT_PATH = '/content/'if not os.path.exists(DATA_OUT_PATH): os.akedirs(DATA_OUT_PATH)est = tf.estimator.Estimator(model_fn, model_dir=DATA_OUT_PATH + 'checkpoint') 1est.train(train_input_fn) 1est.evaluate(eval_input_fn) ë°ì´í„° ì œì¶œ1234567TEST_Q1_DATA_FILE = 'test_q1.npy'TEST_Q2_DATA_FILE = 'test_q2.npy'TEST_ID_DATA_FILE = 'test_id.npy'test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, 'rb'), allow_pickle=True)test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, 'rb'), allow_pickle=True)test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA_FILE, 'rb'), allow_pickle=True) ì…ë ¥ í•¨ìˆ˜ë‚˜ ê²€ì¦ í•¨ìˆ˜ì²˜ëŸ¼ ë³„ë„ì˜ í•¨ìˆ˜ë¡œ ì •ì˜í•˜ì§€ ì•Šê³  Estimatorì˜ ê¸°ë³¸ numpy_input_fn í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. ì…ë ¥ í˜•íƒœëŠ” ì•ì„œ ë‹¤ë¥¸ ì…ë ¥ í•¨ìˆ˜ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë‘ ì§ˆë¬¸ì„ dictionary í˜•íƒœë¡œ ë§Œë“¤ì—ˆë‹¤. ê·¸ë¦¬ê³  shffle=Falseë¡œ ì„¤ì •í•˜ëŠ”ë° ì´ëŠ” ë‘ ê°œì˜ ì§ˆë¬¸ìŒì´ ê°™ì€ ìˆœì„œë¡œ ì…ë ¥ë¼ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ì œ ì´ í•¨ìˆ˜ë¥¼ í™œìš©í•´ Estimatorì˜ predict í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•  ê²ƒì´ë‹¤. predict í•¨ìˆ˜ë¥¼ í™œìš©í•˜ê³  ë°˜ë³µë¬¸ì„ í†µí•´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì„ ë°›ì„ ìˆ˜ ìˆê²Œ í•œë‹¤. ì´ë•Œ ë°›ê³ ì í•˜ëŠ” ì˜ˆì¸¡ê°’ì— ëŒ€í•´ì„œëŠ” is_duplicateë¼ëŠ” key ê°’ìœ¼ë¡œ ì •ì˜í–ˆê¸° ë•Œë¬¸ì— ì•„ë˜ì™€ ê°™ì´ ìœ ì‚¬ë„ ì˜ˆì¸¡ê°’ë§Œì„ ë°›ì„ ê²ƒì´ë‹¤. 12345678predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;\"x1\":test_q1_data, \"x2\":test_q2_data&#125;, shuffle=False)predictions = np.array([p['is_duplicate'] for p in est.predict(input_fn=predict_input_fn)])output = pd.DataFrame( data=&#123;\"test_id\":test_id_data, \"is_duplicate\": list(predictions)&#125; )output.to_csv(\"cnn_predict.csv\", index=False, quoting=3) kaggle ì œì¶œ1!kaggle competitions submit quora-question-pairs -f \"cnn_predict.csv\" -m \"cnn conv1d 3layer 10 Epoches\" MaLSTM ë§ˆì§€ë§‰ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì¸¡ì •ì„ ìœ„í•´ ì‚¬ìš©í•  ëª¨ë¸ì€ MaLSTM ëª¨ë¸ì´ë‹¤. ìˆœì„œê°€ ìˆëŠ” ì…ë ¥ ë°ì´í„°ì— ì í•©í•˜ë‹¤ëŠ” í‰ì„ ë°›ëŠ” RNN ëª¨ë¸ì„ í†µí•´ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•œë‹¤. ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ í™œìš©í•˜ëŠ” ëŒ€í‘œì ì¸ ëª¨ë¸ì¸ MaLSTM ëª¨ë¸ì€ 2016ë…„ MITì—ì„œ Jonas Muellerê°€ ì“´ â€œSiamese Recurrent Architectures for Learning Sentence Similarityâ€ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ì²˜ìŒ ì†Œê°œ ë˜ì—ˆë‹¤. MaLSTMì´ë€ Manhattan Distance + LSTM ì˜ ì¤„ì„ë§ë¡œì¨, ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ë•Œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹  ë§¨í•˜íƒ„ ê±°ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë‹¤. ì´ì „ì˜ í•©ì„±ê³± ì‹ ê²½ë§ ëª¨ë¸ì—ì„œë„ ë‘ ê°œì˜ ë¬¸ì¥ ì…ë ¥ê°’ì— ëŒ€í•´ ê°ê° í•©ì„±ê³± ì¸µì„ ì ìš©í•œ í›„ ìµœì¢…ì ìœ¼ë¡œ ê° ë¬¸ì¥ì— ëŒ€í•´ ì˜ë¯¸ ë²¡í„°ë¥¼ ê°ê° ë½‘ì•„ë‚´ì„œ concatenateí•œ í›„ dese layerë¥¼ í†µí•´ ì„ í˜• ë³€í™˜ í•´ì¤€ ë’¤ ë¡œì§€ìŠ¤í‹± ëª¨í˜•ê³¼ ê°™ì´ ê°’ì„ êµ¬í•´ ë‘ ë¬¸ì¥ì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í–ˆë‹¤. ì´ë²ˆì—ëŠ” ë§¨í•˜íƒ„ ê±°ë¦¬ë¡œ ë¹„êµí•˜ëŠ” í˜•íƒœì˜ ëª¨ë¸ë¡œì„œ, LSTMì˜ ë§ˆì§€ë§‰ ìŠ¤í…ì˜ LSTM hidden stateëŠ” ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ì— ëŒ€í•œ ì •ë³´ê°€ ë°˜ì˜ëœ ê°’ìœ¼ë¡œ ì „ì²´ ë¬¸ì¥ì„ ëŒ€í‘œí•˜ëŠ” ë²¡í„°ê°€ ëœë‹¤. ì´ë ‡ê²Œ ë½‘ì€ ë‘ ë²¡í„°ì— ëŒ€í•´ ë§¨í•˜íƒ„ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ì„œ ë‘ ë¬¸ì¥ ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì • í•  ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ê³„ì‚°í•œ ìœ ì‚¬ë„ë¥¼ ì‹¤ì œ ë¼ë²¨ê³¼ ë¹„êµí•´ì„œ í•™ìŠµí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ ì„¤ê³„í•  ê²ƒì´ë‹¤. 123456789import sysimport tensorflow as tfimport numpy as npimport osimport pandas as pdfrom sklearn.model_selection import train_test_splitimport json ëª¨ë¸ êµ¬í˜„ ë¯¸ë¦¬ Global ë³€ìˆ˜ë¥¼ ì§€ì •í•˜ì. íŒŒì¼ ëª…, íŒŒì¼ ìœ„ì¹˜, ë””ë ‰í† ë¦¬ ë“±ì´ ìˆë‹¤. 123456789101112131415161718192021DATA_IN_PATH = '/content/'DATA_OUT_PATH = '/content/'TRAIN_Q1_DATA_FILE = 'q1_train.npy'TRAIN_Q2_DATA_FILE = 'q2_train.npy'TRAIN_LABEL_DATA_FILE = 'label_train.npy'NB_WORDS_DATA_FILE = 'data_configs.json'## í•™ìŠµì— í•„ìš”í•œ íŒŒë¼ë©”í„°ë“¤ì— ëŒ€í•´ì„œ ì§€ì •í•˜ëŠ” ë¶€ë¶„ì´ë‹¤.## CPUì—ì„œëŠ” Epoch í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê±¸ ê¶Œì¥í•œë‹¤.BATCH_SIZE = 4096EPOCH = 50HIDDEN = 64NUM_LAYERS = 3DROPOUT_RATIO = 0.2TEST_SPLIT = 0.1RNG_SEED = 13371447EMBEDDING_DIM = 128MAX_SEQ_LEN = 31 ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë¶€ë¶„ì´ë‹¤. íš¨ê³¼ì ì¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°ë¥¼ ìœ„í•´, ë¯¸ë¦¬ ë„˜íŒŒì´ í˜•íƒœë¡œ ì €ì¥ì‹œí‚¨ ë°ì´í„°ë¥¼ ë¡œë“œí•œë‹¤. 1234567q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, 'rb'))q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, 'rb'))labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, 'rb'))prepro_configs = Nonewith open(DATA_IN_PATH + NB_WORDS_DATA_FILE, 'r') as f: prepro_configs = json.load(f) 12VOCAB_SIZE = prepro_configs['vocab_size']BUFFER_SIZE = len(labels) í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ ë°ì´í„° ë‚˜ëˆ„ê¸° ë°ì´í„°ë¥¼ ë‚˜ëˆ„ì–´ ì €ì¥í•˜ì. sklearnì˜ train_test_splitì„ ì‚¬ìš©í•˜ë©´ ìœ ìš©í•˜ë‹¤. í•˜ì§€ë§Œ, ì¿¼ë¼ ë°ì´í„°ì˜ ê²½ìš°ëŠ” ì…ë ¥ì´ 1ê°œê°€ ì•„ë‹ˆë¼ 2ê°œì´ë‹¤. ë”°ë¼ì„œ, np.stackì„ ì‚¬ìš©í•˜ì—¬ ë‘ê°œë¥¼ í•˜ë‚˜ë¡œ ìŒ“ì€ë‹¤ìŒ í™œìš©í•˜ì—¬ ë¶„ë¥˜í•œë‹¤. 12345678X = np.stack((q1_data, q2_data), axis=1)y = labelstrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)train_Q1 = train_X[:,0]train_Q2 = train_X[:,1]test_Q1 = test_X[:,0]test_Q2 = test_X[:,1] 123456789101112131415161718192021def rearrange(base, hypothesis, labels): features = &#123;\"base\": base, \"hypothesis\": hypothesis&#125; return features, labelsdef train_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y)) dataset = dataset.shuffle(buffer_size=len(train_Q1)) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.map(rearrange) dataset = dataset.repeat(EPOCH) iterator = dataset.make_one_shot_iterator() return iterator.get_next()def eval_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((test_Q1, test_Q2, test_y)) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.map(rearrange) iterator = dataset.make_one_shot_iterator() return iterator.get_next() ëª¨ë¸ ì„¤ê³„ ì–‘ë°©í–¥ LSTMì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ì¦‰, 2ê°œì˜ LSTMì„ ë¨¼ì € ì •ì˜í•´ì•¼ í•œë‹¤. ì •ë°©í–¥ LSTMì¸µê³¼ ì—­ë°©í–¥ LSTM ì¸µì„ ë¨¼ì € ì •ì˜í•  ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„  ì´ 2ê°œì˜ LSTM ì¸µì— ë°ì´í„°ë¥¼ ì ìš©í•œ í›„ ê²°ê³¼ê°’ì„ í•˜ë‚˜ë¡œ concatenateí•œë‹¤. ì–‘ë°©í–¥ ìˆœí™˜ ì‹ ê²½ë§ í•¨ìˆ˜ì˜ ê²½ìš° 2ê°œì˜ return ê°’ì´ ìˆëŠ”ë°, í•˜ë‚˜ëŠ” ìˆœí™˜ ì‹ ê²½ë§ì˜ ì¶œë ¥ ê°’ì´ê³ , ë‚˜ë¨¸ì§€ í•˜ë‚˜ëŠ” ìˆœí™˜ ì‹ ê²½ë§ ë§ˆì§€ë§‰ ìŠ¤í…ì˜ hidden state ë²¡í„° ê°’ì´ë‹¤. ì‚¬ìš©í•´ì•¼ í•  ê²ƒì€ ë§ˆì§€ë§‰ hidden state ë²¡í„°ì´ë¯€ë¡œ ê°ê° q_output_statesì™€ sim_output_statesë¡œ í• ë‹¹í•œë‹¤. ì´ë ‡ê²Œ ë½‘ì€ hidden state ë²¡í„°ì˜ ê²½ìš° í•´ë‹¹ ëª¨ë¸ì´ ì–‘ë°©í–¥ ìˆœí™˜ ì‹ ê²½ë§ì„ í™œìš©í•´ 2ê°œì˜ hidden state ê°’ì„ concatenateí•˜ì—¬ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë§Œë“ ë‹¤. ì´ëŠ” ìˆœí™˜ ì‹ ê²½ë§ì´ ë¬¸ì¥ì˜ ìˆœë°©í–¥ê³¼ ì—­ë°©í–¥ ëª¨ë‘ í•™ìŠµí•¨ìœ¼ë¡œì¨ ì„±ëŠ¥ ê°œì„ ì— ë„ì›€ì„ ì¤€ë‹¤. ë§¨í•˜íƒ„ ê±°ë¦¬ì˜ ê²½ìš° ë‘ ë²¡í„°ë¥¼ ëº€ í›„ ì ˆëŒ€ê°’ì„ ì·¨í•˜ë©´ ëœë‹¤. ì´ë ‡ê²Œ ëº€ ê°’ì˜ ê²½ìš° ë²¡í„° í˜•íƒœì´ê¸° ë•Œë¬¸ì— í•˜ë‚˜ì˜ ìƒìˆ˜, ì¦‰ scalarê°’ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ reduce_sum í•¨ìˆ˜ë¥¼ ì´ìš©í•œë‹¤. ì´ë ‡ê²Œ ë˜ë©´ êµ¬í•œ ê°’ì´ 0~1ì‚¬ì´ì˜ ê°’ì„ ê°–ê²Œ ë  ê²ƒì´ë‹¤. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879def Malstm(features, labels, mode): TRAIN = mode == tf.estimator.ModeKeys.TRAIN EVAL = mode == tf.estimator.ModeKeys.EVAL PREDICT = mode == tf.estimator.ModeKeys.PREDICT def basic_bilstm_network(inputs, name): with tf.variable_scope(name, reuse=tf.AUTO_REUSE): lstm_fw = [ tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(HIDDEN), output_keep_prob=DROPOUT_RATIO) for layer in range(NUM_LAYERS) ] lstm_bw = [ tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(HIDDEN), output_keep_prob=DROPOUT_RATIO) for layer in range(NUM_LAYERS) ] multi_lstm_fw = tf.nn.rnn_cell.MultiRNNCell(lstm_fw) multi_lstm_bw = tf.nn.rnn_cell.MultiRNNCell(lstm_bw) (fw_outputs, bw_outputs), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=multi_lstm_fw, cell_bw=multi_lstm_bw, inputs=inputs, dtype=tf.float32) outputs = tf.concat([fw_outputs, bw_outputs], 2) return outputs[:,-1,:] embedding = tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM) base_embedded_matrix = embedding(features['base']) hypothesis_embedded_matrix = embedding(features['hypothesis']) base_sementic_matrix = basic_bilstm_network(base_embedded_matrix, 'base') hypothesis_sementic_matrix = basic_bilstm_network(hypothesis_embedded_matrix, 'hypothesis') base_sementic_matrix = tf.keras.layers.Dropout(DROPOUT_RATIO)(base_sementic_matrix) hypothesis_sementic_matrix = tf.keras.layers.Dropout(DROPOUT_RATIO)(hypothesis_sementic_matrix)# merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1)# logit_layer = tf.keras.layers.dot([base_sementic_matrix, hypothesis_sementic_matrix], axes=1, normalize=True) # logit_layer = K.exp(-K.sum(K.abs(base_sementic_matrix - hypothesis_sementic_matrix), axis=1, keepdims=True)) logit_layer = tf.exp(-tf.reduce_sum(tf.abs(base_sementic_matrix - hypothesis_sementic_matrix), axis=1, keepdims=True)) logit_layer = tf.squeeze(logit_layer, axis=-1) if PREDICT: return tf.estimator.EstimatorSpec( mode=mode, predictions=&#123; 'is_duplicate':logit_layer &#125;) #prediction ì§„í–‰ ì‹œ, None if labels is not None: labels = tf.to_float(labels)# loss = tf.reduce_mean(tf.keras.metrics.binary_crossentropy(y_true=labels, y_pred=logit_layer)) loss = tf.losses.mean_squared_error(labels=labels, predictions=logit_layer)# loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(labels, logit_layer)) if EVAL: accuracy = tf.metrics.accuracy(labels, tf.round(logit_layer)) eval_metric_ops = &#123;'acc': accuracy&#125; return tf.estimator.EstimatorSpec( mode=mode, eval_metric_ops= eval_metric_ops, loss=loss) elif TRAIN: global_step = tf.train.get_global_step() train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step) return tf.estimator.EstimatorSpec( mode=mode, train_op=train_op, loss=loss) í•™ìŠµ ë° í‰ê°€12345678# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #For GPUmodel_dir = os.path.join(os.getcwd(), DATA_OUT_PATH + \"checkpoint/rnn2/\")os.makedirs(model_dir, exist_ok=True)config_tf = tf.estimator.RunConfig()lstm_est = tf.estimator.Estimator(Malstm, model_dir=model_dir) 1lstm_est.train(train_input_fn) 1lstm_est.evaluate(eval_input_fn) í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ë° ìºê¸€ ì œì¶œí•˜ê¸°1234567TEST_Q1_DATA_FILE = 'test_q1.npy'TEST_Q2_DATA_FILE = 'test_q2.npy'TEST_ID_DATA_FILE = 'test_id.npy'test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, 'rb'), allow_pickle=True)test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, 'rb'), allow_pickle=True)test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA_FILE, 'rb'), allow_pickle=True) 12345predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;\"base\":test_q1_data, \"hypothesis\":test_q2_data&#125;, shuffle=False)predictions = np.array([p['is_duplicate'] for p in lstm_est.predict(input_fn=predict_input_fn)]) 1234print(len(predictions)) #2345796output = pd.DataFrame( data=&#123;\"test_id\":test_id_data, \"is_duplicate\": list(predictions)&#125; )output.to_csv( \"rnn_predict.csv\", index=False, quoting=3 ) 1!kaggle competitions submit quora-question-pairs -f \"rnn_predict.csv\" -m \"MaLSTM Model with 5layers BiLSTM 50 Epoches\" êµ¬ì¡°ì— ëŒ€í•œ íŠœí† ë¦¬ì–¼í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ë‹¤ë³´ë‹ˆ ì„ë² ë”©ì˜ ì§ˆì´ ë–¨ì–´ì§„ë‹¤ë©´ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ì´ ë”¥ëŸ¬ë‹ ë°©ì‹ë³´ë‹¤ ê²°ê³¼ê°€ ë” ì¢‹ì„ ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ë‹¤ì‹œ í•œë²ˆ ì²´ê°í•  ìˆ˜ ìˆëŠ” ì‘ì—…ì´ì—ˆë‹¤. ì¶”í›„ì— TF-IDFí–‰ë ¬, Word2Vecê³¼ ë¬¸ì¥ ë‹¨ìœ„ LSAë¥¼ ì‹œí–‰í•´ ì–»ì€ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì‹œ í•œë²ˆ ê²°ê³¼ë¥¼ ë¹„êµí•  ê²ƒì´ë‹¤.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP ì‹¤ìŠµ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ - 01 (ë°ì´í„° EDA ë° ì „ì²˜ë¦¬)","slug":"NLP_10","date":"2020-02-09T17:34:30.000Z","updated":"2020-02-10T18:28:10.695Z","comments":true,"path":"2020/02/10/NLP_10/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/10/NLP_10/","excerpt":"","text":"í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¬¸ì œí•œ ë‘ ë¬¸ì¥(ê¸€)ì´ ìˆì„ ë•Œ ë‘ ë¬¸ì¥ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ë¬¸ì œì†Œê°œ ë°ì´í„° ì´ë¦„ : Quora Question Pairs í…ìŠ¤íŠ¸ ìš©ë„ : í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ í•™ìŠµì„ ëª©ì ìœ¼ë¡œ ì‚¬ìš© ë°ì´í„° ê¶Œí•œ : Quora ê¶Œí•œì„ ê°€ì§€ê³  ìˆìœ¼ë©° Kaggle ê°€ì… í›„ ë°ì´í„°ë¥¼ ë‚´ë ¤ë°›ìœ¼ë©´ ë¬¸ì œì—†ë‹¤. ë°ì´í„° ì¶œì²˜ : https://www.kaggle.com/c/quora-question-pairs/data ì´ë²ˆì—ë„ Kaggleì˜ ëŒ€íšŒ ì¤‘ í•˜ë‚˜ë¥¼ í•´ê²°í•´ ë³´ë ¤ê³  í•œë‹¤. â€œQuora Questions Pairsâ€ë¼ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•´ë³´ë„ë¡ í•  ê²ƒì´ë‹¤. QuoraëŠ” ì§ˆë¬¸ì„ í•˜ê³  ë‹¤ë¥¸ ì‚¬ìš©ìë“¤ë¡œë¶€í„° ë‹µë³€ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ì´ë‹¤. ì‹¤ì œë¡œ ë”¥ëŸ¬ë‹ ê³µë·°í•  ë•Œë„ Quoraì˜ ì§ˆë¬¸ë“¤ì€ ì°¸ê³ í•˜ë©´ì„œ ë§ì€ ê³µë¶€ë¥¼ í•  ìˆ˜ ìˆë‹¤. Quoraì˜ ì›” ì‚¬ìš©ìëŠ” ëŒ€ëµ 1ì–µëª… ì •ë„ ëœë‹¤. ë§¤ì¼ ìˆ˜ ë§ì€ ì§ˆë¬¸ë“¤ì´ ì‚¬ì´íŠ¸ì— ì˜¬ë¼ì˜¬ í…ë° ì´ ë§ì€ ì§ˆë¬¸ ì¤‘ì—ëŠ” ë¶„ëª…íˆ ì¤‘ë³µëœ ê²ƒë“¤ì´ í¬í•¨ë  ê²ƒì´ë‹¤. ë”°ë¼ì„œ Quora ì…ì¥ì—ì„œëŠ” ì¤‘ë³µëœ ì§ˆë¬¸ë“¤ì„ ì˜ ì°¾ê¸°ë§Œ í•œë‹¤ë©´ ì´ë¯¸ ì˜ ì‘ì„±ëœ ë‹µë³€ë“¤ì„ ì‚¬ìš©ìë“¤ì´ ì°¸ê³ í•˜ê²Œ í•  ìˆ˜ ìˆê³ , ë” ì¢‹ì€ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆê²Œ ëœë‹¤. ëª©í‘œ : ì—¬ëŸ¬ ì§ˆë¬¸ë“¤ ì¤‘ì—ì„œ ì–´ë–¤ ì§ˆë¬¸ì´ ì„œë¡œ ìœ ì‚¬í•œì§€ íŒë‹¨í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒ ìºê¸€ APIë¥¼ colabì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì¸ì¦ ë° google storageì— ì—…ë¡œë“œ ë˜ì–´ìˆëŠ” ì¸ì¦í‚¤ íŒŒì¼ í˜„ì¬ colab pwdë¡œ ë³µì‚¬í•´ì˜¨ í›„ ì„¤ì •ì™„ë£Œí•˜ê¸° 12345678from google.colab import authimport warnings%matplotlib inline%config InlineBackend.figure_format = 'retina'warnings.filterwarnings(\"ignore\")auth.authenticate_user()!gsutil cp gs://kaggle_key/kaggle.json kaggle.json 1234!mkdir -p ~/.kaggle!mv ./kaggle.json ~/.kaggle/!chmod 600 ~/.kaggle/kaggle.json!pip install kaggle ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°ì™€ ë¶„ì„í•˜ê¸° ë°ì´í„°ë¥¼ ë‚´ë ¤ë°›ëŠ” ê²ƒë¶€í„° ì‹œì‘í•  ê²ƒì´ë‹¤. í•„ìëŠ” Google colabì—ì„œ kaggle APIë¥¼ í†µí•´ ë‹¤ìš´ë¡œë“œ ë°›ì„ ê²ƒì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì€ errorê°€ ë°œìƒí•œë‹¤ë©´ kaggle API TokeníŒŒì¼ì„ ë‹¤ì‹œ ë°›ì§€ ë§ê³  ê·¸ ì „ì— ë¨¼ì € í•´ë‹¹ competitionì˜ ruleì„ ìˆ˜ë½ì„ í–ˆëŠ”ì§€ë¥¼ í™•ì´í•´ë³´ì•„ì•¼ í•œë‹¤. https://www.kaggle.com/c/quora-question-pairs/rules 1!kaggle competitions download -c quora-question-pairs í•´ë‹¹ ë°ì´í„°ê°€ ì˜ ë‹¤ìš´ë¡œë“œ ëëŠ”ì§€ í™•ì¸í•œë‹¤. í™•ì¸í•´ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì´ 3ê°€ì§€ íŒŒì¼ì´ ìˆì„ ê²ƒì´ë‹¤. sample_submission.csv.zip test.csv.zip train.csv.zip ì´ 3ê°œì˜ íŒŒì¼ì´ zip í˜•ì‹ìœ¼ë¡œ ì••ì¶•ëœ í˜•íƒœë‹¤. ì´ íŒŒì¼ë“¤ì˜ ì••ì¶•ì„ í’€ì–´ì£¼ëŠ” ê³¼ì •ê¹Œì§€ í•  ê²ƒì´ë‹¤. 123456789import zipfileDATA_IN_PATH = '/content/'zip_list=['sample_submission.csv.zip', 'test.csv.zip', 'train.csv.zip']for file in zip_list: zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r') zipRef.extractall(DATA_IN_PATH) zipRef.close() ë³¸ê²©ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ í›„ ë°ì´í„° ë¶„ì„ì„ í•´ë³´ê¸° ìœ„í•´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ëª¨ë‘ Import í•  ê²ƒì´ë‹¤. 1234567import numpy as npimport pandas as pdimport osimport matplotlib.pyplot as pltimport seaborn as snsimport pathlib as Path%matplotlib inline ê°€ì¥ ë¨¼ì € í•™ìŠµ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì–´ë–¤ í˜•íƒœë¡œ ë°ì´í„°ê°€ êµ¬ì„±ë¼ ìˆëŠ”ì§€ í™•ì¸í•´ ë³¼ ê²ƒì´ë‹¤. ë°ì´í„°ëŠ” â€˜idâ€™, â€˜qid1â€™, â€˜qid2â€™, â€˜question1â€™, â€˜question2â€™, â€˜is_duplicateâ€™ì—´ë¡œ êµ¬ì„±ë¼ ìˆë‹¤. ê°ê°ì˜ descriptionì€ ì•„ë˜ì™€ ê°™ë‹¤. id : ê° í–‰ ë°ì´í„°ì˜ ê³ ìœ í•œ index ê°’ qid1 : ì§ˆë¬¸ë“¤ì˜ ê³ ìœ í•œ index ê°’ qid2 : ì§ˆë¬¸ë“¤ì˜ ê³ ìœ í•œ index ê°’ question1 : ì§ˆë¬¸ì˜ ë‚´ìš© question2 : ì§ˆë¬¸ì˜ ë‚´ìš© is_duplicate : 0 ë˜ëŠ” 1(0ì´ë©´ ë‘ ê°œì˜ ì§ˆë¬¸ì´ ì¤‘ë³µì´ ì•„ë‹˜ì„ ì˜ë¯¸, 1ì´ë©´ ë‘ ê°œì˜ ì§ˆë¬¸ì´ ì¤‘ë³µì„ ì˜ë¯¸) 12train_data = pd.read_csv(DATA_IN_PATH + 'train.csv')train_data.head() ì‚¬ìš©í•  ë°ì´í„°ê°€ ì–´ë–¤ ë°ì´í„°ì´ê³ , í¬ê¸°ëŠ” ì–´ëŠ ì •ë„ ë˜ëŠ”ì§€ ì•Œì•„ë³´ê¸° ìœ„í•´ ë°ì´í„° íŒŒì¼ì˜ ì´ë¦„ê³¼ í¬ê¸°ë¥¼ ê°ê° ì¶œë ¥í•´ì„œ í™•ì¸í•´ ë³¼ ê²ƒì´ë‹¤. ëŒ€ë¶€ë¶„ train dataê°€ test data ë³´ë‹¤ í¬ê¸°ê°€ í°ë°, ì´ ë°ì´í„°ëŠ” test dataê°€ train data ë³´ë‹¤ 5ë°° ì •ë„ ë” í° ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. test dataê°€ í° ì´ìœ ëŠ” Quoraì˜ ê²½ìš° ì§ˆë¬¸ì— ëŒ€í•´ ë°ì´í„°ì˜ ìˆ˜ê°€ ì ë‹¤ë©´ ê°ê°ì„ ê²€ìƒ‰ì„ í†µí•´ ì¤‘ë³µì„ ì°¾ì•„ë‚´ëŠ” í¸ë³ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì´ëŸ¬í•œ í¸ë²•ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ Quoraì—ì„œ ì§ì ‘ ì»´í“¨í„°ê°€ ë§Œë“  ì§ˆë¬¸ ì‹¸ì„ test dataì— ì„ì˜ì ìœ¼ë¡œ ì¶”ê°€í–ˆê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ test dataê°€ í¬ì§€ë§Œ ì‹¤ì œ question dataëŠ” ì–¼ë§ˆ ë˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë¦¬ê³  Kaggleì˜ ê²½ìš° ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì œì¶œí•˜ë©´ ì ìˆ˜ë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ”ë°, ì»´í“¨í„°ê°€ ë§Œë“  ì§ˆë¬¸ ìŒì— ëŒ€í•œ ì˜ˆì¸¡ì€ ì ìˆ˜ì— í¬í•¨ë„ì§€ ì•ŠëŠ”ë‹¤. 1234print('íŒŒì¼ í¬ê¸°: ')for file in os.listdir(DATA_IN_PATH): if ('csv' in file) and ('zip' not in file): print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + 'MB') ê²°ê³¼1234íŒŒì¼ í¬ê¸°:test.csv 314.02MBsample_submission.csv 22.35MBtrain.csv 63.4MB ì „ì²´ ë°ì´í„°ì˜ ê°œìˆ˜ì™€ í•™ìŠµ ë°ì´í„°ì•ˆì˜ NULLê°’ì´ ì¡´ì¬í•˜ëŠ”ì§€ ë¨¼ì € í™•ì¸ í•  ê²ƒì´ë‹¤. ê²°ê³¼ë¥¼ ë³´ë©´ ì „ì²´ ì§ˆë¬¸ ìŒì˜ ê°œìˆ˜ëŠ” ëŒ€ëµ 40ë§Œê°œì´ë©° 3ê°œì˜ ë°ì´í„°ì— NULLê°’ì´ ì¡´ì¬í•œë‹¤. 1train_data.info() ê²°ê³¼1234567891011&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 404290 entries, 0 to 404289Data columns (total 6 columns):id 404290 non-null int64qid1 404290 non-null int64qid2 404290 non-null int64question1 404289 non-null objectquestion2 404288 non-null objectis_duplicate 404290 non-null int64dtypes: int64(4), object(2)memory usage: 18.5+ MB ì „ì²´ ì§ˆë¬¸(ë‘ ê°œì˜ ì§ˆë¬¸)ì„ í•œë²ˆì— ë¶„ì„í•˜ê¸° ìœ„í•´ Pandasì˜ Seriesë¥¼ í†µí•´ ë‘ ê°œì˜ ì§ˆë¬¸ì„ í•˜ë‚˜ë¡œ í•©ì¹œë‹¤. ê° ì§ˆë¬¸ì„ listë¡œ ë§Œë“  ë’¤ í•˜ë‚˜ì˜ Series ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë§Œë“ ë‹¤. ê²°ê³¼ë¥¼ ë³´ë©´ ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ë¡œ í•©ì³ì¡Œë‹¤. ê¸°ì¡´ ë°ì´í„°ì—ì„œ ì§ˆë¬¸ ìŒì˜ ìˆ˜ê°€ 40ë§Œê°œ ì •ë„ì´ê³  ê°ê° ì§ˆë¬¸ì´ 2ê°œ ì´ë¯€ë¡œ ëŒ€ëµ 80ë§Œê°œ ì •ë„ì˜ ì§ˆë¬¸ì´ ìˆë‹¤. 12train_set = pd.Series(train_data['question1'].to_list() + train_data['question2'].to_list()).astype(str)train_set.tail() ì´ì œ ì§ˆë¬¸ë“¤ì˜ ì¤‘ë³µ ì—¬ë¶€ë¥¼ í™•ì¸í•´ ë³¼ ê²ƒì´ë‹¤. Numpyì˜ uniqueí•¨ìˆ˜ë¥¼ ì´ìš©í•´ ì¤‘ë³µì„ ì œê±°í•œ ì´ ì§ˆë¬¸ì˜ ìˆ˜ì™€ ë°˜ë³µí•´ì„œ ë‚˜ì˜¤ëŠ” ì§ˆë¬¸ì˜ ìˆ˜ë¥¼ í™•ì¸í•œë‹¤. ê²°ê³¼ë¥¼ ë³´ë©´ 80ë§Œ ê°œì˜ ë°ì´í„°ì—ì„œ 537,361ê±´ì´ Uniqueí•œ ë°ì´í„°ì´ë¯€ë¡œ 262,639ê±´ì´ ì¤‘ë³µë¼ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ 262,639ê°œ ë°ì´í„°ëŠ” 131,318ê°œì˜ ë™ì¼í•œ ì§ˆë¬¸ ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.(1ìŒì€ NULLê°’ì´ê³ , 1ìŒì€ ê°’ì´ í•˜ë‚˜ë§Œ ì¡´ì¬í•˜ë¯€ë¡œ) 12print('êµìœ¡ ë°ì´í„°ì˜ ì´ ì§ˆë¬¸ ìˆ˜ : &#123;&#125; ê±´'.format(len(np.unique(train_set))))print('ë°˜ë³µí•´ì„œ ë‚˜íƒ€ë‚˜ëŠ” ì§ˆë¬¸ì˜ ìˆ˜ : &#123;&#125; ê±´'.format(np.sum(train_set.value_counts() &gt; 1))) ê²°ê³¼12êµìœ¡ ë°ì´í„°ì˜ ì´ ì§ˆë¬¸ ìˆ˜ : 537361 ê±´ë°˜ë³µí•´ì„œ ë‚˜íƒ€ë‚˜ëŠ” ì§ˆë¬¸ì˜ ìˆ˜ : 111873 ê±´ ìœ„ì˜ ê²°ê³¼ë¥¼ ì‹œê°í™” í•´ ë³¼ ê²ƒì´ë‹¤. yì¶•ì˜ ë²”ìœ„ë¥¼ ì¤„ì´ê¸° ìœ„í•´ logì„ ì‚¬ìš©í–ˆë‹¤. xê°’ì€ ì¤‘ë³µì˜ ê°œìˆ˜ì´ë©°, yê°’ì€ ë™ì¼í•œ ì¤‘ë³µ íšŸìˆ˜ë¥¼ ê°€ì§„ ì§ˆë¬¸ì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. histogramì„ ì‚´í´ë³´ë©´ ìš°ì„  ì¤‘ë³µ íšŸìˆ˜ê°€ 1ì¸ ì§ˆë¬¸ë“¤, ì¦‰ ìœ ì¼í•œ ì§ˆë¬¸ë“¤ì´ ê°€ì¥ ë§ê³  ëŒ€ë¶€ë¶„ì˜ ì§ˆë¬¸ì´ ì¤‘ë³µ íšŸìˆ˜ê°€ 50ë²ˆ ì´í•˜ì´ë‹¤. ê·¸ë¦¬ê³  ë§¤ìš° í° ë¹ˆë„ë¥¼ ê°€ì§„ ì§ˆë¬¸ì€ ì´ìƒì¹˜ê°€ ë  ê²ƒì´ë‹¤. 123456plt.figure(figsize=(12,5))plt.hist(train_set.value_counts(), bins=50, alpha=0.5, color='r', label='word')plt.yscale('log', nonposy='clip')plt.title('Log-Histogram of question appearance counts')plt.ylabel('Number of questions')plt.show() 1print('êµìœ¡ ë°ì´í„°ì˜ ì´ ì§ˆë¬¸ ìˆ˜ : &#123;&#125; ê±´'.format(len(np.unique(train_set)))) ê²°ê³¼1êµìœ¡ ë°ì´í„°ì˜ ì´ ì§ˆë¬¸ ìˆ˜ : 537361 ê±´ ì¤‘ë³µì´ ìµœëŒ€ë¡œ ë°œìƒí•œ ê°œìˆ˜ëŠ” 161ë²ˆì´ê³ , í‰ê· ì ìœ¼ë¡œ ë³´ë©´ ë¬¸ì¥ë‹¹ 1.5ê°œì˜ ì¤‘ë³µì„ ê°€ì§€ë©°, í‘œì¤€í¸ì°¨ëŠ” 1.9ë‹¤. ì¤‘ë³µì´ ë°œìƒí•˜ëŠ” íšŸìˆ˜ì˜ í‰ê· ì´ 1.5ë¼ëŠ” ê²ƒì€ ë§ì€ ë°ì´í„°ê°€ ìµœì†Œ 1ê°œ ì´ìƒ ì¤‘ë³µë¼ ìˆìŒì„ ì˜ë¯¸í•œë‹¤. ì¦‰ ì¤‘ë³µì´ ë§ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. 1train_set.value_counts().describe() ê²°ê³¼123456789count 537361.000000mean 1.504724std 1.911439min 1.00000025% 1.00000050% 1.00000075% 1.000000max 161.000000dtype: float64 ì´ì œ box plotì„ í†µí•´ ì¤‘ë³µíšŸìˆ˜ì™€ ê´€ë ¨í•´ì„œ ë°ì´í„°ë¥¼ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•´ ë³´ì. ì•„ë˜ì˜ ë¶„í¬ëŠ” ì¤‘ë³µ íšŸìˆ˜ì˜ ì´ìƒì¹˜ê°€ ë„ˆë¬´ ë„“ê³  ë§ì´ ë¶„í¬í•´ì„œ box plotì˜ ë‹¤ë¥¸ ê°’ì„ í™•ì¸í•˜ê¸°ì¡°ì°¨ ì–´ë ¤ìš´ ë°ì´í„°ì´ë‹¤. 12345plt.figure(figsize=(12, 5))plt.boxplot([train_set.value_counts()], labels=['counts'], showmeans=True)plt.show() ë°ì´í„°ì— ì–´ë–¤ ë‹¨ì–´ê°€ í¬í•¨ëëŠ”ì§€ ê°„ë‹¨íˆ ì•Œì•„ë³´ê¸° ìœ„í•´ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ì›Œë“œ í´ë¼ìš°ë“œë¡œ ê·¸ë ¤ì§„ ê²°ê³¼ë¥¼ í™•ì¸í•´ ë³´ë©´ best, way, good, difference ë“±ì˜ ë‹¨ì–´ë“¤ì´ ì§ˆë¬¸ì„ í•  ë•Œ ì¼ë°˜ì ìœ¼ë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. íŠ¹ì´í•œ ì ì€ í•´ë‹¹ ê²°ê³¼ì—ì„œ â€˜Donald Trumpâ€™ê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì´ë‹¤. â€˜Donald Trumpâ€™ê°€ ì¡´ì¬í•˜ëŠ” ì´ìœ ëŠ” ì„ ê±° ê¸°ê°„ ì¤‘ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“¤ì—ˆê¸° ë•Œë¬¸ì´ë¼ê³  ë§ì€ ìºê¸€ëŸ¬ë“¤ì´ ë§í•˜ê³  ìˆë‹¤. 123456from wordcloud import WordCloudcloud = WordCloud(width=700, height=400).generate(' '.join(train_set.astype(str)))plt.figure(figsize=(15,13))plt.imshow(cloud)plt.axis('off')plt.show() ì§ˆë¬¸ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ë°ì´í„°ì˜ ë¼ë²¨ì¸ â€˜is_duplicateâ€™ì— ëŒ€í•´ count plotì„ í†µí•´ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ë¼ë²¨ê°’ì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•´ ë³´ë©´ ì´ 40ë§Œ ê°œì˜ ë°ì´í„°ì—ì„œ ì¤‘ë³µì´ ì•„ë‹Œ ë°ì´í„°ê°€ 25ë§Œê°œ ì •ë„ì´ê³  ì¤‘ë³µëœ ë°ì´í„°ê°€ ì•½ 15ë§Œê°œ ì •ë„ë¡œ ë³´ì¸ë‹¤. ì´ ìƒíƒœë¡œ í•™ìŠµí•œë‹¤ë©´ ì¤‘ë³µì´ ì•„ë‹Œ ë°ì´í„° 25ë§Œê°œì— ì˜ì¡´ë„ê°€ ë†’ì•„ì§€ë©´ì„œ ë°ì´í„°ê°€ í•œìª½ ë¼ë²¨ë¡œ í¸í–¥ëœë‹¤. ì´ëŸ¬í•œ ê²½ìš° í•™ìŠµì´ ì›í™œí•˜ê²Œ ë˜ì§€ ì•Šì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ìµœëŒ€í•œ ë¼ë²¨ì˜ ê°œìˆ˜ë¥¼ ê· í˜• ìˆê²Œ ë§ì¶°ì¤€ í›„ ì§„í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ë§ì€ ìˆ˜ì˜ ë°ì´í„°ë¥¼ ì¤„ì¸ í›„ í•™ìŠµí•  ìˆ˜ë„ ìˆê³ , ì ì€ ìˆ˜ì˜ ë°ì´í„°ë¥¼ ëŠ˜ë¦° í›„ í•™ìŠµí•  ìˆ˜ë„ ìˆë‹¤. 1234fig, axe = plt.subplots(ncols=1)fig.set_size_inches(10, 5)sns.countplot(train_data['is_duplicate'])plt.show() í•™ìŠµ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë¶„ì„í•´ ë³¼ ê²ƒì´ë‹¤. ë¬¸ìë‹¨ìœ„ë¡œ ë¨¼ì € ê¸¸ì´ë¥¼ ë¶„ì„í•œ í›„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ì„ í•  ê²ƒì´ë‹¤. ë°ì´í„°ì˜ ê° ì§ˆë¬¸ì˜ ê¸¸ì´ ë¶„í¬ëŠ” 15 ~ 150ì— ëŒ€ë¶€ë¶„ ëª¨ì—¬ ìˆìœ¼ë©° ê¸¸ì´ê°€ 150ì—ì„œ ê¸‰ê²©í•˜ê²Œ ì£¼ì–´ë“œëŠ” ê²ƒì„ ë³¼ ë•Œ Quoraì˜ ì§ˆë¬¸ ê¸¸ì´ ì œí•œì´ 150 ì •ë„ë¼ëŠ” ê²ƒì„ ì¶”ì •í•´ ë³¼ ìˆ˜ ìˆë‹¤. ê¸¸ì´ê°€ 150 ì´ìƒì¸ ë°ì´í„°ëŠ” ê±°ì˜ ì—†ê¸° ë•Œë¬¸ì— í•´ë‹¹ ë°ì´í„° ë•Œë¬¸ì— ë¬¸ì œê°€ ë˜ì§€ëŠ” ì•Šì„ ê²ƒì´ë‹¤. 123456789train_length = train_set.apply(len)plt.figure(figsize=(15, 10))plt.hist(train_length, bins=200, range=[0, 200], facecolor='r', normed=True, label='train')plt.title('Normalized histogram of chracter count in questions', fontsize=15)plt.legend()plt.xlabel('Number of characters', fontsize=15)plt.ylabel('Probability', fontsize=15)plt.show() ê·¸ì— ë”°ë¥¸ ê¸°ì´ˆ í†µê³„ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ìœ¼ë©°, í‰ê· ì ìœ¼ë¡œ ê¸¸ì´ê°€ 60 ì •ë„ë¼ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì¤‘ì•™ê°’ì˜ ê²½ìš° 51 ì •ë„ì´ë‹¤. í•˜ì§€ë§Œ ìµœëŒ“ê°’ì„ í™•ì¸í•´ ë³´ë©´ 1169ë¡œì„œ í‰ê· , ì¤‘ì•™ê°’ì— ë¹„í•´ ë§¤ìš° í° ì°¨ì´ë¥¼ ë³´ì¸ë‹¤. ì´ëŸ° ë°ì´í„°ëŠ” ì œì™¸í•˜ê³  í•™ìŠµí•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒì´ë‹¤. 1train_length.describe() ê²°ê³¼123456789count 808580.000000mean 59.822548std 31.963751min 1.00000025% 39.00000050% 51.00000075% 72.000000max 1169.000000dtype: float64 ë°ì´í„°ì˜ ì§ˆë¬¸ ê¸¸ì´ê°’ì— ëŒ€í•´ì„œë„ box plotì„ ê·¸ë ¤ì„œ í™•ì¸í•´ ë³¼ ê²ƒì´ë‹¤. ë¶„í¬ë¥¼ ë³´ë©´ ë¬¸ì ìˆ˜ì˜ ì´ìƒì¹˜ ë°ì´í„°ê°€ ë„ˆë¬´ ë§ì´ ë¶„í¬í•´ì„œ box plotì˜ ë‹¤ë¥¸ ê°’ì„ í™•ì¸í•˜ê¸° ì¡°ì°¨ ì–´ë ¤ìš´ ìƒíƒœë‹¤. 123plt.figure(figsize=(12, 5))plt.boxplot(train_length, labels=['char counts'], showmeans=True)plt.show() ì´ì œ ë¬¸ìê°€ ì•„ë‹Œ ë‹¨ì–´ë¥¼ í•œ ë‹¨ìœ„ë¡œ ì‚¬ìš©í•´ ê¸¸ì´ê°’ì„ ë¶„ì„í•´ ë³¼ ê²ƒì´ë‹¤. í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ì€ ë‹¨ìˆœíˆ ë„ì–´ì“°ê¸°ë¡œ ì •ì˜í•  ê²ƒì´ë‹¤. histogramì„ ë³´ë©´ ëŒ€ë¶€ë¶„ 10ê°œ ì •ë„ì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ëœ ë°ì´í„°ê°€ ê°€ì¥ ë§ë‹¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. 20ê°œ ì´ìƒì˜ ë‹¨ì–´ë¡œ êµ¬ì„±ë˜ ë°ì´í„°ëŠ” ë§¤ìš° ì ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 123456789train_word_counts = train_set.apply(lambda x: len(x.split(' ')))plt.figure(figsize=(15, 10))plt.hist(train_word_counts, bins=50, range=[0, 50], color='r', label='train', normed=True)plt.title('Normalized histogram of word count in one question', fontsize=15)plt.legend()plt.xlabel('Number of Words', fontsize=15)plt.ylabel('Probability')plt.show() ê·¸ì— ë”°ë¥¸ ê¸°ì´ˆí†µê³„ëŸ‰ì„ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ë°ì´í„°ì˜ ë¬¸ì ë‹¨ìœ„ ê¸¸ì´ë¥¼ í™•ì¸í–ˆì„ ë•Œì™€ ë¹„ìŠ·í•œ ë¶„í¬ë¥¼ ê°–ëŠ”ë‹¤. í‰ê·  ê°œìˆ˜ì˜ ê²½ìš° 11ê°œì´ë©°, ì¤‘ì•™ê°’ì˜ ê²½ìš° í‰ê·  ë³´ë‹¤ 1ê°œ ì ì€ 10ê°œë¥¼ ê°–ëŠ”ë‹¤. ë¬¸ì ê¸¸ì´ì˜ ìµœëŒ“ê°’ì¸ ê²½ìš° 1100 ì •ë„ì˜ ê°’ì„ ë³´ì¸ë‹¤. ë‹¨ì–´ ê¸¸ì´ëŠ” ìµœëŒ€ 237ê°œì´ë‹¤. í•´ë‹¹ ë°ì´í„°ì˜ ê²½ìš° ì§€ë‚˜ì¹˜ê²Œ ê¸´ ë¬¸ì ê¸¸ì´ì™€ ë‹¨ì–´ ê°œìˆ˜ë¥¼ ë³´ì—¬ì¤€ë‹¤. 1train_word_counts.describe() ê²°ê³¼123456789count 808580.000000mean 11.064856std 5.889168min 1.00000025% 7.00000050% 10.00000075% 13.000000max 237.000000dtype: float64 1np.quantile(train_word_counts, 0.99) ê²°ê³¼131.0 box plotì„ í†µí•´ ë°ì´í„° ë¶„í¬ë¥¼ ë‹¤ì‹œ í•œë²ˆ í™•ì¸í•´ë³´ì. ë¬¸ì ê¸¸ì´ì— ëŒ€í•œ box plotê³¼ ë¹„ìŠ·í•œ ëª¨ì–‘ì˜ ê·¸ë˜í”„ë¥¼ ë³´ì—¬ì¤€ë‹¤. Quora ë°ì´í„°ì˜ ê²½ìš° ì´ìƒì¹˜ê°€ ë„“ê³  ë§ì´ ë¶„í¬ ë¼ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. 123plt.figure(figsize=(12, 5))plt.boxplot(train_word_counts, labels=['word counts'], showmeans=True)plt.show() ëª‡ ê°€ì§€ íŠ¹ì • ê²½ìš°ì— ëŒ€í•œ ë¹„ìœ¨ì„ í™•ì¸í•´ ë³¼ ê²ƒì´ë‹¤. íŠ¹ìˆ˜ ë¬¸ì ì¤‘ êµ¬ë‘ì , ë¬¼ìŒí‘œ, ë§ˆì¹¨í‘œê°€ ì‚¬ìš©ëœ ë¹„ìœ¨ê³¼ ìˆ˜í•™ ê¸°í˜¸ê°€ ì‚¬ìš©ëœ ë¹„ìœ¨, ëŒ€/ì†Œë¬¸ìì˜ ë¹„ìœ¨ì„ í™•ì¸í•´ ë³¸ë‹¤. ëŒ€ë¬¸ìê°€ ì²« ê¸€ìì¸ ì§ˆë¬¸ê³¼ ë¬¼ìŒí‘œë¥¼ ë™ë°˜í•˜ëŠ” ì§ˆë¬¸ì´ 99% ì´ìƒì„ ì°¨ì§€í•œë‹¤. ì „ì²´ì ìœ¼ë¡œ ì§ˆë¬¸ë“¤ì´ ë¬¼ìŒí‘œì™€ ëŒ€ë¬¸ìë¡œ ëœ ì²« ë¬¸ìë¥¼ ê°€ì§€ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¼ ì—¬ê¸°ì„œ ìƒê°í•´ ë³¼ ë¶€ë¶„ì´ ìˆë‹¤. ì¦‰, ëª¨ë“  ì§ˆë¬¸ì´ ë³´í¸ì ìœ¼ë¡œ ê°€ì§€ê³  ìˆëŠ” ì´ íŠ¹ì§•ì˜ ìœ ì§€ ì—¬ë¶€ì— ëŒ€í•´ì„œì¸ë°, ëª¨ë‘ê°€ ê°€ì§€ê³  ìˆëŠ” ë³´í¸ì ì¸ íŠ¹ì§•ì€ ì—¬ê¸°ì„œ ì œê±°í•œë‹¤. 12345678910111213qmarks = np.mean(train_set.apply(lambda x : '?' in x))math = np.mean(train_set.apply(lambda x : '[math]' in x))fullstop = np.mean(train_set.apply(lambda x : '.' in x))capital_first = np.mean(train_set.apply(lambda x : x[0].isupper()))capitals = np.mean(train_set.apply(lambda x : max([y.isupper() for y in x]))) # ëŒ€ë¬¸ìê°€ ì‚¬ìš©ëœ ì§ˆë¬¸ì´ ëª‡ ê°œì¸ì§€numbers = np.mean(train_set.apply(lambda x : max([y.isdigit() for y in x]))) # ìˆ«ìê°€ ì‚¬ìš©ëœ ì§ˆë¬¸ì´ ëª‡ ê°œì¸ì§€print('ë¬¼ìŒí‘œê°€ ìˆëŠ” ì§ˆë¬¸: &#123;:.2f&#125;%'.format(qmarks * 100))print('ìˆ˜í•™ íƒœê·¸ê°€ ìˆëŠ” ì§ˆë¬¸: &#123;:.2f&#125;%'.format(math * 100))print('ë§ˆì¹¨í‘œê°€ ìˆëŠ” ì§ˆë¬¸: &#123;:.2f&#125;%'.format(fullstop * 100))print('ì²« ê¸€ìê°€ ëŒ€ë¬¸ìì¸ ì§ˆë¬¸: &#123;:.2f&#125;%'.format(capital_first * 100))print('ëŒ€ë¬¸ìê°€ ìˆëŠ” ì§ˆë¬¸: &#123;:.2f&#125;%'.format(capitals * 100))print('ìˆ«ìê°€ ìˆëŠ” ì§ˆë¬¸: &#123;:.2f&#125;%'.format(numbers * 100)) ê²°ê³¼123456ë¬¼ìŒí‘œê°€ ìˆëŠ” ì§ˆë¬¸: 99.87%ìˆ˜í•™ íƒœê·¸ê°€ ìˆëŠ” ì§ˆë¬¸: 0.12%ë§ˆì¹¨í‘œê°€ ìˆëŠ” ì§ˆë¬¸: 6.31%ì²« ê¸€ìê°€ ëŒ€ë¬¸ìì¸ ì§ˆë¬¸: 99.81%ëŒ€ë¬¸ìê°€ ìˆëŠ” ì§ˆë¬¸: 99.95%ìˆ«ìê°€ ìˆëŠ” ì§ˆë¬¸: 11.83% ë°ì´í„° ì „ì²˜ë¦¬ ì§€ê¸ˆê¹Œì§€ ë°ì´í„° EDA(íƒìƒ‰ì  ë°ì´í„° ë¶„ì„)ë¥¼ í†µí•´ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ ë¶„í¬ë¥¼ í™•ì¸í–ˆë‹¤. ì§ˆë¬¸ ë°ì´í„°ì˜ ì¤‘ë³µ ì—¬ë¶€ ë¶„í¬, ì¦‰ ë¼ë²¨ì˜ ë¶„í¬ê°€ í¬ê²Œ ì°¨ì´ë‚˜ì„œ í•™ìŠµì— í¸í–¥ì„ ì£¼ë¯€ë¡œ ì¢‹ì§€ ì•Šì€ ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ë¶„í¬ë¥¼ ë§ì¶°ì¤„ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ëŒ€ë¶€ë¶„ì˜ ì§ˆë¬¸ì— í¬í•¨ëœ ì²« ë²ˆì§¸ ëŒ€ë¬¸ìëŠ” ì†Œë¬¸ìë¡œ í†µì¼í•œë‹¤. ë¬¼ìŒí‘œ ê°™ì€ êµ¬ë‘ì ì€ ì‚­ì œí•˜ëŠ” ì‹ìœ¼ë¡œ ë³´í¸ì ì¸ íŠ¹ì„±ì€ ì œê±°í•¨ìœ¼ë¡œì¨ í•„ìš”í•œ ë¶€ë¶„ë§Œ í•™ìŠµí•˜ê²Œ í•˜ëŠ” ì´ì ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. 12345import reimport jsonfrom tensorflow.python.keras.preprocessing.text import Tokenizerfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences 123DATA_IN_PATH = '/content/'train_data = pd.read_csv(DATA_IN_PATH + 'train.csv', encoding='utf-8') ë§¨ ë¨¼ì € ì§„í–‰í•  ì „ì²˜ë¦¬ ê³¼ì •ì€ ì•ì„œ ë¶„ì„ ê³¼ì •ì—ì„œ í™•ì¸í–ˆë˜ ë‚´ìš© ì¤‘ í•˜ë‚˜ì¸ ë¼ë²¨ ê°œìˆ˜ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ê²ƒì´ë‹¤. ì•ì„œ ë¶„ì„ ê³¼ì •ì—ì„œ í™•ì¸í–ˆë“¯ì´ ì¤‘ë³µì´ ì•„ë‹Œ ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ë”ìš± ë§ê¸° ë•Œë¬¸ì— ì´ ê²½ìš°ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜ë¥¼ ì¤„ì¸ í›„ ë¶„ì„ì„ ì§„í–‰í•˜ê² ë‹¤. ë¨¼ì € ì¤‘ë³µì¸ ê²½ìš°ì™€ ì•„ë‹Œ ê²½ìš°ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆˆ í›„ ì¤‘ë³µì´ ì•„ë‹Œ ê°œìˆ˜ê°€ ë¹„ìŠ·í•˜ë„ë¡ ë°ì´í„°ì˜ ì¼ë¶€ë¥¼ ë‹¤ì‹œ ë½‘ëŠ”ë‹¤. 1234567train_duplicate_data = train_data.loc[train_data['is_duplicate']==1]train_non_duplicate_data = train_data.loc[train_data['is_duplicate']==0]class_difference = len(train_non_duplicate_data) - len(train_duplicate_data)sample_frac = 1 - (class_difference / len(train_non_duplicate_data))train_non_duplicate_data = train_non_duplicate_data.sample(frac = sample_frac) ìƒ˜í”Œë§í•œ í›„ ë°ì´í„°ì˜ ê°œìˆ˜ê°€ ë™ì¼í•´ì¡Œë‹¤. ì´ì œ í•´ë‹¹ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ë¨„ ê· í˜• ìˆê²Œ í•™ìŠµí•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. 12print(\"ì¤‘ë³µ ì§ˆë¬¸ ê°œìˆ˜ : &#123;&#125; ê±´\".format(len(train_duplicate_data)))print(\"ì¤‘ë³µì´ ì•„ë‹Œ ì§ˆë¬¸ ê°œìˆ˜ : &#123;&#125; ê±´\".format(len(train_non_duplicate_data))) ê²°ê³¼12ì¤‘ë³µ ì§ˆë¬¸ ê°œìˆ˜ : 149263 ê±´ì¤‘ë³µì´ ì•„ë‹Œ ì§ˆë¬¸ ê°œìˆ˜ : 149263 ê±´ ìš°ì„  ë¼ë²¨ì— ë”°ë¼ ë‚˜ëˆ ì§„ ë°ì´í„°ë¥¼ ë‹¤ì‹œ í•˜ë‚˜ë¡œ í•©ì¹˜ì. 1train_data = pd.concat([train_non_duplicate_data, train_duplicate_data]) ì•ì„œ ì „ì²˜ë¦¬ì—ì„œ ë¶„ì„í•œ ëŒ€ë¡œ ë¬¸ì¥ ë¬¸ìì—´ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ ë¨¼ì € ì§„í–‰í•œë‹¤. ìš°ì„  í•™ìŠµ ë°ì´í„°ì˜ ì§ˆë¬¸ ìŒì„ í•˜ë‚˜ì˜ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ê³ , ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•´ ë¬¼ìŒí‘œì™€ ë§ˆì¹¨í‘œ ê°™ì€ êµ¬ë‘ì  ë° ê¸°í˜¸ë¥¼ ì œê±°í•˜ê³  ëª¨ë“  ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë°”ê¾¸ëŠ” ì²˜ë¦¬ë¥¼ í•œë‹¤. 1train_data.head() ë¬¼ìŒí‘œì™€ ë§ˆì¹¨í‘œ ê°™ì€ ê¸°í˜¸ì— ëŒ€í•´ ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•´ re ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œë‹¤. 123456789101112131415FILTERS = \"([~.,!?\\\"':;)(])\"change_filter = re.compile(FILTERS)questions1 = [str(s) for s in train_data['question1']]questions2 = [str(s) for s in train_data['question2']]filtered_questions1 = []filtered_questions2 = []for q in questions1: filtered_questions1.append(re.sub(change_filter, \"\", q).lower())for q in questions2: filtered_questions2.append(re.sub(change_filter, \"\", q).lower()) ì´ì œ ë‚¨ì€ ê³¼ì •ì€ ì •ì œëœ ìœ„ì˜ í…ìŠ¤íŠ¸ í…Œì´í„°ë¥¼ í† í¬ë‚˜ì´ì§•í•˜ê³  ê° ë‹¨ì–´ë¥¼ ì¸ë±ìŠ¤ë¡œ ë°”ê¾¼ í›„, ì „ì²´ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì •ì˜í•œ ìµœëŒ€ ê¸¸ì´ë³´ë‹¤ ê¸´ ë¬¸ì¥ì€ ìë¥´ê³  ì§§ì€ ë¬¸ì¥ì€ íŒ¨ë”© ì²˜ë¦¬ë¥¼ í•˜ëŠ” ê²ƒì´ë‹¤. ë¬¸ìì—´ í† í¬ë‚˜ì´ì§•ì€ tensorflow kerasì—ì„œ ì œê³µí•˜ëŠ” NLP Processing ëª¨ë“ˆì„ í™œìš©í•œë‹¤.ê°ì²´ë¥¼ ë§Œë“¤ ë•ŒëŠ” ë‘ ì§ˆë¬¸ í…ìŠ¤íŠ¸ë¥¼ í•©ì¹œ ë¦¬ìŠ¤íŠ¸ì— ì ìš©í•˜ê³ , í† í¬ë‚˜ì´ì§•ì€ í•´ë‹¹ ê°ì²´ë¥¼ í™œìš©í•´ ê° ì§ˆë¬¸ì— ëŒ€í•´ ë”°ë¡œ ì§„í–‰í•  ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì€ ë‘ ì§ˆë¬¸ì— ëŒ€í•´ ë™ì¼í•œ í† í¬ë‚˜ì´ì§• ë°©ì‹ì„ ì‚¬ìš©í•´ì•¼í•˜ë©°, ë‘ ì§ˆë¬¸ì„ í•©ì¹œ ì „ì²´ vocabularyë¥¼ ë§Œë“¤ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. í† í¬ë‚˜ì´ì§• ì´í›„ì—ëŠ” íŒ¨ë”© ì²˜ë¦¬ë¥¼ í•œ ë²¡í„°í™”ë¥¼ ì§„í–‰í•  ê²ƒì´ë‹¤. 12345tokenizer = Tokenizer()tokenizer.fit_on_texts(filtered_questions1 + filtered_questions2)questions1_sequence = tokenizer.texts_to_sequences(filtered_questions1)questions2_sequence = tokenizer.texts_to_sequences(filtered_questions2) ì´ì œ ëª¨ë¸ì— ì ìš©í•˜ê¸° ìœ„í•´ íŠ¹ì • ê¸¸ì´ë¡œ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ í•œë‹¤. ë”°ë¼ì„œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì •í•œ í›„ ê·¸ ê¸¸ì´ë³´ë‹¤ ê¸´ ì§ˆë¬¸ì€ ìë¥´ê³ , ì§§ì€ ì§ˆë¬¸ì€ ë¶€ì¡±í•œ ë¶€ë¶„ì„ 0ìœ¼ë¡œ ì±„ìš°ëŠ” íŒ¨ë”© ê³¼ì •ì„ ì§„í–‰ í•  ê²ƒì´ë‹¤. ìµœëŒ€ ê¸¸ì´ëŠ” ì•ì„œ EDAì—ì„œ í™•ì¸í–ˆë˜ ë‹¨ì–´ ê°œìˆ˜ì˜ 99%ì¸ 31ë¡œ ì„¤ì •í–ˆë‹¤. ì´ë ‡ê²Œ ì„¤ì •í•œ ì´ìœ ëŠ” ì´ìƒì¹˜ë¥¼ ëº€ ë‚˜ë¨¸ì§€ë¥¼ í¬í•¨í•˜ê¸° ìœ„í•´ì„œì´ë‹¤.(ë‹¤ì–‘í•œ ê°’ìœ¼ë¡œ ì‹¤í—˜í–ˆì„ ë•Œ ì´ ê°’ì´ ê°€ì¥ ì¢‹ì€ ê°’ì´ì—ˆë‹¤.) ì „ì²˜ë¦¬ ëª¨ë“ˆì˜ íŒ¨ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ìµœëŒ€ ê¸¸ì´ë¡œ ìë¥´ê³  ì§§ì€ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ë°ì´í„° ë’¤ì— íŒ¨ë”©ê°’ì„ ì±„ì›Œë„£ì—ˆë‹¤. 1234MAX_SEQUENCE_LENGTH = 31q1_data = pad_sequences(questions1_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')q2_data = pad_sequences(questions2_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post') ì „ì²˜ë¦¬ê°€ ëë‚œ ë°ì´í„°ë¥¼ ì €ì¥í•œë‹¤. ì €ì¥í•˜ê¸° ì „ì— ë¼ë²¨ê°’ê³¼ ë‹¨ì–´ ì‚¬ì „ì„ ì €ì¥í•˜ê¸° ìœ„í•´ ê°’ì„ ì €ì¥í•œ í›„ ê° ë°ì´í„°ì˜ í¬ê¸°ë¥¼ í™•ì¸í•´ ë³´ì. ë‘ ê°œì˜ ì§ˆë¬¸ ë¬¸ì¥ì˜ ê²½ìš° ê°ê° ê¸¸ì´ë¥¼ 31ë¡œ ì„¤ì •í–ˆê³ , vocabularyì˜ ê¸¸ì´ì¸ ì „ì²´ ë‹¨ì–´ ê°œìˆ˜ëŠ” 76,594ê°œë¡œ ë¼ ìˆë‹¤. 123456789word_vocab = &#123;&#125;word_vocab = tokenizer.word_indexlabels = np.array(train_data['is_duplicate'], dtype=int)print('Shape of question1 data : &#123;&#125;'.format(q1_data.shape))print('Shape of question2 data : &#123;&#125;'.format(q2_data.shape))print('Shape of question1 data : &#123;&#125;'.format(labels.shape))print('Words in index : &#123;&#125;'.format(len(word_vocab))) ê²°ê³¼1234Shape of question1 data : (298526, 31)Shape of question2 data : (298526, 31)Shape of question1 data : (298526,)Words in index : 76594 ë‹¨ì–´ ì‚¬ì „ê³¼ ì „ì²´ ë‹¨ì–´ì˜ ê°œìˆ˜ëŠ” dictionary í˜•íƒœë¡œ ì €ì¥í•´ ë‘˜ ê²ƒì´ë‹¤. 123data_configs = &#123;&#125;data_configs['vocab'] = word_vocabdata_configs['vocab_size'] = len(word_vocab)+1 ì´ì œ ê° ë°ì´í„°ë¥¼ ëª¨ë¸ë§ ê³¼ì •ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ì €ì¥í•˜ë©´ ëœë‹¤. 12345678910TRAIN_Q1_DATA = 'q1_train.npy'TRAIN_Q2_DATA = 'q2_train.npy'TRAIN_LABEL_DATA = 'label_train.npy'DATA_CONFIGS = 'data_configs.npy'np.save(open(DATA_IN_PATH + TRAIN_Q1_DATA, 'wb'), q1_data)np.save(open(DATA_IN_PATH + TRAIN_Q2_DATA, 'wb'), q2_data)np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), labels)json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w')) ì´ì œ í‰ê°€ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ë™ì¼í•œ ì „ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•´ì¤„ ê²ƒì´ë‹¤. 123test_data = pd.read_csv(DATA_IN_PATH + 'test.csv', encoding='utf-8')valid_ids = [type(x) == int for x in test_data.test_id]test_data = test_data[valid_ids].drop_duplicates() 1234567891011test_questions1 = [str(s) for s in test_data['question1']]test_questions2 = [str(s) for s in test_data['question2']]filtered_test_questions1 = list()filtered_test_questions2 = list()for q in test_questions1: filtered_test_questions1.append(re.sub(change_filter, \"\", q).lower())for q in test_questions2: filtered_test_questions2.append(re.sub(change_filter, \"\", q).lower()) 12345test_questions1_sequence = tokenizer.texts_to_sequences(filtered_test_questions1)test_questions2_sequence = tokenizer.texts_to_sequences(filtered_test_questions2)test_q1_data = pad_sequences(test_questions1_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')test_q2_data = pad_sequences(test_questions2_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post') 12345test_id = np.array(test_data['test_id'])print('Shape of question1 data: &#123;&#125;'.format(test_q1_data.shape))print('Shape of question2 data:&#123;&#125;'.format(test_q2_data.shape))print('Shape of ids: &#123;&#125;'.format(test_id.shape)) 1234567TEST_Q1_DATA = 'test_q1.npy'TEST_Q2_DATA = 'test_q2.npy'TEST_ID_DATA = 'test_id.npy'np.save(open(DATA_IN_PATH + TEST_Q1_DATA, 'wb'), test_q1_data)np.save(open(DATA_IN_PATH + TEST_Q2_DATA , 'wb'), test_q2_data)np.save(open(DATA_IN_PATH + TEST_ID_DATA , 'wb'), test_id)","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP ë¬¸ì¥ ìˆ˜ì¤€ ì„ë² ë”© - 02","slug":"NLP_09","date":"2020-02-07T16:02:58.000Z","updated":"2020-02-09T16:16:52.359Z","comments":true,"path":"2020/02/08/NLP_09/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/08/NLP_09/","excerpt":"","text":"ELMo(Embedding from Language Models) ë¯¸êµ­ ì—°êµ¬ê¸°ê´€ Allen Institute for Artificial Intelligenceì™€ ë¯¸êµ­ ì›Œì‹±í„´ëŒ€í•™êµ ê³µë™ì—°êµ¬íŒ€ì´ ë°œí‘œí•œ ë¬¸ì¥ ì„ë² ë”© ê¸°ë²•ì´ë‹¤. Computer vision ë¶„ì•¼ì—ì„œ ë„ë¦¬ ì“°ì´ê³  ìˆì—ˆë˜ Transfer leaningì„ ìì—°ì–´ ì²˜ë¦¬ì— ì ‘ëª©í•˜ì—¬ ì£¼ëª©ë°›ì•˜ë‹¤. Transfer learningì´ë€ ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì„ ë‹¤ë¥¸ Deep learning ëª¨ë¸ì˜ ì…ë ¥ê°’ ë˜ëŠ” ë¶€ë¶„ìœ¼ë¡œ ì¬ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì„ ì¼ì»«ëŠ”ë‹¤. ELMoê°€ ì œì•ˆëœ ì´í›„ ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œëŠ” ëª¨ë¸ì„ Pretrainí•œ í›„ ì´ë¥¼ ê°ì¢… DownStream Taskì— ì ìš©í•˜ëŠ” ì–‘ìƒì´ ì¼ë°˜í™”ëë‹¤. BERT(Bidirectional Encoder Representations from Transfomer), GPT(Generative Pre-Training)ë“±ì´ ì´ ë°©ì‹ì„ ë”°ë¥¸ë‹¤. Pretrainí•œ ëª¨ë¸ì„ downstream taskì— ë§ê²Œ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì„ Fine-tuningì´ë¼ê³  í•œë‹¤. ELMoëŠ” Language Modelì´ë‹¤. ë‹¨ì–´ sequenceê°€ ì–¼ë§ˆë‚˜ ìì—°ìŠ¤ëŸ¬ìš´ì§€ í™•ë¥ ê°’ì„ ë¶€ì—¬í•œë‹¤. ì˜ˆë¥¼ë“¤ì–´, â€˜ë°œ ì—†ëŠ” ë§ì´ ì²œë¦¬â€™ë¼ëŠ” ë‹¨ì–´ sequence ë‹¤ìŒì— â€˜ê°„ë‹¤â€™ë¼ëŠ” ë‹¨ì–´ê°€ ìì£¼ ë“±ì¥í–ˆë‹¤ë©´, ëª¨ë¸ì€ â€˜ë°œ ì—†ëŠ” ë§ì´ ì²œë¦¬â€™ë¥¼ ì¼ë ¥ë°•ì•„ â€˜ê°„ë‹¤â€™ë¥¼ ì¶œë ¥í•´ì•¼ í•œë‹¤. ELMoëŠ” í¬ê²Œ 3ê°€ì§€ ìš”ì†Œë¡œ êµ¬ì„±ë¼ ìˆë‹¤. 1) ë¬¸ì ë‹¨ìœ„ Convolution Layer ê° ë‹¨ì–´ ë‚´ ë¬¸ìë“¤ ì‚¬ì´ì˜ ì˜ë¯¸ì , ë¬¸ë²•ì  ê´€ê³„ë¥¼ ë„ì¶œí•œë‹¤. 2) ì–‘ë°©í–¥ LSTM Layer ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ì˜ë¯¸ì , ë¬¸ë²•ì  ê´€ê³„ë¥¼ ì¶”ì¶œí•´ë‚´ëŠ” ì—­í• ì„ í•œë‹¤. 3) ELMo Layer ë¬¸ì ë‹¨ìœ„ convolution Layerì™€ ì–‘ë°©ì•¼ LSTM LayerëŠ” ELMoë¥¼ Pretrainí•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤. í•˜ì§€ë§Œ ELMo LayerëŠ” Pretrainì´ ëë‚œ í›„ êµ¬ì²´ì ì¸ DownStream taskë¥¼ ìˆ˜í–‰í•˜ëŠ” ê³¼ì •ì— í•™ìŠµëœë‹¤. ë¬¸ìë‹¨ìœ„ conv layerì™€ ì–‘ë°©í–¥ LSTM layerì˜ ì¶œë ¥ë²¡í„°ë“±ì„ ê°€ì¤‘í•©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê³„ì‚°ëœë‹¤. ì´ë“¤ ê°€ì¤‘ì¹˜ë“¤ì€ downstream taskì˜ í•™ìŠµ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ë©´ì„œ í•™ìŠµëœë‹¤. ë¬¸ì ë‹¨ìœ„ Convolution Layer ELMo ì…ë ¥ì€ ë¬¸ìë‹¤ êµ¬ì²´ì ìœ¼ë¡œëŠ” ìœ ë‹ˆì½”ë“œ IDì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ corpusë¥¼ í•´ë‹¹ ë‹¨ì–´ë¥¼ ìœ ë‹ˆì½”ë“œë¡œ ë³€í™˜í•´ì•¼í•œë‹¤. í•œê¸€ ìœ ë‹ˆì½”ë“œ ë¸”ë¡ì€ UTF-8ì—ì„œ 3byteë¡œ í‘œí˜„ë˜ê¸° ë•Œë¬¸ì— ì˜ˆë¥¼ ë“¤ì–´ â€˜ë°¥â€™ì´ë¼ëŠ” ë‹¨ì–´ì˜ ìœ ë‹ˆì½”ë“œë¥¼ 10ì§„ìˆ˜ë¡œ ë°”ê¾¸ë©´ 3ê°€ì§€ ìˆ«ìê°€ ëœë‹¤. ì—¬ê¸°ì„œ ë‹¨ì–´(ë¬¸ìê°€ ì•„ë‹˜)ì˜ ì‹œì‘ê³¼ ëì„ ì•Œê²Œí•˜ê¸° ìœ„í•´ BOWì™€ EOWì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ìœ ë‹ˆì½”ë“œ ì• ë’¤ë¡œ ë¶™ì¸ë‹¤. ì´í›„ì— ë¬¸ì ì„ë² ë”© í–‰ë ¬ì—ì„œ ê°ê°ì˜ IDì— í•´ë‹¹í•˜ëŠ” í–‰ ë²¡í„°ë¥¼ ì°¸ì¡°í•´ ë¶™ì¸ë‹¤. ë¬¸ìì˜ ê¸¸ì´ê°€ ê°ê° ë‹¤ë¥´ë¯€ë¡œ ì²˜ìŒì— ë¬¸ìì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì •í•´ì£¼ë©´ ê·¸ì—ë”°ë¥¸ paddingì²˜ë¦¬ë¥¼ í•´ì¤€ë‹¤. Convolution filterì˜ í¬ê¸°ëŠ” (ê°™ì´ ë³´ê³  ì‹¶ì€ ë¬¸ìê¸¸ì´) $\\times$ (ë¬¸ì ì„ë² ë”©ì˜ ì°¨ì› ìˆ˜)ê°€ ëœë‹¤. ì´ë¥¼í†µí•´ í”¼ì²˜ë§µì„ ì–»ê³  ì—¬ê¸°ì„œ max poolingì„ í•´ì£¼ì–´ ê²°ê³¼ë¥¼ ë‚¸ë‹¤. ìœ„ì—ì„œ ê°™ì´ ë³´ê³  ì‹¶ì€ ë¬¸ìê¸¸ì´ë¥¼ ì¡°ì •í•´ ê°€ë©´ì„œ ì—¬ëŸ¬ê°œì˜ filterë¥¼ ì‚¬ìš©í•´ ì–»ì€ í’€ë§ ë²¡í„°ë“¤ì„ concatenateí•œ ë’¤ highway networkì™€ projection(ì°¨ì› ì¡°ì •)ì„ í•œë‹¤. ì–‘ë°©í–¥ LSTM, ìŠ¤ì½”ì–´ ë ˆì´ì–´ ë¬¸ì ë‹¨ìœ„ convolution layerê°€ ë°˜í™˜ í•œ ë‹¨ì–´ë²¡í„° sequence(ë§¨ í•˜ë‹¨ì˜ ë³´ë¼ìƒ‰ ë²¡í„°ë“¤)ì—ì„œ ì‹œì‘ê³¼ ëì„ ì•Œì´ëŠ” , í† í°ì„ ì• ë’¤ë¡œ ë¶™ì¸ ë’¤ í•™ìŠµ ì‹œí‚¨ë‹¤. ìˆœë°©í–¥ LSTM layerì™€ ì—­ë°©í–¥ LSTM layerì— ëª¨ë‘ ìœ„ì˜ ë²¡í„° sequenceë“¤ì„ ì…ë ¥í•˜ëŠ”ë° ê°ê° nê°œì˜ LSTM layerë¥¼ êµ¬ì„±í•œë‹¤. ELMo ê¸°ë³¸ ëª¨ë¸ì€ n=2ë¡œ ì„¤ì •í•˜ê³  ìˆë‹¤. ELMoì—ëŠ” LSTM layerì— residual connection êµ¬ì¡°ë¥¼ ì ìš©ì‹œì¼œ ì¼ë¶€ ê³„ì‚° ë…¸ë“œë¥¼ ìƒëµí•˜ê²Œí•˜ì—¬ íš¨ìœ¨ì ì¸ Gradient ì „íŒŒë¥¼ ë•ëŠ”ë‹¤. í”„ë¦¬íŠ¸ë ˆì¸ ë‹¨ê³„ì—ì„œ Word2vecì—ì„œ ì‚¬ìš©ë˜ì—ˆë˜ negative sampling ê¸°ë²•ì´ ì‚¬ìš©ëœë‹¤. ELMo ë ˆì´ì–´ ELMo ì…ë² ë”©ì€ Pretrainì´ ëë‚˜ê³  êµ¬ì²´ì ì¸ DownStream taskë¥¼ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ë„ì¶œë˜ë©°, ê° layerë³„ hidden stateë¥¼ ê°€ì¤‘í•©í•œ ê²°ê³¼ì´ë‹¤. ì„ì˜ì˜ taskë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë¬¸ì¥ kë²ˆì§¸ Tokenì˜ ELMo ì„ë² ë”©ì˜ êµ¬ì²´ì ì¸ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. $h_{k,j}^{LM}$ : kë²ˆì§¸ Tokenì˜ jë²ˆì§¸ layerì˜ ìˆœë°©í–¥, ì—­ë°©í–¥ LSTM hidden stateë¥¼ concatenateí•œ ë²¡í„°ë¥¼ ì˜ë¯¸í•œë‹¤. $s_{j}^{task}$ : jë²ˆì§¸ layerê°€ í•´ë‹¹ task ìˆ˜í–‰ì— ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ë¥¼ ì˜ë¯¸í•˜ëŠ” scalarê°’ì´ë‹¤. downstream taskë¥¼ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ lossë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ì„ ì—…ë°ì´íŠ¸í•œë‹¤. $\\gamma^{task}$ : ELMo ë²¡í„°ì˜ í¬ê¸°ë¥¼ scalingí•˜ì—¬ í•´ë‹¹ task ìˆ˜í–‰ì„ ë•ëŠ” ì—­í• ì„ í•œë‹¤. L : ì–‘ë°©í–¥ LSTM layer ê°œìˆ˜(ë³´í†µ 2ë¡œ ì„¤ì •í•¨) j=0 -&gt; ë¬¸ì ë‹¨ìœ„ convolution Layer j=1 -&gt; ì–‘ë°©í–¥ LSTM layerì˜ ì²«ë²ˆì§¸ ì¶œë ¥ j=2 -&gt; ì–‘ë°©í–¥ LSTM layerì˜ ë‘ë²ˆì§¸ ì¶œë ¥ ELMo_{k}^{task} = \\gamma^{task} \\sum^{L}_{j=0} s_{j}^{task} h_{k,j}^{LM}íŠ¸ëœìŠ¤í¬ë©” ë„¤íŠ¸ì›Œí¬ íŠ¸ëœìŠ¤í¬ë¨¸ ë„¤íŠ¸ì›Œí¬ëŠ” êµ¬ê¸€ ì—°êµ¬ íŒ€ì´ NIPSì— ê³µê°œí•œ ë”¥ëŸ¬ë‹ ì•„í‚¤í…ì²˜ë‹¤. ë›°ì–´ë‚œ ì„±ëŠ¥ìœ¼ë¡œ ì£¼ëª©ë°›ì•˜ë‹¤. ì´í›„ ë°œí‘œëœ GPT, BERT ë“± ê¸°ë²•ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì„ ê¸°ë³¸ ëª¨ë¸ë¡œ ì“°ê³  ìˆë‹¤. í¬ê²Œ ë‘ê°€ì§€ ì‘ë™ì›ë¦¬ë¡œ ë‚˜ëˆŒìˆ˜ ìˆë‹¤. Multi-head Attentionê³¼ feedforward Networkì´ë‹¤. Scaled Dot-Product Attention Scaled Dot-Product Attentionì˜ ì…ë ¥(x)ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í–‰ë ¬ í˜•íƒœë¥¼ ê°€ì§€ë©° ê·¸ í¬ê¸°ëŠ” ì…ë ¥ ë¬¸ì¥ì˜ ë‹¨ì–´ìˆ˜ $\\times$ ì…ë ¥ ì„ë² ë”©ì˜ ì°¨ì› ìˆ˜ì´ë‹¤. íŠ¸ëœìŠ¤ í¬ì–´ë¯œ Scaled Dot-Product Attention ë§¤ì»¤ë‹ˆì¦˜ì€ query, key, value 3ê°€ì§€ ì‚¬ì´ì˜ ê´€ê³„ê°€ í•µì‹¬ì´ë‹¤. ì…ë ¥í–‰ë ¬ Xì™€ Query, Key, Valueì— ë”°ë¥´ëŠ” ê°€ì¤‘ì¹˜ í–‰ë ¬($W^{q}, W^{k}, W^{v}$)ì„ ê°ê° ê³±í•´ ê³„ì‚°í•œë‹¤. ì´í›„ queryì™€ keyê°€ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ ë‘ ë²¡í„°ê°„ ë‚´ì ì„ êµ¬í•´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•œë‹¤. ì´ë¥¼ í†µí•´ ì–´ë–¤ queryì™€ keyê°€ íŠ¹ì • task ìˆ˜í–‰ì— ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆë‹¤ë©´ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì€ ì´ë“¤ ì‚¬ì´ì˜ ë‚´ì ê°’ì„ í‚¤ìš°ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•œë‹¤. ì•„ë˜ ì‹ì—ì„œ ì œê³±ê·¼ ìŠ¤ì¼€ì¼ì„ í•˜ëŠ” ì´ìœ ëŠ” query-key ë‚´ì  í–‰ë ¬ì˜ ë¶„ì‚°ì„ ì¤„ì´ê²Œ ë¼ softmax í•¨ìˆ˜ì˜ gradientê°€ ì§€ë‚˜ì¹˜ê²Œ ì‘ì•„ì§€ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. softmax ë…¸ë“œì˜ gradientëŠ” softmax í™•ë¥  ë²¡í„° yì˜ ê°œë³„ ìš”ì†Œ ê°’ì— ì•„ì£¼ ë¯¼ê°í•˜ê¸° ë•Œë¬¸ì— softmax í™•ë¥  ë²¡í„°ì˜ ì¼ë¶€ ê°’ì´ ì§€ë‚˜ì¹˜ê²Œ ì‘ë‹¤ë©´ gradient vanishing ë¬¸ì œê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‹¤. Scaled Dot-Product AttentionAttention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt(d_{k})}) \\cdot Vì†Œí”„íŠ¸ë§¥ìŠ¤ ë…¸ë“œì˜ gradient\\frac{\\delta y_{i}}{\\delta x_{i}} = y_{i}(1-y_{i})\\frac{\\delta y_{i}}{\\delta x_{j}} = -y_{i}y_{j} ì•„ë˜ ê·¸ë¦¼ì€ Scaled Dot-Product Attention ê¸°ë²•ìœ¼ë¡œ Query, Key, Value ì‚¬ì´ì˜ ê´€ê³„ë“¤ì´ ë†ì¶•ëœ ìƒˆë¡œìš´ Zë¥¼ ë§Œë“œëŠ” ì˜ˆì‹œì´ë‹¤. íŒŒë€ìƒ‰ ì„ ìœ¼ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ í–‰ë ¬ì€ Query, Key ë‚´ì ì„ $\\sqrt(d_{k})$ë¡œ ë‚˜ëˆˆ ë’¤ softmax í•¨ìˆ˜ë¥¼ ì·¨í•œ ê²°ê³¼ì´ë‹¤. ì´ í–‰ë ¬ì˜ í–‰ì€ Query ë‹¨ì–´ë“¤ì— ëŒ€ì‘í•˜ë©°, ì—´ì€ Key ë‹¨ì–´ë“¤ì— ëŒ€ì‘í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ Queryì™€ Keyê°’ì´ ë™ì¼í•œ attentionì„ self-attentionì´ë¼ê³  í•œë‹¤. ì´ëŠ” ê°™ì€ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ ìŒ ì‚¬ì´ì˜ ì˜ë¯¸ì , ë¬¸ë²•ì  ê´€ê³„ë¥¼ í¬ì°©í•´ë‚¸ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. softmaxë¥¼ ì·¨í•œ ê²°ê³¼ëŠ” í™•ë¥ ì´ ëœë‹¤. ë”°ë¼ì„œ ê° í–‰ì˜ í•©ì€ 1ì´ë‹¤. &#39;ë“œë””ì–´-ê¸ˆìš”ì¼&#39;ê°’ì´ ê°€ì¥ ë†’ì•„ ë²¡í„° ê³µê°„ìƒì—ì„œë„ ê°€ê¹Œì´ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ê³  ë‘ ë‹¨ì–´ì‚¬ì´ì˜ ê´€ê³„ê°€ task ìˆ˜í–‰(ë²ˆì—­, ë¶„ë¥˜ë“±)ì— ì¤‘ìš”í•˜ë‹¤ëŠ” ì´ì•¼ê¸°ì´ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œëŠ” softmax í™•ë¥ ì„ ê°€ì¤‘ì¹˜ ì‚¼ì•„ ê° ê°’ ë²¡í„°ë“¤ì„ ê°€ì¤‘í•©í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. ìƒˆë¡­ê²Œ ë§Œë“¤ì–´ì§„ &#39;ë“œë””ì–´&#39;ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ëŠ” í•´ë‹¹ ë¬¸ì¥ ë‚´ ë‹¨ì–´ ìŒê°„ ê´€ê³„ê°€ ëª¨ë‘ ë†ì¶•ëœ ê²°ê³¼ì´ë‹¤. self-attentionì€ RNN, CNNë³´ë‹¤ ì¥ì ì´ ë§ë‹¤. CNNì˜ ê²½ìš° ì‚¬ìš©ìê°€ ì§€ì •í•œ windowë‚´ì˜ contextë§Œ ì‚´í”¼ê¸° ë•Œë¬¸ì— ë¬¸ì¥ì´ ê¸¸ê³  ì²˜ìŒ ë‹¨ì–´ì™€ ë§ˆì§€ë§‰ ë‹¨ì–´ ì‚¬ì´ì˜ ì—°ê´€ì„± íŒŒì•…ì´ task ìˆ˜í–‰ì— ì¤‘ìš”í•œ ë°ì´í„°ë¼ë©´ í•´ê²°í•˜ê¸° ì–´ë µë‹¤. RNNì€ sequenceì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§ˆìˆ˜ë¡ gradient vanishingì´ ì¼ì–´ë‚˜ê¸° ì‰½ê¸° ë•Œë¬¸ì— ì²˜ìŒ ì…ë ¥ë°›ì•˜ë˜ ë‹¨ì–´ë¥¼ ê¸°ì–µí•˜ê¸° ì‰½ì§€ ì•Šë‹¤. í•˜ì§€ë§Œ self-attentionì€ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ìŒ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëŠ˜ ì „ì²´ì ìœ¼ë¡œ íŒŒì•Œí•  ìˆ˜ ìˆë‹¤. Multi-Head Attention Scaled Dot-Product Attentionì„ ì—¬ëŸ¬ ë²ˆ ì‹œí–‰í•˜ëŠ” ê²ƒì„ ê°€ë¦¬í‚¨ë‹¤. ë™ì¼í•œ ë¬¸ì¥ì„ ì—¬ëŸ¬ ëª…ì˜ ë…ìê°€ ë™ì‹œì— ë¶„ì„í•´ ìµœì„ ì˜ ê²°ê³¼ë¥¼ ë‚´ë ¤ê³  í•˜ëŠ” ê²ƒì— ë¹„ìœ í•  ìˆ˜ ìˆë‹¤. Multi-Head attentionì˜ ê³„ì‚° ê³¼ì •ì€ ì•„ë˜ì˜ ìˆ˜ì‹ê³¼ ê·¸ë¦¼ê³¼ ê°™ì´ ì´ë£¨ì–´ì§„ë‹¤. Query, Key, Valueë¥¼ Scaled Dot-Productë¥¼ í†µí•´ ì–»ì€ Attention Valueë¥¼ concatenateí•œë‹¤. ê·¸ í›„ ì—¬ê¸°ì— $W^{0}$ë¥¼ ë‚´ì í•´ Multi-Head Attention ìˆ˜í–‰ ê²°ê³¼ í–‰ë ¬ì˜ í¬ê¸°ë¥¼ Scaled Dot-Product Attentionì˜ ì…ë ¥ í–‰ë ¬ê³¼ ë™ì¼í•˜ê²Œ ë§ì¶˜ë‹¤. Multi-Head Attention ìˆ˜ì‹MultiHead(Q, K, V) = Concat(head_{1}, \\cdots, head_{h})W^{0}head_{i} = Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i}) Position-wise Feedforward Networks Multi-Head Attention Layerì˜ ì…ë ¥ í–‰ë ¬ê³¼ ì¶œë ¥ í–‰ë ¬ì˜ í¬ê¸°ëŠ” ì…ë ¥ ë‹¨ì–´ ìˆ˜ $\\times$ íˆë“  ë²¡í„° ì°¨ì› ìˆ˜ë¡œ ë™ì¼í•˜ë‹¤. Position-wise Feedforward Networks Layerì—ì„œëŠ” Multi-Head Attention Layerì˜ ì¶œë ¥ í–‰ë ¬ì„ í–‰ ë²¡í„° ë‹¨ìœ„ë¡œ, ë‹¤ì‹œ ë§í•´ ë‹¨ì–´ ë²¡í„° ê°ê°ì— ê´€í•´ ì•„ë˜ì˜ ìˆ˜ì‹ì„ ì ìš©í•œë‹¤. Multi-Head Layerì˜ ì¶œë ¥ í–‰ë ¬ ê°€ìš´ë° í•˜ë‚˜ì˜ ë‹¨ì–´ ë²¡í„°ë¥¼ xë¼ê³  í•˜ì. ì´ xì— ê´€í•´ ë‘ ë²ˆì˜ ì„ í˜•ë³€í™˜ì„ í•˜ëŠ”ë° ê·¸ ì‚¬ì´ì— activationì„ í•´ì„œ ì ìš©í•œë‹¤. Position-wise Feedforward Networks ìˆ˜ì‹FFN(x) = max(0, x \\cdot W_{1} + b_{1})W_{2} + b_{2}íŠ¸ëœìŠ¤í¬ë¨¸ì˜ í•™ìŠµ ì „ëµ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ í•™ìŠµ ì „ëµì€ warm upì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì‚¬ìš©ìê°€ ì •í•œ stepìˆ˜ì— ì´ë¥´ê¸° ê¹Œì§€ learning rateë¥¼ ë†’ì˜€ë‹¤ê°€ step ìˆ˜ë¥¼ ë§Œì¡±í•˜ë©´ ì¡°ê¸ˆì”© ë–¨ì–´ë„ë¦¬ëŠ” ë°©ì‹ì´ë‹¤. ëŒ€ê·œëª¨ ë°ì´í„°, í° ëª¨ë¸ í•™ìŠµì— ì í•©í•˜ë‹¤. ì´ ì „ëµì€ BERT ë“± ì´í›„ ì œì•ˆëœ ëª¨ë¸ì—ë„ ë„ë¦¬ ì“°ì´ê³  ìˆë‹¤. ì´ë°–ì— Layer Normalization ë“±ë„ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì•ˆì •ì ì¸ í•™ìŠµì— ê¸°ì—¬í•˜ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ì¢€ë” ìì„¸í•œ ì‚¬í•­ì„ ì•Œê³  ì‹¶ë‹¤ë©´ ì—¬ê¸°ë¥¼ ëˆŒëŸ¬ ê³µë¶€í•´ë³´ì. BERT(Bidirectional Encoder Representations from Transformer) BERTëŠ” êµ¬ê¸€ì—ì„œ ê³µê°œí•œ ëª¨ë¸ì´ë‹¤. ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ ë„ë¦¬ ì“°ì´ê³  ìˆë‹¤. BERT, ELMo, GPT BERTì˜ ì„±ê³µ ë¹„ê²°ì€ ê·¸ performanceê°€ ê²€ì¦ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì„ ì¼ì„ë¿ë”ëŸ¬ ëª¨ë¸ì˜ ì†ì„±ì´ ì–‘ë°©í–¥ì„ ì§€í–¥í•œë‹¤ëŠ” ì ì— ìˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ BERT ì´ì „ì˜ ëª¨ë¸ì¸ GPT(Generative Pre-Training)ì™€ ELMo ëª¨ë¸ê³¼ì˜ ì°¨ì´ì ì„ ì‹œê°í™”í•œ ê²ƒì´ë‹¤. GPTëŠ” ë‹¨ì–´ sequenceë¥¼ ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ ë°©í–¥ìœ¼ë¡œë§Œ ë³´ëŠ” ì•„í‚¤í…ì³ì´ë‹¤. ELMoëŠ” Bi-LSTM Layerì˜ ìƒë‹¨ì€ ì–‘ë°©í–¥ì´ì§€ë§Œ ì¤‘ê°„ LayerëŠ” ì—­ì‹œ í•œ ë°©í–¥ì¸ ëª¨ë¸ì´ë‹¤. ë°˜ë©´ BERTì˜ ê²½ìš° ëª¨ë“  Layerì—ì„œ ì–‘ë°©í–¥ ì„±ì§ˆì„ ìƒì§€ ì•Šê³  ìˆë‹¤. BERTì™€ GPT ëª¨ë¸ì€ ëª¨ë‘ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤. GPTëŠ” ì£¼ì–´ì§„ ë‹¨ì–´ sequenceë¥¼ ê°€ì§€ê³  ê·¸ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµí•˜ëŠ” Language Modelì´ê¸° ë•Œë¬¸ì— ì…ë ¥ ë‹¨ì–´ ì´í›„ì˜ ë‹¨ì–´ë¥¼ ëª¨ë¸ì—ê²Œ ì•Œë ¤ì£¼ëŠ” ê²ƒì„ í•˜ì§€ ëª»í•œë‹¤. ë”°ë¼ì„œ ì•„ë˜ ê·¸ë¦¼ ì¤‘ 1ë²ˆì— ì†í•œë‹¤. ì´ ë¬¸ì œë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ Masked Language Modelì´ ì œì•ˆë˜ì—ˆë‹¤. ì£¼ì–´ì§„ sequence ë‹¤ìŒ ë‹¤ìŒë¥¼ ë§ì¶”ëŠ” ê²ƒì—ì„œ ë²—ì–´ë‚˜, ì¼ë‹¨ ë¬¸ì¥ ì „ì²´ë¥¼ ëª¨ë¸ì— ì•Œë ¤ì£¼ê³ , Maskingì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ê°€ ì–´ë–¤ ë‹¨ì–´ì¼ì§€ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµí•´ë³´ìëŠ” ì•„ì´ë””ì–´ì´ë‹¤. ì´ëŠ” ì•„ë˜ ê·¸ë¦¼ ì¤‘ 2ë²ˆì— ì†í•œë‹¤. Masked Language Model Taskì—ì„œëŠ” ëª¨ë¸ì— ë¬¸ì¥ ì „ì²´ë¥¼ ë‹¤ ì£¼ì–´ë„ ë°˜ì¹™ì´ ë  ìˆ˜ ì—†ë‹¤. BERT ëª¨ë¸ì€ ë¹ˆì¹¸ì„ ì±„ì›Œì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì€ GPTê°€ Scaled Dot-Product Attentionì„ í•˜ëŠ” ê³¼ì •ì„ ë„ì‹í™”í•œ ê²ƒì´ë‹¤. ì˜ˆì¸¡í•´ì•¼ í•  ë‹¨ì–´ë¥¼ ë³´ì§€ ì•Šê¸° ìœ„í•´ softmax score í–‰ë ¬ì˜ ì¼ë¶€ ê°’ì„ 0ìœ¼ë¡œ ë§Œë“ ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì…ë ¥ ë¬¸ì¥ì´ â€˜ëœ¨ëˆí•œ, êµ­ë°¥, í•œ ê·¸ë¦‡â€™ì´ê³  ì´ë²ˆì— ì˜ˆì¸¡í•´ì•¼ í•  ë‹¨ì–´ê°€ â€˜êµ­ë°¥â€™ì´ë¼ë©´ GPTëŠ” ì´ì „ ë‹¨ì–´ì¸ â€˜ëœ¨ëˆí•œâ€™ë§Œ ì°¸ê³ í•´ì•¼ í•œë‹¤. ë°˜ë©´ BERTëŠ” ë¹ˆì¹¸ë§Œ ë§ì¶”ë©´ ë˜ê¸° ë•Œë¬¸ì— ë¬¸ì¥ ë‚´ ë‹¨ì–´ ìŒ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëª¨ë‘ ë³¼ ìˆ˜ ìˆë‹¤. BERT ì„ë² ë”©ì„ ê°ì¢… Downstream Taskì— ì ìš©í•´ ì‹¤í—˜í•œ ê²°ê³¼ BERTì˜ ì„ë² ë”© í’ˆì§ˆì´ GPTë³´ë‹¤ ì¢‹ìŒì„ ì…ì¦í–ˆë‹¤. ë˜í•œ ê°™ì€ BERT ëª¨ë¸ì´ë”ë¼ë„ Pre-Trainì„ í•  ë•Œ í•œ ë°©í–¥(Left-to-Right)ë§Œ ë³´ê²Œ í•  ê²½ìš° ê·¸ ì„±ëŠ¥ì´ ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„ í¬ê²Œ ê°ì†Œí•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ê·¸ë§Œí¼ ëª¨ë¸ì´ ì–‘ë°©í–¥ ì „í›„ contextë¥¼ ëª¨ë‘ ë³´ê²Œ í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ì´ì•¼ê¸°ì´ë‹¤. Pre-Train Taskì™€ í•™ìŠµ ë°ì´í„° êµ¬ì¶• BERTì˜ Pre-Train Taskì—ëŠ” í¬ê²Œ Masked Language Modelê³¼ ë‹¤ìŒ ë¬¸ì¥ì¸ì§€ ì—¬ë¶€ ë§ì¶”ê¸°(NSP, Next Sentence Prediction)ë¡œ ë˜ì–´ìˆëŠ”ë°, ì´ ë‘ê°€ì§€ë¡œ ì¸í•´ BERTê°€ ì–‘ë°©í–¥ ëª¨ë¸ì´ ë  ìˆ˜ ìˆì—ˆë‹¤. Masked Language Model Task ìˆ˜í–‰ì„ ìœ„í•œ í•™ìŠµ ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë§Œë“ ë‹¤. 1) í•™ìŠµ ë°ì´í„° í•œ ë¬¸ì¥ Tokenì˜ 15%ë¥¼ Maskingí•œë‹¤. 2) Masking ëŒ€ìƒ Token ê°€ìš´ë° 80%ëŠ” ì‹¤ì œ ë¹ˆì¹¸ìœ¼ë¡œ ë§Œë“¤ê³ , ëª¨ë¸ì€ ê·¸ ë¹ˆì¹¸ì„ ì±„ìš´ë‹¤. ex) ëœ¨ëˆí•œ êµ­ë°¥ [Mask] í•˜ëŠ”ê²Œ ë‚«ì§€ -&gt; í•œ ê·¸ë¦‡ 3) Masking ëŒ€ìƒ Token ê°€ìš´ë° 10%ëŠ” ëœë¤ìœ¼ë¡œ ë‹¤ë¥¸ Tokenìœ¼ë¡œ ëŒ€ì²´í•˜ê³ , ëª¨ë¸ì€ í•´ë‹¹ ìœ„ì¹˜ì˜ ì •ë‹µ ë‹¨ì–´ê°€ ë¬´ì—‡ì¼ì§€ ë§ì¶”ë„ë¡ í•œë‹¤. ex) ëœ¨ëˆí•œ êµ­ë°¥ [í•œ ê°œ] í•˜ëŠ”ê²Œ ë‚«ì§€ -&gt; í•œ ê·¸ë¦‡ 4) Masking ëŒ€ìƒ Token ê°€ìš´ë° 10%ëŠ” Tokenì„ ê·¸ëŒ€ë¡œ ë‘ê³ , ëª¨ë¸ì€ í•´ë‹¹ ìœ„ì¹˜ì˜ ì •ë‹µ ë‹¨ì–´ê°€ ë¬´ì—‡ì¼ì§€ ë§ì¶”ë„ë¡ í•œë‹¤. ex) ëœ¨ëˆí•œ êµ­ë°¥ [í•œ ê·¸ë¦‡] í•˜ëŠ”ê²Œ ë‚«ì§€ -&gt; í•œ ê·¸ë¦‡ ìœ„ì™€ ê°™ì´ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“¤ê²Œ ë˜ë©´ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì ë“¤ì„ ê¸°ëŒ€í•˜ê²Œ ëœë‹¤. â€˜ëœ¨ëˆí•œ êµ­ë°¥ [Mask] í•˜ëŠ”ê²Œ ë‚«ì§€â€™ì˜ ë¹ˆì¹¸ì„ ì±„ì›Œì•¼ í•˜ê¸° ë•Œë¬¸ì— ë¬¸ì¥ ë‚´ ì–´ëŠ ìë¦¬ì— ì–´ë–¤ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ”ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ì§€ ì•ë’¤ ë¬¸ë§¥ì„ ì½ì–´ë‚¼ ìˆ˜ ìˆê²Œ ëœë‹¤. â€˜ëœ¨ëˆí•œ êµ­ë°¥ í•œ ê·¸ë¦‡ í•˜ëŠ”ê²Œ ë‚«ì§€â€™, â€˜ëœ¨ëˆí•œ êµ­ë°¥ í•œ ê°œ í•˜ëŠ”ê²Œ ë‚«ì§€â€™ë¥¼ ë¹„êµí•´ ë³´ë©´ì„œ ì£¼ì–´ì§„ ë¬¸ì¥ì´ ì˜ë¯¸/ë¬¸ë²•ìƒ ë¹„ë¬¸ì¸ì§€ ì•„ë‹Œì§€ ë¶„ë³„í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ëª¨ë¸ì€ ì–´ë–¤ ë‹¨ì–´ê°€ Masking ë ì§€ ì „í˜€ ëª¨ë¥´ê¸° ë•Œë¬¸ì— ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ ì‚¬ì´ì˜ ì˜ë¯¸ì , ë¬¸ë²•ì  ê´€ê³„ë¥¼ ì„¸ë°€íˆ ì‚´í”¼ê²Œ ëœë‹¤. ë‹¤ìŒ ë¬¸ìì¸ì§€ ì—¬ë¶€(NSP)ë¥¼ ë§ì¶”ê¸° ìœ„í•œ í•™ìŠµ ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë§Œë“ ë‹¤. 1) ëª¨ë“  í•™ìŠµ ë°ì´í„°ëŠ” 1ê±´ë‹¹ ë¬¸ì¥ 2 ê°œë¡œ êµ¬ì„±ëœë‹¤. 2) ì´ ê°€ìš´ë° ì ˆë°˜ì€ ë™ì¼í•œ ë¬¸ì„œì—ì„œ ì‹¤ì œ ì´ì–´ì§€ëŠ” ë¬¸ì¥ì„ 2 ê°œ ë½‘ê³ , ê·¸ ì •ë‹µìœ¼ë¡œ Trueë¥¼ ë¶€ì—¬í•œë‹¤. 3) ë‚˜ë¨¸ì§€ ì ˆë°˜ì€ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ì„œì—ì„œ ë¬¸ì¥ 1ê°œì”© ë½‘ê³ , ê·¸ ì •ë‹µìœ¼ë¡œ Falseë¥¼ ë¶€ì—¬í•œë‹¤. 4) max_num_tokensë¥¼ ì •ì˜í•œë‹¤. í•™ìŠµ ë°ì´í„°ì˜ 90%ëŠ” max_num_tokensê°€ ì‚¬ìš©ìê°€ ì •í•œ max_sequence_lengthê°€ ë˜ë„ë¡ í•œë‹¤. ë‚˜ë¨¸ì§€ 10%ëŠ” max_num_tokensê°€ max_sequence_lengthë³´ë‹¤ ì§§ê²Œ ë˜ë„ë¡ ëœë¤ìœ¼ë¡œ ì •í•œë‹¤. 5) ì´ì „ì— ë½‘ì€ ë¬¸ì¥ 2 ê°œì˜ ë‹¨ì–´ ì´ ìˆ˜ê°€ max_num_tokenì„ ë„˜ì§€ ëª»í•  ë•Œê¹Œì§€ ë‘ ë¬¸ì¥ ì¤‘ ë‹¨ì–´ ìˆ˜ê°€ ë§ì€ ìª½ì„ 50%ì˜ í™•ë¥ ë¡œ ë¬¸ì¥ ë§¨ ì• ë˜ëŠ” ë§¨ ë’¤ ë‹¨ì–´ í•˜ë‚˜ì”© ì œê±°í•œë‹¤. ì´ê°™ì´ í•™ìŠµ ë°ì´í„°ë¥¼ ë§Œë“¤ë©´ ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì ë“¤ì„ ê¸°ëŒ€í•˜ê²Œ ëœë‹¤. ëª¨ë¸ì€ â€˜ë‚´ì¼ì€ ë¹„ê°€ ì˜¬ ê²ƒì´ë‹¤â€™, â€˜ìš°ì‚°ì„ ì±™ê²¨ì•¼ í•  ê²ƒ ê°™ë‹¤â€™ê°€ ì´ì–´ì§„ ë¬¸ì¥ì¸ì§€ ì•„ë‹Œì§€ ë°˜ë³µ í•™ìŠµí•œë‹¤. ë”°ë¼ì„œ ë¬¸ì¥ ê°„ ì˜ë¯¸ ê´€ê³„ë¥¼ ì´í•´í•  ìˆ˜ ìˆë‹¤. NSP Taskê°€ ë„ˆë¬´ ì‰¬ì›Œì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë¬¸ì¥ ë§¨ ì• ë˜ëŠ” ë§¨ ë’¤ìª½ ë‹¨ì–´ ì¼ë¶€ë¥¼ ì‚­ì œí–ˆê¸° ë•Œë¬¸ì— ì¼ë¶€ ë¬¸ì¥ ì„±ë¶„ì´ ì—†ì–´ë„ ì „ì²´ ì˜ë¯¸ë¥¼ ì´í•´í•˜ëŠ” ë° í° ë¬´ë¦¬ê°€ ì—†ë‹¤. í•™ìŠµ ë°ì´í„°ì˜ 10%ëŠ” ì‚¬ìš©ìê°€ ì •í•œ ìµœëŒ€ ê¸¸ì´(max_sequence_length)ë³´ë‹¤ ì§§ì€ ë°ì´í„°ë¡œ êµ¬ì„±ë¼ ìˆê¸° ë•Œë¬¸ì— í•™ìŠµ ë°ì´í„°ì— ì§§ì€ ë¬¸ì¥ì´ í¬í•¨ë¼ ìˆì–´ë„ ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì§€ì§€ ì•ŠëŠ”ë‹¤. BERT ëª¨ë¸ì˜ ë¬¸ì¥ êµ¬ì¡° BERT ëª¨ë¸ì€ íŠ¸ëœìŠ¤í¬ë¨¸ Encoderë¥¼ ì¼ë¶€ ë³€í˜•í•œ ì•„í‚¤í…ì³ì´ë‹¤. ì°¸ê³ ë¡œ GPTëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ Decoder êµ¬ì¡°ë¥¼ ì¼ë¶€ ë³€í˜•í•œ ì•„í‚¤í…ì³ì´ë‹¤. Original íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ì°¨ì´ì ì„ ìœ„ì£¼ë¡œ ì„¤ëª…í•  ê²ƒì´ë‹¤. BERT ëª¨ë¸ì€ ë¬¸ì¥ì˜ ì‹œì‘ì„ ì•Œë¦¬ëŠ” [CLS], ë¬¸ì¥ì˜ ì¢…ê²°ì„ ì˜ë¯¸í•˜ëŠ” [SEP], ë§ˆìŠ¤í¬ Token [MASK], ë°°ì¹˜ ë°ì´í„°ì˜ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ê¸° ìœ„í•œ [PAD] ë“±ì˜ 4ê°€ì§€ ìŠ¤í˜ì…œ Tokenì„ ì‚¬ìš©í•œë‹¤. BERT ëª¨ë¸ì˜ ì…ë ¥ Layerì„ ì‹œê°í™”í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. ìš°ì„  ì…ë ¥ Tokenì— í•´ë‹¹í•˜ëŠ” Token ë²¡í„°ë¥¼ ì°¸ì¡°í•´ Token ì„ë² ë”©ì„ ë§Œë“ ë‹¤. ì—¬ê¸°ì— ì²«ë²ˆì§¸ ë¬¸ì¥ì¸ì§€, ë‘ ë²ˆì§¸ ë¬¸ì¥ì¸ì§€ì— í•´ë‹¹í•˜ëŠ” segment ì„ë² ë”©ì„ ì°¸ì¡°í•´ ë”í•´ì¤€ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ì…ë ¥ Tokenì˜ ë¬¸ì¥ ë‚´ ì ˆëŒ€ì ì¸ ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” Position Embeddingì„ ë” í•œë‹¤. ì´ë ‡ê²Œ 3ê°œ ì„ë² ë”©ì„ ë”í•œ ê°ê°ì˜ ë²¡í„°ì— Layer Normalizationì„ í•˜ê³  Dropoutì„ ì‹œí–‰í•´ ì²« ë²ˆì§¸ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì˜ ì…ë ¥ í–‰ë ¬ì„ êµ¬ì„±í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì²˜ëŸ¼ Token ìˆ˜ê°€ 11ê°œì¸ ë¬¸ì¥ì´ë¼ë©´ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì˜ ì…ë ¥í–‰ë ¬ì˜ í¬ê¸°ëŠ” 11$\\times$ Hidden ì°¨ì›ìˆ˜ê°€ ëœë‹¤. Token, Segment, Position ë²¡í„°ë¥¼ ë§Œë“¤ ë•Œ ì°¸ì¡°í•˜ëŠ” í–‰ë ¬ì€ Pre-Train Task ìˆ˜í–‰ì„ ì˜í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë‹¤ë¥¸ í•™ìŠµ parameterì™€ í•¨ê»˜ ì—…ë°ì´íŠ¸ëœë‹¤. BERTê°€ ì‚¬ìš©í•˜ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì—ì„œ Original íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ê°€ì¥ í° ì°¨ì´ì ì„ ë³´ì´ëŠ” ëŒ€ëª©ì€ Position-wise Feedforward Networks ë¶€ë¶„ì´ë‹¤. Activation functionì„ ê¸°ì¡´ì˜ ReLU ëŒ€ì‹  GELU(Gaussian Error Linear Units)ë¥¼ ì‚¬ìš©í•œë‹¤. ì •ê·œë¶„í¬ì˜ ëˆ„ì ë¶„í¬í•¨ìˆ˜(cumulative distribution functions)ì¸ GELUëŠ” ReLUë³´ë‹¤ 0 ì£¼ìœ„ì—ì„œ ë¶€ë“œëŸ½ê²Œ ë³€í™”í•´ í•™ìŠµ ì„±ëŠ¥ì„ ë†’ì¸ë‹¤. Original íŠ¸ëœìŠ¤í¬ë¨¸ì™€ BERTê°€ ê°€ì¥ í¬ê²Œ ì°¨ì´ë¥¼ ë³´ì´ëŠ” ë˜ í•˜ë‚˜ì˜ ë¶€ë¶„ì€ ë§ˆì§€ë§‰ Prediction Layerì´ë‹¤. Mask Language Model, NSPë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. Mask Language Modelê³¼ ê´€ë ¨ëœ LayerëŠ” ì‹¤ì œ ëª¨ë¸ì—ì„œ Pre-Trainì´ ëë‚˜ë©´ ê·¸ ì—­í• ì„ ë‹¤í•˜ê³  ì œê±°ë˜ì–´ Transfer learningì˜ Pine Tuningí•  ê²½ìš°ì—ëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤. Mask Language Model Layerì˜ ì…ë ¥ì€ ë§ˆì§€ë§‰ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì˜ Mask ìœ„ì¹˜ì— í•´ë‹¹í•˜ëŠ” Token ë²¡í„°ì´ë‹¤. ì˜ˆë¥¼ ë“¤ë©´, BERT ëª¨ë¸ì˜ ì…ë ¥ ë¬¸ì¥ì´ â€˜ë„ˆ ì˜¤ëŠ˜ [MASK] ëª‡ì‹œì— í•  êº¼ì•¼â€™ì´ê³ , ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ Tokenì„ ë‚˜ëˆˆë‹¤ë©´ 3ë²ˆì§¸ ë²¡í„°ê°€ Input_tensorê°€ ëœë‹¤. ì´ ë²¡í„°ë¥¼ ì…ë ¥ ë‹¹ì‹œì™€ ë™ì¼í•œ ì°¨ì› ìˆ˜ë¡œ ì„ í˜•ë³€í™˜ì„ í•œ ë’¤ Layer Normalizationì„ ì‹œí–‰í•œë‹¤. ì´í›„ Vocabulary ìˆ˜ ë§Œí¼ìœ¼ë¡œ Projectioní•˜ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ ë²¡í„°ì¸ output_weightsë¥¼ ê³±í•˜ê³  output_biasë¥¼ ë”í•´ logit ë²¡í„°ë¥¼ ë§Œë“ ë‹¤. ì—¬ê¸°ì„œ ì£¼ëª©í•  ì ì€ ì…ë ¥ Layerì—ì„œ Token ë²¡í„°ë¥¼ ë§Œë“¤ ë•Œ ì°¸ì¡°í•˜ëŠ” í–‰ë ¬ì„ output_weightsë¡œ ì¬ì‚¬ìš©í•œë‹¤ëŠ” ì ì´ë‹¤. BERT-base ë‹¤êµ­ì–´ ëª¨ë¸ì˜ ë‹¨ì–´ ìˆ˜ê°€ 10ë§Œ ê°œ ì•ˆíŒì¸ ì ì„ ê³ ë ¤í•˜ë©´ ê³„ì‚°, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ëª¨ë‘ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ì „ëµì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. NSP Layerì˜ ì…ë ¥ì€ ë§ˆì§€ë§‰ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì˜ ì²« ë²ˆì§¸ Token([[CLS]])ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ì´ë‹¤. ì´ ë²¡í„°ë¥¼ 2ì°¨ì›ìœ¼ë¡œ projectioní•˜ëŠ” ê°€ì¤‘ì¹˜ í–‰ë ¬ output_weightsë¥¼ ê³±í•˜ê³ , ì—¬ê¸°ì— 2ì°¨ì› í¬ê¸°ì˜ ë°”ì´ì–´ìŠ¤ ë²¡í„°ë¥¼ ë”í•œ ë’¤ softmaxí•¨ìˆ˜ë¥¼ ì·¨í•œë‹¤. ì´ í™•ë¥  ë²¡í„°ì™€ ì •ë‹µ(True or False)ê³¼ ë¹„êµí•´ CrossEntropyë¥¼ êµ¬í•˜ê³  ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ Parameterë“¤ì„ ì—…ë°ì´íŠ¸í•œë‹¤. Pre-Train Tutorial BERT ëª¨ë¸ì„ Pre-Trainí•˜ë ¤ë©´ GPUê°€ ì—¬ëŸ¬ ê°œ ìˆì–´ì•¼ í•œë‹¤. GPU 8ê°œë¥¼ ì¼ì„ ë•Œ 12ê°œ Layer í¬ê¸°ì˜ ê¸°ë³¸ ëª¨ë¸(BERT-base)ë¥¼ Pre-Train í•˜ëŠ”ë° 10~15ì¼ ì •ë„ ì†Œìš”ëœë‹¤. ë¦¬ì†ŒìŠ¤ê°€ ë§ì§€ ì•Šì€ ë¶„ë“¤ì€ ì´ë¯¸ ê³µê°œë¼ ìˆëŠ” BERT Pre-Train ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤. ìì—°ì–´ ì²˜ë¦¬ ì—°êµ¬ì ì˜¤ì—°íƒ ë‹˜ê»˜ì„œ í•œêµ­ì–´ BERT Pre-Train ëª¨ë¸ì„ ê³µê°œí–ˆë‹¤. Pre-Train ê³¼ì • ë° hyper parameter ì„¸íŒ… ë“± ìì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ì—ì„œ í™•ì¸ í•´ë³¼ ìˆ˜ ìˆë‹¤.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP ë¬¸ì¥ ìˆ˜ì¤€ ì„ë² ë”© - 01","slug":"NLP_08","date":"2020-02-05T15:32:51.000Z","updated":"2020-02-07T19:58:36.698Z","comments":true,"path":"2020/02/06/NLP_08/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/06/NLP_08/","excerpt":"","text":"ì°¸ê³ ë¡œ ì´ ëª¨ë“  ë‚´ìš©ì€ ì´ê¸°ì°½ ë‹˜ì˜ í•œêµ­ì–´ ì„ë² ë”©ì´ë¼ëŠ” ì±…ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ê³  ìˆë‹¤.ë¬¸ì¥ ìˆ˜ì¤€ ì„ë² ë”© í¬ê²ŒëŠ” í–‰ë ¬ ë¶„í•´, í™•ë¥  ëª¨í˜•, Neural Network ê¸°ë°˜ ëª¨ë¸ ë“± ì„¸ ì¢…ë¥˜ë¥¼ ì†Œê°œí•  ê²ƒì´ë‹¤. í–‰ë ¬ ë¶„í•´ LSA(ì ì¬ ì˜ë¯¸ ë¶„ì„) í™•ë¥  ëª¨í˜• LDA(ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹) Neural Network Doc2Vec ELMo GPT (transformer êµ¬ì¡° - self-attention) BERT (transformer êµ¬ì¡° - self-attention) ì ì¬ ì˜ë¯¸ ë¶„ì„(LSA, Latent Semantic Analysis) ë‹¨ì–´ ìˆ˜ì¤€ ì„ë² ë”©ì—ì„œì˜ LSA ë°©ë²•ë¡ ë“¤ì€ word-documents í–‰ë ¬ì´ë‚˜ TF-IDF í–‰ë ¬, word-context í–‰ë ¬ ë˜ëŠ” PMI í–‰ë ¬ì— SVDë¡œ ì°¨ì› ì¶•ì†Œë¥¼ ì‹œí–‰í•˜ê³ , ì—¬ê¸°ì—ì„œ ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ë¥¼ ì·¨í•´ ì„ë² ë”©ì„ ë§Œë“œëŠ” ë°©ë²•ì´ì—ˆë‹¤. ë¬¸ì¥ ìˆ˜ì¤€ ì…ë² ë”©ì—ì„œì˜ LSA ë°©ë²•ì€ ë‹¨ì–´ ìˆ˜ì¤€ ì„ë² ë”©ì—ì„œì˜ LSA ë°©ë²•ë¡ ì„ í†µí•´ ì–»ê²Œëœ ì •í™•íˆ ë§í•˜ìë©´ SVDë¥¼ í†µí•´ ì¶•ì†Œëœ í–‰ë ¬ì—ì„œ ë¬¸ì„œì— ëŒ€ì‘í•˜ëŠ” ë²¡í„°ë¥¼ ì·¨í•´ ë¬¸ì„œ ì„ë² ë”©ì„ ë§Œë“œëŠ” ë°©ì‹ì´ë‹¤. ì‹¤ìŠµ ëŒ€ìƒ ë°ì´í„°ëŠ” ratsgo.github.uoì˜ ì•„í‹°í´ í•˜ë‚˜ë¡œ markdwon ë¬¸ì„œì˜ ì œëª©ê³¼ ë³¸ë¬¸ì„ ê·¸ëŒ€ë¡œ í…ìŠ¤íŠ¸ë¡œ ì €ì¥í•œ í˜•íƒœì´ë‹¤. 1ê°œ ë¼ì¸ì´ 1ê°œ ë¬¸ì„œì— í•´ë‹¹í•œë‹¤. ë¶ˆí•„ìš”í•œ ê¸°í˜¸ë‚˜ LaTex math íŒ¨ê¸°ì§€ì˜ ë¬¸ë²•ìœ¼ë¡œ ì‘ì„±ë˜ì–´ìˆëŠ” ë¶€ë¶„ë“¤ì´ ë‹¤ìˆ˜ ì¡´ì¬í•œë‹¤. ìš°ì„  ì´ ì‹¤ìŠµì˜ ê°€ì •ì„ ìˆ˜ì‹ì´ë‚˜ ê¸°í˜¸ëŠ” ë¶„ì„ì— ìˆì–´ì„œ í° ì˜ë¯¸ë¥¼ ê°–ì§€ ì•ŠëŠ”ë‹¤ë¼ê³  ê°€ì •í•˜ê³  ì‹œì‘í•˜ê² ë‹¤. ìš°ì„ , í˜•íƒœì†Œë¶„ì„ê¸°ë¥¼ ì–´ë–¤ê²ƒì„ ì‚¬ìš©í•˜ë˜ ê°€ëŠ¥í•˜ê²Œ í•¨ìˆ˜ë¥¼ í•˜ë‚˜ ë§Œë“¤ì–´ì¤€ë‹¤. 1234567891011121314151617181920from khaiii import KhaiiiApifrom konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkmadef get_tokenizer(tokenizer_name): if tokenizer_name == \"komoran\": tokenizer = Komoran() elif tokenizer_name == \"okt\": tokenizer = Okt() elif tokenizer_name == \"mecab\": tokenizer = Mecab() elif tokenizer_name == \"hannanum\": tokenizer = Hannanum() elif tokenizer_name == \"kkma\": tokenizer = Kkma() elif tokenizer_name == \"khaiii\": tokenizer = KhaiiiApi() else: tokenizer = Mecab() return tokenizer í•œ ë¬¸ë‹¨ ë³„ë¡œ êµ¬ë¶„ìë¥¼ ì–´ë–¤ê²ƒìœ¼ë¡œ í–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° í•˜ë‚˜ì”© í”„ë¦°íŠ¸í•´ë³´ì•˜ë‹¤. 12345678910corpus_fname = \"./data/processed/processed_blog.txt\"with open(corpus_fname, 'r', encoding='utf-8') as f: print(f.readline()) print(\"---------------------------------------------------------------------------------------------------------------------\") print(f.readline()) print(\"---------------------------------------------------------------------------------------------------------------------\") print(f.readline()) print(\"---------------------------------------------------------------------------------------------------------------------\") print(f.readline()) ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ì œì¼ ì²˜ìŒ ë¬¸ì„œì˜ ì„ë² ë”©ê³¼ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ë¬¸ì„œ ì„ë² ë”©ì˜ ì œëª©ì„ returní•´ì¤€ë‹¤. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.decomposition import TruncatedSVDfrom sklearn.preprocessing import normalizefrom sklearn.manifold import TSNEfrom sklearn.metrics.pairwise import cosine_similarityfrom bokeh.io import export_png, output_notebook, showfrom bokeh.plotting import figurefrom bokeh.models import Plot, Range1d, MultiLine, Circle, HoverTool, TapTool, BoxSelectTool, LinearColorMapper, ColumnDataSource, LabelSet, SaveTool, ColorBar, BasicTickerfrom bokeh.models.graphs import from_networkx, NodesAndLinkedEdges, EdgesAndLinkedNodesfrom bokeh.palettes import Spectral8def LSAeval(corpus_file, doc_idx, nth_top): tokenizer = get_tokenizer(\"mecab\") titles, raw_corpus, noun_corpus = [], [], [] with open(corpus_fname, 'r', encoding='utf-8') as f: for line in f: try: title, document = line.strip().split('\\u241E') titles.append(title) raw_corpus.append(document) nouns = tokenizer.nouns(document) noun_corpus.append(' '.join(nouns)) except: continue # ë¬¸ì„œ(ë‹¨ë½)ì—ì„œ ê¸°í˜¸ë“¤ê³¼ ì¡°ì‚¬ë¥¼ ì œì™¸í•˜ê³  ëª…ì‚¬ë“¤ë§Œ ì¶”ì¶œí•œ ë°ì´í„° ì¤‘ Unigram(ngram_range(1,1)), # DFê°€ 1ì´ìƒ(min_df=1)ì¸ ë°ì´í„°ë¥¼ ì¶”ë ¤ TF-IDF í–‰ë ¬ì„ ë§Œë“¤ ê²ƒì´ë‹¤. vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,1), # tokenizingì „ì— ëª¨ë“  ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë°”ê¿”ì¤€ë‹¤. lowercase=True, # analyzer == 'word'ì¸ ê²½ìš°ë§Œ ì‚¬ìš©ê°€ëŠ¥. tokenizer=lambda x: x.split()) # í–‰ì€ ë¬¸ì„œ, ì—´ì€ ë‹¨ì–´ì— ê°ê° ëŒ€ì‘í•œë‹¤. (204 x 37153) input_matrix = vectorizer.fit_transform(noun_corpus) id2vocab = &#123;vectorizer.vocabulary_[token]:token for token in vectorizer.vocabulary_.keys()&#125; # curr_doc : Corpus ì²«ë²ˆ ì§¸ ë¬¸ì„œì˜ TF-IDF ë²¡í„° curr_doc, result = input_matrix[doc_idx], [] # curr_docì—ì„œ TF-IDF ê°’ì´ 0ì´ ì•„ë‹Œ ìš”ì†Œë“¤ì€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ # curr_docì€ 105ê°œì˜ ì›ì†Œ(ë‹¨ì–´)ë§Œì´ ì €ì¥ë˜ì–´ ìˆëŠ” Compressed Sparse Row formatì´ë‹¤. # ê·¸ëŸ¬ë¯€ë¡œ indices(CSR format index array of the matrix)ë¡œ í•´ë‹¹ indexì— ìœ„ì¹˜í•˜ëŠ” ë‹¨ì–´ì™€ ê·¸ì—ëŒ€í•œ tf-idfê°’ì„ ìŒìœ¼ë¡œ tupleí˜•íƒœë¡œ ë„£ì–´ì¤€ë‹¤. for idx, el in zip(curr_doc.indices, curr_doc.data): result.append((id2vocab[idx], el)) sorted(result, key=lambda x : x[1], reverse=True)[:5] # ì´ë²ˆì—ëŠ” ì´ TF-IDF í–‰ë ¬ì— 100ì°¨ì› SVDë¥¼ ìˆ˜í–‰í•  ê²ƒì´ë‹¤. 204 x 37153ì˜ í¬ì†Œ í–‰ë ¬ì„ # 204 x 100 í¬ê¸°ì˜ Dense Matrixë¡œ linear Transforamtioní•˜ëŠ” ê²ƒì´ë‹¤. svd = TruncatedSVD(n_components=100) vecs = svd.fit_transform(input_matrix) svd_l2norm_vectors = normalize(vecs, axis=1, norm='l2') cosine_similarity = np.dot(svd_l2norm_vectors, svd_l2norm_vectors[doc_idx]) query_sentence = titles[doc_idx] return titles, svd_l2norm_vectors, [query_sentence, sorted(zip(titles, cosine_similarity), key=lambda x: x[1], reverse=True)[1:nth_top + 1]] ìƒìœ„ 5ê°œì˜ ë²¡í„°ì˜ ë‚´ì ì´ ë†’ì€ ìˆœìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¥í–ˆì„ë•Œì˜ ê²°ê³¼ë¬¼ ì¶œë ¥ 12titles, svd_l2norm_vectors, top_five = LSAeval(corpus_file=\"./data/processed/processed_blog.txt\", doc_idx=0, nth_top=5)top_five ì„ë² ë”© ì‹œê°í™” t-SNE ê¸°ë²•ì„ ì‚¬ìš©í•´ì„œ ë²¡í„°ê³µê°„ì„ 2ì°¨ì›ìœ¼ë¡œ ì¤„ì—¬ì¤€ ë’¤ ì‹œê°í™” í•  ê²ƒì´ë‹¤. ë˜í•œ ë²¡í„°ë“¤ê°„ì˜ ì „ì²´ì ì¸ ìœ ì‚¬ë„ëŠ” ì‹œê°ì ìœ¼ë¡œ ê·¸ë¦¬ê¸°ë³´ë‹¤ëŠ” ìƒê´€í–‰ë ¬ ë°©ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ ì¤„ ê²ƒì´ë‹¤. ì‹œê°í™”ì— í•„ìš”í•œ í•¨ìˆ˜ë“¤ ì •ì˜ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172def visualize(titles, vectors, mode=\"between\", num_sents=30, palette=\"Viridis256\", use_notebook=False): doc_idxes = random.sample(range(len(titles)), num_sents) sentences = [titles[idx] for idx in doc_idxes] vecs = [vectors[idx] for idx in doc_idxes] if mode == \"between\": visualize_between_sentences(sentences, vecs, palette, use_notebook=use_notebook) else: visualize_sentences(vecs, sentences, palette, use_notebook=use_notebook)def visualize_between_sentences(sentences, vec_list, palette=\"Viridis256\", filename=\"between-sentences.png\", use_notebook=False): df_list, score_list = [], [] for sent1_idx, sentence1 in enumerate(sentences): for sent2_idx, sentence2 in enumerate(sentences): vec1, vec2 = vec_list[sent1_idx], vec_list[sent2_idx] if np.any(vec1) and np.any(vec2): score = cosine_similarity(X=[vec1], Y=[vec2]) # [0][0]ì¸ ì´ìœ ëŠ” ê°’ë§Œ ë½‘ì•„ ë‚´ê¸° ìœ„í•´ì„œì´ë‹¤. df_list.append(&#123;'x': sentence1, 'y': sentence2, 'similarity': score[0][0]&#125;) score_list.append(score[0][0]) df = pd.DataFrame(df_list) color_mapper = LinearColorMapper(palette=palette, low=np.max(score_list), high=np.min(score_list)) TOOLS = \"hover,save,pan,box_zoom,reset,wheel_zoom\" p = figure(x_range=sentences, y_range=list(reversed(sentences)), x_axis_location=\"above\", plot_width=900, plot_height=900, toolbar_location='below', tools=TOOLS, tooltips=[('sentences', '@x @y'), ('similarity', '@similarity')]) p.grid.grid_line_color = None p.axis.axis_line_color = None p.axis.major_tick_line_color = None p.axis.major_label_standoff = 0 p.xaxis.major_label_orientation = 3.14 / 3 p.rect(x=\"x\", y=\"y\", width=1, height=1, source=df, fill_color=&#123;'field': 'similarity', 'transform': color_mapper&#125;, line_color=None) color_bar = ColorBar(ticker=BasicTicker(desired_num_ticks=5), color_mapper=color_mapper, major_label_text_font_size=\"7pt\", label_standoff=6, border_line_color=None, location=(0, 0)) p.add_layout(color_bar, 'right') if use_notebook: output_notebook() show(p) else: export_png(p, filename) print(\"save @ \" + filename)def visualize_sentences(vecs, sentences, palette=\"Viridis256\", filename=\"/notebooks/embedding/sentences.png\", use_notebook=False): tsne = TSNE(n_components=2) tsne_results = tsne.fit_transform(vecs) df = pd.DataFrame(columns=['x', 'y', 'sentence']) df['x'], df['y'], df['sentence'] = tsne_results[:, 0], tsne_results[:, 1], sentences source = ColumnDataSource(ColumnDataSource.from_df(df)) labels = LabelSet(x=\"x\", y=\"y\", text=\"sentence\", y_offset=8, text_font_size=\"12pt\", text_color=\"#555555\", source=source, text_align='center') color_mapper = LinearColorMapper(palette=palette, low=min(tsne_results[:, 1]), high=max(tsne_results[:, 1])) plot = figure(plot_width=900, plot_height=900) plot.scatter(\"x\", \"y\", size=12, source=source, color=&#123;'field': 'y', 'transform': color_mapper&#125;, line_color=None, fill_alpha=0.8) plot.add_layout(labels) if use_notebook: output_notebook() show(plot) else: export_png(plot, filename) print(\"save @ \" + filename) í˜¹ì‹œ ì´ëŸ¬í•œ errorê°€ ë‚œë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ PhantomJSë¥¼ ì„¤ì¹˜í•œë‹¤. ê°„ë‹¨íˆ ë§í•˜ìë©´ PhantomJSë„ Seleniumê°™ì´ ì›¹ë¸Œë¼ìš°ì ¸ ê°œë°œìš©ìœ¼ë¡œ ë§Œë“¤ì–´ì§„ í”„ë¡œê·¸ë¨ì´ë‹¤. bokehëŠ” javascriptê¸°ë°˜ìœ¼ë¡œ ì§œì—¬ì ¸ìˆì–´ì„œ í•„ìš”í•œ ê²ƒ ê°™ë‹¤. 1conda install -c conda-forge phantomjs 1visualize(titles, svd_l2norm_vectors, mode=\"between\", num_sents=30, palette=\"Viridis256\", use_notebook=True) 1visualize(titles, svd_l2norm_vectors, mode=\"tsne\", num_sents=30, palette=\"Viridis256\", use_notebook=True) Doc2Vecëª¨ë¸ ê°œìš” Word2Vecì— ì´ì–´ êµ¬ê¸€ ì—°êµ¬ íŒ€ì´ ê°œë°œí•œ ë¬¸ì„œ ì„ë² ë”© ê¸°ë²•ì´ë‹¤. ì´ì „ ë‹¨ì–´ sequence kê°œê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê·¸ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë§ì¶”ëŠ” ì–¸ì–´ ëª¨ë¸ì„ ë§Œë“¤ì—ˆë‹¤. ì´ ëª¨ë¸ì€ ë¬¸ì¥ ì „ì²´ë¥¼ ì²˜ìŒë¶€í„° ëê¹Œì§€ í•œ ë‹¨ì–´ì”© ìŠ¬ë¼ì´ë”©í•´ ê°€ë©´ì„œ ë‹¤ìŒ ë‹¨ì–´ê°€ ë¬´ì—‡ì¼ì§€ ì˜ˆì¸¡í•œë‹¤. ë¡œê·¸ í™•ë¥  í‰ê· ì˜ ê°’ì´ ì»¤ì§„ë‹¤ëŠ” ì˜ë¯¸ëŠ” ì´ì „ kê°œ ë‹¨ì–´ë“¤ì„ ì…ë ¥í•˜ë©´ ëª¨ë¸ì´ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ ì˜ˆì¸¡í•˜ë¯€ë¡œ ë¡œê·¸ í™•ë¥  í‰ê· ì„ ìµœëŒ€í™”ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤. NPLMì—ì„œ ì„¤ëª…í–ˆë˜ ë°©ì‹ì²˜ëŸ¼ ë¬¸ì¥ ì „ì²´ë¥¼ í•œ ë‹¨ì–´ì”© ìŠ¬ë¼ì´ë”©í•´ê°€ë©´ì„œ ë‹¤ìŒ target wordë¥¼ ë§ì¶”ëŠ” ê³¼ì •ì—ì„œ context wordì— í•´ë‹¹í•˜ëŠ” $(w_{t-k}, \\cdots, w_{t-1})$ì— í•´ë‹¹í•˜ëŠ” W í–‰ë ¬ì˜ ë²¡í„°ë“¤ì´ ì—…ë°ì´íŠ¸ í•œë‹¤. ë”°ë¼ì„œ ì£¼ë³€ ì´ì›ƒ ë‹¨ì–´ ì§‘í•© ì¦‰ contextê°€ ìœ ì‚¬í•œ ë‹¨ì–´ë²¡í„°ëŠ” ë²¡í„° ê³µê°„ì— ê°€ê¹ê²Œ ì„ë² ë”© ëœë‹¤.í•™ìŠµì´ ì¢…ë£Œë˜ë©´ Wë¥¼ ê° ë‹¨ì–´ì˜ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. Doc2vec ì–¸ì–´ ëª¨ë¸ $T$ : í•™ìŠµ ë°ì´í„° ë¬¸ì¥ í•˜ë‚˜ì˜ ë‹¨ì–´ ê°œìˆ˜ $w_{t}$ : ë¬¸ì¥ì˜ të²ˆì§¸ ë‹¨ì–´ $y_{i}$ : corpus ì „ì²´ ì–´íœ˜ ì§‘í•© ì¤‘ ië²ˆì§¸ ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” ì ìˆ˜ 1) ì´ì „ kê°œ ë‹¨ì–´ë“¤ì„ Wë¼ëŠ” ë‹¨ì–´ í–‰ë ¬ì—ì„œ ì°¸ì¡°í•œ ë’¤ í‰ê· ì„ ì·¨í•˜ê±°ë‚˜ ì´ì–´ ë¶™ì¸ë‹¤. ì—¬ê¸°ì— Uë¼ëŠ” í–‰ë ¬ì„ ë‚´ì í•˜ê³  bias ë²¡í„°ì¸ bë¥¼ ë”í•´ì¤€ ë’¤ softmaxë¥¼ ì·¨í•œë‹¤. Uì˜ í¬ê¸°ëŠ” ì–´íœ˜ì§‘í•© í¬ê¸° $\\times$ ì„ë² ë”© ì°¨ì› ìˆ˜ ì´ë‹¤. $h$ : ë²¡í„° sequenceê°€ ì£¼ì–´ì¡Œì„ ë•Œ í‰ê· ì„ ì·¨í•˜ê±°ë‚˜ concatenateí•˜ì—¬ ê³ ì •ëœ ê¸¸ì´ì˜ ë²¡í„° í•˜ë‚˜ë¥¼ ë°˜í™˜í•˜ëŠ” ì—­í• ì„ í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤. L = frac{1}{T} \\sum^{T-1}-{t=k} log(w_{t}|w_{t-k}, \\cdots, w_{t-1})Doc2Vec ì–¸ì–´ ëª¨ë¸ Score ê³„ì‚°P(w_{t}| w_{t-k}, \\cdots, w_{t-1}) = \\frac{exp(y_{w_{t}})}{\\sum_{i} exp(y_{i})}y = b + U \\cdot h(w_{t-k}, \\cdots, w_{t-1}; W) ìœ„ì˜ ì´ˆê¸° êµ¬ì¡°ì—ì„œ ë¬¸ì„œ idë¥¼ ì¶”ê°€í•´ ì´ì „ kê°œ ë‹¨ì–´ë“¤ê³¼ ë¬¸ì„œ idë¥¼ ë„£ì–´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê²Œ í–ˆë‹¤. yë¥¼ ê³„ì‚°í•  ë•Œ Dë¼ëŠ” ë¬¸ì„œ í–‰ë ¬(Paragraph matrix)ì—ì„œ í•´ë‹¹ ë¬¸ì„œ IDì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ë¥¼ ì°¸ì¡°í•´ h í•¨ìˆ˜ì— ë‹¤ë¥¸ ë‹¨ì–´ ë²¡í„°ë“¤ê³¼ í•¨ê»˜ ì…ë ¥í•˜ëŠ” ê²ƒ ì™¸ì— ë‚˜ë¨¸ì§€ ê³¼ì •ì€ ë™ì¼í•˜ë‹¤. ì´ëŸ° êµ¬ì¡°ë¥¼ PV-DM(the Distributed Memory Model of Paragraph Vectors)ì´ë¼ê³  ë¶€ë¥¸ë‹¤. í•™ìŠµì´ ì¢…ë£Œë˜ë©´ ë¬¸ì„œ ìˆ˜ $\\times$ ì„ë² ë”© ì°¨ì› ìˆ˜ í¬ê¸°ë¥¼ ê°€ì§€ëŠ” ë¬¸ì„œ í–‰ë ¬ Dë¥¼ ê° ë¬¸ì„œì˜ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. ì´ë ‡ê²Œ ë§Œë“  ë¬¸ì„œ ì„ë² ë”©ì´ í•´ë‹¹ ë¬¸ì„œì˜ ì£¼ì œ ì •ë³´ë¥¼ í•¨ì¶•í•œë‹¤ê³  ì„¤ëª…í•œë‹¤. PV-DMì€ ë‹¨ì–´ ë“±ì¥ ìˆœì„œë¥¼ ê³ ë ¤í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ìˆœì„œ ì •ë³´ë¥¼ ë¬´ì‹œí•˜ëŠ” Bag of Words ê¸°ë²• ëŒ€ë¹„ ê°•ì ì´ ìˆë‹¤ê³  í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ë˜í•œ, Word2Vecì˜ Skip-gramì„ ë³¸ëœ¬ PV-DBOW(the Distributed Bag of Words version of Paragraph Vectors)ë„ ì œì•ˆí–ˆë‹¤. Skip-gramì€ target wordë¥¼ ê°€ì§€ê³  context wordë“¤ì„ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµë˜ì—ˆë‹¤. PV-DBOWë„ ë¬¸ì„œ idë¥¼ ê°€ì§€ê³  context wordë“¤ì„ ë§ì¶˜ë‹¤. ë”°ë¼ì„œ ë¬¸ì„œ idì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œ ì„ë² ë”©ì—” ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” ëª¨ë“  ë‹¨ì–´ì˜ ì˜ë¯¸ ì •ë³´ê°€ ë°˜ì˜ëœë‹¤. Doc2Vec ì‹¤ìŠµì‹¤ìŠµ ë°ì´í„° ì˜í™” ëŒ“ê¸€ê³¼ í•´ë‹¹ ì˜í™”ì˜ IDê°€ ë¼ì¸ í•˜ë‚˜ë¥¼ êµ¬ì„±í•˜ê³  ìˆë‹¤. ì˜í™”í•˜ë‚˜ë¥¼ ë¬¸ì„œë¡œ ë³´ê³  Doc2Vec ëª¨ë¸ì„ í•™ìŠµí•  ì˜ˆì •ì´ë‹¤. ë”°ë¼ì„œ ì˜í™” IDê°€ ë™ì¼í•œ ë¬¸ì¥ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ì²˜ë¦¬í•´ ì¤„ ê²ƒì´ë‹¤. 12345678910111213with open(\"./data/processed/processed_review_movieid.txt\") as f: print(f.readline()) print(\"--------------------------------------------------------------------------------------------\") print(f.readline()) print(\"--------------------------------------------------------------------------------------------\") print(f.readline()) print(\"--------------------------------------------------------------------------------------------\") print(f.readline()) print(\"--------------------------------------------------------------------------------------------\") count = 4 for sentence in f: count+=1 print(\"ì´ sentenceì˜ ê°œìˆ˜ : &#123;&#125;ê°œ\".format(count)) ê²°ê³¼12345678910111213ì¢…í•© í‰ì ì€ 4ì  ë“œë¦½ë‹ˆë‹¤.â92575--------------------------------------------------------------------------------------------ì›ì‘ì´ ì¹­ì†¡ë°›ëŠ” ì´ìœ ëŠ” ì›¹íˆ° ê³„ ìì²´ì˜ ì§ˆì  ì €í•˜ê°€ ì‹¬ê°í•˜ê¸° ë•Œë¬¸. ì›ì‘ì´ë‚˜ ì˜í™”ë‚˜ ë³„ë¡œì¸ê±´ ë§ˆì°¬ê°€ì§€.â92575--------------------------------------------------------------------------------------------ë‚˜ë¦„ì˜ ê°ë™ë„ ìˆê³  ì•ˆíƒ€ê¹Œìš´ ë§ˆìŒì— ê°€ìŠ´ë„ ë¨¹ë¨¹ ë°°ìš°ë“¤ì˜ ì—°ê¸°ê°€ good ê¹€ìˆ˜í˜„ ìµœê³ ~â92575--------------------------------------------------------------------------------------------ì´ëŸ°ê±¸ ëˆì£¼ê³  ë³¸ ë‚´ìì‹ ì´ í›„íšŒìŠ¤ëŸ½ë‹¤ ìµœì•…ì˜ ì“°ë ˆê¸° ì˜í™” ê¹€ìˆ˜í˜„ ë°–ì—ì—†ëŠ” ì €ì§ˆ ì‚¼ë¥˜ì˜í™”â92575--------------------------------------------------------------------------------------------ì´ sentenceì˜ ê°œìˆ˜ : 712532ê°œ Doc2Vec ëª¨ë¸ í•™ìŠµì„ ìœ„í•´ Python gensim ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Doc2Vec í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ”ë° Doc2VecInputì€ ì´ í´ë˜ìŠ¤ê°€ ìš”êµ¬í•˜ëŠ” ì…ë ¥ í˜•íƒœë¥¼ ë§ì¶°ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. 12345678910111213141516171819202122232425262728293031323334353637from khaiii import KhaiiiApifrom konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkmafrom gensim.models.doc2vec import TaggedDocumentdef get_tokenizer(tokenizer_name): if tokenizer_name == \"komoran\": tokenizer = Komoran() elif tokenizer_name == \"okt\": tokenizer = Okt() elif tokenizer_name == \"mecab\": tokenizer = Mecab() elif tokenizer_name == \"hannanum\": tokenizer = Hannanum() elif tokenizer_name == \"kkma\": tokenizer = Kkma() elif tokenizer_name == \"khaiii\": tokenizer = KhaiiiApi() else: tokenizer = Mecab() return tokenizerclass Doc2VecInput: def __init__(self, fname, tokenizer_name='mecab'): self.fname = fname self.tokenizer = get_tokenizer(tokenizer_name) def __iter__(self): with open(self.fname, encoding='utf-8') as f: for line in f: try: sentence, movie_id = line.strip().split(\"\\u241E\") tokens = self.tokenizer.morphs(sentence) tagged_doc = TaggedDocument(words=tokens, tags=['movie_%s' % movie_id]) yield tagged_doc except: continue dm : 1 (default) -&gt; PV-DM, 0- &gt; PV-DBOW 12345678from gensim.models import Doc2Veccorpus_fname = './data/processed/processed_review_movieid.txt'output_fname = './doc2vec.model'corpus = Doc2VecInput(corpus_fname)model = Doc2Vec(corpus, dm=1, vector_size=100)model.save(output_fname) í•™ìŠµì´ ì˜ ë˜ì—ˆëŠ”ì§€ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì´ í‰ê°€ í´ë˜ìŠ¤ë¥¼ ì´ìš©í•  ê²ƒì´ë©°, í‰ê°€ë¥¼ í•˜ë©´ tagëœ ì˜í™” idê°€ ë‚˜ì˜¬í…ë° ì§ê´€ì ìœ¼ë¡œ ê·¸ ì˜í™”ê°€ ì–´ë–¤ ì˜í™”ì¸ì§€ ëª¨ë¥¼ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì§ê´€ì ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ í•™ìŠµ ë°ì´í„°ì—ëŠ” ì—†ëŠ” ì˜í™” ì œëª©ì„ ë„¤ì´ë²„ ì˜í™” ì‚¬ì´íŠ¸ì— ì ‘ì†í•´ idì— ë§ëŠ” ì˜í™” ì œëª©ì„ ìŠ¤í¬ë˜í•‘í•´ ì˜¬ ê²ƒì´ë‹¤. 1234567891011121314151617181920212223242526272829import requestsfrom lxml import htmlclass Doc2VecEvaluator: def __init__(self, model_fname=\"data/doc2vec.vecs\", use_notebook=False): self.model = Doc2Vec.load(model_fname) self.doc2idx = &#123;el:idx for idx, el in enumerate(self.model.docvecs.doctags.keys())&#125; self.use_notebook = use_notebook def most_similar(self, movie_id, topn=10): similar_movies = self.model.docvecs.most_similar('movie_' + str(movie_id), topn=topn) for movie_id, score in similar_movies: print(self.get_movie_title(movie_id), score) def get_titles_in_corpus(self, n_sample=5): movie_ids = random.sample(self.model.docvecs.doctags.keys(), n_sample) return &#123;movie_id: self.get_movie_title(movie_id) for movie_id in movie_ids&#125; def get_movie_title(self, movie_id): url = 'http://movie.naver.com/movie/point/af/list.nhn?st=mcode&amp;target=after&amp;sword=%s' % movie_id.split(\"_\")[1] resp = requests.get(url) root = html.fromstring(resp.text) try: title = root.xpath('//div[@class=\"choice_movie_info\"]//h5//a/text()')[0] except: title = \"\" return title 123model_fname='./doc2vec.model'model = Doc2VecEvaluator(model_fname)print(\"ì˜í™”ì˜ ì¢…ë¥˜ : &#123;&#125; ê°œ\".format(len(model.doc2idx.keys()))) ê²°ê³¼1ì˜í™”ì˜ ì¢…ë¥˜ : 14730 ê°œ í•™ìŠµ ë°ì´í„°ì— í¬í•¨ëœ ì•„ë¬´ ì˜í™” 10ê°œì˜ ì œëª©ì„ ë³´ì—¬ì¤€ë‹¤. 1model.get_titles_in_corpus(n_sample=14730) ê²°ê³¼12345678910&#123;'movie_89743': 'ì—¬ìê°€ ë‘ ë²ˆ í™”ì¥í•  ë•Œ', 'movie_16490': 'íˆ¬ ë¬¸ ì •ì…˜ 2', 'movie_100953': 'ë” í¼ì§€', 'movie_84375': 'í¼í™íŠ¸ ì„¼ìŠ¤', 'movie_24203': 'ì„€í„°ë“œ ì´ë¯¸ì§€', 'movie_12054': 'ë‚˜í´ë ˆì˜¹', 'movie_10721': 'ë”í‹° í•´ë¦¬', 'movie_11440': 'í‚¹ ë‰´ìš•', 'movie_20896': 'ë¯¸ë§ì¸', 'movie_123068': 'ìº ê±¸'&#125; í•´ë‹¹ idì™€ ìœ ì‚¬í•œ ì˜í™” ìƒìœ„ 5ê°œë¥¼ ë³´ì—¬ì¤€ë‹¤. 1model.most_similar(37758, topn=5) ê²°ê³¼12345ëˆë¹„ ì–´í”„ë ˆì´ë“œ-ì–´ë‘  ì†ì˜ ì†ì‚­ì„ 0.746237576007843ë” í¼ì§€:ê±°ë¦¬ì˜ ë°˜ë€ 0.7402248382568359ê³ ì–‘ì´: ì£½ìŒì„ ë³´ëŠ” ë‘ ê°œì˜ ëˆˆ 0.6967850923538208ATM 0.690518856048584í›ì³ 0.6751468777656555 ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹(LDA, Latent Dirichlet Allocation) ì£¼ì–´ì§„ ë¬¸ì„œì— ëŒ€í•˜ì—¬ ê° ë¬¸ì„œì— ì–´ë–¤ topicë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ì— ëŒ€í•œ í™•ë¥  ëª¨í˜•ì´ë‹¤. corpusì˜ ì´ë©´ì— ì ì¬ëœ topicì„ ì¶”ì¶œí•œë‹¤ëŠ” ì˜ë¯¸ì—ì„œ topic modelingì´ë¼ê³  ë¶€ë¥´ê¸°ë„ í•œë‹¤. ë¬¸ì„œë¥¼ topic í™•ë¥  ë¶„í¬ë¡œ ë‚˜íƒ€ë‚´ ê°ê°ì„ ë²¡í„°í™”í•œë‹¤ëŠ” ì ì—ì„œ LDAë¥¼ ì„ë² ë”© ê¸°ë²•ì˜ ì¼ì¢…ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤. ëª¨ë¸ ê°œìš” LDAëŠ” topicë³„ ë‹¨ì–´ì˜ ë¶„í¬, ë¬¸ì„œë³„ topicì˜ ë¶„í¬ë¥¼ ëª¨ë‘ ì¶”ì •í•´ ë‚¸ë‹¤. LDAëŠ” topicì— íŠ¹ì • ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚  í™•ë¥ ì„ ë‚´ì–´ ì¤€ë‹¤. ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ê° ìƒ‰ê¹”ë³„ë¡œ topicì— ë”°ë¥¸ ë‹¨ì–´ê°€ ë“±ì¥í•  í™•ë¥ ì„ ë³´ì—¬ì£¼ê³ ìˆë‹¤. ë¬¸ì„œë¥¼ ë³´ë©´ ë…¸ë€ìƒ‰ topicì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ê°€ ë§ê¸° ë•Œë¬¸ì— ìœ„ ë¬¸ì„œì˜ ë©”ì¸ ì£¼ì œëŠ” ë…¸ë€ìƒ‰ topicì¸ ìœ ì „ì ê´€ë ¨ topicì¼ ê°€ëŠ¥ì„±ì´ í´ ê²ƒì´ë‹¤. ì´ë ‡ë“¯ ë¬¸ì„œì˜ topic ë¹„ì¤‘ ë˜í•œ LDAì˜ ì‚°ì¶œ ê²°ê³¼ê°€ ëœë‹¤. LDAê°€ ê°€ì •í•˜ëŠ” ë¬¸ì„œ ìƒì„± ê³¼ì •ì€ ìš°ë¦¬ê°€ ê¸€ì„ ì“¸ë•Œì™€ ê°™ë‹¤. ì‹¤ì œ ê¸€ì„ ì‘ì„±í•  ë•ŒëŠ” ê¸€ê° ë‚´ì§€ ì£¼ì œë¥¼ ë¨¼ì € ê²°ì •í•œ í›„ ì–´ë–¤ ë‹¨ì–´ë¥¼ ì¨ì•¼ í• ì§€ ê²°ì •í•œë‹¤. ì´ì™€ ë§ˆì°¬ê°€ì§€ë¡œ LDAëŠ” ìš°ì„  corpusë¡œ ë¶€í„° ì–»ì€ topic ë¶„í¬ë¡œë¶€í„° topicì„ ë½‘ëŠ”ë‹¤. ì´í›„ í•´ë‹¹ topicì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ë“¤ì„ ë½‘ëŠ”ë‹¤. ê·¸ëŸ°ë° corpusì— ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ ê°ê°ì— ê¼¬ë¦¬í‘œê°€ ë‹¬ë ¤ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— í˜„ì¬ ë¬¸ì„œì— ë“±ì¥í•œ ë‹¨ì–´ë“¤ì€ ì–´ë–¤ topicì—ì„œ ë½‘íŒ ë‹¨ì–´ë“¤ì¸ì§€ ìš°ë¦¬ê°€ ëª…ì‹œì ìœ¼ë¡œ ì•Œê¸°ëŠ” ì–´ë ¤ìš¸ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, LDAëŠ” ì´ëŸ° corpus ì´ë©´ì— ì¡´ì¬í•˜ëŠ” ì •ë³´ë¥¼ ì¶”ë¡ í•´ë‚¼ ìˆ˜ ìˆë‹¤. ì•„í‚¤í…ì²˜ LDAê°€ ê°€ì •í•˜ëŠ” ë¬¸ì„œ ìƒì„± ê³¼ì •ì€ ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤. D : corpus ì „ì²´ ë¬¸ì„œ ê°œìˆ˜ K : ì „ì²´ topic ìˆ˜(hyper parameter) N : dë²ˆì§¸ ë¬¸ì„œì˜ ë‹¨ì–´ ìˆ˜ ë„¤ëª¨ì¹¸ : í•´ë‹¹ íšŸìˆ˜ë§Œí¼ ë°˜ë³µí•˜ë¼ëŠ” ì˜ë¯¸ $\\phi_{k}$ : kë²ˆì§¸ topicì— í•´ë‹¹í•˜ëŠ” ë²¡í„° $\\phi_{k} \\in R^{|V|}$, $\\phi_{k}$ëŠ” word-topic í–‰ë ¬ì˜ kë²ˆì§¸ ì—´ì„ ì˜ë¯¸í•œë‹¤. $\\phi_{k}$ì˜ ê° ìš”ì†Œ ê°’ì€ í•´ë‹¹ ë‹¨ì–´ê°€ kë²ˆì§¸ í† í”½ì—ì„œ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì„ ì˜ë¯¸í•˜ë©° í™•ë¥ ê°’ì´ë¯€ë¡œ ì´ ë²¡í„°ì˜ ìš”ì†Œê°’ì˜ í•©ì€ 1ì´ ëœë‹¤. ì´ëŸ° topicì˜ ë‹¨ì–´ë¹„ì¤‘ì„ ì˜ë¯¸í•˜ëŠ” $\\phi_{k}$ëŠ” ë””ë¦¬í´ë ˆ ë¶„í¬ë¥¼ ë”°ë¥¸ ë‹¤ëŠ” ê°€ì •ì„ ì·¨í•˜ë¯€ë¡œ $\\beta$ì˜ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. $\\theta_{d}$ : dë²ˆì§¸ ë¬¸ì„œê°€ ê°€ì§„ topic ë¹„ì¤‘ì„ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„°ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì „ì²´ topicì˜ ê°œìˆ˜ë§Œí¼ì˜ ê¸¸ì´ ê°–ìœ¼ë©°, ê° ë²¡í„°ëŠ” í™•ë¥ ì„ ì˜ë¯¸í•˜ë¯€ë¡œ í•©ì€ 1ì´ëœë‹¤. ë¬¸ì„œì˜ topic ë¹„ì¤‘ $\\theta_{d}$ëŠ” ë””ë¦¬í´ë ˆ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê°€ì •ì„ ì·¨í•˜ë¯€ë¡œ $\\alpha$ì˜ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. $ z_{d,n}$ : dë²ˆì§¸ ë¬¸ì„œì—ì„œ në²ˆì§¸ ë‹¨ì–´ê°€ ì–´ë–¤ topicì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì´ ë³€ìˆ˜ëŠ” dë²ˆì§¸ ë¬¸ì„œì˜ topic í™•ë¥  ë¶„í¬ì¸ $\\theta_{d}$ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ë™ê·¸ë¼ë¯¸ : ë³€ìˆ˜ë¥¼ ì˜ë¯¸ í™”ì‚´í‘œê°€ ì‹œì‘ë˜ëŠ” ë³€ìˆ˜ëŠ” ì¡°ê±´, í™”ì‚´í‘œê°€ í–¥í•˜ëŠ” ë³€ìˆ˜ëŠ” ê²°ê³¼ì— í•´ë‹¹í•˜ëŠ” ë³€ìˆ˜ ê´€ì°° ê°€ëŠ¥í•œ ë³€ìˆ˜ëŠ” dë²ˆì§¸ ë¬¸ì„œì— ë“±ì¥í•œ në²ˆì§¸ ë‹¨ì–´ $w_{d,n}$ì´ ìœ ì¼í•˜ë‹¤. ìš°ë¦¬ëŠ” ì´ ì •ë³´ë§Œì„ ê°€ì§€ê³  í•˜ì´í¼íŒŒë¼ë©”í„°(ì‚¬ìš©ì ì§€ì •) Î±,Î²ë¥¼ ì œì™¸í•œ ëª¨ë“  ì ì¬ ë³€ìˆ˜ë¥¼ ì¶”ì •í•´ì•¼ í•œë‹¤. ì˜ˆì‹œë¥¼ ë“¤ìë©´, ì•„ë˜ Document-topic í–‰ë ¬ì„ ì‚´í´ë³´ì. 3ë²ˆì§¸ ë¬¸ì„œì— ì†í•œ ë‹¨ì–´ë“¤ì€ ê°€ì¥ ë†’ì€ í™•ë¥ ê°’ 0.625ë¥¼ ê°–ëŠ” topic-2ì¼ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. $w_{d,n}$ì€ dë²ˆì§¸ ë¬¸ì„œ ë‚´ì— në²ˆì§¸ë¡œ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë¥¼ ì˜ë¯¸í•˜ë©°, ë™ì‹œì— ìš°ë¦¬ê°€ ìœ ì¼í•˜ê²Œ corpusì—ì„œ ê´€ì°°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ì´ë‹¤. ì´ëŠ” $\\phi_{k}$ì™€ $\\z_{d,n}$ì— ë™ì‹œì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ì˜ˆë¥¼ ë“¤ë©´, $z_{3,1}$ê°€ topic-2ì´ë¼ê³  ê°€ì •í–ˆì„ ê²½ìš°, $w_{3,1}$ì€ word-topic í–‰ë ¬ì—ì„œ ë³´ê²Œë˜ë©´ ì œì¼ ë†’ì€ í™•ë¥ ê°’ 0.393ì„ ê°–ëŠ” â€˜ì½”ë¡œë‚˜ ë°”ì´ëŸ¬ìŠ¤â€™ì¼ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ì´ì²˜ëŸ¼ LDAëŠ” topicì˜ word ë¶„í¬($\\phi$)ì™€ ë¬¸ì„œì˜ topic ë¶„í¬($\\theta$)ì˜ ê²°í•©ìœ¼ë¡œ ë¬¸ì„œ ë‚´ ë‹¨ì–´ë“¤ì´ ìƒì„±ëœë‹¤ê³  ê°€ì •í•œë‹¤.Document-topic í–‰ë ¬ ë¬¸ì„œ topic-1 topic-2 topic-3 ë¬¸ì„œ1 0.400 0.000 0.600 ë¬¸ì„œ2 0.000 0.600 0.400 ë¬¸ì„œ3 0.375 0.625 0.000 word-topic í–‰ë ¬ ë‹¨ì–´ topic-1 topic-2 topic-3 ì½”ë¡œë‚˜ ë°”ì´ëŸ¬ìŠ¤ 0.000 0.393 0.000 ìš°í•œíë ´ 0.000 0.313 0.000 AWS 0.119 0.000 0.000 ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ 0.181 0.000 0.000 Hadoop 0.276 0.000 0.000 Spark 0.142 0.000 0.000 ë‚­ë§Œ ë‹¥í„° ê¹€ì‚¬ë¶€2 0.000 0.012 0.468 tensorflow 0.282 0.000 0.000 ë§ˆìŠ¤í¬ 0.000 0.282 0.000 ì‚¬ë‘ì˜ ë¶ˆì‹œì°© 0.000 0.000 0.532 í•© 1.0 1.0 1.0 ì‹¤ì œ ê´€ì°° ê°€ëŠ¥í•œ corpusë¥¼ ê°€ì§€ê³  ì•Œê³  ì‹¶ì€ topicì˜ word ë¶„í¬, ë¬¸ì„œì˜ topic ë¶„í¬ë¥¼ ì¶”ì •í•˜ëŠ” ê³¼ì •ì„ í†µí•´ LDAëŠ” í•™ìŠµí•œë‹¤. ì¦‰, topicì˜ word ë¶„í¬ì™€ ë¬¸ì„œì˜ topic ë¶„í¬ì˜ ê²°í•© í™•ë¥ ì´ ì»¤ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµì„ í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. LDAì˜ ë‹¨ì–´ ìƒì„± ê³¼ì •p(\\phi_{1:K}, \\theta_{1:D}, z_{1:D}, w_{1:D}) =\\prod^{K}_{i=1} p(\\phi_{i}|\\beta) \\prod^{D}_{d=1} p(\\theta_{d}|\\alpha) \\left(\\prod^{N}_{n=1} p(z_{d,n}|\\theta_{d}) p(w_{d,n}|\\phi_{1:K}, z_{d, n}) \\right) ìš°ë¦¬ê°€ êµ¬í•´ì•¼í•  ì‚¬í›„í™•ë¥  ë¶„í¬ëŠ” $ p(z, \\phi, \\theta|w) = p(z, \\phi, \\theta, w)/p(w)$ë¥¼ ìµœëŒ€ë¡œ ë§Œë“œëŠ” $ z, \\phi, \\theta$ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤. ì´ ì‚¬í›„í™•ë¥ ì„ ì§ì ‘ ê³„ì‚°í•˜ë ¤ë©´ ë¶„ìë„ ê³„ì‚°í•˜ê¸° ì–´ë µê² ì§€ë§Œ ë¶„ëª¨ê°€ ë˜ëŠ” $ p(w)$ë„ ë°˜ë“œì‹œ êµ¬í•´ì•¼ í™•ë¥ ê°’ìœ¼ë¡œ ë§Œë“¤ì–´ ì¤„ ìˆ˜ ìˆë‹¤. $ p(w)$ëŠ” ì ì¬ë³€ìˆ˜ $ z, \\phi, \\theta$ì˜ ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ë¥¼ ê³ ë ¤í•œ ê° ë‹¨ì–´(w)ì˜ ë“±ì¥ í™•ë¥ ì„ ì˜ë¯¸í•˜ëŠ”ë°, $ z, \\phi, \\theta$ë¥¼ ì§ì ‘ ê´€ì°°í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ ê¹ìŠ¤ ìƒ˜í”Œë§(gibbs sampling)ê°™ì€ í‘œë³¸ ì¶”ì¶œ ê¸°ë²•ì„ ì‚¬ìš©í•´ ì‚¬í›„í™•ë¥ ì„ ê·¼ì‚¬ì‹œí‚¤ê²Œ ëœë‹¤. ê¹ìŠ¤ ìƒ˜í”Œë§ì´ë€ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ëŠ” ê³ ì •ì‹œí‚¨ ì±„ í•˜ë‚˜ì˜ ëœë¤ë³€ìˆ˜ë§Œì„ ëŒ€ìƒìœ¼ë¡œ í‘œë³¸ì„ ë½‘ëŠ” ê¸°ë²•ì´ë‹¤. LDAì—ì„œëŠ” ì‚¬í›„í™•ë¥  ë¶„í¬ $ p(z, \\phi, \\theta|w)$ë¥¼ êµ¬í•  ë•Œ topicì˜ ë‹¨ì–´ ë¶„í¬($\\phi$)ì™€ ë¬¸ì„œì˜ topic ë¶„í¬($\\theta$)ë¥¼ ê³„ì‚°ì—ì„œ ìƒëµí•˜ê³  topic(z)ë§Œì„ ì¶”ë¡ í•œë‹¤. zë§Œ ì•Œ ìˆ˜ ìˆìœ¼ë©´ ë‚˜ë¯¸ì € ë³€ìˆ˜ë¥¼ ì´ë¥¼ í†µí•´ ê³„ì‚° í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ í–ˆê¸° ë•Œë¬¸ì´ë‹¤. ê¹ìŠ¤ ìƒ˜í”Œë§ ì°¸ì¡° ê¹ìŠ¤ ìƒ˜í”Œë§ì„ í™œìš©í•œ LDAp(z_{d, i} = j|z_{-i}, w) = \\frac{ n_{d,k} + \\alpha_{j} }{ \\sum^{K}_{i=1} (n_{d,i}) + \\alpha_{i} } \\times \\frac{ v_{k, w_{d,n} } + \\beta_{w_{d,n}} }{ \\sum^{V}_{ã…“=1} ( v_{k,j} + \\beta_{j} ) } = ABLDA ë³€ìˆ˜ í‘œê¸°ë²• í‘œê¸° ë‚´ìš© $n_{d,k}$ kë²ˆì§¸ topicì— í• ë‹¹ëœ dë²ˆì§¸ ë¬¸ì„œì˜ ë¹ˆë„ $v_{k,w_{d,n}}$ ì „ì²´ corpusì—ì„œ kë²ˆì§¸ topicì— í• ë‹¹ëœ ë‹¨ì–´ $w_{d,n}$ì˜ ë¹ˆë„ $w_{d,n}$ dë²ˆì§¸ ë¬¸ì„œì— në²ˆì§¸ë¡œ ë“±ì¥í•œ ë‹¨ì–´ $\\alpha$ ë¬¸ì„œì˜ topic ë¶„í¬ ìƒì„±ì„ ìœ„í•œ ë””ë¦¬í´ë ˆ ë¶„í¬ íŒŒë¼ë¯¸í„° $\\beta$ topicì˜ word ë¶„í¬ ìƒì„±ì„ ìœ„í•œ ë””ë¦¬í´ë ˆ ë¶„í¬ íŒŒë¼ë¯¸í„° K ì‚¬ìš©ìê°€ ì§€ì •í•˜ëŠ” topic ê°œìˆ˜ V corpusì— ë“±ì¥í•˜ëŠ” ì „ì²´ wordì˜ ìˆ˜ A dë²ˆì§¸ ë¬¸ì„œê°€ kë²ˆì§¸ topicê³¼ ë§ºê³  ìˆëŠ” ì—°ê´€ì„± ì •ë„ B dë²ˆì§¸ ë¬¸ì„œì˜ në²ˆì§¸ ë‹¨ì–´($ w_{d,n}$)ê°€ kë²ˆì§¸ topicê³¼ ë§ºê³  ìˆëŠ” ì—°ê´€ì„± ì •ë„ LDAì™€ ê¹ìŠ¤ ìƒ˜í”Œë§ LDAê°€ ê° ë‹¨ì–´ì— ì ì¬ëœ ì£¼ì œë¥¼ ì¶”ë¡ í•˜ëŠ” ë°©ì‹ì„ ì‚´í´ë³¸ë‹¤. ì•„ë˜í‘œì™€ ê°™ì´ ë‹¨ì–´ 5ê°œë¡œ êµ¬ì„±ëœ ë¬¸ì„œ1ì˜ ëª¨ë“  ë‹¨ì–´ì— ì£¼ì œ(z)ê°€ ì´ë¯¸ í• ë‹¹ë¼ ìˆë‹¤ê³  ê°€ì •í•´ë³´ì. LDAëŠ” ì´ë ‡ê²Œ ë¬¸ì„œ ì „ì²´ì˜ ëª¨ë“  ë‹¨ì–´ì˜ ì£¼ì œë¥¼ ëœë¤í•˜ê²Œ í• ë‹¹ì„ í•˜ê³  í•™ìŠµì„ ì‹œì‘í•˜ê¸° ë•Œë¬¸ì— ì´ë ‡ê²Œ ê°€ì •í•˜ëŠ” ê²Œ í¬ê²Œ ë¬´ë¦¬ê°€ ì—†ë‹¤. ë˜í•œ, topic ìˆ˜ëŠ” ì‚¬ìš©ìê°€ 3ê°œë¡œ ì´ë¯¸ ì§€ì •í•´ ë†“ì€ ìƒíƒœë¼ê³  í•˜ì. ë¬¸ì„œ1ì˜ ì²« ë²ˆì§¸ ë‹¨ì–´($ w_{11} = ì²œì£¼êµ)$ ì˜ ì£¼ì œ($z_{11}$)ëŠ” 3ë²ˆ topicì´ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ë¬¸ì„œ1ì˜ 3 ë²ˆì§¸ ë‹¨ì–´($ w_{13} = ê°€ê²©$)ì˜ ì£¼ì œ($z_{13}$)ëŠ” 1ë²ˆ topicì´ë‹¤. ì´ëŸ° ë°©ì‹ìœ¼ë¡œ Corpus ì „ì²´ ë¬¸ì„œ ëª¨ë“  ë‹¨ì–´ì— topicì´ ì´ë¯¸ í• ë‹¹ëë‹¤ê³  ê°€ì •í•œë‹¤. ì´ë¡œë¶€í„° word-topic í–‰ë ¬ì„ ë§Œë“¤ìˆ˜ ìˆë‹¤. ì „ì²´ ë¬¸ì„œ ëª¨ë“  ë‹¨ì–´ì— ë‹¬ë¦° ì£¼ì œë“¤ì„ ì¼ì¼ì´ ì„¸ì–´ì„œ ë§Œë“ ë‹¤. ê°™ì€ ë‹¨ì–´ë¼ë„ topicì´ ë‹¤ë¥¸ ë°°(ë™ìŒë‹¤ì˜ì–´)ê°™ì€ ê²½ìš°ê°€ ìˆìœ¼ë¯€ë¡œ ê° ë‹¨ì–´ë³„ë¡œ topic ë¶„í¬ê°€ ìƒê²¨ë‚œë‹¤. ë¬¸ì„œ 1ì˜ ë‹¨ì–´ë³„ topic ë¶„í¬($\\theta$) $z_{1i}$ 3 2 1 3 1 $w_{1,n}$ ì²œì£¼êµ ë¬´ì—­ ê°€ê²© ë¶ˆêµ ì‹œì¥ word-topic í–‰ë ¬ ë‹¨ì–´ topic-1 topic-2 topic-3 ì²œì£¼êµ 1 0 35 ì‹œì¥ 50 0 1 ê°€ê²© 42 1 0 ë¶ˆêµ 0 0 20 ë¬´ì—­ 10 8 1 $\\cdots$ $\\cdots$ $\\cdots$ $\\cdots$ ê¹ìŠ¤ ìƒ˜í”Œë§ìœ¼ë¡œ ë¬¸ì„œ1 ë‘ ë²ˆì§¸ ë‹¨ì–´ì˜ ì ì¬ëœ topicì´ ë¬´ì—‡ì¸ì§€ ì¶”ë¡ í•´ë³´ìë©´, ê¹ìŠ¤ ìƒ˜í”Œë§ì„ ì ìš©í•˜ê¸° ìœ„í•´ ë¬¸ì„œ1ì˜ ë‘ ë²ˆì§¸ topicì •ë³´ë¥¼ ì§€ìš¸ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ í‘œë¡œ ë³€í™”ë  ê²ƒì´ë‹¤. ë¬¸ì„œ1ì˜ ë‹¨ì–´ë³„ topic ë¶„í¬ $z_{1i}$ 3 ? 1 3 1 $w_{1,n}$ ì²œì£¼êµ ë¬´ì—­ ê°€ê²© ë¶ˆêµ ì‹œì¥ word-topic í–‰ë ¬ ë‹¨ì–´ topic-1 topic-2 topic-3 ì²œì£¼êµ 1 0 35 ì‹œì¥ 50 0 1 ê°€ê²© 42 1 0 ë¶ˆêµ 0 0 20 ë¬´ì—­ 10 7 = (8 - 1) 1 $\\cdots$ $\\cdots$ $\\cdots$ $\\cdots$ p($ z_{1,2} $)ëŠ” Aì™€ Bì˜ ê³±ìœ¼ë¡œ ë„ì¶œëœë‹¤. Aê°’ì€ íŒŒë€ìƒ‰ ì˜ì—­ì„ ì˜ë¯¸í•˜ë©°, ë¬¸ì„œ ë‚´ ë‹¨ì–´ë“¤ì˜ topic ë¶„í¬ì—($\\theta$)ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ë˜í•œ, Bê°’ì€ topicì˜ ë‹¨ì–´ ë¶„í¬($\\phi$)ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤. Aì™€ Bë¥¼ ê°ê° ì§ì‚¬ê°í˜•ì˜ ë†’ì´ì™€ ë„ˆë¹„ë¡œ ë‘”ë‹¤ë©´, p($ z_{1,2} $)ëŠ” ì•„ë˜ì™€ ê°™ì´ ì§ì‚¬ê°í˜•ì˜ ë„“ì´ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤. ì´ì™€ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ëª¨ë“  ë¬¸ì„œ, ëª¨ë“  ë‹¨ì–´ì— ê´€í•´ ê¹ìŠ¤ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•˜ë©´ ëª¨ë“  ë‹¨ì–´ë§ˆë‹¤ topicì„ í• ë‹¹í•´ì¤„ ìˆ˜ê°€ ìˆê²Œ ëœë‹¤. ì¦‰, word-topic í–‰ë ¬ì„ ì™„ì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ë³´í†µ 1,000íšŒ ~ 10,000íšŒ ë°˜ë³µ ìˆ˜í–‰í•˜ë©´ ê·¸ ê²°ê³¼ê°€ ìˆ˜ë ´í•œë‹¤ê³  í•˜ë©°, ì´ë¥¼ í† ëŒ€ë¡œ ë¬¸ì„œì˜ topic ë¶„í¬, topic ë‹¨ì–´ ë¶„í¬ ë˜í•œ êµ¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤. $\\theta$ì˜ ê²½ìš° ê° ë¬¸ì„œì— ì–´ë–¤ ë‹¨ì–´ì‚¬ ì“°ì˜€ëŠ”ì§€ ì¡°ì‚¬í•´ ê·¸ ë‹¨ì–´ì˜ topic ë¶„í¬ë¥¼ ë”í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ê³„ì‚°í•œë‹¤. ì‚¬ìš©ìê°€ ì§€ì •í•˜ëŠ” í•˜ì´í¼íŒŒë¼ë©”í„° Î± ì¡´ì¬ ë•ë¶„ì— Aê°€ ì•„ì˜ˆ 0ìœ¼ë¡œ ë˜ëŠ” ì¼ì„ ë§‰ì„ ìˆ˜ ìˆê²Œ ëœë‹¤. ì¼ì¢…ì˜ smoothing ì—­í• ì„ í•œë‹¤. ë”°ë¼ì„œ Î±ê°€ í´ìˆ˜ë¡ í† í”½ë“¤ì˜ ë¶„í¬ê°€ ë¹„ìŠ·í•´ì§€ê³ , ì‘ì„ ìˆ˜ë¡ íŠ¹ì • í† í”½ì´ í¬ê²Œ ë‚˜íƒ€ë‚˜ê²Œ ëœë‹¤. ì´ëŠ” Î²ê°€ Bì—ì„œ ì°¨ì§€í•˜ëŠ” ì—­í• ë„ ë™ì¼í•˜ë‹¤. ìµœì  í† í”½ ìˆ˜ ì°¾ê¸° LDAì˜ í† í”½ìˆ˜ KëŠ” ì—¬ëŸ¬ ì‹¤í—˜ì„ í†µí•´ ì‚¬ìš©ìê°€ ì§€ì •í•˜ëŠ” ë¯¸ì§€ìˆ˜ì¸ hyper parameterì´ë‹¤. ìµœì  í† í”½ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ë° ì“°ëŠ” Perplexity ì§€í‘œìˆë‹¤. p(w)ëŠ” í´ìˆ˜ë¡ ì¢‹ì€ inferenceì´ë¯€ë¡œ exp(âˆ’log(p(w)))ëŠ” ì‘ì„ìˆ˜ë¡ ì¢‹ë‹¤. ë”°ë¼ì„œ í† í”½ ìˆ˜ Kë¥¼ ë°”ê¿”ê°€ë©´ì„œ Perplexityë¥¼ êµ¬í•œ ë’¤ ê°€ì¥ ì‘ì€ ê°’ì„ ë‚´ëŠ” Kë¥¼ ìµœì ì˜ í† í”½ìˆ˜ë¡œ ì‚¼ìœ¼ë©´ ëœë‹¤. Perplexity(w)=exp\\left[ -\\frac { log\\left\\{ p(w) \\right\\} }{ \\sum _{ d=1 }^{ D }{ \\sum _{ j=1 }^{ V }{ { n }^{ jd } } } } \\right]LDA ì‹¤ìŠµë°ì´í„° ì†Œê°œ| ë„¤ì´ë²„ ì˜í™” corpusë¥¼ soynlpë¡œ ë„ì–´ì“°ê¸° êµì •í•œ ê²°ê³¼ë¥¼ LDAì˜ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ì•„ë˜ì˜ ì½”ë“œëŠ” LDA ëª¨ë¸ í”¼ì²˜ë¥¼ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•œë‹¤. LDAì˜ ì…ë ¥ê°’ì€ ë¬¸ì„œ ë‚´ ë‹¨ì–´ì˜ ë“±ì¥ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  í•´ë‹¹ ë‹¨ì–´ê°€ ëª‡ ë²ˆ ì“°ì˜€ëŠ”ì§€ ê·¸ ë¹ˆë„ë§Œì„ ë”°ì§„ë‹¤. ê·¸ëŸ°ë° â€˜ë…¸ì¼! ë…¸ì¼! ë…¸ì¼!â€™ ê°™ì´ íŠ¹ì • ë‹¨ì–´ê°€ ì¤‘ë³µìœ¼ë¡œ ì‚¬ìš©ëœ ë¬¸ì„œê°€ ìˆë‹¤ë©´ í•´ë‹¹ ë¬¸ì„œì˜ topic ë¶„í¬ê°€ í•œìª½ìœ¼ë¡œ ë„ˆë¬´ ì ë¦´ ì—¼ë ¤ê°€ ìˆë‹¤. ì´ ë•Œë¬¸ì— tokenì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ì¤‘ë³µì„ ì œê±°í•œ í˜•íƒœë¡œ LDA í”¼ì²˜ë¥¼ ë§Œë“¤ ê²ƒì´ë‹¤. 123456789101112131415161718192021from gensim import corporafrom khaiii import KhaiiiApifrom konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkmadef get_tokenizer(tokenizer_name): if tokenizer_name == \"komoran\": tokenizer = Komoran() elif tokenizer_name == \"okt\": tokenizer = Okt() elif tokenizer_name == \"mecab\": tokenizer = Mecab() elif tokenizer_name == \"hannanum\": tokenizer = Hannanum() elif tokenizer_name == \"kkma\": tokenizer = Kkma() elif tokenizer_name == \"khaiii\": tokenizer = KhaiiiApi() else: tokenizer = Mecab() return tokenizer 12345678910111213corpus_fname = './data/processed/corrected_ratings_corpus.txt'documents, tokenized_corpus = [], []tokenizer = get_tokenizer('mecab')with open(corpus_fname, 'r', encoding='utf-8') as f: for document in f: tokens = list(set(tokenizer.morphs(document.strip()))) documents.append(document) tokenized_corpus.append(tokens)dictionary = corpora.Dictionary(tokenized_corpus)corpus = [dictionary.doc2bow(text) for text in tokenized_corpus] corpora.Dictionary ì°¸ì¡° dictionaryí˜•íƒœë¡œ vocabulary dictionaryë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒì´ë‹¤. add_documentsë¡œ ë¬¸ì„œë¥¼ ì¶”ê°€í•  ìˆ˜ë„ ìˆë‹¤. doc2bowëŠ” bag-of-words (BoW) í˜•íƒœë¡œ ë¬¸ì„œë¥¼ ë³€í™˜ì‹œì¼œì¤€ë‹¤. [(token_id, token_count)]í˜•íƒœì´ë‹¤. doc2bowì˜ ì˜µì…˜ìœ¼ë¡œ return_missing=Trueë¥¼ ì£¼ë©´ í•´ë‹¹ sentenceì˜ ë‹¨ì–´ì¤‘ ë¯¸ë“±ë¡ ë‹¨ì–´ì™€ ì¹´ìš´íŠ¸ë¥¼ ê°™ì´ ì¶œë ¥í•´ì¤€ë‹¤. ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ LDAë¥¼ í•™ìŠµí•˜ê³  ê·¸ ê²°ê³¼ë¥¼ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. LdaMulticoreì—ì„œ num_topicssëŠ” í† í”½ ìˆ˜(K)ì— í•´ë‹¹ë˜ëŠ” parameterì´ë‹¤. get_document_topicsë¼ëŠ” í•¨ìˆ˜ëŠ” í•™ìŠµì´ ëë‚œ LDA ëª¨ë¸ë¡œë¶€í„° ê° ë¬¸ì„œë³„ topic ë¶„í¬ë¥¼ ë¦¬í„´í•œë‹¤. minimum_probability ì¸ìë¥¼ 0.5ë¥¼ ì¤¬ëŠ”ë°, ì´ëŠ” 0.5ë¯¸ë§Œì˜ topic ë¶„í¬ëŠ” ë¬´ì‹œí•œë‹¤ëŠ” ëœ»ì´ë‹¤. íŠ¹ì • í† í”½ì˜ í™•ë¥ ì´ 0.5ë³´ë‹¤ í´ ê²½ìš°ì—ë§Œ ë°ì´í„°ë¥¼ ë¦¬í„´í•œë‹¤. í™•ë¥ ì˜ í•©ì€ 1ì´ê¸° ë•Œë¬¸ì— í•´ë‹¹ í† í”½ì´ í•´ë‹¹ ë¬¸ì„œì—ì„œ í™•ë¥ ê°’ì´ ê°€ì¥ í° í† í”½ì´ ëœë‹¤. 12345from gensim.models import ldamulticoreLDA = ldamulticore.LdaMulticore(corpus, id2word=dictionary, num_topics=30, workers=4)all_topics = LDA.get_document_topics(corpus, minimum_probability=0.5, per_word_topics=False) ì•„ë˜ ê²°ê³¼ë¥¼ í•´ì„í•˜ìë©´, 0ë²ˆ ë¬¸ì„œëŠ” ì „ì²´ topic 30ê°œ ì¤‘ 19ë²ˆì— í•´ë‹¹í•˜ëŠ” topicì˜ í™•ë¥  ê°’ì´ ì œì¼ ë†’ìœ¼ë©° ê·¸ ê°’ì€ 0.7227057ì´ë‹¤. 3ë²ˆ ë¬¸ì„œ ê°™ì€ ê²½ìš° ì „ì²´ topic ì¤‘ 0.5ë¥¼ ë„˜ëŠ” topicì´ ì—†ìŒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. 12for doc_idx, topic in enumerate(all_topics[:5]): print(doc_idx, topic) ê²°ê³¼123450 [(19, 0.7227057)]1 [(9, 0.5350808)]2 []3 [(19, 0.7778823)]4 [(14, 0.80464107)] ëª¨ë¸ ì €ì¥123456789output_fname = './lda'all_topics = LDA.get_document_topics(corpus, minimum_probability=0.5, per_word_topics=False)with open(output_fname + \".results\", 'w') as f: for doc_idx, topic in enumerate(all_topics): if len(topic) == 1: # tuple í˜•íƒœë¡œ ë˜ì–´ìˆëŠ” ë°ì´í„°ë¡œ ê°€ì ¸ì™€ì„œ ë‚˜ëˆ ì¤Œ topic_id, prob = topic[0] f.writelines(documents[doc_idx].strip() + \"\\u241E\" + ' '.join(tokenized_corpus[doc_idx]) + \"\\u241E\" + str(topic_id) + \"\\u241E\" + str(prob) + \"\\n\")LDA.save(output_fname + \".model\") LDA í‰ê°€123456789101112131415161718192021222324252627282930313233343536from gensim.models import LdaModelfrom collections import defaultdictclass LDAEvaluator: def __init__(self, model_path=\"./lda\", tokenizer_name=\"mecab\"): self.tokenizer = get_tokenizer(tokenizer_name) self.all_topics = self.load_results(model_path + \".results\") self.model = LdaModel.load(model_path + \".model\") def load_results(self, results_fname): topic_dict = defaultdict(list) with open(results_fname, 'r', encoding='utf-8') as f: for line in f: sentence, _, topic_id, prob = line.strip().split(\"\\u241E\") topic_dict[int(topic_id)].append((sentence, float(prob))) for key in topic_dict.keys(): topic_dict[key] = sorted(topic_dict[key], key=lambda x: x[1], reverse=True) return topic_dict def show_topic_docs(self, topic_id, topn=10): return self.all_topics[topic_id][:topn] def show_topic_words(self, topic_id, topn=10): return self.model.show_topic(topic_id, topn=topn) def show_new_document_topic(self, documents): tokenized_documents = [self.tokenizer.morphs(document) for document in documents] curr_corpus = [self.model.id2word.doc2bow(tokenized_document) for tokenized_document in tokenized_documents] topics = self.model.get_document_topics(curr_corpus, minimum_probability=0.5, per_word_topics=False) for doc_idx, topic in enumerate(topics): if len(topic) == 1: topic_id, prob = topic[0] print(documents[doc_idx], \", topic id:\", str(topic_id), \", prob:\", str(prob)) else: print(documents[doc_idx], \", there is no dominant topic\") ëª¨ë¸ ì´ˆê¸°í™”1model = LDAEvaluator('./lda') topic ë¬¸ì„œ í™•ì¸ show_topic_docs í•¨ìˆ˜ì— topic IDë¥¼ ì¸ìë¡œ ì£¼ì–´ ì‹¤í–‰í•˜ë©´ í•´ë‹¹ topic í™•ë¥  ê°’ì´ ê°€ì¥ ë†’ì€ ë¬¸ì„œ ìƒìœ„ 10ê°œë¥¼ ì¶œë ¥í•œë‹¤. 1model.show_topic_docs(topic_id=0) ê²°ê³¼123456789101112[('ë‚´ê°€ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ì˜í™”! ìƒ‰ê°ê³¼ ì˜ìƒ ì¸ë¬¼ë“¤ì˜ ê°ì •ì´ì… ëŒ€ì‚¬ í•œë§ˆë”” í•œë§ˆë””ê°€ ë„ˆë¬´ë‚˜ ì™„ë²½í•œ. ëª‡ë²ˆì„ ë´ë„ ë˜ ë³´ê³ ì‹¶ì€ ì˜í™”.', 0.9707017), ('ì˜í™”ë³´ë‹¤ê°€ ìš´ì ì€ ì •ìš° ì£¼ì—° ì˜í™” ë°”ëŒë§ê³ ëŠ” ì—†ëŠ”ë° ì´ê±´ ê°ì •ì´ì…ì´ ë˜ì„œ ê·¸ëŸ°ì§€ ëª°ë¼ë„ ì§„ì§œ ëˆˆ ì¶©í˜ˆë˜ë„ë¡ í‘í‘ìš¸ì—ˆìŒ', 0.9677608), ('ì„¸ ëª…í’ˆë°°ìš°, ëª°ì…ë„ ìµœê³ ì˜ í˜„ì¶œ,ê°„ê²°í•˜ê³  ì‹œê°™ì€ ëŒ€ì‚¬,ìƒì²˜ë°›ì€ ì‚¬ëŒë“¤ì˜ ì•„ë¦„ë‹¤ìš´ ì¹˜ìœ !', 0.95923144), ('\"ì•„.. ë”°ë“¯í•˜ë‹¤.. \"\"ì²œêµ­ì˜ ì•„ì´ë“¤\"\" ëª»ë³´ì‹  ë¶„ ê¼­ ë³´ì„¸ìš”.. ê°™ì€ ê°ë…ì„..\"', 0.957968), ('ëª¸ì´ ë§ˆìŒì²˜ëŸ¼ ì›€ì§ì—¬ ì£¼ì§€ ì•ŠëŠ” ì§€ì²´ì¥ì• ìš°ë“¤ì˜ í˜¼ì‹ ì˜ ë…¸ë ¥ê³¼ ì—´ì—°ì´ ë‹ë³´ì´ëŠ” ì˜í™”ì˜€ë‹¤', 0.95778286), ('ì˜ìƒë¯¸ ì•„ë¦„ë‹µê³  ì£¼ì¸ê³µì˜ ì‚¬ë‘ì´ ìˆœìˆ˜í•˜ê³  í’‹í’‹í•˜ë‹¤. í•œ ë²ˆ ë” ë³´ê³ ì‹¶ì€ ì˜í™”!', 0.9539619), ('ìŒì•…ì´ ì•„ë¦„ë‹µê³  ê°€ìŠ´ì´ ë­‰í´í•˜ë‹ˆ ê°ë™ì ì´ì—ˆì–´. ë§ˆìŒì´ ë”°ëœ»í•´ì§€ëŠ” ì˜í™”ì˜ˆìš”.', 0.9539556), ('í”¼ì•„ë‹ˆìŠ¤íŠ¸ì™€ ê°™ì€ ë™ê¸‰ ì˜í™”ë¼ ìƒê°í•©ë‹ˆë‹¤ ë³´ì‹œë©´ í›„íšŒì—†ì„ê±°ì˜ˆìš” ì‹¤í™”ì˜í™”ì´ë‹ˆìš”', 0.953952), ('ê·¸ëƒ¥ ê³ ë¯¼ë§ê³  ë³´ì„¸ìš”.ì§„ì§œ ì´ê±´ ëª…ì‘ì´ë¼ëŠ” ë§ë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤..ê¼­ ë³´ì„¸ìš”!', 0.9491169), ('ì˜í™”ë¥¼ ë³´ëŠ”ë‚´ë‚´ ê°ì •ì´ ì´ì…ë˜ê³  ì²«ì‚¬ë‘ì´ ë³´ê³ ì‹¶ì–´ì§€ëŠ” ê·¸ëŸ° ì˜í™”ì…ë‹ˆë‹¤.', 0.9491167)] topic ë³„ ë‹¨ì–´ í™•ì¸ í•´ë‹¹ topic IDì—ì„œ ê°€ì¥ ë†’ì€ í™•ë¥  ê°’ì„ ì§€ë‹ˆëŠ” ë‹¨ì–´ë“¤ ì¤‘ ìƒìœ„ nê°œì˜ ëª©ë¡ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì–´ë¯¸ë‚˜ ì¡°ì‚¬ê°€ ë§ì´ ë¼ì–´ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. LDAì˜ í’ˆì§ˆì„ ëŒì–´ ì˜¬ë¦¬ê¸° ìœ„í•´ í”¼ì²˜ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì—ì„œ ëª…ì‚¬ë§Œ ì“°ê¸°ë„ í•œë‹¤. 1model.show_topic_words(topic_id=0) ê²°ê³¼12345678910[('ë³´', 0.03599964), ('ëŠ”', 0.03479155), ('.', 0.030642439), ('ê³ ', 0.030473521), ('ì˜í™”', 0.029688885), ('ì´', 0.018665483), ('ë¡œ', 0.017465018), ('ë‚´ë‚´', 0.016784767), ('ë‹¤', 0.016693212), ('í•œ', 0.013309637)] ìƒˆë¡œìš´ ë¬¸ì„œì˜ topic í™•ì¸ show_new_document_topic í•¨ìˆ˜ëŠ” ìƒˆë¡œìš´ ë¬¸ì„œì˜ topicì„ í™•ì¸í•˜ëŠ” ì—­í• ì„ í•œë‹¤. ë¬¸ì„œë¥¼ í˜•íƒœì†Œ ë¶„ì„í•œ ë’¤ ì´ë¥¼ LDA ëª¨ë¸ì— ë„£ì–´ topicì„ ì¶”ë¡ í•´ ê°€ì¥ ë†’ì€ í™•ë¥  ê°’ì„ ì§€ë‹ˆëŠ” topic idì™€ ê·¸ í™•ë¥ ì„ ë¦¬í„´í•´ì¤€ë‹¤. í•´ë‹¹ ë¬¸ì„œì˜ topic ë¶„í¬ ì¤‘ 0.5ë¥¼ ë„˜ëŠ” ì§€ë°°ì ì¸ topicì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° â€˜there is no dominant topicâ€™ë©”ì‹œì§€ë¥¼ ë¦¬í„´í•œë‹¤. 1model.show_new_document_topic([\"ë„ˆë¬´ ì‚¬ë‘ìŠ¤ëŸ¬ìš´ ì˜í™”\", \"ì¸ìƒì„ ë§í•˜ëŠ” ì˜í™”\"]) ê²°ê³¼12ë„ˆë¬´ ì‚¬ë‘ìŠ¤ëŸ¬ìš´ ì˜í™” , topic id: 28 , prob: 0.8066608ì¸ìƒì„ ë§í•˜ëŠ” ì˜í™” , topic id: 9 , prob: 0.7323683 12 12 12","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP - ë‹¨ì–´ ìˆ˜ì¤€ ì„ë² ë”©","slug":"NLP_06","date":"2020-02-01T11:47:03.000Z","updated":"2020-02-07T19:26:13.782Z","comments":true,"path":"2020/02/01/NLP_06/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/01/NLP_06/","excerpt":"","text":"ë‹¨ì–´ ìˆ˜ì¤€ ì„ë² ë”© ì˜ˆì¸¡ ê¸°ë°˜ ëª¨ë¸ NPLM Word2Vec FastText í–‰ë ¬ ë¶„í•´ ê¸°ë°˜ ëª¨ë¸ LSA GloVe Swivel ë‹¨ì–´ ì„ë² ë”©ì„ ë¬¸ì¥ ìˆ˜ì¤€ ì„ë² ë”©ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ë°©ë²• ê°€ì¤‘ ì„ë² ë”©(Weighted Embedding) NPLM(Neural Probabilistic Language Model) NLP ë¶„ì•¼ì—ì„œ ì„ë² ë”© ê°œë…ì„ ë„ë¦¬ í¼ëœ¨ë¦¬ëŠ” ë° ì¼ì¡°í•œ ì„ êµ¬ìì  ëª¨ë¸ë¡œì„œ ì„ë² ë”© ì—­ì‚¬ì—ì„œ ì°¨ì§€í•˜ëŠ” ì—­í• ì´ ì‘ì§€ ì•Šë‹¤. â€˜ë‹¨ì–´ê°€ ì–´ë–¤ ìˆœì„œë¡œ ì“°ì˜€ëŠ”ê°€â€™ë¼ëŠ” ê°€ì •ì„ í†µí•´ ë§Œë“¤ì–´ì§„ í†µê³„ ê¸°ë°˜ì˜ ì „í†µì ì¸ ì–¸ì–´ ëª¨ë¸ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ëŠ” ê³¼ì •ì—ì„œ ë§Œë“¤ì–´ì¡Œë‹¤ëŠ”ë° ì˜ì˜ê°€ ìˆìœ¼ë©° NPLM ìì²´ê°€ ë‹¨ì–´ ì„ë² ë”© ì—­í• ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ì ë“¤ì´ ê¸°ì¡´ì—ëŠ” ì¡´ì¬í–ˆë‹¤. 1) í•™ìŠµ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì— ëŒ€í•´ ë‚˜íƒ€ë‚  í™•ë¥ ì„ 0ìœ¼ë¡œ ë¶€ì—¬ í•œë‹¤. ë¬¼ë¡ , ë°±ì˜¤í”„(back-off)ë‚˜ ìŠ¤ë¬´ë”©(smoothing)ìœ¼ë¡œ ì™„í™”ì‹œì¼œ ì¤„ ìˆœ ìˆì§€ë§Œ ê·¼ë³¸ì ì¸ í•´ê²°ë°©ì•ˆì€ ì•„ë‹ˆì˜€ë‹¤. 2) ë¬¸ì¥ì˜ ì¥ê¸° ì˜ì¡´ì„±(long-term dependency)ì„ í¬ì°©í•´ë‚´ê¸° ì–´ë µë‹¤. ë‹¤ì‹œ ë§í•´ì„œ n-gramëª¨ë¸ì˜ nì„ 5 ì´ìƒìœ¼ë¡œ ê¸¸ê²Œ ì„¤ì •í•  ìˆ˜ ì—†ë‹¤. nì´ ì»¤ì§ˆìˆ˜ë¡ í™•ë¥ ì´ 0ì´ë  ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì´ë‹¤. 3) ë‹¨ì–´/ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ë‹¤. NLPMì˜ í•™ìŠµ NLPMì€ ì§ì „ê¹Œì§€ ë“±ì¥í•œ n-1ê°œ(n: gramìœ¼ë¡œ ë¬¶ì„ ìˆ˜) ë‹¨ì–´ë“¤ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë§ì¶”ëŠ” n-gram ì–¸ì–´ ëª¨ë¸ì´ë‹¤. NLPM êµ¬ì¡°ì˜ ë§ë‹¨ ì¶œë ¥ $|V|$(corpusì˜ token vector, ì–´íœ˜ì§‘í•©ì˜ í¬ê¸°) ì°¨ì›ì˜ score ë²¡í„° $y_{w_{t}}$ì— softmax í•¨ìˆ˜ë¥¼ ì ìš©í•œ $|V|$ì°¨ì›ì˜ í™•ë¥  ë²¡í„°ì´ë‹¤. NPLMì€ í™•ë¥  ë²¡í„°ì—ì„œ ê°€ì¥ ë†’ì€ ìš”ì†Œì˜ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ê°€ ì‹¤ì œ ì •ë‹µ ë‹¨ì–´ì™€ ì¼ì¹˜í•˜ë„ë¡ í•™ìŠµ í•œë‹¤. P(w_{t})|w_{t-1}, \\cdots ,w_{t-n+1}) = \\frac{exp(y_{w_{t}})}{\\sum_{i} exp(y_{i}) }y_{w_{t}} \\in R^{|V|} : w_{t}ë¼ëŠ” ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” ì ìˆ˜ ë²¡í„° NLPM êµ¬ì¡°ì˜ ì…ë ¥ ë¬¸ì¥ ë‚´ të²ˆì§¸ ë‹¨ì–´($w_{t}$)ì— ëŒ€ì‘í•˜ëŠ” ë‹¨ì–´ ë²¡í„° $x_{t}$ë¥¼ ë§Œë“œëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. ë¨¼ì € $|V| \\times m (m: x_{t}ì˜ ì°¨ì› ìˆ˜)$í¬ê¸°ë¥¼ ê°–ëŠ” í–‰ë ¬ Cì—ì„œ $w_{t}$ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ë¥¼ ì°¸ì¡°í•˜ëŠ” í˜•íƒœì´ë‹¤. C í–‰ë ¬ì˜ ì›ì†Œê°’ì€ ì´ˆê¸°ì— ëœë¤ ì„¤ì •í•œë‹¤. ì°¸ì¡°í•œë‹¤ëŠ” ì˜ë¯¸ëŠ” ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ ì–´íœ˜ ì§‘í•©ì— ì†í•œ ë‹¨ì–´ê°€ 5ê°œì´ê³  $w_{t}$ê°€ 4ë²ˆì§¸ ì¸ë±ìŠ¤ë¥¼ ì˜ë¯¸í•œë‹¤ê³  í•˜ë©´ í–‰ë ¬ $C$ì™€ $w_{t}$ì— í•´ë‹¹í•˜ëŠ” one-hot ë²¡í„°ë¥¼ ë‚´ì í•œ ê²ƒê³¼ ê°™ë‹¤. ì¦‰ $C$ë¼ëŠ” í–‰ë ¬ì—ì„œ $w_{t}$ì— í•´ë‹¹í•˜ëŠ” í–‰ë§Œ ì°¸ì¡°í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ì´ ê³¼ì •ì„ Look-up tableì´ë¼ëŠ” ìš©ì–´ë¡œ ë§ì´ ì‚¬ìš©í•œë‹¤ ë¬¸ì¥ ë‚´ ëª¨ë“  ë‹¨ì–´ë“¤ì„ í•œ ë‹¨ì–´ì”© í›‘ìœ¼ë©´ì„œ Corpus ì „ì²´ë¥¼ í•™ìŠµí•˜ê²Œ ëœë‹¤ë©´ NPLM ëª¨ë¸ì˜ C í–‰ë ¬ì— ê° ë‹¨ì–´ì˜ ë¬¸ë§¥ ì •ë³´ë¥¼ ë‚´ì¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤. x_{t} = w_{t} \\cdot C = C(w_{t}), C \\in R^{|v| \\times m} ëª¨ë¸ êµ¬ì¡° ë° ì˜ë¯¸ì •ë³´ì´ ë•Œ, walkingì„ ë§ì¶”ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•œ ì†ì‹¤(train loss)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê·¸ë˜ë””ì–¸íŠ¸(gradient)ë¥¼ ë°›ì•„ ê° ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” í–‰ë“¤ì´ ë™ì¼í•˜ê²Œ ì—…ë°ì´íŠ¸ ëœë‹¤. ë”°ë¼ì„œ ì´ ë‹¨ì–´ë“¤ì˜ ë²¡í„°ëŠ” ë²¡í„° ê³µê°„ì—ì„œ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ì¡°ê¸ˆì”© ì›€ì§ì¸ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ì„ë² ë”© ë²¡í„° ê³µê°„ì—ì„œ í•´ë‹¹ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ê±°ë¦¬ê°€ ê°€ê¹ë‹¤ëŠ” ê²ƒì€ ì˜ë¯¸ê°€ ìœ ì‚¬í•˜ë‹¤ë¼ëŠ” ì˜ë¯¸ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤. NPLMì˜ íŠ¹ì§• NPLMì€ ê·¸ ìì²´ë¡œ ì–¸ì–´ ëª¨ë¸ ì—­í• ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ê¸°ì¡´ì˜ í†µê³„ ê¸°ë°˜ n-gram ëª¨ë¸ì€ í•™ìŠµ ë°ì´í„°ì— í•œ ë²ˆë„ ë“±ì¥í•˜ì§€ ì•Šì€ íŒ¨í„´ì— ëŒ€í•´ì„œëŠ” ê·¸ ë“±ì¥ í™•ë¥ ì„ 0ìœ¼ë¡œ ë¶€ì—¬í•˜ëŠ” ë¬¸ì œì ì„ NPLMì€ ë¬¸ì¥ì´ Corpusì— ì—†ì–´ë„ ë¬¸ë§¥ì´ ë¹„ìŠ·í•œ ë‹¤ë¥¸ ë¬¸ì¥ì„ ì°¸ê³ í•´ í™•ë¥ ì„ ë¶€ì—¬í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê·¹ë³µí•˜ëŠ”ë° í° ì˜ì˜ê°€ ìˆë‹¤. í•˜ì§€ë§Œ, í•™ìŠµ íŒŒë¼ë¯¸í„°ê°€ ë„ˆë¬´ ë§ì•„ ê³„ì‚°ì´ ë³µì¡í•´ì§€ê³ , ìì—°ìŠ¤ëŸ½ê²Œ ëª¨ë¸ ê³¼ì í•©(overfitting)ì´ ë°œìƒí•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” í•œê³„ë¥¼ ê°€ì§„ë‹¤. ì´ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì„ë² ë”© ë°©ë²•ë¡ ì¸ Word2Vecì´ ë“±ì¥í•˜ì˜€ë‹¤. Word2VecWord2vecì€ ê°€ì¥ ë„ë¦¬ ì“°ì´ê³  ìˆëŠ” ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ì´ë‹¤. Skip-gramê³¼ CBOWë¼ëŠ” ëª¨ë¸ì´ ì œì•ˆë˜ì—ˆê³ , ì´ ë‘ ëª¨ë¸ì„ ê·¼ê°„ìœ¼ë¡œ í•˜ë˜ negative samplingë“± í•™ìŠµ ìµœì í™” ê¸°ë²•ì„ ì œì•ˆí•œ ë‚´ìš©ì´ í•µì‹¬ì´ë‹¤. CBOW ì£¼ë³€ì— ìˆëŠ” context wordë“¤ì„ ê°€ì§€ê³  target word í•˜ë‚˜ë¥¼ ë§Ÿì¶”ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤. ì…,ì¶œë ¥ ë°ì´í„° ìŒ {context words, target word} Skip-gram ì²˜ìŒ ì œì•ˆëœ ë°©ì‹ì€ target word í•˜ë‚˜ë¥¼ ê°€ì§€ê³  context wordë“¤ì´ ë¬´ì—‡ì¼ì§€ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤. í•˜ì§€ë§Œ, ì´ ë°©ì‹ì€ ì •ë‹µ ë¬¸ë§¥ ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚  í™•ë¥ ì€ ë†’ì´ê³  ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤ í™•ë¥ ì€ ê·¸ì— ë§ê²Œ ë‚®ì¶°ì•¼ í•œë‹¤. ê·¸ëŸ°ë° ì–´íœ˜ ì§‘í•©ì— ì†í•œ ë‹¨ì–´ ìˆ˜ëŠ” ë³´í†µ ìˆ˜ì‹­ë§Œ ê°œë‚˜ë˜ë¯€ë¡œ ì´ë¥¼ ëª¨ë‘ ê³„ì‚°í•˜ë ¤ë©´ ë¹„íš¨ìœ¨ ì ì´ë‹¤. ì´ëŸ° ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ negative samplingì´ë¼ëŠ” target wordì™€ context word ìŒì´ ì£¼ì–´ì¡Œì„ ë•Œ í•´ë‹¹ ìŒì´ positive sampleì¸ì§€ negative sampleì¸ì§€ ì´ì§„ ë¶„ë¥˜í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ì œì•ˆí–ˆë‹¤. ì´ëŸ°ë‹¤ë©´ í•™ìŠµ stepë§ˆë‹¤ 1ê°œì˜ positive sampleê³¼ ë‚˜ë¨¸ì§€ kê°œ(ì„ì˜ì˜ k:target ë‹¨ì–´ì˜ negative sampling ê°œìˆ˜)ë§Œ ê³„ì‚°í•˜ë©´ ë˜ë¯€ë¡œ ì°¨ì›ìˆ˜ê°€ 2ì¸ ì‹œê·¸ëª¨ì´ë“œë¥¼ k+1íšŒë§Œ ê³„ì‚°í•˜ë©´ëœë‹¤. ì´ì „ì˜ ë§¤ stepë§ˆë‹¤ ì–´íœ˜ ì§‘í•© í¬ê¸°ë§Œí¼ì˜ ì°¨ì›ì„ ê°–ëŠ” softmaxë¥¼ 1íšŒ ê³„ì‚°í•˜ëŠ” ë°©ë²•ë³´ë‹¤ ê³„ì‚°ëŸ‰ì´ í›¨ì”¬ ì ë‹¤. ë˜í•œ Corpusì—ì„œ ìì£¼ ë“±ì¥í•˜ì§€ ì•ŠëŠ” í¬ê·€í•œ ë‹¨ì–´ê°€ negative sampleë¡œ ì¡°ê¸ˆ ë” ì˜ ë½‘í ìˆ˜ ìˆë„ë¡ í•˜ê³  ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ëŠ” í•™ìŠµì—ì„œ ì œì™¸í•˜ëŠ” subsamplingì´ë¼ëŠ” ê¸°ë²•ì„ ì ìš©í•˜ì˜€ë‹¤. Skip-gramì€ Corpusë¡œ ë¶€í„° ì—„ì²­ë‚˜ê²Œ ë§ì€ í•™ìŠµ ë°ì´í„° ìŒì„ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê³ ë¹ˆë„ ë‹¨ì–´ì˜ ê²½ìš° ë“±ì¥ íšŸìˆ˜ë§Œí¼ ëª¨ë‘ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ë¹„íš¨ìš¸ì ì´ë¼ê³  ë³´ì•˜ë‹¤. ì´ ë˜í•œ, í•™ìŠµëŸ‰ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¤„ì—¬ ê³„ì‚°ëŸ‰ì„ ê°ì†Œì‹œí‚¤ëŠ” ì „ëµì´ë‹¤. ì‘ì€ CorpusëŠ” k=5~20, í° CorpusëŠ” k=2~5ë¡œ í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤. skip-gramì´ ê°™ì€ ë§ë­‰ì¹˜ë¡œë„ ë” ë§ì€ í•™ìŠµ ë°ì´í„°ë¥¼ í™•ë³´í•  ìˆ˜ ìˆì–´ ì„ë² ë”© í’ˆì§ˆì´ CBOWë³´ë‹¤ ì¢‹ì€ ê²½í–¥ì´ ìˆë‹¤. ì…,ì¶œë ¥ ë°ì´í„° ìŒ {target word, target word ì „ì „ ë‹¨ì–´}, {target word, target word ì§ì „ ë‹¨ì–´}, {target word, target word ë‹¤ìŒ ë‹¨ì–´}, {target word, target word ë‹¤ë‹¤ìŒ ë‹¨ì–´} negative sample Prob P_{negative}(w_{i}) = \\frac{U(w_{i})^{3/4}}{\\sum^{n}_{j=0}} U(w_{i]}^{3/4})U(w_{i]}) = \\frac{í•´ë‹¹ ë‹¨ì–´ ë¹ˆë„}{ì „ì²´ ë‹¨ì–´ ìˆ˜} = í•´ë‹¹ ë‹¨ì–´ì˜ Unigram Prob subsampling Prob P_{subsampling}(w_{i}) = 1 - \\sqrt(\\frac{t}{f(w_{i})}) = w_{i}ê°€ í•™ìŠµì—ì„œ ì œì™¸ë  í™•ë¥ f(w_{i]}) = w_{i]}'s frequency, t = 0.00001 t, cê°€ positive sample(=target word ì£¼ë³€ì— context wordê°€ ì¡´ì¬)ì¼ í™•ë¥  target wordì™€ contextê°€ ì‹¤ì œ positive sampleì´ë¼ë©´ ì•„ë˜ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ ìµœëŒ€í™”í•´ì•¼ í•œë‹¤. ëª¨ë¸ì˜ í•™ìŠµ parameterëŠ” Uì™€ V í–‰ë ¬ ë‘ê°œ ì¸ë°, ë‘˜ì˜ í¬ê¸°ëŠ” ì–´íœ˜ ì§‘í•© í¬ê¸°$(|V|) \\times ì„ë² ë”© ì°¨ì› ìˆ˜(d)$ë¡œ ë™ì¼í•˜ë‹¤. Uì™€ VëŠ” ê°ê° target wordì™€ context wordì— ëŒ€ì‘í•œë‹¤. P(+|t, c) = \\frac{1}{1 + exp(-u_{t}v_{c})} \u001cìœ„ì˜ ì‹ì„ ìµœëŒ€í™” í•˜ë ¤ë©´ ë¶„ëª¨ë¥¼ ì¤„ì—¬ì•¼í•œë‹¤. ë¶„ëª¨ë¥¼ ì¤„ì´ë ¤ë©´ $exp(-u_{t}v_{c})$ë¥¼ ì¤„ì—¬ì•¼ í•œë‹¤. ê·¸ëŸ¬ë ¤ë©´ ë‘ ë²¡í„°ì˜ ë‚´ì ê°’ì´ ì»¤ì§€ê²Œ í•´ì•¼í•œë‹¤. ì´ëŠ” ì½”ì‚¬ì¸ìœ ì‚¬ë„ì™€ ë¹„ë¡€í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤. ê²°ë¡ ì ìœ¼ë¡œ ë‘ ë²¡í„°ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë†’ì¸ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì˜ ì´í•´ê°€ ê°€ì§€ ì•ŠëŠ”ë‹¤ë©´ ì•„ë˜ê³¼ ê·¸ë¦¼ì„ ë³´ì. A ê°€ Bì— ì •í™•íˆ í¬ê°œì–´ì ¸ ìˆì„ ë•Œ(Î¸=0ë„) cos(Î¸)ëŠ” 1ì´ë‹¤. ë…¹ìƒ‰ì„ ì˜ ê¸¸ì´ê°€ ë‹¨ìœ„ì› ë°˜ì§€ë¦„ê³¼ ì¼ì¹˜í•˜ê¸° ë•Œë¬¸ì´ë‹¤. BëŠ” ê³ ì •í•œ ì±„ Aê°€ yì¶• ìƒë‹¨ìœ¼ë¡œ ì˜®ê²¨ê°„ë‹¤(Î¸ê°€ 0ë„ì—ì„œ 90ë„ë¡œ ì¦ê°€)ê³  í• ë•Œ cos(Î¸)ëŠ” ì ì  ê°ì†Œí•˜ì—¬ 0ì´ ë˜ê²Œ ë©ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì˜ ê²½ìš° ë¹¨ê°„ìƒ‰ ì§ì„ ì´ xì¶•ê³¼ ë§Œë‚˜ëŠ” ì ì´ ë°”ë¡œ cos(Î¸)ë¥¼ ì˜ë¯¸í•œë‹¤. t, cê°€ negative sample(target wordì™€ context wordê°€ ë¬´ê´€í• ë•Œ)ì¼ í™•ë¥  ë§Œì•½ í•™ìŠµë°ì´í„°ê°€ negative sampleì— í•´ë‹¹í•œë‹¤ë©´ ì•„ë˜ì˜ ì¡°ê±´ë¶€ í™•ë¥ ì„ ìµœëŒ€í™”í•˜ì—¬ì•¼ í•œë‹¤. ì´ ë•ŒëŠ” ë¶„ìë¥¼ ìµœëŒ€í™” í•´ì£¼ì–´ì•¼ í•˜ë¯€ë¡œ, ë‘ ë²¡í„°ì˜ ë‚´ì ê°’ì„ ì¤„ì—¬ì•¼ í•œë‹¤. P(-|t, c) = 1 - P(+|t,c) = \\frac{exp(-u_{t}v_{c})}{1 + exp(-u_{t}v_{c})} ëª¨ë¸ì˜ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì´ì œ ì•Œì•˜ìœ¼ë‹ˆ ìµœëŒ€í™”ë¥¼ í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ìœ¼ë ¤ë©´ MLEë¥¼ êµ¬í•´ì•¼ í•  ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ log likelihood functionì€ ì•„ë˜ì™€ ê°™ì„ ê²ƒì´ë‹¤. ì„ì˜ì˜ ëª¨ìˆ˜ì¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì¸ $\\theta$ë¼ê³  ê°€ì • í–ˆì„ë•Œ, $\\theta$ë¥¼ í•œ ë²ˆ ì—…ë°ì´íŠ¸í•  ë•Œ 1ê°œ ìŒì˜ positive sampleê³¼ kê°œì˜ negative sampleì´ í•™ìŠµëœë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. Word2vecì€ ê²°êµ­ ë‘ ë‹¨ì–´ë²¡í„°ì˜ ìœ ì‚¬ë„ ë¿ë§Œì•„ë‹ˆë¼ ì „ì²´ Corpus ë¶„í¬ ì •ë³´ë¥¼ ë‹¨ì–´ Embeddingì— í•¨ì¶•ì‹œí‚¤ê²Œ ëœë‹¤. ë¶„í¬ê°€ ìœ ì‚¬í•œ ë‹¨ì–´ ìŒì€ ê·¸ ì†ì„± ë˜í•œ ê³µìœ í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. ìœ ì‚¬ë„ ê²€ì‚¬ë¥¼ í†µí•´ ë¹„ìŠ·í•œ ë‹¨ì–´ë“¤ì„ ì¶œë ¥ í–ˆì„ë•Œ, ê·¸ ë‹¨ì–´ë“¤ì´ ë°˜ë“œì‹œ ìœ ì˜ ê´€ê³„ë¥¼ ë³´ì—¬ì¤€ë‹¤ê¸° ë³´ë‹¤ëŠ” ë™ì¼í•œ ì†ì„±ì„ ê°–ëŠ” ê´€ë ¨ì„±ì´ ë†’ì€ ë‹¨ì–´ë¥¼ ì¶œë ¥í•œë‹¤ëŠ” ì˜ë¯¸ë¡œ ì´í•´í•´ì•¼í•œë‹¤. ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œë˜ë©´ U(target_wordì— ê´€í•œ í–‰ë ¬)ë§Œ dì°¨ì›ì˜ ë‹¨ì–´ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆê³ , U+V.t í–‰ë ¬ì„ ì„ë² ë”©ìœ¼ë¡œ ì“¸ ìˆ˜ë„ ìˆë‹¤. í˜¹ì€ concatenate([U, V.t])ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. L(\\theta) = log P (+|t_{p},c_{p}) + \\sum^{k}_{i=1} log P (-|t_{n_{i}},c_{n_{i}})ì°¸ê³ 12345678910111213141516171819202122232425262728293031323334353637# from gensim.models import word2vecfrom gensim.models import Word2Veccorpus = 'ì›í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ tokenizeí•œ ìƒíƒœì˜ íŒŒì¼(ê°€ì¥ ì‰½ê²ŒëŠ” ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ)'# model = word2vec.Word2Vec()model = Word2Vec(corpus, size=ì„ë² ë”© íŠ¹ì§• ë²¡í„° ì°¨ì›ìˆ˜, # ëª¨ë¸ì— ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë¥¼ ê°€ì§€ê³  í•™ìŠµí•˜ê¸° ìœ„í•´ ì ì€ ë¹ˆë„ ìˆ˜ì˜ ë‹¨ì–´ë“¤ì€ í•™ìŠµí•˜ì§€ ì•ŠëŠ”ë‹¤. min_count=ë‹¨ì–´ì— ëŒ€í•œ ìµœì†Œ ë¹ˆë„ ìˆ˜, # default negative=5, ë³´\u001cí†µ 5~20ì„ ë§ì´ ì‚¬ìš© negative=negative sampleì„ ë½‘ëŠ” k, workers=í•™ìŠµì‹œ ì‚¬ìš©í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜, window=context window í¬ê¸°, # í•™ìŠµì„ ìˆ˜í–‰í•  ë•Œ ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ ì •ë‹µ ë‹¨ì–´ ë¼ë²¨ì— ëŒ€í•œ ë‹¤ìš´ìƒ˜í”Œë§ ë¹„ìœ¨ì„ ì§€ì •í•œë‹¤. # ìœ ìš©í•œ ë²”ìœ„ëŠ” (0, 1e-5)ì´ë©°, ë³´í†µ 0.001ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ê³  í•œë‹¤. sample=ë‹¤ìš´ ìƒ˜í”Œë§ ë¹„ìœ¨, # default sg=0 =&gt; CBOW, if sg=1 =&gt; skip-gram sg=1 )model.save(\"ëª¨ë¸ì„ ì €ì¥í•  directory path\")# ì €ì¥í–ˆë˜ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ì„œ ì¶”ê°€ì ìœ¼ë¡œ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆë‹¤.model = Word2Vec.load(\"ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ëª¨ë¸ì˜ directory path\")model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)# í›ˆë ¨ëœ ë²¡í„°ë¥¼ KeyedVectorë¡œ ë¶„ë¦¬í•˜ëŠ” ì´ìœ ëŠ” ì „ì²´ ëª¨ë¸ ìƒíƒœê°€ ë” ì´ìƒ í•„ìš”í•˜ì§€ ì•Šì„ ê²½ìš°(í›ˆë ¨ì„ ê³„ì†í•  í•„ìš”ê°€ ì—†ìŒ)# ëª¨ë¸ì´ íê¸°ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— í”„ë¡œì„¸ìŠ¤ ê°„ì— RAMì˜ ë²¡í„°ë¥¼ ë¹ ë¥´ê²Œ ë¡œë“œí•˜ê³  ê³µìœ í•  ìˆ˜ ìˆëŠ” í›¨ì”¬ ì‘ê³  ë¹ ë¥¸ ìƒíƒœë¡œ ë§Œë“œëŠ” ê²ƒì´ë‹¤.vector = model.wv['computer']from gensim.models import KeyedVectorspath = get_tmpfile(\"wordvectors íŒŒì¼ëª…\")model.wv.save(path)wv = KeyedVectors.load(\"model.wv\", mmap='r')vector = wv['computer'] í•™ìŠµì´ ì™„ë£Œëœ ì„ë² ë”© ê²°ê³¼ë¬¼ì„ í™œìš”í•˜ì—¬ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ë‹¨ì–´ë“¤ì„ ë½‘ì•„ ì„ë² ë”©ì„ í‰ê°€í•´ ë³¼ ìˆ˜ë„ ìˆë‹¤. ì´ëŠ” ì¶”í›„ì— í•œë²ˆì— ì†Œê°œí•  ê²ƒì´ë‹¤. FastText Facebookì—ì„œ ê°œë°œí•´ ê³µê°œí•œ ë‹¨ì–´ ì„ë² ë”© ê¸°ë²•ì´ë‹¤. ê° ë‹¨ì–´ë¥¼ ë¬¸ìë‹¨ìœ„ n-gramìœ¼ë¡œ í‘œí˜„í•œë‹¤. ì´ì™¸ì˜ ì ì€ ëª¨ë‘ Word2Vecê³¼ ê°™ë‹¤. ë™ì¼í•˜ê²Œ negative samplingì„ ì‚¬ìš©í•˜ë©°, ì¡°ê¸ˆ ë‹¤ë¥¸ ì ì€ FasttextëŠ” target word(t), context word(c) ìŒì„ í•™ìŠµí•  ë•Œ target word(t)ì— ì†í•œ ë¬¸ì ë‹¨ìœ„ n-gram ë²¡í„°(z)ë“¤ì„ ëª¨ë‘ ì—…ë°ì´íŠ¸ í•œë‹¤ëŠ” ì ì´ë‹¤. ì„¤ì¹˜ ë°©ë²•ì€ gensimì—ì„œ FastTextë¥¼ ì œê³µí•˜ê³  ìˆê¸°ì— pipë¥¼ í†µí•´ ì„¤ì¹˜í•´ì£¼ê±°ë‚˜ ì´ ë°©ë²•ì´ ì•ˆëœë‹¤ë©´, ì°¸ì¡°í˜ì´ì§€ë¥¼ í´ë¦­í•´ì„œ ì§ì ‘ C++ë°©ì‹ìœ¼ë¡œ ë°›ì•„ë„ ìƒê´€ì—†ë‹¤. ëª¨ë¸ ê¸°ë³¸ êµ¬ì¡° ì˜ˆë¥¼ ë“¤ì–´ ì‹œë‚˜ë¸Œë¡œë¼ëŠ” ë‹¨ì–´ì˜ ë¬¸ì ë‹¨ìœ„ 3-gramì€ ë‹¤ìŒê³¼ ê°™ì´ n-gram ë²¡í„°ì˜ í•©ìœ¼ë¡œ í‘œí˜„í•œë‹¤. ì•„ë˜ ì‹ì—ì„œ $G_{t}$ëŠ” target word tì— ì†í•œ ë¬¸ì ë‹¨ìœ„ n-gramì§‘í•©ì„ ì˜ë¯¸í•œë‹¤. Fasttextì˜ ë‹¨ì–´ ë²¡í„° í‘œí˜„(&lt;,&gt;ëŠ” ë‹¨ì–´ì˜ ê²½ê³„ë¥¼ ë‚˜íƒ€ë‚´ ì£¼ê¸° ìœ„í•´ ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” ê¸°í˜¸) u_{ì‹œë‚˜ë¸Œë¡œ} = z_{} + z_{ì‹œë‚˜ë¸Œë¡œ}, u_{t}=\\sum_{g \\in G_{t}} z_{g} n-gram ì°¸ì¡° ë° NLPì— ë„ì›€ì´ ë˜ëŠ” ì‚¬ì´íŠ¸ nì„ ì‘ê²Œ ì„ íƒí•˜ë©´ í›ˆë ¨ ì½”í¼ìŠ¤ì—ì„œ ì¹´ìš´íŠ¸ëŠ” ì˜ ë˜ê² ì§€ë§Œ ê·¼ì‚¬ì˜ ì •í™•ë„ëŠ” í˜„ì‹¤ì˜ í™•ë¥ ë¶„í¬ì™€ ë©€ì–´ì§„ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ì ì ˆí•œ nì„ ì„ íƒí•´ì•¼ í•œë‹¤. trade-off ë¬¸ì œë¡œ ì¸í•´ ì •í™•ë„ë¥¼ ë†’ì´ë ¤ë©´ nì€ ìµœëŒ€ 5ë¥¼ ë„˜ê²Œ ì¡ì•„ì„œëŠ” ì•ˆ ëœë‹¤ê³  ê¶Œì¥ë˜ê³  ìˆë‹¤. ì†ì‹¤í•¨ìˆ˜ ìì²´ëŠ” ìœ„ì˜ ì‹ì„ word2vec ì†ì‹¤í•¨ìˆ˜ $u_{t}$ì— ëŒ€ì…í•´ ì£¼ê¸°ë§Œ í•˜ë©´ëœë‹¤. FastText ëª¨ë¸ì˜ ê°•ì ì€ ì¡°ì‚¬ë‚˜ ì–´ë¯¸ê°€ ë°œë‹¬í•œ í•œêµ­ì–´ì— ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. ìš©ì–¸(ë™ì‚¬, í˜•ìš©ì‚¬)ì˜ í™œìš©ì´ë‚˜ ê·¸ì™€ ê´€ê³„ëœ ì–´ë¯¸ë“¤ì´ ë²¡í„° ê³µê°„ìƒ ê°€ê¹ê²Œ ì„ë² ë”© ë˜ê¸° ë•Œë¬¸ì´ë‹¤.(ì˜ˆë¥¼ë“¤ë©´, â€˜í•˜ì˜€ë‹¤â€™ê°€ tì´ê³ , â€˜ê³µë¶€â€™ê°€ cë¼ë©´ â€˜ê³µë¶€â€™ì™€ â€˜í–ˆ(ë‹¤), í•˜(ë‹¤), í•˜(ì˜€ìœ¼ë©°)â€™ë“±ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ë„ ë¹„ìŠ·í•œ ê³µê°„ìƒì— ìˆë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.) í•œê¸€ì€ ìì†Œ ë‹¨ìœ„(ì´ˆì„±, ì¤‘ì„±, ì¢…ì„±)ë¡œ ë¶„í•´í•  ìˆ˜ ìˆê³ , ì´ ìì†Œ ê°ê°ì„ í•˜ë‚˜ì˜ ë¬¸ìë¡œ ë³´ê³  FastTextë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆë‹¤ëŠ” ì ë„ ê°•ì ì´ë‹¤. ë˜í•œ, ê° ë‹¨ì–´ì˜ ì„ë² ë”©ì„ ë¬¸ì ë‹¨ìœ„ n-gram ë²¡í„°ì˜ í•©ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ë•Œë¬¸ì— ì˜¤íƒ€ë‚˜ ë¯¸ë“±ë¡ë‹¨ì–´(unknown word)ì—ë„ robustí•˜ë‹¤. ê·¸ë˜ì„œ ë¯¸ë“±ë¡ëœ ë‹¨ì–´ë„ ë²¡í„°ë¥¼ ë½‘ì•„ë‚¼ìˆ˜ ìˆë‹¤. ë™ì¼í•œ ìŒì ˆì´ë‚˜ ë‹¨ì–´ë¥¼ ê°€ì§„ ê³µê°„ìƒì˜ ë²¡í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë‹¤ë¥¸ ë‹¨ì–´ ì„ë² ë”© ê¸°ë²•ì´ ë¯¸ë“±ë¡ ë‹¨ì–´ ë²¡í„°ë¥¼ ì•„ì˜ˆ ì¶”ì¶œí•  ìˆ˜ ì—†ë‹¤ëŠ” ì‚¬ì‹¤ì„ ê°ì•ˆí•˜ë©´ FastTextëŠ” ê²½ìŸë ¥ì´ ìˆë‹¤. Fasttext ì°¸ì¡° 123456789101112131415161718192021222324from gensim.models import FastTextcorpus = 'ì›í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ tokenizeí•œ ìƒíƒœì˜ íŒŒì¼(ê°€ì¥ ì‰½ê²ŒëŠ” ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ)'model = Word2Vec(corpus, size=ì„ë² ë”© íŠ¹ì§• ë²¡í„° ì°¨ì›ìˆ˜, # ëª¨ë¸ì— ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë¥¼ ê°€ì§€ê³  í•™ìŠµí•˜ê¸° ìœ„í•´ ì ì€ ë¹ˆë„ ìˆ˜ì˜ ë‹¨ì–´ë“¤ì€ í•™ìŠµí•˜ì§€ ì•ŠëŠ”ë‹¤. min_count=ë‹¨ì–´ì— ëŒ€í•œ ìµœì†Œ ë¹ˆë„ ìˆ˜, # default negative=5, ë³´\u001cí†µ 5~20ì„ ë§ì´ ì‚¬ìš© negative=negative sampleì„ ë½‘ëŠ” k, workers=í•™ìŠµì‹œ ì‚¬ìš©í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜, window=context window í¬ê¸°, # í•™ìŠµì„ ìˆ˜í–‰í•  ë•Œ ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ ì •ë‹µ ë‹¨ì–´ ë¼ë²¨ì— ëŒ€í•œ ë‹¤ìš´ìƒ˜í”Œë§ ë¹„ìœ¨ì„ ì§€ì •í•œë‹¤. # ìœ ìš©í•œ ë²”ìœ„ëŠ” (0, 1e-5)ì´ë©°, ë³´í†µ 0.001ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ê³  í•œë‹¤. sample=ë‹¤ìš´ ìƒ˜í”Œë§ ë¹„ìœ¨, # default sg=0 =&gt; CBOW, if sg=1 =&gt; skip-gram sg=1, # default word_ngrams=1 =&gt; n-gram ì‚¬ìš©, 0 =&gt; ë¯¸ì‚¬ìš©(word2vecê³¼ ë™ì¼) word_ngrams=1, # n-gram ìµœì†Œ ë‹¨ìœ„ min_n=3, # n-gram ìµœëŒ€ ë‹¨ìœ„ (ìµœì†Œë‹¨ìœ„ë³´ë‹¨ ì»¤ì•¼í•œë‹¤.) max_n=6, ) ì ì¬ ì˜ë¯¸ ë¶„ì„(LSA, Latent Semantic Analysis) word-document í–‰ë ¬ì´ë‚˜ TF-IDF í–‰ë ¬, word-context í–‰ë ¬ ê°™ì€ ì»¤ë‹¤ë€ í–‰ë ¬ì— ì°¨ì› ì¶•ì†Œ ë°©ë²•ì˜ ì¼ì¢…ì¸ íŠ¹ì´ê°’ ë¶„í•´ë¥¼ ìˆ˜í–‰í•´ ë°ì´í„°ì˜ ì°¨ì› ìˆ˜ë¥¼ ì¤„ì—¬ ê³„ì‚° íš¨ìœ¨ì„±ì„ í‚¤ìš°ëŠ” í•œí¸ í–‰ê°„ì— ìˆ¨ì–´ìˆëŠ” ì ì¬ ì˜ë¯¸ë¥¼ ì¶”ì¶œí•´ë‚´ëŠ” ë°©ë²•ë¡ ì´ë‹¤. ì˜ˆë¥¼ ë“¤ë©´, word-documents í–‰ë ¬ì´ë‚˜ word-context í–‰ë ¬ ë“±ì— SVDë¥¼ í•œ ë‹¤ìŒ ê·¸ ê²°ê³¼ë¡œ ë„ì¶œë˜ëŠ” í–‰ë²¡í„°ë“¤ì„ ë‹¨ì–´ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì ì¬ ì˜ë¯¸ ë¶„ì„ì€ GloVeë‚˜ Swivelê³¼ ë”ë¶ˆì–´ Matrix Factorization ê¸°ë°˜ì˜ ê¸°ë²•ìœ¼ë¡œ ë¶„ë¥˜ëœë‹¤. PPMI(ì ë³„ ìƒí˜¸ ì •ë³´ëŸ‰) í–‰ë ¬ word-document í–‰ë ¬, TF-IDF í–‰ë ¬, word-context í–‰ë ¬, PMI í–‰ë ¬ì— ëª¨ë‘ LSAë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ì´ ì¤‘ PMI í–‰ë ¬ì„ ë³´ì™„í•˜ëŠ” PPMI í–‰ë ¬ì— ëŒ€í•´ ì†Œê°œí•˜ê³ ìí•œë‹¤. PMI í–‰ë ¬ê³¼ ìœ„ì˜ í–‰ë ¬ë“¤ì„ ëª¨ë¥¸ë‹¤ë©´ í´ë¦­! PPMIë€ ê°„ë‹¨íˆ ë§í•´ ìš°ë¦¬ê°€ ê°€ì§„ ë§ë­‰ì¹˜ì˜ í¬ê¸°ê°€ ì¶©ë¶„íˆ í¬ì§€ ì•Šë‹¤ë©´, PMIì‹ì˜ ë¡œê·¸ ì•ˆ ë¶„ìê°€ ë¶„ëª¨ë³´ë‹¤ ì‘ì„ ë•Œ ìŒìˆ˜ê°€ ë˜ê±°ë‚˜, ê·¹ë‹¨ì ìœ¼ë¡œ ë‹¨ì–´ A,Bê°€ ë‹¨ í•œë²ˆë„ ê°™ì´ ë“±ì¥í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ $-inf$ê°’ì„ ê°–ê²Œ ëœë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ NLP ë¶„ì•¼ì—ì„œëŠ” PMI ëŒ€ì‹  PPMI(Positive Pointwise Mutual Information)ë¥¼ ì§€í‘œë¡œ ì‚¬ìš©í•œë‹¤. PMIê°€ ì–‘ìˆ˜ê°€ ì•„ë‹Œ ê²½ìš° ê·¸ ê°’ì„ ì‹ ë¢°í•˜ê¸° í˜ë“¤ì–´ 0ìœ¼ë¡œ ì¹˜í™˜í•´ ë¬´ì‹œí•œë‹¤ëŠ” ëœ»ì´ë‹¤. PPMI(A, B) = max(PMI(A,B), 0) Shifted PMI(SPMI)ëŠ” Word2Vecê³¼ ê¹Šì€ ì—°ê´€ì´ ìˆë‹¤ëŠ” ë…¼ë¬¸ì´ ë°œí‘œë˜ê¸°ë„ í–ˆë‹¤. SPMI(A, B) = PMI(A, B) - log k, k > 0í–‰ë ¬ ë¶„í•´ë¡œ ì´í•´í•˜ëŠ” ì ì¬ ì˜ë¯¸ ë¶„ì„ Eigenvalue Decomposition(ê³ ìœ ê°’ ë¶„í•´)ë¥¼ ìš°ì„  ì•Œê³  ìˆë‹¤ëŠ” ì „ì œì¡°ê±´ìœ¼ë¡œ SVDë¥¼ ëª¨ë¥´ì‹¤ìˆ˜ë„ ìˆëŠ” ë¶„ë“¤ì„ ìœ„í•´ ê°„ëµíˆ ì„¤ëª…í•˜ìë©´, ê³ ìœ ê°’ ë¶„í•´ëŠ” í–‰ë ¬ Aê°€ ì •ë°©í–‰ë ¬ì¼ ê²½ìš°ë§Œ ê°€ëŠ¥í•œë°, ë§Œì•½ ì •ë°©í–‰ë ¬ì´ ì•„ë‹Œ í–‰ë ¬ì€ ê³ ìœ ê°’ ë¶„í•´ë¥¼ ì–´ë–»ê²Œ í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ê°œë…ì´ë¼ê³  ë§í•  ìˆ˜ ìˆê² ë‹¤. í˜¹ì‹œ ê³ ìœ ê°’ ë¶„í•´ë„ ì˜ ëª¨ë¥´ì‹œê² ë‹¤ë©´ ì´ê³³ì„ í´ë¦­í•´ì„œ í•„ìê°€ ì¶”ì²œí•˜ëŠ” ê°•ì˜ë“¤ì„ ê¼­ ê³µë¶€í•´ ë³´ì‹œê¸¸ ì¶”ì²œí•œë‹¤. í•„ìëŠ” ê°œì¸ì ìœ¼ë¡œ ì„ í˜•ëŒ€ìˆ˜ëŠ” Computer Science(or ë°ì´í„° ë¶„ì„)ë¥¼ í•˜ëŠ”ë° ê¸°ë³¸ì ìœ¼ë¡œ ì–´ëŠ ì •ë„ ì•Œê³  ìˆì–´ì•¼ í•œë‹¤ê³  ìƒê°í•œë‹¤. ì°¸ì¡° ì˜ˆë¥¼ ë“¤ì–´, í–‰ë ¬ Aì˜ mê°œì˜ word, nê°œ documentsë¡œ ì´ë£¨ì–´ì ¸ shapeì´ $ m \\times n $ì¸ word-documents í–‰ë ¬ì— truncated SVDë¥¼ í•˜ì—¬ LSAë¥¼ ìˆ˜í–‰í•œë‹¤ê³  ê°€ì •í•´ë³¸ë‹¤. ê·¸ë ‡ë‹¤ë©´ UëŠ” ë‹¨ì–´ ì„ë² ë”©, V.tëŠ” ë¬¸ì„œ ì„ë² ë”©ì— ëŒ€ì‘í•œë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ mê°œ ë‹¨ì–´, mê°œ ë‹¨ì–´ë¡œ ì´ë£¨ì–´ì§„ PMI í–‰ë ¬ì— LSAë¥¼ í•˜ë©´ dì°¨ì› í¬ê¸°ì˜ ë‹¨ì–´ ì„ë² ë”©ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ê°ì¢… ì—°êµ¬ë“¤ì— ë”°ë¥´ë©´ LSAë¥¼ ì ìš©í•˜ë©´ ë‹¨ì–´ì™€ ë¬¸ë§¥ ê°„ì˜ ë‚´ì¬ì ì¸ ì˜ë¯¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë³´ì¡´í•  ìˆ˜ ìˆê²Œ ë¼ ê²°ê³¼ì ìœ¼ë¡œ ë¬¸ì„œ ê°„ ìœ ì‚¬ë„ ì¸¡ì • ë“± ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ë˜í•œ ì…ë ¥ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆ, sparsity(í¬ì†Œ)ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤. í–‰ë ¬ ë¶„í•´ë¡œ ì´í•´í•˜ëŠ” Word2Vec negative sampling ê¸°ë²•ìœ¼ë¡œ í•™ìŠµëœ Word2Vecì˜ Skip-gram ëª¨ë¸(SGNS, Skip-Gram with Negative Sampling)ì€ Shifted PMI í–‰ë ¬ì„ ë¶„í•´í•œ ê²ƒê³¼ ê°™ë‹¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. $A_{ij}$ëŠ” SPMIí–‰ë ¬ì˜ i,jë²ˆì§¸ ì›ì†Œì´ë‹¤. këŠ” Skip-gram ëª¨ë¸ì˜ negative sample ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ k=1ì¸ negative sample ìˆ˜ê°€ 1ê°œì¸ Skip-gram ëª¨ë¸ì€ PMI í–‰ë ¬ì„ ë¶„í•´í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. A^{SGNS}_{ij} = U_{i} \\cdot V_{j} = PMI(i,j) - log k soynlpì—ì„œ ì œê³µí•˜ëŠ” sent_to_word_contexts_matrix í•¨ìˆ˜ë¥¼ í™œìš©í•˜ë©´ word-context í–‰ë ¬ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤. dynamic_weight=TrueëŠ” target wordì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ì¹´ìš´íŠ¸í•˜ëŠ” ë™ì‹œ ë“±ì¥ ì ìˆ˜(co-occurrence score)ë¥¼ ì¡°ê¸ˆì”© ê¹ëŠ”ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. dynamic_weight=Falseë¼ë©´ window ë‚´ì— í¬í•¨ëœ context wordë“¤ì˜ ë™ì‹œ ë“±ì¥ ì ìˆ˜ëŠ” target wordì™€ì˜ ê±°ë¦¬ì™€ ê´€ê³„ ì—†ì´ ëª¨ë‘ 1ë¡œ ê³„ì‚°í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ window=3ì´ê³  â€˜ë„ëŒ€ì²´ ì–¸ì œì¯¤ì´ë©´ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ë¶„ì•¼ë¥¼ ì¡°ê¸ˆì€ ê³µë¶€í–ˆë‹¤ê³  ë§í•  ìˆ˜ ìˆì„ê¹Œâ€¦â€™ë¼ëŠ” ë¬¸ì¥ì˜ target wordê°€ â€˜ë¶„ì•¼â€™ë¼ë©´, â€˜ë¥¼â€™ê³¼ â€˜ì‚¬ì´ì–¸ìŠ¤â€™ì˜ ë™ì‹œ ë“±ì¥ ì ìˆ˜ëŠ” 1, â€˜ë°ì´í„°â€™, â€˜ì¡°ê¸ˆâ€™ì€ 0.66, â€˜ì€â€™, â€˜ì´ë©´â€™ì€ 0.33ì´ ëœë‹¤. word-context í–‰ë ¬ì„ í™œìš©í•œ LSA 12345678910111213141516from sklearn.decomposition import TruncatedSVDfrom soynlp.vectorizer import sent_to_word_contexts_matrixcorpus_filecorpus = [sent.replace('\\n', '').strip() for sent in open(corpus_file, 'r').readlines()]input_matrix, idx2vocab = sent_to_word_contexts_matrix(corpus, window=3, # ìµœì†Œ ë‹¨ì–´ ë¹ˆë„ ìˆ˜ min_tf=10, dynamic_weight=True, verbose=True)cooc_svd = TruncatedSVD(n_components=100)cooc_vecs = cooc_svd.fit_transform(input_matrix) êµ¬ì¶•í•œ word-context í–‰ë ¬ì— soynlpì—ì„œ ì œê³µí•˜ëŠ” pmi í•¨ìˆ˜ë¥¼ ì ìš©í•œë‹¤. min_pmi ë³´ë‹¤ ë‚®ì€ PMI ê°’ì€ 0ìœ¼ë¡œ ì¹˜í™˜í•œë‹¤. ë”°ë¼ì„œ min_pmi=0ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ì •í™•íˆ PPMIì™€ ê°™ë‹¤. ë˜í•œ, pmi matrixì˜ ì°¨ì›ìˆ˜ëŠ” ì–´íœ˜ ìˆ˜ x ì–´íœ˜ ìˆ˜ì˜ ì •ë°© í–‰ë ¬ì´ë‹¤. 1234from soynlp import pmippmi_matrix, _, _ = pmi(input_matrix, min_pmi=0)ppmi_svd = TruncatedSVD(n_components=100)ppmi_vecs = ppmi_svd.fit_transform(input_matrix) GloVe(Global Word Vectors) ë¯¸êµ­ ìŠ¤íƒ í¬íŠ¸ëŒ€í•™êµì—°êµ¬íŒ€ì—ì„œ ê°œë°œí•œ ë‹¨ì–´ ì„ë² ë”© ê¸°ë²•ì´ë‹¤. ì„ë² ë”©ëœ ë‹¨ì–´ ë²¡í„° ê°„ ìœ ì‚¬ë„ ì¸¡ì •ì„ ìˆ˜ì›”í•˜ê²Œ í•˜ë©´ì„œë„ Corpus ì „ì²´ì˜ í†µê³„ ì •ë³´ë¥¼ ì¢€ ë” ì˜ ë°˜ì˜í•˜ëŠ” ê²ƒì„ ì§€í–¥í•˜ì—¬ Vanilla Word2Vecê³¼ LSA ë‘ ê¸°ë²•ì˜ ë‹¨ì ì„ ê·¹ë³µí•˜ê³ ì í–ˆë‹¤. LSA(ì ì¬ ì˜ë¯¸ ë¶„ì„)ì€ Corpus ì „ì²´ì˜ í†µê³„ëŸ‰ì„ ëª¨ë‘ í™œìš©í•  ìˆ˜ ìˆì§€ë§Œ, ê·¸ ê²°ê³¼ë¬¼ë¡œ ë‹¨ì–´ ê°„ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ê¸°ëŠ” ì–´ë µë‹¤. ë°˜ëŒ€ë¡œ Vanilla Word2Vecì€ ë‹¨ì–´ ë²¡í„° ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ë°ëŠ” LSAë³´ë‹¤ ìœ ë¦¬í•˜ì§€ë§Œ ì‚¬ìš©ìê°€ ì§€ì •í•œ window ë‚´ì˜ local contextë§Œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— Corpus ì „ì²´ì˜ í†µê³„ ì •ë³´ëŠ” ë°˜ì˜ë˜ê¸° ì–´ë µë‹¤ëŠ” ë‹¨ì ì„ ì§€ë‹Œë‹¤. ë¬¼ë¡  GloVe ì´í›„ ë°œí‘œëœ Skip-gram ëª¨ë¸ì´ Corpus ì „ì²´ì˜ Globalí•œ í†µê³„ëŸ‰ì¸ SPMI í–‰ë ¬ì„ ë¶„í•´í•˜ëŠ” ê²ƒê³¼ ë™ì¹˜ë¼ëŠ” ì ì„ ì¦ëª…í•˜ê¸°ëŠ” í–ˆë‹¤. ì†ì‹¤ í•¨ìˆ˜ ì„ë² ë”©ëœ ë‘ ë‹¨ì–´ ë²¡í„°ì˜ ë‚´ì ì´ ë§ë­‰ì¹˜ ì „ì²´ì—ì„œì˜ ë™ì‹œ ì¦ì¥ ë¹ˆë„ì˜ ë¡œê·¸ ê°’ì´ ë˜ë„ë¡ ì •ì˜í–ˆë‹¤. ë‹¨ì–´ i,j ê°ê°ì— í•´ë‹¹í•˜ëŠ” ë²¡í„° $U_{i}$, $V_{j}$ ì‚¬ì´ì˜ ë‚´ì ê°’ê³¼ ë‘ ë‹¨ì–´ ë™ì‹œ ë“±ì¥ ë¹ˆë„ $A_{ij}$ì˜ ë¡œê·¸ê°’ ì‚¬ì´ì˜ ì°¨ì´ê°€ ìµœì†Œí™”ë ìˆ˜ë¡ í•™ìŠµ ì†ì‹¤ì´ ì‘ì•„ì§„ë‹¤. biasí•­ 2ê°œì™€ f(A_{ij})ëŠ” ì„ë² ë”© í’ˆì§ˆì„ ë†’ì´ê¸° ìœ„í•´ ê³ ì•ˆëœ ì¥ì¹˜ì´ë‹¤. GloveëŠ” word-context í–‰ë ¬ Aë¥¼ ë§Œë“  í›„ì— í•™ìŠµì´ ëë‚˜ë©´ Uë¥¼ ë‹¨ì–´ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ U+V.t, concatenate([U, V.t])ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. J = \\sum^{|V|}_{i,j=1} f(A_{ij}) (U_{i} \\cdot V_{j} + b_{i} + b+{j} - log A_{ij})^{2}Swivel Google ì—°êµ¬íŒ€ì´ ë°œí‘œí•œ í–‰ë ¬ ë¶„í•´ ê¸°ë°˜ì˜ ë‹¨ì–´ ì„ë² ë”© ê¸°ë²•ì´ë‹¤. PMI í–‰ë ¬ì„ Uì™€ Vë¡œ ë¶„í•´í•˜ê³ , í•™ìŠµì´ ì¢…ë£Œë˜ë©´ Uë¥¼ ë‹¨ì–´ ì„ë² ë”©ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆìœ¼ë©° U+V.t, concatenate([U, V.t])ë„ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. PMI í–‰ë ¬ì„ ë¶„í•´í•œë‹¤ëŠ” ì ì—ì„œ word-context í–‰ë ¬ì„ ë¶„í•´í•˜ëŠ” GloVeì™€ ë‹¤ë¥´ë©°, Swivelì€ ëª©ì í•¨ìˆ˜ë¥¼ PMIì˜ ë‹¨ì ì„ ë³´ì™„í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„í–ˆë‹¤. ë‘ ë‹¨ì–´ê°€ í•œë²ˆë„ ë™ì‹œì— ë“±ì¥í•˜ì§€ ì•Šì•˜ì„ ê²½ìš° PMIê°€ -infë¡œ ê°€ëŠ¥ í˜„ìƒì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ì´ëŸ°ê²½ìš°ì˜ ì†ì‹¤í•¨ìˆ˜ë¥¼ ë”°ë¡œ ì •ì˜í–ˆë‹¤. ê·¸ ê²°ê³¼, i,jê°€ ê°ê° ê³ ë¹ˆë„ ë‹¨ì–´ì¸ë° ë‘ ë‹¨ì–´ì˜ ë™ì‹œ ë“±ì¥ë¹ˆë„ê°€ 0ì´ë¼ë©´ ë‘ ë‹¨ì–´ëŠ” ì •ë§ë¡œ ë“±ì¥í•˜ì§€ ì•ŠëŠ” ì˜ë¯¸ìƒ ë¬´ê´€ê³„í•œ ë‹¨ì–´ë¼ê³  ê°€ì •í•˜ê³ , ë‹¨ì–´ i,jê°€ ì €ë¹ˆë„ ë‹¨ì–´ì¸ë° ë‘ ë‹¨ì–´ì˜ ë™ì‹œ ë“±ì¥ë¹ˆë„ê°€ 0ì¸ ê²½ìš°ì—ëŠ” ë‘ ë‹¨ì–´ëŠ” ì˜ë¯¸ìƒ ê´€ê³„ê°€ ì¼ë¶€ ìˆì„ ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•œë‹¤. ë‹¨ì–´ ì„ë² ë”© í‰ê°€ ë°©ë²• ì°¸ê³ ë¡œ ì¹´ì¹´ì˜¤ë¸Œë ˆì¸ ë°•ê·œë³‘ ë‹˜ê»˜ì„œëŠ” í•œêµ­ì–´, ì¼ë³¸ì–´, ì¤‘êµ­ì–´ ë“± 30ê°œ ì–¸ì–´ì˜ ë‹¨ì–´ ì„ë² ë”©ì„ í•™ìŠµí•´ ê³µê°œí–ˆë‹¤. ëª¨ë¸ì€ ì£¼ë¡œ í•´ë‹¹ ì–¸ì–´ì˜ ìœ„í‚¤ë°±ê³¼ ë“±ìœ¼ë¡œ í•™ìŠµëìœ¼ë©° ë²¡í„° ì°¨ì› ìˆ˜ëŠ” 100, 3000ì°¨ì› ë‘ ì¢…ë¥˜ê°€ ìˆë‹¤. https://github.com/Kyubyong/wordvectors ë‹¨ì–´ ìœ ì‚¬ë„ í‰ê°€(word similarity test) ì¼ë ¨ì˜ ë‹¨ì–´ ìŒì„ ë¯¸ë¦¬ êµ¬ì„±í•œ í›„ì— ì‚¬ëŒì´ í‰ê°€í•œ ì ìˆ˜ì™€ ë‹¨ì–´ ë²¡í„° ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚¬ì´ì˜ ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•´ ë‹¨ì–´ ì„ë² ë”©ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì´ë‹¤. Word2Vecê³¼ FastText ê°™ì€ ì˜ˆì¸¡ ê¸°ë°˜ ì„ë² ë”© ê¸°ë²•ë“¤ì´ GloVe, Swivel ë“± í–‰ë ¬ ë¶„í•´ ë°©ë²•ë“¤ì— ë¹„í•´ ìƒê´€ê´€ê³„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ê°•í•œ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë¬¼ë¡  ë¬´ì¡°ê±´ ì˜ˆì¸¡ê¸°ë°˜ì´ ì¢‹ë‹¤ëŠ” ì˜ë¯¸ëŠ” ì•„ë‹ˆë‹¤. ë°ì´í„°ì— ë‹¤ë¥´ê² ì§€ë§Œ ë³´í†µì€ ì €ëŸ° ê²°ê³¼ë¥¼ ì–»ì„ ê²ƒì´ë‹¤. ë‹¨ì–´ ìœ ì¶” í‰ê°€(word analogy test) ì˜ë¯¸ë¡ ì  ìœ ì¶”ì—ì„œ ë‹¨ì–´ ë²¡í„° ê°„ ê³„ì‚°ì„ í†µí•´ ê°‘ - ì„ + ë³‘ = ì •ì„ í†µí•´ í‰ê°€í•˜ëŠ” ë°©ë²•ì´ë‹¤. ê°‘ - ì„ + ë³‘ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ì— ëŒ€í•´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ ë²¡í„°ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ê°€ ì‹¤ì œ ì •ì¸ì§€ë¥¼ í™•ì¸í•œë‹¤. ë‹¨ì–´ ì„ë² ë”© ì‹œê°í™” ì‹œê°í™” ë˜í•œ ë‹¨ì–´ ì„ë² ë”©ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì´ë‹¤. ë‹¤ë§Œ ë‹¨ì–´ ì„ë² ë”©ì€ ë³´í†µ ê³ ì°¨ì› ë²¡í„°ì´ê¸° ë•Œë¬¸ì— ì‚¬ëŒì´ ì¸ì‹í•˜ëŠ” 2, 3ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•´ ì‹œê°í™”ë¥¼ í•˜ê²Œ ëœë‹¤. t-SNE(t-Stochastic Neighbor Embedding)ì€ ê³ ì°¨ì›ì˜ ì›ê³µê°„ì— ì¡´ì¬í•˜ëŠ” ë²¡í„° xì˜ ì´ì›ƒ ê°„ì˜ ê±°ë¦¬ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ëŠ” ì €ì°¨ì› ë²¡í„° yë¥¼ í•™ìŠµí•˜ëŠ” ë°©ë²•ë¡ ì´ë‹¤. ì› ê³µê°„ì˜ ë°ì´í„° í™•ë¥  ë¶„í¬ì™€ ì¶•ì†Œëœ ê³µê°„ì˜ ë¶„í¬ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë²¡í„° ê³µê°„ì„ ì—…ë°ì´íŠ¸í•œë‹¤. ê°€ì¤‘ ì„ë² ë”© ë‹¨ì–´ ì„ë² ë”©ì„ ë¬¸ì¥ ìˆ˜ì¤€ ì„ë² ë”©ìœ¼ë¡œ í™•ì¥í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•˜ê² ë‹¤. ì•„ì£¼ ê°„ë‹¨í•œ ë°©ë²•ì´ì§€ë§Œ ì„±ëŠ¥ íš¨ê³¼ê°€ ì¢‹ì•„ì„œ ì‚¬ìš©í•´ë³¼ë§Œí•œ ë°©ë²•ì´ë‹¤. ë¯¸êµ­ í”„ë¦°ìŠ¤í„´ ëŒ€í•™êµ ì—°êµ¬íŒ€ì´ ICLRì— ë°œí‘œí•œ ë°©ë²•ë¡ ì´ë‹¤. ëª¨ë¸ ê°œìš” Arora et al.(2016)ì€ ë¬¸ì„œ ë‚´ ë‹¨ì–´ì˜ ë“±ì¥ì€ ì €ìê°€ ìƒê°í•œ ì£¼ì œì— ì˜ì¡´í•œë‹¤ê³  ê°€ì •í–ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì£¼ì œ ë²¡í„°(discourse vector)ë¼ëŠ” ê°œë…ì„ ë„ì…í–ˆë‹¤. ì£¼ì œ ë²¡í„° $c_{s}$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì–´ë–¤ ë‹¨ì–´ wê°€ ë‚˜íƒ€ë‚  í™•ë¥ ì„ ì•„ë˜ì™€ ê°™ì´ ì •ì˜í–ˆë‹¤. $\\tilde{c_{s}}$ëŠ” $c_{s}$ë¡œ ë¶€í„° ë„ì¶œë˜ëŠ”ë° ê·¸ ê³¼ì •ì€ ìƒëµí•˜ê³ , ê°„ë‹¨íˆ ë§í•˜ë©´ ì£¼ì œ ë²¡í„° c_{s}ì™€ ê±°ì˜ ë¹„ìŠ·í•œ ì—­í• ì„ í•˜ëŠ” ì„ì˜ì˜ ì–´ë–¤ ë²¡í„°ë¼ê³  ë³´ê² ë‹¤. ZëŠ” ìš°ë³€ ë‘ë²ˆì§¸ í•­ì´ í™•ë¥  ê°’ì´ ë˜ë„ë¡ í•´ì£¼ëŠ” Normalize Factorì´ë‹¤. ìš°ë³€ì˜ ì²«ì§¸í•­ì€ ë‹¨ì–´ wê°€ ì£¼ì œì™€ ìƒê´€ì—†ì´ ë“±ì¥í•  í™•ë¥ ì´ë©°, í•œêµ­ì–´ì—ì„œëŠ” ì¡°ì‚¬(ì€,ëŠ”,ì´,ê°€ ë“±)ê°€ P(w)ê°€ ë†’ì€ ì¶•ì— ì†í•œë‹¤. ë‘ ë²ˆì§¸ í•­ì€ ë‹¨ì–´ wê°€ ì£¼ì œì™€ ê´€ë ¨ì„ ê°€ì§ˆ í™•ë¥ ì„ ì˜ë¯¸í•œë‹¤. ì£¼ì œ ë²¡í„° $\\tilde{c_{s}}$ì™€ wì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ ë²¡í„° $v_{w}$ê°€ ë‚´ì ê°’ì´ í´ìˆ˜ë¡ ê·¸ ê°’ì´ ì»¤ì§„ë‹¤. $\\alpha$ëŠ” ì‚¬ìš©ìê°€ ì§€ì •í•˜ëŠ” hyper parameterì´ë‹¤. ë‹¨ì–´ ë“±ì¥ í™•ë¥  P(w|c_{s}) = \\alpha P(w) + (1-\\alpha) frac{ exp( \\tilde{c_{s}} \\cdot v_{w}) }{Z} ë‹¨ì–´ sequenceëŠ” ë¬¸ì¥ì´ë‹¤. ë¬¸ì¥ ë“±ì¥ í™•ë¥ (ë‹¨ì–´ë“¤ì´ ë™ì‹œì— ë“±ì¥í•  í™•ë¥ )ì€ ë¬¸ì¥ì— ì†í•œ ëª¨ë“  ë‹¨ì–´ë“¤ì´ ë“±ì¥í•  í™•ë¥ ì˜ ëˆ„ì  ê³±ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ê·¸ëŸ°ë° í™•ë¥ ì„ ëˆ„ì í•´ì„œ ê³±í•˜ë©´ ë„ˆë¬´ ì‘ì•„ì§€ëŠ” underflow ë¬¸ì œê°€ ë°œìƒí•˜ë¯€ë¡œ ë¡œê·¸ë¥¼ ì·¨í•´ ë§ì…ˆì„ í•˜ëŠ” ê²ƒìœ¼ë¡œ ëŒ€ì²´í•œë‹¤. ë¬¸ì¥ ë“±ì¥í™•ë¥  P(s|c_{s}) \\propto \\sum_{w \\in s} log P(w|c_{s}) = \\sum_{w \\in s} f_{w}(\\tilde{c_{s}}) ë‹¨ì–´ ë“±ì¥ í™•ë¥ ì˜ Taylor Series approximation f_{w}(\\tilde{c_{s}}) \\approx f_{w}(0) + \\triangledown f_{w}(0)^{T} \\tilde{c_{s}} = constant + frac{ (1-\\alpha) / \\alpha Z }{ P(w) + (1-\\alpha) / \\alpha Z } \\tilde{c_{s}} \\cdot v_{w} ìš°ë¦¬ê°€ ê´€ì°°í•˜ê³  ìˆëŠ” ë‹¨ì–´ wê°€ ë“±ì¥í•  í™•ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” ì£¼ì œë²¡í„° $ c_{s} / \\tilde{c_{s}} $ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. wê°€ ë“±ì¥í•  í™•ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” $ c_{s} / \\tilde{c_{s}} $ë¥¼ ì°¾ê²Œ ëœë‹¤ë©´ ì´ $ c_{s} / \\tilde{c_{s}} $ ëŠ” í•´ë‹¹ ë‹¨ì–´ì˜ ì‚¬ìš©ì„ ì œì¼ ì˜ ì„¤ëª…í•˜ëŠ” ì£¼ì œ ë²¡í„°ê°€ ë  ê²ƒì´ë‹¤. ì§ê´€ì ìœ¼ë¡œ ë§í•˜ìë©´, ìš°ë¦¬ê°€ ê´€ì°°í•˜ê³  ìˆëŠ” ë¬¸ì¥ì´ ë“±ì¥í•  í™•ë¥ ì„ ìµœëŒ€í™”í•˜ëŠ” ì£¼ì œ ë²¡í„° $ c_{s} / \\tilde{c_{s}} $ëŠ” ë¬¸ì¥ì— ì†í•œ ë‹¨ì–´ë“¤ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ ë²¡í„°ì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ ë§Œë“  ìƒˆë¡œìš´ ë²¡í„°ë“¤ì˜ í•©ì— ë¹„ë¡€í•œë‹¤. í¬ê·€í•œ ë‹¨ì–´ë¼ë©´ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ í•´ë‹¹ ë‹¨ì–´ ë²¡í„°ì˜ í¬ê¸°ë¥¼ í‚¤ìš°ê³ , ê³ ë¹ˆë„ ë‹¨ì–´ë¼ë©´ í•´ë‹¹ ë²¡í„°ì˜ í¬ê¸°ë¥¼ ì¤„ë‹ˆë‹¤. ì´ëŠ” ì •ë³´ì„±ì´ ë†’ì€, í¬ê·€í•œ ë‹¨ì–´ì— ê°€ì¤‘ì¹˜ë¥¼ ë†’ê²Œ ì£¼ëŠ” TF-IDFì˜ ì² í•™ê³¼ë„ ë§ë‹¿ì•„ ìˆëŠ” ë¶€ë¶„ì´ë‹¤. ë˜í•œ ë¬¸ì¥ ë‚´ ë‹¨ì–´ì˜ ë“±ì¥ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì—ì„œ Bag of Words ê°€ì •ê³¼ë„ ì—°ê²°ëœë‹¤. ëª¨ë¸ êµ¬í˜„ ë¬¸ì¥ì„ Tokenìœ¼ë¡œ ë‚˜ëˆˆ ë’¤ í•´ë‹¹ Tokenë“¤ì— ëŒ€ì‘í•˜ëŠ” ë²¡í„°ë“¤ì˜ í•©ìœ¼ë¡œ ë¬¸ì¥ì˜ ì„ë² ë”©ì„ êµ¬í•œë‹¤. ì˜ˆì¸¡ì€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì´ ë“¤ì–´ì˜¤ë©´ Token ë²¡í„°ì˜ í•©ìœ¼ë¡œ ë§Œë“¤ê³ , ì´ ë²¡í„°ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ í•™ìŠµ ë°ì´í„° ë¬¸ì¥ì˜ ì„ë² ë”©ì„ ì°¾ëŠ”ë‹¤. ì´í›„ í•´ë‹¹ í•™ìŠµ ë°ì´í„° ë¬¸ì¥ì— ë‹¬ë ¤ ìˆëŠ” ë ˆì´ë¸”ì„ ë¦¬í„´í•˜ëŠ” ë°©ì‹ì´ë‹¤. ì˜ˆë¥¼ë“¤ì–´, â€˜ì˜í™” ì •ë§ ì¬ë°Œë‹¤.â€™ê°€ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì´ê³ , ì´ ë¬¸ì¥ê³¼ ìœ ì‚¬í•œ í•™ìŠµ ë°ì´í„° ì„ë² ë”©ì´ â€˜ì˜í™”ê°€ ì§„ì§œ ì¬ë¯¸ì§€ë„¤ìš”.+ê¸ì •â€™ì´ë¼ë©´, í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì„ ê¸ì •ì´ë¼ê³  ì˜ˆì¸¡í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ë˜í•œ, ê³¼ì—° ì–´ëŠì •ë„ì˜ íš¨ê³¼ê°€ ìˆëŠ”ì§€ ë¹„êµí•˜ê¸°ìœ„í•´ ëŒ€ì¡°êµ°ìœ¼ë¡œ ì¼ë°˜ì ì¸ í•©ì„ í†µí•œ ì„ë² ë”© ë°©ì‹ë„ ìˆ˜í–‰í•´ë³¼ê²ƒì´ë‹¤. Weighted Sumì„ ì´ìš©í•œ Documents Classification Model ì°¸ê³ ë¡œ í•´ë‹¹ ëª¨ë¸ì„ ìˆ˜í–‰í•˜ë ¤ë©´ ë¨¼ì € í˜•íƒœì†Œ ë¶„ì„ì´ ì™„ë£Œëœ Corpus fileê³¼ Corpusë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„ Embedding Fileì´ ì¡´ì¬í•´ì•¼í•œë‹¤. ë¨¼ì € tokenizerë¥¼ ì„ íƒí•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê° Tokenizerì— ë”°ë¥¸ ê°ì²´ë¥¼ ìƒì„±í•´ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤. 1234567891011121314151617181920from khaiii import KhaiiiApifrom konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkmadef get_tokenizer(tokenizer_name): if tokenizer_name == \"komoran\": tokenizer = Komoran() elif tokenizer_name == \"okt\": tokenizer = Okt() elif tokenizer_name == \"mecab\": tokenizer = Mecab() elif tokenizer_name == \"hannanum\": tokenizer = Hannanum() elif tokenizer_name == \"kkma\": tokenizer = Kkma() elif tokenizer_name == \"khaiii\": tokenizer = KhaiiiApi() else: tokenizer = Mecab() return tokenizer ëª¨ë¸ì„ ì €ì¥í•  pathê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ directoryë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤. 12345678import osdef make_save_path(full_path): if full_path[:4] == \"data\": full_path = os.path.join(os.path.abspath(\".\"), full_path) model_path = '/'.join(full_path.split(\"/\")[:-1]) if not os.path.exists(model_path): os.makedirs(model_path) ì•„ë˜ embedding_methodì˜ defaultê°’ì€ fasttextì´ì§€ë§Œ ì‹¤ì œë¡œ í•„ìê°€ ì‹¤í–‰ì‹œì—ëŠ” word2vecì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤. defaultdictì€ ë§ ê·¸ëŒ€ë¡œ ì²˜ìŒì— ê°’ì„ ì§€ì •í•´ì£¼ì§€ ì•Šìœ¼ë©´ defaultê°’ì„ ë„£ì–´ì¤€ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ë¹„êµ í•  ì‚¬í•­ embedding method : fasttext vs word2vec sum method : weighted sum vs sum average or nor : average vs not average ì´ ê¸€ì—ì„œëŠ” 2ë²ˆì§¸ í•­ëª©ì˜ ë¹„êµí•œ ê²°ê³¼ë§Œì„ ë³´ì—¬ ì¤„ ê²ƒì´ë‹¤. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205from collections import defaultdictfrom gensim.models import Word2Vecclass CBoWModel(object): def __init__(self, train_fname, embedding_fname, model_fname, embedding_corpus_fname, embedding_method=\"fasttext\", is_weighted=None, average=False, dim=100, tokenizer_name=\"mecab\"): # configurations make_save_path(model_fname) self.dim = dim # í‰ê· ì„ ë‚´ì¤„ê²ƒì¸ì§€ ì•„ë‹ˆë©´ í•©ë§Œì„ ì‚¬ìš©í•  ê²ƒì¸ì§€ì— ëŒ€í•œ ì˜µì…˜ì´ë‹¤. self.average = average if is_weighted: model_full_fname = model_fname + \"-weighted\" else: model_full_fname = model_fname + \"-original\" self.tokenizer = get_tokenizer(tokenizer_name) if is_weighted: # ready for weighted embeddings # dictionary í˜•íƒœë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤. (embedding[\"word\"]=embedding_vaector) self.embeddings = self.load_or_construct_weighted_embedding(embedding_fname, embedding_method, embedding_corpus_fname) print(\"loading weighted embeddings, complete!\") else: # ready for original embeddings words, vectors = self.load_word_embeddings(embedding_fname, embedding_method) self.embeddings = defaultdict(list) for word, vector in zip(words, vectors): self.embeddings[word] = vector print(\"loading original embeddings, complete!\") # ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ìƒˆë¡­ê²Œ í›ˆë ¨ì‹œí‚¤ê³  ì¡´ì¬í•œë‹¤ë©´ loadí•´ ì˜¨ë‹¤. if not os.path.exists(model_full_fname): print(\"train Continuous Bag of Words model\") self.model = self.train_model(train_fname, model_full_fname) else: print(\"load Continuous Bag of Words model\") self.model = self.load_model(model_full_fname) def evaluate(self, test_data_fname, batch_size=3000, verbose=False): print(\"evaluation start!\") test_data = self.load_or_tokenize_corpus(test_data_fname) data_size = len(test_data) num_batches = int((data_size - 1) / batch_size) + 1 eval_score = 0 for batch_num in range(num_batches): batch_sentences = [] batch_tokenized_sentences = [] batch_labels = [] start_index = batch_num * batch_size end_index = min((batch_num + 1) * batch_size, data_size) features = test_data[start_index:end_index] for feature in features: sentence, tokens, label = feature batch_sentences.append(sentence) batch_tokenized_sentences.append(tokens) batch_labels.append(label) preds, curr_eval_score = self.predict_by_batch(batch_tokenized_sentences, batch_labels) eval_score += curr_eval_score if verbose: for sentence, pred, label in zip(batch_sentences, preds, batch_labels): print(sentence, \", pred:\", pred, \", label:\", label) print(\"number of correct:\", str(eval_score), \", total:\", str(len(test_data)), \", score:\", str(eval_score / len(test_data))) def predict(self, sentence): # ë¬¸ì¥ì„ ì˜ˆì¸¡ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ìš°ì„  í˜•íƒœì†Œë¥¼ ë¶„ì„ì„ í•´ì•¼í•œë‹¤. tokens = self.tokenizer.morphs(sentence) # ë¬¸ì¥ì˜ í˜•íƒœì†Œë“¤ì„ ì„ë² ë”© ë²¡í„°ì™€ ê°™ì€ í¬ê¸°ì˜ ì˜ë²¡í„°ë¥¼ ë§Œë“ í›„ ê³„ì†í•´ì„œ ë”í•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ë¬¸ì¥ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•œë‹¤. # ë§Œì•½ average=Trueí–ˆë‹¤ë©´, sentence_vector = self.get_sentence_vector(tokens) # ëª¨ë¸ì˜ ë¬¸ì¥ ì„ë² ë”© ë²¡í„°ì™€ sentence ë¬¸ì¥ ë²¡í„°ì™€ì˜ ë‚´ì ìœ¼ë¡œ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•œë‹¤. scores = np.dot(self.model[\"vectors\"], sentence_vector) # ì œì¼ë†’ì€ ìœ ì‚¬ë„ë¥¼ ì§€ë‹Œ ë¼ë²¨ì„ ì¶œë ¥í•´ì¤€ë‹¤. pred = self.model[\"labels\"][np.argmax(scores)] return pred def predict_by_batch(self, tokenized_sentences, labels): sentence_vectors, eval_score = [], 0 for tokens in tokenized_sentences: sentence_vectors.append(self.get_sentence_vector(tokens)) scores = np.dot(self.model[\"vectors\"], np.array(sentence_vectors).T) preds = np.argmax(scores, axis=0) for pred, label in zip(preds, labels): if self.model[\"labels\"][pred] == label: eval_score += 1 return preds, eval_score def get_sentence_vector(self, tokens): vector = np.zeros(self.dim) for token in tokens: if token in self.embeddings.keys(): vector += self.embeddings[token] if self.average: vector /= len(tokens) vector_norm = np.linalg.norm(vector) if vector_norm != 0: unit_vector = vector / vector_norm else: unit_vector = np.zeros(self.dim) return unit_vector def load_or_tokenize_corpus(self, fname): data = [] if os.path.exists(fname + \"-tokenized\"): with open(fname + \"-tokenized\", \"r\") as f1: for line in f1: sentence, tokens, label = line.strip().split(\"\\u241E\") data.append([sentence, tokens.split(), label]) else: with open(fname, \"r\") as f2, open(fname + \"-tokenized\", \"w\") as f3: for line in f2: sentence, label = line.strip().split(\"\\u241E\") tokens = self.tokenizer.morphs(sentence) data.append([sentence, tokens, label]) f3.writelines(sentence + \"\\u241E\" + ' '.join(tokens) + \"\\u241E\" + label + \"\\n\") return data def compute_word_frequency(self, embedding_corpus_fname): total_count = 0 # &#123;ë‹¨ì–´ : í•´ë‹¹ ë‹¨ì–´ ê°œìˆ˜&#125;ë¡œ í‘œí˜„í•´ì£¼ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ defaultdictì„ ì‚¬ìš©í–ˆë‹¤. # defaultdict ì„ ì‚¬ìš©í•œ ì´ìœ ëŠ” ê°’ì„ ë”°ë¡œ ì§€ì •í•´ ì£¼ì§€ ì•ŠëŠ”ë‹¤ë©´ default ê°’ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. words_count = defaultdict(int) with open(embedding_corpus_fname, \"r\") as f: for line in f: tokens = line.strip().split() for token in tokens: words_count[token] += 1 total_count += 1 return words_count, total_count def load_word_embeddings(self, vecs_fname, method): if method == \"word2vec\": model = Word2Vec.load(vecs_fname) words = model.wv.index2word vecs = model.wv.vectors else: words, vecs = [], [] with open(vecs_fname, 'r', encoding='utf-8') as f1: if \"fasttext\" in method: next(f1) # skip head line for line in f1: if method == \"swivel\": splited_line = line.replace(\"\\n\", \"\").strip().split(\"\\t\") else: splited_line = line.replace(\"\\n\", \"\").strip().split(\" \") words.append(splited_line[0]) vec = [float(el) for el in splited_line[1:]] vecs.append(vec) return words, vecs def load_or_construct_weighted_embedding(self, embedding_fname, embedding_method, embedding_corpus_fname, a=0.0001): dictionary = &#123;&#125; # ì´ë¯¸ ë§Œë“¤ì–´ì§„ ê°€ì¤‘í•© embeddingì´ ì¡´ì¬í•  ê²½ìš° if os.path.exists(embedding_fname + \"-weighted\"): with open(embedding_fname + \"-weighted\", \"r\") as f2: for line in f2: # \\u241E : Symbol for Record Seperator word, weighted_vector = line.strip().split(\"\\u241E\") weighted_vector = [float(el) for el in weighted_vector.split()] dictionary[word] = weighted_vector else: # ìœ„ì—ì„œ embedding-weighted íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ í›ˆë ¨ì„ í•´ì•¼í•˜ë¯€ë¡œ # ìš°ì„  ì´ë¯¸ embeddingëœ íŒŒì¼ì˜ ë‹¨ì–´ì™€ í•´ë‹¹ë‹¨ì–´ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤. # load pretrained word embeddings # í•´ë‹¹ ì„ë² ë”© íŒŒì¼ì— ìˆëŠ” ë‹¨ì–´ì™€ ê·¸ì— í•´ë‹¹í•˜ëŠ” ì„ë² ë”©ë²¡í„°ë¥¼ ìˆœì„œëŒ€ë¡œ ë¶ˆëŸ¬ì˜¨ë‹¤. words, vecs = self.load_word_embeddings(embedding_fname, embedding_method) # compute word frequency words_count, total_word_count = self.compute_word_frequency(embedding_corpus_fname) # construct weighted word embeddings # embedding_fname - weightedë¡œ ê°€ì¤‘í•©ì„ ê³„ì‚°í•œ ì„ë² ë”©ë²¡í„° íŒŒì¼ì„ ìƒì„±í•œë‹¤. with open(embedding_fname + \"-weighted\", \"w\") as f3: for word, vec in zip(words, vecs): if word in words_count.keys(): word_prob = words_count[word] / total_word_count else: word_prob = 0.0 weighted_vector = (a / (word_prob + a)) * np.asarray(vec) dictionary[word] = weighted_vector f3.writelines(word + \"\\u241E\" + \" \".join([str(el) for el in weighted_vector]) + \"\\n\") return dictionary def train_model(self, train_data_fname, model_fname): model = &#123;\"vectors\": [], \"labels\": [], \"sentences\": []&#125; # [sentence, tokens, label]í˜•íƒœë¡œ ì¶œë ¥ train_data = self.load_or_tokenize_corpus(train_data_fname) with open(model_fname, \"w\") as f: for sentence, tokens, label in train_data: tokens = self.tokenizer.morphs(sentence) sentence_vector = self.get_sentence_vector(tokens) model[\"sentences\"].append(sentence) model[\"vectors\"].append(sentence_vector) model[\"labels\"].append(label) str_vector = \" \".join([str(el) for el in sentence_vector]) f.writelines(sentence + \"\\u241E\" + \" \".join(tokens) + \"\\u241E\" + str_vector + \"\\u241E\" + label + \"\\n\") return model def load_model(self, model_fname): model = &#123;\"vectors\": [], \"labels\": [], \"sentences\": []&#125; with open(model_fname, \"r\") as f: for line in f: sentence, _, vector, label = line.strip().split(\"\\u241E\") vector = np.array([float(el) for el in vector.split()]) model[\"sentences\"].append(sentence) model[\"vectors\"].append(vector) model[\"labels\"].append(label) return model ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„° ê°’ ì„¤ì •123456train_fname = \"./data/processed/processed_ratings_train.txt\"embedding_fname = \"./data/word-embeddings/word2vec/word2vec\"model_fname = \"./data/word-embeddings/cbow/word2vec\"embedding_corpus_fname = \"./data/tokenized/ratings_mecab.txt\"embedding_method = \"word2vec\"test_data_fname = \"./data/processed/processed_ratings_test.txt\" ëª¨ë¸ í•™ìŠµ ë° í‰ê°€í•´ë‹¹ ë¬¸ì¥ì— ëŒ€í•œ ë‹¨ì–´ë²¡í„°ë“¤ì˜ í•©ë§Œì„ ê°€ì§€ê³  ì˜ˆì¸¡1234original_Model=CBoWModel(train_fname=train_fname, embedding_fname=embedding_fname, model_fname=model_fname, embedding_corpus_fname=None, embedding_method=embedding_method, is_weighted=False, average=False, dim=100, tokenizer_name=\"mecab\")original_Model.evaluate(test_data_fname) ê²°ê³¼12345loading original embeddings, complete!train Continuous Bag of Words modelevaluation start!number of correct: 36498 , total: 49997 , score: 0.7300038002280137 í•´ë‹¹ ë¬¸ì¥ì— ëŒ€í•œ ë‹¨ì–´ë²¡í„°ë“¤ì˜ ê°€ì¤‘í•©ì„ ê°€ì¤‘í•©ì„ ê°€ì§€ê³  ì˜ˆì¸¡12345weighted_Model=CBoWModel(train_fname=train_fname, embedding_fname=embedding_fname, model_fname=model_fname, embedding_corpus_fname=embedding_corpus_fname,embedding_method=embedding_method, is_weighted=True, average=False, dim=100, tokenizer_name=\"mecab\")weighted_Model.evaluate(test_data_fname) ê²°ê³¼12345loading weighted embeddings, complete!train Continuous Bag of Words modelevaluation start!number of correct: 34208 , total: 49997 , score: 0.6842010520631238","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP ì‹¤ìŠµ í…ìŠ¤íŠ¸ ë¶„ë¥˜(Conv1d CNN, LSTM) -03","slug":"NLP_05","date":"2020-02-01T07:57:48.000Z","updated":"2020-02-03T14:30:51.344Z","comments":true,"path":"2020/02/01/NLP_05/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/01/NLP_05/","excerpt":"","text":"ìˆœí™˜ì‹ ê²½ë§ ë¶„ë¥˜ ëª¨ë¸ ì•ì„  ëª¨ë¸ë“¤ê³¼ ë‹¬ë¦¬ ì´ë¯¸ ì£¼ì–´ì§„ ë‹¨ì–´ íŠ¹ì§• ë²¡í„°ë¥¼ í™œìš©í•´ ëª¨ë¸ì„ í•™ìŠµí•˜ì§€ ì•Šê³  í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ì…ë ¥í•´ì„œ ë¬¸ì¥ì— ëŒ€í•œ íŠ¹ì§• ì •ë³´ë¥¼ ì¶”ì¶œí•œë‹¤. RNNì€ í˜„ì¬ ì •ë³´ëŠ” ì´ì „ ì •ë³´ê°€ ì ì¸µì ìœ¼ë¡œ ìŒ“ì´ë©´ì„œ ì •ë³´ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ë‹¤. ë”°ë¼ì„œ ì‹œê°„ì— ì˜ì¡´ì ì¸ ë˜ëŠ” ìˆœì°¨ì ì¸ ë°ì´í„°ì— ëŒ€í•œ ë¬¸ì œì— í™œìš©ëœë‹¤. ì´ ëª¨ë¸ì€ í•œë‹¨ì— ëŒ€í•œ ì •ë³´ë¥¼ ì…ë ¥í•˜ë©´ ì´ ë‹¨ì–´ ë‹¤ìŒì— ë‚˜ì˜¬ ë‹¨ì–´ë¥¼ ë§ì¶”ëŠ” ëª¨ë¸ì´ë¼ ìˆœì°¨ì ì¸ ë°ì´í„°ì— ëŒ€í•œ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•œ ê²ƒì´ë‹¤. 1234567891011121314DATA_IN_PATH = '/content/'DATA_OUT_PATH = '/content/'INPUT_TRAIN_DATA_FILE_NAME = 'train_input.npy'LABEL_TRAIN_DATA_FILE_NAME = 'train_label.npy'DATA_CONFIGS_FILE_NAME = 'data_configs.json'train_input = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'rb'))train_label = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'rb'))prepro_configs = Nonewith open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, 'r') as f: prepro_configs = json.load(f) í•™ìŠµê³¼ ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬12345from sklearn.model_selection import train_test_splitTEST_SPLIT=0.1RANDOM_SEED=13371447input_train, input_eval, label_train, label_eval = train_test_split(train_input, train_label, test_size=TEST_SPLIT, random_state=RANDOM_SEED) ë°ì´í„° ì…ë ¥ í•¨ìˆ˜1234567891011121314151617181920212223242526import tensorflow as tfBATCH_SIZE = 16NUM_EPOCHS = 20def mapping_fn(X, Y): inputs, labels = &#123;'x' : X&#125;, Y return inputs, labelsdef train_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train)) dataset = dataset.shuffle(buffer_size=50000) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.repeat(count=NUM_EPOCHS) dataset = dataset.map(mapping_fn) iterator = dataset.make_one_shot_iterator() return iterator.get_next()def eval_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((input_eval, label_eval)) dataset = dataset.map(mapping_fn) dataset = dataset.batch(BATCH_SIZE) iterator = dataset.make_one_shot_iterator() return iterator.get_next() ëª¨ë¸ í•¨ìˆ˜ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜123456789VOCAB_SIZE = prepro_configs['vocab_size']WORD_EMBEDDING_DIM = 100HIDDEN_STATE_DIM = 150DENSE_FEATURE_DIM = 150learning_rate = 0.001 ëª¨ë¸ êµ¬í˜„ ë¨¼ì € ëª¨ë¸ì—ì„œ ë°°ì¹˜ ë°ì´í„°ë¥¼ ë°›ê²Œ ëœë‹¤ë©´ ë‹¨ì–´ ì¸ë±ìŠ¤ë¡œ êµ¬ì„±ëœ Sequence í˜•íƒœë¡œ ì…ë ¥ì´ ë“¤ì–´ì˜¨ë‹¤. ë°ì´í„° ì…ë ¥ í•¨ìˆ˜ì—ì„œ ì •ì˜í–ˆë“¯ì´ ëª¨ë¸ í•¨ìˆ˜ì˜ ì…ë ¥ ì¸ìì¸ featuresëŠ” Python dictionary í˜•íƒœë¡œ êµ¬ì„±ë¼ ìˆë‹¤. ëª¨ë¸ì— ë“¤ì–´ì˜¨ ì…ë ¥ ë°ì´í„°ëŠ” ë³´í†µ Embedding Layerë¥¼ ê±°ì¹œë‹¤. êµ¬í˜„í•˜ê³ ì í•˜ëŠ” ëª¨ë¸ì—ì„œëŠ” tf.keras.Embeddingí•¨ìˆ˜ê°€ ì´ ê°™ì€ ì—­í• ì„ ìˆ˜í–‰í•œë‹¤. Embedding Layerë¥¼ ê±°ì³ ë‚˜ì˜¨ ë°ì´í„°ëŠ” ìˆœí™˜ ì‹ ê²½ë§ ì¸µì„ ê±°ì³ ë¬¸ìì˜ ë²¡í„°ë¥¼ ì¶œë ¥í•œë‹¤. ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì‹¬ì¸µ ìˆœí™˜ ì‹ ê²½ë§ ëª¨ë¸ë¡œ LSTM ëª¨ë¸ì„ í†µí•´ êµ¬í˜„í•œë‹¤. ìˆœí™˜ ì‹ ê²½ë§ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œëŠ” RNNCellì´ë€ ê°ì²´ë¥¼ í™œìš©í•¨ã„´ë‹¤. RNNCellì€ ìˆœí™˜ ì‹ ê²½ë§ ê°ì²´ë¼ ë³´ë©´ëœë‹¤. LSTMìœ¼ë¡œ ìˆœí™˜ ì‹ ê²½ë§ì„ êµ¬í˜„í•˜ê¸° ìœ„í•´ tf.nn.rnn_cell.LSTMCellê°ì²´ë¥¼ ìƒì„±í•˜ë©°, ì´ ê°ì²´ëŠ” í•˜ë‚˜ì˜ LSTM Cellì„ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ í•´ë‹¹ Cell ê°ì²´ë¥¼ ì—¬ëŸ¬ê°œ ìƒì„±í•´ì„œ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ ì¤€ë‹¤. LSTMCellì„ ìƒì„±í•  ë•ŒëŠ” ì€ë‹‰ ìƒíƒœ ë²¡í„°(Hidden state vector)ì— ëŒ€í•œ ì°¨ì›ë§Œ ì •ì˜í•˜ë©´ ëœë‹¤. ì—¬ëŸ¬ LSTMCellì„ ìŒ€ê²Œ ë˜ë©´ ì´ë¥¼ í•˜ë‚˜ì˜ MultiRNNìœ¼ë¡œ ë¬¶ì–´ì•¼, ì¦‰ wrappingí•´ì•¼í•œë‹¤. tf.nn.rnn_cell.MultiRNNCellì„ ìƒì„±í•¨ìœ¼ë¡œì¨ Stack êµ¬ì¡°ì˜ LSTM ì‹ ê²½ë§ì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. ë‹¨ìˆœíˆ RNNCell ë§Œìœ¼ë¡œ êµ¬ì„±í•´ ëª¨ë¸ ì—°ì‚° ê·¸ë˜í”„ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤. RNNCell ê°ì²´ëŠ” Sequence í•œ ìŠ¤í…ì— ëŒ€í•œ ì—°ì‚°ë§Œ ê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ ì—¬ëŸ¬ ìŠ¤í…ì— ëŒ€í•œ ì—°ì‚°ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” for ë¬¸ì„ í™œìš©í•´ ì—°ì‚°ì„ í•  ìˆ˜ ìˆê²Œ êµ¬í˜„í•´ì•¼í•œë‹¤. í•˜ì§€ë§Œ ì´ë³´ë‹¤ ë” ê°„ë‹¨í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ tf.nn.dynamic_rnn í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ í•¨ìˆ˜ëŠ” for ë¬¸ ì—†ì´ ìë™ìœ¼ë¡œ ìˆœí™˜ ì‹ ê²½ë§ì„ ë§Œë“¤ì–´ ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. dynamic_rnn í•¨ìˆ˜ì— í•„ìš”í•œ ì…ë ¥ ì¸ìëŠ” 2ê°œë‹¤. ì²« ë²ˆì§¸ ìˆœí™˜ ì‹ ê²½ë§ ê°ì²´ì¸ MultiRNNCell ê°ì²´ì´ê³ , ë‚˜ë¨¸ì§€ í•˜ë‚˜ëŠ” ì…ë ¥ê°’ì„ ë„£ì–´ì£¼ë©´ëœë‹¤. Denseì— ì ìš©ì‹œí‚¤ëŠ” ì…ë ¥ê°’ì€ LSTM ì‹ ê²½ë§ì˜ ë§ˆì§€ë§‰ ì¶œë ¥ê°’ì„ ë„£ì–´ì¤€ë‹¤. ì¶œë ¥ê°’ì— [:, -1, :]ë¡œ ë§ˆì§€ë§‰ ê°’ë§Œ ë½‘ì•„ë‚¸ í›„ Denseì— ì ìš©ì‹œí‚¨ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ê°ì •ì´ ê¸ì •ì¸ì§€ ë¶€ì •ì¸ì§€ íŒë‹¨í•  ìˆ˜ ìˆë„ë¡ ì¶œë ¥ê°’ì„ í•˜ë‚˜ë¡œ ë§Œë“¤ì–´ì•¼ í•œë‹¤. ë³´í†µ ì„ í˜•ë³€í™˜ì„ í†µí•´ ì…ë ¥ ë²¡í„°ì— ëŒ€í•œ ì°¨ì›ìˆ˜ë¥¼ ë°”ê¾¼ë‹¤. ëª¨ë¸ í•™ìŠµ, ê²€ì • ë° í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ êµ¬í˜„ ì•ì„œ ëª¨ë¸ì—ì„œ êµ¬í˜„í•œ ê°’ê³¼ ì •ë‹µ labelì„ ê°€ì§€ê³  loss ê°’ì„ êµ¬í•´ Adam optimizerë¥¼ í™œìš©í•´ ëª¨ë¸ parameterë¥¼ ìµœì í™” í•´ ë³¼ ê²ƒì´ë‹¤. ëª¨ë¸ ì˜ˆì¸¡ lossê°’ì€ ëª¨ë¸ì—ì„œ êµ¬í•œ logits ë³€ìˆ˜ì˜ ê²½ìš° ì•„ì§ Logistic í•¨ìˆ˜ë¥¼ í†µí•´ 0~1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ìŠ¤ì¼€ì¼ì„ ë§ì¶°ë‘ì§€ ì•Šì•˜ë‹¤. ë¬¼ë¡  ì•ì„œ dense ì¸µì—ì„œ activation ì¸ìë¥¼ tf.nn.sigmoidë¡œ ì„¤ì •í•´ë‘˜ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” tf.losses.sigmoid_cross_entropy í•¨ìˆ˜ë¥¼ í™œìš©í•´ ì†ì‹¤ê°’ì„ êµ¬í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— dense ì¸µì—ì„œ ì„¤ì •í•˜ì§€ ì•Šì•˜ë‹¤. ì˜ˆì¸¡ lossê°’ì„ êµ¬í•˜ê³  ë‚˜ë©´ ì´ì œ parameter optimizationì„ í•˜ê³ ì SGDë¥¼ ì§„í–‰í•œë‹¤. ì—¬ê¸°ì„œëŠ” tf.train.AdamOptimizerí´ë˜ìŠ¤ë¥¼ í™œìš©í•  ê²ƒì´ë‹¤. tf.train.AdamOptimizer.minimize í•¨ìˆ˜ë¥¼ ì„ ì–¸ í•  ë•Œ ì „ì²´ í•™ìŠµì— ëŒ€í•œ global stepê°’ì„ ë„£ì–´ì•¼ í•œë‹¤. tf.train.get_global_stepì„ ì„ ì–¸í•˜ë©´ í˜„ì¬ í•™ìŠµ global stepì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ë³´í†µ ì§ì ‘ ëª¨ë¸ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ê²Œ ë˜ë©´ tf.estimator.EstimatorSpec ê°ì²´ë¥¼ ìƒì„±í•´ì„œ ë°˜í™˜í•˜ê²Œ í•œë‹¤. ì´ ê°ì²´ëŠ” í˜„ì¬ í•¨ìˆ˜ê°€ ì–´ëŠ ëª¨ë“œì—ì„œ ì‹¤í–‰ë˜ê³  ìˆëŠ”ì§€ í™•ì¸í•œë‹¤. ê·¸ë¦¬ê³  ê° ëª¨ë“œì— ë”°ë¼ í•„ìš”í•œ ì…ë ¥ ì¸ìê°€ ë‹¤ë¥´ë‹¤. 12345678910111213141516171819202122232425262728293031323334353637383940414243def model_fn(features, labels, mode): TRAIN = mode == tf.estimator.ModeKeys.TRAIN EVAL = mode == tf.estimator.ModeKeys.EVAL PREDICT = mode == tf.estimator.ModeKeys.PREDICT embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM)(features['x']) embedding_layer = tf.keras.layers.Dropout(0.2)(embedding_layer) rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [HIDDEN_STATE_DIM, HIDDEN_STATE_DIM]] multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers) outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell, inputs=embedding_layer, dtype=tf.float32) outputs = tf.keras.layers.Dropout(0.2)(outputs) hidden_layer = tf.keras.layers.Dense(DENSE_FEATURE_DIM, activation=tf.nn.tanh)(outputs[:, -1, :]) hidden_layer = tf.keras.layers.Dropout(0.2)(hidden_layer) logits = tf.keras.layers.Dense(1)(hidden_layer) logits = tf.squeeze(logits, axis=-1) sigmoid_logits = tf.nn.sigmoid(logits) if PREDICT: predictions = &#123;'sentiment': sigmoid_logits&#125; return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions) loss = tf.losses.sigmoid_cross_entropy(labels, logits) if EVAL: accuracy = tf.metrics.accuracy(labels, tf.round(sigmoid_logits)) eval_metric_ops = &#123;'acc':accuracy&#125; return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops) if TRAIN: global_step = tf.train.get_global_step() train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step) return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss) TF Estimator í™œìš©í•œ ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ê²€ì¦12345678910DATA_OUT_PATH = '/content/'if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_OUT_PATH)est = tf.estimator.Estimator(model_fn, model_dir=DATA_OUT_PATH + 'checkpoint')os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"est.train(train_input_fn) validation dataì— ëŒ€í•œ ì„±ëŠ¥ì´ ì•½ 85%ì •ë„ì˜€ë‹¤. ì˜¤íˆë ¤ ì•ì˜ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ë“¤ ì¤‘ ì–´ë–¤ ê¸°ë²•ë³´ë‹¤ëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆì—ˆì§€ë§Œ, test dataì— ëŒ€í•œ ì„±ëŠ¥ì„ í•œë²ˆ ì²´í¬í•´ ë³´ì•„ì•¼ í•  ê²ƒ ê°™ë‹¤. 1234est.evaluate(eval_input_fn)# ê²°ê³¼# &#123;'acc': 0.8472, 'global_step': 18291, 'loss': 0.6007853&#125; ë°ì´í„° ì œì¶œ1234DATA_OUT_PATH = '/content/'TEST_INPUT_DATA = 'test_input.npy'test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb')) estimatorë¥¼ í†µí•´ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œëŠ” ë°ì´í„° ì…ë ¥ í•¨ìˆ˜ë¥¼ ì •ì˜í•´ì•¼ í–ˆë‹¤. ì´ ê²½ìš°ëŠ” tf.estimator.inputs.numpy_input_fn í•¨ìˆ˜ë¥¼ í™œìš©í•´ ë°ì´í„° ì…ë ¥ í•¨ìˆ˜ë¥¼ ìƒì„±í•œë‹¤. 123predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;\"x\": test_input_data&#125;, shuffle=False)predictions = np.array([p['sentiment'] for p in est.predict(input_fn=predict_input_fn)]) 1234TEST_ID_DATA = 'test_id.npy'test_id = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb'), allow_pickle='True')output = pd.DataFrame(&#123;'id': test_id, 'sentiment': list(predictions)&#125;)output.to_csv(DATA_OUT_PATH + \"rnn_predic.csv\", index=False, quoting=3) 1!kaggle competitions submit word2vec-nlp-tutorial -f \"rnn_predic.csv\" -m \"LSTM Model with Epoch 10\" CNNì„ ì´ìš©í•œ ë¬¸ì¥ ë¶„ë¥˜ CNNì€ ë³´í†µ imageì—ì„œ ë§ì´ ì‚¬ìš©ëœë‹¤ê³  ìƒê°ë“¤ì§€ë§Œ, í…ìŠ¤íŠ¸ì—ì„œë„ ì¢‹ì€ íš¨ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ì ì„ Yoon Kimm(2014) ë°•ì‚¬ê°€ ì“´ â€œConvolutional Neural Network for Sentence Classificationâ€ì„ í†µí•´ ì…ì¦ë˜ì—ˆë‹¤. RNNì´ ë‹¨ì–´ì˜ ì…ë ¥ ìˆœì„œë¥¼ ì¤‘ìš”í•˜ê²Œ ë°˜ì˜í•œë‹¤ë©´ CNNì€ ë¬¸ì¥ì˜ ì§€ì—­ì •ë³´ë¥¼ ë³´ì¡´í•˜ë©´ì„œ ê° ë¬¸ì¥ ì„±ë¶„ì˜ ë“±ì¥ ì •ë³´ë¥¼ í•™ìŠµì— ë°˜ì˜í•˜ëŠ” êµ¬ì¡°ë¡œ í’€ì–´ê°€ê³  ìˆë‹¤. í•™ìŠµí•  ë•Œ ê° í•„í„° í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ë©´ì„œ ì–¸ì–´ì˜ íŠ¹ì§• ê°’ì„ ì¶”ì¶œí•˜ê²Œ ë˜ëŠ”ë°, ê¸°ì¡´ì˜ n-gram(2ê·¸ë¨, 3ê·¸ë¨) ë°©ì‹ê³¼ ìœ ì‚¬í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ëª¨ë¸ êµ¬í˜„123456789# ê¸°ë³¸ì ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¶ˆëŸ¬ì˜¨ë‹¤import sysimport osimport numpy as npimport jsonfrom sklearn.model_selection import train_test_splitimport tensorflow as tffrom tensorflow import keras 1234567891011121314151617# ì´ì „ì— ì €ì¥í–ˆë˜ í•™ìŠµì— í•„ìš”í•œ ë””ë ‰í„°ë¦¬ ì„¤ì • ë° í•™ìŠµ/í‰ê°€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ë‹¤.DATA_IN_PATH = '/content/'DATA_OUT_PATH = '/content/'INPUT_TRAIN_DATA_FILE_NAME = 'train_input.npy'LABEL_TRAIN_DATA_FILE_NAME = 'train_label.npy'INPUT_TEST_DATA_FILE_NAME = 'test_input.npy'DATA_CONFIGS_FILE_NAME = 'data_configs.json'train_input_data = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'rb'))train_label_data = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'rb'))test_input_data = np.load(open(DATA_IN_PATH + INPUT_TEST_DATA_FILE_NAME, 'rb'))with open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, 'r') as f: prepro_configs = json.load(f) print(prepro_configs.keys()) í•™ìŠµê³¼ ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬12345678910# íŒŒë¼ë¯¸í„° ë³€ìˆ˜RNG_SEED = 1234BATCH_SIZE = 16NUM_EPOCHS = 10VOCAB_SIZE = prepro_configs['vocab_size']EMB_SIZE = 128VALID_SPLIT = 0.2# í•™ìŠµ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ train_test_split í•¨ìˆ˜ë¥¼ í™œìš©í•´ ë‚˜ëˆˆë‹¤.train_input, eval_input, train_label, eval_label = train_test_split(train_input_data, train_label_data, test_size=VALID_SPLIT, random_state=RNG_SEED) ë°ì´í„° ì…ë ¥ í•¨ìˆ˜1234567891011121314151617181920212223242526# ì „ì²˜ë¦¬ í•™ìŠµì„ ìœ„í•´ tf.dataë¥¼ ì„¤ì •í•œë‹¤.def mapping_fn(X, Y=None): input, label = &#123;'x': X&#125;, Y return input, labeldef train_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label)) dataset = dataset.shuffle(buffer_size=len(train_input)) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.map(mapping_fn) dataset = dataset.repeat(count=NUM_EPOCHS) iterator = dataset.make_one_shot_iterator() return iterator.get_next()def eval_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((eval_input, eval_label)) dataset = dataset.shuffle(buffer_size=len(eval_input)) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.map(mapping_fn) iterator = dataset.make_one_shot_iterator() return iterator.get_next() ëª¨ë¸ êµ¬í˜„ í•©ì„±ê³± ì—°ì‚°ì˜ ê²½ìš° ì¼€ë¼ìŠ¤ ëª¨ë“ˆ ì¤‘ Conv1Dë¥¼ í™œìš©í•´ ì§„í–‰í•œë‹¤. ì´ 3ê°œì˜ í•©ì„±ê³± ì¸µì„ ì‚¬ìš©í•˜ëŠ”ë°, ê°ê° í•„í„°ì˜ í¬ê¸°ë¥¼ ë‹¤ë¥´ê²Œ í•´ì„œ ì ìš©í•œë‹¤. ì¦‰, kernel_sizeë¥¼ 3,4,5ë¡œ ì„¤ì •í•  ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ê°ê° ë‹¤ë¥¸ í•„í„°ì˜ í¬ê¸°ë¡œ ì ìš©í•œ í•©ì„±ê³± ì¸µ ì¶œë ¥ê°’ì„ í•˜ë‚˜ë¡œ í•©ì¹  ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì¶”ê°€ë¡œ ê° í•©ì„±ê³± ì‹ ê²½ë§ ì´í›„ì— max pooling ì¸µì„ ì ìš©í•œë‹¤. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def model_fn(features, labels, mode): TRAIN = mode == tf.estimator.ModeKeys.TRAIN EVAL = mode == tf.estimator.ModeKeys.EVAL PREDICT = mode == tf.estimator.ModeKeys.PREDICT # embedding layerë¥¼ ì„ ì–¸ embedding_layer = keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features['x']) # embedding layerì— ëŒ€í•œ outputì— ëŒ€í•´ dropoutì„ ì·¨í•œë‹¤. dropout_emb = keras.layers.Dropout(0.5)(embedding_layer) ## filters = 128ì´ê³  kernel_size = 3,4,5ì´ë‹¤. ## ê¸¸ì´ê¸°ã… 3, 4, 5ì¸ 128ê°œì˜ ë‹¤ë¥¸ í•„í„°ë¥¼ ìƒì„±í•œë‹¤. 3, 4, 5 gramì˜ íš¨ê³¼ì²˜ëŸ¼ ë‹¤ì–‘í•œ ê°ë„ì—ì„œ ë¬¸ì¥ì„ ë³´ëŠ” íš¨ê³¼ê°€ ìˆë‹¤. ## conv1dëŠ” (ë°°ì¹˜ í¬ê¸°, ê¸¸ì´, ì±„ë„)ë¡œ ì…ë ¥ê°’ì„ ë°›ëŠ”ë°, ë°°ì¹˜ ì‚¬ì´ì¦ˆ : ë¬¸ì¥ ìˆ«ì | ê¸¸ì´ : ê° ë¬¸ì¥ì˜ ë‹¨ì–´ì˜ ê°œìˆ˜ | ì±„ë„ : ì„ë² ë”© ì¶œë ¥ ì°¨ì›ìˆ˜ conv1 = keras.layers.Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb) pool1 = keras.layers.GlobalMaxPool1D()(conv1) conv2 = keras.layers.Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb) pool2 = keras.layers.GlobalMaxPool1D()(conv2) conv3 = keras.layers.Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb) pool3 = keras.layers.GlobalMaxPool1D()(conv3) # 3,4,5 gramì´í›„ ëª¨ì•„ì£¼ê¸° concat = keras.layers.concatenate([pool1, pool2, pool3]) hidden = keras.layers.Dense(250, activation=tf.nn.relu)(concat) dropout_hidden = keras.layers.Dropout(0.5)(hidden) logits = keras.layers.Dense(1, name='logits')(dropout_hidden) logits = tf.squeeze(logits, axis=-1) # ìµœì¢…ì ìœ¼ë¡œ í•™ìŠµ, ê²€ì¦, í‰ê°€ì˜ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ í™œìš© if PREDICT: return tf.estimator.EstimatorSpec(mode=mode, predictions=&#123;'prob': tf.nn.sigmoid(logits)&#125;) loss = tf.losses.sigmoid_cross_entropy(labels, logits) if EVAL: pred = tf.nn.sigmoid(logits) accuracy = tf.metrics.accuracy(labels, tf.round(pred)) return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=&#123;'acc':accuracy&#125;) if TRAIN: global_step = tf.train.get_global_step() train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step) return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss) ëª¨ë¸ í•™ìŠµ123456789model_dir = os.path.join(os.getcwd(), \"data_out/checkpoint/cnn\")os.makedirs(model_dir, exist_ok=True)config_tf = tf.estimator.RunConfig(save_checkpoints_steps=200, keep_checkpoint_max=2, log_step_count_steps=400)# Estimator ê°ì²´ ìƒì„±cnn_est = tf.estimator.Estimator(model_fn, model_dir=model_dir)cnn_est.train(train_input_fn) ê²€ì¦ ë°ì´í„° í‰ê°€ ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ì •í™•ë„ê°€ ì•½ 88%ì •ë„ë¡œ ì¸¡ì •ë˜ì—ˆë‹¤. ì§€ê¸ˆê» ê°„ë‹¨í•œ ëª¨ë¸ë“¤ ì¤‘ ì œì¼ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ê³  ìˆì–´ í•„ìëŠ” ì•½ê°„ ê¸°ëŒ€í•˜ê³  ìˆì—ˆë‹¤. ì´ì—ë”°ë¥¸ test dataì˜ ì„±ëŠ¥ì„ ì•Œì•„ë³´ê¸° ìœ„í•´ ìºê¸€ì— test dataì˜ ì˜ˆì¸¡ê°’ì„ ì œì¶œí•´ ë³¼ ê²ƒì´ë‹¤. 1234cnn_est.evaluate(eval_input_fn)# ê²°ê³¼# &#123;'acc': 0.8774, 'global_step': 94200, 'loss': 1.3248637&#125; 1234567DATA_IN_PATH = '/content/'DATA_OUT_PATH = '/content/'TEST_INPUT_DATA = 'test_input.npy'TEST_ID_DATA = 'test_id.npy'test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb'))ids = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb'), allow_pickle=True) 1234567predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;'x':test_input_data&#125;, shuffle=False)predictions = np.array([p['prob'] for p in cnn_est.predict(input_fn=predict_input_fn)])output = pd.DataFrame(&#123;\"id\": list(ids), \"sentiment\":list(predictions)&#125;)output.to_csv(DATA_OUT_PATH + \"Bag_of_Words_model_test.csv\", index=False, quoting=3) 1!kaggle competitions submit word2vec-nlp-tutorial -f \"Bag_of_Words_model_test.csv\" -m \"CNN 1d Model with EPOCHS 10\"","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP ì‹¤ìŠµ í…ìŠ¤íŠ¸ ë¶„ë¥˜(TF-IDF, CountVectorizer, Word2Vec) -02","slug":"NLP_04","date":"2020-01-29T15:13:48.000Z","updated":"2020-02-04T13:24:23.984Z","comments":true,"path":"2020/01/30/NLP_04/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/30/NLP_04/","excerpt":"","text":"ëª¨ë¸ë§ ì†Œê°œ ì„ í˜•ëª¨ë¸ ë¡œì§€ìŠ¤í‹±íšŒê·€ ëª¨ë¸ ì…ë ¥ ë²¡í„°ë¥¼ word2vecê³¼ tf-idfë¥¼ ì‚¬ìš©í•´ë³¸ë‹¤. ëœë˜í¬ë ˆìŠ¤íŠ¸ TF-IDFë¥¼ í™œìš©í•œ ëª¨ë¸ êµ¬í˜„ ëª¨ë¸ì˜ ì…ë ¥ê°’ìœ¼ë¡œ TF-IDF ê°’ì„ ê°–ëŠ” ë²¡í„°ë¥¼ ì‚¬ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì— scikit-learnì˜ TfidfVectorizerë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” ì…ë ¥ê°’ì´ í…ìŠ¤íŠ¸ë¡œ ì´ë¤„ì§„ ë°ì´í„° í˜•íƒœì´ì–´ì•¼ í•œë‹¤. 123train_data = pd.read_csv('train_clean.csv')reviews = list(train_data['clean_review'])sentiments = list(train_data['sentiment']) TF-IDF Vectorizing ë°ì´í„°ì— ëŒ€í•´ TF-IDF ê°’ìœ¼ë¡œ ë²¡í„°í™”ë¥¼ ì§„í–‰í•œë‹¤. min_df : ì„¤ì •í•œ ê°’ë³´ë‹¤ íŠ¹ì • Tokenì˜ df ê°’ì´ ë” ì ê²Œ ë‚˜ì˜¤ë©´ ë²¡í„°í™” ê³¼ì •ì—ì„œ ì œê±° anlayzer : ë¶„ì„ ë‹¨ìœ„ë¥¼ ì˜ë¯¸, â€˜wordâ€™ì˜ ê²½ìš° ê°„ì–´ í•˜ë‚˜ë¥¼ ë‹¨ìœ„ë¡œ, â€˜charâ€™ëŠ” ë¬¸ì í•˜ë‚˜ë¥¼ ë‹¨ìœ„ë¡œ sublinear_tf : ë¬¸ì„œì˜ ë‹¨ì–´ ë¹ˆë„ìˆ˜(tf:term frequency)ì— ëŒ€í•œ smoothing ì—¬ë¶€ë¥¼ ì„¤ì • ngram_range : ë¹ˆë„ì˜ ê¸°ë³¸ ë‹¨ìœ„ë¥¼ ì–´ë–¤ ë²”ìœ„ì˜ n-gramìœ¼ë¡œ ì„¤ì •í•  ê²ƒì¸ì§€ë¥¼ ë³´ëŠ” ì¸ì max_features : ê° ë²¡í„°ì˜ ìµœëŒ€ ê¸¸ì´(íŠ¹ì§•ì˜ ê¸¸ì´)ë¥¼ ì„¤ì • 123456from sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(min_df=0.0, analyzer='char', sublinear_tf=True, ngram_range=(1,3), max_features=5000)X = vectorizer.fit_transform(reviews)X 1train_data.shape í•™ìŠµê³¼ ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬123456789from sklearn.model_selection import train_test_splitimport numpy as npRANDOM_SEED = 42TEST_SPLIT = 0.2y = np.array(sentiments)X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT) class_wight=â€™balancedâ€™ë¡œ ì„¤ì •í•´ì„œ ê° labelì— ëŒ€í•´ ê· í˜• ìˆê²Œ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•œ ê²ƒì´ë‹¤. 1234from sklearn.linear_model import LogisticRegressionlgs = LogisticRegression(class_weight = 'balanced')lgs.fit(X_train, y_train) 1print(\"Accuracy: &#123;&#125;\".format(lgs.score(X_eval, y_eval))) í•„ìëŠ” Accuracy: 0.8676ì„ ì¶œë ¥ìœ¼ë¡œ ë°›ì•˜ë‹¤. validation dataì— ëŒ€í•œ ì„±ëŠ¥ì´ ì•½ 87%ì˜ ì •í™•ë„ë¥¼ ê°–ìœ¼ë¯€ë¡œ test dataì— ëŒ€í•´ì„œë„ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì¼ ê²ƒì´ë¼ê³  ê¸°ëŒ€í•˜ë©° kaggleì— test dataì˜ ì˜ˆì¸¡ê°’ì„ ì œì¶œí•´ ë³¼ ê²ƒì´ë‹¤. ë°ì´í„° ì œì¶œí•˜ê¸° ë§Œë“  ëª¨ë¸ì„ í™œìš©í•´ í‰ê°€ ë°ì´í„° ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ê³  ìºê¸€ì— ì œì¶œí•  ìˆ˜ ìˆë„ë¡ íŒŒì¼ë¡œ ì €ì¥í•  ê²ƒì´ë‹¤. 123456test_data = pd.read_csv('test_clean.csv')testDataVecs = vectorizer.transform(test_data[\"review\"])test_predicted = lgs.predict(testDataVecs)print(test_predicted) 123456if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_OUT_PATH)ids = list(test_data['id'])answer_dataset = pd.DataFrame(&#123;'id' : ids, \"sentiment\" : test_predicted&#125;)answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_tfidf_answer.csv', index=False, quoting=3) 1!kaggle competitions submit word2vec-nlp-tutorial -f \"lgs_tfidf_answer.csv\" -m \"LogisticRegression Model with tf-idf\" Woed2vec(CBOW)ì„ í™œìš©í•œ ëª¨ë¸ êµ¬í˜„ ì´ë²ˆì—ëŠ” word2vecì„ í™œìš©í•´ ëª¨ë¸ì„ êµ¬í˜„í•  ê²ƒì´ë‹¤. ìš°ì„  ê° ë‹¨ì–´ì— ëŒ€í•´ word2vecìœ¼ë¡œ ë²¡í„°í™”í•´ì•¼ í•œë‹¤. word2vecì˜ ê²½ìš° ë‹¨ì–´ë¡œ í‘œí˜„ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ì…ë ¥ê°’ìœ¼ë¡œ ë„£ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì „ì²˜ë¦¬í•œ ë„˜íŒŒì´ ë°°ì—´ì„ ë°”ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ í›„ ê° ë‹¨ì–´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë‚˜ëˆ ì•¼ í•œë‹¤. 123456789101112DATA_IN_PATH = \"/content/\"TRAIN_CLEAN_DATA = 'train_clean.csv'train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)reviews = list(train_data['review'])sentiments = list(train_data['sentiment'])sentences = []for review in reviews: sentences.append(review.split()) word2ve ë²¡í„°í™” num_features : ê° ë‹¨ì–´ì— ëŒ€í•´ ì„ë² ë”©ëœ ë²¡í„°ì˜ ì°¨ì›ì„ ì •í•œë‹¤. min_word_count : ëª¨ë¸ì— ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë¥¼ ê°€ì§€ê³  í•™ìŠµí•˜ê¸° ìœ„í•´ ì ì€ ë¹ˆë„ ìˆ˜ì˜ ë‹¨ì–´ë“¤ì€ í•™ìŠµ í•˜ì§€ ì•Šê¸° ìœ„í•´ ìµœì†Œ ë¹ˆë„ìˆ˜ë¥¼ ì„¤ì •í•œë‹¤. num_workers : ëª¨ë¸ í•™ìŠµ ì‹œ í•™ìŠµì„ ìœ„í•œ í”„ë¡œì„¸ìŠ¤ ê°œìˆ˜ë¥¼ ì§€ì •í•œë‹¤. context : word2vecì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ context ìœˆë„ìš° í¬ê¸°ë¥¼ ì§€ì •í•œë‹¤. downsampling : word2vec í•™ìŠµì„ ìˆ˜í–‰í•  ë•Œ ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ ì •ë‹µ ë‹¨ì–´ labelì— ëŒ€í•œ downsampling ë¹„ìœ¨ì„ ì§€ì •í•œë‹¤. ë³´í†µ 0.001ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ë‹¤ê³  í•œë‹¤. ì°¸ê³ ë¡œ parameter ì¤‘ì— sgì˜ defaultê°’ì¸ 0ì„ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ ì´ ëª¨ë¸ì€ Word2vecì˜ CBOWëª¨ë¸ì´ë‹¤.12345num_features = 300min_word_count = 40num_workers = 4context = 10downsampling = 1e-3 1!pip install gensim word2vecì„ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ ì§„í–‰ ìƒí™©ì„ í™•ì¸í•´ ë³´ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ loggingì„ í†µí•´ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆë‹¤. ë¡œê¹…ì„ í•  ë•Œ formatì„ ìœ„ì™€ ê°™ì´ ì§€ì •í•˜ê³ , ë¡œê·¸ ìˆ˜ì¤€ì€ INFOì— ë§ì¶”ë©´ word2vecì˜ í•™ìŠµê³¼ì •ì—ì„œ ë¡œê·¸ ë©”ì‹œì§€ë¥¼ ì–‘ì‹ì— ë§ê²Œ INFO ìˆ˜ì¤€ìœ¼ë¡œ ë³´ì—¬ì¤€ë‹¤. 12import logginglogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) 1234from gensim.models import word2vecprint(\"Training model ....\")model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling) word2vecìœ¼ë¡œ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì˜ ê²½ìš° ëª¨ë¸ì„ ë”°ë¡œ ì €ì¥í•´ë‘ë©´ ì´í›„ì— ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì €ì¥í•´ ë‘ê³  ì´í›„ì— í•™ìŠµí•œ ê°’ì´ ì¶”ê°€ë¡œ í•„ìš”í•  ê²½ìš° ì‚¬ìš©í•˜ë©´ ëœë‹¤. 12345# ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•œ ë‚´ìš©ì„ ëª¨ë¸ ì´ë¦„ì— ë‹´ëŠ”ë‹¤ë©´ ë‚˜ì¤‘ì— ì°¸ê³ í•˜ê¸°ì— ì¢‹ë‹¤.# ëª¨ë¸ì„ ì €ì¥í•˜ë©´ Word2Vec.load()ë¥¼ í†µí•´ ëª¨ë¸ì„ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.model_name = \"300features_40minwords_10context\"model.save(model_name) word2vec ëª¨ë¸ì„ í™œìš©í•´ì„œ ì„ í˜• íšŒê·€ ëª¨ë¸ì„ í•™ìŠµí•  ê²ƒì´ë‹¤. ìš°ì„  í•™ìŠµì„ í•˜ê¸° ìœ„í•´ì„œëŠ” í•˜ë‚˜ì˜ reviewë¥¼ ê°™ì€ í˜•íƒœì˜ ì…ë ¥ê°’ìœ¼ë¡œ ë§Œë“¤ì–´ì•¼ í•œë‹¤. ì§€ê¸ˆì€ word2vec ëª¨ë¸ì—ì„œ ê° ë‹¨ì–´ê°€ ë²¡í„°ë¡œ í‘œí˜„ë˜ì–´ ìˆë‹¤. ê·¸ë¦¬ê³  review ë§ˆë‹¤ ë‹¨ì–´ì˜ ê°œìˆ˜ê°€ ëª¨ë‘ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì…ë ¥ê°’ì„ í•˜ë‚˜ì˜ í˜•íƒœë¡œ ë§Œë“¤ì–´ì•¼ í•œë‹¤. ì•„ë˜ modelì„ í†µí•´ ì–»ì€ ë‹¨ì–´ í•˜ë‚˜ì˜ featureëŠ” (300,)ì˜ shapeë¥¼ ê°–ê²Œ ë  ê²ƒì´ë‹¤. ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì€ ë¬¸ì¥ì— ìˆëŠ” ëª¨ë“  ë‹¨ì–´ì˜ ë²¡í„°ê°’ì— ëŒ€í•´ í‰ê· ì„ ë‚´ì„œ ë¦¬ë·° í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ë§Œë“œëŠ” ë°©ë²•ì´ ìˆë‹¤. words : ë‹¨ì–´ì˜ ëª¨ìŒì¸ í•˜ë‚˜ì˜ review model : í•™ìŠµí•œ word2vec ëª¨ë¸ num_features : word2vecìœ¼ë¡œ ì„ë² ë”©í•  ë•Œ ì •í–ˆë˜ ë²¡í„°ì˜ ì°¨ì› ìˆ˜ 1234567891011121314151617def get_features(words, model, num_features): # ì¶œë ¥ ë²¡í„° ì´ˆê¸°í™” feature_vector = np.zeros((num_features), dtype=np.float32) num_words = 0 # ì–´íœ˜ì‚¬ì „ ì¤€ë¹„ index2word_set = set(model.wv.index2word) for w in words: if w in index2word_set: num_words +=1 # ì‚¬ì „ì— í•´ë‹¹í•˜ëŠ” ë‹¨ì–´ì— ëŒ€í•´ ë‹¨ì–´ ë²¡í„°ë¥¼ ë”í•¨ feature_vector = np.add(feature_vector, model[w]) # ë¬¸ì¥ì˜ ë‹¨ì–´ ìˆ˜ë§Œí¼ ë‚˜ëˆ„ì–´ ë‹¨ì–´ ë²¡í„°ì˜ í‰ê· ê°’ì„ ë¬¸ì¥ ë²¡í„°ë¡œ í•¨ feature_vector = np.divide(feature_vector, num_words) return feature_vector 123456789def get_dataset(reviews, model, num_features): dataset = list() for s in reviews: dataset.append(get_features(s, model, num_features)) reviewFeatureVecs = np.stack(dataset) return reviewFeatureVecs 1train_data_vecs = get_dataset(sentences, model, num_features) í•™ìŠµê³¼ ê²€ì¦ ë°ì´í„°ì…‹ ë¶„ë¦¬12345678910from sklearn.model_selection import train_test_splitimport numpy as npX = train_data_vecsy = np.array(sentiments)RANDOM_SEED = 42TEST_SPLIT = 0.2X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED) ëª¨ë¸ ì„ ì–¸ ë° í•™ìŠµ1234from sklearn.linear_model import LogisticRegressionlgs = LogisticRegression(class_weight='balanced')lgs.fit(X_train, y_train) ê²€ì¦ ë°ì´í„°ì…‹ì„ ì´ìš©í•œ ì„±ëŠ¥ í‰ê°€ ì´ì „ì˜ TF-IDFë¥¼ ì‚¬ìš©í•´ì„œ í•™ìŠµí•œ ê²ƒë³´ë‹¨ ìƒëŒ€ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤. word2vecì´ ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ë³´ëŠ” ê´€ì ì—ì„œëŠ” ë¶„ëª…íˆ íš¨ê³¼ì ì¼ ìˆ˜ëŠ” ìˆì§€ë§Œ word2vecì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í•­ìƒ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¥í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ë‹¤ì‹œ í•œë²ˆ ì•Œ ìˆ˜ ìˆë‹¤!!! 1print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval)) validation dataì— ëŒ€í•œ ì •í™•ë„ëŠ” 83%ì •ë„ë¡œ TF-IDFë¡œ í–ˆë˜ ê²ƒë³´ë‹¨ ì¡°ê¸ˆ ë–¨ì–´ì§€ì§€ë§Œ ìºê¸€ì— ì œì¶œí•´ë³´ê³  overfittingì´ ë°œìƒí–ˆëŠ”ì§€ ì ê²€í•´ ë³¸ë‹¤. 12345TEST_CLEAN_DATA = 'test_clean.csv'test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)test_review = list(test_data['review']) 123test_sentences = []for review in test_review: test_sentences.append(review.split()) 1test_data_vecs = get_dataset(test_sentences, model, num_features) 123456789DATA_OUT_PATH = '/content/'test_predicted = lgs.predict(test_data_vecs)if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_OUT_PATH)test_data['id']=test_data['id'].apply(lambda x : x[1:-1])ids = list(test_data['id']) 12answer_dataset = pd.DataFrame(&#123;'id': ids, 'sentiment': test_predicted&#125;)answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_answer.csv', index=False) 1!kaggle competitions submit word2vec-nlp-tutorial -f \"lgs_answer.csv\" -m \"LogisticRegression Model with Word2vec\" ëœë¤í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ ëª¨ë¸CountVectorizerë¥¼ í™œìš©í•œ ë²¡í„°í™” CountVectorizerëŠ” TF-IDF vectorizingê³¼ ë™ì¼í•˜ê²Œ ë¬¸ì¥ì„ inputìœ¼ë¡œ ë°›ê¸° ë•Œë¬¸ì— Word2vecì²˜ëŸ¼ ê³µë°±ë‹¨ìœ„ë¡œ ìª¼ê°œ ë‹¨ì–´ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤. 12345678import pandas as pdDATA_IN_PATH = '/content/'TRAIN_CLEAN_DATA = 'train_clean.csv'train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)reviews = list(train_data['clean_review'])y = np.array(train_data['sentiment']) 12345from sklearn.feature_extraction.text import CountVectorizervectorizer = CountVectorizer(analyzer = 'word', max_features = 5000)train_data_features = vectorizer.fit_transform(reviews) í•™ìŠµê³¼ ê²€ì¦ ë°ì´í„° ë¶„ë¦¬1234TEST_SIZE = 0.2RANDOM_SEED = 42train_input, eval_input, train_label, eval_label = train_test_split(train_data_features, y, test_size=TEST_SIZE, random_state=RANDOM_SEED) ëª¨ë¸ êµ¬í˜„ ë° í•™ìŠµ1234567from sklearn.ensemble import RandomForestClassifier# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ë¶„ë¥˜ê¸°ì— 100ê°œì˜ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•œë‹¤.forest = RandomForestClassifier(n_estimators=100)# ë‹¨ì–´ ë¬¶ìŒì„ ë²¡í„°í™”í•œ ë°ì´í„°ì™€ ì •ë‹µ ë°ì´í„°ë¥¼ ê°€ì§€ê³  í•™ìŠµì„ ì‹œì‘í•œë‹¤.forest.fit(train_input, train_label) ê²€ì¦ ë°ì´í„°ì…‹ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ë¥¼ ë³´ë©´ ëŒ€ëµ 85%ì˜ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤. ì•™ìƒë¸” ëª¨ë¸ì¸ë°ë„ ì•ì„œ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ëª¨ë¸(TF_IDFë³´ë‹¨ ìƒëŒ€ì ìœ¼ë¡œ)ë³´ë‹¤ ì¢‹ì§€ ì•Šì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ë¬¸ì œì¼ ìˆ˜ë„ ìˆê³  ë°ì´í„°ì—ì„œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì˜ ë¬¸ì œì¼ ìˆ˜ë„ ìˆë‹¤. ì¦‰, ëª¨ë¸ì„ ë°”ê¾¸ì§€ ì•Šë”ë¼ë„ íŠ¹ì§• ì¶”ì¶œ ë°©ë²•ì„ ì•ì„œ ì‚¬ìš©í•œ TF-IDFë‚˜ word2vecì„ ì‚¬ìš©í•´ì„œ ì…ë ¥ê°’ì„ ë§Œë“ ë‹¤ë©´ ì„±ëŠ¥ì´ ë†’ì•„ì§ˆ ìˆ˜ ìˆë‹¤. 1print(\"Accuracy: %f\" % forest.score(eval_input, eval_label)) ë°ì´í„° ì œì¶œ1234567TEST_CLEAN_DATA = 'test_clean.csv'DATA_OUT_PATH = '/content/'test_data = pd.read_csv(DATA_OUT_PATH + TEST_CLEAN_DATA)test_reviews = list(test_data['review'])ids = list(test_data['id']) 1test_data_features = vectorizer.transform(test_reviews) 12345678if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_OUT_PATH)result = forest.predict(test_data_features)output = pd.DataFrame(&#123;'id': ids, \"sentiment\": result&#125;)output.to_csv(DATA_OUT_PATH + 'Randomforest_model_with_Countvectorizer.csv', index=False, quoting=3) 1!kaggle competitions submit word2vec-nlp-tutorial -f \"Randomforest_model_with_Countvectorizer.csv\" -m \"Randomforest Model with Countvectorizer\"","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP ì‹¤ìŠµ í…ìŠ¤íŠ¸ ë¶„ë¥˜ -01","slug":"NLP_03","date":"2020-01-29T14:40:09.000Z","updated":"2020-02-01T09:00:50.736Z","comments":true,"path":"2020/01/29/NLP_03/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/29/NLP_03/","excerpt":"","text":"ì˜ì–´ í…ìŠ¤íŠ¸ ë¶„ë¥˜ í•œêµ­ì–´ëŠ” ë„ì–´ì“°ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ìƒëŒ€ì ìœ¼ë¡œ ì „ì²˜ë¦¬í•˜ê¸° ì‰¬ìš´ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ê°€ì§€ê³  ë¨¼ì € ê°ê°ì„ í‚¤ì›Œë³´ê² ë‹¤. ë°ì´í„° ì´ë¦„ : Bag of Words Meets Bags of Popcorn ë°ì´í„° ìš©ë„ : í…ìŠ¤íŠ¸ ë¶„ë¥˜ í•™ìŠµì„ ëª©ì ìœ¼ë¡œ ì‚¬ìš© ë°ì´í„° ê¶Œí•œ : MIT ë°ì´í„° ì¶œì²˜ : https://www.kaggle.com/c/word2vec-nlp-tutorial/data ë¬¸ì œ ì†Œê°œì˜ì–´ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë¬¸ì œ ì¤‘ ìºê¸€ì˜ ëŒ€íšŒì¸ ì›Œë“œíŒì½˜ ë¬¸ì œë¥¼ í™œìš©í•  ê²ƒì´ë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©´ì„œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ê¸°ìˆ ì„ ì•Œì•„ë³¼ê²ƒì´ë‹¤. ì›Œë“œ íŒì½˜ ì›Œë“œ íŒì½˜ì€ ì¸í„°ë„· ì˜í™” ë°ì´í„°ë² ì´ìŠ¤(IMDB)ì—ì„œ ë‚˜ì˜¨ ì˜í™” í‰ì  ë°ì´í„°ë¥¼ í™œìš©í•œ ìºê¸€ ë¬¸ì œë‹¤. ì˜í™” í‰ì  ë°ì´í„°ì´ë¯€ë¡œ ê° ë°ì´í„°ëŠ” ì˜í™” ë¦¬ë·° í…ìŠ¤íŠ¸ì™€ í‰ì ì— ë”°ë¥¸ ê°ì • ê°’(ê¸ì • í˜¹ì€ ë¶€ì •)ìœ¼ë¡œ êµ¬ì„±ë¼ ìˆë‹¤. ì´ ë°ì´í„°ëŠ” ë³´í†µ ê°ì„± ë¶„ì„(sentiment analysis) ë¬¸ì œì—ì„œ ìì£¼ í™œìš©ëœë‹¤. ëª©í‘œ 1) ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì •ì œë˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ í™œìš©í•˜ê¸° ì‰½ê²Œ ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì • 2) ë°ì´í„° ë¶„ì„ ê³¼ì • ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì–´ë–»ê²Œ ë¬¸ì œë¥¼ í’€ì–´ê°€ì•¼ í• ì§€ ì ‘ê·¼í•˜ëŠ” ê³¼ì • 3) ì‹¤ì œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì•Œê³ ë¦¬ì¦˜ì„ ëª¨ë¸ë§í•˜ëŠ” ê³¼ì • ìºê¸€ APIë¥¼ colabì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì¸ì¦ ë° google storageì— ì—…ë¡œë“œ ë˜ì–´ìˆëŠ” ì¸ì¦í‚¤ íŒŒì¼ í˜„ì¬ colab pwdë¡œ ë³µì‚¬í•´ì˜¨ í›„ ì„¤ì •ì™„ë£Œí•˜ê¸° 12345678from google.colab import authimport warnings%matplotlib inline%config InlineBackend.figure_format = 'retina'warnings.filterwarnings(\"ignore\")auth.authenticate_user()!gsutil cp gs://kaggle_key/kaggle.json kaggle.json 1234567!mkdir -p ~/.kaggle!mv ./kaggle.json ~/.kaggle/!chmod 600 ~/.kaggle/kaggle.json!pip install kaggle# ìºê¸€ competition ëª©ë¡í™•ì¸#!kaggle competitions list ëª©í‘œ competitionì˜ ë°ì´í„° ë‹¤ìš´ë¡œë“œ 12345# íŒŒì¼ í™•ì¸!kaggle competitions files -c word2vec-nlp-tutorial# íŒŒì¼ ë‹¤ìš´ë¡œë“œ!kaggle competitions download -c word2vec-nlp-tutorial ë°ì´í„° ë¶„ì„ ë° ì „ì²˜ë¦¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ì „ì— ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ëŠ” ê³¼ì •ì„ ê±°ì³ì•¼ í•œë‹¤. ì „ì²˜ë¦¬ëŠ” ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì ìš©í•˜ê¸°ì— ì í•©í•˜ë„ë¡ ë°ì´í„°ë¥¼ ì •ì œí•˜ëŠ” ê³¼ì •ì´ë‹¤. ê·¸ì „ì— ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ë¶„ì„í•˜ëŠ” ê³¼ì •ì„ ì„ í–‰í•  ê²ƒì´ë‹¤. EDAê³¼ì •ì„ ê±°ì¹œ í›„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ì²˜ë¦¬ ì‘ì—…ì„ í•  ê²ƒì´ë‹¤. ì°¸ê³ ë¡œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ”ë° 403 errorê°€ ì¶œë ¥ëœë‹¤ë©´, ìš°ì„ ì ìœ¼ë¡œ ëŒ€íšŒì˜ ruleì„ checkí–ˆëŠ”ì§€ í™•ì¸í•´ ë³´ì•„ì•¼ í•œë‹¤. sampleSubmission.csv íŒŒì¼ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ íŒŒì¼ì´ zipìœ¼ë¡œ ì••ì¶•ë¼ ìˆê¸° ë•Œë¬¸ì— ì••ì¶•ì„ í‘¸ëŠ” ê³¼ì •ë¶€í„° ì‹œì‘í•œë‹¤. ì••ì¶•ì„ í’€ê¸° ìœ„í•´ zipfileì´ë¼ëŠ” ë‚´ì¥ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. - ì••ì¶•ì„ í’€ê¸° ìœ„í•´ ê²½ë¡œì™€ ì••ì¶•ì„ í’€ íŒŒì¼ëª…ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì„ ì–¸í•œ í›„ ë°˜ë³µë¬¸ì„ ì‚¬ìš©í•´ ì••ì¶•ì„ í’€ ê²ƒì´ë‹¤. 123456789101112import zipfileDATA_IN_PATH = '/content/'file_list = ['labeledTrainData.tsv.zip', 'testData.tsv.zip', 'unlabeledTrainData.tsv.zip']for file in file_list: # ì••ì¶•í’€ê¸° ëŒ€ìƒ ì„¤ì • ë° ëª¨ë“œ ì„¤ì • zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r') # ì••ì¶• í’€ê¸° ë° ì €ì¥ ê²½ë¡œ ì„¤ì • zipRef.extractall(DATA_IN_PATH) # í˜¸ì¶œ ì¢…ë£Œ zipRef.close() 12345678import numpy as npimport pandas as pdimport osimport matplotlib.pyplot as pltimport seaborn as sns# ê·¸ë˜í”„ë¥¼ ë°”ë¡œ ê·¸ë¦¬ë„ë¡ í•¨%matplotlib inline í˜„ì¬ ì‚¬ìš©í•  ë°ì´í„°ëŠ” tap(\\t)ìœ¼ë¡œ êµ¬ë¶„ë¼ ìˆìœ¼ë¯€ë¡œ delimeter=â€™\\tâ€™ë¡œ ì„¤ì •í•´ì£¼ì—ˆê³ , ê° ë°ì´í„°ì— ê° í•­ëª©ëª…(Header)ì´ í¬í•¨ë¼ ìˆê¸° ë•Œë¬¸ì— headerì¸ìì— 0ì„ ì„¤ì •í•œë‹¤. Rì—ì„œëŠ” header=Tureë¡œ í•˜ëŠ” ì—­í• ê³¼ ê°™ë‹¤ê³  ë³´ë©´ëœë‹¤. ê·¸ë¦¬ê³  ìŒë”°ì˜´í‘œë¥¼ ë¬´ì‹œí•˜ê¸° ìœ„í•´ quoting=3ì„ ì„¤ì •í•´ ì£¼ì—ˆë‹¤. 123train_data = pd.read_csv(DATA_IN_PATH+\"labeledTrainData.tsv\", header=0, delimiter='\\t', quoting=3)train_data.head() ë°ì´í„° ë¶„ì„ ì§„í–‰ ìˆœì„œ 1) ë°ì´í„° í¬ê¸° 2) ë°ì´í„°ì˜ ê°œìˆ˜ 3) ê° ë¦¬ë·°ì˜ ë¬¸ì ê¸¸ì´ ë¶„í¬ 4) ë§ì´ ì‚¬ìš©ëœ ë‹¨ì–´ 5) ê¸ì •, ë¶€ì • ë°ì´í„°ì˜ ë¶„í¬ 6) ê° ë¦¬ë·°ì˜ ë‹¨ì–´ ê°œìˆ˜ ë¶„í¬ 7) íŠ¹ìˆ˜ë¬¸ì ë° ëŒ€ë¬¸ì, ì†Œë¬¸ì ë¹„ìœ¨ 1234567891011# ë°ì´í„° í¬ê¸°print(\"íŒŒì¼ í¬ê¸° : \")for file in os.listdir(DATA_IN_PATH): if 'tsv' in file and 'zip' not in file: print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + 'MB')# í•™ìŠµ ë°ì´í„°ì˜ ê°œìˆ˜print('ì „ì²´ í•™ìŠµ ë°ì´í„°ì˜ ê°œìˆ˜: &#123;&#125;'.format(len(train_data)))# ê²°ê³¼# ì „ì²´ í•™ìŠµ ë°ì´í„°ì˜ ê°œìˆ˜: 25000 ê° reviewì˜ ê¸¸ì´ë¥¼ ë¶„ì„ 12train_length = train_data['review'].apply(len)train_length.head() ê° ë¦¬ë·°ì˜ ë¬¸ì ê¸¸ì´ê°€ ëŒ€ë¶€ë¶„ 6,000 ì´í•˜ì´ê³  ëŒ€ë¶€ë¶„ 2,000ì´í•˜ì— ë¶„í¬ë¼ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì¼ë¶€ ë°ì´í„°ì˜ ê²½ìš° ì´ìƒì¹˜ë¡œ 10,000 ì´ìƒì˜ ê°’ì„ ê°€ì§€ê³  ìˆë‹¤. 123456789101112plt.figure(figsize=(12, 5))plt.hist(train_length, bins=200, alpha=0.5, color='r', label='word')# yì¶•ì˜ ë²”ìœ„ë¥¼ logë‹¨ìœ„ë¡œ ë°”ê¿”ì£¼ê³  non-positiveì— ëŒ€í•´ì„œëŠ” ì•„ì£¼ì‘ì€ ì–‘ìˆ˜ë¡œ í´ë¦¬í•‘í•œë‹¤.plt.yscale('log', nonposy='clip')plt.title('Log-Histogram of length of review')plt.xlabel('Length of review')plt.ylabel('Number of review') 12# ê¸°ì´ˆ í†µê³„ëŸ‰ í™•ì¸train_length.describe() 123plt.figure(figsize=(12, 5))plt.boxplot(train_length, labels=['train data review length'], showmeans=True) wordcloudë¥¼ í†µí•´ ì‹œê°ì ìœ¼ë¡œ ë¹ˆë„ìˆ˜ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ ì„¤ì¹˜í•œë‹¤. ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ë³´ë©´ ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ ë‹¨ì–´ëŠ” brì´ë¼ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. HTML íƒœê·¸ì¸ br í•´ë‹¹ ë°ì´í„°ê°€ ë†’ì€ ë¹ˆë„ìˆ˜ë¥¼ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ ë¯¸ë£¨ì–´ë³´ì•„ ì •ì œë˜ì§€ ì•Šì€ ì¸í„°ë„· ìƒì˜ ë¦¬ë·° í˜•íƒœë¡œ ì‘ì„±ë¼ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´í›„ ì „ì²˜ë¦¬ ì‘ì—…ì—ì„œ ì´ íƒœê·¸ë“¤ì„ ëª¨ë‘ ì œê±°í•˜ê² ë‹¤. 1!pip install wordcloud 12345from wordcloud import WordCloudcloud = WordCloud(width=800, height=600).generate(' '.join(train_data['review']))plt.figure(figsize=(20, 15))plt.imshow(cloud)plt.axis('off') ì´ì œ ê° ë¼ë²¨ì˜ ë¶„í¬ë¥¼ í™•ì¸í•´ ë³¸ë‹¤. í•´ë‹¹ ë°ì´í„°ì˜ ê²½ìš° ê¸ì •ê³¼ ë¶€ì •ì´ë¼ëŠ” ë‘ ê°€ì§€ ë¼ë²¨ë§Œ ê°€ì§€ê³  ìˆë‹¤. ë¶„í¬ì˜ ê²½ìš° ë˜ ë‹¤ë¥¸ ì‹œê°í™” ë„êµ¬ì¸ seabornì„ ì‚¬ìš©í•´ ì‹œê°í™”í•˜ê² ë‹¤. labelì˜ ë¶„í¬ ê·¸ë˜í”„ë¥¼ ë³´ë©´ ê±°ì˜ ë™ì¼í•œ ê°œìˆ˜ë¡œ ë¶„í¬ë¼ ìˆìŒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. 123fig, axe = plt.subplots(ncols=1)fig.set_size_inches(6, 3)sns.countplot(train_data['sentiment']) 123456print(\"ê¸ì • ë¦¬ë·° ê°œìˆ˜: &#123;&#125;\".format(train_data['sentiment'].value_counts()[1]))print(\"ë¶€ì • ë¦¬ë·° ê°œìˆ˜: &#123;&#125;\".format(train_data['sentiment'].value_counts()[0]))# ê²°ê³¼# ê¸ì • ë¦¬ë·° ê°œìˆ˜: 12500# ë¶€ì • ë¦¬ë·° ê°œìˆ˜: 12500 ê° ë¦¬ë·°ë¥¼ ë‹¨ì–´ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ ì„œ ê° ë¦¬ë·°ë‹¹ ë‹¨ì–´ì˜ ê°œìˆ˜ë¥¼ í™•ì¸í•´ ë³¸ë‹¤. ë‹¨ì–´ëŠ” ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ í•˜ë‚˜ì˜ ë‹¨ì–´ë¼ ìƒê°í•˜ê³  ê°œìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤. ìš°ì„  ê° ë‹¨ì–´ì˜ ê¸¸ì´ë¥¼ ê°€ì§€ëŠ” ë³€ìˆ˜ë¥¼ í•˜ë‚˜ ì„¤ì •í•˜ì. 1train_word_counts = train_data['review'].apply(lambda x: len(x.split(' '))) ëŒ€ë¶€ë¶„ì˜ ë‹¨ì–´ê°€ 1000ê°œ ë¯¸ë§Œì˜ ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ìˆê³ , ëŒ€ë¶€ë¶„ 200ê°œ ì •ë„ì˜ ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.1234567plt.figure(figsize=(15,10))plt.hist(train_word_counts, bins=50, facecolor='r', label='train')plt.title('Log-Histogram of word count in review', fontsize=15)plt.yscale('log', nonposy='clip')plt.legend()plt.xlabel('Number of words', fontsize=15)plt.ylabel('Number of reviews', fontsize=15) reviewì˜ 75%ê°€ 300ê°œ ì´í•˜ì˜ ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ìˆìŒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.1train_word_counts.describe() ë§ˆì§€ë§‰ìœ¼ë¡œ ê° reviewì— ëŒ€í•´ êµ¬ë‘ì ê³¼ ëŒ€ì†Œë¬¸ì ë¹„ìœ¨ ê°’ì„ í™•ì¸í•œë‹¤. ëŒ€ë¶€ë¶„ ë§ˆì¹¨í‘œë¥¼ í¬í•¨í•˜ê³  ìˆê³ , ëŒ€ë¬¸ìë„ ëŒ€ë¶€ë¶„ ì‚¬ìš©í•˜ê³  ìˆë‹¤. ë”°ë¼ì„œ ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ëŒ€ë¬¸ìì˜ ê²½ìš° ëª¨ë‘ ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³  íŠ¹ìˆ˜ ë¬¸ìì˜ ê²½ìš° ì œê±°í•œë‹¤. ì´ ê³¼ì •ì€ í•™ìŠµì— ë°©í•´ê°€ ë˜ëŠ” ìš”ì†Œë“¤ì„ ì œê±°í•˜ê¸° ìœ„í•¨ì´ë‹¤. 1234567891011121314151617181920212223# ë¬¼ìŒí‘œê°€ êµ¬ë‘ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë¹„ìœ¨qmarks = np.mean(train_data['review'].apply(lambda x : '?' in x))# ë§ˆì¹¨í‘œê°€ êµ¬ë‘ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë¹„ìœ¨fullstop = np.mean(train_data['review'].apply(lambda x : '.' in x))# ì²« ë²ˆì§¸ ëŒ€ë¬¸ìì˜ ë¹„ìœ¨capital_first = np.mean(train_data['review'].apply(lambda x : x[0].isupper()))# ëŒ€ë¬¸ì ë¹„ìœ¨capitals = np.mean(train_data['review'].apply(lambda x : max([y.isupper() for y in x])))# ìˆ«ì ë¹„ìœ¨numbers = np.mean(train_data['review'].apply(lambda x : max([y.isdigit() for y in x])))print('ë¬¼ìŒí‘œê°€ ìˆëŠ” ì§ˆë¬¸: &#123;:.2f&#125;%'.format(qmarks * 100))print('ë§ˆì¹¨í‘œê°€ ìˆëŠ” ì§ˆë¬¸: &#123;:.2f&#125;%'.format(fullstop * 100))print('ì²« ê¸€ìê°€ ëŒ€ë¬¸ìì¸ ì§ˆë¬¸: &#123;:.2f&#125;%'.format(capital_first * 100))print('ëŒ€ë¬¸ìê°€ ìˆëŠ” ì§ˆë¬¸: &#123;:.2f&#125;%'.format(capitals * 100))print('ìˆ«ìê°€ ìˆëŠ” ì§ˆë¬¸: &#123;:.2f&#125;%'.format(numbers * 100))# ê²°ê³¼# ë¬¼ìŒí‘œê°€ ìˆëŠ” ì§ˆë¬¸: 29.55%# ë§ˆì¹¨í‘œê°€ ìˆëŠ” ì§ˆë¬¸: 99.69%# ì²« ê¸€ìê°€ ëŒ€ë¬¸ìì¸ ì§ˆë¬¸: 0.00%# ëŒ€ë¬¸ìê°€ ìˆëŠ” ì§ˆë¬¸: 99.59%# ìˆ«ìê°€ ìˆëŠ” ì§ˆë¬¸: 56.66% ë°ì´í„° ì „ì²˜ë¦¬ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œë‹¤. ë¨¼ì € ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ë¶ˆëŸ¬ì˜¬ ê²ƒì´ë‹¤. ìš°ì„  ì „ì²˜ë¦¬ë¥¼ ìœ„í•´ nltkì˜ stopwordë¥¼ ì´ìš©í•˜ê¸° ìœ„í•´ nltkì—ì„œ ë‹¤ìš´ë¡œë“œë¥¼ ë°›ì•„ì•¼í•œë‹¤. í•„ìëŠ” allë¥¼ ì„ íƒí•˜ì—¬ì„œ ëª¨ë“  íŒŒì¼ì„ downloadë¥¼ ë°›ì•˜ì§€ë§Œ, stopwordë§Œ ë°›ì•„ë„ ìƒê´€ì—†ë‹¤. re, BeautifulSoup : ë°ì´í„° ì •ì œ stopwords : ë¶ˆìš©ì–´ ì œê±° pad_sequence, Tokenizer : ë°ì´í„° ì „ì²˜ë¦¬ 12345678910import reimport pandas as pdimport numpy as npimport jsonfrom bs4 import BeautifulSoupfrom nltk.corpus import stopwordsfrom tensorflow.python.keras.preprocessing.sequence import pad_sequencesfrom tensorflow.python.keras.preprocessing.text import Tokenizerimport nltknltk.download() ë¦¬ë·° ë°ì´í„°ë¥¼ ë³´ë©´ ë¬¸ì¥ ì‚¬ì´ì— ê³¼ ê°™ì€ HTML íƒœê·¸ì™€ â€˜\\â€™, â€˜â€¦â€™ ê°™ì€ íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ëœ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë¬¸ì¥ë¶€í˜¸ ë° íŠ¹ìˆ˜ë¬¸ìëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ìì˜ ì˜ë¯¸ì— í¬ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ìµœì í™”ëœ í•™ìŠµì„ ìœ„í•´ ì œê±°í•˜ì BeautifulSoupì˜ get_textí•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ HTML íƒœê·¸ë¥¼ ì œê±°í•œ ë‚˜ë¨¸ì§€ í…ìŠ¤íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆê³  re.subì„ ì´ìš©í•´ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì œê±°í•œë‹¤. stopwords(ë¶ˆìš©ì–´)ë¥¼ ì‚­ì œí•  ê²ƒì´ë‹¤. ë¶ˆìš©ì–´ë€ ë¬¸ì¥ì—ì„œ ìì£¼ ì¶œí˜„í•˜ë‚˜ ì „ì²´ì ì¸ ì˜ë¯¸ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ë‹¨ì–´ë¥¼ ë§í•œë‹¤. ì˜ˆë¥¼ë“¤ì–´, ì˜ì–´ì—ì„œëŠ” ì¡°ì‚¬, ê´€ì‚¬ ë“±ê³¼ ê°™ì€ ì–´íœ˜ê°€ ìˆë‹¤. ë°ì´í„°ì— ë”°ë¼ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ëŠ” ê²ƒì€ ì¥ë‹¨ì ì´ ìˆë‹¤. ê²½ìš°ì— ë”°ë¼ ë¶ˆìš©ì–´ê°€ í¬í•¨ëœ ë°ì´í„°ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë° ìˆì–´ ë…¸ì´ì¦ˆë¥¼ ì¤„ ìˆ˜ ìˆëŠ” ìš”ì¸ì´ ë  ìˆ˜ ìˆì–´ ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ìˆ˜ ìˆë‹¤. ê·¸ë ‡ì§€ë§Œ ë°ì´í„°ê°€ ë§ê³  ë¬¸ì¥ êµ¬ë¬¸ì— ëŒ€í•œ ì „ì²´ì ì¸ íŒ¨í„´ì„ ëª¨ë¸ë§í•˜ê³ ì í•œë‹¤ë©´ ì´ëŠ” ì—­íš¨ê³¼ë¥¼ ì¤„ ìˆ˜ë„ ìˆë‹¤. ì§€ê¸ˆ ì‹œí–‰í•˜ê³ ì í•˜ëŠ” ë¶„ì„ì€ ê°ì„± ë¶„ì„ì„ í•˜ê³  ìˆìœ¼ë¯€ë¡œ ë¶ˆìš©ì–´ê°€ ê°ì • íŒë‹¨ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤ê³  ê°€ì •í•˜ê³  ë¶ˆìš©ì–´ë¥¼ ì œê±°í•œë‹¤. ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ë ¤ë©´ ë”°ë¡œ ì •ì˜í•œ ë¶ˆìš©ì–´ ì‚¬ì „ì„ ì´ìš©í•´ì•¼ í•œë‹¤. ì‚¬ìš©ìê°€ ì§ì ‘ ì •ì˜í•  ìˆ˜ë„ ìˆì§€ë§Œ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ê²½ìš°ê°€ ë„ˆë¬´ ë§ì•„ì„œ ë³´í†µ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì •ì˜í•´ë†“ì€ ë¶ˆìš©ì–´ ì‚¬ì „ì„ ì´ìš©í•œë‹¤. NLTKì˜ ë¶ˆìš©ì–´ ì‚¬ì „ì„ ì´ìš©í•  ê²ƒì´ë©°, NLTKì—ì„œ ì œê³µí•˜ëŠ” ë¶ˆìš©ì–´ ì‚¬ì „ì€ ì „ë¶€ ì†Œë¬¸ì ë‹¨ì–´ë¡œ êµ¬ì„±ë¼ ìˆê¸° ë•Œë¬¸ì— ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ê¸° ìœ„í•´ì„œëŠ” ëª¨ë“  ë‹¨ì–´ë¥¼ ì†Œë¬¸ìë¡œ ë°”ê¿”ì•¼í•œë‹¤. review_textë¥¼ lowerí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ëª¨ë‘ ì†Œë¬¸ìë¡œ ë°”ê¿”ì£¼ì—ˆê³ , ì´í›„ split í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê³µë°±ì„ ê¸°ì¤€ìœ¼ë¡œ reivew_textë¥¼ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¾¼ í›„ ë¶ˆìš©ì–´ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ë‹¨ì–´ë§Œ ë‹¤ì‹œ ëª¨ì•„ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì—ˆë‹¤. ê²°ê³¼ë¥¼ ë³´ë©´ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ê°€ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë°”ë€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë°ì´í„°ë¥¼ í•œë²ˆì— ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ìœ„ì˜ ê³¼ì •ì„ í•˜ë‚˜ì˜ í•¨ìˆ˜ë¡œ ì‘ì„±í•œë‹¤ìŒì— applyë¡œ ì ìš©ì‹œí‚¨ë‹¤. í•¨ìˆ˜ì˜ ê²½ìš° ë¶ˆìš©ì–´ ì œê±°ëŠ” ì¸ìê°’ìœ¼ë¡œ ë°›ì•„ì„œ ì„ íƒí•  ìˆ˜ ìˆê²Œ í•˜ì˜€ë‹¤. 1234567891011121314151617181920212223242526272829def preprocessing(review, remove_stopwords=False): # ë¶ˆìš©ì–´ ì œê±°ëŠ” ì˜µì…˜ìœ¼ë¡œ ì„ íƒ # 1. HTML íƒœê·¸ ì œê±° review_text = BeautifulSoup(review, 'html5lib').get_text() # 2. ì˜ì–´ê°€ ì•„ë‹Œ íŠ¹ìˆ˜ë¬¸ìë¥¼ ê³µë°±(\" \")ìœ¼ë¡œ ëŒ€ì²´ review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text) # 3. ëŒ€ë¬¸ìë¥¼ ì†Œë¬¸ìë¡œ ë°”ê¾¸ê³  ê³µë°± ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“ ë‹¤. words = review_text.lower().split() if remove_stopwords: # 4. ë¶ˆìš©ì–´ ì œê±° # ì˜ì–´ ë¶ˆìš©ì–´ ë¶ˆëŸ¬ì˜¤ê¸° stops = set(stopwords.words('english')) # ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ ë‹¨ì–´ë¡œ ì´ë¤„ì§„ ìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ ìƒì„± words = [w for w in words if not w in stops] # 5. ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê³µë°±ì„ ë„£ì–´ì„œ í•˜ë‚˜ì˜ ê¸€ë¡œ í•©ì¹œë‹¤. clean_review = ' '.join(words) else: # ë¶ˆìš©ì–´ë¥¼ ì œê±°í•˜ì§€ ì•Šì„ ë•Œ clean_review = ' '.join(words) return clean_review 123train_data['clean_review']=train_data['review'].apply(lambda x : preprocessing(review=x, remove_stopwords=True))train_data['clean_review'][0] ìš°ì„  ì „ì²˜ë¦¬í•œ ë°ì´í„°ì—ì„œ ê° ë‹¨ì–´ë¥¼ ì¸ë±ìŠ¤ë¡œ ë²¡í„°í™”í•´ì•¼ í•œë‹¤. ê·¸ë¦¬ê³  ëª¨ë¸ì— ë”°ë¼ ì…ë ¥ê°’ì˜ ê¸¸ì´ê°€ ë™ì¼í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì¼ì • ê¸¸ì´ë¡œ ìë¥´ê³  ë¶€ì¡±í•œ ë¶€ë¶„ì€ íŠ¹ì •ê°’ìœ¼ë¡œ ì±„ìš°ëŠ” íŒ¨ë”© ê³¼ì •ì„ ì§„í–‰í•´ì•¼ í•œë‹¤. í•˜ì§€ë§Œ ëª¨ë¸ì— ë”°ë¼ ê° reviewê°€ ë‹¨ì–´ë“¤ì˜ ì¸ë±ìŠ¤ë¡œ êµ¬ì„±ëœ ë²¡í„°ê°€ ì•„ë‹Œ í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë¼ì•¼ í•˜ëŠ” ê²½ìš°ë„ ìˆë‹¤. ë”°ë¼ì„œ ì§€ê¸ˆê¹Œì§€ ì „ì²˜ë¦¬í•œ ë°ì´í„°ë¥¼ pandasì˜ DataFrameìœ¼ë¡œ ë§Œë“¤ì–´ ë‘ê³  ì´í›„ì— ì „ì²˜ë¦¬ ê³¼ì •ì´ ëª¨ë‘ ëë‚œ í›„ ì „ì²˜ë¦¬í•œ ë°ì´í„°ë¥¼ ì €ì¥í•  ë•Œ í•¨ê»˜ ì €ì¥í•˜ê²Œ í•œë‹¤. 123456# from tensorflow.python.keras.preprocessing.sequence import pad_sequences# from tensorflow.python.keras.preprocessing.text import Tokenizertokenizer = Tokenizer()tokenizer.fit_on_texts(train_data['clean_review'])text_sequences = tokenizer.texts_to_sequences(train_data['clean_review']) ìœ„ì™€ ê°™ì´ ì‚¬ì „ì— ë“±ë¡ë˜ì–´ì§„ ì¸ë±ìŠ¤ë¡œ ê° reviewì˜ ê°’ë“¤ì´ ë³€ê²½ë˜ì—ˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. ë‹¨ì–´ ì‚¬ì „ì€ ì•ì„œ ì •ì˜í•œ tokenizer ê°ì²´ì— word_index ê°’ì„ ë½‘ìœ¼ë©´ dictionary í˜•íƒœë¡œ êµ¬ì„±ë˜ì–´ ìˆìŒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ë˜í•œ ë‹¨ì–´ ì‚¬ì „ ë¿ë§Œ ì•„ë‹ˆë¼ ì „ì²´ ë‹¨ì–´ ê°œìˆ˜ë„ ì´í›„ ëª¨ë¸ì—ì„œ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— ì €ì¥í•´ ë‘”ë‹¤. 123word_vocab = tokenizer.word_indexprint(word_vocab)print(\"ì „ì²´ ë‹¨ì–´ ê°œìˆ˜:\", len(word_vocab)) 12345data_configs = &#123;&#125;data_configs['vocab'] = word_vocabdata_configs['vocab_size'] = len(word_vocab) + 1 í˜„ì¬ ê° ë°ì´í„°ëŠ” ì„œë¡œ ê¸¸ì´ê°€ ë‹¤ë¥¸ë° ì´ ê¸¸ì´ë¥¼ í•˜ë‚˜ë¡œ í†µì¼í•´ì•¼ ì´í›„ ëª¨ë¸ì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— íŠ¹ì • ê¸¸ì´ë¥¼ ìµœëŒ€ ê¸¸ì´ë¡œ ì •í•˜ê³  ë” ê¸´ ë°ì´í„°ì˜ ê²½ìš° ë’·ë¶€ë¶„ì„ ìë¥´ê³  ì§§ì€ ë°ì´í„°ì˜ ê²½ìš°ì—ëŠ” 0 ê°’ìœ¼ë¡œ íŒ¨ë”©í•˜ëŠ” ì‘ì—…ì„ ì§„í–‰í•œë‹¤. íŒ¨ë”© ì²˜ë¦¬ì—ëŠ” ì•ì„œ ë¶ˆëŸ¬ì˜¨ pad_sequences í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤. ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ì¸ìë¡œ íŒ¨ë”©ì„ ì ìš©í•  ë°ì´í„°, ìµœëŒ€ ê¸¸ì´ê°’, 0 ê°’ì„ ë°ì´í„° ì•ì— ë„£ì„ì§€ ë’¤ì— ë„£ì„ ì§€ ì—¬ë¶€ë¥¼ ì„¤ì •í•œë‹¤. ë˜í•œ, ì œì¼ ë§ˆì§€ë§‰ ë‹¨ì–´ë¶€í„° ë‹¨ì–´ë¥¼ ì¹´ìš´íŠ¸í•œë‹¤ëŠ” ê²ƒì— ìœ ì˜í•˜ì. ì—¬ê¸°ì„œ ìµœëŒ€ ê¸¸ì´ë¥¼ 174ë¡œ ì„¤ì •í–ˆëŠ”ë°, ì´ëŠ” ì•ì„œ ë°ì´í„° ë¶„ì„ ê³¼ì •ì—ì„œ ë‹¨ì–´ ê°œìˆ˜ì˜ í†µê³„ë¥¼ ê³„ì‚°í–ˆì„ ë•Œ ë‚˜ì™”ë˜ ì¤‘ì•™ê°’(median)ì´ë‹¤. ë³´í†µ í‰ê· ì´ ì•„ë‹Œ ì¤‘ì•™ê°’(median)ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ì€ë°, í‰ê· ì€ ì´ìƒì¹˜ì— ë¯¼ê°í•˜ê¸° ë•Œë¬¸ì´ë‹¤. 1234567# ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´MAX_SEQUENCE_LENGTH = 174# paddingì„ ë’·ë¶€ë¶„ì— í•œë‹¤.train_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')print('Shape of train data: ', train_inputs.shape) ë§ˆì§€ë§‰ìœ¼ë¡œ í•™ìŠµ ì‹œ label ê°’ì„ ë„˜íŒŒì´ ë°°ì—´ë¡œ ì €ì¥í•œë‹¤. ê·¸ ì´ìœ ëŠ” ì´í›„ ì „ì²˜ë¦¬í•œ ë°ì´í„°ë¥¼ ì €ì¥í•  ë•Œ ë„˜íŒŒì´ í˜•íƒœë¡œ ì €ì¥í•˜ê¸° ë•Œë¬¸ì´ë‹¤. 12train_labels = np.array(train_data['sentiment'])print('Shape of label tensor: ', train_labels.shape) ì´ì œ ì „ì²˜ë¦¬í•œ ë°ì´í„°ë¥¼ ì´í›„ ëª¨ë¸ë§ ê³¼ì •ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì €ì¥í•  ê²ƒì´ë‹¤. ì—¬ê¸°ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì´ 4ê°œì˜ ë°ì´í„°ë¥¼ ì €ì¥í•  ê²ƒì´ë‹¤. í…ìŠ¤íŠ¸ ë°ì´í„°ì˜ ê²½ìš° CSV íŒŒì¼ë¡œ ì €ì¥í•˜ê³ , ë²¡í„°í™”í•œ ë°ì´í„°ì™€ ì •ë‹µ ë¼ë²¨ì˜ ê²½ìš° ë„˜íŒŒì´ íŒŒì¼ë¡œ ì €ì¥í•œë‹¤. ë§ˆì§€ë§‰ ë°ì´í„° ì •ë³´ì˜ ê²½ìš° dictionary í˜•íƒœì´ê¸° ë•Œë¬¸ì— Json íŒŒì¼ë¡œ ì €ì¥í•œë‹¤. ì •ì œëœ í…ìŠ¤íŠ¸ ë°ì´í„° ë²¡í„°í™”í•œ ë°ì´í„° ì •ë‹µ ë¼ë²¨ ë°ì´í„° ì •ë³´ ìš°ì„  ê²½ë¡œì™€ íŒŒì¼ëª…ì„ ì„¤ì •í•˜ê³  os ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í´ë”ê°€ ì—†ëŠ” ê²½ìš° í´ë”ë¥¼ ìƒì„±í•œë‹¤. 1234567891011121314151617181920DATA_OUT_PATH = '/content/'TRAIN_INPUT_DATA = 'train_input.npy'TRAIN_LABEL_DATA = 'train_label.npy'TRAIN_CLEAN_DATA = 'train_clean.csv'DATA_CONFIGS = 'data_configs.json'import os# ì €ì¥í•˜ëŠ” ë””ë ‰í„°ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_IN_PATH)# ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ numpy í˜•íƒœë¡œ ì €ì¥np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)# ì •ì œëœ í…ìŠ¤íŠ¸ë¥¼ CSV í˜•íƒœë¡œ ì €ì¥train_data.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index=False)# ë°ì´í„° ì‚¬ì „ì„ JSON í˜•íƒœë¡œ ì €ì¥json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'), ensure_ascii=False) ì§€ê¸ˆê¹Œì§€ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ì „ì²˜ë¦¬ë¥¼ í–ˆìœ¼ë¯€ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ìœ„ì™€ ë™ì¼í•œ ê³¼ì •ì„ ì§„í–‰í•˜ë©´ ëœë‹¤. ë‹¤ë¥¸ ì ì€ í‰ê°€ ë°ì´í„°ì˜ ê²½ìš° ë¼ë²¨ ê°’ì´ ì—†ê¸° ë•Œë¬¸ì— ë¼ë²¨ì€ ë”°ë¡œ ì €ì¥í•˜ì§€ ì•Šì•„ë„ ë˜ë©°, ë°ì´í„° ì €ì˜µì¸ ë‹¨ì–´ ì‚¬ì „ê³¼ ë‹¨ì–´ ê°œìˆ˜ì— ëŒ€í•œ ì •ë³´ë„ í•™ìŠµ ë°ì´í„°ì˜ ê²ƒì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì €ì¥í•˜ì§€ ì•Šì•„ë„ ëœë‹¤. ì¶”ê°€ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•´ ì €ì¥í•´ì•¼ í•˜ëŠ” ê°’ì´ ìˆëŠ”ë° ê° review ë°ì´í„°ì— ëŒ€í•´ reviewì— ëŒ€í•œ &#39;id&#39;ê°’ì„ ì €ì¥í•´ì•¼ í•œë‹¤. í‰ê°€ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬ í•  ë•Œ í•œ ê°€ì§€ ì¤‘ìš”í•œ ì ì€ Tokenizerë¥¼ í†µí•´ ì¸ë±ìŠ¤ ë²¡í„°ë¡œ ë§Œë“¤ ë•Œ Tokenizing ê°ì²´ë¡œ ìƒˆë¡­ê²Œ ë§Œë“œëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê¸°ì¡´ì— í•™ìŠµ ë°ì´í„°ì— ì ìš©í•œ Tokenizer ê°ì²´ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ë§Œì•½ ìƒˆë¡­ê²Œ ë§Œë“¤ ê²½ìš° í•™ìŠµ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ì— ëŒ€í•œ ê° ë‹¨ì–´ì˜ë“¤ì˜ ì¸ë±ìŠ¤ê°€ ë‹¬ë¼ì ¸ì„œ ëª¨ë¸ì— ì •ìƒì ìœ¼ë¡œ ì ìš©í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. fit_on_texts, fit_on_sequencesëŠ” ì‚¬ì „ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” í–‰ìœ„ì¸ë° ì•„ë˜ì˜ ì½”ë“œì—ì„œ fit_on_textsë¥¼ ì‹¤í–‰í•˜ì§€ ì•ŠëŠ” ì´ìœ ëŠ” Tokenizer ê°ì²´ë¥¼ ìƒˆë¡œ ìƒì„±í•˜ì§€ ì•Šì•˜ê¸°ì— Train dataì˜ ì‚¬ì „ì„ ê°–ê³  ë§Œì•½ì— Train dataì— í¬í•¨ë˜ì–´ ìˆì§€ ì•Šì€ ë‹¨ì–´ê°€ Test dataì— ì¡´ì¬í•œë‹¤ë©´ í™•ë¥ ì„ 0ìœ¼ë¡œ ì£¼ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.1234567test_data = pd.read_csv(DATA_IN_PATH + 'testData.tsv', header=0, delimiter='\\t', quoting=3)test_data['review'] = test_data['review'].apply(lambda x: preprocessing(x, True))test_id = np.array(test_data['id'])text_sequences = tokenizer.texts_to_sequences(test_data['review'])test_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') 1234567TEST_INPUT_DATA = 'test_input.npy'TEST_CLEAN_DATA = 'test_clean.csv'TEST_ID_DATA = 'test_id.npy'np.save(open(DATA_IN_PATH + TEST_INPUT_DATA, 'wb'), test_inputs)np.save(open(DATA_IN_PATH + TEST_ID_DATA, 'wb'), test_id)test_data.to_csv(DATA_IN_PATH + TEST_CLEAN_DATA, index=False)","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"Scrapy ì›¹ í¬ë¡¤ë§ 04 - ì‹¤ìŠµ","slug":"Crawling_03","date":"2020-01-28T05:55:17.000Z","updated":"2020-01-28T15:50:27.205Z","comments":true,"path":"2020/01/28/Crawling_03/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/28/Crawling_03/","excerpt":"","text":"Scrapy PracticeDaum í¬ë¡¤ë§í•˜ê¸° ë‹¤ìŒ ë””ì§€í„¸ ë‰´ìŠ¤ í˜ì´ì§€ì—ì„œ í˜„ì¬ URL, ê¸°ì‚¬ íƒ€ì´í‹€ì— ê±¸ë ¤ìˆëŠ” href URL, ê¸°ì‚¬ í˜ì´ì§€ë¡œ ì´ë™í•œ í›„ ê¸°ì‚¬ ì œëª©, ê¸°ì‚¬ ë‚´ìš©ì„ í¬ë¡¤ë§í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í¬ë¡¤ëŸ¬ë¥¼ ë§Œë“¤ê²ƒ items.py ë¨¼ì €, í¬ë¡¤ë§ ëŒ€ìƒì„ itemsë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ items.pyì— Fieldë¥¼ ìƒì„±í•œë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ ì‚¬í•­ë¿ë§Œ ì•„ë‹ˆë¼ SQLiteì— ì €ì¥í• ë•Œ, ìˆ˜ì§‘ëœ ì‹œê°„ì„ ë¡œê·¸ë¡œ ë‚¨ê²¨ ë†“ê¸° ìœ„í•œ Fieldë„ ìƒì„±ì‹œì¼œ ì¤€ë‹¤. 12345678910111213141516171819import scrapyclass PracticeItem(scrapy.Item): # name = scrapy.Field() # ê¸°ì‚¬ ì œëª© headline = scrapy.Field() # ê¸°ì‚¬ ë³¸ë¬¸ contents = scrapy.Field() # ìš”ì²­ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ parent_link = scrapy.Field() # ê¸°ì‚¬ í˜ì´ì§€ article_link = scrapy.Field() # ìˆ˜ì§‘ëœ ì‹œê°„ crawled_time = scrapy.Field() setting.pyMiddlware ê°œì¸ì´ ë§Œë“  ê¸°ëŠ¥ì„ ì¶”ê°€í•´ì„œ ì‚¬ìš©ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ”ê²ƒ, ì¦‰, Pipelineì€ Itemì´ Exportë˜ì–´ íŒŒì¼ì— ì €ì¥ë˜ê¸° ì§ì „ì— ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤ë©´, MiddlewareëŠ” ìš”ì²­í•˜ê¸° ì§ì „, ì‘ë‹µ í›„ì—, ì–´ë–¤í•¨ìˆ˜ê°€ ì‹¤í–‰ì „ì—, ì´ëŸ° ì¤‘ê°„ì—ì„œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ë¼ê³  ë¹„êµí•´ë³¼ ìˆ˜ ìˆë‹¤. ë‹¤ë¥¸ í¬ë¡¤ë§í• ë•Œì™€ ë§ˆì°¬ê°€ì§€ë¡œ ë™ì¼í•œ User-agentë¥¼ ê°€ì§€ê³  í•œë‹¤ë©´, ì„œë²„ì— ì§€ì†ì ì¸ ë¶€í•˜ë¥¼ ì£¼ê²Œë˜ì–´ì„œ ë²¤ì„ ë‹¹í•œë‹¤ê±°ë‚˜ í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ Fake User-agentë¥¼ í™œìš©í•˜ì—¬ í¬ë¡¤ë§ì„ í•  ê²ƒì´ë‹¤. Middlwareì˜ ì¥ì ì€ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ ì´ë¯¸ ê°œë°œí•´ ë†“ì€ ê²ƒë“¤ì´ ë§ë‹¤. íŠ¹íˆ Githubì—ì„œ ê²€ìƒ‰í•œë‹¤ë©´ starê°€ ë§ì€ ê²ƒì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ëŠ” ê²ƒì€ ëˆ„êµ¬ë‚˜ ì•Œê³  ìˆëŠ” ì‚¬ì‹¤!! í•„ìê°€ ì‚¬ìš©í•œ Middlewareì˜ ì‚¬ì´íŠ¸ë¥¼ ë³´ë©´ scrapy 1.0ì´ìƒê³¼ 1.0ë¯¸ë§Œ ë²„ì „ì— ë”°ë¼ ì‚¬ìš©ë°©ë²•ì´ ë‹¤ë¥¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. í•„ìì˜ ê²½ìš° 1.8ì´ë¯€ë¡œ 1.0ì´ìƒì˜ ë°©ë²•ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤. 1pip install scrapy-fake-useragent ì•„ë˜ ì£¼ì„ ì²˜ë¦¬ëœ USER_AGENT ë³€ìˆ˜ëŠ” ì‹¤ì œ ìì‹ ì˜ user-agentë¥¼ ì‚¬ìš©í•´ì•¼í•˜ë©°, ì£¼ì„ì²˜ë¦¬í•œ ì´ìœ ëŠ” ì¶”í›„ì—ëŠ” ì„œë²„ì— ê³¼ë¶€í•˜ì£¼ì–´ ë²¤ë‹¹í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ fake-agentë¥¼ ì‚¬ìš©í•  ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051BOT_NAME = 'practice'SPIDER_MODULES = ['practice.spiders']NEWSPIDER_MODULE = 'practice.spiders'# User-agent ì„¤ì •(ê°œë°œìë„êµ¬ì—ì„œ Networkì°½ì—ì„œ ì°¾ì•„ì„œ ìì‹ ì˜ ì •ë³´ë¥¼ ë³µì‚¬#USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'# Obey robots.txt rulesROBOTSTXT_OBEY = False# ë‹¤ìš´ë¡œë“œ ê°„ê²©DOWNLOAD_DELAY = 2# ì¿ í‚¤ì‚¬ìš©COOKIES_ENABLED = True# Referer ì‚½ì…# daumì€ ë³´ì•ˆì´ ì—„ê²©í•˜ê¸°ì— refererì†ì„±ì„ ì£¼ì–´ì•¼ í•œë‹¤.DEFAULT_REQUEST_HEADERS = &#123;'Referer' : 'https://news.daum.net/breakingnews/digital?page=2'&#125;# ì¬ì‹œë„ íšŸìˆ˜RETRY_ENABLED = TrueRETRY_TIME = 2# í•œê¸€ ì“°ê¸°(ì¶œë ¥ ì¸ì½”ë”©)FEED_EXPORT_ENCODING = 'utf-8'# User-agent ë¯¸ë“¤ì›¨ì–´ ì‚¬ìš©DOWNLOADER_MIDDLEWARES = &#123; 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'scrapy.downloadermiddlewares.retry.RetryMiddleware': None, 'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400, 'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,&#125;# íŒŒì´í”„ ë¼ì¸ í™œì„±í™”# ìˆ«ìê°€ ì‘ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ìƒìœ„ITEM_PIPELINES = &#123; 'practice.pipelines.PracticePipeline': 300,&#125;# ìºì‹œ ì‚¬ìš©#HTTPCACHE_ENABLED = True#HTTPCACHE_EXPIRATION_SECS = 0#HTTPCACHE_DIR = 'httpcache'#HTTPCACHE_IGNORE_HTTP_CODES = []#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage' pipelines.py í¬ë¡¤ë§ìœ¼ë¡œ ìˆ˜ì§‘ëœ ì‹œê°„ì˜ ë¡œê·¸ë¥¼ ë‚¨ê¸°ê¸° ìœ„í•´ datetime ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼, DBì— ì €ì¥í•˜ê¸° ìœ„í•´ sqlite3, ë§ˆì§€ë§‰ìœ¼ë¡œ ì˜ˆì™¸ì ì¸ ì²˜ë¦¬ë¥¼ ìœ„í•´ DropItem ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import datetimeimport sqlite3from scrapy.exceptions import DropItemclass PracticePipeline(object): # ì´ˆê¸°í™” ë©”ì†Œë“œ def __init__(self): # DB ì„¤ì •(ìë™ ì»¤ë°‹) # isolation_level=None =&gt; Auto Commit self.conn = sqlite3.connect('ì €ì¥í•  ìœ„ì¹˜ì— ëŒ€í•œ path/ì €ì¥í•  íŒŒì¼ëª….db', isolation_level=None) # DB ì—°ê²° self.c = self.conn.cursor() # ìµœì´ˆ 1íšŒ ì‹¤í–‰ def open_spider(self, spider): spider.logger.info('NewsSpider Pipeline Started.') self.c.execute(\"CREATE TABLE IF NOT EXISTS NEWS_DATA(id INTEGER PRIMARY KEY AUTOINCREMENT, headline text, contents text, parent_link text, article_link text, crawled_time text)\" # Item ê±´ìˆ˜ ë³„ ì‹¤í–‰ def process_item(self, item, spider): if not item.get('contents') is None: # ì‚½ì… ì‹œê°„ crawled_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') # í¬ë¡¤ë§ ì‹œê°„ í•„ë“œ ì¶”ê°€ item['crawled_time'] = crawled_time # ë°ì´í„° -&gt; DB ì‚½ì… # tuple(item[k] for k in item.keys()) ë¡œ ëŒ€ì‹ í•´ë„ ëœë‹¤. self.c.execute('INSERT INTO NEWS_DATA(headline, contents, parent_link, article_link, crawled_time) VALUES(?, ?, ?, ?, ?);', (item.get('headline'), item.get('contents'), item.get('parent_link'), item.get('article_link'), item.get('crawled_time'))) # tuple(item[k] for k in item.keys()) # ë¡œê·¸ spider.logger.info('Item to DB inserted.') # ê²°ê³¼ ë¦¬í„´ return item else: raise DropItem('Dropped Item. Because This Contents is Empty.') # ë§ˆì§€ë§‰ 1íšŒ ì‹¤í–‰ def close_spider(self, spider): spider.logger.info('NewsSpider Pipeline Stopped.') # commit(auto commitìœ¼ë¡œ ì„¤ì •í–ˆì§€ë§Œ í˜¹ì‹œ ëª¨ë¥´ë‹ˆ) self.conn.commit() # ì—°ê²° í•´ì œ self.conn.close() Spider.py ë¨¼ì € ë„ë©”ì¸ê³¼ ì‹œì‘í•˜ëŠ” URLì„ ì •í•˜ê³ ë‚˜ì„œ í˜ì´ì§€ì˜ ê·œì¹™ì„ ì°¾ì•„ë³¸ë‹¤. ê·œì¹™ì„ ì •í•´ì£¼ë©´ LinkExtractorë¡œ ë°˜ë³µë˜ëŠ” URLì„ ë³´ë‚´ì¤„ ìˆ˜ ìˆë‹¤. ë‹¨, 1ìë¦¬ìˆ˜ ì´ì™¸ì˜ 2ìë¦¬ìˆ˜ë¶€í„°ì˜ ë°˜ë³µì€ Ruleí•¨ìˆ˜ì— follow=Trueë¡œ ì£¼ì–´ì•¼í•œë‹¤. ë˜í•œ ë³€ìˆ˜ëª… rulesë¡œ Ruleê°ì²´ë¥¼ ë°›ì•„ì•¼ ì‚¬ìš©ê°€ëŠ¥í•¨ì„ ê¸°ì–µí•˜ì. parent pageì—ì„œ í•´ë‹¹ ê¸°ì‚¬ë“¤ì— ëŒ€í•œ urlì„ parse_child í•¨ìˆ˜ë¡œ ë„˜ê²¨ì£¼ëŠ”ë°, ì´ë•Œ ê·¸ëƒ¥ ë„˜ê²¨ì£¼ì§€ ì•Šê³  parent pageì—ì„œ ì–»ì€ ì •ë³´ ë˜í•œ meta parameterë¥¼ í†µí•´ ê°™ì´ ë„˜ê²¨ ì¤„ìˆ˜ ìˆë‹¤. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Ruleimport syssys.path.insert(0, 'items.pyê°€ ìˆëŠ” ì ˆëŒ€ path')from items import PracticeItemclass NewcralSpider(CrawlSpider): name = 'newcral' allowed_domains = ['news.daum.net'] start_urls = ['https://news.daum.net/breakingnews/digital'] # ë§í¬ í¬ë¡¤ë§ ê·œì¹™(ì •ê·œí‘œí˜„ì‹ ì‚¬ìš© ì¶”ì²œ) # page=\\d$ : 1ìë¦¬ ìˆ˜ # page=\\d+ : ì—°ì†, follow=True rules = [ Rule(LinkExtractor(allow=r'/breakingnews/digital\\?page=\\d+'), callback='parse_parent', follow=True), ] def parse_parent(self, response): # ë¶€ëª¨ URL ë¡œê¹… self.logger.info('Parent Response URL : %s' % response.url) for url in response.css('ul.list_news2.list_allnews &gt; li &gt; div'): # URL ì‹ ë¬¸ ê¸°ì‚¬ URL article_link = url.css('strong &gt; a::attr(href)').extract_first().strip() yield scrapy.Request(article_link, self.parse_child, meta=&#123;'parent_url': response.url&#125;) def parse_child(self, response): # ë¶€ëª¨, ìì‹ ìˆ˜ì‹  ì •ë³´ ë¡œê¹… self.logger.info('----------------------------------------') self.logger.info('Response From Parent URL : %s' % response.meta['parent_url']) self.logger.info('Child Response URL : %s' % response.url) self.logger.info('Child Response Status ; %s' % reponse.status) self.logger.info('----------------------------------------') # ìš”ì²­ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ parent_link = response.meta['parent_url'] # ê¸°ì‚¬ í˜ì´ì§€ article_link = response.url # í—¤ë“œë¼ì¸ headline = response.css('h3.tit_view::text').extract_first().strip() # ë³¸ë¬¸ c_list = response.css('div.article_view &gt; section &gt; p::text').extract() contents = ''.join(c_list).strip() yield PracticeItem(headline=headline, contents=contents, article_link=article_link, parent_link=parent_link) ë„ì›€ì´ ë˜ëŠ” í•™ìŠµ ë¹„ë™ê¸°(asyncio), ë³‘ë ¬í”„ë¡œê·¸ë˜ë°, ìŠ¤ë ˆë“œ, ë©€í‹° í”„ë¡œì„¸ì‹±ë“± routine ê°œë…ì„ í•™ìŠµí•´ì•¼ ë„¤íŠ¸ì›Œí¬ìƒì˜ ë¸”ë¡ ë˜ëŠ” ë…¼ë¸”ëŸ­ ioë¡œ ì¸í•´ ì§€ì—°ì‹œê°„ì´ ë°œìƒë˜ëŠ”ë°, ì œì–´ê¶Œì„ ë„˜ê²¨ì£¼ë©´ì„œ ì¢€ ë” ì„±ëŠ¥ì„ ì˜¬ë¦´ ìˆ˜ ìˆë‹¤. scrapy twistedëŠ” ì˜ˆë¥¼ ë“¤ì–´, ìœ„ì˜ ì‹¤ìŠµì—ì„œì™€ ê°™ì´ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•œ í›„ pipelineì„ í†µí•´ DBì— insert í•  ë•Œ ì´ ì‘ì—…ì´ ë‹¤ ëë‚˜ì§€ ì•Šìœ¼ë©´, ë‹¤ìŒ ë°ì´í„°ì— ëŒ€í•œ ì‘ì—…ìœ¼ë¡œ ë„˜ì–´ ê°ˆ ìˆ˜ ì—†ì–´ ì‹œê°„ì ìœ¼ë¡œ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆìˆ˜ ìˆëŠ”ë°, ì´ëŸ° ê²½ìš° ë¹„ë™ê¸°ì‹ìœ¼ë¡œ ì²˜ë¦¬ë¥¼ í•´ì¤Œìœ¼ë¡œì¨ ì„±ëŠ¥ì„ ì˜¬ë¦´ìˆ˜ ìˆê²Œë” í•˜ëŠ” frameworkì´ë‹¤.","categories":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/categories/crawling/"}],"tags":[]},{"title":"Scrapy ì›¹ í¬ë¡¤ë§ 03 - Exports, Settings, pipeline","slug":"Crawling_02","date":"2020-01-23T16:56:11.000Z","updated":"2020-01-27T16:05:56.552Z","comments":true,"path":"2020/01/24/Crawling_02/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/24/Crawling_02/","excerpt":"","text":"Exports ìš°ë¦¬ê°€ ì‹¤í–‰í›„ í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” pathë¥¼ ì‹¤í–‰í• ë•Œë§ˆë‹¤ ì§€ì •í•˜ê±°ë‚˜ ì‹¤í–‰í–ˆëŠ”ë°, ì¼ì¢…ì˜ templateê°™ì´ ë¯¸ë¦¬ ë§Œë“¤ì–´ ë†“ì„ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì´ Exportsì´ë‹¤. Exports ì°¸ì¡° ì‚¬ì´íŠ¸ : https://docs.scrapy.org/en/latest/topics/feed-exports.html 1234# ì•„ë˜ 2ê°€ì§€ ë°©ë²•ì€ ë™ì¼í•œ ë°©ë²•ì´ë‹¤.scrapy runspider using_items.py -o test.json -t jsonscrapy runspider using_items.py --output test.json --output-format json ìœ„ì—ì„œì™€ ê°™ì´ í¬ë¡¤ë§ì„ í•  ê²½ìš°ì— ëª…ë ¹ì–´ë¥¼ í†µí•´ ê²°ê³¼ì˜ í˜•ì‹ê³¼ íŒŒì¼ ì´ë¦„ì„ ì§€ì •í•´ì£¼ëŠ” ê²ƒê³¼ ë‹¤ë¥´ê²Œ Settings.pyì—ì„œ ë¯¸ë¦¬ ì§€ì •í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì»¤ë§¨ë“œë¼ì¸ì—ì„œë„ ê°€ëŠ¥í•˜ì§€ë§Œ, ëª¨ë“  í…ŒìŠ¤íŠ¸ë¥¼ ë‹¤ ê±°ì¹œ í›„ í™•ì •ì ìœ¼ë¡œ ì‚¬ìš©í•  ê²ƒì´ë¼ë©´, settings.pyì—ì„œ ë³€ìˆ˜ì„¤ì •ì„ í•˜ëŠ” ê²ƒì´ ë” ì¢‹ë‹¤. ìš°ë¦¬ê°€ í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸ https://globalvoices.org/ ì‚¬ì´íŠ¸ì˜ ê¸°ì‚¬ë“¤ì˜ ì œëª©ë§Œì„ í¬ë¡¤ë§ í•  ê²ƒì´ë‹¤. 1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-import scrapy# Scrapy í™˜ê²½ì„¤ì •# ì¤‘ìš”# ì‹¤í–‰ë°©ë²•# 1.ì»¤ë§¨ë“œ ë¼ì¸ ì‹¤í–‰ -&gt; scrapy crawl í¬ë¡¤ëŸ¬ëª… -s(--set) &lt;NAME&gt;=&lt;VALUE&gt;class ScrapyWithSettingsSpider(scrapy.Spider): name = 'scrapy_with_settings' allowed_domains = ['globalvoices.org'] start_urls = ['https://globalvoices.org/'] def parse(self, response): # ì•„ë˜ 3ê°€ì§€ëŠ” ë™ì¼í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ # response.css('#main &gt; div.post-archive-container &gt; div#post-archive div.dategroup div.post-excerpt-container &gt; h3 &gt; a::text').getall() # response.xpath('//*[@id=\"post-archive\"]//div[@class=\"dategroup\"]//div[@class=\"post-summary-content\"]//div[@class=\"post-excerpt-container\"]/h3/a/text()').getall() # xpath + css í˜¼í•© for i, v in enumerate(response.xpath('//div[@class=\"post-summary-content\"]').css('div.post-excerpt-container &gt; h3 &gt; a::text').extract(),1): # ì¸ë±ìŠ¤ ë²ˆí˜¸, í—¤ë“œë¼ì¸ yield dict(num=i, headline=v) settings.pyì—ì„œ exportí•˜ëŠ” ë³€ìˆ˜ ì„¤ì • settings.pyì— ì•„ë˜ì™€ ê°™ì´ í•„ìš”í•œ ë³€ìˆ˜ë¥¼ ì¶”ê°€ë¡œ ì„¤ì •í•˜ë©´ëœë‹¤. ì €ì¥ì†Œ, ì €ì¥ í˜•ì‹ ê´€ë ¨ ë ˆí¼ëŸ°ìŠ¤ 12345678910111213# ì¶œë ¥(Exports)ì„¤ì •# íŒŒì¼ì´ë¦„ ë° ê²½ë¡œ# ë§Œì•½ ë‹¤ë¥¸ íŠ¹ì • ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ê³  ì‹¶ë‹¤ë©´ ê°€ëŠ¥í•˜ë‹¤.FEED_URI = 'result.json'# íŒŒì¼ í˜•ì‹FEED_FORMAT = 'json'# ì¶œë ¥ ì¸ì½”ë”©FEED_EXPORT_ENCODING = 'utf-8'# ê¸°ë³¸ ë“¤ì—¬ì“°ê¸°FEED_EXPORT_INDENT = 2 ë˜í•œ, ë™ì¼í•œ ìì›ì„ ë°˜ë³µí•´ì„œ í¬ë¡¤ë§í•  ê²½ìš° ì„œë²„ì— ê³¼ë¶€í•˜ë¥¼ ì£¼ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´ cacheë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. ì´ ë˜í•œ, setting.pyì—ì„œ ë³€ìˆ˜ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. pipeline ì°¸ê³  pipelineì€ itemë“¤ì´ ìµœì¢…ì ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” íŒŒ\u001cì¼ì„ ë§Œë“¤ê¸° ì „\u001cì— ì•½ê°„ì˜ ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ” ì‘ì—…ì´ë¼ê³  ìƒê°í•˜ë©´ëœë‹¤. ë¬¼ë¡  spiderì—ì„œë„ ê°€ëŠ¥í•˜ì§€ë§Œ ì¥ê¸°ì ìœ¼ë¡œ ì½”ë“œ ê´€ë¦¬ì ì¸ ë©´ì„ ë´¤ì„ ë•Œ ë„ˆë¬´ ì¢‹ì§€ ì•Šì€ ë°©ì‹ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í¬ë¡¤ëŸ¬ë¥¼ ë§Œë“¤ì—ˆë˜ ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ê°€ ë°”ë€Œì—ˆë‹¤ë©´ í•œ python scriptì— ëª¨ë“  ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤ë©´ ë³€ê²½ëœ ì‚¬ì´íŠ¸ì˜ êµ¬ì¡°ì— ë§ì¶° ì½”ë“œë¥¼ ë³€ê²½í•˜ë ¤ë©´ ì½”ë“œë¥¼ í•´ì„í•˜ëŠ”ë° ì˜¤ëœì‹œê°„ì„ íˆ¬ìí•´ì•¼ í•  ê²ƒì´ë‹¤. Item pipelineì˜ ì „í˜•ì ì¸ ì˜ˆì‹œ HTML data ì œê±° ì •í™•í•˜ì§€ ì•Šì€ ë°ì´í„°(ë˜ëŠ” ë™ì¼ ë°ì´í„°)ê°€ ìˆ˜ì§‘ë˜ì—ˆë‹¤ë©´ ì¶œë ¥ ì „ pipelineë‹¨ê³„ì—ì„œ validationì„ í•  ìˆ˜ ìˆë‹¤. ì¤‘ë³µ ì²´í¬ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ pipeline ì‚¬ìš©ì„ ìœ„í•œ ìƒˆë¡œìš´ í¬ë¡¤ë§ ì‚¬ì´íŠ¸ : https://www.alexa.com/topsites ìì‹ ì˜ ì‚¬ì´íŠ¸ ë°©ë¬¸ 50ìœ„ ì‚¬ì´íŠ¸ë¥¼ ì•Œ ìˆ˜ ìˆëŠ” ì›¹ì‚¬ì´íŠ¸ì´ë‹¤. ì‚¬ì´íŠ¸ ìˆœìœ„, ì‚¬ì´íŠ¸ ëª…, í•˜ë£¨ì— ë°©ë¬¸í•˜ëŠ” í‰ê·  ì‹œê°„, í•˜ë£¨ì— ë°©ë¬¸í•˜ëŠ” í‰ê·  í˜ì´ì§€ë·°ìˆ˜ë¥¼ í¬ë¡¤ë§í•  ê²ƒì´ë‹¤. itemsë¥¼ ì‚¬ìš©í•  ê²ƒì´ë©°, ëª¨ë“  ì •ë³´ë¥¼ í¬ë¡¤ë§í•œí›„ pipelineì„ í†µí•´ 40ìœ„ê¶Œì•ˆì˜ ìˆœìœ„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ ì €ì¥í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•  ê²ƒì´ë‹¤. setting.pyì—ì„œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ pipelineì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì£¼ì„ì„ í’€ì–´ì£¼ì–´ì„œ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ë§Œì•½ ì•„ë˜ì—ì„œì™€ ë‹¤ë¥´ê²Œ ì—¬ëŸ¬ê°œì˜ pipelineì„ ì‚¬ìš©í•œë‹¤ë©´ ìˆ«ìê°€ ë‚®ì„ ìˆ˜ë¡ ìš°ì„  ìˆœìœ„ë¥¼ ê°–ëŠ”ë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì. spiderë¥¼ ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±í•˜ì˜€ë‹¤. 12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-import scrapyimport sys# items.pyì— ëŒ€í•œ path ì¶”ê°€sys.path.insert(0, '../project/chat_bot_project/section01_2/section01_2')from items import SiteRankItemsclass Pipeline01Spider(scrapy.Spider): name = 'pipeline_01' allowed_domains = ['alexa.com/topsites'] start_urls = ['https://www.alexa.com/topsites'] def parse(self, response): \"\"\" :param :response : return : SiteRankItems \"\"\" for p in response.css('div.listings.table &gt; div.tr.site-listing'): # ì•„ì´í…œ ê°ì²´ ìƒì„± item = SiteRankItems() # ìˆœìœ„ item['rank_num'] = p.xpath('./div[1]/text()').get() # ì‚¬ì´íŠ¸ëª… item['site_name'] = p.xpath('./div[2]/p/a/text()').get() # í‰ê·  ì ‘ì† ì‹œê°„ item['daily_time_site'] = p.xpath('./div[3]/p/text()').get() # í‰ê·  ë³¸ íšŸìˆ˜ item['daily_page_view'] = p.xpath('./div[4]/p/text()').get() yield item~ ìœ„ì˜ ì½”ë“œë¥¼ í†µí•´ í¬ë¡¤ë§í•œ ë°ì´í„°ë¥¼ ì´ì œ pipelineì„ í†µí•´ ì²˜ë¦¬í•´ë³´ì. ê°„ë‹¨íˆ csvíŒŒì¼ê³¼ ì—‘ì…€íŒŒì¼ì„ ë§Œë“¤ì–´ ë³¼ ê²ƒì´ë‹¤. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Section012Pipeline(object): # ì´ˆê¸°í™” method # init methodë„ classê°€ ì´ˆê¸°í™”ë  ë•Œ ìµœì´ˆë¡œ ì‹¤í–‰ë˜ë¯€ë¡œ open_spiderì™€ ë™ì¼í•˜ê²Œ ì‚¬ìš©ê°€ëŠ¥ def __init__(self): # ì—‘ì…€ ì²˜ë¦¬ ì„ ì–¸ self.workbook = xlsxwriter.Workbook(\"../chat_bot_project/section01_2/section01_2/spiders/result_excel.xlsx\") # CSV ì²˜ë¦¬ ì„ ì–¸ (a, w ì˜µì…˜ ë³€ê²½) self.file_opener = open(\"../chat_bot_project/section01_2/section01_2/spiders/result_csv.csv\", \"w\") self.csv_writer = csv.DictWriter(self.file_opener, fieldnames=['rank_num','site_name','daily_time_site','daily_page_view','is_pass']) # ì›Œí¬ì‹œíŠ¸ self.worksheet = self.workbook.add_worksheet() # ì‚½ì… ìˆ˜ self.rowcount = 1 # ìµœì´ˆ 1íšŒ ì‹¤í–‰ def open_spider(self, spider): spider.logger.info(\"TestSpider Pipelines Started.\") # ë°ì´í„°ë¥¼ í¬ë¡¤ë§í• ë•Œ ë§¤ë²ˆì‹¤í–‰ def process_item(self, item, spider): # í˜„ì¬ itemì€ spiderì—ì„œ itemì„ í™œìš©í•´ì„œ ì‘ì„±í–ˆìœ¼ë¯€ë¡œ dictionaryë¡œ ë˜ì–´ìˆë‹¤. # rank_numì´ 40ìœ„ ì•ˆì— ìˆëŠ” ì‚¬ì´íŠ¸ë“¤ë§Œ ì €ì¥í•˜ê¸° ìœ„í•œ ì½”ë“œ if int(item.get('rank_num')) &lt; 41 : item['is_pass'] = True # ì—‘ì…€ ì €ì¥ # item['rank_num']ì²˜ëŸ¼ ì ‘ê·¼ê°€ëŠ¥í•˜ì§€ë§Œ ë°ì´í„°ê°€ ì—†ë‹¤ë©´ ì—ëŸ¬ê°€ ë°œìƒí•˜ë¯€ë¡œ ì•„ë˜ì—ì„œ ì²˜ëŸ¼ get methodë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. self.worksheet.write('A%s' % self.rowcount, item.get('rank_num' )) self.worksheet.write('B%s' % self.rowcount, item.get('site_name' )) self.worksheet.write('C%s' % self.rowcount, item.get('daily_tiem_site' )) self.worksheet.write('D%s' % self.rowcount, item.get('daily_page_view' )) self.worksheet.write('E%s' % self.rowcount, item.get('is_pass' )) self.rowcount += 1 # csv ì €ì¥ self.csv_writer.writerow(item) return item else: # raise DropItem(f'Dropped Item. Because This Site Rank is &#123;item.get(\"rank_num\")&#125;') # ë§ˆì§€ë§‰ 1íšŒ ì‹¤í–‰ def close_spider(self, spider): # ì—‘ì…€ íŒŒì¼ ë‹«ê¸° self.workbook.close() # CSV íŒŒì¼ ë‹«ê¸° self.file_opener.close() spider.logger.info(\"TestSpider Pipelines Finished\")","categories":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/categories/crawling/"}],"tags":[]},{"title":"Attention mechanismì„ ì‚¬ìš©í•œ Seq2seq êµ¬í˜„","slug":"deep_learning_11","date":"2020-01-21T20:15:09.000Z","updated":"2020-01-23T17:01:03.099Z","comments":true,"path":"2020/01/22/deep_learning_11/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/22/deep_learning_11/","excerpt":"","text":"Vallina Seq2seq tf.functionì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ tensorflow 2.0.0-beta1ë²„ì „ì„ ì„¤ì¹˜í•œë‹¤. í•œê¸€ í…ìŠ¤íŠ¸ì˜ í˜•íƒœì†Œë¶„ì„ì„ ìœ„í•´ konlpyì—ì„œ Okt(Original Korean tag, Twitterì—ì„œ ê³µê°œí•œ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬)ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì„¤ì¹˜í•´ì¤€ë‹¤. 12!pip install tensorflow==2.0.0-beta1!pip install konlpy í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import 123import randomimport tensorflow as tffrom konlpy.tag import Okt tensorflow ë²„ì „ì´ ë§ëŠ”ì§€ í™•ì¸ 1print(tf.__version__) í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •123EPOCHS = 200# ê°€ì¥ ë§ì´ ì‚¬ìš©ëœ 2000ê°œë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´NUM_WORDS = 2000 Encoder1234567891011121314class Encoder(tf.keras.Model): def __init__(self): super(Encoder, self).__init__() # 2000ê°œì˜ ë‹¨ì–´ë“¤ì„ 64í¬ê¸°ì˜ vectorë¡œ Embeddingí•´ì¤Œ. self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64) # return_stateëŠ” returní•˜ëŠ” Outputì— ìµœê·¼ì˜ stateë¥¼ ë”í•´ì£¼ëŠëƒì— ëŒ€í•œ ì˜µì…˜ # ì¦‰, Hidden stateì™€ Cell stateë¥¼ ì¶œë ¥í•´ì£¼ê¸° ìœ„í•œ ì˜µì…˜ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. # defaultëŠ” Falseì´ë¯€ë¡œ ì£¼ì˜í•˜ì! self.lstm = tf.keras.layers.LSTM(512, return_state=True) def call(self, x, training=False, mask=None): x = self.emb(x) _, h, c = self.lstm(x) return h, c Decoder1234567891011121314151617class Decoder(tf.keras.Model): def __init__(self): super(Decoder, self).__init__() self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64) # return_sequenceëŠ” return í•  Outputì„ full sequence ë˜ëŠ” Sequenceì˜ ë§ˆì§€ë§‰ì—ì„œ ì¶œë ¥í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” ì˜µì…˜ # FalseëŠ” ë§ˆì§€ë§‰ì—ë§Œ ì¶œë ¥, TrueëŠ” ëª¨ë“  ê³³ì—ì„œì˜ ì¶œë ¥ self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True) self.dense = tf.keras.layers.Dense(NUM_WORDS, activation='softmax') def call(self, inputs, training=False, mask=None): x, h, c = inputs x = self.emb(x) # initial_stateëŠ” ì…€ì˜ ì²« ë²ˆì§¸ í˜¸ì¶œë¡œ ì „ë‹¬ ë  ì´ˆê¸° ìƒíƒœ í…ì„œ ëª©ë¡ì„ ì˜ë¯¸ # ì´ì „ì˜ Encoderì—ì„œ ë§Œë“¤ì–´ì§„ Hidden stateì™€ Cell stateë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì•¼ í•˜ë¯€ë¡œ x, h, c = self.lstm(x, initial_state=[h, c]) return self.dense(x), h, c Seq2seq1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Seq2seq(tf.keras.Model): def __init__(self, sos, eos): super(Seq2seq, self).__init__() self.enc = Encoder() self.dec = Decoder() self.sos = sos self.eos = eos def call(self, inputs, training=False, mask=None): if training is True: # í•™ìŠµì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ìš°ë¦¬ê°€ ì…ë ¥ê³¼ ì¶œë ¥ ë‘ê°€ì§€ë¥¼ ë‹¤ ì•Œê³  ìˆì–´ì•¼ í•œë‹¤. # ì¶œë ¥ì´ í•„ìš”í•œ ì´ìœ ëŠ” Decoderë‹¨ì˜ ì…ë ¥ìœ¼ë¡œ shited_ouputì„ ë„£ì–´ì£¼ê²Œ ë˜ì–´ìˆê¸° ë•Œë¬¸ì´ë‹¤. x, y = inputs # LSTMìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆê¸° ë•Œë¬¸ì— Hidden Stateì™€ Cell Stateë¥¼ ì¶œë ¥ìœ¼ë¡œ ë‚´ì¤€ë‹¤. h, c = self.enc(x) # Hidden stateì™€ cell state, shifted outputì„ ì´ˆê¸°ê°’ìœ¼ë¡œ ì…ë ¥ ë°›ê³  # ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” yëŠ” Decoderì˜ ê²°ê³¼ì´ê¸° ë•Œë¬¸ì— ì „ì²´ ë¬¸ì¥ì´ ë  ê²ƒì´ë‹¤. y, _, _ = self.dec((y, h, c)) return y else: x = inputs h, c = self.enc(x) # Decoder ë‹¨ì— ì œì¼ ë¨¼ì € sosë¥¼ ë„£ì–´ì£¼ê²Œë” tensorí™”ì‹œí‚¤ê³  y = tf.convert_to_tensor(self.sos) # shapeì„ ë§ì¶°ì£¼ê¸° ìœ„í•œ ì‘ì—…ì´ë‹¤. y = tf.reshape(y, (1, 1)) # ìµœëŒ€ 64ê¸¸ì´ ê¹Œì§€ ì¶œë ¥ìœ¼ë¡œ ë°›ì„ ê²ƒì´ë‹¤. seq = tf.TensorArray(tf.int32, 64) # tf.keras.Modelì— ì˜í•´ì„œ call í•¨ìˆ˜ëŠ” auto graphëª¨ë¸ë¡œ ë³€í™˜ì´ ë˜ê²Œ ë˜ëŠ”ë°, # ì´ë•Œ, tf.rangeë¥¼ ì‚¬ìš©í•´ forë¬¸ì´ë‚˜ whileë¬¸ì„ ì‘ì„±ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ tf í•¨ìˆ˜ë¡œ ë˜ì–´ìˆë‹¤ë©´ # ê·¸ forë¬¸ê³¼ whileë¬¸ì´ êµ‰ì¥íˆ íš¨ìœ¨ì ìœ¼ë¡œ ëœë‹¤. for idx in tf.range(64): y, h, c = self.dec([y, h, c]) # ì•„ë˜ ë‘ê°€ì§€ ì‘ì—…ì€ test dataë¥¼ ì˜ˆì¸¡í•˜ë¯€ë¡œ ì²˜ìŒ ì˜ˆì¸¡í•œê°’ì„ ë‹¤ì‹œ ë‹¤ìŒ stepì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ì–´ì•¼í•˜ê¸°ì— í•´ì•¼í•˜ëŠ” ì‘ì—…ì´ë‹¤. # ìœ„ì˜ ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¨ yëŠ” softmaxë¥¼ ì§€ë‚˜ì„œ ë‚˜ì˜¨ ê°’ì´ë¯€ë¡œ # ê°€ì¥ ë†’ì€ ê°’ì˜ indexê°’ì„ tf.int32ë¡œ í˜•ë³€í™˜í•´ì£¼ê³  # ìœ„ì—ì„œ ë§Œë“¤ì–´ ë†“ì•˜ë˜ TensorArrayì— idxì— yë¥¼ ì¶”ê°€í•´ì¤€ë‹¤. y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32) # ìœ„ì˜ ê°’ì„ ê·¸ëŒ€ë¡œ ë„£ì–´ì£¼ê²Œ ë˜ë©´ Dimensionì´ í•˜ë‚˜ë°–ì— ì—†ì–´ì„œ # ì‹¤ì œë¡œ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•  ë•Œ Batchë¥¼ ê³ ë ¤í•´ì„œ ì‚¬ìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— (1,1)ìœ¼ë¡œ ì„¤ì •í•´ ì¤€ë‹¤. y = tf.reshape(y, (1, 1)) seq = seq.write(idx, y) if y == self.eos: break # stackì€ ê·¸ë™ì•ˆ TensorArrayë¡œ ë°›ì€ ê°’ì„ ìŒ“ì•„ì£¼ëŠ” ì‘ì—…ì„ í•œë‹¤. return tf.reshape(seq.stack(), (1, 64)) í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë£¨í”„ ì •ì˜1234567891011121314151617181920# Implement training loop@tf.functiondef train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy): # output_labelsëŠ” ì‹¤ì œ outputê³¼ ë¹„êµí•˜ê¸° ìœ„í•¨ # shifted_labelsëŠ” Decoderë¶€ë¶„ì— ì…ë ¥ì„ ë„£ê¸° ìœ„í•¨ output_labels = labels[:, 1:] shifted_labels = labels[:, :-1] with tf.GradientTape() as tape: predictions = model([inputs, shifted_labels], training=True) loss = loss_object(output_labels, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train_loss(loss) train_accuracy(output_labels, predictions)# Implement algorithm test@tf.functiondef test_step(model, inputs): return model(inputs, training=False) ë°ì´í„°ì…‹ ì¤€ë¹„ http://www.aihub.or.krì—ì„œ textë°ì´í„° ì¤‘ AI chatbot ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ì´ ë°ì´í„°ë¥¼ ë‹¤ìš´ë°›ì•„ í•„ìëŠ” google storage ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•´ì„œ ê¸°ì¡´ì˜ ìƒì„±í•´ë†“ì•˜ë˜ ë²„í‚·ì„ í†µí•´ ë°ì´í„°ë¥¼ ì—…ë¡œë“œ í•œ í›„, ë°›ì•„ì™€ì„œ ì‚¬ìš©í•  ê²ƒì´ë‹¤. ì´ ë°©ë²•ì€ google storageì—ì„œ íŒŒì¼ì„ ë°›ì•„ ì‚¬ìš©í•˜ëŠ” gsutil ë°©ì‹ì´ë©° ë¹ ë¥´ë‹¤ëŠ” ì ì´ ì¥ì ì´ì§€ë§Œ í˜„ì¬ ì„¸ì…˜ì´ ì¢…ë£Œë˜ê±°ë‚˜ ìƒˆë¡œì‹œì‘í•  ê²½ìš° ë‹¤ì‹œ ì‹¤í–‰ ì‹œì¼œì£¼ì–´ì•¼ í•˜ëŠ” ë°©ì‹ì´ë‹¤. ë˜í•œ í•„ìì²˜ëŸ¼ google colabì´ ì•„ë‹Œ ìì‹ ì˜ ë¡œì»¬PCë¡œ ì‹¤í–‰í•  ê²½ìš° ì•„ë˜ ë‹¨ê³„ëŠ” ê±´ë„ˆ ë›°ì–´ë„ ìƒê´€ì—†ë‹¤. 12345from google.colab import authauth.authenticate_user()import pandas as pd!gsutil cp gs://kaggle_key/chatbot_data.csv chatbot_data.csv chatbot_data.csv íŒŒì¼ì´ í˜„ì¬ pathì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ 1%ls chatbot_data.csvíŒŒì¼ì„ pandas DataFrameìœ¼ë¡œ ì½ì–´ ì–´ë–¤ ë°ì´í„°ë“¤ì´ ì¡´ì¬í•˜ê³  ì¶”í›„ì— x(Question)ì™€ y(Answer)ë¡œ ë‚˜ëˆ ì£¼ë ¤ë©´ íŒ¨í„´ì„ ì°¾ì•„ì•¼ í•˜ê¸° ë•Œë¬¸ì— ëª¨ë“  ë°ì´í„°ë¥¼ ë³¼ ê²ƒì´ë‹¤. ì „ì²´ ë°ì´í„°ëŠ” 999ê°œì´ê¸° ë–„ë¬¸ì— ì¶œë ¥ë˜ì–´ì§€ëŠ” rowì˜ ìˆ˜ë¥¼ 1000ê°œë¡œ ë§ì¶°ì¤€ë‹¤. 1pd.options.display.max_rows = 1000 12chatbot_data = pd.read_csv('chatbot_data.csv',header=0)chatbot_data ìœ„ì—ì„œ pandasë¡œ ë¶ˆëŸ¬ë“¤ì¸ QA(Question &amp; Answer) dataë¥¼ ë³´ë©´ Questionê³¼ Answerë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤. ì¦‰, ìˆœì°¨ì ì¸ ë°ì´í„°ì¸ ê²ƒì´ë‹¤. ë˜í•œ ëŒ€í™”ì˜ ëì´ ë‚˜ëˆ„ì–´ì ¸ ìˆì§€ ì•Šì•„ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ë ¤ë©´ Dataë¥¼ Questionê³¼ Answer ìŒìœ¼ë¡œ ê°€ê³µí•´ì£¼ì–´ì•¼ í•  ê²ƒì´ë‹¤. ë§¨ì²˜ìŒ ì¤„ë¶€í„° Question ê·¸ë‹¤ìŒì€ Answer ì´ìˆœìœ¼ë¡œ ë˜ì–´ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778dataset_file = 'chatbot_data.csv'okt = Okt()with open(dataset_file, 'r') as file: lines = file.readlines() # okt ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í†µí•´ í˜•íƒœì†Œ ë¶„ì„ì„ í•œì¤„ì”© ì§„í–‰í•˜ì˜€ê³  # ë‚˜ëˆ„ì–´ì§„ í˜•íƒœì†Œë“¤ì„ í•˜ë‚˜ì˜ sequenceë¡œ ë¬¶ì–´ì£¼ê¸°ìœ„í•´ # êµ¬ë¶„ìëŠ” ê³µë°±ì„ ì‚¬ìš©í•´ì„œ joiní•´ì£¼ì—ˆë‹¤. # êµ¬ë¶„ìë¥¼ spaceë¡œ í•œ ì´ìœ ëŠ” ë‚˜ì¤‘ì— ì‚¬ìš©í•  tokenizerì—ì„œ spaceë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ë¥¼ êµ¬ë¶„í•˜ê¸° ë•Œë¬¸ì´ë‹¤. seq = [\" \".join(okt.morphs(line)) for line in lines]questions = seq[::2]# tapì€ Decoderë‹¨ì—ì„œ Shifted Outputì„ ì…ë ¥ìœ¼ë¡œ ë°›ì„ë•Œ ì‹œì‘ì ì„ ì•Œë ¤ì£¼ê¸° ìœ„í•œ SOSë¡œ tap(\\t)ì„ ì‚¬ìš©answers = ['\\t' + lines for lines in seq[1::2]]num_sample = len(questions)perm = list(range(num_sample))random.seed(0)random.shuffle(perm)train_q = list()train_a = list()test_q = list()test_a = list()for idx, qna in enumerate(zip(questions, answers)): q, a = qna if perm[idx] &gt; num_sample//5: train_q.append(q) train_a.append(a) else: test_q.append(q) test_a.append(a)# filtersì˜ defaultì—ëŠ” \\t,\\në„ ì œê±°í•˜ê¸° ë•Œë¬¸ì— ì´ ë‘˜ì„ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ë¬¸ì¥ê¸°í˜¸ë“¤ë§Œ ì œê±°í•˜ê²Œë” ë³€ê²½í•´ì£¼ì—ˆë‹¤.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS, filters='!\"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`&#123;|&#125;~')# ì‹œí€€ìŠ¤ ëª©ë¡ì„ ê¸°ë°˜ìœ¼ë¡œ ë‚´ë¶€ ì–´íœ˜ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.tokenizer.fit_on_texts(train_q + train_a)# ìœ„ì—ì„œ ì—…ë°ì´íŠ¸í•œ ì–´íœ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ìˆ˜í˜•íƒœì˜ ë²¡í„° í˜•íƒœë¡œ ë‚˜íƒ€ë‚´ ì¤€ë‹¤.# ì¶œë ¥ì„ í†µí•´ ë‚˜íƒ€ë‚˜ëŠ” ì‹¤ìˆ˜ëŠ” countì˜ ìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤!train_q_seq = tokenizer.texts_to_sequences(train_q)train_a_seq = tokenizer.texts_to_sequences(train_a)test_q_seq = tokenizer.texts_to_sequences(test_q)test_a_seq = tokenizer.texts_to_sequences(test_a)# yê°’ì—ëŠ” maxlen=65ì¸ ì´ìœ ëŠ” ì•ì— SOSì™€ ë’¤ì— EOSê°€ ë¶™ì–´ ìˆëŠ” ìƒí™©ì´ë¯€ë¡œ í•™ìŠµì‹œì—ëŠ” ì•ì— í•˜ë‚˜ë¥¼ ë–¼ê³ # í•™ìŠµí•˜ë¯€ë¡œ ì‹¤ì œë¡œëŠ” 64ê¸¸ì´ë§Œ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•˜ê²Œ ëœë‹¤.x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq, value=0, padding='pre', maxlen=64)y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq, value=0, padding='post', maxlen=65)x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq, value=0, padding='pre', maxlen=64)y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq, value=0, padding='post', maxlen=65)# prefetch(1024)ëŠ” GPUì— ë¯¸ë¦¬ 1024ê°œì˜ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ fetchí•˜ëŠ” ê¸°ëŠ¥!# ê·¼ë° batch sizeë„ ì•„ë‹ˆê³  ì™œ 1024ê°œ??train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024) í•™ìŠµ í™˜ê²½ ì •ì˜ëª¨ë¸ ìƒì„±, ì†ì‹¤ í•¨ìˆ˜, ìµœì í™” ì•Œê³ ë¦¬ì¦˜, í‰ê°€ì§€í‘œ ì •ì˜1234567891011# ëª¨ë¸ ìƒì„±model = Seq2seq(sos=tokenizer.word_index['\\t'], eos=tokenizer.word_index['\\n'])# ì†ì‹¤í•¨ìˆ˜ ë° ìµœì í™” ê¸°ë²• ì •ì˜loss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()# ì„±ëŠ¥ ì§€í‘œ ì •ì˜train_loss = tf.keras.metrics.Mean(name='train_loss')train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy') í•™ìŠµ ë£¨í”„ ë™ì‘12345678for epoch in range(EPOCHS): for seqs, labels in train_ds: train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy) template='Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy:&#123;&#125;' print(template.format(epoch + 1, train_loss.result(), train_accuracy.result() * 100)) í…ŒìŠ¤íŠ¸ ë£¨í”„1234567891011for test_seq, test_labels in test_ds: prediction = test_step(model, test_seq) test_text = tokenizer.sequences_to_texts(test_seq.numpy()) # ground_truth gt_text = tokenizer.sequences_to_texts(test_labels.numpy()) # prediction texts = tokenizer.sequences_to_texts(prediction.numpy()) print('_') print('q: ', test_text) print('a: ', gt_text) print('p: ', texts) ì˜ˆì¸¡ëœ ê°’ë“¤ì„ ë³´ë©´ train dataì— ê³¼ì í•©ëœ ê²ƒì„ ì¶©ë¶„íˆ ì•Œ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ì´ì œ ì—¬ê¸°ì„œ Attention mechanismì„ ì ìš©ì‹œì¼œë³´ì.Encoder, Decoder, Seq2seq ë¶€ë¶„ì„ ìˆ˜ì •í•˜ë©´ëœë‹¤.Encoder ì´ì „ê³¼ ë‹¤ë¥´ê²Œ LSTM êµ¬ì¡°ì—ì„œ return_sequences=Trueë¥¼ ë„£ì–´ ì „ì²´ Hidden Stateë¥¼ ì¶œë ¥í•˜ê²Œ í•´ì£¼ì—ˆë‹¤. ì´ë¥¼ Key-Valueë¡œ ì‚¬ìš©í•  ê²ƒì´ë‹¤. Embedding ê²°ê³¼ì˜ Dimension : (32(batch_szie), 64(sequenceì˜ ê¸¸ì´), 64(Embedding Featureì˜ ìˆ˜)) LSTMì˜ ê²°ê³¼ì˜ Dimension : (32(batch_szie), 64(sequenceì˜ ê¸¸ì´), 512(LSTM unitì˜ ê°¯ìˆ˜)) H : 32(batch size) 64(sequenceì˜ ê¸¸ì´) 512(LSTM unitìˆ˜) h(s0) : 32(batch size) * 512 (LSTM unitìˆ˜) c(c0) : 32(batch size) * 512 (LSTM unitìˆ˜)12345678910111213141516class Encoder(tf.keras.Model): def __init__(self): super(Encoder, self).__init__() # 2000ê°œì˜ ë‹¨ì–´ë“¤ì„ 64í¬ê¸°ì˜ vectorë¡œ Embeddingí•´ì¤Œ. self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64) # return_stateëŠ” returní•˜ëŠ” Outputì— ìµœê·¼ì˜ stateë¥¼ ë”í•´ì£¼ëŠëƒì— ëŒ€í•œ ì˜µì…˜ # ì¦‰, Hidden stateì™€ Cell stateë¥¼ ì¶œë ¥í•´ì£¼ê¸° ìœ„í•œ ì˜µì…˜ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. # defaultëŠ” Falseì´ë¯€ë¡œ ì£¼ì˜í•˜ì! # return_sequence=Trueë¡œí•˜ëŠ” ì´ìœ ëŠ” Attention mechanismì„ ì‚¬ìš©í•  ë•Œ ìš°ë¦¬ê°€ keyì™€ valueëŠ” # Encoderì—ì„œ ë‚˜ì˜¤ëŠ” Hidden state ë¶€ë¶„ì„ ì‚¬ìš©í–ˆì–´ì•¼ í–ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ëª¨ë“  Hidden Stateë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ë°”ê¿”ì¤€ë‹¤. self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True) def call(self, x, training=False, mask=None): x = self.emb(x) H, h, c = self.lstm(x) return H, h, c Decoder LSTM ë‹¤ìŒì— Attention êµ¬ì¡°ë¥¼ ë„£ì–´ì£¼ê³ , Encoderì˜ ì¶œë ¥ ì¤‘ ëª¨ë“  sequenceì˜ Hidden Stateë¥¼ ëª¨ì•„ë†“ì€ Hì™€ s0, c0, shifted Outputì„ ë°›ì•„ì„œ Attention valueë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ì½”ë“œë¥¼ ìˆ˜ì •ì‹œí‚¨ë‹¤. Dimension : x : shifted_labelsë¡œ ë§¨ë§ˆì§€ë§‰ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ë°ì´í„°ë“¤ =&gt; 32(batch szie) * 64(sequenceì˜ ê¸¸ì´) s0 : ì´ì „ stepì˜ hidden state =&gt; 32(batch size) * 512(LSTMì˜ Unit ê°¯ìˆ˜) c0 : ì´ì „ stepì˜ cell state =&gt; 32(batch size) * 512(LSTMì˜ Unit ê°¯ìˆ˜) H : Encoderë‹¨ì˜ ëª¨ë“  Hidden stateë¥¼ ëª¨ì€ ê²ƒ =&gt; 32(batch size) 64(sequenceì˜ ê¸¸ì´) 512(LSTMì˜ Featureì˜ ê°¯ìˆ˜) embedding ê²°ê³¼ =&gt; 32(batch size) 64(sequenceì˜ ê¸¸ì´) 64(Embedding Featureì˜ ìˆ˜) LSTMì˜ ê²°ê³¼ì˜ Dimension : (32(batch_szie), 64(sequenceì˜ ê¸¸ì´), 512(LSTM unitì˜ ê°¯ìˆ˜)) S : 32(batch size) 64(sequenceì˜ ê¸¸ì´) 512(LSTM unitìˆ˜) h : 32(batch size) * 512 (LSTM unitìˆ˜) c : 32(batch size) * 512 (LSTM unitìˆ˜) S_ì˜ Dimension: 32(batch size) 64(sequenceì˜ ê¸¸ì´) 512(LSTM unit ìˆ˜) Aì˜ Dimension: 32(batch size) 64(sequenceì˜ ê¸¸ì´) 512(LSTM unit ìˆ˜) yì˜ Dimension: 32(batch size) 64(sequenceì˜ ê¸¸ì´) 1024 1234567891011121314151617181920212223242526272829303132333435class Decoder(tf.keras.Model): def __init__(self): super(Decoder, self).__init__() self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64) # return_sequenceëŠ” return í•  Outputì„ full sequence ë˜ëŠ” Sequenceì˜ ë§ˆì§€ë§‰ì—ì„œ ì¶œë ¥í• ì§€ë¥¼ ê²°ì •í•˜ëŠ” ì˜µì…˜ # FalseëŠ” ë§ˆì§€ë§‰ì—ë§Œ ì¶œë ¥, TrueëŠ” ëª¨ë“  ê³³ì—ì„œì˜ ì¶œë ¥ self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True) # LSTM ì¶œë ¥ì—ë‹¤ê°€ Attention valueë¥¼ denseì— ë„˜ê²¨ì£¼ëŠ” ê²ƒì´ Attention mechanismì´ë¯€ë¡œ self.att = tf.keras.layers.Attention() self.dense = tf.keras.layers.Dense(NUM_WORDS, activation='softmax') def call(self, inputs, training=False, mask=None): # x : shifted output, s0 : Decoderë‹¨ì˜ ì²˜ìŒë“¤ì–´ì˜¤ëŠ” Hidden state # c0 : Decoderë‹¨ì˜ ì²˜ìŒë“¤ì–´ì˜¤ëŠ” cell state H: Encoderë‹¨ì˜ Hidden state(Keyì™€ valueë¡œ ì‚¬ìš©) x, s0, c0, H = inputs x = self.emb(x) # initial_stateëŠ” ì…€ì˜ ì²« ë²ˆì§¸ í˜¸ì¶œë¡œ ì „ë‹¬ ë  ì´ˆê¸° ìƒíƒœ í…ì„œ ëª©ë¡ì„ ì˜ë¯¸ # ì´ì „ì˜ Encoderì—ì„œ ë§Œë“¤ì–´ì§„ Hidden stateì™€ Cell stateë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì•¼ í•˜ë¯€ë¡œ # S : Hidden stateë¥¼ ì „ë¶€ë‹¤ ëª¨ì•„ë†“ì€ ê²ƒì´ ë  ê²ƒì´ë‹¤.(Queryë¡œ ì‚¬ìš©) S, h, c = self.lstm(x, initial_state=[s0, c0]) # Queryë¡œ ì‚¬ìš©í•  ë•ŒëŠ” í•˜ë‚˜ ì•ì„  ì‹œì ì„ ì‚¬ìš©í•´ì¤˜ì•¼ í•˜ë¯€ë¡œ # s0ê°€ ì œì¼ ì•ì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ”ë° í˜„ì¬ Encoder ë¶€ë¶„ì—ì„œì˜ ì¶œë ¥ì´ batch í¬ê¸°ì— ë”°ë¼ì„œ lengthê°€ í˜„ì¬ 1ì´ê¸° ë•Œë¬¸ì— 2ì°¨ì›í˜•íƒœë¡œ ë“¤ì–´ì˜¤ê²Œ ëœë‹¤. # ê·¸ëŸ¬ë¯€ë¡œ ì´ì œ 3ì°¨ì› í˜•íƒœë¡œ í™•ì¥í•´ ì£¼ê¸° ìœ„í•´ì„œ newaxisë¥¼ ë„£ì–´ì¤€ë‹¤. # ë˜í•œ decoderì˜ S(Hidden state) ì¤‘ì— ë§ˆì§€ë§‰ì€ ì˜ˆì¸¡í•  ë‹¤ìŒì´ ì—†ìœ¼ë¯€ë¡œ ë°°ì œí•´ì¤€ë‹¤. S_ = tf.concat([s0[:, tf.newaxis, :], S[:, :-1, :]], axis=1) # Attention ì ìš© # ì•„ë˜ []ì•ˆì—ëŠ” ì›ë˜ Query, Keyì™€ value ìˆœìœ¼ë¡œ ì…ë ¥í•´ì•¼í•˜ëŠ”ë° ì•„ë˜ì²˜ëŸ¼ ë‘ê°€ì§€ë§Œ ì…ë ¥í•œë‹¤ë©´ # ë§ˆì§€ë§‰ ê²ƒì„ Keyì™€ valueë¡œ ì‚¬ìš©í•œë‹¤. A = self.att([S_, H]) y = tf.concat([S, A], axis=-1) return self.dense(y), h, c Seq2seq ì´ì „ì˜ ì½”ë“œì—ì„œ encoderì˜ ì¶œë ¥ì— ì „ì²´ Hidden Stateë¥¼ ëª¨ì•„ë†“ì€ ê²ƒê³¼ decoderì˜ ì…ë ¥ìœ¼ë¡œ ì´ê°’ì„ ë°›ëŠ” ì½”ë“œë¥¼ ì¶”ê°€í•´ì£¼ì—ˆë‹¤. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Seq2seq(tf.keras.Model): def __init__(self, sos, eos): super(Seq2seq, self).__init__() self.enc = Encoder() self.dec = Decoder() self.sos = sos self.eos = eos def call(self, inputs, training=False, mask=None): if training is True: # í•™ìŠµì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ìš°ë¦¬ê°€ ì…ë ¥ê³¼ ì¶œë ¥ ë‘ê°€ì§€ë¥¼ ë‹¤ ì•Œê³  ìˆì–´ì•¼ í•œë‹¤. # ì¶œë ¥ì´ í•„ìš”í•œ ì´ìœ ëŠ” Decoderë‹¨ì˜ ì…ë ¥ìœ¼ë¡œ shited_ouputì„ ë„£ì–´ì£¼ê²Œ ë˜ì–´ìˆê¸° ë•Œë¬¸ì´ë‹¤. x, y = inputs # LSTMìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆê¸° ë•Œë¬¸ì— Hidden Stateì™€ Cell Stateë¥¼ ì¶œë ¥ìœ¼ë¡œ ë‚´ì¤€ë‹¤. H, h, c = self.enc(x) # Hidden stateì™€ cell state, shifted outputì„ ì´ˆê¸°ê°’ìœ¼ë¡œ ì…ë ¥ ë°›ê³  # ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¤ëŠ” yëŠ” Decoderì˜ ê²°ê³¼ì´ê¸° ë•Œë¬¸ì— ì „ì²´ ë¬¸ì¥ì´ ë  ê²ƒì´ë‹¤. y, _, _ = self.dec((y, h, c, H)) return y else: x = inputs H, h, c = self.enc(x) # Decoder ë‹¨ì— ì œì¼ ë¨¼ì € sosë¥¼ ë„£ì–´ì£¼ê²Œë” tensorí™”ì‹œí‚¤ê³  y = tf.convert_to_tensor(self.sos) # shapeì„ ë§ì¶°ì£¼ê¸° ìœ„í•œ ì‘ì—…ì´ë‹¤. y = tf.reshape(y, (1, 1)) # ìµœëŒ€ 64ê¸¸ì´ ê¹Œì§€ ì¶œë ¥ìœ¼ë¡œ ë°›ì„ ê²ƒì´ë‹¤. seq = tf.TensorArray(tf.int32, 64) # tf.keras.Modelì— ì˜í•´ì„œ call í•¨ìˆ˜ëŠ” auto graphëª¨ë¸ë¡œ ë³€í™˜ì´ ë˜ê²Œ ë˜ëŠ”ë°, # ì´ë•Œ, tf.rangeë¥¼ ì‚¬ìš©í•´ forë¬¸ì´ë‚˜ whileë¬¸ì„ ì‘ì„±ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ tf í•¨ìˆ˜ë¡œ ë˜ì–´ìˆë‹¤ë©´ # ê·¸ forë¬¸ê³¼ whileë¬¸ì´ êµ‰ì¥íˆ íš¨ìœ¨ì ìœ¼ë¡œ ëœë‹¤. for idx in tf.range(64): y, h, c = self.dec([y, h, c, H]) # ì•„ë˜ ë‘ê°€ì§€ ì‘ì—…ì€ test dataë¥¼ ì˜ˆì¸¡í•˜ë¯€ë¡œ ì²˜ìŒ ì˜ˆì¸¡í•œê°’ì„ ë‹¤ì‹œ ë‹¤ìŒ stepì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ì–´ì•¼í•˜ê¸°ì— í•´ì•¼í•˜ëŠ” ì‘ì—…ì´ë‹¤. # ìœ„ì˜ ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¨ yëŠ” softmaxë¥¼ ì§€ë‚˜ì„œ ë‚˜ì˜¨ ê°’ì´ë¯€ë¡œ # ê°€ì¥ ë†’ì€ ê°’ì˜ indexê°’ì„ tf.int32ë¡œ í˜•ë³€í™˜í•´ì£¼ê³  # ìœ„ì—ì„œ ë§Œë“¤ì–´ ë†“ì•˜ë˜ TensorArrayì— idxì— yë¥¼ ì¶”ê°€í•´ì¤€ë‹¤. y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32) # ìœ„ì˜ ê°’ì„ ê·¸ëŒ€ë¡œ ë„£ì–´ì£¼ê²Œ ë˜ë©´ Dimensionì´ í•˜ë‚˜ë°–ì— ì—†ì–´ì„œ # ì‹¤ì œë¡œ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•  ë•Œ Batchë¥¼ ê³ ë ¤í•´ì„œ ì‚¬ìš©í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— (1,1)ìœ¼ë¡œ ì„¤ì •í•´ ì¤€ë‹¤. y = tf.reshape(y, (1, 1)) seq = seq.write(idx, y) if y == self.eos: break # stackì€ ê·¸ë™ì•ˆ TensorArrayë¡œ ë°›ì€ ê°’ì„ ìŒ“ì•„ì£¼ëŠ” ì‘ì—…ì„ í•œë‹¤. return tf.reshape(seq.stack(), (1, 64))","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"Attention ê¸°ë²•","slug":"deep_learning_10","date":"2020-01-21T08:10:39.000Z","updated":"2020-02-03T15:46:55.627Z","comments":true,"path":"2020/01/21/deep_learning_10/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/21/deep_learning_10/","excerpt":"","text":"Attention ê¸°ë²•Sequence-to-sequence ìš°ì„ , Attention ê¸°ë²•ì´ ê°€ì¥ ë¨¼ì € ì ìš©ë˜ì—ˆë˜ ëª¨ë¸ì¸ Sequence-to-sequence ëª¨ë¸ì„ ì‚´í´ë³´ë©´ì„œ ê°„ëµí•˜ê²Œ conceptionì ì¸ ê²ƒì„ ì‚´í´ë³´ê² ë‹¤. ì•„ë˜ ê·¸ë¦¼ì˜ ì™¼ìª½ ë¶€ë¶„ì€ Encoder êµ¬ì¡°ë¡œ ë˜ì–´ ìˆì–´, Inputì— ë²ˆì—­í•˜ê³ ì í•˜ëŠ” ë¬¸ì¥ì„ ë‹¨ì–´ í•˜ë‚˜ì”© ë°›ëŠ” í˜•íƒœë¡œ ë˜ì–´ìˆë‹¤. ë§ˆì§€ë§‰ ì…ë ¥ ë‹¨ì–´ì—ëŠ” EOS(End-Of-Sequence)ë¼ëŠ” íŠ¹ë³„í•œ ë‹¨ì–´(Token)ë¥¼ ë°›ë„ë¡ ë˜ì–´ìˆë‹¤. ë§ˆì§€ë§‰ EOSê¹Œì§€ ë°›ì€ ë’¤ì˜ Outputì„ Contextë¼ê³  í•œë‹¤. ê·¸ë ‡ê²Œ ë‚˜ì˜¨ Contextë¥¼ Decoder êµ¬ì¡°ì—ì„œ ë„˜ê²¨ ë°›ìœ¼ë©°, ë™ì‹œì— SOS(Start-Of-Sequence)ë¼ëŠ” Tokenì„ ì²˜ìŒ Inputì—ì„œ ì…ë ¥í•´ì£¼ë©´ ë„˜ê²¨ ë°›ì€ Contextë¡œ ë¶€í„° ì²«ë²ˆì§¸ ë‹¨ì–´ë¥¼ ìƒì„±í•œë‹¤. ìƒì„±ëœ ë‹¨ì–´ë¥¼ ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ê³  ë˜ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ìƒì„±í•˜ê³  ì´ëŸ° ì‘ì—…ì„ EOS Tokenì´ Outputìœ¼\u001dë¡œ ë‚˜ì˜¬ë•Œê¹Œì§€ ë°˜ë³µí•´ì¤€ë‹¤. EncoderëŠ” ê²°êµ­ Contextë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œ RNNìœ¼ë¡œ ë™ì‘í•˜ëŠ” êµ¬ì¡°ì´ê³ , DecoderëŠ” í•œë‹¨ì–´ì”© ì¶œë ¥í•˜ëŠ”\u001cë°, ê·¸ ì¶œë ¥ëœ Output(ë‹¨ì–´)ì„ ì˜¤ë¥¸ìª½ìœ¼ë¡œ Shiftí•´ì„œ ì…ë ¥ìœ¼ë¡œ ë°›ì€ ë’¤ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ë§Œë“¤ì–´ì£¼ëŠ” êµ¬ì¡°ì´ë‹¤. ì˜ì–´ ë¬¸ì¥ì˜ ë°ì´í„°í™” í•œê¸€ ë¬¸ì¥ì˜ ë°ì´í„°í™” Seq2seq ëª¨ë¸ê°™ì€ ê²½ìš°ëŠ” ì…ì¶œë ¥ê°„ì˜ stepì´ ë„ˆë¬´ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆìœ¼ë©´ Gradient Vanishingì´ ì¼ì–´ë‚˜ ì˜ í•™ìŠµë˜ì§€ ì•ŠëŠ”ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì™€ ê°™ì€ ê·¸ë¦¼ì—ì„œ ì²˜ìŒ ì…ë ¥ì¸ $x_{0}$ê°€ $y_{1}$ì— ì˜í–¥ì„ ì¤€ë‹¤ë©´(ì˜ì–´ì—ì„œ í•œê¸€ë¡œ ë²ˆì—­í• ì‹œ ì–´ìˆœì´ ë°˜ëŒ€ë¡œ ë˜ì–´ìˆëŠ” ê²½ìš°ë¥¼ ì˜ˆì‹œë¡œ ìƒê°í•˜ë©´ ì´í•´í•˜ê¸° ì‰½ë‹¤.), BPTTë¡œ í¼ì³ë†“ê³  ë´¤ì„ë•Œ, ê¸°ë³¸ì ìœ¼ë¡œ Encoderë‹¨ê³¼ Decoderë‹¨ì´ ë¶„ë¦¬ê°€ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, Ecoderë‹¨ì˜ ì•ìª½ê³¼ Decoder ë’·ìª½ê³¼ì€ ê±°ë¦¬ê°€ ë©€ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì•„ë¬´ë¦¬ LSTMì„ ì“°ê³  GRUë¥¼ ì‚¬ìš©í•œë‹¤\bê³  í•˜ë”ë¼ë„ í•œê³„ê°€ ìˆë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ ê²ƒê³¼ ê°™ì´ ê·¸ë ‡ë‹¤ë©´ Gradient Vanishing ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì´ ì—†ì„ê¹Œ? ê·¸ì— ëŒ€í•œ ë‹µì„ ì•„ë˜ì˜ ê·¸ë¦¼ì„ í†µí•´ ì„¤ëª…í•˜ê² ë‹¤. ì´ì „ì—ëŠ” Encoder ë¶€ë¶„ì—ì„œ Context í•˜ë‚˜(Feature vector í•˜ë‚˜)ë§Œ ë„˜ê²¨ì¤˜ì„œ ê·¸ Contextë¥¼ í†µí•´ Decoderê°€ ì¶œë ¥ì„ ë‚´ì£¼ì—ˆì–´ì•¼ í–ˆëŠ”ë°, ì§ê´€ì ìœ¼ë¡œ ë³¼ë•Œ Inputë“¤ì„ í†µí•´ ë§Œë“  ë§ˆì§€ë§‰ Hidden stateì¸ Feature vectorê°€ Encoderë‹¨ì˜ ëª¨ë“  Inputë“¤ì„ ë§ì€ ë¶€ë¶„ ì»¤ë²„í•  ìˆ˜ ì—†ìœ¼ë©°, Decoder ë¶€ë¶„ì˜ RNNì„ í†µí•´ ì§€ë‚˜ë©´ì„œ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ë°©í–¥ëŒ€ë¡œ ë„˜ì–´ê°ˆê²ƒì´ë¼ëŠ” ë³´ì¥ ë˜í•œ ì—†ë‹¤. ê·¸ë˜ì„œ ì´ì œëŠ” Encoder Hidden stateë¥¼ ëª¨ì•„ì„œ Decoderë¡œ ê°ê° ì „ë‹¬ì‹œì¼œ ì¤Œìœ¼ë¡œì¨ ê° ì¶œë ¥ì— í•„ìš”í•œ Contextë¥¼ ìƒˆë¡œì´ ë½‘ì„ ìˆ˜ ìˆëŠ” êµ¬ì¡°ë¡œ ë³€ê²½í•´ì£¼ì–´ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤í•´ì„œ ëª¨ë“  Encoderë‹¨ì˜ ëª¨ë“  Hidden stateë¥¼ ë‹¤ concateí•´ì„œ í•˜ë‚˜ì˜ ê¸´ Feature vectorë¡œ ë„˜ê²¨ì¤€ë‹¤ë©´ ì¢‹ì€ ì„±ëŠ¥ì„ ê¸°ëŒ€í•˜ê¸´ ì–´ë µìš¸ ê²ƒì´ë‹¤. ì™œëƒí•˜ë©´ ë¬´ì¡°ê±´ Feature vectorì˜ ë°ì´í„°ëŸ‰ì„ ëŠ˜ë ¤ ì¤€ë‹¤ë©´, ê·¸ë§Œí¼ ë§ì€ ë°ì´í„°ì…‹ì„ í†µí•´ ë°ì´í„°ì…‹ì´ coverë¥¼ í•´ì£¼ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. (ìì„¸íˆ ë§í•˜ë©´ í•™ìŠµë°ì´í„°ëŸ‰ì€ ê³ ì •ì ìœ¼ë¡œ ë™ì¼í•œë°, Input vectorì¸ Feature vectorì˜ ì°¨ì›ë§Œ ëŠ˜ë ¤ì£¼ê²Œ ëœë‹¤ë©´ Underfitting ë¬¸ì œê°€ ë°œìƒí•˜ë©´ì„œ Sparsity ë¬¸ì œê°€ ë°œìƒí•´ì„œ í•™ìŠµì´ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤.) ì´ëŸ° êµ¬ì¡°ë¥¼ ì–´ë–»ê²Œ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆì„ê¹Œì— ëŒ€í•œ ë‹µì´ Attention ë©”ì»¤ë‹ˆì¦˜ì´ë‹¤. Attention ë©”ì»¤ë‹ˆì¦˜ì—ì„œëŠ” ì˜ˆë¥¼ ë“¤ì–´ Encoder ë¶€ë¶„ê³¼ Decoder ë¶€ë¶„ì˜ Layerë“¤ì´ ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ ìƒ‰ê¹”ë³„ë¡œ ì—°ê´€ë˜ì–´ ìˆë‹¤ê³  í–ˆì„ë•Œ, ê° ì—°ê´€ë˜ì–´ ìˆëŠ” ë¶€ë¶„ì„ ì•Œê²Œ í•´ì£¼ëŠ” ê²ƒì´ Attention Mechanismì˜ ê¸°ë³¸ ì•„ì´ë””ì–´ì´ë‹¤. Attention ì‹ ê²½ë§ Dictionaryí˜•íƒœë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤ëŠ” ì ì€ ìˆœì„œëŠ” ìƒê´€ ì—†ë‹¤ëŠ” ì˜ë¯¸ì„ì„ ëª…ì‹¬í•˜ì! ì§ˆì˜-ì‘ë‹µì´ ì´ë£¨ì–´ì§€ëŠ” ë©”ì»¤ë‹ˆì¦˜ì„ ì‚´í´ë³´ë©´, queryë¥¼ ë‚ ë ¤ì£¼ê²Œ ë˜ë©´, ìš°ì„  keyë“¤ì„ ë‚˜ì—´í•˜ì—¬ ë™ì¼í•œ ê²ƒì„ ì°¾ì•„ë‚¸ë‹¤. ê·¸ í›„ì—ëŠ” key-valueìŒì—ì„œ ë™ì¼í•œ ë¶€ë¶„ì˜ valueë¥¼ ì¶œë ¥í•´ì¤€ë‹¤. Attention mechanismì€ \bkey-value ìŒì´ ìˆê³ , queryë¥¼ ë‚ ë ¤ì„œ queryì™€ keyë¥¼ ìœ ì‚¬í•œì§€ ë¹„êµë¥¼ í•´ì¤€ë’¤, ìœ ì‚¬ë„ë¥¼ ê³ ë ¤í•œ Valueë“¤ì„ ì„ì–´ì„œ Aggregation(í•©ì„±)ì„ í•´ì¤€ê²ƒì´ Attention valueì´ë‹¤. ëŒ€ë¶€\bë¶„ì˜ Attention networkì—ì„œëŠ” keyì™€ valueë¥¼ ê°™ì€ ê°’ì„ ì‚¬ìš©í•œë‹¤. Seq2seqì—ì„œëŠ” Encoderì˜ Hidden Layerë“¤ì„ keyì™€ valueë¡œ ì‚¬ìš©í•œë‹¤. ì§ê´€ì ìœ¼ë¡œ ìƒê°ì„ í•´ë³´ë©´, Decoderì—ì„œ ì–´ë–¤ê²ƒì„ ì°¾ê³ ì í•œë‹¤ë©´, ì°¾ê³ ì í•˜ëŠ” ê²ƒì— ëŒ€í•œ ì •ë³´ëŠ” Encoderì—ì„œ ì°¾ì„ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ë ‡ê¸°ì— Key-valueê°€ Encoderì˜ Hidden Layerê°€ ë˜ëŠ” ê²ƒì´ë‹¤. ë˜í•œ, Seq2seqì—ì„œëŠ” Decoderì˜ Hidden Layerë“¤ì„ Queryë¡œ ì‚¬ìš©í•œë‹¤. ì£¼ì˜í•  ì ì€, Encoderì™€ ë‹¬ë¦¬ í•˜ë‚˜ ì•ì„  time-stepì˜ Hidden Layerë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì ì´ë‹¤. ì™œëƒí•˜ë©´ í˜„ì¬ì˜ ì¶œë ¥ì„ ë‚´ê¸° ìœ„í•´ì„œëŠ” í˜„ì¬ì˜ ì¶œë ¥ì´ ì‚¬ìš©ë  ìˆ˜ëŠ” ì—†ê¸° ë•Œë¬¸ì— ì¦‰, ë¯¸ë˜(ì˜ˆì¸¡)ë¥¼ ê°€ì§€ê³  í˜„ì¬ ì¶œë ¥ì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ëŠ” ì—†ê¸° ë•Œë¬¸ì— í•˜ë‚˜ ì•ì„  Hidden Layerë¥¼ queryë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. Keyì™€ \bValueëŠ” ì„œë¡œ ë™ì¼í•œ Encoderì˜ Hidden Layerë“¤ì´ ì‚¬ìš©ë˜ë©°, QueryëŠ” Decoderì— ìˆëŠ” ê°ê°ì˜ Hidden Layerë“¤ì´ ë  ê²ƒì´ë©°, i-ë²ˆì§¸ time stepì— ëŒ€í•œ Queryë¥¼ ë‚ ë ¤ì„œ Encoderì— ìˆëŠ” ëª¨ë“  Keyì™€ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•´ì„œ ìµœì¢…ì ìœ¼ë¡œëŠ” ìœ ì‚¬ë„ë¥¼ ê³ ë ¤í•œ Aggregation(ì¢…í•©)í•œ Attention Valueë¥¼ ì¶œë ¥í•´ ì¤€ë‹¤. ì´ë ‡ê²Œ Attention Value($a_{0}$)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ $s_{0}$ Hidden Layerì—ì„œ LSTM êµ¬ì¡°ë¥¼ ê±°ì³ $s_{1}$ Hidden Layerê°€ ë‚˜ì˜¬ ê²ƒì´ë‹¤. ì´ ìƒˆë¡­ê²Œ ì–»ì–´ì§„ $s_{1}$ Hidden Layerì—ë‹¤ê°€ $s_{0}$ë¥¼ í†µí•´ ì–»ì–´ì§„ Attention Value($a_{0}$) Concatenateë¥¼ í•´ì„œ ì¶œë ¥ì„ ë‚´ì¤€ë‹¤.($v_{1}$) ì´ì „ì—ëŠ” Decoderì—ì„œ ê·¸ëŒ€ë¡œ Hidden Layerê°€ ë‚˜ì˜¤ë˜ ê²ƒì´ ì´ì œëŠ” Encoderì˜ Hidden Layerë“¤ì„ ë¹„êµí•´ì„œ ë§Œë“¤ì–´ë‚¸ Attention Valueë¥¼ ê°™ì´ ì¶œë ¥ìœ¼ë¡œ ëƒ„ìœ¼ë¡œì¨, Encoder ë¶€ë¶„\u001cì˜ valueë“¤ì„ ì˜ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ í•´ì£¼ì—ˆë‹¤. Attention is all you need - Transformer ëª¨ë¸ RNN ê°™ì€ ê²½ìš°ëŠ” ì…ë ¥ì„ ìˆœì„œëŒ€ë¡œ ë„£ì–´ì£¼ê¸° ë•Œë¬¸ì— ì…ë ¥ëœ ë‹¨ì–´ì˜ ìœ„ì¹˜ë¥¼ ë”°ë¡œ í‘œì‹œí•˜ì§€ ì•Šì•„ë„ ë˜ì§€ë§Œ, Transformer êµ¬ì¡° ê°™ì€ ê²½ìš°ì—ëŠ” ë³‘ë ¬ì ìœ¼ë¡œ ê³„ì‚°ì„ í•˜ê¸° ë•Œë¬¸ì—, í˜„ì¬ ê³„ì‚°í•˜ê³  ìˆëŠ” ë‹¨ì–´ê°€ ì–´ëŠ ìœ„ì¹˜ì— ìˆëŠ” ë‹¨ì–´ì¸ì§€ë¥¼ í‘œí˜„ì„ í•´ì£¼ì–´ì•¼ í•´ì„œ positional encodingì„ ì‚¬ìš©í•œë‹¤. ìš°ì„ , Transformerì™€ Seq2seq ëª¨ë¸ì„ ë¹„êµí•˜ìë©´, Seq2seq ëª¨ë¸ì€ Encoderì™€ Decoderê°€ ìˆê³  ê·¸ ì‚¬ì´ì— Contextê°€ ì „ë‹¬ë˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤. Transformer ëª¨ë¸ì˜ ê²½ìš°ëŠ” Inputìª½(ì™¼ìª½ì˜ ë¹¨ê°„ìƒ‰ ë°•ìŠ¤)ê³¼ Outputìª½(ì™¼ìª½ì˜ íŒŒë€ìƒ‰ ë°•ìŠ¤)ë¡œ êµ¬ì„±ë˜ë©°, Inputìª½ì—ì„œëŠ” Input embeddingì´ ë“¤ì–´ê°€ì„œ Encodingì´ ì¼ì–´ë‚˜ê³  Contextê°€ ì „ë‹¬ì´ ë˜ì–´ Outputìª½ì˜ Decoderë¶€ë¶„ì—ì„œ Decodingì´ ë˜ì„œ ì¶œë ¥ì´ ë‚˜ì˜¤ê²Œ ëœë‹¤. ì „ì²´ì ìœ¼ë¡œ êµ¬ì¡°ëŠ” ë¹„ìŠ·í•´ë³´ì´ì§€ë§Œ, Seq2seq ëª¨ë¸ì€ RNNë¡œ êµ¬ì„±ë˜ì–´ ìˆì–´ì„œ ìˆœì°¨ì ìœ¼ë¡œ ì´ë£¨ì–´ì§€ê²Œ ë˜ì–´ìˆê³ , Transformer ëª¨ë¸ì€ ë³‘ë ¬ì ìœ¼ë¡œ ê³„ì‚°ë˜ë¯€ë¡œ Inputìª½ì´ ë™ì‹œì— ê³„ì‚°ë˜ê³  Outputìª½ì´ ë™ì‹œì— ê³„\u001cì‚°ë˜ëŠ” í˜•íƒœë¡œ í•™ìŠµì´ë˜ëŠ” ì ì´ ì°¨ì´ì ì´ë‹¤. Inputì„ ë¨¼ì € ë³´ë©´, ì•„ë˜ì˜ ë…¸ë€ìƒ‰ Matrix í˜•íƒœë¡œ ë˜ì–´ ìˆìœ¼ë©°, ì¼ë°˜ì ìœ¼ë¡œëŠ” ì…ë ¥ ë‹¨ì–´ì˜ ê°€ì§“ìˆ˜ì™€ ì¶œë ¥ ë‹¨ì–´ì˜ ê°€ì§“ìˆ˜ëŠ” ë™ì¼í•  ê²ƒì´ë‹¤. ë§Œì•½ ê¸°ê³„ë²ˆì—­ì²˜ëŸ¼ 2ê°œì˜ ì–¸ì–´ê°€ ë‹¤ë¥´ë‹¤ë©´, ë‹¤ë¥¼ ê²ƒì´ë‹¤! Outputì€ ì›ë˜ Seq2seqì˜ êµ¬ì¡°ì—ì„œ ë³´ì•˜ë“¯ì´ shift ì‹œí‚¨ ì…ë ¥ì„ ë„£ì–´ì£¼ì—ˆì—ˆë˜ ê²ƒê³¼ ê°™ì´ SOSë¥¼ ë„£ì–´ì£¼ê³  EOSë¥¼ ë¹¼ì¤€ í˜•íƒœë¡œ Outputsì— ë„£ì–´ì¤€ë‹¤. One-hot encodingìœ¼ë¡œ ë˜ì–´ìˆë˜ ê²ƒë“¤ì„ Embeddingí•´ì„œ ê°ê°ì˜ Embeddingì— ë„£ì–´ì¤€ë‹¤. Positional Encodingì€ ì‹œê°„ì ìœ¼ë¡œ ìœ„ì¹˜ê°€ ë”°ë¥´ë•Œë§ˆë‹¤ ê³ ìœ ì˜ ì½”ë“œë¥¼ ìƒì„±í•´ì„œ Input Embeddingì— ë”í•´ì£¼ëŠ” í˜•íƒœë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. ì´ë ‡ê²Œ í•´ì¤Œìœ¼ë¡œì¨, ì „ì²´ Sequenceì˜ ê¸¸ì´ ì¤‘ ìƒëŒ€ì  ìœ„ì¹˜ì— ë”°ë¼ì„œ ê³ ìœ ì˜ ë²¡í„°ë¥¼ ìƒì„±í•˜ì—¬ Embeddingëœ ë²¡í„°ì— ë”í•´\u001cì£¼ê²Œ ëœë‹¤. ì˜ˆë¥¼ë“¤ë©´, ë³´í†µ Embeddingëœ ë²¡í„°ë“¤ì€ 0ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í¬ê°€ë˜ì–´ìˆëŠ”ë° Input Embeddingì— sinê³¼ cosineì„ ì¡°í•©í•´ì„œ ë§Œë“¤ì–´ì§„ feature ë²¡í„°ë¥¼ ë”í•´ì£¼ëŠ” ê²ƒì´ë¼ê³  ë³´ë©´ëœë‹¤. ìœ„ì—ì„œ ë§í–ˆë“¯, Embeddingë“¤ì€ 0ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í¬í•˜ë¯€ë¡œ ì—¬ê¸°ì— sinê³¼ cosineì„ ì¡°í•©í•´ ë§Œë“  Feature ë²¡í„°ë¥¼ ë”í•´ì¤€ë‹¤í•´\bë„ í¬ê²Œ ì†ìƒì´ ê°€ì§€ ì•Šê¸° ë•Œë¬¸ì— ê±±ì •í•˜ì§€ ì•Šì•„ë„ëœë‹¤. Dot-ProductëŠ” ìš°ë¦¬ê°€ ì•Œê³ ìˆëŠ” ë‚´ì ê³¼ ë™ì¼í•˜ë‹¤. ê·¸ë¦¬ê³  Maskë¥¼ ì´ìš©í•´ì„œ Illegal connectionì˜ Attentionì„ ê¸ˆ\bì§€í•œë‹¤ëŠ” ì˜ë¯¸ëŠ” self-attentionì— ëŒ€í•œ ì´ì•¼ê¸°ì¸ë°, ì¼ë°˜ì ì¸ Attention êµ¬ì¡°ëŠ” Decoderìª½ì— Hidden Layerë¥¼ í†µí•´ Outputì„ ë‚´ë ¤ë©´ Encoder ìª½ì— Hidden Layer ì „ì²´ì™€ ë¹„êµí•´ì„œ ì‚°ì¶œì„í•´ì•¼í•˜ë¯€ë¡œ ì´ëŸ° ê²½ìš°ëŠ” ê´œì°®ì§€ë§Œ, Self-attentionì—ì„œëŠ” Decoderë¥¼ ë˜‘ê°™ì€ Decoder ìê¸° ìì‹ ê³¼ Attentionì„ í•  ìˆ˜ê°€ ìˆëŠ”ë° ì—¬ê¸°ì„œ Decoder ë¶€ë¶„ì˜ í•´ë‹¹ Hidden Layerë¥¼ ì‚°ì¶œí•˜ë ¤ë©´ ìˆœì°¨ì ìœ¼ë¡œ ì¶œë ¥ì´ ë‚˜ì˜¨ë‹¤ê³  í–ˆì„ë•Œ í•´ë‹¹ ë¶€ë¶„ì˜ Decoderë³´ë‹¤ ì´í›„ ì‹œì ì€ ì•„ì§ ê²°ê³¼ê°€ ì‚°ì¶œë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì— ê·¸ë³´ë‹¤ ì•ì„  ì‹œì ì˜ Decoderë¶€ë¶„ì—ì„œì˜ Hidden Layerë“¤ë§Œì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì´ì•¼ê¸°ì´ë‹¤. ì—¬ê¸°ì„œ ë¹„êµí•  ë•Œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” Hidden Layerë“¤ì„ Illegal connectionì´ë¼ê³  í•œë‹¤. ì´ëŸ° Illegal connectionì€ Maskë¥¼ í†µí•´ -infë¡œ ë³´ë‚´ë²„ë¦¬ë©´ Softmaxì—ì„œ ê°’ì´ 0ì´ë˜ëŠ” ê²ƒì„ ì´ìš©í•˜ì—¬ attentionì´ ì•ˆë˜ë„ë¡ êµ¬í˜„í•˜ê³  ìˆë‹¤. Multi-Head Attentionì€ ì‰½ê²Œ ë§í•´ Scaled Dot-Product Attentionì„ hê°œë¥¼ ëª¨ì•„ì„œ ë³‘ë ¬ì ìœ¼ë¡œ ì—°ì‚°ì„ í•  ìˆ˜ ìˆê²Œë”í•˜ëŠ” ê²ƒì´ë‹¤. hê°œë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ê°™ì€ ì…ë ¥ì— ëŒ€í•´ì„œ ë” í’ë¶€í•œ ì¶œë ¥ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‹¤. ë˜í•œ, ì—¬ê¸°ì„œ ì²˜ìŒ Linear ì—°ì‚°(Matrix Mult)ì„ í•˜ëŠ” ê²ƒì€ Query, Key, Value ê°ê° ì¤‘ íŠ¹ì • ì°¨ì›ë§Œì„ ë³´ê² ë‹¤ëŠ” ì´ì•¼ê¸°ì´ë©°, ì°¨ì›ì„ ì¤„ì—¬ì£¼ì–´ ë³‘ë ¬ì— ìœ ë¦¬í•œ êµ¬ì¡°ë¥¼ ë§Œë“œëŠ” ì—­í• ë„ ìˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì´ ì—°ì‚°ì„ í•œ í›„ì— hê°€ì§€ë¡œ ë³‘ë ¬ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ í’ë¶€í•œ ì¶œë ¥ì„ ì–»ì„ ìˆ˜ ìˆëŠ” ê²ƒì„ ì´í•´í•  ìˆ˜ ìˆë‹¤. ì œì¼ ì•„ë˜ ë‹¨ê³„ì˜ Linearì—°ì‚°ì„ í†µí•´Q,K,Vì˜ ì°¨ì›ì„ ê°ì†Œ(hê°œë¡œ ë‚˜ëˆ ì§)ì‹œí‚¨ë‹¤ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ë˜í•œ ì•„ë˜ ìˆ˜ì‹ì—ì„œ ê°€ì¤‘ì¹˜ $W_{V,i}, W_{K,i}, W_{Q,i}$ì˜ ê°ê°ì˜ Dimensionë³´ë‹¤ ë” ì‘ì€ ê°’ìœ¼ë¡œ ëª¨ë¸ì˜ Dimension($d_{model}$)ì„ í•´ì¤€ë‹¤. ì´ëŠ” value, key, queryì˜ ì°¨ì›ì„ ëª¨ë¸ì— ì‚¬ìš©í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ì°¨ì›ì„ ë³€í™˜ì‹œì¼œì£¼ëŠ” ì˜ë¯¸ì´ê¸°ë„ í•˜ë‹¤. MaskëŠ” RNNì˜ Decoderë‹¨ì„ ìƒê°í•´ë³´ì•˜ì„ë•Œ, contextê°€ ì•ì—ì„œ ë’¤ë¡œ ë„˜ì–´ê°€ë©´ì„œ ì´ë¯¸ êµ¬í•œ ê²ƒë“¤ë§Œ ì°¸ì¡°ë¥¼ í•  ìˆ˜ ìˆëŠ”ë°, Transformerêµ¬ì¡°ì—ì„œëŠ” ë³‘ë ¬ì ìœ¼ë¡œ ê³„ì‚°ì„ í•˜ê¸° ë•Œë¬¸ì— self-attentionì„ í•  ê²½ìš°ì—ëŠ” ì‹œê°„ì ìœ¼ë¡œ ì•ì—\u001cì„œ ì¼ì–´ë‚œ ê²ƒë“¤ì— ëŒ€í•´ì„œë§Œ ì˜í–¥ì„ ë°›ê²Œí•´ì£¼ì–´ì•¼ RNNê³¼ ë™ì¼í•œ êµ¬ì¡°ê°€ ë˜ê¸° ë•Œë¬¸\u001dì— Maskë¥¼ ì´ìš©í•´ì„œ ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ì‹œì ì„ í¬í•¨í•œ ë¯¸ë˜ê°’ë“¤ì„ ê°€ë ¤ì¤€ë‹¤. Multi-Head Attentionì´ Transformerì— ì–´ë–»ê²Œ ì ìš©ë˜ì–´ ìˆëŠ”ì§€ ì‚´í´ë³´ì. Self-Attentionì€ Decoderì™€ ë™ì¼í•œ Decoderë¥¼ ì°¸ì¡°í•˜ë¯€ë¡œ Keyì™€ Queryì™€ ValueëŠ” ëª¨ë‘ ê°™ì€ ê²ƒì´ë‹¤. Encoding ê°™ì€ ê²½ìš°ì—ëŠ” causual systemì¼ í•„ìš”ê°€ ë”±íˆ ì—†ê¸° ë•Œë¬¸ì— Mask ì—†ì´ Key, Value, Queryê°€ ê·¸ëŒ€ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆì§€ë§Œ, Decoder ë¶€ë¶„ê°™ì€ ê²½ìš°ì—ëŠ” í˜„\bì¬ Queryí•˜ë ¤ê³  í•˜ëŠ” ê²ƒì´ Keyì™€ valueê°€ Queryë³´ë‹¤ ë” ì•ì„œì„œ ë‚˜ì˜¬ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— Maskë¥¼ í™œìš©í•œ Masked Multi-Head Attentionì„ ì‚¬ìš©í•œë‹¤. ì´ë ‡ê²Œ Encoderë‹¨ì˜ Self-Attentionì„ í†µí•´ì„œ Attentionì´ ê°•ì¡°ë˜ì–´ ìˆëŠ” Featureë“¤ì„ ì¶”ì¶œì„ í•´ì£¼ê³  Decoderë‹¨ì—ì„œëŠ” Output Embedding(or ì´ì „ì˜ ì¶œë ¥ê°’)ì´ ë“¤ì–´ì™”ì„ ë•Œ ì´ê²ƒì„ í†µí•´ Masked Multi-Head Attentionì„ í†µí•´ Feature ì¶”ì¶œì„ í•´ì£¼ê³ , ê·¸ ë‹¤ìŒì— ë¶‰ì€ ìƒ‰ ë°•ìŠ¤ ë¶€ë¶„ì—ì„œëŠ” ì´ Decoderë¥¼ í†µí•´ ì¶”ì¶œëœ Featureê°€ Queryë¡œ ë“¤ì–´ê°€ê³ , ë‚˜ë¨¸ì§€ Key, ValueëŠ” Encoderë¥¼ í†µí•´ ë§Œë“¤ì–´ì§„ ì¶œë ¥ì„ ê°€ì§€ê³  ì…ë ¥ì„ ë°›ê²Œëœë‹¤. ê²°êµ­ì—ëŠ” ì´ëŸ° êµ¬ì¡°ëŠ” Seq2seq ëª¨ë¸ì˜ Attentionê³¼ ë™ì¼í•œ êµ¬ì¡°ê°€ ë˜ê²Œ ë  ê²ƒì´ë‹¤. ì‹¤ì œë¡œëŠ” Multi-Head Attentionì´ ë³‘ë ¬ì ìœ¼ë¡œ ê³„ì‚°ë¨ìœ¼ë¡œì¨ self-attentionì´ RNNì„ ëŒ€ì²´í•´ì„œ ë“¤ì–´ê°„ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. Position-wise Feed-ForwardëŠ” íŠ¹ë³„í•œ ê²ƒì€ ì•„ë‹ˆê³  ì•ì„œì„œ ë§í–ˆë˜ ê²ƒê³¼ ê°™ì´ ì•„ë˜ ì´ˆë¡ìƒ‰ ë°•ìŠ¤ëŠ” ê°€ë¡œê°€ ë¬¸ì¥ì˜ ê¸¸ì´, ì„¸ë¡œê°€ One-hot vectorë¥¼ í¬ê¸°ë¡œ ê°–ëŠ” í–‰ë ¬ì¸ë° ë³‘ë ¬ì ì„ ì²˜ë¦¬ë˜ëŠ” input ë‹¨ì–´ í•˜ë‚˜ë§ˆë‹¤ ë™ì¼í•œ êµ¬ì¡°ì˜ activationì´ ReLuì¸ FC Layerì¸µì„ ê³µìœ í•´ì„œ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥ì„ ë‚´ë³´ë‚¸ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì´ë¥¼ í†µí•´ ë³‘ë ¬ì ìœ¼ë¡œ ê³„ì‚°í•˜ì§€ë§Œ ê¸°ì¡´ì˜ FeedForward propagationì„ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. Feed-Forwardê°€ ì¼ì–´ë‚œ ë‹¤ìŒì´ë‚˜ Self-Attentionì´ ì¼ì–´ë‚œ ë‹¤ìŒì—ëŠ” ì´ì „ì˜ ê²ƒ(Skip connection)ì„ ê°€ì ¸ì™€ì„œ ë” í•´ì¤€ ë’¤ Layer Normalizationì„ ìˆ˜í–‰í•´ì„œ ì‚¬ìš©í•˜ê³  ìˆë‹¤. Layer Normalizationì€ Batchì˜ ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ” Normalizationì´ë¼ê³  ìƒê°í•˜ë©´ëœë‹¤. ë§ˆì§€ë§‰ Feed-Forwardì— ì˜í•´ì„œ ë§ˆì§€ë§‰ Feature ì¶œë ¥ì´ ë‚˜ì˜¤ê²Œ ë˜ë©´ Linear ì—°ì‚°(Matrix Mult)ì„ ì‚¬ìš©í•´ì„œ ì¶œë ¥ ë‹¨ì–´ ì¢…ë¥˜ì˜ ìˆ˜ì— ë§ì¶”ê²Œ One-hot vectorë¡œ ë§Œë“¤ì–´ì¤€ë‹¤. ê·¸ëŸ° ë‹¤ìŒ Softmaxë¥¼ ì´ìš©í•´ì„œ ì–´ë–¤ ë‹¨ì–´ì¸ì§€ classificationì„ í•œë‹¤. Attention ì‹ ê²½ë§ì˜ ìˆ˜ì‹ì  ì´í•´ Attention mechanismì€ Keyì™€ Queryë¥¼ ë¹„êµí•˜ëŠ” Comparisonì„ í†µí•´ ê·¸ì— ë”°ë¥¸ ìœ ì‚¬ë„ë¥¼ ê°€ì¤‘ì¹˜ì²˜ëŸ¼ ì‚¬ìš©í•˜ì—¬ Keyì— ë§ëŠ” valueë“¤ì˜ ì¡°í•©ìœ¼ë¡œ Aggregation(ê°€ì¤‘í•©)ì„ í†µí•˜ì—¬ Attention valueë¥¼ ë§Œë“¤ì–´ ì£¼ëŠ” êµ¬ì¡°ê°€ Attention mechanismì´ì—ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ê²°êµ­ Queryì™€ ë¹„ìŠ·í•˜ë©´ ë¹„ìŠ·í•  ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ì–´ ì¶œë ¥ì„ ì£¼ëŠ” ê²ƒì´ë‹¤. compare í•¨ìˆ˜ë¡œëŠ” Dot-Productê°€ ë§ì´ ì“°ì´ë©°, ì €ê¸°ì„œ kì™€ qê°€ ê°ê° ë²¡í„°ì˜ normì´ 1ì´ë¼ë©´ ê²°êµ­ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•´ ì§ˆ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ê¸¸ì´ê°€ 1ì´ ì•„ë‹ ê²½ìš°ë¥¼ ìƒê°í•´ì„œ Dot-productì´í›„ì— softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ì˜ í•©ì„ 1ë¡œ ë§Œë“¤ì–´ ì£¼ê²Œë”í•˜ì—¬ ê°ê°ì˜ ê°€ì¤‘ì¹˜ë“¤ì„ í•˜ë‚˜ì˜ í™•ë¥ ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œë” ë³€í™˜í•´ ì£¼ì–´ ì‚¬ìš©í•œë‹¤. Seq2seq ëª¨ë¸ì€ Encoderêµ¬ì¡°ë¥¼ í†µí•´ Featureë“¤ì„ ë§Œë“¤ê²Œ ë˜ê³  ìµœì¢…ì ìœ¼ë¡œëŠ” ì¶œë ¥ìœ¼ë¡œ Contextë¥¼ ìƒì„±í•˜\u001cì—¬ ì´ Context í•˜ë‚˜ì— ì˜ì§€í•´ì„œ DecoderëŠ” SOS(Start Of Sequence)ë¥¼ ì‹œì‘ìœ¼ë¡œ ì¶œë ¥ìœ¼ë¡œëŠ” ë‹¨ì–´ë¥¼ í•˜ë‚˜ì”© ë‚´ì–´ì£¼ëŠ” ëª¨ë¸ì´ë‹¤. Key-valueìŒì€ ê¸°ì¡´ì˜ Contextë§Œì„ ë³´ë©° ì¶œë ¥ì„ ë‚´ì£¼ì—ˆë˜ ê²ƒê³¼ ë‹¤ë¥´ê²Œ Decoderë¶€ë¶„ì˜ Hidden Layerì— ëŒ€í•œ ì¶œë ¥ì„ ë‚¼ë•Œ, Encoder ë¶€ë¶„ì— ì¤‘ê°„ì¤‘ê°„ ë¶€ë¶„ì„ ì•Œê²Œí•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©ë˜ì–´ì§„ë‹¤. QueryëŠ” Decoderì˜ Hidden Layerë“¤ì„ ì‚¬ìš©í•˜ëŠ”ë°, í•´ë‹¹ ì¶œë ¥ì„ í•´ì•¼í•˜ëŠ” RNNêµ¬ì¡°ì˜ í•˜ë‚˜ ì´ì „ì˜ time-stepì˜ Hidden Layerë¥¼ Queryë¡œ ì‚¬ìš©í•œë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ì! ì•„ë˜ ê·¸ë¦¼ì—ì„œëŠ” $s_{0}, s_{1}, s_{2}$ê°€ í•´ë‹¹í•œë‹¤. i-th queryê°€ ë“¤ì–´ì˜¤ê²Œë˜ë©´ ê°ê° Keyì™€ ë¹„êµë¥¼ í•˜ê²Œë˜ê³ , ì•ì—ì„œ ë§í•œê²ƒê³¼ ê°™ì´ ë‚´ì í•œ ë’¤ Softmaxë¥¼ í•´ì¤˜ ê°€ì¤‘ì¹˜ë¡œ ë§Œë“ ë’¤ì— ê°ê°ì— í•´ë‹¹í•˜ëŠ” Valueì™€ ê³±í•´ ê°€ì¤‘í•©ì„ í•œ ê²ƒì„ Attention valueë¡œ ì‚°ì¶œí•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì¶œë ¥ê³¼ RNN êµ¬ì¡°ì‚¬ì´ì— ì‹¤ì œë¡œëŠ” FC Layerê°€ í•˜ë‚˜ ì¡´ì¬í•´ì„œ ì¶œë ¥ì„ One-hot vectorë¡œ ë§Œë“¤ì–´ì¤€ë‹¤. $X\\in R^{BXLXN}$ì—ì„œ B:Batch size, L:ë¬¸ì¥ì˜ ê¸¸ì´, \bN:One-hot vetorë‚˜ embedding Featureì˜ ê¸¸ì´ë¥¼ ì˜ë¯¸í•˜ë©°, ì—¬ê¸°ì„œ Decode ìª½ìœ¼ë¡œ Contextë¥¼ ë„˜ê¸¸ë•ŒëŠ” LSTMì´ë¼ë©´ Hidden Stateì™€ Cell State ë‘˜ë‹¤ ë„˜ê²¨ì£¼ì–´ì•¼ í•˜ê¸°ì— Batch_size X Hidden stateì˜ Feature ê°¯ìˆ˜ì¸ Mì„ ì‚¬ì´ì¦ˆë¡œ ê°–ëŠ” tensorë¥¼ ë„˜ê²¨ì¤„ ê²ƒì´ë‹¤. Encoderì˜ Hidden Stateì¸ Hê°€ Attention mechanismì— Keyì™€ Valueë¡œ ì…ë ¥ì´ ë˜ê³ , Queryì—ëŠ” Decoderì˜ í•œ step ì•ì„  Hidden Stateë¥¼ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. ë§Œì•½ì— Inputì˜ ì–¸ì–´ì™€ Outputì˜ ì–¸ì–´ê°€ ë‹¤ë¥´ë‹¤ë©´, ë‹¨ì–´ì˜ ê°€ì§“ìˆ˜ë‚˜ ê¸¸ì´ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ìˆë‹¤ëŠ” ì ì„ ì£¼ì˜í•˜ì! sinë²•ì¹™ê³¼ cosineë²•ì¹™ì— ì˜í•´ ê°ê° ë¶„ë¦¬í•´ì„œ ì“¸ìˆ˜ ìˆëŠ”ë° ê²°êµ­ ë§ì…ˆê³¼ ëº„ì…ˆìœ¼ë¡œ ì´ Positional Encodingì´ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸\bì— FC Layerì—ì„œ í•™ìŠµí•˜ëŠ”ë° ìš©ì´í•˜ê²Œ ë  ê²ƒì´ë‹¤. ì „ì²´ì ì¸ êµ¬ì¡°ëŠ” Attention mechanismì„ ì ìš©í•œ Seq2seq ëª¨ë¸ê³¼ ìœ ì‚¬í•˜ì§€ë§Œ Scaleì´ ë˜ëŠ” ë¶€ë¶„ê³¼ Maskë¥¼ ì‚¬ìš©í•˜ëŠ” ë¶€ë¶„\bì´ ë‹¤ë¥´ë‹¤. ë˜í•œ, ê°€ì¥ ì¤‘ìš”í•œ ì ì€ ì•„ë˜ ìˆ˜ì‹ ì¤‘ Queryì™€ key, value ë¶€ë¶„ì„ ëª¨ì•„ì„œ í•˜ë‚˜ì˜ metricsë¡œ ë§Œë“¤ì–´ ì¤Œìœ¼ë¡œì¨ ìš°ë¦¬ê°€ ì²˜ìŒ ë°°ì› ë˜ shallow NNê³¼ ê°™ì´ ë³‘ë ¬ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆê²Œë” í•´ì£¼ì—ˆë‹¤ëŠ” ê²ƒì´ ê°€ì¥ í° Transformer ëª¨ë¸ì˜ ìš”ì†Œì¼ ê²ƒì´ë‹¤. scale ì²˜ë¦¬ë¥¼ í•´ì¤Œìœ¼ë¡œì¨ ë‚´ì ì˜ ê°’ì´ ë„ˆë¬´ ì»¤ì ¸ì„œ saturationë˜ì„œ Softmaxê°’ì´ ì°¨ì´ê°€ ë§ì´ë‚˜ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤. ì œì¼ ì•„ë˜ ë‹¨ê³„ì˜ Linearì—°ì‚°ì„ í†µí•´Q,K,Vì˜ ì°¨ì›ì„ ê°ì†Œ(hê°œë¡œ ë‚˜ëˆ ì§)ì‹œí‚¨ë‹¤ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ë˜í•œ ì•„ë˜ ìˆ˜ì‹ì—ì„œ ê°€ì¤‘ì¹˜ $W_{V,i}, W_{K,i}, W_{Q,i}$ì˜ ê°ê°ì˜ Dimensionë³´ë‹¤ ë” ì‘ì€ ê°’ìœ¼ë¡œ ëª¨ë¸ì˜ Dimension($d_{model}$)ì„ í•´ì¤€ë‹¤. ì´ëŠ” value, key, queryì˜ ì°¨ì›ì„ ëª¨ë¸ì— ì‚¬ìš©í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ì°¨ì›ì„ ë³€í™˜ì‹œì¼œì£¼ëŠ” ì˜ë¯¸ì´ê¸°ë„ í•˜ë‹¤.","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"ìˆœí™˜ì‹ ê²½ë§(Vanilla RNN ë° LSTM êµ¬í˜„)","slug":"deep_learning_09","date":"2020-01-20T20:39:33.000Z","updated":"2020-01-21T08:10:51.058Z","comments":true,"path":"2020/01/21/deep_learning_09/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/21/deep_learning_09/","excerpt":"","text":"ìˆœí™˜ ì‹ ê²½ë§ êµ¬í˜„ ë° í•™ìŠµVanilla RNN1!pip install tensorflow==2.0.0-beta1 1import tensorflow as tf tensorflow version í™•ì¸1print(tf.__version__) í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •1234EPOCHS = 10# ìš°ë¦¬ê°€ ë¶„ì„í•  ë•Œ 10000ê°œì˜ ë‹¨ì–´ë§Œ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ë¡œ ì„¤ì •í•˜ì˜€ë‹¤.NUM_WORDS = 10000 ëª¨ë¸ì •ì˜123456789101112class MyModel(tf.keras.Model): def __init__(self): super(MyModel, self).__init__() # input_dim, output_dim self.emb = tf.keras.layers.Embedding(NUM_WORDS, 16) self.rnn = tf.keras.layers.SimpleRNN(32) self.dense = tf.keras.layers.Dense(2, activation='softmax') def call(self, x, training=None, mask=None): x = self.emb(x) x = self.rnn(x) return self.dense(x) í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë£¨í”„ ì •ì˜1234567891011121314151617181920# Implement training loop@tf.functiondef train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy): with tf.GradientTape() as tape: predictions = model(inputs, training=True) loss = loss_object(labels, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train_loss(loss) train_accuracy(labels, predictions)# Implement algorithm test@tf.functiondef test_step(model, images, labels, loss_object, test_loss, test_accuracy): predictions = model(images, training=False) t_loss = loss_object(labels, predictions) test_loss(t_loss) test_accuracy(labels, predictions) ë°ì´í„°ì…‹ ì¤€ë¹„IMDB reviewë¥¼ ë³´ê³  ê¸ì •ì¸ì§€ ë¶€ì •ì¸ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œì´ë©°, y(target value)ëŠ” binary value(0 or 1)ë¥¼ ê°€ì§€ì§€ë§Œ x_data(feature)ì—ì„œ ê°ê°ì˜ reviewì˜ ê¸¸ì´ê°€ ë‹¤ë¥´ë¯€ë¡œ ì…ë ¥ì—ì„œ ì¶œë ¥ì´ ë‚˜ì˜¤ëŠ” ê¸°ì¤€ì„ ë§ì¶”ê¸° ìœ„í•´ zero-paddingì„ í•´ì£¼ëŠ” ì‘ì—…ì„ ì‹¤í–‰í•  ê²ƒì´ë‹¤. ì•„ë˜ì˜ â€˜pad_sequenceâ€™í•¨ìˆ˜ì—ì„œ maxlen=32ëŠ” ìµœëŒ€ ê¸¸ì´ë¥¼ 32ê¸€ìë¡œ ë§ì¶”ê² ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. maxlen=32ë¡œ í•¨ìœ¼ë¡œì¨ ì›ë˜ ë³¸ ë°ì´í„°ì˜ ë§¨ ë’¤ë¶€ë¶„ì—ì„œ ì‹œì‘í•´ì„œ 32ë²ˆì§¸ ë°ì´í„° ê¹Œì§€ë¥¼ ì˜ë¼ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë©°, ì´ ë¶€ë¶„ì— ë°ì´í„°ê°€ ì—†ì„ ì‹œ 0ìœ¼ë¡œ padding ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ” í•¨ìˆ˜ì´ë‹¤. 1234567891011121314151617imdb = tf.keras.datasets.imdb(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=NUM_WORDS)# padding='post' ë’¤ìª½ìœ¼ë¡œ paddingí•´ì¤€ë‹¤.x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, value=0, padding='pre', maxlen=32)x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, value=0, padding='pre', maxlen=32)train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) í•™ìŠµ í™˜ê²½ ì •ì˜ëª¨ë¸ ìƒì„±, ì†ì‹¤í•¨ìˆ˜, ìµœì í™” ì•Œê³ ë¦¬ì¦˜, í‰ê°€ì§€í‘œ ì •ì˜12345678910111213# ëª¨ë¸ ìƒì„±model = MyModel()# ì†ì‹¤í•¨ìˆ˜ ë° ìµœì í™” ê¸°ë²• ì •ì˜loss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()# ì„±ëŠ¥ ì§€í‘œ ì •ì˜train_loss = tf.keras.metrics.Mean(name=\"train_loss\")train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")test_loss = tf.keras.metrics.Mean(name=\"test_loss\")test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\") í•™ìŠµ ë£¨í”„ ë™ì‘12345678910111213for epoch in range(EPOCHS): for seqs, labels, in train_ds: train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy) for test_seqs, test_labels in test_ds: test_step(model, test_seqs, test_labels, loss_object, test_loss, test_accuracy) template = \"Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;\" print(template.format(epoch + 1, train_loss.result(), train_accuracy.result() * 100, test_loss.result(), test_accuracy.result() * 100)) LSTM kerasëŠ” ê³ ìˆ˜ì¤€ APIì´ë¯€ë¡œ ì´ë¯¸ ë‚´ë¶€ì— êµ¬í˜„ì´ ë˜ì–´ìˆì–´ ë‹¤ìŒê³¼ ê°™ì´ ë³€ê²½í•´ì£¼ëŠ” ê²ƒë§Œìœ¼ë¡œ LSTMì„ êµ¬í˜„ í•  ìˆ˜ ìˆë‹¤. 123456789101112131415class MyModel(tf.keras.Model): def __init__(self): super(MyModel, self).__init__() # input_dim, output_dim self.emb = tf.keras.layers.Embedding(NUM_WORDS, 16) # tf.keras.layers.GRU(32)ë„ ê°€ëŠ¥ # ì°¸ê³ ë¡œ RNNì€ ì¸µì„ ìŒ“ì„ìˆ˜ë¡ ì„±ëŠ¥ì´ ì•ˆì¢‹ì•„ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” ì ì„ ì£¼ì˜í•˜ì! self.rnn = tf.keras.layers.LSTM(32) self.dense = tf.keras.layers.Dense(2, activation='softmax') def call(self, x, training=None, mask=None): x = self.emb(x) x = self.rnn(x) return self.dense(x)","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"NLP ì „ì²˜ë¦¬","slug":"NLP_02","date":"2020-01-19T06:29:26.000Z","updated":"2020-02-03T10:53:23.888Z","comments":true,"path":"2020/01/19/NLP_02/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/19/NLP_02/","excerpt":"","text":"í˜•íƒœì†Œ Tokenizing ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ì–´ Tokenizing ë¼ì´ë¸ŒëŸ¬ë¦¬ 1) NLTK íŒŒì´ì¬ì—ì„œ ì˜ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì‘ì—…ì„ í•˜ëŠ”ë° ë§ ì“°ì´ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” 50ì—¬ ê°œê°€ ë„˜ëŠ” ë§ë­‰ì¹˜ ë¦¬ì†Œë¥¼ í™œìš©í•´ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•  ìˆ˜ ìˆê²Œ ì œê³µ í•œë‹¤. ì§ê´€ì ìœ¼ë¡œ í•¨ìˆ˜ë¥¼ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ êµ¬ì„±ë¼ ìˆì–´ ë¹ ë¥´ê²Œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ë¥¼ í•  ìˆ˜ ìˆë‹¤. ë˜í•œ ë‹¨ì–´ ë‹¨ìœ„ í† í¬ë‚˜ì´ì§•ê³¼ ë¬¸ì¥ ë‹¨ìœ„ í† í¬ë‚˜ì´ì§•ì„ í•˜ëŠ” ëª¨ë“ˆì´ ë”°ë¡œ ìˆìœ¼ë©° â€˜aâ€™, â€˜theâ€™ ê°™ì€ ê´€ì‚¬ë‚˜ â€˜isâ€™ì™€ ê°™ì´ ìì£¼ ì˜ë¯¸ëŠ” ë³„ë¡œ ì—†ì§€ë§Œ ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ì¸ ë¶ˆìš©ì–´ë“¤ì„ ëª¨ì•„ ë¶ˆìš©ì–´ ì‚¬ì „ì„ êµ¬ì„±í•˜ê³  ìˆì–´ ë”°ë¡œ ë¶ˆìš©ì–´ë¥¼ ì •ì˜í•  í•„ìš”ì—†ì´ ë°”ë¡œ ì‚¬ìš©ê°€ëŠ¥í•˜ë‹¤. 2) Spacy NLTKì™€ ê°™ì€ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. ì£¼ë¡œ ìƒì—…ìš© ëª©ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¡Œë‹¤ëŠ” ì ì´ NLTKì™€ ë‹¤ë¥´ë©°, ì˜ì–´ë¥¼ í¬í•¨í•œ 8ê°œ ì–¸ì–´ì— ëŒ€í•œ ìì—°ì–´ ì „ì²˜ë¦¬ ëª¨ë“ˆì„ ì œê³µí•˜ê³ , ë¹ ë¥¸ ì†ë„ë¡œ ì „ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤. ì›í•˜ëŠ” ì–¸ì–´ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ í•œ ë²ˆì— í•´ê²°í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìœ¼ë©°, íŠ¹íˆ ë”¥ëŸ¬ë‹ ì–¸ì–´ ëª¨ë¸ì˜ ê°œë°œë„ ì§€ì›í•˜ê³  ìˆì–´ ë§¤ë ¥ì ì´ë‹¤. NLTKì™€ ë‹¤ë¥´ê²Œ ë‹¨ì–´ ë‹¨ìœ„, ë¬¸ì¥ ë‹¨ìœ„ í† í¬ë‚˜ì´ì§•ì„ í•œê°€ì§€ ëª¨ë“ˆì„ í†µí•´ ì²˜ë¦¬í•œë‹¤. ì´ëŸ¬í•œ ì˜ì–´ í† í¬ë‚˜ì´ì§• ë„êµ¬ëŠ” í•œêµ­ì–´ì— ì ìš©í•  ìˆ˜ ì—†ë‹¤!! í•œê¸€ í† í¬ë‚˜ì´ì§• ë¼ì´ë¸ŒëŸ¬ë¦¬ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ê° ì–¸ì–´ë§ˆë‹¤ ëª¨ë‘ íŠ¹ì§•ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì²œí¸ì¼ë¥ ì ìœ¼ë¡œ ë™ì¼í•œ ë°©ë²•ì„ ì ìš©í•˜ê¸°ëŠ” ì–´ë µë‹¤. í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ì— ë§ì´ ì‚¬ìš©ë˜ëŠ” íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ KoNLPyë¥¼ ì†Œê°œí•˜ê² ë‹¤. 1) KoNLPy(ì§€ë„í•™ìŠµ ê¸°ë²•ìœ¼ë¡œ í•™ìŠµ) í•œê¸€ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ì‰½ê³  ê°„ê²°í•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ì§„ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤. ë˜í•œ êµ­ë‚´ì— ì´ë¯¸ ë§Œë“œì–´ì ¸ ì‚¬ìš©ë˜ê³  ìˆëŠ” ì—¬ëŸ¬ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í—ˆìš©í•œë‹¤. í˜•íƒœì†Œ ë¶„ì„ìœ¼ë¡œ í˜•íƒœì†Œ ë‹¨ìœ„ì˜ í† í¬ë‚˜ì´ì§•ì„ ê°€ëŠ¥í•˜ê²Œ í• ë¿ë§Œ ì•„ë‹ˆë¼ êµ¬ë¬¸ ë¶„ì„ì„ ê°€ëŠ¥í•˜ê²Œ í•´ì„œ ì–¸ì–´ ë¶„ì„ì„ í•˜ëŠ” ë° ìœ ìš©í•œ ë„êµ¬ë‹¤. í•œê¸€ í…ìŠ¤íŠ¸ì˜ ê²½ìš°ì—ëŠ” í˜•íƒœì†Œ ë‹¨ìœ„ í† í¬ë‚˜ì´ì§•ì´ í•„ìš”í•  ë•Œê°€ ìˆë‹¤. KoNLPyì—ì„œëŠ” ì—¬ëŸ¬ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì œê³µí•˜ë©°, ê° í˜•íƒœì†Œ ë¶„ì„ê¸°ë³„ë¡œ ë¶„ì„í•œ ê²°ê³¼ê°€ ë‹¤ë¥´ë¯€ë¡œ ìì‹ ì˜ ë¶„ì„ ë°ì´í„°ì— ë§ëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì„ íƒí•´ì„œ ì‚¬ìš©í•  ê²ƒì„ ê¶Œí•œë‹¤. Mecabì˜ ê²½ìš° ì›ë„ìš°ì—ì„œëŠ” ì‚¬ìš©í•  ìˆ˜ ì—†ìœ¼ë‹ˆ ì°¸ê³ í•´ì„œ ì‚¬ìš©í•˜ì. Hannanum (í•œë‚˜ëˆ”) Kkma (ê¼¬ê¼¬ë§ˆ) Komoran (ì½”ëª¨ë€) Mecab (ë©”ì¼€ë¸Œ) Okt(Twitter) KAISTì—ì„œ ê°œë°œëœ í•œë‚˜ëˆ”ì€ ë‹¤ìŒê³¼ ê°™ì€ ë©”ì†Œë“œë¥¼ ì œê³µí•œë‹¤. ì„œìš¸ëŒ€í•™êµì—ì„œ ê°œë°œëœ í•œë‚˜ëˆ”ì€ ë‹¤ìŒê³¼ ê°™ì€ ë©”ì†Œë“œë¥¼ ì œê³µí•œë‹¤. ì½”ëª¨ë€ì€ ë‹¤ìŒê³¼ ê°™ì€ ë©”ì†Œë“œë¥¼ ì œê³µí•œë‹¤. mecabì€ ì€ì „í•œë‹¢ì´ë€ ì˜ë¯¸ë¥¼ ì§€ë‹ˆê³  ìˆìœ¼ë©°(TMIì¸ë“¯), ë¹ ë¥´ê³  ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë©”ì†Œë“œë¥¼ ì œê³µí•œë‹¤. OktëŠ” êµ¬ Twitterë¡œ ë¶ˆë¦¬ë©°, Twitterì—ì„œ ë§Œë“¤ì—ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë©”ì†Œë“¤ë¥´ ì œê³µí•œë‹¤. macOSì—ì„œ ì„¤ì¹˜123456789# JPype1ì€ íŒŒì´ì¬ì—ì„œ ìë°” í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì´ë‹¤.# ë§Œì•½ windowë¼ë©´ https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpypeì—ì„œ ë§ëŠ” ì‚¬ì–‘ì„ ì„¤ì¹˜# 64bit-python3.6 ë²„ì „ì´ë¼ë©´ JPype1-0.63-cp36-cp36m-win_amd64.whl ì„ ì„¤ì¹˜í•˜ë©´ëœë‹¤.# pip install JPype1-0.63-cp36-cp36m-win_amd64.whl# MacOsì—ì„ conda install -c conda-forge jpype1pip install konlpy í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš©ë²• ê°ê°ì˜ í˜•íƒœì†Œ ë¶„ì„ê¸°ëŠ” í´ë˜ìŠ¤ìƒì„±ë§Œ ë‹¤ë¥´ê³  ë‚˜ë¨¸ì§€ëŠ” ë™ì¼í•œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì•„ë˜ ì˜ˆì‹œì—ì„œëŠ” í˜•íƒœì†Œ ë¶„ì„ê¸° ì¤‘ ì œì¼ ì†ë„ê°€ ë¹ ë¥´ë‹¤ê³  ì•Œë ¤ì ¸ ìˆëŠ” Mecabì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤. posë¥¼ í†µí•´ ì–»ëŠ” í’ˆì‚¬ì˜ íƒœê¹…ì˜ ì˜ë¯¸ë¥¼ ì•Œê³  ì‹¶ë‹¤ë©´ í´ë¦­ tokenizer.morphs() í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤. ì˜µì…˜ìœ¼ë¡œëŠ” normê³¼ stemì´ìˆë‹¤. ê°ê° True í˜¹ì€ False ê°’ì„ ë°›ìœ¼ë©°, normì€ normalizeì˜ ì•½ìë¡œì„œ ë¬¸ì¥ì„ ì •ê·œí™”í•˜ëŠ” ì—­íì„ í•˜ê³ , stemì€ ê° ë‹¨ì–´ì—ì„œ ì–´ê°„ì„ ì¶”ì¶œí•˜ëŠ” ê¸°ëŠ¥(ì˜ˆì‹œ: í•´ì•¼ì§€ -&gt; í•˜ë‹¤)ì´ë‹¤. ê°ê° Trueë¡œ ì„¤ì •í•˜ë©´ ê° ê¸°ëŠ¥ì´ ì ìš© ëœë‹¤. ë‘˜ ë‹¤ defaultëŠ” Falseì´ë‹¤. tokenizer.nouns() í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ë½‘ì•„ë‚¸ë‹¤. tokenizer.phrases() í…ìŠ¤íŠ¸ì—ì„œ ì–´ì ˆì„ ë½‘ì•„ë‚¸ë‹¤. tokenizer.pos() ìœ„ì˜ ì„¸ í•¨ìˆ˜ëŠ” ì¶”ì¶œê¸°ì¸ ë°˜ë©´ì—, pos í•¨ìˆ˜ëŠ” íƒœê¹…í•¨ìˆ˜ì´ë‹¤. ê° í’ˆì‚¬ë¥¼ íƒœê¹…í•˜ëŠ” ì—­í• ì„ í•œë‹¤. norm, stem ì˜µì…˜ì´ ì¡´ì¬í•˜ë©° join=Trueë¡œ í•˜ê²Œ ë˜ë©´ (í˜•íƒœì†Œ, í’ˆì‚¬)ì˜ í˜•íƒœì—ì„œ í˜•íƒœì†Œ/í’ˆì‚¬ í˜•íƒœë¡œ ë¶™ì—¬ì„œ ë¦¬ìŠ¤íŠ¸í™”í•œë‹¤. ì–´ë–¤ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í• ì§€ëŠ” ìì‹ ì´ ê°€ì§„ ë°ì´í„°ë¡œ ì‹¤í—˜ ì‚¼ì•„ í˜•íƒœì†Œ ë¶„ì„ì„ í•´ë³´ê³  ì†ë„ë‚˜ í’ˆì§ˆì„ ë¹„êµí•´ì„œ ê³ ë¥´ëŠ” ê²ƒì´ ì¢‹ë‹¤. ìì‹ ì˜ ë¶„ì„ì—ì„œ ì‚¬ì „ì— ì¶”ê°€í•´ì•¼í•  ë‹¨ì–´ë“¤ì´ ìˆë‹¤ë©´ ì‚¬ìš©ì ì‚¬ì „ì— ì¶”ê°€í•´ ì£¼ë©´ ëœë‹¤. 12345678910111213141516from konlpy.tag import Mecabtokenizer = Mecab()# í˜•íƒœì†Œ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°tokenizer.morphs(\"ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤.\")# ê²°ê³¼[\"ì•„ë²„ì§€\", \"ê°€\", \"ë°©\", \"ì—\", \"ë“¤ì–´ê°€\", \"ì‹ ë‹¤\"]# í’ˆì‚¬ íƒœê·¸tokenizer.pos('ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤.')``` ##### ê²°ê³¼``` bash[('ì•„ë²„ì§€', 'NNG'), ('ê°€', 'JKS'), ('ë°©', 'NNG'), ('ì—', 'JKB'), ('ë“¤ì–´ê°€', 'VV'), ('ì‹ ë‹¤', 'EP+EC')] Mecabì— ì‚¬ìš©ì ì‚¬ì „ ì¶”ê°€í•˜ê¸° Komoranì€ ì¶”ê°€í•  ë‚´ìš©ì˜ txtë¥¼ ë§Œë“¤ì–´ ê°ì²´ ìƒì„±ì‹œ userdic íŒŒë¼ë¯¸í„°ì— ë§Œë“  pathë¥¼ ì…ë ¥í•´ ì£¼ë©´ë˜ê³ , Hannanumì€ konlpy/java/data/kE/dic_user.txtì— ì¡´ì¬í•˜ë©° ì—¬ê¸°ì— ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ í˜•ì‹ì— ë§ì¶° ì¶”ê°€í•´ì£¼ë©´ëœë‹¤. KkmaëŠ” ~/anaconda3/lib/python3.6/site-packages/konlpy/java/kkma-2.0.jar ì••ì¶•íŒŒì¼ ë‚´ì˜ .dic í˜•ì‹ì˜ íŒŒì¼ë“¤ì´ dictionary íŒŒì¼ì´ë¯€ë¡œ ì••ì¶•ì„ í‘¼ë’¤ í•´ë‹¹ í’ˆì‚¬ì— ë§ëŠ” íŒŒì¼ì— ë‹¨ì–´ë¥¼ ì¶”ê°€í•´ì£¼ë©´ëœë‹¤. ê·¸ë¦¬ê³ ë‚˜ì„œ ë‹¤ì‹œ .jarë¡œ ì••ì¶•ì„ í•´ì£¼ê³  ì›ë³¸ì€ ë§Œì•½ì„ ìœ„í•´ ë‹¤ë¥¸ ê³³ì— ë³´ê´€í•˜ë©° ì‚¬ìš©í•œë‹¤. í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ë‹¤ ë³´ë©´ ê°€ì¥ ì‹ ê²½ ì¨ì•¼ í•˜ëŠ” ì ì´ ì¤‘ìš” tokenë“¤ì„ ì–´ë–»ê²Œ ì²˜ë¦¬í•´ì•¼ í• ì§€ë‹¤. ì˜ˆë¥¼ë“¤ë©´ ìš°ë¦¬ê°€ â€˜ì²œë¦¬ë§ˆì „ìâ€™ë¼ëŠ” ê¸°ì—…ì˜ ë°ì´í„° ë¶„ì„ íŒ€ì— ì†í•´ ìˆê³  ì²œë¦¬ë§ˆì €ë‚˜ì— ê´€í•œ Corpusë¥¼ ë¶„ì„í•˜ê±°ë‚˜ ì´ë¡œë¶€í„° ì„ë² ë”©ì„ ë§Œë“¤ì–´ì•¼ í•œë‹¤ê³  ê°€ì •í•´ë³´ì. ì´ ê²½ìš° â€˜ì²œë¦¬ë§ˆì „ìâ€™ë¼ëŠ” tokenì€ ì„¬ì„¸í•˜ê²Œ ì²˜ë¦¬í•´ì•¼í•œë‹¤. ë§Œì•½ â€˜ì²œë¦¬ë§ˆì „ì í…”ë ˆë¹„ì „ ì •ë§ ì¢‹ë„¤ìš”â€™ë¼ëŠ” ê°€ìƒì˜ ë¦¬ë·°ë¥¼ ë¶„ì„í•œë‹¤ë©´ ì²œë¦¬ë§ˆ ì „ì ë³´ë‹¤ ì²œë¦¬ë§ˆì „ìë¡œ ë¶„ì„ëì„ ë•Œ ì„ë² ë”© í’ˆì§ˆì´ ë” ì¢‹ì„ ê²ƒì´ë‹¤. ì´ëŸ´ ê²½ìš° ì‚¬ìš©ì ì‚¬ì „ì— ì¶”ê°€í•˜ì—¬ í•˜ë‚˜ì˜ í† í°ìœ¼ë¡œ ë¶„ì„ë  ìˆ˜ ìˆë„ë¡ ê°•ì œí•´ì•¼í•œë‹¤. 2) Khaiii ì‚¬ìš©ë²• (ì§€ë„í•™ìŠµê¸°ë²•ìœ¼ë¡œ í•™ìŠµ) reference Khaiii(Kakao Hangul Analyzer iii)ëŠ” kakaoê°€ 2018ë…„ ë§ ê³µê°œí•œ ì˜¤í”ˆì†ŒìŠ¤ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ë‹¤. êµ­ë¦½êµ­ì–´ì›ì´ êµ¬ì¶•í•œ ì„¸ì¢… ì½”í¼ìŠ¤ë¥¼ ì´ìš©í•´ CNN ëª¨ë¸ì„ ì ìš©í•´ í•™ìŠµí–ˆë‹¤. Khaiiiì˜ ì•„í‚¤í…ì²˜ëŠ” ì…ë ¥ ë¬¸ì¥ì„ ë¬¸ì ë‹¨ìœ„ë¡œ ì½ì–´ ë“¤ì¸ ë’¤ convolution filterê°€ ì´ ë¬¸ìë“¤ì„ ìŠ¬ë¼ì´ë”©í•´ ê°€ë©´ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•œë‹¤. ì¶œë ¥ ë…¸ë“œì—ì„œëŠ” ì´ë ‡ê²Œ ëª¨ì€ ì •ë³´ë“¤ì„ ì¢…í•©í•´ í˜•íƒœì†Œì˜ ê²½ê³„ì™€ í’ˆì‚¬ íƒœê·¸ë¥¼ ì˜ˆì¸¡í•œë‹¤. ì¹´ì¹´ì˜¤ ì¸¡ ì„¤ëª…ì— ë”°ë¥´ë©´ ëª¨ë¸ì„ C++ë¡œ êµ¬í˜„í•´ GPU ì—†ì´ë„ í˜•íƒœì†Œ ë¶„ì„ì´ ê°€ëŠ¥í•˜ë©° ì‹¤í–‰ ì†ë„ ì—­ì‹œ ë¹ ë¥´ë‹¤ê³  í•œë‹¤. 123456789101112131415from khaiii import KhaiiiApitokenizer = KhaiiiApi()data = tokenizer.analyze('ì•„ë²„ì§€ê°€ë°©ì—ë“¤ì–´ê°€ì‹ ë‹¤')tokens = []for word in data: token.extend([str(m).split(\"/\")[0] for m in word.morphs])# ê²°ê³¼['ì•„ë²„ì§€', 'ê°€', 'ë°©ì—', 'ë“¤', 'ì–´', 'ê°€', 'ì‹œ', 'ã„´ë‹¤']# í’ˆì‚¬ ì •ë³´ í™•ì¸ taggingfor word in data: token.extend([str(m) for m in word.morphs]) ê²°ê³¼1['ì•„ë²„ì§€/NNG', 'ê°€/JKS', 'ë°©ì—/NNG', 'ë“¤/VV', 'ì–´/EC', 'ê°€/VV', 'ì‹œ/EP', 'ã„´ë‹¤/EC'] 3) soynlp (ë¹„ì§€ë„í•™ìŠµìœ¼ë¡œ í•™ìŠµ) í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íŒë³„ ë“±ì„ ì§€ì›í•˜ëŠ” íŒŒì´ì¬ ê¸°ë°˜ í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ íŒ¨í‚¤ì§€ë‹¤. ë°ì´í„° íŒ¨í„´ì„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ëŠ” ë¹„ì§€ë„ í•™ìŠµ ì ‘ê·¼ë²•ì„ ì§€í–¥í•˜ê¸° ë•Œë¬¸ì— í•˜ë‚˜ì˜ ë¬¸ì¥ í˜¹ì€ ë¬¸ì„œì—ì„œë³´ë‹¤ëŠ” ì–´ëŠ ì •ë„ ê·œëª¨ê°€ ìˆìœ¼ë©´ì„œ ë™ì§ˆì ì¸ ë¬¸ì„œ ì§‘í•©(homogeneous documents)ì—ì„œ ì˜ ì‘ë™í•œë‹¤. soynlp íŒ¨í‚¤ì§€ì— í¬í•¨ëœ í˜•íƒœì†Œ ë¶„ì„ê¸°ëŠ” ë°ì´í„°ì˜ í†µê³„ëŸ‰ì„ í™•ì¸í•´ ë§Œë“  ë‹¨ì–´ ì ìˆ˜ í‘œë¡œ ì‘ë™í•œë‹¤. ë‹¨ì–´ ì ìˆ˜ëŠ” í¬ê²Œ ì‘ì§‘í™•ë¥ (Cohesion Probability) ê³¼ ë¸Œëœì¹­ ì—”íŠ¸ë¡œí”¼(Branching Entropy)ë¥¼ í™œìš©í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” ì£¼ì–´ì§„ ë¬¸ìë ¹ì´ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ë¼ í•¨ê»˜ ìì£¼ ë‚˜íƒ€ë‚˜ê³ (ì‘ì§‘ í™•ë¥ ì´ ë†’ì„ ë•Œ), ê·¸ ë‹¨ì–´ ì•ë’¤ë¡œ ë‹¤ì–‘í•œ ì¡°ì‚¬, ì–´ë¯¸ í˜¹ì€ ë‹¤ë¥¸ ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ” ê²½ìš°(ë¸Œëœì¹­ ì—”íŠ¸ë¡œí”¼ê°€ ë†’ì„ ë•Œ) í•´ë‹¹ ë¬¸ìì—´ì„ í˜•íƒœì†Œë¡œ ì·¨ê¸‰í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì£¼ì–´ì§„ Corpusì—ì„œ â€˜ê¿€ì¼â€™ì´ë¼ëŠ” ë‹¨ì–´ê°€ ì—°ê²°ë¼ ìì£¼ ë‚˜íƒ€ë‚¬ë‹¤ë©´ â€˜ê¿€ì¼â€™ì„ í˜•íƒœì†Œë¼ê³  ë³¸ë‹¤(ì‘ì§‘ í™•ë¥ ì´ ë†’ìŒ). í•œí¸ â€˜ê¿€ì¼â€™ ì•ì— â€˜ì˜í™”â€™, â€˜ì •ë§â€™, â€˜ë„ˆë¬´â€™ ë“± ë¬¸ìì—´ì´, ë’¤ì— â€˜ã…‹ã…‹â€™, â€˜ã…ã…â€™, â€˜!!â€™ ë“± íŒ¨í„´ì´ ë‹¤ì–‘í•˜ê²Œ ë‚˜íƒ€ë‚¬ë‹¤ë©´ ì´ ì—­ì‹œ â€˜ê¿€ì¼â€™ì„ í˜•íƒœì†Œë¡œ ì·¨ê¸‰í•œë‹¤.(ë¸Œëœì¹­ ì—”íŠ¸ë¡œí”¼ê°€ ë†’ìŒ) Cohesion score + L-Tokenizer Cohesion scoreëŠ” í•œêµ­ì–´ì˜ ë‹¨ì–´ ì¶”ì¶œì„ ìœ„í•˜ì—¬ character n-gram ì„ ì´ìš©í•œë‹¤. ìƒˆë¡œìš´ ê°œë…ì„ ì„¤ëª…í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ë‹¨ì–´ê°€ ë§Œë“¤ì–´ì§€ê¸° ë•Œë¬¸ì— ëª¨ë“  ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ì‚¬ì „ì€ ì¡´ì¬í•  ìˆ˜ ì—†ë‹¤. í•™ìŠµë°ì´í„°ë¥¼ ì´ìš©í•˜ëŠ” supervised algorithms ì€ ê°€ë¥´ì³ì£¼ì§€ ì•Šì€ ë‹¨ì–´ë¥¼ ì¸ì‹í•˜ê¸°ê°€ ì–´ë µë‹¤. ì‹¤ì§ˆì ìœ¼ë¡œëŠ” ì‚¬ì „ì— ë“±ë¡ë˜ì§€ ì•Šì€ ë‹¨ì–´ëŠ” í˜•íƒœì†Œ ë¶„ì„ì„ í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. í†µê³„ ê¸°ë°˜ ë‹¨ì–´ ì¶”ì¶œ ê¸°ë²•ì€ â€˜ìš°ë¦¬ê°€ ë¶„ì„í•˜ë ¤ëŠ” ë°ì´í„°ì—ì„œ ìµœëŒ€í•œ ë‹¨ì–´ë¥¼ ì¸ì‹â€™í•˜ì—¬ í•™ìŠµë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” supervised approach ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•œ ë°©ë²•ì´ë‹¤. ë‹¨ì–´ì˜ ê²½ê³„ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ P(xy|x)ì˜ ê°’ì´ ì»¤ì§€ê³ , ë‹¨ì–´ì˜ ê²½ê³„ë¥¼ ë„˜ì–´ì„œë©´ P(xy|x)ì˜ ê°’ì´ ì¤„ì–´ë“ ë‹¤. ì´ í˜„ìƒì„ ì´ìš©í•˜ì—¬ L part ì—ì„œ ë‹¨ì–´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” character n-gram ê¸°ë°˜ score ë¥¼ ì •ì˜í•œë‹¤. í•„ìê°€ ì´í•´í•˜ê¸°ë¡œëŠ” ê°„ë‹¨í•˜ê²Œ ë§í•˜ë©´ corpusì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ ì¤‘ ì˜ë¯¸ë¥¼ ê°–ëŠ” ë‹¨ìœ„ë¥¼ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ìœ¼ë¡œì¨ Cohesion scoreê°€ ë†’ì€ ê²ƒì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì°¸ì¡° 1234567891011121314151617181920212223242526from soynlp.word import WordExtractorsentence = [ë°ì´í„°]word_extractor = WordExtractor(min_frequency=100, min_cohesion_forward=0.05, min_right_branching_entropy=0.0)word_extractor.train(sentence)# model ì €ì¥word_extractor.save(model_fname_and_path)# ìœ„ì—ì„œ ì €ì¥í•œ ëª¨ë¸ loadimport mathfrom soynlp.tokenizer import LTokenizermodel_fname = 'ìœ„ì—ì„œ ì €ì¥í–ˆë˜ model path'word_extractor = WordExtractor(min_frequency=100, min_cohesion_forward=0.05, min_right_branching_entropy=0.0)word_extractor.load(model_fname)scores = word_extractor.word_scores()scores = &#123;key ; (scores[key].cohesion_forward * math.exp(scores[key].min_right_branching_entropy)) for key in scores.keys()&#125;tokenizer = LTokenizer(scores=scores)tokens = tokenizer.tokenize('ì• ë¹„ëŠ” ì¢…ì´ì—ˆë‹¤.') 4) êµ¬ê¸€ ì„¼í…ìŠ¤í”¼ìŠ¤(sentencepiece) êµ¬ê¸€ì—ì„œ ê³µê°œí•œ ë¹„ì§€ë„ í•™ìŠµê¸°ë°˜ í˜•íƒœì†Œ ë¶„ì„ íŒ¨í‚¤ì§€ì´ë©°, 1994ë…„ ì œì•ˆëœ ë°”ì´íŠ¸ í˜ì–´ ì¸ì½”ë”©(BPE : Byte Pair Encoding)ê¸°ë²• ë“±ì„ ì§€ì›í•˜ë©° pip ì„¤ì¹˜ë¥¼ í†µí•´ íŒŒì´ì¬ ì½˜ì†”ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. BPEì˜ ê¸°ë³¸ ì›ë¦¬ Corpusì—ì„œ ê°€ì¥ ë§ì´ ë“±ì¥í•œ ë¬¸ìì—´ì„ ë³‘í•©í•´ ë¬¸ìì—´ì„ ì••ì¶•í•˜ëŠ” ê²ƒ ì˜ˆì‹œ aaabdaaabac ìœ„ì˜ ë¬¸ìì—´ì—ì„œëŠ” aaê°€ ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚¬ë‹¤. ì´ë¥¼ Zë¡œ ì¹˜í™˜í•˜ë©´ ì›ë˜ ë¬¸ìì—´ì„ ë‹¤ìŒê³¼ ê°™ì´ ì••ì¶•í•  ìˆ˜ ìˆë‹¤. ZabdZabac ì´ë²ˆì—ëŠ” abê°€ ê°€ì¥ ë§ì´ ë‚˜íƒ€ë‚¬ìœ¼ë¯€ë¡œ Yë¡œ ì¹˜í™˜í•˜ê² ë‹¤. ZYdZYac ìì—°ì–´ ì²˜ë¦¬ì—ì„œ BPEê°€ ì²˜ìŒ ì“°ì¸ ê²ƒì€ ê¸°ê³„ ë²ˆì—­ ë¶„ì•¼ë‹¤. BPEë¥¼ í™œìš©í•´ í† í¬ë‚˜ì´ì¦ˆí•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì˜ í•µì‹¬ì€ ì›í•˜ëŠ” ì–´íœ˜ ì§‘í•© í¬ê¸°ê°€ ë  ë•Œê¹Œì§€ ë°˜ë³µì ìœ¼ë¡œ ê³ ë¹ˆë„ ë¬¸ìì—´ë“¤ì„ ë³‘í•©í•´ ì–´íœ˜ ì§‘í•©ì— ì¶”ê°€í•œë‹¤. ì´ê²ƒì´ BPEí•™ìŠµì´ë‹¤. í•™ìŠµì´ ëë‚œ ì´í›„ì˜ ì˜ˆì¸¡ê³¼ì •ì€ ë¬¸ì¥ ë‚´ ê° ì–´ì ˆ(ë„ì–´ì“°ê¸°ë¡œ ë¬¸ì¥ì„ ë‚˜ëˆˆ ê²ƒ)ì— ì–´íœ˜ ì§‘í•©ì— ìˆëŠ” subwordê°€ í¬í•¨ë¼ ìˆì„ ê²½ìš° í•´ë‹¹ subwordë¥¼ ì–´ì ˆì—ì„œ ë¶„ë¦¬í•œë‹¤.(ìµœì¥ ì¼ì¹˜ ê¸°ì¤€) ì´í›„ ì–´ì ˆì˜ ë‚˜ë¨¸ì§€ì—ì„œ ì–´íœ˜ ì§‘í•©ì— ìˆëŠ” subwordë¥¼ ë‹¤ì‹œ ì°¾ê³ , ë˜ ë¶„ë¦¬í•œë‹¤. ì–´ì ˆ ëê¹Œì§€ ì°¾ì•˜ëŠ”ë° ì–´íœ˜ ì§‘í•©ì— ì—†ìœ¼ë©´ ë¯¸ë“±ë¡ ë‹¨ì–´(Unknown word)ë¡œ ì·¨ê¸‰í•œë‹¤. BERT ëª¨ë¸ì€ BPEë¡œ í•™ìŠµí•œ ì–´íœ˜ ì§‘í•©ì„ ì“´ë‹¤. BPEëŠ” ë¬¸ìì—´ ê¸°ë°˜ì˜ ë¹„ì§€ë„ í•™ìŠµ ê¸°ë²•ì´ê¸° ë•Œë¬¸ì— ë°ì´í„°ë§Œ í™•ë³´í•  ìˆ˜ ìˆë‹¤ë©´ ì–´ë–¤ ì–¸ì–´ì—ë“  ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤. ë¬¼ë¡  BERT ëª¨ë¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì–´íœ˜ ì§‘í•©ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆê²Œ í•˜ê¸° ìœ„í•´ì„œëŠ” ì–¸ë”ë°”(_) ë¬¸ìë¥¼ â€˜##â€™ë¡œ ë°”ê¾¸ê³  [PAD], [UNK], [CLS], [MASK], [SEP] ë“± ìŠ¤í˜ì…œ í† í°ì„ ì¶”ê°€í•œë‹¤. êµ¬ê¸€ì´ ê³µê°œí•œ BERT ëª¨ë¸ ì½”ë“œì—ì„œ BPEë¡œ í•™ìŠµí•œ ì–´íœ˜ ì§‘í•©ìœ¼ë¡œ í† í°ì„ ë¶„ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ì‹¤í–‰ 1234567891011121314import sentencepiece as spmtrain =\"\"\"--input=input_file_path \\ --model_prefix=sentence \\ --vocab_size=32000 \\ --model_type=bpe --character_coverage=0.9995\"\"\"spm.SentencePieceTrainer.Train(train)from bert.tokenization import FullTokenizervocab_fname = \"vocabulary_file_path.vocab\"tokenizer = FullTokenizer(vocab_file=vocab_fname, do_lower_case=False)tokenizer.tokenize(\"ì§‘ì—ì¢€ ê°€ì\") ê²°ê³¼1['ì§‘ì—', '##ì¢€', 'ê°€ì'] soynlp í˜•íƒœì†Œ ë¶„ì„ì´ë‚˜ BPE ë°©ì‹ì˜ í† í¬ë‚˜ì´ì¦ˆ ê¸°ë²•ì€ ë„ì–´ì“°ê¸°ì— ë”°ë¼ ë¶„ì„ ê²°ê³¼ê°€ í¬ê²Œ ë‹¬ë¼ì§€ë¯€ë¡œ ì´ë“¤ ëª¨ë¸ì„ í•™ìŠµí•˜ê¸° ì „ ë„ì–´ì“°ê¸° êµì •ì„ ë¨¼ì € ì ìš©í•˜ë©´ ê·¸ ë¶„ì„ í’ˆì§ˆì´ ê°œì„ ë  ìˆ˜ ìˆë‹¤.ë„ì–´ì“°ê¸° êµì • soynlpì—ì„œëŠ” ë„ì–´ì“°ê¸° êµì • ëª¨ë“ˆë„ ì œê³µí•œë‹¤. Corpusì—ì„œ ë„ì–´ì“°ê¸° íŒ¨í„´ì„ í•™ìŠµí•œ ë’¤ í•´ë‹¹ íŒ¨í„´ëŒ€ë¡œ êµì •ì„ ìˆ˜í–‰í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•™ìŠµ ë°ì´í„°ì—ì„œ â€˜í•˜ìê³ â€™ë¼ëŠ” ë¬¸ì ì•ë’¤ë¡œ ë‹¤ìˆ˜ì˜ ê³µë°±ì´ ë°œê²¬ëë‹¤ë©´ ì˜ˆì¸¡ë‹¨ê³„ì—ì„œ â€˜í•˜ìê³ â€™ê°€ ì¶œí˜„í•œë‹¤ë©´ ì•ë’¤ë¥¼ ë„ì–´ì„œ êµì •í•˜ëŠ” ë°©ì‹ì´ë‹¤. 123456789101112from soyspacing.countbase importCountSpace# corpusê°€ ë„ì–´ì“°ê¸°ê°€ ì´ë¯¸ ì˜¬ë°”ë¡œ ë˜ì–´ìˆì–´ì•¼ í’ˆì§ˆì´ ë†’ì•„ì§ˆ ê²ƒì´ë‹¤.corpus_fname = 'corpus_path'model_fname = 'ì €ì¥í• ë•Œ ì‚¬ìš©í•  ëª¨ë¸ path'model = CountSpace()model.train(corpus_fname)model.save(model_fname, json_format=False)model.load_model(model_fname, json_format=False)model.correct(\"ì–´ë¦´ë•Œë³´ê³  ì§€ê¸ˆë‹¤ì‹œë´ë„ ì¬ë°Œì–´ìš”\") ê²°ê³¼1[ì–´ë¦´ë•Œ ë³´ê³  ì§€ê¸ˆ ë‹¤ì‹œë´ë„ ì¬ë°Œì–´ìš”]","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"ì„ë² ë”©ì´ë€?","slug":"NLP_01","date":"2020-01-16T08:29:53.000Z","updated":"2020-02-03T14:23:01.815Z","comments":true,"path":"2020/01/16/NLP_01/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/16/NLP_01/","excerpt":"","text":"ì»´í“¨í„°ê°€ ë°”ë¼ë³´ëŠ” ë¬¸ì ì•„ë˜ì™€ ê°™ì´ ë¬¸ìëŠ” ì»´í“¨í„°ê°€ í•´ì„í•  ë•Œ ê·¸ëƒ¥ ê¸°í˜¸ì¼ ë¿ì´ë‹¤. ì´ë ‡ê²Œ encodingëœ ìƒíƒœë¡œ ë³´ê²Œ ë˜ë©´ ì•„ë˜ì™€ ê°™ì€ ë¬¸ì œì ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ì´ ê¸€ìê°€ ì–´ë–¤ ê¸€ìì¸ì§€ë¥¼ í‘œì‹œí•  ìˆ˜ ìˆê³  ê·¸ì— ë”°ë¥¸ íŠ¹ì„±ì„ ê°–ê²Œ í•˜ë ¤ë©´ ìš°ì„  ê³„ì‚°í•  ìˆ˜ ìˆê²Œ ìˆ«ìë¡œ ë§Œë“¤ì–´ ì£¼ì–´ì•¼ í•  ê²ƒì´ë‹¤. ê·¸ëŸ¬í•œ ë°©ë²• ì¤‘ ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì´ One-hot encodingì„ í†µí•œ ê²ƒì´ë‹¤. í—ˆë‚˜, ì´ëŸ¬í•œ Sparse matrixë¥¼ í†µí•œ ê³„ì‚°ì€ ë„ˆë¬´ ë¹„íš¨ìœ¨ ì ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ denseí•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆì„ì§€ë¥¼ ê³ ë¯¼í•˜ëŠ” ê²ƒì´ ë°”ë¡œ Embeddingì´ë¼ëŠ” ê°œë…ì˜ ë³¸ì§ˆì¼ ê²ƒì´ë‹¤. ì„ë² ë”©(Embedding)ì´ë€? ìì—°ì–´ ì²˜ë¦¬(Natural Language Processing)ë¶„ì•¼ì—ì„œ ì„ë² ë”©(Embedding)ì€ ì‚¬ëŒì´ ì“°ëŠ” ìì—°ì–´ë¥¼ ê¸°ê³„ê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ìí˜•íƒœì¸ vectorë¡œ ë°”ê¾¼ ê²°ê³¼ í˜¹ì€ ê·¸ ì¼ë ¨ì˜ ê³¼ì • ì „ì²´ë¥¼ ì˜ë¯¸í•œë‹¤. ê°€ì¥ ê°„ë‹¨í•œ í˜•íƒœì˜ ì„ë² ë”©ì€ ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ê·¸ëŒ€ë¡œ ë²¡í„°ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ë‹¨ì–´-ë¬¸ì„œ í–‰ë ¬(Term-Document Matrix)ëŠ” rowëŠ” ë‹¨ì–´ columnì€ ë¬¸ì„œì— ëŒ€ì‘í•œë‹¤. êµ¬ë¶„ ë©”ë°€ê½ƒ í•„ ë¬´ë µ ìš´ìˆ˜ ì¢‹ì€ ë‚  ì‚¬ë‘ ì†ë‹˜ê³¼ ì–´ë¨¸ë‹ˆ ì‚¼í¬ ê°€ëŠ” ê¸¸ ê¸°ì°¨ 0 2 10 7 ë§‰ê±¸ë¦¬ 0 1 0 0 ì„ ìˆ ì§‘ 0 1 0 0 ìœ„ì˜ í‘œì—ì„œ ìš´ìˆ˜ì¢‹ì€ ë‚ ì´ë¼ëŠ” ë¬¸ì„œì˜ ì„ë² ë”©ì€ [2, 1, 1]ì´ë‹¤. ë§‰ê±¸ë¦¬ë¼ëŠ” ë‹¨ì–´ì˜ ì„ë² ë”©ì€ [0, 1, 0, 0]ì´ë‹¤. ë˜í•œ ì‚¬ë‘ ì†ë‹˜ê³¼ ì–´ë¨¸ë‹ˆ, ì‚¼í¬ ê°€ëŠ” ê¸¸ì´ ì‚¬ìš©í•˜ëŠ” ë‹¨ì–´ ëª©ë¡ì´ ìƒëŒ€ì ìœ¼ë¡œ ë§ì´ ê²¹ì¹˜ê³  ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ìœ„ì˜ Matrixë¥¼ ë°”íƒ•ìœ¼ë¡œ ìš°ë¦¬ëŠ” ì‚¬ë‘ ì†ë‹˜ê³¼ ì–´ë¨¸ë‹ˆëŠ” ì‚¼í¬ ê°€ëŠ” ê¸¸ê³¼ ê¸°ì°¨ë¼ëŠ” ì†Œì¬ë¥¼ ê³µìœ í•œë‹¤ëŠ” ì ì—ì„œ ë¹„ìŠ·í•œ ì‘í’ˆì¼ ê²ƒì´ë¼ëŠ” ì¶”ì •ì„ í•´ë³¼ ìˆ˜ ìˆë‹¤. ë˜ ë§‰ê±¸ë¦¬ë¼ëŠ” ë‹¨ì–´ì™€ ì„ ìˆ ì§‘ì´ë¼ëŠ” ë‹¨ì–´ê°€ ìš´ìˆ˜ ì¢‹ì€ ë‚ ì´ë¼ëŠ” ì‘í’ˆì—ë§Œ ë“±ì¥í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë§‰ê±¸ë¦¬-ì„ ìˆ ì§‘ ê°„ ì˜ë¯¸ ì°¨ì´ê°€ ë§‰ê±¸ë¦¬ ê¸°ì°¨ ë³´ë‹¤ ì‘ì„ ê²ƒì´ë¼ê³  ì¶”ì •í•´ ë³¼ ìˆ˜ ìˆë‹¤. ì„ë² ë”©ì˜ ì—­í•  1) ë‹¨ì–´/ë¬¸ì¥ ê°„ ê´€ë ¨ë„ ê³„ì‚° ë‹¨ì–´-ë¬¸ì„œ í–‰ë ¬ì€ ê°€ì¥ ë‹¨ìˆœí•œ í˜•íƒœì˜ ì„ë² ë”©ì´ë‹¤. í˜„ì—…ì—ì„œëŠ” ì´ë³´ë‹¤ ë³µì¡í•œ í˜•íƒœì˜ ì„ë² ë”©ì„ ì‚¬ìš©í•œë‹¤. ëŒ€í‘œì ì¸ ì„ë² ë”© ê¸°ë²•ì€ Word2Vecì„ ë½‘ì„ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ì´ë ‡ë“¯ ì»´í“¨í„°ê°€ ê³„ì‚°í•˜ê¸° ì‰½ë„ë¡ ë‹¨ì–´ë¥¼ ì „ì²´ ë‹¨ì–´ë“¤ê°„ì˜ ê´€ê³„ì— ë§ì¶° í•´ë‹¹ ë‹¨ì–´ì˜ íŠ¹ì„±ì„ ê°–ëŠ” ë²¡í„°ë¡œ ë°”ê¾¸ë©´ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ì¼ì´ ê°€ëŠ¥í•´ì§„ë‹¤. ìì—°ì–´ì¼ ë•Œ ë¶ˆê°€ëŠ¥í–ˆë˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•  ìˆ˜ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì´ ì„ë² ë”© ë•ë¶„ì— ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ë˜í•œ ì„ë² ë”©ì„ ìˆ˜í–‰í•˜ë©´ ë²¡í„° ê³µê°„ì„ ê¸°í•˜í•™ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ì‹œê°í™” ì—­ì‹œ ê°€ëŠ¥í•˜ë‹¤. 2) ì˜ë¯¸ì /ë¬¸ë²•ì  ì •ë³´ í•¨ì¶• ì„ë² ë”©ì€ ë²¡í„°ì¸ ë§Œí¼ ì‚¬ì¹™ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ë‹¤. ë‹¨ì–´ ë²¡í„° ê°„ ë§ì…ˆ/ëº„ì…ˆì„ í†µí•´ ë‹¨ì–´ë“¤ ì‚¬ì´ì˜ ì˜ë¯¸ì , ë¬¸ë²•ì  ê´€ê³„ë¥¼ ë„ì¶œí•´ë‚¼ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ë“¤ë©´, ì•„ë“¤ - ë”¸ + ì†Œë…€ = ì†Œë…„ì´ ì„±ë¦½í•˜ë©´ ì„±ê³µì ì¸ ì„ë² ë”©ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì•„ë“¤ - ë”¸ ì‚¬ì´ì˜ ê´€ê³„ì™€ ì†Œë…„ - ì†Œë…€ ì‚¬ì´ì˜ ì˜ë¯¸ ì°¨ì´ê°€ ì„ë² ë”©ì— í•¨ì¶•ë¼ ìˆìœ¼ë©´ í’ˆì§ˆì´ ì¢‹ì€ ì„ë² ë”©ì´ë¼ ë§í•  ìˆ˜ ìˆë‹¤ëŠ” ì´ì•¼ê¸°ì´ë‹¤. ì´ë ‡ê²Œ ë‹¨ì–´ ì„ë² ë”©ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ë‹¨ì–´ ìœ ì¶” í‰ê°€(word analogy test)ë¼ê³  ë¶€ë¥¸ë‹¤. 3) ì „ì´í•™ìŠµ(Transfer learning) í’ˆì§ˆ ì¢‹ì€ ì„ë² ë”©ì€ ëª¨í˜•ì˜ ì„±ëŠ¥ê³¼ ëª¨í˜•ì˜ ìˆ˜ë ´ì†ë„ê°€ ë¹¨ë¼ì§€ëŠ”ë° ì´ëŸ° í’ˆì§ˆ ì¢‹ì€ ì„ë² ë”©ì„ ë‹¤ë¥¸ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ transfer learningì´ë¼ í•œë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ëŒ€ê·œëª¨ Corpusë¥¼ í™œìš©í•´ ì„ë² ë”©ì„ ë¯¸ë¦¬ ë§Œë“¤ì–´ ë†“ëŠ”ë‹¤. ì„ë² ë”©ì—ëŠ” ì˜ë¯¸ì , ë¬¸ë²•ì  ì •ë³´ ë“±ì´ ë…¹ì•„ ìˆë‹¤. ì´ ì„ë² ë”©ì„ ì…ë ¥ê°’ìœ¼ë¡œ ì“°ëŠ” ì „ì´ í•™ìŠµ ëª¨ë¸ì€ ë¬¸ì„œ ë¶„ë¥˜ë¼ëŠ” ì—…ë¬´ë¥¼ ë¹ ë¥´ê²Œ ì˜ í•  ìˆ˜ ìˆê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. ì„ë² ë”© ê¸°ë²•ì˜ ì—­ì‚¬ì™€ ì¢…ë¥˜ í†µê³„ ê¸°ë°˜ -&gt; ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ í†µê³„ ê¸°ë°˜ ê¸°ë²• ì ì¬ ì˜ë¯¸ ë¶„ì„(Latent Semantic Analysis) : ë‹¨ì–´ ì‚¬ìš© ë¹ˆë„ ë“± Corpusì˜ í†µê³„ëŸ‰ ì •ë³´ê°€ ë“¤ì–´ ìˆëŠ” í–‰ë ¬ì— íŠ¹ì´ê°’ ë¶„í•´ë“± ìˆ˜í•™ì  ê¸°ë²•ì„ ì ìš©í•´ í–‰ë ¬ì— ì†í•œ ë²¡í„°ë“¤ì˜ ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì°¨ì›ì„ ì¶•ì†Œí•˜ëŠ” ì´ìœ ëŠ” ì˜ˆë¥¼ ë“¤ì–´ Term-Document matrix ê°™ì€ ê²½ìš°ëŠ” rowê°€ ë” í° sparse matrixì¼ í™•ë¥ ì´ ë†’ê¸° ë•Œë¬¸ì— ì“¸ë° ì—†ì´ ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ìì›ì„ ë‚­ë¹„í•˜ëŠ” ê²ƒì„ ì˜ˆë°©í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. ì—¬ê¸°ì„œ ì°¨ì› ì¶•ì†Œë¥¼ í†µí•´ ì–»ì€ í–‰ë ¬ì„ ê¸°ì¡´ì˜ í–‰ë ¬ê³¼ ë¹„êµí–ˆì„ ë•Œ ë‹¨ì–´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í–ˆë‹¤ë©´ ë‹¨ì–´ ìˆ˜ì¤€ ì„ë² ë”©, ë¬¸ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ í–ˆë‹¤ë©´ ë¬¸ì„œ ì„ë² ë”©ì´ëœë‹¤. ì ì¬ ì˜ë¯¸ ë¶„ì„ ìˆ˜í–‰ ëŒ€ìƒ í–‰ë ¬ì€ ì—¬ëŸ¬ ì¢…ë¥˜ê°€ ë  ìˆ˜ ìˆìœ¼ë©°, Term-Document Matrix, TF-IDF Matrix, Word-Context Matrix, PMI Matrixë“±ì´ ìˆë‹¤. Neural Network ê¸°ë°˜ ê¸°ë²• Neural Probabilistic Language Modelì´ ë°œí‘œëœ ì´í›„ ë¶€í„° Neural Networkê¸°ë°˜ì˜ ì„ë² ë”© ê¸°ë²•ë“¤ì´ ì£¼ëª© ë°›ê³  ìˆë‹¤. Neural NetworkëŠ” êµ¬ì¡°ê°€ ìœ ì—°í•˜ê³  í‘œí˜„ë ¥ì´ í’ë¶€í•˜ê¸° ë•Œë¬¸ì— ìì—°ì–´ì˜ ë¬´í•œí•œ ë¬¸ë§¥ì„ ìƒë‹¹ ë¶€ë¶„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. ë‹¨ì–´ ìˆ˜ì¤€ -&gt; ë¬¸ì¥ ìˆ˜ì¤€ ë‹¨ì–´ ìˆ˜ì¤€ ì„ë² ë”© ê¸°ë²• : ê°ê°ì˜ ë²¡í„°ì— í•´ë‹¹ ë‹¨ì–´ì˜ ë¬¸ë§¥ì  ì˜ë¯¸ë¥¼ í•¨ì¶•í•˜ì§€ë§Œ, ë‹¨ì–´ì˜ í˜•íƒœê°€ ë™ì¼í•˜ë‹¤ë©´ ë™ì¼ë‹¨ì–´ë¡œ ì¸ì‹í•˜ê³ , ëª¨ë“  ë¬¸ë§¥ ì •ë³´ë¥¼ í•´ë‹¹ ë‹¨ì–´ ë²¡í„° íˆ¬ì˜í•˜ë¯€ë¡œ ë™ìŒì´ì˜ì–´ë¥¼ ë¶„ê°„í•˜ê¸° ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ex) NPLM, Word2Vec, GloVe, FastText, Swivel ë“± ë¬¸ì¥ ìˆ˜ì¤€ ì„ë² ë”© ê¸°ë²• : 2018ë…„ ì´ˆì— ELMo(Embedding from Language Models)ê°€ ë°œí‘œëœ ì´í›„ ì£¼ëª© ë°›ê¸° ì‹œì‘í–ˆë‹¤. ê°œë³„ ë‹¨ì–´ê°€ ì•„ë‹Œ ë‹¨ì–´ Sequence ì „ì²´ì˜ ë¬¸ë§¥ì  ì˜ë¯¸ë¥¼ í•¨ì¶• í•˜ê¸° ë•Œë¬¸ì— ë‹¨ì–´ ì„ë² ë”© ê¸°ë²•ë³´ë‹¤ Transfer learning íš¨ê³¼ê°€ ì¢‹ì€ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤. ë˜í•œ, ë‹¨ì–´ ìˆ˜ì¤€ ì„ë² ë”©ì˜ ë‹¨ì ì¸ ë™ìŒì´ì˜ì–´ë„ ë¬¸ì¥ìˆ˜ì¤€ ì„ë² ë”© ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©´ ë¶„ë¦¬í•´ì„œ ì´í•´í•  ìˆ˜ ìˆë‹¤. ex) BERT(Bidirectional Encoder Representations from Transformer), GPT(Generation Pre-Training) ë“± Rule based -&gt; End to End -&gt; Pre-training/fine tuning 1990ë…„ëŒ€ì—ëŠ” ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ ëŒ€ë¶€ë¶„ì€ ìš°ë¦¬ê°€ ë”¥ëŸ¬ë‹ê³¼ ë‹¬ë¦¬ ë¨¸ì‹ ëŸ¬ë‹ì²˜ëŸ¼ ì‚¬ëŒì´ Featureë¥¼ ì§ì ‘ ë½‘ì•˜ë‹¤. ê·¸ë ‡ê¸°ì— Featureë¥¼ ì¶”ì¶œí•  ë•Œ ì–¸ì–´í•™ì ì¸ ì§€ì‹ì„ í™œìš©í•´ì•¼ í–ˆë‹¤. í—ˆë‚˜. 2000ë…„ëŒ€ ì¤‘ë°˜ ì´í›„ NLP ë¶„ì•¼ì—ì„œë„ ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì£¼ëª©ë°›ê¸° ì‹œì‘í•˜ì—¬ Featureë¥¼ ì§ì ‘ ë½‘ì§€ ì•Šì•„ë„ ë˜ì—ˆë‹¤. ë°ì´í„°ë¥¼ ë„£ì–´ì£¼ë©´ ì‚¬ëŒì˜ ê°œì…ì—†ì´ ëª¨ë¸ ìŠ¤ìŠ¤ë¡œ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì´í•´í•˜ëŠ” End-to-End Model ê¸°ë²•ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. ëŒ€í‘œì ìœ¼ë¡œëŠ” ê¸°ê³„ë²ˆì—­ì— ë„ë¦¬ ì‚¬ìš©ëë˜ Sequence-to-Sequence ëª¨ë¸ì´ ìˆë‹¤. 2018ë…„ ELMo ëª¨ë¸ì´ ì œì•ˆëœ ì´í›„ NLP ëª¨ë¸ì€ pre-trainingê³¼ fine tuning ë°©ì‹ìœ¼ë¡œ ë°œì „í•˜ê³  ìˆë‹¤. ìš°ì„  ëŒ€ê·œëª¨ Corpusë¡œ ì„ë² ë”©ì„ ë§Œë“ ë‹¤.(Pre-train) ì´ ì„ë² ë”©ì—ëŠ” Corpusì˜ ì˜ë¯¸ì , ë¬¸ë²•ì  ë§¥ë½ì´ í¬í•¨ë¼ ìˆë‹¤. ì´í›„ ì„ë² ë”©ì„ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ìƒˆë¡œìš´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“œë¡œ ìš°ë¦¬ê°€ í’€ê³  ì‹¶ì€ êµ¬ì²´ì  ë¬¸ì œì— ë§ëŠ” ì†Œê·œëª¨ ë°ì´í„°ì— ë§ê²Œ ì„ë² ë”©ì„ í¬í•¨í•œ ëª¨ë¸ ì „ì²´ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤.(fine tuning) ELMo, GPT, BERTë“±ì´ ì´ ë°©ì‹ì— í•´ë‹¹ëœë‹¤. ìš°ë¦¬ê°€ í’€ê³  ì‹¶ì€ ìì—°ì–´ ì²˜ë¦¬ì˜ êµ¬ì²´ì  ë¬¸ì œë“¤(ì˜ˆì‹œ : í’ˆì‚¬ íŒë³„(Part-Of-Speech tagging), ê°œì²´ëª… ì¸ì‹(Named Entity Recognition), ì˜ë¯¸ì—­ ë¶„ì„(Semantic Role Labeling))ì„ ë‹¤ìš´ ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬(DownStream task)ë¼ê³  í•œë‹¤. ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ì— ì•ì„œ í•´ê²°í•´ì•¼ í•  ê³¼ì œë¼ëŠ” ëœ»ì˜ ì—…ìŠ¤íŠ¸ë¦¼ í…ŒìŠ¤í¬(UpStream task)ëŠ” ë‹¨ì–´/ë¬¸ì¥ ì„ë² ë”©ì„ Pre-trainí•˜ëŠ” ì‘ì—…ì´ í•´ë‹¹ëœë‹¤. ì„ë² ë”©ì˜ ì¢…ë¥˜ì™€ ì„±ëŠ¥1) í–‰ë ¬ ë¶„í•´ Corpus ì •ë³´ê°€ ë“¤ì–´ ìˆëŠ” ì›ë˜ í–‰ë ¬ì„ Decompositionì„ í†µí•´ ì„ë² ë”©í•˜ëŠ” ê¸°ë²•ì´ë‹¤. Decomposition ì´í›„ì—” ë‘˜ ì¤‘ í•˜ë‚˜ì˜ í–‰ë ¬ë§Œ ì‚¬ìš©í•˜ê±°ë‚˜ ë‘˜ì„ sumí•˜ê±°ë‚˜ concatenateí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„ë² ë”©ì„ í•œë‹¤. ex) GloVe, Swivel ë“± 2) ì˜ˆì¸¡ ê¸°ë°˜ ì–´ë–¤ ë‹¨ì–´ ì£¼ë³€ì— íŠ¹ì • ë‹¨ì–´ê°€ ë‚˜íƒ€ë‚ ì§€ ì˜ˆì¸¡í•˜ê±°ë‚˜, ì´ì „ ë‹¨ì–´ë“¤ì´ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¤ìŒ ë‹¨ì–´ê°€ ë¬´ì—‡ì¼ì§€ ì˜ˆì¸¡í•˜ê±°ë‚˜, ë¬¸ì¥ ë‚´ ì¼ë¶€ ë‹¨ì–´ë¥¼ ì§€ìš°ê³  í•´ë‹¹ ë‹¨ì–´ê°€ ë¬´ì—‡ì¼ì§€ ë§ì¶”ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµí•˜ëŠ” ë°©ë²• Neural Networkê¸°ë°˜ ë°©ë²•ë“¤ì´ ì†í•œë‹¤. ex) Word2Vec, FastText, BERT, ELMo, GPT ë“± 3) í† í”½ ê¸°ë°˜ ì£¼ì–´ì§„ ë¬¸ì„œì— ì ì¬ëœ ì£¼ì œë¥¼ ì¶”ë¡ í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„ë² ë”©ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ë²•ì´ë©°, ëŒ€í‘œì ìœ¼ë¡œ ì ì¬ ë””ë¦¬í´ë ˆ í• ë‹¹(LDA)ê°€ ìˆë‹¤. LDA ê°™ì€ ëª¨ë¸ì€ í•™ìŠµì´ ì™„ë£Œë˜ë©´ ê° ë¬¸ì„œê°€ ì–´ë–¤ ì£¼ì œ ë¶„í¬ë¥¼ ê°–ëŠ”ì§€ í™•ë¥  ë²¡í„° í˜•íƒœë¡œ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— ì„ë² ë”© ê¸°ë²•ì˜ ì¼ì¢…ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤. NLP ìš©ì–´ ì •ë¦¬Corpus(ë§ë­‰ì¹˜) ì„ë² ë”© í•™ìŠµì´ë¼ëŠ” íŠ¹ì •í•œ ëª©ì ì„ ê°€ì§€ê³  ìˆ˜ì§‘í•œ í‘œë³¸ì´ë‹¤. Collection(ì»¬ë ‰ì…˜) Corpusì— ì†í•œ ê°ê°€ì˜ ì§‘í•©ì„ ì¹­í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•œêµ­ì–´ ìœ„í‚¤ë°±ê³¼ì™€ ë„¤ì´ë²„ ì˜í™” ë¦¬ë·°ë¥¼ ë§ë­‰ì¹˜ë¡œ ì“´ë‹¤ë©´ ì´ë“¤ ê°ê°ì´ ì»¬ë ‰ì…˜ì´ ëœë‹¤. Sentence(ë¬¸ì¥) ìƒê°ì´ë‚˜ ê°ì •ì„ ë§ê³¼ ê¸€ë¡œ í‘œí˜„í•  ë•Œ ì™„ê²°ëœ ë‚´ìš©ì„ ë‚˜íƒ€ë‚´ëŠ” ìµœì†Œì˜ ë…ë¦½ì ì¸ í˜•ì‹ ë‹¨ìœ„ë¥¼ ê°€ë¦¬í‚¨ë‹¤. ì‹¤ë¬´ì—ì„œëŠ” ì£¼ë¡œ ë¬¸ì¥ì„ ë§ˆì¹¨í‘œ(.)ë‚˜ ëŠë‚Œí‘œ(!), ë¬¼ìŒí‘œ(?)ì™€ ê°™ì€ ê¸°í˜¸ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ë¬¸ì¥ìœ¼ë¡œ ì·¨ê¸‰í•œë‹¤. Document(ë¬¸ì„œ) ìƒê°ì´ë‚˜ ê°ì •, ì •ë³´ë¥¼ ê³µìœ í•˜ëŠ” ë¬¸ì¥ ì§‘í•©ì„ ì˜ë¯¸í•œë‹¤. ë¬¸ì„œëŠ” ë‹¨ë½(Paragraph)ì˜ ì§‘í•©ìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤. ë³„ë„ì˜ ê¸°ì¤€ì´ ì—†ë‹¤ë©´ ì¤„ë°”ê¿ˆ(\\n) ë¬¸ìë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ì„ ë¬¸ì„œë¡œ ì·¨ê¸‰í•œë‹¤. Token(í† í°) ë¬¸ì¥ì€ ì—¬ëŸ¬ê°œì˜ í† í°ìœ¼ë¡œ êµ¬ì„±ëœë‹¤. í† í°ì€ ë‹¨ì–´(Word), í˜•íƒœì†Œ(Morpheme), ì„œë¸Œì›Œë“œ(subword)ë¼ê³ ë„ í•œë‹¤. ë¬¸ì¥ì„ í† í° ì‹œí€€ìŠ¤ë¡œ ë¶„ì„í•˜ëŠ” ê³¼ì •ì„ í† í¬ë‚˜ì´ì¦ˆ(tokenize)ë¼ê³  í•œë‹¤. Vocabulary(ì–´íœ˜ì§‘í•©) Corpusì— ìˆëŠ” ëª¨ë“  Documentë¥¼ Sentenceë¡œ ë‚˜ëˆ„ê³  ì—¬ê¸°ì— Tokenizeë¥¼ ì‹¤í–‰í•œ í›„ ì¤‘ë³µì„ ì œê±°í•œ Tokenë“¤ì˜ ì§‘í•©ì´ë‹¤. Vocabularyì— ì—†ëŠ” tokenì€ ë¯¸ë“±ë¡ ë‹¨ì–´(Unknown word)ë¼ê³  í•œë‹¤. ë²¡í„°ê°€ ì–´ë–»ê²Œ ì˜ë¯¸ë¥¼ ê°€ì§€ê²Œ ë˜ëŠ”ê°€ ìì—°ì–´ì˜ ì˜ë¯¸ë¥¼ ì„ë² ë”©ì— ë…¹ì—¬ë‚´ëŠ” ë°©ë²•ì€ ìì—°ì–´ì˜ í†µê³„ì  íŒ¨í„´ ì •ë³´ë¥¼ í†µì§¸ë¡œ ì„ë² ë”©ì— ë„£ëŠ” ê²ƒì´ë‹¤. ìì—°ì–´ì˜ ì˜ë¯¸(ë¬¸ë²•ì  ì˜ë¯¸, ë‹¨ì–´ì˜ ì˜ë¯¸ë“±)ëŠ” ê·¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ì‚¬ëŒë“¤ì˜ ì¼ìƒ ì–¸ì–´ì— ì •ë³´ê°€ ë“¤ì–´ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì„ë² ë”©ì„ ë§Œë“¤ ë•Œ ì‚¬ìš©í•˜ëŠ” í†µê³„ ì •ë³´ëŠ” í¬ê²Œ 3ê°€ì§€ê°€ ìˆë‹¤. 1) ë¬¸ì¥ì— ì–´ë–¤ ë‹¨ì–´ê°€ ë§ì´ ì“°ì˜€ëŠ”ì§€ -&gt; bag of words(ë°±ì˜¤ë¸Œì›Œì¦ˆ) ê°€ì • 2) ë‹¨ì–´ê°€ ì–´ë–¤ ìˆœì„œë¡œ ë“±ì¥í•˜ëŠ”ì§€ -&gt; Language model(ì–¸ì–´ ëª¨ë¸) ê°€ì • 3) ë¬¸ì¥ì— ì–´ë–¤ ë‹¨ì–´ê°€ ê°™ì´ ë‚˜íƒ€ë‚¬ëŠ”ì§€ -&gt; distribution hypothesis(ë¶„í¬ê°€ì •) 1) BOW(Bag-Of-Words) ê°€ì • ë¬¸ì„œì˜ ì €ìê°€ ìƒê°í•œ ì£¼ì œê°€ ë¬¸ì„œì—ì„œì˜ ë‹¨ì–´ ì‚¬ìš©ì— ë…¹ì•„ìˆë‹¤ëŠ” ìƒê°ìœ¼ë¡œë¶€í„° ë‹¨ì–´ì˜ ìˆœì„œ ì •ë³´ëŠ” ë¬´ì‹œí•˜ê³  ì–´ë–¤ ë‹¨ì–´ê°€ ë§ì´ ì“°ì˜€ëŠ”ì§€ ì •ë³´ë¥¼ ì¤‘ì‹œí•œë‹¤. ê²½ìš°ì— ë”°ë¼ì„œëŠ” ë¹ˆë„ ì—­ì‹œ ë‹¨ìˆœí™”í•´ ë“±ì¥ ì—¬ë¶€(ë“±ì¥ ì‹œ 1, ì•„ë‹ˆë©´ 0)ë§Œì„ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤. ê°„ë‹¨í•œ ì•„ì´ë””ì–´ì§€ë§Œ ì •ë³´ ê²€ìƒ‰(information Retrieval)ë¶„ì•¼ì—ì„œ ì—¬ì „íˆ ë§ì´ ì“°ì´ê³  ìˆë‹¤. ì‚¬ìš©ì ì§ˆì˜ì— ê°€ì¥ ì ì ˆí•œ ë¬¸ì„œë¥¼ ë³´ì—¬ì¤„ ë•Œ ì§ˆì˜ë¥¼ BOW ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì§ˆì˜ì™€ ê²€ìƒ‰ ëŒ€ìƒ ë¬¸ì„œ ì„ë² ë”© ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ êµ¬í•´ ê°€ì¥ ë†’ì€ ë¬¸ì„œë¥¼ ì‚¬ìš©ìì—ê²Œ ë…¸ì¶œ í•œë‹¤. ëŒ€í‘œ í†µê³„ëŸ‰ : TF-IDFê°œë…ì„ ëª¨ë¥¸ë‹¤ë©´ í´ë¦­ ëŒ€í‘œ ëª¨ë¸ : Deep Averaging Network ë‹¨ì–´ì˜ ìˆœì„œë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ë‹¨ì–´ì˜ ì„ë² ë”©ì„ í‰ê· ì„ ì·¨í•´ ë§Œë“ ë‹¤. ê°„ë‹¨í•œ êµ¬ì¡°ì„ì—ë„ ì„±ëŠ¥ì´ ì¢‹ì•„ì„œ í˜„ì—…ì—ì„œë„ ìì£¼ ì“°ì¸ë‹¤. 2) Language model ê°€ì •- `ì‹œí€€ìŠ¤ì— í™•ë¥ ì„ ë¶€ì—¬í•˜ì—¬ ë‹¨ì–´ ì‹œí€€ìŠ¤ë¥¼ ëª…ì‹œì (ìˆœì„œë¥¼ ê³ ë ¤)ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ëª¨ë¸` 2-1) í†µê³„ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ ë‹¨ì–´ê°€ nê°œ ì£¼ì–´ì§„ ìƒí™©ì´ë¼ë©´ Language modelì€ nê°œ ë‹¨ì–´ê°€ ë™ì‹œì— ë‚˜íƒ€ë‚  í™•ë¥ ì„ ë°˜í™˜í•œë‹¤. í†µê³„ ê¸°ë°˜ì˜ ì–¸ì–´ ëª¨ë¸ì€ ë§ë­‰ì¹˜ì—ì„œ í•´ë‹¹ ë‹¨ì–´ ì‹œí€€ìŠ¤ê°€ ì–¼ë§ˆë‚˜ ìì£¼ ë“±ì¥í•˜ëŠ”ì§€ ë¹ˆë„ë¥¼ ì„¸ì–´ í•™ìŠµí•œë‹¤. ì˜ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì´ ìˆë‹¤ë©´ ì£¼ì–´ì§„ ë‹¨ì–´ ì‹œí€€ìŠ¤ ë‹¤ìŒ ë‹¨ì–´ë¡œ í™•ë¥ ì´ ë†’ì€ ìì—°ìŠ¤ëŸ¬ìš´ ë‹¨ì–´ë¥¼ ì„ íƒí•  ê²ƒì´ë‹¤. êµ¬ì²´ì ì¸ ë°©ë²•ì€ í•œ ìƒíƒœì˜ í™•ë¥ ì€ ê·¸ ì§ì „ ìƒíƒœì—ë§Œ ì˜ì¡´í•œë‹¤ëŠ” Markov assumptionì— ê¸°ë°˜í•˜ì—¬ n-gramì„ í†µí•´ í™•ë¥ ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. í—ˆë‚˜ ë°ì´í„°ì— í•œ ë²ˆë„ ë“±ì¥í•˜ì§€ ì•ŠëŠ” n-gramì´ ì¡´ì¬í•  ë•Œ ì˜ˆì¸¡ ë‹¨ê³„ì—ì„œëŠ” í™•ë¥ ê°’ì„ 0ìœ¼ë¡œ ì·¨í•˜ëŠ” ë¬¸ì œê°€ ìˆë‹¤. P(w_{n}|w_{n-1} = \\frac{w_{n-1}}{w_{n}}) ìœ„ì˜ ë¬¸ì œì ë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ Back-off, Smoothing ë“±ì˜ ë°©ì‹ì´ ì œì•ˆëë‹¤. 1) Back-off n-gram ë“±ì¥ ë¹ˆë„ê°€ 0ì¸ ë‹¨ì–´ë“¤ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ n-gram ë“±ì¥ë¹ˆë„ë¥¼ në³´ë‹¤ ì‘ì€ ë²”ìœ„ì˜ ë‹¨ì–´ ì‹œí€€ìŠ¤ ë¹ˆë„ë¡œ ê·¼ì‚¬í•˜ëŠ” ë°©ì‹ì´ë‹¤. nì„ í¬ê²Œ í•˜ë©´ í• ìˆ˜ë¡ ë“±ì¥í•˜ì§€ ì•ŠëŠ” ì¼€ì´ìŠ¤ê°€ ë§ì•„ì§ˆ ê°€ëŠ¥ì„±ì´ ë†’ê¸° ë•Œë¬¸ì´ë‹¤. $\\alpha, \\beta$ëŠ” ì‹¤ì œ ë¹ˆë„ì™€ì˜ ì°¨ì´ë¥¼ ë³´ì •í•´ì£¼ëŠ” parameterì´ë‹¤. Freq(ë‚´ ë§ˆìŒ ì†ì— ì˜ì›íˆ ê¸°ì–µë  ìµœê³ ì˜ ëª…ì‘ì´ë‹¤) \\approx \\alpha Freq(ì˜ì›íˆ ê¸°ì–µë  ìµœê³ ì˜ ëª…ì‘ì´ë‹¤) + \\beta 2) (Add-k) Smoothing ë“±ì¥ ë¹ˆë„ í‘œì— ëª¨ë‘ k ë§Œí¼ ë”í•˜ëŠ” ê¸°ë²•ì´ë‹¤. ë§Œì•½ k=1ë¡œ ì„¤ì •í•œë‹¤ë©´ íŠ¹ë³„íˆ ë¼í”Œë¼ìŠ¤ ìŠ¤ë¬´ë”©(laplace smoothing)ì´ë¼ê³  í•œë‹¤. ìŠ¤ë¬´ë”©ì„ ì‹œí–‰í•˜ë©´ ë†’ì€ ë¹ˆë„ë¥¼ ê°€ì§„ ë¬¸ìì—´ ë“±ì¥ í™•ë¥ ì„ ì¼ë¶€ ê¹ê³  ì „í˜€ ë“±ì¥í•˜ì§€ ì•ŠëŠ” ì¼€ì´ìŠ¤ë“¤ì—ëŠ” ì•½ê°„ì˜ í™•ë¥ ì„ ë¶€ì—¬í•˜ê²Œ ëœë‹¤. 2-2) ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ê¸°ë°˜ ì–¸ì–´ ëª¨ë¸ Neural NetworkëŠ” ì…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ìœ ì—°í•˜ê²Œ í¬ì°©í•´ë‚¼ ìˆ˜ ìˆê³ , ê·¸ ìì²´ë¡œ í™•ë¥  ëª¨ë¸ë¡œ ê¸°ëŠ¥í•  ìˆ˜ ìˆë‹¤. ì£¼ì–´ì§„ ë‹¨ì–´ ì‹œí€€ìŠ¤ë¥¼ ê°€ì§€ê³  ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤. í•™ìŠµì´ ì™„ë£Œë˜ë©´ ì´ë“¤ ëª¨ë¸ì˜ ì¤‘ê°„ í˜¹ì€ ë§ë‹¨ ê³„ì‚° ê²°ê³¼ë¬¼ì„ ë‹¨ì–´ë‚˜ ë¬¸ìì˜ ì„ë² ë”©ìœ¼ë¡œ í™œìš©í•œë‹¤. Language model ê¸°ë°˜ ê¸°ë²•ì€ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ë°›ì•„ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ë§ì¶°ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì¼ë°©í–¥(uni-directional)ì´ì§€ë§Œ Masked language modelì€ ë¬¸ì¥ ì „ì²´ë¥¼ ë‹¤ ë³´ê³  ì¤‘ê°„ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸ì— ì–‘ë°©í–¥(bi-directional)í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤. ê·¸ë¡œì¸í•´ Masked Language model ê¸°ë°˜ì˜ ë°©ë²•ë“¤(ì˜ˆ:BERT)ì€ ê¸°ì¡´ Language model ê¸°ë²•ë“¤ ëŒ€ë¹„ ì„ë² ë”© í’ˆì§ˆì´ ì¢‹ë‹¤. ëŒ€í‘œ ëª¨ë¸ : ELMo, GPT ë“± 3) Distribution hypothesis ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ë¶„í¬ë€ íŠ¹ì • ë²”ìœ„, ì¦‰ Window(í•´ë‹¹ ë‹¨ì–´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë²”ìœ„ì— í¬í•¨ì‹œí‚¬ ì•ë’¤ ë‹¨ì–´ ìˆ˜, ì˜ˆë¥¼ ë“¤ì–´ ìœˆë„ìš°ê°€ 2ë¼ë©´ íƒ€ê¹ƒ ë‹¨ì–´ ì•ë’¤ë¡œ 2ê°œì˜ ë¬¸ë§¥ë‹¨ì–´ì˜ ë¹ˆë„ë¥¼ ê³„ì‚°) ë‚´ì— ë™ì‹œì— ë“±ì¥í•˜ëŠ” ì´ì›ƒ ë‹¨ì–´ ë˜ëŠ” ë¬¸ë§¥(context)ì˜ ì§‘í•©ì„ ê°€ë¦¬í‚¨ë‹¤. ì–´ë–¤ ë‹¨ì–´ ìŒì´ ë¹„ìŠ·í•œ ë¬¸ë§¥ í™˜ê²½ì—ì„œ ìì£¼ ë“±ì¥í•œë‹¤ë©´ ê·¸ ì˜ë¯¸ ë˜í•œ ìœ ì‚¬í•  ê²ƒì´ë¼ëŠ” ê²ƒì´ Distribution hypothesisì˜ ì „ì œì´ë‹¤. í˜•íƒœì†Œì˜ ê²½ê³„ë¥¼ ì •í•˜ê±°ë‚˜ í’ˆì‚¬ë¥¼ ë‚˜ëˆ„ëŠ” ê²ƒê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì–¸ì–´í•™ì  ë¬¸ì œëŠ” ë§ë­‰ì¹˜ì˜ ë¶„í¬ ì •ë³´ì™€ ê¹Šì€ ê´€ê³„ë¥¼ ê°–ê³  ìˆë‹¤. ì´ ë•ë¶„ì— ì„ë² ë”©ì— ë¶„í¬ ì •ë³´ë¥¼ í•¨ì¶•í•˜ê²Œ ë˜ë©´ í•´ë‹¹ ë²¡í„°ì— í•´ë‹¹ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë‚´ì œì‹œí‚¬ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. ëŒ€í‘œ í†µê³„ëŸ‰ : PMI(Pointwise Mutual Information : ì ë³„ ìƒí˜¸ ì •ë³´ëŸ‰) ë‘ ë‹¨ì–´ì˜ ë“±ì¥ì´ ë…ë¦½ì¼ ë•Œ ëŒ€ë¹„í•´ ì–¼ë§ˆë‚˜ ìì£¼ ê°™ì´ ë“±ì¥í•˜ëŠ”ì§€ë¥¼ ìˆ˜ì¹˜í™”í•œ ê²ƒ PMI(A, B) = log\\frac{P(A,B)}{P(A)P(B)} Term-context matrixëŠ” íŠ¹ì • ë‹¨ì–´ ê¸°ì¤€ìœ¼ë¡œ Windowì— ì¡´ì¬í•˜ëŠ” ë‹¨ì–´ë“¤ì„ countí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì§€ëŠ”ë°, ì—¬ê¸°ì—ì„œ PMI ìˆ˜ì‹ì„ ì ìš©ì‹œí‚¤ë©´ëœë‹¤. ì´ë ‡ê²Œ êµ¬ì¶•í•œ PMI í–‰ë ¬ì˜ í–‰ ë²¡í„° ìì²´ë¥¼ í•´ë‹¹ ë‹¨ì–´ì˜ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. ëŒ€í‘œ ëª¨ë¸ : Word2Vec CBOW ëª¨ë¸ ë¬¸ë§¥ ë‹¨ì–´ë“¤ì„ ê°€ì§€ê³  íƒ€ê¹ƒ ë‹¨ì–´ í•˜ë‚˜ë¥¼ ë§ì¶”ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤. 1) ê° ì£¼ë³€ ë‹¨ì–´ë“¤ì„ one-hot ë²¡í„°ë¡œ ë§Œë“¤ì–´ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš© (ì…ë ¥ì¸µ ë²¡í„°) 2) ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê° one-hot ë²¡í„°ì— ê³±í•´ì„œ n-ì°¨ì› ë²¡í„°ë¥¼ ë§Œë“ ë‹¤. (N-ì°¨ì› ì€ë‹‰ì¸µ) 3) ë§Œë“¤ì–´ì§„ n-ì°¨ì› ë²¡í„°ë¥¼ ëª¨ë‘ ë”í•œ í›„ ê°œìˆ˜ë¡œ ë‚˜ëˆ  í‰ê·  n-ì°¨ì› ë²¡í„°ë¥¼ ë§Œë“ ë‹¤. (ì¶œë ¥ì¸µ ë²¡í„°) 4) n-ì°¨ì› ë²¡í„°ì— ë‹¤ì‹œ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•´ì„œ one-hot ë²¡í„°ì™€ ê°™ì€ ì°¨ì›ì˜ ë²¡í„°ë¡œ ë§Œë“ ë‹¤. 5) ë§Œë“¤ì–´ì§„ ë²¡í„°ë¥¼ ì‹¤ì œ ì˜ˆì¸¡í•˜ë ¤ê³  í•˜ëŠ” ë‹¨ì–´ì˜ one-hot ë²¡í„°ì™€ ë¹„êµí•´ì„œ í•™ìŠµí•œë‹¤. Skip-gram ëª¨ë¸ íƒ€ê¹ƒ ë‹¨ì–´ë¥¼ ê°€ì§€ê³  ë¬¸ë§¥ ë‹¨ì–´ê°€ ë¬´ì—‡ì¼ì§€ ì˜ˆì¸¡í•˜ëŠ” ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤. 1) í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼ one-hot ë²¡í„°ë¡œ ë§Œë“¤ì–´ì„œ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.(ì…ë ¥ì¸µ ë²¡í„°) 2) ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ one-hot ë²¡í„°ì— ê³±í•´ì„œ n-ì°¨ì› ë²¡í„°ë¥¼ ë§Œë“ ë‹¤.(N-ì°¨ì› ì€ë‹‰ì¸µ) 3) n-ì°¨ì› ë²¡í„°ì— ë‹¤ì‹œ ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ ê³±í•´ì„œ one-hot ë²¡í„°ì™€ ê°™ì€ ì°¨ì›ì˜ ë²¡í„°ë¡œ ë§Œë“ ë‹¤.(ì¶œë ¥ì¸µ ë²¡í„°) 4) ë§Œë“¤ì–´ì§„ ë²¡í„°ë¥¼ ì‹¤ì œ ì˜ˆì¸¡í•˜ë ¤ëŠ” ì£¼ë³€ ë‹¨ì–´ë“¤ ê°ê°ì˜ one-hot ë²¡í„°ì™€ ë¹„êµí•´ì„œ í•™ìŠµí•œë‹¤. ë‘ ëª¨ë¸ì˜ í™•ì‹¤í•œ ì°¨ì´ì ì€ CBOWì—ì„œëŠ” ì…ë ¥ê°’ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ê³ , í•™ìŠµì„ ìœ„í•´ í•˜ë‚˜ì˜ ë‹¨ì–´ì™€ ë¹„êµí•˜ì§€ë§Œ, Skip-gramì—ì„œëŠ” ì…ë ¥ê°’ì´ í•˜ë‚˜ì˜ ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ê³ , í•™ìŠµì„ ìœ„í•´ ì£¼ë³€ì˜ ì—¬ëŸ¬ ë‹¨ì–´ì™€ ë¹„êµí•œë‹¤. ìœ„ì˜ í•™ìŠµ ê³¼ì •ì„ ëª¨ë‘ ëë‚¸ í›„ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ ê° í–‰ì„ ë‹¨ì–´ ë²¡í„°ë¡œ ì‚¬ìš©í•œë‹¤. ì¹´ìš´íŠ¸ ê¸°ë°˜ ë°©ë²•(Bag of Words ê°€ì • ë°©ë²•ë“¤)ë¡œ ë§Œë“  ë‹¨ì–´ ë²¡í„°ë³´ë‹¤ ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì˜ ì¸¡ì •í•˜ë©°, ë‹¨ì–´ë“¤ì˜ ë³µì¡í•œ íŠ¹ì§•ê¹Œì§€ë„ ì˜ ì¡ì•„ë‚¸ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. ë³´í†µ CBOWë³´ë‹¤ Skip-gramì˜ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ ìì£¼ ì‚¬ìš©ëœë‹¤. í•˜ì§€ë§Œ ë¬´ì¡°ê±´ì ìœ¼ë¡œ ì¢‹ì€ ê²ƒì€ ì•„ë‹ˆë‹¤!","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"Regression(03) - íšŒê·€ì§„ë‹¨","slug":"machine_learning_04","date":"2020-01-15T09:24:28.000Z","updated":"2020-04-14T05:59:46.236Z","comments":true,"path":"2020/01/15/machine_learning_04/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/15/machine_learning_04/","excerpt":"","text":"êµí˜¸ì‘ìš© ì„±ë³„, ê²°í˜¼ì—¬ë¶€, í˜¹ì€ ì†Œì† ì •ì¹˜ë‹¨ì²´ ë“±ê³¼ ê°™ì€ ì§ˆì (qualitative) ë˜ëŠ” ë²”ì£¼í˜•(categorical)ìš”ì¸ë“¤ì´ íšŒê·€ë¶„ì„ì—ì„œ ì¢…ì†(ë°˜ì‘)ë³€ìˆ˜ì˜ ë³€í™”ë¥¼ ì„¤ëª…í•˜ëŠ” ë° ë§¤ìš° ìœ ìš©í•œ ë…ë¦½(ì„¤ëª…) ë³€ìˆ˜ ì—­í• ì„ í•  ë•Œê°€ ìˆë‹¤. ì´ëŸ° ì§ˆì  ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë¡œ ì´ìš©í•  ê²½ìš° ì´ë“¤ì€ ì§€ì‹œë³€ìˆ˜(Indicator variable) ë˜ëŠ” ê°€ë³€ìˆ˜(dummy variable)ì˜ í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•´ì•¼í•œë‹¤. ê°€ë³€ìˆ˜ëŠ” ë‹¤ì–‘í•œ ìš©ë„ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, íšŒê·€ê´€ê³„ì— ì˜í–¥ì„ ì£¼ëŠ” ì§ˆì  ìš”ì¸ì„ ê³ ë ¤í•  ë•Œë§ˆë‹¤ í•­ìƒ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. -ì—¬ëŸ¬ ë²”ì£¼ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•˜ì—¬ ê°€ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²½ìš° í•„ìš”í•œ ê°€ë³€ìˆ˜ì˜ ê°œìˆ˜ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê°€ëŠ¥í•œ ë²”ì£¼ì˜ ìˆ˜ë³´ë‹¤ í•˜ë‚˜ ì‘ê²Œ ì¡ìœ¼ë©´ ëœë‹¤.ì™œëƒí•˜ë©´, ê°€ë³€ìˆ˜ë¥¼ ì¢…í•©í•˜ë©´ êµìœ¡ìˆ˜ì¤€ì— ê´€í•œ 3ê°œì˜ ë²”ì£¼ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ê²Œë‹¤ê°€, ë²”ì£¼ë¥¼ ëª¨ë‘ ë‹¤ ì§€ì‹œë³€ìˆ˜ë¡œ ì‚¬ìš©í•˜ë©´ ë²”ì£¼í™”ëœ ë³€ìˆ˜ë“¤ë¼ë¦¬ ì™„ë²½í•œ ì„ í˜•ê´€ê³„ê°€ ì„±ë¦½ë˜ì–´ ê·¹ë‹¨ì ì¸ ë‹¤ì¤‘ê³µì„ ì„±ì„ ë³´ì¼ìˆ˜ ìˆë‹¤. ì„ í˜•ëŒ€ìˆ˜ ì¸¡ë©´ì—ì„œë„ ê° Column vectorë“¤ë¼ë¦¬ ì„œë¡œ linearly independent í•´ì•¼ í•´ë¥¼ ê°–ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìœ„ì˜ ë°©ë²•ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì´ ì˜³ì€ ë°©ë²•ì´ë‹¤. ì—¬ê¸°ì„œ ì§€ì‹œë³€ìˆ˜ ë˜ëŠ” ê°€ë³€ìˆ˜ì— ì˜í•˜ì—¬ í‘œí˜„ë˜ì§€ ì•ŠëŠ” ë²”ì£¼ëŠ” ê¸°ì €ë²”ì£¼(base category) ë˜ëŠ” ëŒ€ì¡° ê·¸ë£¹(control group)ì´ë¼ê³  ë¶ˆë¦¬ëŠ”ë°, ì§€ì‹œë³€ìˆ˜ì˜ íšŒê·€ê³„ìˆ˜ê°€ ëŒ€ì¡° ê·¸ë£¹ì— ëŒ€í•œ ìƒëŒ€ì ì¸ ê°’ìœ¼ë¡œ í•´ì„ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ì•„ë˜ í‘œì—ì„œ ë§Œì¼, ìµœì¢…í•™ë ¥ì´ ëŒ€í•™ì›ì¸ ì‚¬ëŒê³¼ ëŒ€í•™êµì¸ ì‚¬ëŒì˜ í‰ê· ì ì¸ ì°¨ì´ê°€ ê¶ê¸ˆí•  ê²½ìš°ëŠ” $B_{2}-B-{1}=2,000$ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤. ë˜í•œ ì•„ë˜ í•´ì„ì€ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì„ ê³ ì •ì‹œì¼°ì„ ê²½ìš°ì— í•´ë‹¹í•œë‹¤. ìˆ˜ì… = ( \\beta_{0} + ëŒ€í•™êµ + ëŒ€í•™ì› ) ë°˜ì‘ë³€ìˆ˜ëŠ” ê¸‰ë£Œ(S), ì„¤ëª…ë³€ìˆ˜ë¥¼ ê²½ë ¥ë…„ìˆ˜(X), êµìœ¡ìˆ˜ì¤€(E), ê´€ë¦¬ì§ì—¬ë¶€(M)ë¼ê³  í•  ë•Œ, êµìœ¡ìˆ˜ì¤€ì€ 3ê°€ì§€(ê³ êµì¡¸ì—…, ëŒ€í•™ì¡¸ì—…, ëŒ€í•™ì›ì¡¸ì—…)ë¡œì„œ ê³ êµì¡¸ì—…($E_{i1}$), ëŒ€í•™ì¡¸ì—…($E_{i2}$)ì˜ ê°€ë³€ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì•„ë˜ì™€ ê°™ì€ íšŒê·€ëª¨í˜•ì„ ë§Œë“¤ì—ˆë‹¤ê³  ê°€ì •í•˜ì. S = \\beta_{0} + \\beta_{1}X + \\beta_{2}E_{1} + \\beta_{3}E_{2}, + \\beta_{4}M + \\varepsilon ì•„ë˜ ê·¸ë˜í”„ë¥¼ ë³´ì•˜ì„ ë•Œ ì™¼ìª½ì˜ ê²½ë ¥ë…„ìˆ˜ì™€ í‘œì¤€í™” ì”ì°¨ì˜ í”Œë¡¯ì„ ë³´ê²Œë˜ë©´, ì…‹ ë˜ëŠ” ê·¸ ì´ìƒì˜ ì„œë¡œ ë‹¤ë¥¸ ìˆ˜ì¤€ì˜ ì”íƒ€ê°€ ìˆìŒì„ ë³¼ ìˆ˜ ìˆë‹¤. ì´ëŠ” êµìœ¡ìˆ˜ì¤€ê³¼ ê´€ë¦¬ì§ ì—¬ë¶€ì˜ ì˜í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ì¦‰ í˜„ì¬ ë³´ì—¬ì£¼ëŠ” ë³€ìˆ˜ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ì´ ì ì ˆí•˜ì§€ ì•ŠìŒìœ¼ë¡œì¨ ìƒê¸´ í˜„ìƒì¼ ìˆ˜ ë„ ìˆë‹¤. ì˜¤ë¥¸ìª½ì˜ êµìœ¡ìˆ˜ì¤€ $\\times$ ê´€ë¦¬ì˜ ê°€ëŠ¥í•œ 6ê°€ì§€ ë²”ì£¼ì¡°í•©ì˜ ì”ì°¨ í”Œë¡¯ì„ ì‚´í´ë³´ë©´ 6ê°€ì§€ ë²”ì£¼ì¡°í•©ì— ë”°ë¼ ì²´ê³„ì ì¸ êµ°ì§‘ì„ ì´ë£¨ê³  ìˆìŒì„ ë³¼ ìˆ˜ ìˆë‹¤. í—ˆë‚˜ ê° ì¡°í•©ë‚´ì—ì„œ ì”ì°¨ë“¤ì€ ê±°ì˜ ì „ë¶€ ì–‘ì´ê±°ë‚˜ ìŒì˜ ê°’ì„ ì·¨í•˜ê³  ìˆë‹¤. ì´ëŸ¬í•œ í˜„ìƒì€ ìœ„ì˜ ëª¨í˜•ì‹ì´ ê¸‰ë£Œ(S), ê²½ë ¥ë…„ìˆ˜(X), êµìœ¡ìˆ˜ì¤€(E), ê´€ë¦¬ì§ì˜ ì—¬ë¶€(M)ì˜ ê´€ê³„ë¥¼ ì ì ˆíˆ í‘œí˜„í•˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ìœ„ì˜ ëª¨í˜•ì‹ì—ì„œ ì•„ë˜ì™€ ê°™ì´ êµìœ¡ê³¼ ê´€ë¦¬ì§ì˜ ì—¬ë¶€ì˜ êµí˜¸ì‘ìš©í•­ì„ ì¶”ê°€í•´ ì£¼ì–´ì„œ ë‹¤ì‹œ í‘œì¤€í™”ì”ì°¨ vs ê²½ë ¥ë…„ìˆ˜ì˜ í”Œë¡¯ì„ ì‚´í´ë³´ë©´ ì•„ë˜ ê·¸ë˜í”„ì™€ ê°™ë‹¤. ë‹¨ í•˜ë‚˜ì˜ ê´€ì¸¡ê°œì²´ì— ì˜í•´ íšŒê·€ê³„ìˆ˜ì˜ ì¶”ì •ê°’ì´ ì§€ë‚˜ì¹˜ê²Œ ë§ì€ ì˜í–¥ì„ ë°›ê³  ìˆìœ¼ë¯€ë¡œ, í•´ë‹¹ íŠ¹ì´ê°’ì„ ì œì™¸í•˜ê³  ì‹œí–‰í•œ íšŒê·€ë¶„ì„ì˜ ê²°ê³¼ ì „ì²´ì ìœ¼ë¡œ íšŒê·€ê³„ìˆ˜ì˜ ì¶”ì •ê°’ì—ëŠ” ë³„ ë³€ë™ì´ ì—†ì—ˆìœ¼ë©°, ì”ì°¨ë“¤ì˜ í‘œì¤€í¸ì°¨ê°€ ì˜¤íˆë ¤ ì¤„ì–´ë“¤ì—ˆê³ , ê²°ì •ê³„ìˆ˜ê°€ ëŠ˜ì–´ë‚¬ë‹¤ê³  ê°€ì •í•˜ì. ê·¸ë ‡ë‹¤ë©´, í•´ë‹¹ íŠ¹ì´ê°’ì„ ì œì™¸í•œ ëª¨í˜•ì‹ìœ¼ë¡œ ì í•©ì„ ì‹œì¼œì£¼ëŠ” ê²ƒì´ ì˜³ì€ ê²ƒì´ë‹¤. S = \\beta_{0} + \\beta_{1}X + \\beta_{2}E_{1} + \\beta_{3}E_{2}, + \\beta_{4}M + \\beta_{5}(E_{1} \\cdot M) + \\beta_{6} (E_{2} \\cdot M)+ \\varepsilon í•´ë‹¹ íŠ¹ì´ê°’ì„ ì œì™¸í•œ í›„ í‘œì¤€í™” ì”ì°¨ì™€ ê° ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ì˜ í”Œë¡¯ì„ ê·¸ë ¤ë³´ë‹ˆ ì•„ë˜ì™€ ê°™ì´ ëœë¤í•˜ê²Œ ë¶„í¬ë˜ì–´ìˆë‹¤ë©´, ì´ì „ì˜ ëª¨í˜•ë³´ë‹¤ ì„¤ëª…ì„ ë” ì˜í•˜ëŠ” íšŒê·€ëª¨í˜•ì‹ì„ ì°¾ì•˜ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜, í•´ë‹¹ ëª¨í˜•ì˜ ì§ˆì ë³€ìˆ˜ì˜ íšŒê·€ê³„ìˆ˜ì— ëŒ€í•œ í•´ì„ì„ í•¨ì— ìˆì–´ì„œëŠ” ê° ë²”ì£¼ë³„ë¡œ íšŒê·€ê³„ìˆ˜ë¥¼ ë”í•´ì£¼ë©´ ëœë‹¤. ë¬¼ë¡  ìƒìˆ˜í•­ì„ í¬í•¨í•˜ì—¬ ê³„ì‚°í•˜ì—¬ì•¼ í•  ê²ƒì´ë‹¤. ìœ„ì˜ ë°©ë²•ê³¼ ë‹¤ë¥´ê²Œ ì²˜ìŒë¶€í„° 6ê°œì˜ ë²”ì£¼ë¥¼ ë§Œë“¤ì–´ ê°€ë³€ìˆ˜ë¥¼ ì·¨í•´ ë™ì¼í•œ ëª¨í˜•ì„ ì í•©í•˜ëŠ” ë°©ë²•ë„ ìˆëŠ”ë°, ìœ„ì˜ ë°©ë²•ì´ ê°€ì§€ëŠ” ì¥ì ì€ 3ê°€ì§€ ì˜ˆì¸¡ë³€ìˆ˜ (êµìœ¡ìˆ˜ì¤€, ê´€ë¦¬ì§ì˜ ì—¬ë¶€, êµìœ¡ìˆ˜ì¤€-ê´€ë¦¬ì§ì˜ì—¬ë¶€)ê°€ ê°€ì§€ëŠ” íš¨ê³¼ë¥¼ ëª…ë°±í•˜ê²Œ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. ê°€ë³€ìˆ˜(dummy variable)ê°€ ì‚¬ìš©ë˜ëŠ” ë˜ ë‹¤ë¥¸ íšŒê·€ì‹ í•˜ë‚˜ì˜ ë°ì´í„°ê°€ ê°ê° ë‹¤ë¥¸ íšŒê·€ì‹ì„ í•„ìš”ë¡œ í•˜ëŠ” ë‘ ê°œ ë˜ëŠ” ê·¸ ì´ìƒì˜ ë¶€ë¶„ì§‘ë‹¨ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆì„ ìˆ˜ë„ ìˆë‹¤. ì´ëŸ° ê²½ìš°ì— ëª¨ë“  ë¶€ë¶„ì§‘ë‹¨ë“¤ì´ í•©ì³ì§„ ì „ì²´ ë°ì´í„°ì— ëŒ€í•˜ì—¬ ë‹¨ í•˜ë‚˜ì˜ íšŒê·€ê´€ê³„ë§Œ ì‚¬ìš©ëœë‹¤ë©´ ì‹¬ê°í•œ í¸í–¥ì˜ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ë°ì´í„°ì˜ ë¶€ë¶„ì§‘í•©ë“¤ì— ëŒ€í•´ ë³„ë„ì˜ íšŒê·€ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ê¸°ë²•ì€ íš¡ë‹¨ë©´(cross-sectional) ë°ì´í„°ë‚˜ ì‹œê³„ì—´(time series) ë°ì´í„° ë“±ì— ì‘ìš©ë  ìˆ˜ ìˆë‹¤. ê° ì§‘ë‹¨ì´ ë¶„ë¦¬ëœ íšŒê·€ëª¨í˜•ì„ ê°€ì§€ëŠ” ê²½ìš°(ë‹¤ë¥¸ ê¸°ìš¸ê¸°ì™€ ë‹¤ë¥¸ ì ˆí¸í•­ì„ ê°€ì§€ëŠ” ëª¨í˜•) ë‘ ì¸ì¢…ì§‘ë‹¨ì—ì„œ ì—…ë¬´ ìˆ˜í–‰ëŠ¥ë ¥ê³¼ ê³ ìš©ì „ ê²€ì‚¬ì ìˆ˜ì˜ íšŒê·€ê´€ê³„ê°€ ì„œë¡œ ë‹¤ë¥´ë‹¤ëŠ” ê°€ì„¤ì„ ê²€ì¦í•œë‹¤ê³  ê°€ì •í•´ë³´ì. ëª¨í˜• 1ì€ ì¸ì¢… ê°„ì˜ ì°¨ì´ê°€ ì—†ë‹¤ê³  ìƒê°í•˜ê³  í†µí•©ëœ ë°ì´í„°ì— ëŒ€í•´ í•˜ë‚˜ì˜ íšŒê·€ì§ì„ ì„ ê³ ë ¤í•œë‹¤. ì´ì— ëŒ€í•´ ëª¨í˜• 2ëŠ” ë‘ ì¸ì¢…ì§‘ë‹¨ì— ëŒ€í•´ ê°ê° ë”°ë¡œë”°ë¡œ íšŒê·€ê´€ê³„ë¥¼ ê³ ë ¤í•œë‹¤. ì´ë•Œ ê° ì¸ì¢…ì§‘ë‹¨ì— ëŒ€í•œ ì”ì°¨ì˜ ë¶„ì‚°ì€ ë™ì¼í•˜ë‹¤ê³  ê°€ì •í•œë‹¤. ëª¨í˜• 1 (í†µí•©ëª¨í˜•) \\hspace{0.5cm} y_{ij} = \\beta_{0} + \\beta_{1} x_{ij} + \\varepsilon_{ij}, j=1,2; i=1,2,\\cdots,n_{j}ëª¨í˜• 2 (ì†Œìˆ˜ë¯¼ì¡±) - y_{i1} = \\beta_{01} + \\beta_{11} x_{i1} + \\varepsilon_{i1}ëª¨í˜• 2 (ë°±ì¸) - y_{i2} = \\beta_{02} + \\beta_{12} x_{i2} + \\varepsilon_{i2} ë¶„ì„ì— ì•ì„œì„œ, ì–»ì–´ì§„ ê²°ê³¼ë¥¼ í•´ì„í•˜ê³  ì´ë¥¼ ì‘ìš©í•  ë•Œ ë²”í• ì§€ë„ ëª¨ë¥¼ ê°€ëŠ¥í•œ ì˜¤ë¥˜ ëª‡ ê°€ì§€ë¥¼ ìƒê°í•´ ë³´ì. ì•„ë˜ ê·¸ë˜í”„ì—ì„œ $Y_{0}$ë¥¼ ê³ ìš©ì—ì„œ ìš”êµ¬ë˜ëŠ” ìµœì†Œí•œì˜ ì—…ë¬´ìˆ˜í–‰ëŠ¥ë ¥ì´ë¼ê³  í•˜ì. ì´ë•Œ ëª¨í˜• 1ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì—ëŠ” ê³ ìš©ì „ ê²€ì‚¬ì ìˆ˜ê°€ ì ì–´ë„ $X_{p}$ë³´ë‹¤ ë†’ì•„ì•¼ ì„ ë°œë  ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ë§Œì¼ ëª¨í˜• 2ê°€ ì˜³ë‹¤ë©´ ìš”êµ¬ë˜ëŠ” ê³ ìš©ì „ ê²€ì‚¬ì ìˆ˜ëŠ” ë°±ì¸ì˜ ê²½ìš° $X_{w}$, ê·¸ë¦¬ê³  ì†Œìˆ˜ë¯¼ì¡±ì—ê²ŒëŠ” $X_{m}$ì´ìƒì´ë©´ ë  ê²ƒì´ë‹¤. $X_{m}$ì´ë‚˜ $X_{w}$ ëŒ€ì‹ ì— ì¼ê´„ì ìœ¼ë¡œ $X_{p}$ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì€ ë°±ì¸ì—ê²ŒëŠ” ê³ ìš©ì „ ê²€ì‚¬ì ìˆ˜ì— í˜œíƒì„ ì£¼ëŠ” ë°˜ë©´ì— ì†Œìˆ˜ë¯¼ì¡±ì—ê²ŒëŠ” ë¶ˆì´ìµì´ ê°€ê²Œ ë¨ì„ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ ë§Œì¼ í‹€ë¦° ëª¨í˜•ì„ ì‚¬ìš©í•˜ì—¬ ì§€ì›ìë¥¼ í‰ê°€í•œë‹¤ë©´ ì´ëŠ” ê³§ ì¸ì¢…ì°¨ë³„ì˜ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¬ ì†Œì§€ê°€ ìˆë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ìœ„ì˜ ëª¨í˜•2 ëŠ” ì•„ë˜ ëª¨í˜• 3ìœ¼ë¡œë¶€í„° ë§Œë“¤ì–´ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰, ì†Œìˆ˜ë¯¼ì¡± ëª¨í˜•ì€ $x_{ij}=x_{i1}$, $z_{ij} = 1$ì„ ì•„ë˜ ëª¨í˜• 3ì— ëŒ€ì…í•˜ë©´ ê³„ì‚°ë˜ë©°, ë°±ì¸ ëª¨í˜• ë˜í•œ $x_{ij}=x_{i2}$, $z_{ij} = 0$ì„ ëŒ€ì…í•˜ë©´ ë™ì¼í•˜ê²Œ ë‚˜ì˜¨ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ëª¨í˜• 1ê³¼ 2ë¥¼ ë¹„êµí•˜ëŠ” ê²ƒì€ ëª¨í˜• 1ê³¼ 3ì„ ë¹„êµí•˜ëŠ” ê²ƒê³¼ ë™ì¼í•˜ë‹¤. ëª¨í˜• 3ì— $\\gamma = \\delta = 0$ì„ ëŒ€ì…í•˜ë©´ ëª¨í˜• 1ì„ ì–»ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ëª¨í˜• 3ì„ ì™„ì „ëª¨í˜•(FM)ìœ¼ë¡œ ëª¨í˜• 1ì„ ì¶•ì†Œëª¨í˜•(RM)ìœ¼ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆë‹¤. ëª¨í˜• 3 \\hspace{0.5cm} y_{ij} = \\beta_{0} + \\beta_{1}x_{ij} + \\gamma z_{ij} + \\delta(z_{ij} \\cdot x_{ij}) + \\varepsilon_{ij} ê·¸ëŸ¬ë¯€ë¡œ $H_{0} : \\gamma = \\delta = 0$ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì´ F ê²€ì •ì„ í†µí•´ ê²€ì •í•  ìˆ˜ ìˆë‹¤. (k=2, p=3) F-í†µê³„ëŸ‰ ê°’ì¸ 3.4ëŠ” í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê²°ê³¼ì´ë©°, $H_{0}$ë¥¼ ê¸°ê°í•˜ì—¬ $H_{1}$ë¥¼ ì±„íƒí•œë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ë‘ ì§‘ë‹¨ì˜ íšŒê·€ ê´€ê³„ê°€ ì„œë¡œ ë‹¤ë¥´ë‹¤ê³  ê²°ë¡ ì§€ì„ ìˆ˜ ìˆë‹¤. F = \\frac{[SSE(RM) - SSE(FM)]/p+1-k}{SSE(FM)/n-p-1}= \\frac{(45.51 - 31.81)/2}{31.81/16} = 3.4ëª¨í˜•ë“¤ì´ ë™ì¼í•œ ì ˆí¸í•­ì„ ê°€ì§€ì§€ë§Œ ê¸°ìš¸ê¸°ëŠ” ë‹¤ë¥¸ ê²½ìš° ë‘ ì§‘ë‹¨ì´ ë™ì¼í•œ ê¸°ìš¸ê¸° $\\beta_{1}$ì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ë¯¿ì„ ë§Œí•œ ì´ìœ ê°€ ìˆë‹¤ê³  ê°€ì •í•˜ê³ , ë‘ ì§‘ë‹¨ì´ ë™ì¼í•œ ì ˆí¸í•­ì„ ê°€ì§„ë‹¤ëŠ” ê°€ì„¤ $H_{0}: \\beta_{01} = \\beta_{02}$ì„ ê²€ì •í•˜ëŠ” ê²½ìš°ë¥¼ ë‹¤ë£¨ì–´ ë³¼ ê²ƒì´ë‹¤. ëª¨í˜• 1 (í†µí•©ëª¨í˜•) \\hspace{0.5cm} y_{ij} = \\beta_{0} + \\beta_{1} x_{ij} + \\varepsilon_{ij}, j=1,2; i=1,2,\\cdots,n_{j}ëª¨í˜• 2 (ì†Œìˆ˜ë¯¼ì¡±) - y_{i1} = \\beta_{01} + \\beta_{11} x_{i1} + \\varepsilon_{i1}ëª¨í˜• 2 (ë°±ì¸) - y_{i2} = \\beta_{02} + \\beta_{12} x_{i2} + \\varepsilon_{i2} ë‘ ëª¨í˜•ì´ ë™ì¼í•œ ê¸°ìš¸ê¸° $\\beta_{1}$ì„ ê°€ì§€ê³  ìˆìœ¼ë‚˜ ì„œë¡œ ë‹¤ë¥¸ ì ˆí¸í•­ $\\beta_{01}$ê³¼ $\\beta_{02}$ë¥¼ ê°€ì§€ê³  ìˆìŒì„ ì£¼ëª©í•˜ì. ë˜í•œ ìœ„ì˜ ëª¨í˜•2ëŠ” ì•„ë˜ì™€ ê°™ì´ ëª¨í˜• 3ìœ¼ë¡œ ë°”ê¿” ì“¸ ìˆ˜ ìˆë‹¤. ëª¨í˜• 3ì—ì„œ ìƒí˜¸ì‘ìš© ë³€ìˆ˜ $(z_{ij} \\cdot x_{ij})$ê°€ ì—†ìŒì„ ì£¼ëª©í•˜ì. ë§Œì•½ ìƒí˜¸ì‘ìš©í•­ì´ ìˆë‹¤ë©´, ë‘ ì§‘ë‹¨ì€ ë‹¤ë¥¸ ê¸°ìš¸ê¸°ì™€ ë‹¤ë¥¸ ì ˆí¸í•­ì„ ê°–ëŠ” ë‘ ëª¨í˜•ì„ ê°€ì§€ê²Œ ëœë‹¤. ëª¨í˜• 3 : \\hspace{0.5cm} y_{ij} = \\beta_{0} + \\beta_{1}x_{ij} + \\gamma z_{ij} + \\varepsilon_{ij} ë”°ë¼ì„œ ëª¨í˜•2(ë˜ëŠ” ëª¨í˜• 3)ëŠ” ì ˆí¸í•­ $\\beta_{0} + \\gamma$ì™€ $\\beta_{0}$ë¥¼ ê°€ì§„ ë‘ ê°œì˜ í‰í–‰ì¸ ì§ì„ ì„ ë‚˜íƒ€ë‚¸ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ê·€ë¬´ê°€ì„¤ì€ $H_{0}: \\gamma = 0$ì´ ëœë‹¤. ì´ ê°€ì„¤ì„ ê²€ì •í•˜ëŠ” ê²ƒë„ ë™ì¼í•˜ê²Œ ê·€ë¬´ê°€ì„¤ì— í•´ë‹¹í•˜ëŠ” ëª¨í˜•ì„ ì¶•ì†Œëª¨í˜•ìœ¼ë¡œ ê°„ì£¼í•˜ê³  F ê²€ì •ì„ ì‹¤ì‹œí•˜ë©´ ëœë‹¤. ëª¨í˜•ë“¤ì´ ë™ì¼í•œ ê¸°ìš¸ê¸°ë¥¼ ê°€ì§€ì§€ë§Œ ì ˆí¸í•­ì€ ë‹¤ë¥¸ ê²½ìš° ë‘ ì§‘ë‹¨ì´ ë™ì¼í•œ ì ˆí¸í•­ $\\beta$ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ë‘ ì§‘ë‹¨ì´ ê¸°ìš¸ê¸°ë„ ë™ì¼í•˜ë‹¤ëŠ” ê°€ì„¤$(H_{0}:\\beta_{11} = \\beta_{12})$ì„ ê²€ì •í•˜ëŠ” ê²½ìš°ë¥¼ ë‹¤ë£¨ì–´ ë³¼ ê²ƒì´ë‹¤. ë‘ ëª¨í˜•ì´ ë™ì¼í•œ ì ˆí¸í•­ $\\beta_{0}$ì„ ê°€ì§€ê³  ìˆìœ¼ë‚˜ ì„œë¡œ ë‹¤ë¥¸ ê¸°ìš¸ê¸° \\beta_{11}ê³¼ \\beta_{12}ë¥¼ ê°€ì§€ê³  ìˆìŒì„ ì£¼ëª©í•˜ì. ëª¨í˜• 1 (í†µí•©ëª¨í˜•) \\hspace{0.5cm} y_{ij} = \\beta_{0} + \\beta_{1} x_{ij} + \\varepsilon_{ij}, j=1,2; i=1,2,\\cdots,n_{j}ëª¨í˜• 2 (ì†Œìˆ˜ë¯¼ì¡±) - y_{i1} = \\beta_{0} + \\beta_{11} x_{i1} + \\varepsilon_{i1}ëª¨í˜• 2 (ë°±ì¸) - y_{i2} = \\beta_{0} + \\beta_{12} x_{i2} + \\varepsilon_{i2} ìœ„ì˜ ëª¨í˜• 2ëŠ” ì•„ë˜ì™€ ê°™ì´ ëª¨í˜• 3ìœ¼ë¡œ ëŒ€ì²´ ë  ìˆ˜ ìˆë‹¤. ì´ ëª¨í˜•ì—ì„œ ìƒí˜¸ì‘ìš© ë³€ìˆ˜ $(x_{ij} \\cdot z_{ij})$ê°€ ìˆì§€ë§Œ ë³€ìˆ˜ Zì˜ ê°œë³„ ê³µí—Œì€ ì—†ìŒì„ ì£¼ëª©í•˜ì. ê°ê°ì— ëŒ€ì…í•´ ë³´ë©´, ìœ„ì—ì„œì™€ ê°™ì´ ëª¨í˜• 3ì—ì„œ $H_{0} : \\delta = 0$ì„ F-ê²€ì •í•˜ë©´ëœë‹¤. ëª¨í˜• 3 : \\hspace{0.5cm} y_{ij} = \\beta_{0} + \\beta_{1}x_{ij} + \\delta(x_{ij} \\cdot z_{ij}) + \\varepsilon_{ij}ê°€ë³€ìˆ˜(ì§€ì‹œë³€ìˆ˜)ì— ëŒ€í•œ ë‹¤ë¥¸ ì‘ìš©ë“¤ ì•ì—ì„œ ë¬˜ì‚¬ëœ ê°€ë³€ìˆ˜(ì§€ì‹œë³€ìˆ˜)ì˜ ì‘ìš©ë“¤ì€ ë‹¤ì–‘í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í™•ì¥ë  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ Kê°œì˜ ëª¨ì§‘ë‹¨ í‰ê· ì„ ë¹„êµí•˜ëŠ” ê²½ìš° ë¶„ì‚°ë¶„ì„(ANOVA)ë„ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. í¬ê¸° $n_{j}$ì˜ í‘œë³¸ì´ jë²ˆì§¸ ëª¨ì§‘ë‹¨ìœ¼ë¡œ ë¶€í„° ì¶”ì¶œë˜ì—ˆë‹¤ê³  í•˜ì.(j=1, \\cdots, k) ì¢…ì†(ë°˜ì‘)ë³€ìˆ˜ì— ëŒ€í•˜ì—¬ ì´ $n=n_{1}+ \\cdots + n_{k}$ê°œì˜ ê´€ì¸¡ê°œì²´ê°€ ì£¼ì–´ì§„ë‹¤. $y_{ij}$ë¥¼ jë²ˆì§¸ í‘œë³¸ì—ì„œ ië²ˆì§¸ ë°˜ì‘ê°’ì´ë¼ê³  í•˜ì. ê·¸ëŸ¬ë©´ y_{ij}ì— ëŒ€í•œ ëª¨í˜•ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤. y_{ij} = \\mu_{0} + \\mu_{1}x_{i1} + \\cdots + \\mu_{p}x_{ip} + \\varepsilon_{ij} ìœ„ì˜ ëª¨í˜•ì—ì„œëŠ” $p = k-1$ê°œì˜ ì§€ì‹œ ì„¤ëª…ë³€ìˆ˜ë“¤ $x_{i1}, \\cdots, x_{ip}$ì´ ìˆë‹¤. ê° ë³€ìˆ˜ $x_{ij}$ëŠ” ëŒ€ì‘ ë˜ëŠ” ë°˜ì‘ê°’ì´ jë²ˆì§¸ ëª¨ì§‘ë‹¨ìœ¼ë¡œë¶€í„° ë‚˜ì™”ì„ ë•Œ 1ì„ ê°€ì§€ë©°, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0ì„ ê°€ì§„ë‹¤. ìƒëµëœ ëª¨ì§‘ë‹¨ì€ ëŒ€ì¡°(control)ì§‘ë‹¨ìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤. ëŒ€ì¡°ì§‘ë‹¨ì— ëŒ€í•œ ëª¨ë“  ì§€ì‹œë³€ìˆ˜ë“¤ì€ 0ì´ë‹¤. ë”°ë¼ì„œ, ì•„ë˜ì™€ ê°™ì€ ëª¨í˜•ì‹ì´ ëœë‹¤. $\\varepsilon_{ij}$ëŠ” í‰ê·  0, ë¶„ì‚° $\\sigma^{2}$ì„ ê°€ì§€ê³  ë…ë¦½ì¸ ì •ê·œë¶„í¬ì— ë”°ë¥´ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •ë˜ëŠ” ëœë¤ì˜¤ì°¨ì´ë‹¤. ìƒìˆ˜í•­ $\\mu_{0}$ëŠ” ëŒ€ì¡°ì§‘ë‹¨ì˜ í‰ê· ì„ ë‚˜íƒ€ë‚´ë©°, íšŒê·€ê³„ìˆ˜ $\\mu_{j}$ëŠ” ëŒ€ì¡°ì§‘ë‹¨ê³¼ jë²ˆì§¸ ì§‘ë‹¨ì˜ í‰ê· ì°¨ì´ë¡œ í•´ì„ë  ìˆ˜ ìˆë‹¤. ë§Œì•½ ëª¨ë“  ì§‘ë‹¨ì´ ë™ì¼í•œ í‰ê· ì„ ê°€ì§„ë‹¤ëŠ” ê·€ë¬´ê°€ì„¤ $H_{0} : \\mu_{1}= \\cdots =\\mu_{p}=0$ì„ ê²€ì •í•˜ëŠ” ê²ƒì€ ì•„ë˜ ëª¨í˜•ì‹ê³¼ ìœ„ì˜ ëª¨í˜•ì‹ì„ ê°ê° ì¶•ì†Œëª¨í˜•ê³¼ ì™„ì „ëª¨í˜•ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ F-ê²€ì •ì„ í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•˜ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì§€ì‹œë³€ìˆ˜ì˜ ì‚¬ìš©ì€ íšŒê·€ë¶„ì„ì˜ íŠ¹ë³„í•œ ê²½ìš°ë¡œì„œ ANOVA ê¸°ë²•ì„ í‘œí˜„í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. y_{ij} = \\mu_{0} + \\varepsilon_{ij} ì´ì™¸ì˜ ì˜ˆë“¤ë¡œëŠ” ê°€ë³€ìˆ˜ê°€ ì¢…ì†ë³€ìˆ˜ë¡œ ì‚¬ìš©ë˜ëŠ” ë¡œì§€ìŠ¤í‹±íšŒê·€ëª¨í˜•ê³¼ ì‹œê³„ì—´ ë°ì´í„°ì— ëŒ€í•˜ì—¬ ê³„ì ˆì„±ê³¼ ì‹œê°„ì˜ ê²½ê³¼ì— ë”°ë¥¸ ëª¨ìˆ˜ì˜ ì•ˆì •ì„±ì˜ ë¬¸ì œë¥¼ ë‹¤ë£° ìˆ˜ ìˆë‹¤. ê³„ì ˆì„±ì€ ì˜ˆë¥¼ ë“¤ì–´ ë¶„ê¸°ë³„ë¡œ ë˜ì–´ìˆëŠ” ë°ì´í„°ë¥¼ ê°€ë³€ìˆ˜ë¡œ ë§Œë“¤ì–´ì£¼ì–´ ë¶„ê¸°ë³„ë¡œ ì£¼ì–´ì§€ëŠ” ë§¤ì¶œì•¡ì— ì˜í–¥ì„ ì£¼ëŠ” ê³„ì ˆì„±ì˜ ì¡´ì¬ë¥¼ ê²€ì •í•˜ëŠ” ê²ƒì„ ë§í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ì‹œê°„ì˜ ê²½ê³¼ì— ë”°ë¥¸ ëª¨ìˆ˜ì˜ ì•ˆì •ì„±ì€ ì‹œê°„ì— ë”°ë¥¸ ê°€ë³€ìˆ˜ë¥¼ í†µí•´ ì´ë“¤ì˜ ê³„ìˆ˜ê°€ 0ì„ì„ ê²€ì •í•˜ë©´ í•´ë‹¹ ê¸°ê°„ë™ì•ˆì˜ íšŒê·€ê´€ê³„ê°€ ë¶ˆë³€í•œë‹¤ëŠ” ê²ƒì„ ë³´ì¼ ìˆ˜ë„ ìˆë‹¤. ë³€ìˆ˜ ì„ íƒë²• íšŒê·€ ë¶„ì„ì˜ ì§„ë‹¨ : ëª¨í˜• ìœ„ë°˜ì˜ ê²€ì¶œ ì£¼ì–´ì§„ ë°ì´í„°ì— ëª¨í˜•ì„ ì í•©í•¨ì— ìˆì–´ì„œ, í•œ ê°œ ë˜ëŠ” ëª‡ ê°œì˜ ê´€ì¸¡ ê°œì²´ë“¤ì— ì˜í•˜ì—¬ ì í•©ì´ ê³¼ë„í•˜ê²Œ ê²°ì •ë˜ëŠ” ê²ƒìœ¼ ë°”ëŒì§ í•˜ì§€ ì•Šë‹¤. ì•ì„œ ë§í•œ ê°€ì„¤ê²€ì •ë“±ì€ í‘œì¤€ì ì¸ íšŒê·€ì˜ ê°€ì •ë“¤ì´ ë§Œì¡±ë  ë•Œë§Œ ìœ ì˜ë¯¸í•˜ë‹¤. ì´ë“¤ ê°€ì •ì´ ìœ„ë°˜ëœë‹¤ë©´, ì´ì „ì— ì–¸ê¸‰ëœ í‘œì¤€ì ì¸ ê²°ê³¼ë“¤ì€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©° ê²°ê³¼ì˜ ì‘ìš©ì´ ì‹¬ê°í•œ ì˜¤ë¥˜ë¥¼ ì•¼ê¸°í•  ìˆ˜ë„ ìˆë‹¤. ëª¨í˜•ìœ„ë°˜ì„ ê²€í† í•˜ê¸° ìœ„í•´ ì—„ê²©í•œ ìˆ˜ì¹˜ì  ê·œì¹™ë“¤ì„ ì ìš©í•˜ëŠ” ê²ƒ ëŒ€ì‹ ì— ì£¼ë¡œ ê·¸ë˜í”„ì ì¸ ë°©ë²•ë“¤ì„ ì†Œê°œí•  ê²ƒì´ë‹¤. íšŒê·€ë¶„ì„ì˜ í‘œì¤€ì ì¸ ê°€ì •\u001dë“¤ 1) ì„ í˜•ì„± ê°€ì • : ì¢…ì†(ë°˜ì‘)ë³€ìˆ˜ Yì™€ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ Xë“¤ì„ ê´€ê³„ì‹œí‚¤ëŠ” ëª¨í˜•ì´ íšŒê·€ê³„ìˆ˜ $\\beta$ë“¤ì— ëŒ€í•˜ì—¬ ì„ í˜•ì„ì„ ê°€ì •í•œë‹¤. ë§Œì•½, ì„ í˜•ì„± ê°€ì •ì´ ë§Œì¡±ë˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ì¢…ì¢… ë°ì´í„°ì— ëŒ€í•œ ë³€í™˜ì„ í†µí•´ ì„ í˜•ì„±ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤. ë‹¨ìˆœíšŒê·€ì—ì„œëŠ” ì´ ê°€ì •ì„ Yì™€ Xì˜ ì‚°ì ë„ë¥¼ í†µí•´ ì‰½ê²Œ í™•ì¸ í•  ìˆ˜ ìˆìœ¼ë‚˜, ë‹¤ì¤‘íšŒê·€ì—ì„œëŠ” ê³ ì°¨ì›ì„± ë•Œë¬¸ì— ì‚°ì ë„ë¥¼ í†µí•´ í™•ì¸ì´ ì–´ë µë‹¤. Y = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p} + \\epsilon 2) ì”ì°¨ì— ëŒ€í•œê°€ì • : $\\epsilon_{i} \\sim^{i.i.d} N(0, \\sigma^{2})$ ì´ ê°€ì •ì„ í†µí•´ ì•„ë˜ì˜ ê°€ì •ë“¤ì„ ë§Œì¡±í•´ì•¼í•œë‹¤. 1) ì”ì°¨ì˜ ì •ê·œì„± : ì”ì°¨ $\\epsilon_{i}$ëŠ” ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤. ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ì˜ ê°’ì´ ë°˜ë³µë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ ì‰½ê²Œ ìœ„ë°˜ë˜ì§€ ì•ŠëŠ”ë‹¤. 2) ì”ì°¨ì˜ ë“±ë¶„ì‚°ì„± : ë™ì¼í•œ ìƒìˆ˜ë¶„ì‚° $\\sigma^{2}$ì„ ê°€ì ¸ì•¼ í•œë‹¤. ì´ ê°€ì •ì´ ë§Œì¡±í•˜ì§€ ì•Šì„ ë•Œ ì´ë¶„ì‚°ì„±ì„ ëˆë‹¤ëŠ” ë¬¸ì œê°€ ìˆë‹¤ê³  í•œë‹¤. 3) ì”ì°¨ì˜ ë…ë¦½ì„± : ì”ì°¨ë“¤ì´ ì„œë¡œ ë…ë¦½ì´ë¯€ë¡œ ê·¸ë“¤ì˜ ê³µë¶„ì‚°ì€ ëª¨ë‘ 0ì´ë‹¤. ì´ ê°€ì •ì´ ë§Œì¡±ë˜ì§€ ì•Šìœ¼ë©´ ìê¸°ìƒê´€ì˜ ë¬¸ì œê°€ ìˆë‹¤ê³  í•œë‹¤. 3) ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ì— ëŒ€í•œ ê°€ì •(1,2ëŠ” ì‹¤ì œë¡œ í‰ê°€ ë¶ˆê°€í•˜ë¯€ë¡œ 3ì´ ì¤‘ìš”!!) : 1) ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ì€ í™•ë¥ ë³€ìˆ˜ê°€ ì•„ë‹ˆë‹¤. ë§Œì•½ ì‹¤í—˜ ì„¤ê³„ì— ì˜í•´ì„œ ì–»ì–´ì§„ ë°ì´í„° ê°’ë“¤ì— ì˜í•œ ê²ƒì´ ì•„ë‹Œ ë¹„ì‹¤í—˜ ë˜ëŠ” ê´€ì¸¡ì˜ ìƒí™©ì—ì„œëŠ” ì´ê²ƒì´ ë§Œì¡±ë˜ì§€ ì•Šì„ ê²ƒì´ë¼ëŠ” ê²ƒì€ ëª…í™•í•˜ë©°, ì´ì— ëŒ€í•œ í•´ì„ë„ ìˆ˜ì •ë˜ì–´ì•¼í•œë‹¤. ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ì´ í™•ë¥ ë³€ìˆ˜ì´ë©´ ëª¨ë“  ì¶”ë¡ ì€ ê´€ì¸¡ëœ ë°ì´í„°ì— ì˜ì¡´í•˜ì—¬ ì¡°ê±´ë¶€ì ì´ë‹¤. 2) ê°’ $x_{1j}, x_{2j}, \\cdots ,x_{nj}$ëŠ” ì˜¤ì°¨ ì—†ì´ ì¸¡ì •ëœ ê²ƒìœ¼ë¡œ ê°€ì •ëœë‹¤. í—ˆë‚˜ ì´ ê°€ì •ì€ ë§Œì¡±ë˜ê¸° ì‰½ì§€ ì•Šë‹¤. ì¸¡ì •ì—ì„œì˜ ì˜¤ì°¨ëŠ” ì”ì°¨ì˜ ë¶„ì‚°, ë‹¤ì¤‘ìƒê´€ê³„ìˆ˜, íšŒê·€ê³„ìˆ˜ì˜ ê°œë³„ ì¶”ì •ì¹˜ë“¤ì— ì˜í–¥ì„ ì¤„ ê²ƒì´ë‹¤. ì¶”ì •ëœ íšŒê·€ê³„ìˆ˜ë¡œë¶€í„° ì¸¡ì •ì˜¤ì°¨ì˜ ì˜í–¥ì„ ì œê±°í•˜ëŠ” ê²ƒì€ ê±°ì˜ ê¸°ëŒ€í•˜ê¸° í˜ë“¤ë‹¤.ê·¸ëŸ¬ë¯€ë¡œ ë³€ìˆ˜ë“¤ì´ ì˜¤ì°¨ë¥¼ ê°€ì§€ê³  ìˆì–´ì„œ íšŒê·€ê³„ìˆ˜ì˜ ì¶”ì •ì— ë¬¸ì œê°€ ìˆë”ë¼ë„ íšŒê·€ë°©ì •ì‹ì´ ì˜ˆì¸¡ì„ ìœ„í•´ ì—¬ì „íˆ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ì— ì¡´ì¬í•˜ëŠ” ì˜¤ì°¨ëŠ” ì˜ˆì¸¡ì˜ ì •í™•ë„ë¥¼ ê°ì†Œ ì‹œí‚¬ ê²ƒì´ë‹¤. 3) ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ëŠ” ì„ í˜•ì¢…ì†ì´ ì•„ë‹Œ ê²ƒìœ¼ë¡œ ê°€ì •ëœë‹¤. ì¦‰, ìœ„í•´ì„œ ì–¸ê¸‰í–ˆì—ˆë˜ linearly independentí•´ì•¼ í•œë‹¤ëŠ” ì˜ë¯¸ì´ë©° ì´ ê°€ì •ìœ¼ë¡œ ì¸í•´ ì •ê·œë°©ì •ì‹ì˜ í•´ì˜ ìœ ì¼ì„±ì„ ë³´ì¥ë°›ì„ ìˆ˜ ìˆë‹¤. ì´ ê°€ì •ì´ ìœ„ë°˜ ë˜ëŠ” ê²ƒì´ ê³µì„ ì„±(collinearity)ì˜ ë¬¸ì œì´ë‹¤. 4) ê´€ì¸¡ê°œì²´ì— ëŒ€í•œ ê°€ì • : ëª¨ë“  ê´€ì¸¡ê°œì²´ë“¤ì€ ë™ì¼í•˜ê²Œ ì‹ ë¢°í•  ë§Œí•˜ë©°, íšŒê·€ì˜ ê²°ê³¼ë¥¼ ê²°ì •í•˜ê³  ê²°ë¡ ì„ ë„ì¶œí•¨ì— ìˆì–´ì„œ ê±°ì˜ ë™ë“±í•œ ì—­í• ì„ í•œë‹¤. ìµœì†Œì œê³±ë²•ì˜ íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” ê¸°ë³¸ ê°€ì •ì— ëŒ€í•œ ì‚¬ì†Œí•œ ë˜ëŠ” ì‘ì€ ìœ„ë°˜ì´ ë¶„ì„ìœ¼ë¡œë¶€í„° ë„ì¶œëœ ì¶”ë¡ ì´ë‚˜ ê²°ë¡ ì„ ë¬´íš¨í™”í•  ë§Œí¼ í° ì˜í–¥ì„ ì£¼ì§€ëŠ” ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë‚˜ ëª¨í˜•ì˜ ê°€ì •ì— ëŒ€í•œ í° ìœ„ë°˜ì€ ê²°ë¡ ì„ ì‹¬ê°í•˜ê²Œ ì™œê³¡ ì‹œí‚¤ë¯€ë¡œ ê²°ë¡ ì ìœ¼ë¡œ, ê·¸ë˜í”„ë¥¼ í†µí•´ì„œ ì”ì°¨ì˜ êµ¬ì¡°ì™€ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì¡°ì‚¬í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•˜ë‹¤. ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì”ì°¨ë“¤ íšŒê·€ë¶„ì„ì— ìˆì–´ì„œ ëª¨í˜•ì´ ê°€ì§€ëŠ” ê°€ëŠ¥í•œ ê²°í•¨ì„ ì°¾ì•„ë‚´ëŠ” ë° ê°€ì¥ ê°„ë‹¨í•˜ê³  íš¨ê³¼ì ì¸ ë°©ë²•ì€ ì”ì°¨í”Œë¡¯ì„ ì‚´í´ë³´ëŠ” ê²ƒì´ë‹¤. ë”ìš±ì´, ë¶„ì„ì´ ìš”ì•½í†µê³„ëŸ‰ì—ë§Œ ê·¼ê±°í•  ê²½ìš° ê°„ê³¼í• ì§€ë„ ëª¨ë¥¼ ë°ì´í„°ì˜ ì¤‘ìš”í•œ êµ¬ì¡°ì™€ ì •ë³´ë“¤ì„ ì”ì°¨ë¶„ì„ì„ í†µí•´ ë°œê²¬í•  ìˆ˜ë„ ìˆë‹¤. \\hat{Y} = X\\hat{\\beta} = PYP = X(X^{T}X)X^{T} ì¦‰, Yë¥¼ $\\hat{Y}$ë¡œ ë§Œë“¤ê¸° ìœ„í•œ linear transform matrixë¥¼ ëª¨ì(hat) ë˜ëŠ” ì‚¬ì˜(Projection) matrix Pë¼ê³  í•œë‹¤. ì—¬ê¸°ì„œ $i=jì¼ ë•Œ, p_{ii}=p_{ij}=p_{ji}=p_{jj}$ëŠ” ì‚¬ì˜í–‰ë ¬(P)ì˜ ië²ˆì§¸ ëŒ€ê°ì›ì†Œì´ë‹¤. ì´ê²ƒì€ ië²ˆì§¸ ê´€ì¸¡ê°œì²´ì— ëŒ€í•œ ì§€ë ˆê°’(Leverage value)ìœ¼ë¡œ ë¶ˆë¦°ë‹¤. ì•„ë˜ì˜ ì‹ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ $\\hat{y}_{i}$ì€ Yì˜ ëª¨ë“  ê´€ì¸¡ê°’ë“¤ì˜ ê°€ì¤‘í•©ì´ë©°, $p_{ii}$ëŠ” ië²ˆì§¸ ì í•©ê°’ $\\hat{y}_{i}$ì„ ê²°ì •í•¨ì— ìˆì–´ì„œ $y_{i}$ì— ë¶€ì—¬ë˜ëŠ” ê°€ì¤‘ì¹˜(ì§€ë ˆ)ì´ê¸° ë•Œë¬¸ì´ë‹¤. \\hat{y}_{i} = p_{i1}y_{1} + p_{i2}y_{2} + \\cdots + p_{in}y_{n}, i=1,2,...,n ë˜í•œ, ì”ì°¨($e_{i}$)ì˜ ë¶„ì‚°ì€ ê·¸ì˜ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ì–´ í‘œì¤€í™”í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ í‰ê·  0ê³¼ í‘œì¤€í¸ì°¨ 1ì„ ê°€ì§€ëŠ” í‘œì¤€í™” ì”ì°¨(standardized residual)ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ìì„¸í•˜ê²ŒëŠ” $\\sigma$ë¥¼ ì–´ë–¤ ê²ƒì„ ì‚¬ìš©í•˜ëƒì— ë”°ë¼ ë‚´ì  í‘œì¤€í™”ì”ì°¨ì™€ ì™¸ì  í‘œì¤€í™”ì”ì°¨ë¡œ ë‚˜ë‰˜ì–´ì§€ì§€ë§Œ, ê²°êµ­ í‘œë³¸í¬ê¸°ê°€ ì¶©ë¶„íˆ í´ë•Œ(30ì´ìƒ) ì´ ì”ì°¨ë“¤ì€ ê·¼ì‚¬ì ìœ¼ë¡œ í‘œì¤€ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤. ë˜í•œ ì”ì°¨ë“¤ì€ ì—„ë°€í•˜ê²ŒëŠ” ì„œë¡œ ë…ë¦½ì´ ì•„ë‹ˆì§€ë§Œ, í‘œë³¸í¬ê¸°ê°€ í¬ë©´ ë…ë¦½ì„±ì˜ ë¬¸ì œëŠ” ë¬´ì‹œ ë  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ, ì”ì°¨í”Œë¡¯ì„ ì‘ì„±í•¨ì— ìˆì–´ì„œ ë‘ê°€ì§€ í˜•íƒœì˜ ì”ì°¨ ì¤‘ ì–´ëŠ ê²ƒì„ ì‚¬ìš©í•˜ëŠ”ê°€ëŠ” ë³„ë¡œ ë¬¸ì œê°€ ë˜ì§€ ì•ŠëŠ”ë‹¤. z_{i} = \\frac{e_{i}}{\\sigma \\sqrt{1-p_{ii}}}ê·¸ë˜í”„ì  ë°©ë²•ë“¤ ê·¸ë˜í”„ì  ë°©ë²•ë“¤ì€ ë°ì´í„° ë¶„ì„ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, íŠ¹íˆ ë°ì´í„°ì— ì„ í˜•ëª¨í˜•ì„ ì í•©í•  ë•Œ ë”ìš± ì¤‘ìš”í•˜ë‹¤. ë¶„ì„ì´ ìˆ˜ì¹˜ì  ê²°ê³¼ì—ë§Œ ì˜ì¡´í•œë‹¤ë©´ ì˜ëª»ëœ ê²°ë¡ ì— ë„ë‹¬í•  ìˆ˜ ìˆìŒì„ ë³¼ ìˆ˜ ìˆë‹¤.ê·¸ ëŒ€í‘œì ì¸ ì˜ˆë¡œëŠ” Anscombeì˜ ë°ì´í„°ë¥¼ ë“¤ ìˆ˜ ìˆë‹¤. ì•„ë˜ì˜ ë°ì´í„°ë“¤ì€ ë™ì¼í•œ ìƒê´€ê³„ìˆ˜ë¥¼ ê°€ì§€ë„ë¡ ë˜ì–´ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‚°ì ë„ëŠ” ì™„ì „íˆ ë‹¤ë¥¸ íŒ¨í„´ì„ ë³´ì—¬ì¤€ë‹¤. ì•„ë˜ì˜ ê·¸ë˜í”„ì—ì„œ ìƒë‹¨ì˜ ë‘ ê·¸ë˜í”„ ì¤‘ ì™¼ìª½ì€ ì„ í˜•ëª¨í˜•ì´ ì ì ˆí•¨ì„ ë³´ì—¬ì¤€ë‹¤. ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ëŠ” ì•„ë§ˆë„ ì„ í˜•í™”ê°€ ê°€ëŠ¥í•œ ë¹„ì„ í˜• ëª¨í˜•ì„ ë‚˜íƒ€ë‚¸ë‹¤. í•˜ë‹¨ì˜ ë‘ ê·¸ë˜í”„ì¤‘ ì™¼ìª½ ê·¸ë˜í”„ëŠ” ì§ì„ ìœ¼ë¡œ ë¶€í„° ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆëŠ” í•˜ë‚˜ì˜ ì ì„ ì œì™¸í•˜ë©´ ë°ì´í„°ê°€ ê±°ì˜ ì„ í˜•ëª¨í˜•ì„ ë”°ë¥´ê³  ìˆìŒì„ ë³´ì—¬ì¤€ë‹¤. ì´ ì ì€ íŠ¹ì´ê°’ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë°ì´í„°ë¡œë¶€í„° ì–´ë–¤ ê²°ë¡ ì„ ë„ì¶œí•˜ê¸° ì „ì— ì¡°ì‚¬ë˜ì–´ì•¼ë§Œ í•œë‹¤. ë§ˆì§€ë§‰ ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ëŠ” ë¹„íš¨ìœ¨ì ì¸ ì‹¤í—˜ ë˜ëŠ” ë‚˜ìœ í‘œë³¸ì„ì„ ë‚˜íƒ€ë‚¸ë‹¤. X=19ì¸ ì ì— ëŒ€í•˜ì—¬, ëŒ€ì‘ë˜ëŠ” Yì˜ ê°’ì´ ì•„ë¬´ë¦¬ í¬ê±°ë‚˜ ì‘ë”ë¼ë„ ì´ ì ì—ì„œì˜ ì”ì°¨ê°€ í•­ìƒ 0(ë¶„ì‚° 0ì„ ê°€ì§€ëŠ”)ì´ë‹¤. ì´ ì ì„ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ë°ì´í„°ë¡œë¶€í„° êµ¬í•œ ìµœì†Œì œê³± ì¶”ì •ì¹˜ëŠ” ë” ì´ìƒ ìœ ì¼í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ë³´ì¼ ìˆ˜ ìˆë‹¤. íšŒê·€ ê²°ê³¼ì— ê³¼ë„í•˜ê²Œ í° ì˜í–¥ì„ ì£¼ëŠ” ê´€ì¸¡ê°œì²´ë¥¼ ì˜í–¥ë ¥ ìˆëŠ” ê´€ì¸¡ê°œì²´(influential observation)ë¼ê³  í•˜ëŠ”ë°, X=19ì— ìˆëŠ” ì ì€ ì´ê²ƒì´ ì í•©ì„ ì˜ ì ˆí¸ê³¼ ê¸°ìš¸ê¸°ë¥¼ ì™„ì „íˆ ê²°ì •í•˜ê¸° ë•Œë¬¸ì— ê·¹ë‹¨ì ìœ¼ë¡œ ì˜í–¥ë ¥ì´ ìˆë‹¤. íŠ¹ì • ê·¸ë˜í”„ë¥¼ íƒìƒ‰í•˜ê¸°ì— ì•ì„œ, ì–´ë–¤ ê°€ì •ì´ ë§Œì¡±ë  ë•Œ ê·¸ ê·¸ë˜í”„ê°€ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚˜ì•¼ í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ì•¼ í•œë‹¤. ê·¸ëŸ¬ê³  ë‚˜ì„œ ê·¸ ê·¸ë˜í”„ê°€ ê¸°ëŒ€ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ë¥¼ ì‚´í´ë³´ì•„ì•¼ í•œë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ ê°€ì •ì˜ ì˜¬ë°”ë¦„ ë˜ëŠ” ê·¸ë¦‡ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. 1) ëª¨í˜•ì„ ì í•©í•˜ê¸° ì´ì „ì˜ ê·¸ë˜í”„ ì¢…ì†(ë°˜ì‘)ë³€ìˆ˜ì™€ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ëª¨í˜•ì˜ í˜•íƒœëŠ” ì´ë¡ ì  ë°°ê²½ ë˜ëŠ” ê²€ì •ë  ê°€ì„¤ì— ê·¼ê±°í•´ì•¼ í•œë‹¤. 1) ì¼ì°¨ì› ê·¸ë˜í”„ : ê°œë³„ ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ ê°œëµì ìœ¼ë¡œ ì‚´í´ë³´ê¸° ìœ„í•´ ê·¸ë¦°ë‹¤. ì´ë¥¼ í†µí•´ ì–´ë–¤ ë³€ìˆ˜ê°€ ë§¤ìš° ì¹˜ìš°ì³ì ¸ ìˆë‹¤ë©´ ë³€í™˜ì´ ìˆ˜í–‰ë˜ì–´ì•¼ í•œë‹¤. ë¹„ëŒ€ì¹­ì˜ ì •ë„ê°€ ì‹¬í•œ ë³€ìˆ˜ì— ëŒ€í•˜ì—¬ ë¡œê·¸ ë³€í™˜ì´ ì¶”ì²œëœë‹¤. ì¼ë³€ëŸ‰ ê·¸ë˜í”„ëŠ” ì›ë˜ì˜ ë³€ìˆ˜ë¥¼ ì´ìš©í•´ì•¼ í• ì§€ ì•„ë‹ˆë©´ ë³€í™˜ëœ ë³€ìˆ˜ë¥¼ ê°€ì§€ê³  ë¶„ì„ì„ ìˆ˜í–‰í•´ì•¼ í•˜ì§€ì— ëŒ€í•˜ì—¬ ì •ë³´ë¥¼ ì œê³µí•œë‹¤. ë˜í•œ ì¼ë³€ëŸ‰ ê·¸ë˜í”„ëŠ” ë³€ìˆ˜ì— ìˆëŠ” íŠ¹ì´ê°’ì˜ ì¡´ì¬ ìœ ë¬´ë¥¼ ì œì‹œí•œë‹¤. íŠ¹ì´ê°’ì€ ê·¸ê²ƒì´ ì…ë ¥ì˜¤ë¥˜ ë“±ì— ì˜í•œ ê²ƒì¸ì§€(ì¸¡ì •í›„ ì˜ëª» ê¸°ì…ëœ ê²½ìš°ì™€ ê°™ì€)ë¥¼ ì•Œì•„ë³´ê¸° ìœ„í•´ ì¡°ì‚¬ë˜ì–´ì•¼í•œë‹¤. ë˜í•œ íŠ¹ì´ê°’ì€ ì´í›„ì˜ ë¶„ì„ì—ì„œ ë¬¸ì œë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ë¶„ì„ì„ ìˆ˜í–‰í•  ë•Œ ì£¼ì˜ê¹Šê²Œ ë‹¤ë£¨ì–´ì ¸ì•¼ í•œë‹¤. ex) histogram, stem-and-leaf display, dot plot, box plot 2) ì´ì°¨ì› ê·¸ë˜í”„ : ë³€ìˆ˜ì˜ ìˆ˜ê°€ ë§ì€ ê²½ìš° í•´ë‹¹ ì°¨ì›ê³¼ ê°™ì€ ì°¨ì›ì—ì„œ ë³€ìˆ˜ë“¤ì„ ë³¼ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ê° ë³€ìˆ˜ë“¤ì˜ ìŒì— ëŒ€í•œ ê´€ê³„ë¥¼ íƒìƒ‰í•˜ê³  ì¼ë°˜ì ì¸ íŒ¨í„´ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ì‚°ì ë„ë¥¼ í†µí•´ ì‚´í´ë³¼ ìˆ˜ ìˆë‹¤. ì‚°ì ë„í–‰ë ¬ì„ ì‚´í´ë³¼ë•Œ ì£¼ì˜í•  ì ì€ ìƒê´€ê³„ìˆ˜ëŠ” ì˜¤ì§ ì„ í˜•ê´€ê³„ë§Œì„ ì¸¡ì •í•˜ë©° robustí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìŒë³„ ìƒê´€ê³„ìˆ˜ëŠ” ëŒ€ì‘ë˜ëŠ” ì‚°ì ë„ì™€ ì—°ê´€í•˜ì—¬ í•´ì„í•´ì•¼ í•œë‹¤ëŠ” ì ì´ë‹¤. ë‹¨ìˆœíšŒê·€ì—ì„œëŠ” YëŒ€ Xì˜ ì‚°ì ë„ê°€ ì„ í˜•ì˜ í˜•íƒœë¥¼ ë³´ì¼ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë˜ë‚˜, ë‹¤ì¤‘íšŒê·€ì—ì„œëŠ” YëŒ€ ê° ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ì˜ ì‚°ì ë„ê°€ ì„ í˜•ì˜ í˜•íƒœë¥¼ ë³´ì¼ ìˆ˜ë„ ìˆê³  ê·¸ë ‡ì§€ ì•Šì„ ìˆ˜ë„ ìˆë‹¤. ì¦‰, ì„ í˜•ì˜ í˜•íƒœê°€ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤ê³  í•´ì„œ ì£¼ì–´ì§„ ì„ í˜•ëª¨í˜•ì´ ì˜³ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ì§€ ì•ŠëŠ—ë‹¤. ë˜í•œ, ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ì´ ì„ í˜•ì ìœ¼ë¡œ ë…ë¦½ì„ì„ ê°€ì •í•˜ê³  ìˆê¸° ë•Œë¬¸ì—, ê°ê°ì˜ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ë¼ë¦¬ ì„ í˜•íŒ¨í„´ì„ ë³´ì´ì§€ ì•Šì•„ì•¼ í•œë‹¤. ë‹¤ë§Œ, ë‘ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ì˜ ì‚°ì ë„ì— ì„ í˜•ê´€ê³„ê°€ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ ì „ì²´ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ì˜ ì§‘í•©ì´ ì„ í˜•ì ìœ¼ë¡œ ë…ë¦½ì´ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ì§€ëŠ” ì•Šê¸° ë•Œë¬¸ì— ì£¼ì˜ê°€ í•„ìš”í•˜ë‹¤. ì„ í˜• ê´€ê³„ëŠ” ë‘ê°œ ì´ìƒì˜ ë³€ìˆ˜ë“¤ì„ í¬í•¨í•˜ê³  ìˆì„ ìˆ˜ ìˆë‹¤. scatter plotì„ í†µí•´ì„œëŠ” ê·¸ëŸ° ë‹¤ë³€ëŸ‰ ê´€ê³„ë¥¼ ê²€ì¶œí•˜ëŠ” ê²ƒì´ ì‰½ì§€ ì•Šë‹¤. ê·¸ëŸ¬í•œ ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œëŠ” ì•ì„œ ë‹¤ë£¬ ë°©ë²•ê³¼ ê°™ì´ í•´ê²°í•˜ë ¤ê³  í•´ë³´ì•„ì•¼ í•œë‹¤. 3) íšŒì „ë„í‘œ 4) ë™ì ê·¸ë˜í”„ ë‹¤ë³€ëŸ‰ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ ê´€ê³„ë¥¼ íƒìƒ‰í•˜ëŠ” ë° ìœ ìš©í•œ ë„êµ¬ì´ë‹¤. 2) ëª¨í˜•ì„ ì í•©í•œ ì´í›„ì˜ ê·¸ë˜í”„ ì•ì—ì„œ ì†Œê°œëœ ê·¸ë˜í”„ë“¤ì€ ë°ì´í„° ê²€í† ì™€ ëª¨í˜•ì„¤ì • ë‹¨ê³„ì—ì„œ ìœ ìš©í•˜ë‹¤. ë°ì´í„°ì— ëª¨í˜•ì„ ì í•©í•œ ì´í›„ì˜ ê·¸ë˜í”„ë“¤ì€ ê°€ì •ë“¤ì„ ê²€í† í•˜ê³  ì£¼ì–´ì§„ ëª¨í˜•ì˜ ì í•©ë„ë¥¼ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì„ ì¤€ë‹¤. 1) ì„ í˜•ì„±ê³¼ ì •ê·œì„± ê°€ì •ì„ ê²€í† í•˜ê¸° ìœ„í•œ ê·¸ë˜í”„í‘œì¤€í™”ì”ì°¨ì˜ ì •ê·œí™•ë¥  plot (Q-Q plot) í‘œì¤€í™” ì”ì°¨ì˜ ë¶„ìœ„ìˆ˜ì™€ í‘œì¤€ì •ê·œë¶„í¬ì˜ ë¶„ìœ„ìˆ˜ì˜ scatter plotì´ë¼ê³  ë³´ë©´ëœë‹¤. ë§Œì•½ ì”ì°¨ê°€ ì •ê·œì„±ì„ ëˆë‹¤ë©´ ëŒ€ê°ì„ ê³¼ ìµœëŒ€í•œ ë¹„ìŠ·í•˜ê²Œ ê·¸ë ¤ì ¸ì•¼í•œë‹¤. (Standardized) Residual vs Predictor(ë…ë¦½ë³€ìˆ˜) ì‚°ì ë„ í‘œì¤€ì ì¸ ê°€ì • í•˜ì—ì„œ í‘œì¤€í™”ì”ì°¨ëŠ” ê° ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ê³¼ ìƒê´€ë˜ì–´ ìˆì§€ ì•Šë‹¤. ì´ ê°€ì •ì´ ë§Œì¡±ëœë‹¤ë©´ ì´ í”Œë¡¯ì€ ëœë¤í•˜ê²Œ í©ì–´ì§„ ì ë“¤ì´ ë‚˜íƒ€ë‚˜ì•¼ í•œë‹¤. ì´ plotì—ì„œ íŠ¹ì •í•œ íŒ¨í„´ì´ ë°œê²¬ëœë‹¤ë©´ ì–´ë–¤ ê°€ì •ë“¤ì´ ìœ„ë°˜ë˜ì—ˆìŒì„ ì˜ë¯¸í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì—\u001dì„œ (a)ëŠ” ì„ í˜•ì„± ê°€ì •ì´ ë§Œì¡±ë˜ì§€ ì•Šì•˜ì„ ë•Œ ë‚˜íƒ€ë‚˜ëŠ” í”Œë¡¯ ì¤‘ í•˜ë‚˜ì´ë©°, ì´ ê²½ìš°ì—ëŠ” Y ë˜ëŠ” íŠ¹ì • ì˜ˆì¸¡ ë³€ìˆ˜ì— ëŒ€í•œ ë³€í™˜ì´ ì„ í˜•ì„±ì„ ìœ„í•˜ì—¬ í•„ìš”í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¼ (b)ëŠ” ì´ë¶„ì‚°ì„±ì„ ì˜ë¯¸í•˜ë©° ë¶„ì‚°ì˜ ì•ˆì •í™”ë¥¼ ìœ„í•˜ì—¬ ë°ì´í„° ë³€í™˜ì´ í•„ìš”í•  ê²ƒì´ë‹¤. ì”ì°¨ì™€ Xì˜ ì‚°ì ë„ë¥¼ ê·¸ë ¸ì„ ë•Œ ì”ì°¨ë“¤ì´ 2ì°¨ ê³¡ì„  ëª¨ì–‘ì„ ë³´ì¸ë‹¤ë©´ í•´ë‹¹ ë³€ìˆ˜ì˜ ì œê³±í•­ì„ ì¶”ê°€í•´ë³´ëŠ” ê²ƒì„ ê¶Œí•œë‹¤. (Standardized) Residual vs fitted-value plot í‘œì¤€ì ì¸ ê°€ì • í•˜ì—ì„œ í‘œì¤€í™”ì”ì°¨ëŠ” ì í•©ê°’ê³¼ë„ ìƒê´€ë˜ì–´ ìˆì§€ ì•Šë‹¤. ë”°ë¼ì„œ ì´ ê°€ì •ì´ ë§Œì¡±ëœë‹¤ë©´ ì´ plotì€ ëœë¤í•˜ê²Œ í©ì–´ì§„ ì ë“¤ì„ ë‚˜íƒ€ë‚´ì•¼ í•œë‹¤. ë‹¨ìˆœíšŒê·€ì—ì„œëŠ” (Standardized) Residual vs Predictor(ë…ë¦½ë³€ìˆ˜) ì‚°ì ë„ì™€ ë™ì¼í•œ íŒ¨í„´ì„ ê°€ì§„ë‹¤. í‘œì¤€í™”ì”ì°¨ì˜ ì¸ë±ìŠ¤ plot í‘œì¤€í™”ì”ì°¨ vs ê´€ì¸¡ê°œì²´ ë²ˆí˜¸ì˜ plotì´ë‹¤. ì•„ë˜ì™€ ê°™ì´ í•´ì„í•  ìˆ˜ ìˆìœ¼ë©°, ë§Œì¼ ê´€ì¸¡ê°œì²´ì˜ ì·¨í•´ì§„ ìˆœì„œê°€ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ë‹¤ë©´, (ì˜ˆì»¨ë°, ê°œì²´ê°€ ì‹œê°„ ë˜ëŠ” ê³µê°„ ìƒì˜ ìˆœì„œì— ë”°ë¼ ì·¨í•´ì¡Œì„ ë•Œ), ì—°ì†ì ì¸ ìˆœì„œì— ì˜í•œ ì”ì°¨ plotì€ ì˜¤ì°¨ì˜ ë…ë¦½ì„± ê°€ì •ì„ ê²€í† í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. ë…ë¦½ì„± ê°€ì • í•˜ì—ì„œ ì ë“¤ì€ 0 ì£¼ìœ„ì˜ ìˆ˜í‰ ë (ë°´ë“œ) ì•ˆì—ì„œ ëœë¤í•˜ê²Œ í©ì–´ì ¸ ìˆì–´ì•¼ í•œë‹¤. 2) íŠ¹ì´ê°’ê³¼ ì˜í–¥ë ¥ ìˆëŠ” ê°œì²´ë¥¼ ê²€ì¶œí•˜ê¸° ìœ„í•œ ê·¸ë˜í”„ì§€ë ˆì , ì˜í–˜ë ¥, íŠ¹ì´ê°’ ì£¼ì–´ì§„ ë°ì´í„°ì— ëª¨í˜•ì„ ì í•©í•¨ì— ìˆì–´ì„œ í•œë‘ ê°œì˜ ê´€ì¸¡ê°’ë“¤ì— ì˜í•´ ì í•©ì´ ê³¼ë„í•˜ê²Œ ê²°ì •ë˜ë©´ ë¶„ì„ì´ ì œëŒ€ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•Šì€ ê²ƒì´ë¯€ë¡œ ì´ëŸ° ê´€ì¸¡ê°’ë“¤ì€ ë³´í†µ ì”ì°¨ê°€ 0ì— ê°€ê¹ê±°ë‚˜ 0ì´ê¸° ë•Œë¬¸ì— íŠ¹ì´ê°’ì´ ì•„ë‹ˆë‚˜ ì˜í–¥ë ¥ìˆëŠ” ê°œì²´ì´ë‹¤. ì´ëŸ° ìƒí™©ì—ì„œëŠ” ì”ì°¨ë¥¼ ì‚´í´ë³´ëŠ” ê²ƒì€ ê±°ì˜ ë„ì›€ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤. ì–´ë–¤ ì ì´ ì œì™¸ë˜ì—ˆì„ ë•Œ í˜¼ìì„œ ë˜ëŠ” ë‹¤ë¥¸ ì ë“¤ê³¼ ê²°í•©í•˜ì—¬ ì í•©ëª¨í˜•(ì¶”ì •ëœ íšŒê·€ê³„ìˆ˜, ì í•©ê°’, t-í†µê³„ëŸ‰ ë“±)ì— í° ë³€í™”ë¥¼ ì¤€ë‹¤ë©´ ê·¸ ì ì„ ì˜í–¥ë ¥ ìˆëŠ” ì ì´ë¼ê³  í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì–´ë–¤ ì ì„ ì œì™¸í•˜ë©´ ì•½ê°„ì´ë¼ë„ ì í•©ì— ë³€í™”ê°€ ìˆì„ ê²ƒì´ë‹¤. ì—¬ê¸°ì—ì„œì˜ ê´€ì‹¬ì€ ê·¸ ì ì´ ê³¼ë„í•œ ì˜í–¥ë ¥ì´ ìˆëŠ”ê°€ì´ë‹¤.ë”°ë¼ì„œ, ì˜í–¥ë ¥ì´ ìˆëŠ” ê´€ì¸¡ê°œì²´ê°€ ë°ì´í„°ì— ì¡´ì¬í•œë‹¤ë©´ ê·¸ê²ƒì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì˜í–¥ë ¥ ìˆëŠ” ê°œì²´ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì¢…ì†(ë°˜ì‘)ë³€ìˆ˜ Y ë˜ëŠ” ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ X ê³µê°„ì— ëŒ€í•˜ì—¬ íŠ¹ì´ê°’ì´ë‹¤. ë°˜ì‘(ì¢…ì†) ë³€ìˆ˜ì— ëŒ€í•œ íŠ¹ì´ê°’ : ì”ì°¨ plotì„ í†µí•´ íŒŒì•…ë  ìˆ˜ ìˆìœ¼ë©°, ì”ì°¨ plotì€ ì¡´ì¬í•˜ëŠ” ì´ì²´ì ì¸ ëª¨í˜•ìœ„ë°˜ë“¤ì„ ë‚˜íƒ€ë‚¼ ê²ƒì´ë©°, ì”ì°¨ plotì˜ íƒìƒ‰ì€ ë¶„ì„ì—ì„œ ì£¼ìš” ë„êµ¬ ì¤‘ í•˜ë‚˜ì´ë‹¤. ë…ë¦½(ì„¤ëª…) ë³€ìˆ˜ì— ëŒ€í•œ íŠ¹ì´ê°’ : ì•ì—ì„œ ì„¤ëª…í•œ ì§€ë ˆê°’($p_{ii}$)ëŠ” X-ê³µê°„ì—ì„œ íŠ¹ì´ì„±ì„ ì¸¡ì •í•˜ëŠ” ë° ì´ìš©ë  ìˆ˜ ìˆë‹¤. í° ì§€ë ˆê°’ì„ ê°€ì§€ëŠ” ê´€ì¸¡ê°œì²´ëŠ” X-ê³µê°„ì—ì„œ íŠ¹ì´ê°’ì´ê¸° ë•Œë¬¸ì´ë‹¤. ë°˜ì‘ë³€ìˆ˜ì— ëŒ€í•œ íŠ¹ì´ê°’(í° í‘œì¤€í™”ì”ì°¨ë¥¼ ê°€ì§„ ì )ê³¼ êµ¬ë³„í•˜ê¸° ìœ„í•˜ì—¬ ë†’ì€ ì§€ë ˆì (high leverage point)ë¼ê³  í•œë‹¤. ìœ„ì˜ ë°˜ì‘ ë³€ìˆ˜ì— ëŒ€í•œ íŠ¹ì´ê°’ì€ ì”ì°¨ plotì„ í†µí•´ ì¶©ë¶„íˆ ì‚´í´ ë³¼ ìˆ˜ ìˆì§€ë§Œ, ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ì— ëŒ€í•œ íŠ¹ì´ê°’ì€ ì”ì°¨ plotìœ¼ë¡œëŠ” ì°¾ì•„ë³´ê¸° í˜ë“¤ë‹¤. ê·¸ ì´ìœ ëŠ” ì•„ë˜ ì”ì°¨ì™€ ì§€ë ˆê°’ì˜ ê´€ê³„ì— ëŒ€í•œ ì‹ì„ ì‚´í´ë³´ë©´ ë†’ì€ ì§€ë ˆê°’ì„ ê°–ëŠ” ì ë“¤ì€ ì”ì°¨ê°€ ë‚®ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, ì”ì°¨ plotì„ ì‚´í´ë³´ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ì˜ ì‚°ì ë„ì— íšŒê·€ì‹ì„ ê·¸ë ¤ë³´ê±°ë‚˜ ì§€ë ˆê°’ì˜ index plotì„ ê·¸ë ¤ ì‚´í´ ë´ì•¼ í•œë‹¤. í†µìƒì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” $p_{ii}$ì— ëŒ€í•œ ì„ê³„ê°’ì€ $2(p(\\sum p_{ii})+1)/n$ì´ë‹¤. ì¦‰, ì„ê³„ê°’ë“¤ì˜ í‰ê· ì˜ 2ë°°ë¥¼ ë„˜ìœ¼ë©´ ë†’ì€ ì§€ë ˆê°’ì„ ê°–ëŠ” ì ìœ¼ë¡œ íŒë‹¨í•œë‹¤. ë°ì´í„°ê°€ íŠ¹ì´ê°’ì„ ê°€ì§€ê³  ìˆìœ¼ë‚˜ ê·¸ê²ƒì„ ê²€ì¶œí•˜ì§€ ëª»í•˜ëŠ” ê²ƒì„ ê°€ë©´ë¬¸ì œ(masking problem)ì´ë¼ê³  í•œë‹¤. ì´ê²ƒì€ ì–´ë–¤ íŠ¹ì´ê°’ë“¤ì´ ë‹¤ë¥¸ íŠ¹ì´ê°’ë“¤ì— ì˜í•´ ìˆ¨ê²¨ì ¸ ìˆì„ë•Œ ë°œìƒí•  ìˆ˜ ìˆë‹¤. íŠ¹ì´ê°’ì´ ì•„ë‹Œ ì ì„ íŠ¹ì´ê°’ìœ¼ë¡œ ê°„ì£¼í•˜ëŠ” ê²ƒì„ ìˆ˜ë ë¬¸ì œ(swamping problem)ë¼ê³  í•œë‹¤. ì´ê²ƒì€ íŠ¹ì´ê°’ì´ íšŒê·€ì„ ì„ ìê¸°ìª½ìœ¼ë¡œ ëŒì–´ë‹¹ê²¨ì„œ ë‹¤ë¥¸ ì ë“¤ì„ ì í•©ì„ ìœ¼ë¡œë¶€í„° ë©€ê²Œ í•¨ìœ¼ë¡œì¨ ë°œìƒí•  ìˆ˜ ìˆë‹¤. p_{ii} + \\frac{e^2_{i}}{SSE} \\leq 1ì˜í–¥ë ¥ì˜ ì¸¡ë„ Cook&#39;s distance : ì „ì²´ ë°ì´í„°ë¡œë¶€í„° ì–»ì€ íšŒê·€ê³„ìˆ˜ë“¤ê³¼ ië²ˆì§¸ ê°œì²´ë¥¼ ì œê±°í•˜ê³  ì–»ì€ íšŒê·€ê³„ìˆ˜(ë˜ëŠ” ì í•©ê°’)ë“¤ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•œë‹¤. ë˜ëŠ” ë™ì¼í•˜ê²Œ ì „ì²´ ë°ì´í„°ë¡œë¶€í„° ì–»ì€ ì í•©ê°’ë“¤ê³¼ ië²ˆì§¸ ê°œì²´ë¥¼ ì œê±°í•˜ê³  ì–»ì€ ì í•©ê°’ë“¤ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•œë‹¤. ì•„ë˜ 2ë²ˆì§¸ ì‹ì„ ë³´ê²Œ ë˜ë©´, Cookâ€™s distanceëŠ” ê¸°ë³¸ì ì¸ ë‘ ê°’ì˜ ê³±ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. r_{i}ëŠ” ë‚´ì  í‘œì¤€í™”ì”ì°¨ë¥¼ ì˜ë¯¸í•˜ë©°, $\\frac{p_{ii}}{1-p_{ii}}$ëŠ” ì ì¬ì„± í•¨ìˆ˜ë¼ê³  ë¶ˆë¦°ë‹¤. ì–´ë–¤ ê´€ì¸¡ê°œì²´ì˜ ì˜í–¥ë ¥ì´ í¬ë‹¤ë©´ ê·¸ ê°œì²´ë¥¼ ì œì™¸í•  ë•Œ íšŒê·€ë¶„ì„ì˜ ê²°ê³¼ì— í° ë³€í™”ê°€ ì¼ì–´ë‚  ê²ƒì´ê³ , ì´ë•Œ Cook&#39;s distanceì˜ ê°’ì€ í¬ê²Œ ë  ê²ƒì´ë‹¤. ë”°ë¼ì„œ í° $C_{i}$ì˜ ê°’ì€ ê·¸ ì ì´ ì˜í–¥ë ¥ì´ ìˆìŒì„ ë‚˜íƒ€ë‚¸ë‹¤. Cê°’ì— ëŒ€í•œ index plotì„ ê·¸ë ¤ Cê°’ë“¤ì´ ë¹„ìŠ·í•œ ê°’ì„ ê°€ì§€ì§€ ì•Šë‹¤ë©´ ë‹ë³´ì´ëŠ” Cê°’ë“¤ì„ ê°–ëŠ” ë°ì´í„°ë“¤ì„ ì œì™¸í•˜ê³  ëª¨í˜•ì„ ì í•©ì— ë³´ëŠ” ë“±ì˜ ë°©ë²•ì„ ê²€í† í•´ ë´ì•¼í•  ê²ƒì´ë‹¤. ì—„ê²©í•œ ì„ê³„ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒë³´ë‹¤ëŠ” ìƒëŒ€ì ìœ¼ë¡œ í° ì˜í–¥ë ¥ì„ ê°€ì§€ëŠ” ê´€ì¸¡ê°œì²´ë“¤ì„ ê°€ë ¤ë‚´ëŠ” ì¸¡ë„ë¡œ í™œìš©í•˜ê¸°ë¥¼ ê¶Œí•œë‹¤. ë§Œì•½ ëª¨ë“  $C_{i}$ê°’ë“¤ì´ ë¹„ìŠ·í•œ ê°’ì„ ê°€ì§„ë‹¤ë©´ êµ³ì´ íŠ¹ë³„í•œ ì¡°ì¹˜ë¥¼ ì·¨í•  í•„ìš”ê°€ ì—†ì„ ê²ƒì´ë‹¤. ë°˜ë©´ì— ë‚˜ë¨¸ì§€ ì ë“¤ì— ë¹„í•˜ì—¬ íŠ¹ë³„íˆ ë‹ë³´ì´ëŠ” $C_{i}$ ê°’ì„ ê°€ì§€ëŠ” ë°ì´í„° ì ë“¤ì´ ìˆë‹¤ë©´ ì´ëŠ” ë©´ë°€í•˜ê²Œ ê²€í† ë  í•„ìš”ê°€ ìˆë‹¤. C_{i} = \\frac{sum^{n}_{j=1} (\\hat{y_{j}} - \\hat{y_{j}}_{i})^2}{\\hat{\\sigma}^{2} (p+1)} , i=1,2, \\cdots ,n= \\frac{r_{i}^{2}}{p+1} \\times \\frac{p_{ii}}{1-p_{ii}}, i =1,2, \\cdots, n. ì´ì™¸ì˜ Welsch &amp; Kuhì˜ ì¸¡ë„(DFITS)ì™€ Hadiì˜ ì˜í–¥ë ¥ ì¸¡ë„ê°€ ìˆìœ¼ë‚˜, Cookâ€™s distanceë¥¼ í†µí•´ ì¶©ë¶„íˆ ê²€ì‚¬ê°€ëŠ¥í•˜ë¯€ë¡œ ìƒëµí•˜ë„ë¡ í•œë‹¤. ë‹¤ë§Œ, Welsh &amp; Kuhì™€ Cookâ€™s distanceëŠ” ì”ì°¨ì™€ ì§€ë ˆê°’ì— ëŒ€í•œ ìŠ¹ë²•ì (ê³±í•˜ëŠ”)í•¨ìˆ˜ì¸ ë°˜ë©´ì— Haidì˜ ì¸¡ë„ëŠ” ê°€ë²•ì (ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ ê°ê°ì— ëŒ€í•œ ì˜í–¥ë ¥ì˜ ìˆ˜ì¹˜ë¥¼ ë”í•˜ëŠ”)í•¨ìˆ˜ì´ë‹¤. íŠ¹ì´ê°’ì€ ì–¸ì œë‚˜ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì¡°ì‚¬ë˜ì–´ì•¼ ë˜ë©° ì‹¤ë¬´ì—ì„œ ë¶„ì„ì‹œ í•¨ë¶€ë¡œ ì œê±°í•´ì„œëŠ” ì•ˆëœë‹¤. ê·¸ ë°ì´í„° ìì²´ë„ ì˜ë¯¸ê°€ ìˆì„ ìˆ˜ ìˆê¸° ë•Œë¬¸(ì˜ˆë¥¼ ë“¤ë©´, ë°ì´í„°ê°€ ëª¨ì§‘ë‹¨ìœ¼ë¡œ ë¶€í„° ì¶”ì¶œë˜ì§€ ì•Šì•˜ë‹¤ë“ ê°€ ë˜ëŠ” ëª¨í˜•ì´ ì„ í˜•ì´ ì•„ë‹ˆë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•  ìˆ˜ ìˆê¸° ë•Œë¬¸)ì´ë‹¤. ì§€ë ˆëŒ€ íš¨ê³¼ëŠ” ë†’ìœ¼ë‚˜ ì˜í–¥ë ¥ì´ ì‘ì€ ê²½ìš°ëŠ” í° ë¬¸ì œë¥¼ ì¼ìœ¼í‚¤ì§€ëŠ” ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜ ë†’ì€ ì§€ë ˆê°’ì„ ê°€ì§€ë©° ì˜í–¥ë ¥ì´ í° ì ë“¤ì€ ì˜ˆì¸¡ë³€ìˆ˜ë“¤ì˜ ê³µê°„ì—ì„œ ë³´í†µì˜ ê²ƒë“¤ì— ë¹„í•´ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆìœ¼ë©° ì í•©ì— ìœ ì˜ì ì¸ ì˜í–¥ì„ ë¼ì¹˜ê¸° ë•Œë¬¸ì— ì˜ ê²€í† í•  í•„ìš”ê°€ ìˆë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ íŠ¹ì´ê°’ì´ ì˜ë¯¸ìˆëŠ” ê²½ìš°ëŠ” ì§€ìˆ˜í•¨ìˆ˜ë¥¼ ë„ëŠ” ë°•í…Œë¦¬ì•„ì˜ ì¦ì‹ì„ ì˜ˆë¡œ ë“¤ ìˆ˜ ìˆë‹¤. ì¼ì • ì‹œê°„ ê¹Œì§€ëŠ” ê°œì²´ìˆ˜ê°€ ì„œì„œíˆ ì¦ê°€í•˜ë‹¤ê°€ ì–´ë–¤ ì‹œê°„ì˜ ì„ê³„ê°’ì„ ì§€ë‚˜ë©´ ê°œì²´ìˆ˜ê°€ í­ë°œì ìœ¼ë¡œ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì¦‰, Pê°’ë“¤ì˜ index plotê³¼ Cook&#39;s distanceì˜ index plotê³¼ ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ plotì„ ì¢…í•©í•´ì„œ ë¹„êµí•´ ë³´ë©´ì„œ ê°ê°ì˜ ì§€ë ›ê°’ì´ ë†’ì€ ë°ì´í„°ì™€ ì˜í–¥ë ¥ì´ ìˆëŠ” ê°’ì„ ì°¾ì•„ì•¼ í•  ê²ƒì´ë‹¤. criterionì—ëŠ” cooks ì™€ DFITSë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ê·¸ë˜í”„ì˜ í•´ì„ì€ ëª‡ê°€ì§€ ì£¼ì˜í•´ì•¼í•  ê´€ì¸¡ì¹˜ë“¤ì´ ìˆëŠ”ë°, contractorì™€ reporterëŠ” ë‚®ì€ Leverageë¥¼ ê°–ì§€\u001dë§Œ í° ì”ì°¨ë¥¼ ê°–ëŠ”ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. RR.engineerëŠ” ì‘ì€ ì”ì°¨ì™€ ë†’ì€ Leverageë¥¼ ê°–ëŠ”ë‹¤. Conductorì™€ ministerëŠ” ë‘˜ë‹¤ ëª¨ë‘ ë†’ì€ Leverageì™€ ë†’ì€ ì”ì°¨ë¥¼ ê°–ìœ¼ë¯€ë¡œ ì˜í–¥ë ¥ìˆëŠ” ê´€ì¸¡ì¹˜ì´ë‹¤. íŠ¹ì´ê°’(ì”ì°¨ê°€ í° ê´€ì¸¡ì¹˜)ê³¼ ì˜í–¥ë ¥ìˆëŠ” ê´€ì¸¡ê°œì²´(high leverage high residual)ë¥¼ ì‹ë³„í•˜ëŠ” ë° ìœ ìš©í•œ ë‹¤ë¥¸ ì ‘ê·¼ë°©ë²•ì€ ë¡œë²„ìŠ¤íŠ¸ íšŒê·€(robust regression)ì´ë‹¤. ë†’ì€ ì§€ë ˆê°’ì„ ê°€ì§€ëŠ” ê´€ì¸¡ê°œì²´ì— ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ê³  íšŒê·€ì§ì„ ì„ ì í•©ì‹œí‚¨ë‹¤. ë‹¤ìŒì— ë” ìì„¸í•œ ì„¤ëª…ì„ í•  ê²ƒì´ë‹¤. 3) ë³€ìˆ˜ë“¤ì˜ íš¨ê³¼ì— ëŒ€í•œ ì§„ë‹¨í”Œë¡¯ íšŒê·€ë°©ì •ì‹ì˜ ì–´ë–¤ ë³€ìˆ˜ë¥¼ ë³´ìœ í•´ì•¼ í•  ê²ƒì¸ì§€ ì•„ë‹ˆë©´ ì œê±°í•´ì•¼ í•  ê²ƒì´ì§€ë¥¼ ê°ê°ì˜ t-ê²€ì •ì— ëŒ€í•œ ë³´ì¡°ë„êµ¬ë¡œ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. ì²¨ê°€ë³€ìˆ˜ plot ë˜ëŠ” í¸íšŒê·€ plot(added-variable plot ë˜ëŠ” partial regression plot) íšŒê·€ ëª¨í˜•ì— ëŒ€í•œ íŠ¹ì • ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë¥¼ í¬í•¨ì‹œí‚¬ ê²ƒì¸ì§€ì˜ ì—¬ë¶€ë¥¼ ê²€í† í•  ë•Œ, ê·¸ ëŒ€ìƒì´ ë˜ëŠ” ì˜ˆì¸¡ ë³€ìˆ˜ì— ëŒ€í•œ íšŒê·€ê³„ìˆ˜ì˜ í¬ê¸°ë¥¼ ê·¸ë˜í”„ë¥¼ í†µí•˜ì—¬ í‘œí˜„í•œë‹¤. plotì— ë‚˜íƒ€ë‚˜ëŠ” ì ë“¤ì˜ ê¸°ìš¸ê¸°ëŠ” ê³§ í•´ë‹¹ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ì— ëŒ€í•œ íšŒê·€ê³„ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë”°ë¼ì„œ ì´ plotì— ë‚˜íƒ€ë‚œ ì ë“¤ì´ ëšœë ·í•œ ê¸°ìš¸ê¸°ë¥¼ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤ë©´ ì´ëŠ” ê·¸ ë³€ìˆ˜ê°€ ëª¨í˜•ì—ì„œ ë³„ë¡œ ìœ ìš©í•˜ì§€ ì•ŠìŒì„ ì˜ë¯¸í•œë‹¤. Xì¶•ì´ í•´ë‹¹ ì˜ˆì¸¡ë³€ìˆ˜ ê·¸ ìì²´ê°€ ì•„ë‹ˆë¯€ë¡œ ë¹„ì„ í˜•ì„±ì˜ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ì£¼ì§€ëŠ” ì•ŠëŠ” ì ì„ ì£¼ì˜í•´ì•¼ í•œë‹¤. ë˜í•œ ì´ plotì€ ê·¸ ê³„ìˆ˜ì˜ í¬ê¸°ë¥¼ ê²°ì •í•˜ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ë°ì´í„° ì ì„ ì œì‹œí•´ ì£¼ê¸°ë„ í•œë‹¤. ì²¨ê°€ë³€ìˆ˜ plotì€ Y-ì”ì°¨ $(X_{j}$ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ë¡œ ì„¤ëª…ë˜ì§€ ì•Šì€ Yì˜ ë¶€ë¶„) vs $X_{j}$-ì”ì°¨$(X_{j}$ë¥¼ ì¢…ì†ë³€ìˆ˜ë¡œí•˜ì—¬ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ë“¤ë¡œ ì„¤ëª…ë˜ì§€ ì•Šì€ $X_{j}$ì˜ ë¶€ë¶„)ì„ ê·¸ë¦¬ëŠ” plotì´ë‹¤. ì´ ë‘ê°œì˜ ì”ì°¨ë“¤ì„ ìµœì†Œ ì œê³±ë²•ìœ¼ë¡œ ì í•©ì‹œì¼°ì„ ë•Œ, ì í•©ëœ íšŒê·€ì§ì„ ì˜ ê¸°ìš¸ê¸°ëŠ” Xjë¥¼ í¬í•¨í•œ ëª¨ë“  ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ ì–»ì€ íšŒê·€ê³„ìˆ˜($\\hat{\\beta}_{j}$)ì™€ ê°™ë‹¤. ê°œë³„ë¡œ ê·¸ë¦¬ëŠ” ê²ƒì€ indexë¡œ ì‹ë³„ì„ í•  ìˆ˜ ìˆì§€ë§Œ ì•„ë˜ì— ì—¬ëŸ¬ê°€ì§€ë¥¼ í•œ êº¼ë²ˆì— ê·¸ë¦¬ëŠ” ë°©ë²•ì€ ì¸ë±ìŠ¤ë¥¼ ë³¼ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì²˜ìŒì—ëŠ” ì—¬ëŸ¬ê°œë¥¼ ë‹¤ ê°™ì´ ê·¸ë¦° í›„ì— ìì„¸íˆ ì‚´í´ë´ì•¼í•  ë³€ìˆ˜ì— ëŒ€í•´ì„œë§Œ ê°œë³„ë¡œ ê·¸ë¦¬ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒì´ë‹¤. ì„±ë¶„ì”ì°¨ plot(component plus residual plot) íšŒê·€ ë¶„ì„ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ê·¸ë˜í”„ì  ê¸°ë²• ì¤‘ì˜ í•˜ë‚˜ì´ë‹¤. $(e + \\hat{\\beta}_{j} X_{j} VS X_{j})$ì— ëŒ€í•œ ì‚°ì ë„ì´ë‹¤. $\\hat{\\beta}_{j} X_{j}$ì€ jë²ˆì§¸ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ê°€ ì í•©ê°’ì— ê¸°ì—¬í•˜ëŠ” ê³µí—Œë„(ì„±ë¶„)ì„ì„ ì£¼ëª©í•˜ì. ì´ plotì—ì„œ ê¸°ìš¸ê¸°ëŠ” í•´ë‹¹ ë…ë¦½ë³€ìˆ˜ì— ëŒ€í•œ ì¶”ì • íšŒê·€ê³„ìˆ˜ë¥¼ ì˜ë¯¸í•˜ë©°, í•´ë‹¹ ì˜ˆì¸¡ë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ë³´ì—¬ ì¤„ ë¿ ì•„ë‹ˆë¼ ì¢…ì†ë³€ìˆ˜ì™€ í•´ë‹¹ ë…ë¦½ë³€ìˆ˜ì‚¬ì´ì˜ ë¹„ì„ í˜•ì„±ì˜ ì¡´ì¬ë„ ì•Œë ¤ì¤Œìœ¼ë¡œì¨ í•„ìš”í•  ê²½ìš° ë…ë¦½ë³€ìˆ˜ì— ê´€í•œ êµ¬ì²´ì ì¸ ì„ í˜•ë³€í™˜ì˜ ë‚´ìš©ê¹Œì§€ë„ ì œì‹œí•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ ë˜í•œ, added-variable plotì²˜ëŸ¼ ì—¬ëŸ¬ê°œë¥¼ ê·¸ë ¤ë³¸ ë’¤ í•„ìš”í•œ ë³€ìˆ˜ì— ëŒ€í•´ì„œë§Œ ì‚´í´ë³´ëŠ” ê²ƒì„ ì¶”ì²œ. component plus residual plot vs added-variable plot ë‘ ê·¸ë˜í”„ ëª¨ë‘ íšŒê·€ê³„ìˆ˜ì— ëŒ€í•œ ì¶”ì •ì¹˜ë¥¼ ê¸°ìš¸ê¸°ë¡œ ë³´ì—¬ì£¼ì§€ë§Œ, added-variable plotì€ ì–´ë–¤ ë°ì´í„°ê°€ íšŒê·€ê³„ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ”ë° ë§ì€ ì˜í–¥ì„ ì£¼ì—ˆëŠ”ì§€ë¥¼ ì•Œ ìˆ˜ ìˆê²Œ ë„ì™€ì¤€ë‹¤. ë°˜ë©´ì—, component plus residual plotì€ added-variable plotë³´ë‹¤ íŠ¹ì • ë…ë¦½ë³€ìˆ˜ë¥¼ íšŒê·€ëª¨í˜•ì— ë„ì…í•´ì•¼ í•˜ëŠëƒ í•˜ëŠ” ë¬¸ì œì— ëŒ€í•œ ë‹µì´ë‚˜ ê·¸ ë…ë¦½ë³€ìˆ˜ê°€ ê°€ì§€ëŠ” ë¹„ì„ í˜•ì„±ì˜ ì—¬ë¶€ë¥¼ íƒìƒ‰í•˜ëŠ” ë° ë” ë¯¼ê°í•œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤. ì¶”ê°€ì ì¸ ì˜ˆì¸¡ë³€ìˆ˜ì˜ íš¨ê³¼ íšŒê·€ì‹ì— ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ë„ì…í•˜ëŠ” ê²ƒì˜ íš¨ê³¼ì— ëŒ€í•˜ì—¬ ë‹¤ìŒì˜ ë‘ê°€ì§€ ì§ˆë¬¸ì„ ê³ ë ¤í•´ì•¼í•  ê²ƒì´ë‹¤. (a) ìƒˆë¡œìš´ ë³€ìˆ˜ì˜ íšŒê·€ê³„ìˆ˜ê°€ ìœ ì˜í•œê°€? (b)ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ë„ì…í•¨ìœ¼ë¡œì¨ íšŒê·€ì‹ì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆëŠ” ë³€ìˆ˜ë“¤ì˜ íšŒê·€ê³„ìˆ˜ë¥¼ ìœ ì˜í•˜ê²Œ ë³€í™”ì‹œí‚¤ëŠ”ê°€? ì´ ë‘ê°€ì§€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µìœ¼ë¡œ í¬ê²Œ 4ê°€ì§€ ìœ í˜•ì´ ìˆì„ ìˆ˜ ìˆë‹¤. 1) ìƒˆë¡œìš´ ë³€ìˆ˜ê°€ ìœ ì˜í•˜ì§€ ì•Šì€ íšŒê·€ê³„ìˆ˜ë¥¼ ê°€ì§€ë©°, ë‹¤ë¥¸ íšŒê·€ê³„ìˆ˜ë“¤ì€ ì´ì „ì˜ ê°’ì— ë¹„í•´ ê±°ì˜ ë³€í™”ê°€ ì—†ë‹¤. ì–´ë–¤ ë‹¤ë¥¸ ì™¸ë¶€ì ì¸ ì¡°ê±´(ì˜ˆì»¨ëŒ€, ì´ë¡  ë˜ëŠ” ì£¼ì œì— ëŒ€í•œ ê³ ë ¤)ì— ì˜í•˜ì—¬ í•„ìš”ì„±ì´ ìˆì§€ ì•Šë‹¤ë©´, ìƒˆë¡œìš´ ë³€ìˆ˜ëŠ” íšŒê·€ì‹ì— í¬í•¨ë˜ì§€ ì•Šì•„ì•¼ í•œë‹¤. 2) ìƒˆë¡œìš´ ë³€ìˆ˜ê°€ ìœ ì˜í•œ íšŒê·€ê³„ìˆ˜ë¥¼ ê°€ì§€ë©°, ì´ì „ì— ë„ì…ëœ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì˜ íšŒê·€ê³„ìˆ˜ì— í° ë³€í™”ê°€ ìˆë‹¤. ì´ ê²½ìš° ìƒˆë¡œìš´ ë³€ìˆ˜ê°€ ìœ ì§€ë˜ì–´ì•¼ í•˜ë©°, ê·¸ëŸ¬ë‚˜ ê³µì„ ì„±ì— ëŒ€í•œ íƒìƒ‰ì´ í•„ìš”í•˜ë‹¤. ê³µì„ ì„±ì˜ ì¦ê±°ê°€ ì—†ë‹¤ë©´, ê·¸ ë³€ìˆ˜ëŠ” ë°©ì •ì‹ì— í¬í•¨ë˜ì–´ì•¼ í•˜ë©° ë‹¤ë¥¸ ì¶”ê°€ì ì¸ ë³€ìˆ˜ì˜ ë„ì…ì— ëŒ€í•œ íƒìƒ‰ì´ ìˆ˜í–‰ë˜ì–´ì•¼ í•œë‹¤. 3) ìƒˆë¡œìš´ ë³€ìˆ˜ê°€ ìœ ì˜í•œ íšŒê·€ê³„ìˆ˜ë¥¼ ê°€ì§€ë©°, ë‹¤ë¥¸ íšŒê·€ê³„ìˆ˜ë“¤ì€ ì´ì „ì˜ ê°’ì— ë¹„í•˜ì—¬ í° ë³€í™”ê°€ ì—†ë‹¤. ì´ê²ƒì€ ì´ìƒì ì¸ ìƒí™©ì´ë©° ìƒˆë¡œìš´ ë³€ìˆ˜ê°€ ì´ì „ì— ë„ì…ëœ ë³€ìˆ˜ë“¤ê³¼ ìƒê´€ë˜ì–´ ìˆì§€ ì•Šì„ ë•Œ ë°œìƒí•œë‹¤. ì´ ê²½ìš° ìƒˆë¡œìš´ ë³€ìˆ˜ëŠ” ë°©ì •ì‹ì— í¬í•¨ë˜ì–´ì•¼í•œë‹¤. 4) ìƒˆë¡œìš´ ë³€ìˆ˜ê°€ ìœ ì˜í•˜ì§€ ì•Šì€ íšŒê·€ê³„ìˆ˜ë¥¼ ê°€ì§€ë©°, ì´ì „ì— ë„ì…ëœ ë‹¤ë¥¸ íšŒê·€ê³„ìˆ˜ì— í° ë³€í™”ê°€ ìˆë‹¤. ì´ê²ƒì€ ëª…ë°±í•œ ê³µì„ ì„±ì˜ ì¦ê±°ì´ë©°, íšŒê·€ì‹ì— ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ í¬í•¨ì‹œí‚¬ ê²ƒì¸ì§€ ì•„ë‹ˆë©´ ì œ ì œì™¸ì‹œí‚¬ ê²ƒì¸ì§€ë¥¼ ê²°ì •í•˜ê¸° ì „ì— ìˆ˜ì •ì‘ì—…ì´ ì·¨í•´ì ¸ì•¼ í•œë‹¤. ë³€ìˆ˜ë³€í™˜ ì˜ˆë¥¼ ë“¤ë©´, ì•„ë˜ ì²«ë²ˆì§¸ Y(ì¢…ì†ë³€ìˆ˜)ì— logë¥¼ ì·¨í•˜ê±°ë‚˜ rootë¥¼ ì”Œìš°ëŠ” ê²ƒì€ ì¢…ì†ë³€ìˆ˜ì™€ ë°˜ì‘ ë³€ìˆ˜ì˜ ì‚°ì ë„ë¥¼ ì‚´í´ë³´ë©° ê·¸ì— ë§ëŠ” íŒ¨í„´ì´ ê·¸ë ¤ì§€ë©´ ì‚¬ìš©í•´ì•¼ ë  ê²ƒì´ë‹¤. ë¬¼ë¡  ê·¸ ì™¸ì—ë„ Y(ì¢…ì†ë³€ìˆ˜)ì˜ ë‹¨ìœ„ë¥¼ ì¤„ì´ëŠ” ê²½ìš°ë‚˜ ë²”ìœ„ë¥¼ ì œí•œí•˜ëŠ” ê²½ìš°ì—ë„ ì‚¬ìš©í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. íŠ¹íˆ ë¡œê·¸ë³€í™˜ì€ íšŒê·€ë¶„ì„ì—ì„œ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ë³€í™˜ ì¤‘ì˜ í•˜ë‚˜ì´ë‹¤. ë¶„ì„ëŒ€ìƒ ë³€ìˆ˜ê°€ í‰ê· ì— ë¹„í•´ í° í‘œì¤€í¸ì°¨ë¥¼ ê°€ì¡Œì„ ê²½ìš°ì— íŠ¹íˆ ìœ ìš©í•œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤. ì›ë˜ ë³€ìˆ˜ë¥¼ ë¡œê·¸ì²™ë„ë¡œ ë³€í™˜ì‹œí‚¤ë©´ ì´ëŠ” ê·¸ì˜ ë³€ì´ë¥¼ ë¬´ë””ê²Œ í•˜ëŠ” ë™ì‹œì— ë¹„ëŒ€ì¹­ì„±ì„ ì¤„ì´ëŠ” íš¨ê³¼ê°€ ìˆë‹¤. ë˜í•œ ì´ë¶„ì‚°ì„±ì„ ì œê±°í•˜ëŠ” ë°ë„ íš¨ê³¼ì ì´ë‹¤. ì”ì°¨ì— ê´€ë ¨ëœ í”Œë¡¯ë“¤ì„ ê·¸ë ¤ë³¸ í›„ ê°€ì •ì´ ìœ„ë°°ë˜ì—ˆë‹¤ë©´, ì—¬ëŸ¬ ë°©ë²•ë“¤ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•´ë³´ì•„ì•¼ í•  ê²ƒì´ë‹¤. ì„ í˜•ì„± ë˜ëŠ” ì •ê·œì„±ì˜ í™•ë³´, ë¶„ì‚°ì˜ ì•ˆì •í™”ì™€ ê°™ì€ ì–´ë–¤ ëª©ì ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•˜ì—¬ ë³€í™˜ì´ ì ìš©ëœë‹¤. ì–´ë–¤ ê²½ìš°ì—ëŠ” ì›ë˜ì˜ ë³€ìˆ˜ë“¤ë³´ë‹¤ëŠ” ë³€í™˜ëœ ë³€ìˆ˜ë“¤ì— ì„ í˜•íšŒê·€ëª¨í˜•ì„ ì í•©í•  í•„ìš”ë„ ìˆëŠ”ë°, ì‹¤ì œ ë¬¸ì œì—ì„œ ì´ëŸ° ê²½ìš°ëŠ” ë§¤ìš° í”í•˜ë‹¤. ë‹¤ì¤‘íšŒê·€ì—ì„œì˜ ë³€í™˜ì—ëŠ” ë” ë§ì€ ë…¸ë ¥(ì—¬ëŸ¬ ë³€í™˜ì„ í†µí•´ ë§ëŠ” ë³€ìˆ˜ë¥¼ ì°¾ì•„ì•¼í•˜ë¯€ë¡œ) ì£¼ì˜(ë°˜ì‘(ì¢…ì†)ë³€ìˆ˜ì™€ ì„¤ëª…(ë…ë¦½)ë³€ìˆ˜ì˜ ì‚°ì ë„ë¥¼ ê·¸ë ¤ì„œ ë³€í™˜ì˜ íŒíŠ¸ë¥¼ ì–»ì„ ìˆ˜ ì—†ìœ¼ë©°, ê°ê°ì˜ ì‚°ì ë„ë¥¼ ê·¸ë ¤ì„œ ë³€í™˜ì„ ì§„í–‰í•˜ì˜€ì–´ë„ ì „ì²´ì ì¸ ì„¤ëª…ë³€ìˆ˜ë“¤ì˜ ê³µê°„ìƒì—ì„œëŠ” ë§ì§€ ì•ŠëŠ” ê²½ìš°ë„ ìˆê¸° ë•Œë¬¸)ê°€ í•„ìš”í•˜ë‹¤. ê°€ì¥ í”í•˜ê²Œ ìœ„ë°˜ë˜ëŠ” ê°€ì •ì€ ê³ ë ¤í•˜ëŠ” ëª¨í˜•ì˜ ì„ í˜•ì„±ê³¼ ì˜¤ì°¨ë¶„ì‚°ë“¤ì˜ ë™ì¼ì„±ì´ë‹¤. ë‹¤ì‹œ í•œë²ˆ ë§í•˜ì§€ë§Œ, íšŒê·€ë¶„ì„ì—ì„œì˜ ì„ í˜•ì´ë¼í•¨ì€ ë³€ìˆ˜ë“¤ ê°„ì˜ ì„ í˜•ê´€ê³„ë¼ê¸° ë³´ë‹¤ëŠ” ê°€ì • ëœ íšŒê·€ëª¨í˜•ì´ íšŒê·€ëª¨ìˆ˜ì— ëŒ€í•´ ì„ í˜•ëª¨ìˆ˜ì— ëŒ€í•´ ì„ í˜•ì ì„ì„ ì˜ë¯¸í•˜ëŠ” ê²ƒì´ë‹¤. ì•„ë˜ ëª¨í˜•ë“¤ì€ ëª¨ë‘ ì„ í˜•ì´ë‹¤. ì•„ë˜ì˜ ëª¨í˜•ë“¤ì€ ëª¨ë‘ íšŒê·€ëª¨ìˆ˜ $\\beta_{0}, \\beta_{1}, \\beta_{2}$ì— ëŒ€í•´ ì„ í˜•ì´ê¸° ë•Œë¬¸ì— ì„ í˜•ì„±ì„ ê°–ëŠ”ë‹¤. Y = \\beta_{0} + \\beta_{1}X + \\varepsilonY = \\beta_{0} + \\beta_{1}X + \\beta_{2} X^{2} + \\varepsilonY = \\beta_{0} + \\beta_{1} log X + \\varepsilonY = \\beta_{0} + \\beta_{1} \\sqrt{X} + \\varepsilon ë°˜ë©´ì— ì•„ë˜ ëª¨í˜•ì€ ëª¨ìˆ˜ $\\beta_{1}$ì— ëŒ€í•´ ì„ í˜•ê¼´ì´ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ë¹„ì„ í˜•ëª¨í˜•ì´ ëœë‹¤. Y = \\beta_{0} + e^{\\beta_{1}X} + \\varepsilon ë³€ìˆ˜ ë³€í™˜ì´ í•„ìš”í•˜ê²Œ ë˜ëŠ” ëª‡ ê°€ì§€ ì´ìœ ë“¤ì„ ë‹¤ì‹œ ìš”ì•½í•´ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. 1) ì„ í˜•ì„±ì„ ìœ„í•œ ë³€í™˜ë“¤ ex) í•™ìŠµì´ë¡ (ì‹¤í—˜ì‹¬ë¦¬) ë¶„ì•¼ì˜ í•œ ì˜ˆë¡œì„œ ì–´ë–¤ Taskë¥¼ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜í–‰í•  ë•Œ ië²ˆì§¸ ìˆ˜í–‰ì—ì„œ ê±¸ë¦¬ëŠ” ì‹œê°„ $T_{i}$ëŠ” ì´ë¡ ì ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ ê´€ê³„ë¥¼ ê°€ì§„ë‹¤ê³  í•˜ì. ì´ ê²½ìš° $T_{i}$ì™€ $i$ì˜ ê´€ê³„ëŠ” ëª¨ìˆ˜ $\\alpha$, $\\beta$ì— ê´€í•´ ë¹„ì„ í˜•ì´ ë˜ë¯€ë¡œ ì„ í˜•íšŒê·€ë¶„ì„ì—ì„œì˜ ê¸°ë²•ì„ ì§ì ‘ ì ìš©í•  ìˆ˜ ì—†ë‹¤. T_{i} = \\alpha \\beta^{i}, \\alpha > 0, 0 < \\beta < 1 ë°˜ë©´ì— ìœ„ ëª¨í˜•ì˜ ì–‘ë³€ì— ë¡œê·¸ë¥¼ ì·¨í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ì‹ì„ ì–»ê²Œ ë˜ëŠ”ë°, $Y_{i} = log T^{i}$, $\\beta_{0} = log \\alpha, \\beta_{1} = log \\beta$, $X_{i} = i$ë¡œ ë³€í™˜ì‹œí‚¤ë©´ $Y_{i} = \\beta_{0} + \\beta_{1}X_{1}$ê°€ ë˜ì–´ ëª¨ìˆ˜ $\\beta_{0}$, $\\beta_{1}$ì— ê´€í•´ ì„ í˜•ê¼´ì´ë¯€ë¡œ, ì´ì œ ì—¬ê¸°ì— í‘œì¤€ì ì¸ íšŒê·€ë°©ë²•ë“¤ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ì¦‰, ì›ë˜ ë³€ìˆ˜ë“¤ì˜ ê´€ê³„ëŠ” ë¹„ì„ í˜•ì´ì§€ë§Œ ë³€í™”ëœ ë³€ìˆ˜ë“¤ ì‚¬ì´ì˜ ê´€ê³„ëŠ” ì„ í˜•ì´ë‹¤. ì„ í˜•íšŒê·€ë¶„ì„ì— ìˆ˜ë°˜ëœ ê¸°ë³¸ê°€ì •ë“¤ ì¤‘ì˜ í•˜ë‚˜ëŠ” ë°ì´í„°ë¥¼ ë¬˜ì‚¬í•˜ëŠ” íšŒê·€ëª¨í˜•ì˜ í˜•íƒœê°€ ë³€ìˆ˜ë“¤ ê°„ì˜ ì„ í˜•ì  ê´€ê³„ë¥¼ ê°€ì ¸ì•¼ í•˜ë©°, ë™ì‹œì— íšŒê·€ëª¨ìˆ˜ì— ê´€í•´ ì„ í˜•ì ì´ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë¡ ì ì¸ ê·¼ê±°ë¡œë¶€í„° ë˜ëŠ” Yì™€ ì„¤ëª…(ë…ë¦½)ë³€ìˆ˜ Xì˜ ì‚°ì ë„ë¥¼ ê²€í† í•¨ìœ¼ë¡œì¨ ë‘ ë³€ìˆ˜ì˜ ê´€ê³„ê°€ ë¹„ì„ í˜•ì ì„ì„ ì•Œê²Œ ë  ê²½ìš°ê°€ ìˆë‹¤. ë°˜ì‘ë³€ìˆ˜ì™€ ì„¤ëª…ë³€ìˆ˜ì˜ ì‚°ì ë„ë¥¼ ê·¸ë ¸ì„ ê²½ìš° ì•„ë˜ì™€ ê°™ì€ ê·¸ë˜í”„ì˜ íŒ¨í„´ì„ ë‚˜íƒ€ë‚¸ë‹¤ë©´, í•´ë‹¹í•˜ëŠ” ë³€í™˜ì„ ì‚¬ìš©í•´ ë³´ê¸¸ ê¶Œí•œë‹¤. í•¨ìˆ˜ ë³€í™˜ ì„ í˜• í˜•íƒœ ê·¸ë¦¼ $Y = \\alpha X^{\\beta}$ $Yâ€™ = log Y, Xâ€™ = log X$ $Yâ€™ = log \\alpha + \\beta Xâ€™$ ê·¸ë¦¼ 1-1 $Y = \\alpha e^{\\beta X}$ $Yâ€™ = ln Y$ $Yâ€™ = ln \\alpha + \\beta X$ ê·¸ë¦¼ 1-2 $Y = \\alpha + \\beta log X$ $Xâ€™ = log X$ $Y = \\alpha + \\beta Xâ€™$ ê·¸ë¦¼ 1-3 $Y = \\frac{X}{\\alpha X + \\beta}$ $Yâ€™ = \\frac{1}{Y}, Xâ€™ = \\frac{1}{X}$ $Yâ€™ = \\alpha - \\beta Xâ€™$ ê·¸ë¦¼ 1-4(a) $Y = \\frac{e^{\\alpha + \\beta X}}{1 + e^{\\alpha + \\beta X}}$ $Yâ€™ = ln \\frac{Y}{1-Y}$ $Yâ€™ = \\alpha + \\beta X$ ê·¸ë¦¼ 1-4(b) 2) ë¶„ì‚°ì•ˆì •í™”ë¥¼ ìœ„í•œ ë³€í™˜ ë°˜ì‘(ì¢…ì†)ë³€ìˆ˜ Yì— ëŒ€í•œ ë¶„ì‚°ì´ í‰ê· ì— ì˜ì¡´í•˜ëŠ” í™•ë¥ ë¶„í¬ë¥¼ ê°€ì§€ëŠ” ê²½ìš°ê°€ ìˆë‹¤. í†µìƒ ë°˜ì‘(ì¢…ì†)ë³€ìˆ˜ Yì˜ í‰ê· ì´ ì„¤ëª…(ë…ë¦½)ë³€ìˆ˜ Xì™€ ê´€ê³„ë¥¼ ê°€ì§€ê²Œ ë˜ë¯€ë¡œ, ë§Œì¼ Yì˜ ë¶„ì‚°ì´ Yì˜ í‰ê· ì— ì˜ì¡´í•œë‹¤ë©´ ë” ì´ìƒ ìƒìˆ˜ê°€ ì•„ë‹ˆê³  Xì— ë”°ë¼ ë³€í•˜ê²Œ ë  ê²ƒì´ë‹¤. ì´ëŸ° ê²½ìš° YëŠ” ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ°ë° íšŒê·€ë¶„ì„ì—ì„œ ëŒ€ë¶€ë¶„ì˜ ê¸°ë³¸ì ì¸ ìœ ì˜ì„± ê²€ì •ì€ Y(í˜¹ì€ ì˜¤ì°¨í•­)ì˜ ì •ê·œì„± ê°€ì • í•˜ì—ì„œ ì„±ë¦½í•˜ë¯€ë¡œ ì´ì™€ ê°™ì€ ë¹„ì •ê·œì„±ì€ ë¬¸ì œê°€ ë  ìˆ˜ ìˆë‹¤(í‘œë³¸ì´ í° ê²½ìš°ì—ëŠ” ì£¼ë¬¸ì œê°€ ì•„ë‹ ìˆ˜ ìˆì§€ë§Œ). íšŒê·€ëª¨í˜•ì˜ ê¸°ë³¸ê°€ì • ì¤‘ í•˜ë‚˜ì¸ ë“±ë¶„ì‚°ì„±ì„ ìœ„ë°˜í•˜ëŠ” ê²½ìš°ì´ë¯€ë¡œ, íšŒê·€ëª¨ìˆ˜ì— ëŒ€í•œ ì¶”ì •ëŸ‰ë“¤ì€ ê·¸ì˜ ë¶ˆí¸ì„±(unbiasedness)ì€ ìœ ì§€ë˜ì§€ë§Œ ì •í™•ì„±(precision)ì˜ ê´€ì ì—ì„œëŠ” ë” ì´ìƒ ìµœì  ì¶”ì •ëŸ‰ì´ ì•„ë‹ˆë‹¤. ì´ ë•Œ ë³€ìˆ˜ë³€í™˜ ê¸°ë²•ì„ ì´ìš©í•˜ì—¬ ë°ì´í„°ê°€ ë“±ë¶„ì‚°ì„±ê³¼ ì •ê·œì„±ì„ ë™ì‹œì— ê°€ì§€ë„ë¡ í•  ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ ì˜¤ì°¨í•­ì´ ë“±ë¶„ì‚°ì„±ì„ ê°€ì§€ë„ë¡ í•˜ëŠ” ë¶„ì‚°ì•ˆì •í™” ë³€í™˜(variance-stabilizing transformation)ì€ ë§ì€ ê²½ìš°ì— ì •ê·œì„±ë„ ì–´ëŠ ì •ë„ ë§Œì¡±ì‹œí‚¤ê²Œ í•œë‹¤ëŠ” ì‚¬ì‹¤ì´ ì•Œë ¤ì ¸ ìˆë‹¤. ë³€ìˆ˜ë³€í™˜ì€ ì˜¤ì°¨í•­ì˜ ë¶„ì‚°ì„ ì•ˆì •ì‹œì¼œ ë“±ë¶„ì‚°ì„±(homoscedasticity)ì„ ìœ ì§€í•  ëª©ì ìœ¼ë¡œ í™œìš©ë  ìˆ˜ ìˆë‹¤. ì˜¤ì°¨í•­ì˜ ë¶„ì‚°ì´ ëª¨ë“  ê´€ì¸¡ê°’ì— ëŒ€í•˜ì—¬ ë˜‘ê°™ì€ ìƒìˆ˜ê°’ì„ ì·¨í•˜ì§€ ì•Šì„ ê²½ìš° ì˜¤ì°¨í•­ì€ ì´ë¶„ì‚°ì„±ì„ ê°€ì§„ë‹¤ê³  ë§í•œë‹¤. ì”ì°¨ë“¤ì˜ ë³€ì´ê°€ Xì˜ ê°’ì— ë”°ë¼ì„œ ì ì  ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ê¹”ë•Œê¸° í˜•íƒœì˜ ë¶„í¬ë¥¼ ê°€ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤. ì´ë¶„ì‚°ì„±ì´ ì¡´ì¬í•˜ë©´, ì› ë°ì´í„°ì— ìµœì†Œì œê³±ì¶”ì •ë²•ì„ ì ìš©í•˜ëŠ” ê²ƒì´ ì´ë¡ ì  íƒ€ë‹¹ì„±ì„ ë³´ì¥ë°›ì§€ ëª»í•˜ë©°, íšŒê·€ê³„ìˆ˜ì˜ ì¶”ì •ì¹˜ë‚˜ ê·¸ì˜ í‘œì¤€ì˜¤ì°¨ë„ ì •í™•ì„±ì˜ ê´€ì ì—ì„œ ì‹ ë¢°í•  ìˆ˜ ì—†ê²Œ ëœë‹¤. íšŒê·€ ë¶„ì„ì— ìˆì–´ì„œ ë°˜ì‘ ë³€ìˆ˜ Yì˜ ë¶„ì‚°ì´ ê·¸ì˜ í‰ê· ê°’ì˜ í•¨ìˆ˜ì¸ ê²½ìš°ê°€ ìˆë‹¤. ì•„ë˜ í‘œì™€ ê°™ì€ ë¶„í¬ë“¤ì´ ëŒ€í‘œì ì´ë‹¤. ë¶„ì‚°ì•ˆì •í™”ë¥¼ ìœ„í•œ ë³€ìˆ˜ ë³€í™˜ë“¤ì€ ë¶„ì‚°ì„ ì•ˆì •ì‹œí‚¬ ë¿ë§Œ ì•„ë‹ˆë¼ ë³€í™˜ëœ ë³€ìˆ˜ì˜ ë¶„í¬ê°€ ì •ê·œë¶„í¬ì— ê°€ê¹ë„ë¡ í•˜ëŠ” ì´ì¤‘ íš¨ê³¼ë„ ê°€ì§€ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ ìˆë‹¤. í¬ì•„ì†¡ë¶„í¬ë¥¼ ë”°ë¥¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ì€ ì‚¬ê±´ë“¤ì€ ì˜ˆë¥¼ ë“¤ì–´ êµí†µì‚¬ê³ ìˆ˜ ê°™ì€ ì‚¬ê±´ì„ ì˜ˆë¡œ ë“¤ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. Yì˜ í™•ë¥  ë¶„í¬ Yì˜ í‰ê·  $\\mu$ì— ë”°ë¥¸ Yì˜ ë¶„ì‚° ë³€í™˜ ë¶„ì‚°ì˜ ê²°ê³¼ í¬ì•„ì†¡ $\\mu$ $\\sqrt{Y} ë˜ëŠ” (\\sqrt{Y} + \\sqrt{Y + 1})$ 0.25 ì´í•­ $\\mu(1-\\mu)/n$ $sin^{-1} \\sqrt{Y}(ë¼ë””ì•ˆ)$ 0.25/n ìŒì´í•­ $\\mu + \\lambda^{2} \\mu^{2}$ $\\lambda^{-1} sinh^{-1}(\\lambda \\sqrt{Y}) ë˜ëŠ” \\lambda^{-1} sinh^{-1}(\\lambda \\sqrt{Y} + 0.5)$ 0.25 3) ì‹¤ì œ ë³€í™˜ì˜ í•„ìš”ì„±ê³¼ ë°©ë²•ì€ ì£¼ë¡œ ëª¨í˜•ì í•©ì˜ ê³¼ì •ì—ì„œ êµ¬í•œ ì”ì°¨ë¥¼ ê²€í† í•¨ìœ¼ë¡œì¨ ì•Œ ìˆ˜ ìˆë‹¤. ì˜¤ì°¨í•­ì˜ ë¶„ì‚°ì´ ìƒìˆ˜ê°€ ì•„ë‹ ë¿ë”ëŸ¬, ì´ì— ëŒ€í•´ ì·¨í•  ìˆ˜ ìˆëŠ” ì ì ˆí•œ ë³€í™˜ì´ ë¬´ì—‡ì¸ì§€ì— ê´€í•œ ì‚¬ì „ì§€ì‹ì´ë‚˜ ê·¼ê±°ë¥¼ ì°¾ê¸° ì–´ë ¤ìš´ ê²½ìš°ë„ ìˆë‹¤. ì´ëŸ´ ë•ŒëŠ” ê²½í—˜ì  ì ‘ê·¼ì— ë”°ë¥¸ ë¶„ì„ì´ ë¬¸ì œì˜ ì‹¤ë§ˆë¦¬ë¥¼ ì œê³µí•  ìˆ˜ë„ ìˆìœ¼ë©°, ë˜ ì´ë¥¼ í†µí•´ ì ì ˆí•œ ë³€í™˜ë„ ë°œê²¬í•  ìˆ˜ ìˆê²Œ ëœë‹¤. ì´ë¶„ì‚°ì„±ì´ ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì´ë¥¼ ì ì ˆíˆ ì œê±°í•˜ì§€ ì•Šìœ¼ë©´ íšŒê·€ê³„ìˆ˜ì˜ ì¶”ì •ëŸ‰ì´ ê·¸ì˜ ë¶ˆí¸ì„±ì€ ìœ ì§€ë˜ë‚˜ í‘œì¤€ì˜¤ì°¨ ê°’ì€ ì»¤ì§€ê²Œ ëœë‹¤. ì´ì— ë”°ë¼ íšŒê·€ê³„ìˆ˜ì˜ ì‹ ë¢°êµ¬ê°„ì€ ë„“ì–´ì§€ë©° ìœ ì˜ì„± ê²€ì •ì˜ ë¯¼ê°ì„±ì´ ë–¨ì–´ì§„ë‹¤. ê°€ì¤‘ìµœì†Œì œê³±(Weighted Least Squares) ì”ì°¨ê²€ì •ì„ í†µí•œ ê²°ê³¼ë¡œ ë“±ë¶„ì‚°ì„± ê°€ì •ì„ ë§Œì¡±ì‹œí‚¤ì§€ ëª»í•˜ì˜€ì„ ê²½ìš° ë³€ìˆ˜ ë³€í™˜ì„ ì ìš©í•˜ì—¬ ì¼ë‹¨ ì´ë¶„ì‚°ì„±ì˜ ìƒí™©ì„ ìˆ˜ì •í•œ í›„, í†µìƒì ì¸ ìµœì†Œì œê³±(OLS; Ordinary Least Square)ì¶”ì •ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë³´ë‹¤ ë‚˜ì€ ì¶”ì •ì¹˜ë¥¼ êµ¬í•  ìˆ˜ ìˆì—ˆë‹¤. ê°€ì¤‘ìµœì†Œì œê³±(WLS; weighted least squares)ë²•ì€ ë³€í™˜ëœ ë³€ìˆ˜ë“¤ì— ëŒ€í•˜ì—¬ OLSë¥¼ ìˆ˜í–‰ í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•œ ê²ƒì´ë‹¤. ì•ìœ¼ë¡œ ì„¤ëª…í•  WLSë²•ì€ ì´ë¶„ì‚°ì„±ì„ ê°–ëŠ” ì˜¤ì°¨ë¥¼ ë‹¤ë£¨ëŠ” ë°©ë²•ê³¼ ê·¸ ì¶”ì • ë°©ë²•ì— ê´€í•œ ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ WLSëŠ” í•¨ëŸ‰-ë°˜ì‘ ê³¡ì„ (dose-response curve)ê³¼ ë¡œì§€ìŠ¤í‹± ëª¨í˜•(Logistic model)ì— ëŒ€í•œ ì í•©ì—ì„œ OLSë³´ë‹¤ ë” ìš°ìˆ˜í•˜ë‹¤. WLS ì¶”ì •ì€ ë‹¤ìŒì˜ ì‹ì„ ìµœì†Œí™”í•¨ì„ì¨ ì–»ì–´ì§„ë‹¤. ë§Œì•½ ì—¬ê¸°ì„œ ê° ê°€ì¤‘ì¹˜ê°€ 1ì”© ë™ì¼í•˜ê²Œ ë‚˜ëˆ ê°€ì§„ë‹¤ë©´ ì¼ë°˜ì ì¸ OLSì™€ ë™ì¼í•˜ë‹¤. \\sum_{i=1}^{n} w_{i}(y_{i} - \\beta_{0} - \\beta_{1} x_{i1} - \\cdots - \\beta_{p}x_{ip})^{2} ìœ„ì˜ ì‹ì—ì„œ $w_{i}$ëŠ” ë¶„ì‚°ì— ë°˜ë¹„ë¡€í•˜ëŠ” ê°€ì¤‘ì¹˜ì´ë‹¤. ì¦‰, $w_{i} = 1/\\sigma_{i}^{2}$ì´ë‹¤. ë”°ë¼ì„œ WLS ë°©ë²•ì„ ì‚¬ìš©í•  ê²½ìš° ê°–ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ëŠ” ê´€ì°°ê°’ì€ íšŒê·€ê³„ìˆ˜ë“¤ì˜ ê°’ì„ ê²°ì •í•˜ëŠ” ë° ì ì€ ì˜í–¥ì„ ë¯¸ì¹˜ê²Œëœë‹¤. ê·¹ë‹¨ì ì¸ ê²½ìš° $w_{i} = 0$ì´ë©´ ië²ˆì§¸ ê´€ì¸¡ê°œì²´ëŠ” ì¶”ì •ê³¼ì •ì—ì„œ ì œì™¸ë˜ëŠ” íš¨ê³¼ë¥¼ ê°€ì§€ê²Œ ëœë‹¤. WLSì—ì„œëŠ” ë°ì´í„° ìƒì„±ì— ê´€í•œ ì‚¬ì „ì§€ì‹ì´ë‚˜, ì§ê´€, ì”ì°¨ë¶„ì„ ë“± ì´ë¶„ì‚°ì„±ì„ íƒìƒ‰í•˜ëŠ” ê³¼ì •ì— ì–»ì€ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ í™œìš©í•˜ê²Œ ëœë‹¤. ë§Œì•½ ê°€ì¤‘ì¹˜ê°€ ì•Œë ¤ì ¸ ìˆì§€ ì•Šë‹¤ë©´, ì•ì—ì„œì˜ ë°©ë²•ì€ ë‹¤ìŒ ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì§„ë‹¤. ì²« ë‹¨ê³„ì—ì„œëŠ” OLSì— ì˜í•œ ê²°ê³¼ë¡œë¶€í„° ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì •í•˜ê³ , ë‘ë²ˆì§¸ ë‹¨ê³„ëŠ” ì²« ë‹¨ê³„ì—ì„œ êµ¬í•œ ê°€ì¤‘ì¹˜ë¥¼ ê¸°ì´ˆë¡œ WLSë¥¼ ì ìš©í•˜ì—¬ ì¶”ì •ì¹˜ë¥¼ êµ¬í•˜ê²Œ ëœë‹¤. WLS ì¶”ì •ì¹˜ê°€ ê°€ì§€ëŠ” ë‹¤ë¥¸ í•œ ë…¼ë¦¬ëŠ”, ëª¨í˜•ì— ìˆëŠ” íšŒê·€ê³„ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ë‘ê³  ë³€í™˜ëª¨í˜•ì—ì„œ ì˜¤ì°¨í•­ì˜ ë¶„ì‚°ì´ ìƒìˆ˜ê°€ ë˜ë„ë¡ ë°ì´í„° ë³€í™˜ì„ ì‹œí‚¤ëŠ” ë° ê·¼ê±°ë¥¼ ë‘”ë‹¤. í•¨ëŸ‰-ë°˜ì‘ ì—°ê´€ê³¡ì„ ì˜ ì í•© ê°€ì¤‘ìµœì†Œì œê³±ë¶„ì„ë²•ì˜ ì¤‘ìš”í•œ ì‘ìš©ë¶„ì•¼ ì¤‘ì˜ í•˜ë‚˜ë¡œ ë¹„ìœ¨ì˜ í˜•íƒœ(0ê³¼ 1 ì‚¬ì´ì˜ ê°’)ë¥¼ ì·¨í•˜ëŠ” ë°˜ì‘(ì¢…ì†)ë³€ìˆ˜ë¥¼ ì„ í˜•íšŒê·€ë¥¼ í†µí•´ ì í•©ì‹œí‚¤ëŠ” ê²½ìš°ë¥¼ ìƒê°í•  ìˆ˜ ìˆë‹¤. ì–´ë–¤ ì—°êµ¬ìê°€ í”¼ì‹¤í—˜ëŒ€ìƒì—ê²Œ ì—¬ëŸ¬ ê°€ì§€ ë‹¤ë¥¸ ìˆ˜ì¤€ì˜ ìê·¹ì„ ì£¼ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í•œë‹¤ê³  í•˜ì. ì‹¤í—˜ëŒ€ìƒë“¤ì€ ì—¬ëŸ¬ ìˆ˜ì¤€ì˜ ìê·¹ë“¤ì— ì„ì˜ ë°°ì¹˜ë˜ê³  ê° ëŒ€ìƒì´ ê·¸ ìê·¹ì— ë°˜ì‘í•˜ëŠëƒì˜ ì—¬ë¶€ì— ë”°ë¥¸ ì´í•­ë°˜ì‘(binary response)ì„ ê´€ì°°í•œë‹¤ê³  í•˜ì. ì´ëŸ° ì‹¤í—˜ì˜ ì˜ˆëŠ” ìê·¹ì˜ ìˆ˜ì¤€ì´ íˆ¬ì—¬ëœ ì•½ì´ë‚˜ ë…ê·¹ë¬¼ì˜ í•¨ëŸ‰ìœ¼ë¡œ í‘œí˜„ë˜ê³ , ì´í•­ë°˜ì‘ì´ ì‚¬ë§ ë˜ëŠ” ìƒì¡´ì„ ë‚˜íƒ€ë‚´ëŠ” ì•½í•™ì´ë‚˜ ìƒë¬¼ì‹œí—˜(bioassay) ë¶„ì•¼ì—ì„œ ì£¼ë¡œ ë§ì´ ì°¾ì•„ë³¼ ìˆ˜ ìˆë‹¤. ë˜ ë‹¤ë¥¸ ì˜ˆë¡œëŠ” ì—¬ê¸°ì—ì„œì˜ ìê·¹ì„ ì–´ë–¤ ì œí’ˆì— ëŒ€í•œ í• ì¸ì•¡ì— ëŒ€ì‘ì‹œí‚¬ ë•Œ ì´í•­ë°˜ì‘ì´ ê·¸ ì œí’ˆì˜ êµ¬ë§¤ì—¬ë¶€ë¡œ í‘œí˜„ë˜ëŠ” ì†Œë¹„ìí–‰ë™ì„ ì—°êµ¬í•˜ëŠ” ê²½ìš°ë¥¼ ìƒê°í•  ìˆ˜ë„ ìˆë‹¤. ìœ„ì˜ ê²½ìš°ë“¤ì˜ í†µê³„ë¶„ì„ì˜ ëª©ì ì€ íˆ¬ì—¬í•¨ëŸ‰-ë°˜ì‘ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ê²°ì •í•˜ëŠ” ê²ƒë¿ ì•„ë‹ˆë¼, ì–´ë–¤ ì§€ì •ëœ ìˆ˜ì¤€ì˜ ë°˜ì‘ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ í•„ìš”í•œ íˆ¬ì—¬ëŸ‰ì˜ ì¶”ì •ë„ í¬í•¨ëœë‹¤. ë¡œì§€ìŠ¤í‹± ëª¨í˜•(ë˜ëŠ” logit model)ì€ ìƒë¬¼í•™ì´ë‚˜ ì—­í•™ ë¶„ì•¼ ë¿ë§Œ ì•„ë‹ˆë¼ ë¦¬ìŠ¤í¬ ë¶„ì„, í•™ìŠµì´ë¡ , ì†Œë¹„ìí–‰ë™(ì„ íƒëª¨í˜•) ì´ë¡ , ì‹œì¥ í”„ë¡œëª¨ì…˜ ì—°êµ¬ ë“±ì—ë„ ë„ë¦¬ ì‚¬ìš©ëœë‹¤. ìƒê´€ëœ ì˜¤ì°¨í•­ì˜ ë¬¸ì œ ì„ í˜•íšŒê·€ëª¨í˜•ì˜ ê¸°ë³¸ ê°€ì •ë“¤ ì¤‘ í•˜ë‚˜ëŠ” ië²ˆì§¸ jë²ˆì§¸ ê´€ì¸¡ê°œì²´ì— ëŒ€í•œ ì˜¤ì°¨í•­ì¸ $\\varepsilon_{i}$ì™€ $\\varepsilon_{j}$ê°€ ì„œë¡œ ìƒê´€ë˜ì–´ ìˆì§€ ì•Šì€ ë…ë¦½ì ì¸ í™•ë¥ ë³€ìˆ˜ë¼ëŠ” ê²ƒì´ë‹¤. íšŒê·€ëª¨í˜•ì—ì„œ ì˜¤ì°¨í•­ì´ ê°€ì§€ëŠ” ì˜ë¯¸ì— ë¹„ì¶”ì–´ ë³¼ ë•Œ ì´ë“¤ ì‚¬ì´ì˜ ìƒê´€ê´€ê³„ì˜ ì¡´ì¬ëŠ” ê³§ ëª¨í˜•ì— ë°˜ì˜ë˜ì§€ ì•Šì€ ì¶”ê°€ì  ì˜ˆì¸¡ë³€ìˆ˜ì˜ ì¡´ì¬ ê°€ëŠ¥ì„±ì„ ì˜ë¯¸í•  ìˆ˜ë„ ìˆë‹¤. ë¹ ëœ¨ë¦° ì˜ˆì¸¡ë³€ìˆ˜ì— ëŒ€í•œ ì—°ì†ëœ ê°’ë“¤ì´ ì„œë¡œ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ë©´, ì´ëŸ° ìƒê´€ì˜ íš¨ê³¼ëŠ” ì˜¤ì°¨í•­ì— ë°˜ì˜ë˜ê³  ì´ì— ë”°ë¼ ëª¨í˜•ì˜ ì˜¤ì°¨í•­ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ê²Œ ë  ê²ƒì´ë‹¤. ê´€ì¸¡ê°’ë“¤ì´ ì–»ì–´ì§€ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ìˆœì„œì— ë”°ë¼ ê·¸ë“¤ì´ ì„œë¡œ ì—°ê´€ë˜ì–´ ìˆì„ ê²½ìš° ì˜¤ì°¨í•­(í˜¹ì€ ë°˜ì‘ë³€ìˆ˜ì˜ ê´€ì¸¡ê°’)ë“¤ì´ ìê¸°ìƒê´€(autocorrelation)ì„ ê°€ì§€ê³  ìˆë‹¤ê³  í•œë‹¤. ìê¸°ìƒê´€ì„±ì˜ ë¬¸ì œëŠ” ì—¬ëŸ¬ ê°€ì§€ ì´ìœ ë¡œ ì¼ì–´ë‚  ìˆ˜ ìˆë‹¤. ì‹œê°„ì  ë˜ëŠ” ê³µê°„ì ìœ¼ë¡œ ì¸ì ‘ë˜ì–´ ìˆëŠ” ê´€ì¸¡ê°œì²´ë“¤ì— ëŒ€í•œ ì˜¤ì°¨í•­ë“¤ì€ ìœ ì‚¬í•œ ê²½í–¥ì„ ê°€ì§€ê¸° ì‰½ë‹¤. ì‹œê³„ì—´ ë°ì´í„°ì—ì„œëŠ” ì—°ì†ëœ ê´€ì¸¡ê°œì²´ì— ëŒ€í•œ ì˜¤ì°¨í•­ì€ ì–‘ì˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤. ë˜í•œ ê³µê°„ì ìœ¼ë¡œ ì¸ì ‘í•œ ì‹¤í—˜êµ¬ì—­ì—ì„œ ì–»ì€ ê´€ì¸¡ê°’ë“¤ì€ ê³µìœ í•˜ëŠ” ì™¸ì  í™˜ê²½ì˜ ì˜í–¥ìœ¼ë¡œ ì¸í•´ ìƒê´€ëœ ì”ì°¨ë¥¼ ê°€ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤. ì˜¤ì°¨í•­ì˜ ìê¸°ìƒê´€ì„± ë¬¸ì œëŠ” íšŒê·€ë¶„ì„ì— ì—¬ëŸ¬ ê°€ì§€ ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ë˜ëŠ”ë°, ê·¸ ë‚´ìš©ì„ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. 1.ìµœì†Œì œê³±ì¶”ì •ëŸ‰ì´ ê·¸ì˜ ë¶ˆí¸ì„±ì€ ìœ ì§€í•˜ì§€ë§Œ ë” ì´ìƒ ìµœì†Œë¶„ì‚°ì„ ê°€ì§€ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì—ì„œ ì¶”ì •í–¥ì˜ íš¨ìœ¨ì„±ì´ ì—†ì–´ì§„ë‹¤. $\\sigma^{2}$ì´ë‚˜ ëª¨íšŒê·€ê³„ìˆ˜ì˜ í‘œì¤€ì˜¤ì°¨ì˜ ì¶”ì •ëŸ‰ì€ ì‹¤ì œë³´ë‹¤ ì‹¬ê°í•˜ê²Œ ê³¼ì†Œì¶”ì •ë  ìˆ˜ ìˆìœ¼ë©°, ì´ì— ë”°ë¼ ì¶”ì •ëœ íšŒê·€ê³„ìˆ˜ëŠ” ê·¸ì˜ ì •í™•ë„ê°€ ë†’ì€ ê²ƒìœ¼ë¡œ ì˜ëª» íŒë‹¨ë  ìˆ˜ ìˆë‹¤. í†µìƒì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì‹ ë¢°êµ¬ê°„ì´ë‚˜ ìœ ì˜ì„± ê²€ì • ë“±ì´ ì—„ê²©í•œ ì˜ë¯¸ì—ì„œ ë” ì´ìƒ íƒ€ë‹¹í•˜ì§€ ì•Šë‹¤. ìœ„ì™€ ê°™ì€ ì´ìœ ë¡œ ì˜¤ì°¨í•­ë“¤ ê°„ì˜ ìê¸°ìƒê´€ì€ ì‹¬ê°í•œ ë¬¸ì œì´ë©° ê°„ë‹¨íˆ ë¬´\u001dì‹œë˜ì–´ì„œëŠ” ì•ˆ ë  ê²ƒì´ë‹¤.ìê¸°ìƒê´€ì„±ì˜ ë¬¸ì œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒ ë‘ ê°€ì§€ ìœ í˜•ìœ¼ë¡œ êµ¬ë¶„í•  ìˆ˜ ìˆë‹¤. ì²« ë²ˆì§¸ëŠ” íšŒê·€ëª¨í˜•ì— í¬í•¨ë˜ì–´ì•¼ í•  ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ê°€ ë¹ ì§ìœ¼ë¡œì¨ ìƒê¸°ëŠ” ê²½ìš°ì´ë¯€ë¡œ ì´ë•ŒëŠ” í•´ë‹¹ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë§Œ ì°¾ì•„ë‚´ë©´ ë¬¸ì œëŠ” ê³§ í•´ê²°ëœë‹¤. í—ˆë‚˜, ì‹¤ì œë¡œ ë°ì´í„° ë¶„ì„ì„ í•¨ì— ìˆì–´ì„œ í•´ë‹¹ ë³€ìˆ˜ë¥¼ ì°¾ëŠ” ê²ƒì€ ê·¸ë¦¬ ì‰¬ìš´ ë¬¸ì œê°€ ì•„ë‹ˆë‹¤. ë‘ë²ˆì§¸ ìœ í˜•ì€ ìˆœìˆ˜ìê¸°ìƒê´€ì´ë¼ê³  ë¶ˆë¦¬ëŠ” ê²ƒìœ¼ë¡œ ë°ì´í„° ë³€í™˜ ë“±ì„ í†µí•´ í•´ê²°ë  ìˆ˜ ìˆë‹¤. ì‹œê³„ì—´ ë°ì´í„°ì˜ ë¶„ì„ì—ì„œ ê°€ì¥ ìœ ìš©í•œ ê·¸ë˜í”„ëŠ” ì”ì°¨ë¥¼ ì‹œê°„ì— ëŒ€í•´ í”Œë¡¯í•œ ì¸ë±ìŠ¤ í”Œë¡¯(index plot)ì´ë‹¤. ë§Œì•½ í”Œë¡¯ì—ì„œ ê³„ì†ëœ ëª‡ ê°œì˜ ì–‘ì˜ ì”ì°¨ë“¤ì´ ìˆê³ , ê·¸ ë‹¤ìŒ ì—¬ëŸ¬ ê°œì˜ ìŒì˜ ì”ì°¨ê°€ ë”°ë¼ì˜¤ëŠ” ì‹ì˜ ê°™ì€ ë¶€í˜¸ë¥¼ ê°€ì§„ ì”ì°¨ë“¤ì´ êµ°ì§‘í™”í•˜ëŠ” ìƒí™©ì„ ë³¼ ìˆ˜ ìˆë‹¤ë©´, ì•„ë‹ˆë©´ ê·¸ ë°˜ëŒ€ì˜ ëª¨ìŠµë“¤ì´ ë³´ì¸ë‹¤ë©´ í•´ë‹¹ ë°ì´í„°ëŠ” ì˜¤ì°¨í•­ì´ ì„œë¡œ ìƒê´€ë˜ì–´ ìˆëŠ” ê²½ìš°ì´ë‹¤. ì´ëŸ¬í•œ í˜•íƒœì˜ ëª¨í˜•ë“¤ì€ ì˜¤ì°¨í•­ë“¤ê°„ì— ê°•ë ¥í•œ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤ëŠ” ì¦ê±°ë¡œì„œ ì¶”ê°€ì ì¸ ë¶„ì„ì´ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ê·¸ë˜í”„ ë¶„ì„ì— ì¶”ê°€í•˜ì—¬ ì—°(run; ì”ì°¨ì˜ ë¶€í˜¸ê°€ ë™ì¼í•œ ë°ì´í„°ë“¤ì˜ êµ°ì§‘ì´ ì—°ì†ìœ¼ë¡œ ì´ë£¨ì–´ì§)ì˜ í¬ê¸°ë¥¼ ì´ìš©í•˜ì—¬ ì˜¤ì°¨í•­ì˜ ìê¸°ìƒê´€ì„±ì„ ê²€ìƒ‰í•˜ëŠ” ë°©ë²• ì™¸ì— ë”ë¹ˆ-ì™“ìŠ¨ í†µê³„ëŸ‰ì„ ì´ìš©í•  ìˆ˜ë„ ìˆë‹¤. ë”ë¹ˆ-ì™“ìŠ¨ í†µê³„ëŸ‰ ë”ë¹ˆ-ì™“ìŠ¨(Durbin-Watson) í†µê³„ëŸ‰ì€ íšŒê·€ë¶„ì„ì—ì„œ ì˜¤ì°¨í•­ì˜ ìê¸°ìƒê´€ì„± ì—¬ë¶€ë¥¼ ëŒ€ìˆ˜ì  ë°©ë²•ìœ¼ë¡œ ê²€ì •í•˜ê¸° ìœ„í•´ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì´ë‹¤. ì˜¤ì°¨í•­ë“¤ì´ ë‹¤ìŒ í˜•ì‹ì˜ 1ì°¨ì˜ ìê¸°ìƒê´€ê³„ì—´ì„ ì´ë£¬ë‹¤ëŠ” ê°€ì •ì— ê·¸ ê·¼ê±°ë¥¼ ë‘ê³  ìˆë‹¤. $w_{t}$ëŠ” ì„œë¡œ ë…ë¦½ì´ë©°, í‰ê· ì´ 0ì´ê³  ë¶„ì‚°ì´ ìƒìˆ˜ì¸ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ë©°, $\\rho$ëŠ” $\\varepsilon_{t}$ì™€ $\\varepsilon_{t-1}$ì˜ ìƒê´€ê³„ìˆ˜ì´ë‹¤. ì´ëŸ° ê´€ê³„ë¥¼ ê°–ëŠ”ë‹¤ë©´ 1ì°¨ ìê¸°ìƒê´€ì„ ê°€ì§„ë‹¤ê³  ë§í•œë‹¤. \\varepsilon_{t} = \\rho \\varepsilon_{t-1} + w_{t}, | \\rho | < 1 ë”ë¹ˆ-ì™“ìŠ¨ í†µê³„ëŸ‰ dëŠ” ì•„ë˜ì˜ ìˆ˜ì‹ìœ¼ë¡œ ì •ì˜ëœë‹¤. ê·€ë¬´ê°€ì„¤ $H_{0} : \\rho = 0, H_{1}: \\rho &gt; 0$ì— ëŒ€í•œ ê²€ì • í†µê³„ëŸ‰ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤. d = \\frac{\\sum^{n}_{t=2} (e_{t} - e_{t-1})^2 }{\\sum_{t=1}^{n} e_{t}^{2}} ìœ„ì™€ ê°™ì€ ë°©ë²•ë“¤ì„ í†µí•´ ìê¸°ìƒê´€ì„ ì œê±°í•´ì£¼ê¸°ë„ í•˜ì§€ë§Œ ì˜ˆë¥¼ë“¤ì–´, ì‹œê°„ì— ë”°ë¼ ë³€í•˜ëŠ” ì¤‘ìš”í•œ ë…ë¦½ë³€ìˆ˜ë¥¼ íšŒê·€ëª¨í˜•ì—ì„œ ë¹ ëœ¨ë¦° ê²°ê³¼ë¡œ ì¸í•œ í˜„ìƒì¼ìˆ˜ë„ ìˆê¸°ì— ê´€ì¸¡ëœ ìê¸°ìƒê´€ì„±ì˜ ì§•í›„ëŠ” ì˜ëª»ëœ ëª¨í˜•ì„¤ì •ì—ì„œ ë¹„ë¡¯ëœ í˜„ìƒìœ¼ë¡œ í•´ì„í•  ìˆ˜ë„ ìˆë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ì‹¤ì œë¡œ ì§€ì ëœ ìê¸°ìƒê´€ì˜ ì˜¤ì°¨êµ¬ì¡°ë¥¼ ë°˜ì˜í•˜ëŠ” ìê¸°íšŒê·€(autoregressive) ëª¨í˜•ì„ ê°€ì§€ê³  ìƒˆë¡œ ë¶„ì„ì„ ì‹œì‘í•˜ê¸°ë³´ë‹¤ëŠ” ìƒˆë¡œìš´ ë…ë¦½ë³€ìˆ˜ì˜ ë„ì…ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•´ë³´ê³  ì§„í–‰í•˜ëŠ” ê²ƒì´ ë” ë°”ëŒì§í•  ë•Œê°€ ë§ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ìê¸°ìƒê´€ì„ ìˆ˜ì •í•˜ê¸° ìœ„í•´ ë³€ìˆ˜ë³€í™˜ ë°©ë²•ì„ ê³ ë ¤í•˜ëŠ” ì¼ì€ ë§ˆì§€ë§‰ìœ¼ë¡œ ì‹œë„í•´ë³¼ ë§Œí•œ ê²ƒì´ë‹¤.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"NLPë€?","slug":"NLP_07","date":"2020-01-14T19:01:52.000Z","updated":"2020-02-04T07:55:06.549Z","comments":true,"path":"2020/01/15/NLP_07/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/15/NLP_07/","excerpt":"","text":"ìì—°ì–´ë€? NLPë€? NLPì˜ ì–´ë ¤ì›€ ìš°ë¦¬ê°€ ì‹¤ìƒí™œì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ëŠ” ë³µì¡ì„±, ì• ë§¤í•¨, ê·¸ë¦¬ê³  ì˜ì¡´ì„±ì„ ì§€ë‹ˆê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë³µì¡ì„±ì´ë€ ì˜ˆë¥¼ ë“¤ì–´ í•„ìê°€ ì¢‹ì•„í•˜ëŠ” ê²Œì„ì¸ ë°°í‹€ê·¸ë¼ìš´ë“œë¡œ ì˜ˆë¥¼ ë“¤ì–´ ë³´ê² ë‹¤. ëª‡ ì£¼ì „ PUBGì—ì„œëŠ” ë°°í‹€ê·¸ë¼ìš´ë“œì˜ ì‹ ë§µì¸ ì¹´ë¼í‚¨ì— ëŒ€í•´ ë°˜ì‘ì„ ë³´ê¸° ìœ„í•´ ê° ë°°í‹€ê·¸ë¼ìš´ë“œ ì»¤ë®¤ë‹ˆí‹° ì‚¬ì´íŠ¸ì— ëŒ€í•œ ëŒ“ê¸€ì„ ë¶„ì„í•œë‹¤ê³  ê°€ì •í•´ë³´ì. ê·¸ë ‡ë‹¤ë©´, ìš°ì„  ê²Œì„ ìš©ì–´ê°€ ì–´ë–¤ ê²ƒë“¤ì„ ì§€ì¹­í•˜ëŠ” ì§€ ì‚¬ì „ì§€ì‹ì´ í•„ìš”í•  ê²ƒì´ë‹¤. ì´ë ‡ë“¯ ë³µì¡í•˜ê²Œ ì—°ê´€ë˜ì–´ ìˆëŠ” Tokenê°„ì˜ ê´€ê³„ë¥¼ ë³µì¡ì„±ì´ë¼ê³  í•œë‹¤. ì• ë§¤í•¨ì€ ë‹¤ì¤‘ëª¨ë“œë¼ëŠ” ê²ƒì´ ìˆê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìš°ë¦¬ê°€ ì‹¤ìƒí™œì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–´êµ¬ ì¤‘ â€˜ë„ˆ ì°¸ ì˜í•œë‹¤.â€™ë¼ëŠ” ë¬¸ì¥ì€ ì—¬ëŸ¬ê°€ì§€ ìƒí™©ì—ì„œ ì‚¬ìš©ë˜ë©°, ìƒí™©ì— ë”°ë¼ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤. â€˜ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì´ë ‡ê²Œ ì˜í•´ë†¨ì–´? ì™€.. ë„ˆ ì°¸ ì˜í•œë‹¤.â€™ì™€ â€˜ì‘? ì´ê±° ë­ì•¼? ì´ê±° ì™œ ìµœì†Œ ê¸€ììˆ˜ë¥¼ 8ìë¡œí–ˆì–´? ì „ì²´ Corpusì— ë‹¨ì–´ í‰ê·  ê¸¸ì´ëŠ” 11ì¸ë°??? ëª°ëë‹¤ê³ ?? ì°¸ ì˜í•œë‹¤~!â€™ ì•ì˜ ë‘ ë¬¸ì¥ì€ â€˜ì°¸ ì˜í•œë‹¤â€™ì˜ ì˜ë¯¸ê°€ ë¬¸ë§¥ì ìœ¼ë¡œ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì´ë‹¤. ì˜ì¡´ì„±ì€ íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ í• ë•Œ ì •ë³´ê°€ ë¶€ì¡±í•´ì„œ ì˜ì¡´ì ì¸ ë¶€ë¶„ë“¤ì´ ìƒê¸°ëŠ” ë¬¸ì œë¥¼ ì˜ë¯¸í•œë‹¤. í•œê°€ì§€ ì˜ˆë¡œ, â€˜ì ¤ë¦¬ ë¨¹ê³ ì‹¶ì€ë° ì ¤ë¦¬ í•˜ë‚˜ë§Œ ì‚¬ë‹¤ì¤„ë˜â€™ì˜ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ í• ë•Œ, ìœ„ì˜ ì§ˆë¬¸ë§Œ ë“¤ì—ˆì„ ë• ë„ëŒ€ì²´ ì–´ë–¤ ì ¤ë¦¬ë¥¼ ì‚¬ì•¼í• ì§€ ëª¨ë¥¼ê²ƒì´ë‹¤. ì´ëŸ° ìƒí™©ì— ë‹¤ì‹œ â€˜ì–´ë–¤ ì ¤ë¦¬ ë¨¹ê³ ì‹¶ì€ë°â€™ë¼ëŠ” ì§ˆë¬¸ì„ í†µí•œ ìƒí˜¸ì‘ìš©ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. NLPë¶„ì•¼ì—ì„œì˜ Machine learning vs Deep learning ë°˜ë³µí•´ì„œ ì–˜ê¸°í•˜ì§€ë§Œ Featureì˜ ì¶”ì¶œì„ ì‚¬ëŒì´ ì§ì ‘í•˜ëŠ” Machine learningê°™ì€ ê²½ìš°ëŠ” ì–¸ì–´í•™ì— ëŒ€í•œ ì§€ì‹ì„ ê¹Šì´ ì•Œê³  ìˆì–´ì•¼ ê°€ëŠ¥í•  ê²ƒì´ë‹¤. ê·¸ì— ë°˜í•´, ìƒëŒ€ì ìœ¼ë¡œ deep learningì€ featureë¥¼ ë§Œë“¤ì–´ ì¤„ ìˆ˜ ìˆëŠ” êµ¬ì¡°ë¥¼ ë§Œë“¤ë©´ ê·¸ì—ë”°ë¼ Featureë¥¼ ì•Œì•„ì„œ ìƒì„±í•´ì£¼ë¯€ë¡œ ìƒëŒ€ì ìœ¼ë¡œ ì–¸ì–´í•™ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì§€ì‹ì´ ì—†ì–´ë„ ë¶„ì„ì´ ê°€ëŠ¥í•˜ë‹¤. NLPì˜ Applicatioì˜ ì¢…ë¥˜ ë„¤ì´ë²„ì˜ íŒŒíŒŒê³ , êµ¬ê¸€ì˜ êµ¬ê¸€ ë²ˆì—­ê¸° ê°™ì€ ë²ˆì—­ ì„œë¹„ìŠ¤ë¥¼ ì˜ˆë¡œ ë“¤ìˆ˜ ìˆë‹¤. ì—¬ëŸ¬ íšŒì‚¬ì—ì„œ íŠ¹íˆ ì€í–‰ì´ë‚˜ ì¹´ë“œì‚¬, ì‡¼í•‘ëª°ë“±ì—ì„œ ë§ì´ ë³´ì•˜ì„ ë²•í•œ ì±—ë´‡ ì„œë¹„ìŠ¤ë„ NLPì˜ ì‘ìš©ë¶„ì•¼ì´ë‹¤. ìŠ¤ìºí„°ë©ì˜ í•‘íì´ë‚˜, ì‹¬ì‹¬ì´ê°™ì€ ì±—ë´‡ ì„œë¹„ìŠ¤ë“¤ë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, êµ¬ê¸€ ì• ë“œ ì„¼ìŠ¤ ê°™ì€ ì„œë¹„ìŠ¤ê°€ ìˆë‹¤.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"ìˆœí™˜ ì‹ ê²½ë§(RNN) - ìˆœì°¨ ë°ì´í„°ì˜ ì´í•´","slug":"deep_learning_08","date":"2020-01-12T06:25:39.000Z","updated":"2020-01-21T08:10:54.577Z","comments":true,"path":"2020/01/12/deep_learning_08/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/12/deep_learning_08/","excerpt":"","text":"ìˆœì°¨ ë°ì´í„°ì˜ ì´í•´ ìš°ë¦¬ê°€ ìˆœí™˜ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ì…ë ¥ì„ ìˆœí™˜ ì‹ ê²½ë§ìœ¼ë¡œ ë°›ê±°ë‚˜ ì¶œë ¥ì„ ìˆœí™˜ ì‹ ê²½ë§ìœ¼ë¡œ ë‚´ê¸° ìœ„í•´ì„œì´ë‹¤. ì¼ì •í•œ ì‹œê°„ì°¨ì„ ê°–ëŠ” Time Seriesë¼ë©´, xì¶•ì´ íŠ¹ì • ì‹œê°„ì„ ì˜ë¯¸í•˜ëŠ” Temporal Sequenceì™€ëŠ” ë‹¤ë¥´ê²Œ í•˜ë‚˜í•˜ë‚˜ì˜ Stepìœ¼ë¡œ ê°„ì£¼í•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” ìœ„ì—ì„œ ë³´ëŠ” ê²ƒê³¼ ê°™ì´ Temporal Sequenceë¥¼ ë³´ê°„í•˜ì—¬ Time Seriesë¡œ ë³€í™˜í•´ ì¤€ ë’¤ì— ì‚¬ìš©í•œë‹¤. ê°œì¸ ë¹„ì„œ ì„œë¹„ìŠ¤ëŠ” ì˜ˆë¥¼ ë“¤ì–´, sirië‚˜ êµ¬ê¸€ì˜ okay google ê°™ì€ ì„œë¹„ìŠ¤ì´ë‹¤. ê¸°ë³¸ ì ì¸ ìˆœí™˜ ì‹ ê²½ë§(Vanilla RNN) ì•ì„œ ë§í•œ ìˆœì°¨ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ ì›í•˜ëŠ” ì¶œë ¥ì„ í•˜ë ¤ë©´, ê¸°ì–µì‹œìŠ¤í…œì´ ì „ì œë˜ì–´ì•¼ í•œë‹¤. CNNì´ë‚˜ Deep Neural Network, shallow NNì€ Memoryless Systemì´ë‹¤. ë‹¤ìŒ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, RNNì€ ì´ì „ì˜ Network êµ¬ì¡°ì™€ëŠ” ë‹¤ë¥´ê²Œ ì…ë ¥ì¸µì— n-1ë²ˆì§¸ stepì˜ hidden layerë¥¼ në²ˆì§¸ ë°ì´í„°ì™€ concatenationì„ í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ ì´ì „ì˜ ëª¨ë“  ì…ë ¥ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤. shallow Neural Networkë¥¼ Deep Neural Networkë¡œ ë§Œë“¤ì–´ ì£¼ì—ˆë“¯ì´, ë™ì¼í•˜ê²Œ í•˜ì—¬ Multi-Layer RNNì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì™¼ìª½ì˜ ë…¸ë“œë“¤ë§Œ ë³¸ë‹¤ë©´ ë‹¤ìŒ Layerë“¤ì˜ ì´ì „ stepì˜ hidden Layerë¥¼ ê°€ì ¸ì˜¨ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ, ì´ëŸ° êµ¬ì¡°ëŠ” Vanilla RNNê³¼ ë‹¤ë¥´ê²Œ Hidden Layerì˜ ê¸¸ì´ë„ 2ë°°ì´ìƒìœ¼ë¡œ ëŠ˜ì–´ë‚˜ê¸° ë•Œë¬¸ì—, ë³µì¡ë„ê°€ ë†’ì•„ì§€ê²Œ ë˜ë©°, í˜„ì‹¤ì ìœ¼ë¡œ í•™ìŠµì´ ì˜ ë˜ì§€ ì•Šì•„ ê¶Œì¥ë˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ ì´ìœ ëŠ” ê°„ë‹¨í•˜ê²Œë§Œ ë§í•˜ìë©´, ì¼ë°˜ì ì¸ Neural NetworkëŠ” depth ë°©í–¥ìœ¼ë¡œë§Œ gradientê°€ ì˜ í•™ìŠµë˜ë©´ ë˜ì§€ë§Œ, ì´ êµ¬ì¡°ëŠ” inputê¹Œì§€ Gradientì˜ ì˜í–¥ì„ ì£¼ë„ë¡ í•´ì•¼í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì‹¬í™” ìˆœí™˜ ì‹ ê²½ë§ ê·¸ë ‡ë‹¤ë©´ &#39;Vanilla RNNì´ ì™œ ì˜ ì“°ì´ì§€ ì•ŠëŠ”ê°€?&#39;ì— ëŒ€í•œ ê°€ì¥ í° ì´ìœ ë¥¼ ì•ì„œ ì–¸ê¸‰í–ˆë˜ ê²ƒê³¼ ê°™ì´ Gradientê°€ Inputê¹Œì§€ íƒ€ê³  ê°€ì„œ í•™ìŠµì„ ì˜ ëª»í•˜ê¸° ë•Œë¬¸ì´ë¼ê³  ì–¸ê¸‰í–ˆë‹¤. ì¦‰, Gradient Vanishing ë¬¸ì œë¼ëŠ” ê²ƒì´ë‹¤. ê²€ì •ìƒ‰ ì„ ì´ Input Gateì´ë‹¤. ë¹¨ê°„ìƒ‰ ì„ ì€ Vanilla RNNì—ì„œì˜ Hidden Stateì™€ ë™ì¼í•˜ë‹¤. ì…ë ¥ì´ ë“¤ì–´ì˜¤ê³  ì´ì „ Hidden Stateë¥¼ ë°›ì•„ì„œ ê°™ì´ tanh activation functionì„ FC(Fully connected) Layerë¥¼ í†µê³¼ì‹œì¼œ ì¶œë ¥ì„ ë‚´ì£¼ë©´ RNNì˜ Hidden Layerì´ê¸° ë•Œë¬¸ì´ë‹¤. í•´ë‹¹ Layerì—ì„œ í•„ìš”í•œ ì •ë³´ë§Œì„ ì¶œë ¥ì¸µìœ¼ë¡œ ë‚´ì£¼ê³ , í•„ìš”í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ê³„ì† ê¸°ì–µí•˜ê²Œë” ë‹¤ìŒ time stepìœ¼ë¡œ ë„˜ê²¨ì£¼ì–´ì„œ ì´ì „ì— ì–´ë–¤ ì¶œë ¥ì„ ë‚´ì£¼ì—ˆì—ˆë‚˜ë¥¼ Cell Stateì™€ëŠ” ë³„ê°œë¡œ ë˜ ë„˜ê²¨ì£¼ì–´ ê¸°ì–µí•˜ê²Œë” í•œë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì¶œë ¥ì„ ë‚´ë³´ë‚´ê¸° ìœ„í•´ Cell Stateì—ì„œ tanhë¥¼ ê±°ì³ ì£¼ëŠ”ë° ì´ ì‘ì—…ì€ ë‹¤ë¥¸ activation functionë“¤ì´ ì¡´ì¬í•˜ëŠ” ë…¸ë“œë“¤ê³¼ ë‹¬ë¦¬ FC layerë¡œ ì´ë£¨ì–´ì ¸ ìˆì§€ ì•Šê³  ê·¸ëƒ¥ activation functionë§Œ ê±°ì¹˜ê²Œ ëœë‹¤. ê·¸ ì´ìœ ëŠ” Cell Stateê°€ Forget gateë¥¼ ì§€ë‚˜ë©´ì„œëŠ” 0~1ì‚¬ì´ì˜ ê°’ì´ ê³±í•´ì§€ë¯€ë¡œ í¬ê²Œ ë¬¸ì œê°€ ì—†ì§€\u001dë§Œ Input Gateë¥¼ ì§€ë‚˜ë©´ì„œ Featureê°€ ì¶”ê°€ì ìœ¼ë¡œ ë”í•´ì§ˆë•Œ tanhë¥¼ ì§€ë‚˜ë©´ -1~1ì‚¬ì´ì˜ ê°’ì´ ë˜ë¯€ë¡œ ë²”ìœ„ -2~2ë¡œ ëŠ˜ì–´ë‚˜ê²Œ ë˜ì–´ ì¶”í›„ì— Gradient Explodeê°€ ì¼ì–´ë‚  ìˆ˜ ìˆì–´ ì˜ˆë°©ì°¨ì›ì—ì„œ tanhì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ $1 - Forget Gate$ë¥¼ Input Gateë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ Forget Gateì—ì„œ ìŠì–´ë²„ë¦° ë§Œí¼ë§Œ Input Gateë¥¼ í†µí•´ ì±„ì›Œ ì£¼ëŠ” ì˜ë¯¸ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤. Input Gateë¥¼ í†µí•´ ìƒˆë¡œìš´ Featureê°€ ì¶”ê°€ë˜ê¸°ì— ì•ì„œì„œ ì´ì „ Hidden State ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ ìŠê²Œ í•˜ëŠëƒì˜ ì˜ë¯¸ì¸ë°, ì˜ˆë¥¼ ë“¤ì–´ ì•ì˜ ë¬¸ì¥ì´ .ì„ í†µí•´ ë§ˆì³ì¡Œë‹¤ë©´, ê·¸ ë’¤ì˜ ë¬¸ì¥ì€ ë‹¤ë¥¸ ë¬¸ì¥ êµ¬ì¡°ë¥¼ ë„ê²Œ ë˜ë¯€ë¡œ 0ì— ê°€ê¹ê²Œ í•˜ì—¬ Resetì„ ì‹œì¼œì¤„ ê²ƒì´ë‹¤. ì‹œê°„í¼ì¹¨ ì—­ì „íŒŒ í•™ìŠµë²•(BPTT: Back Propagation Through Time) ìˆœí™˜ì‹ ê²½ë§ì€ ê¸°ì¡´ì˜ ê¸°ë³¸ì ì¸ ì—­ì „íŒŒ í•™ìŠµë²•ìœ¼ë¡œëŠ” í•™ìŠµí•  ìˆ˜ ì—†ë‹¤. ê·¸ë ‡ë‹¤ë©´, ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ? ë¬¼ë¡ , ëª¨ë¸ì´ í•™ìŠµí• ë•Œ ì–¸ì œ ì…ë ¥ì´ ëë‚ ì§€ ëª¨ë¥´ê¸°ì— ë§ˆì§€ë§‰ ì…ë ¥ ê°™ì€ ê²½ìš°ëŠ” EOS(End Of Sequence)ë¼ëŠ” íŠ¹ë³„í•œ ë¯¸ë¦¬ì •í•´ì¤€ í•˜ë‚˜ì˜ í† í°ì„ ë‚ ë ¤ì£¼ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì€ Inputì˜ ì‹œì ì— ë”°ë¼ í¼ì³ì ¸ìˆë‹¤ëŠ” ê²ƒì„ ì´í•´í•˜ê¸° ì‰½ê²Œ í¼ì³ ë†“ì€ ê²ƒì¸ë°, ì—¬ê¸°ì„œ ì£¼ì˜í•  ì ì€ ì•„ë˜ì˜ RNNì•ˆì˜ Hyper parameterë“¤ì€ ëª¨ë‘ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì¦‰ ì•„ë˜ì˜ ê·¸ë¦¼ì€ ì¬ê·€ì í˜•íƒœë¥¼ ì‹œê°„ì˜ íë¦„ìƒìœ¼ë¡œ ë‚˜ì—´í•´ ë†“ì€ ê²ƒì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ ê²ƒê³¼ ê°™ì´ ì¶œë ¥(ë˜ëŠ” ì…ë ¥)ì˜ ê¸¸ì´ê°€ ì •í•´ì ¸ìˆì§€ ì•Šì€ RNNì˜ ê²½ìš°, ì•„ë˜ëŠ” ë§ˆì§€ë§‰ ì¶œë ¥ì— EOS\u001d ì¶œë ¥ì„ ë‚´ê²Œë” í•™ìŠµì‹œì¼œì•¼ ëª¨ë“  ì¶œë ¥ì´ ë‚˜ì™”ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ê°€ ìˆë‹¤. ë˜í•œ, Back propagationë„ ë§ˆì°¬ê°€ì§€ë¡œ ê°ê°ì˜ ì¶œë ¥ì— ëŒ€í•œ Lossê°’ ë¶€í„° ì‹œì‘í•´ì„œ Inputì§€ì ê¹Œì§€ í•´ì£¼ë©´ ëœë‹¤. ë‹¨ì¼ ì…ë ¥, ë‹¤ì¤‘ ì¶œë ¥ì˜ ì‹¤ì œ ëª¨ë¸ì—ì„œëŠ” ì…ë ¥ì´ ì—†ëŠ” ë‹¤ë¥¸ ì¸µì—ì„œëŠ” 0ì„ ì…ë ¥í•˜ê±°ë‚˜ ë¯¸ë¦¬ ì •í•´ë†“ì€ ì…ë ¥ì„ ë„£ì–´ì£¼ì–´ í•™ìŠµì„ ì‹œí‚¤ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ë‹¤ì¤‘ ì…ë ¥ì— ëŒ€í•´ì„œ ë‹¤ì¤‘ ì¶œë ¥ì´ ë‚˜ì˜¤ë ¤ë©´ 2ê°€ì§€ ìƒí™©ì´ ìˆì„ ìˆ˜ ìˆë‹¤. í•˜ë‚˜ëŠ” ì•„ë˜ ê·¸ë¦¼ì—ì„œì™€ ê°™ì´ ì…ë ¥ì— ëŒ€í•´ì„œ ì¶œë ¥ì´ ë‚˜ì˜¤ê³  ì…ë ¥ì´ ëë‚˜ë©´ ì¶œë ¥ë„ ëë‚˜ëŠ” ê²ƒì´ ìˆì„ ìˆ˜ ìˆë‹¤.ì´ëŸ° ê²½ìš°ëŠ” ëŒ€í‘œì ìœ¼ë¡œ, ë™ì˜ìƒì˜ í”„ë ˆì„ ë¶„ë¥˜ê°€ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, CFì˜ í•œ í”„ë ˆì„ì´ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì™€ ê° ì¥ë©´ì´ ì–´ë–¤ ì¥ë©´ì¸ì§€ ì„œìˆ í•˜ëŠ” ì‹ìœ¼ë¡œì˜ ë¶„ë¥˜ë¥¼ ë“¤ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ë˜ ë‹¤ë¥¸ í•œ ìƒí™©ì€ ëª¨ë“  ì…ë ¥ì„ ë°›ê³  ê·¸ ë‹¤ìŒì— ì¶œë ¥ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ì´ ê²½ìš°ë„ ë§ˆì°¬ê°€ì§€ë¡œ ì…ë ¥ì˜ ê¸¸ì´ê°€ ì–¸ì œ ëë‚ ì§€ ëª¨ë¥´ë¯€ë¡œ ë§ˆì§€ë§‰ ì…ë ¥ì— EOSë¥¼ ë‚ ë ¤ ì£¼ì–´ì•¼ í•œë‹¤. ì‹¬í™” ìˆœí™˜ ì‹ ê²½ë§ì˜ ìˆ˜ì‹ì  ì´í•´ Vanilla RNNì˜ ìˆ˜ì‹ì€ ì´ì „ì— ê°„ë‹¨íˆ ë‹¤ë£¨ì—ˆë‹¤. ì´ì œ LSTMê³¼ GRUë„ ìˆ˜ì‹ìœ¼ë¡œ ì ‘ê·¼í•´ ë³´ì. íŠ¹ì§•ì´ ì—¬ëŸ¬ì°¨ì›ìœ¼ë¡œ ë˜ì–´ìˆëŠ”ë°, ì´ Forget gate ë˜í•œ ì—¬ëŸ¬ ì°¨ì›ìœ¼ë¡œ ë˜ì–´ìˆì–´ íŠ¹ì§•ë³„ë¡œ ê¸°ì–µí• ì§€ ë§ì§€ë¥¼ ê²°ì •í•  ìˆ˜ ìˆë‹¤. Reset gateëŠ” Hidden stateì—ì„œ ë°”ë¡œ ìŠëŠ” Forget gateì™€ëŠ” ë‹¤ë¥´ê²Œ í˜„ì¬ Featureë¥¼ ë½‘ì„ ë•Œ ì–¼ë§Œí¼ ìŠì–´ì¤„ ê²ƒì¸ê°€ë¥¼ ê²°ì •í•˜ëŠ” ë¶€ë¶„ì´ë‹¤. í° ë§¥ë½ì—ì„œëŠ” ê¸°ì–µí•˜ê³  ìˆì–´ì•¼ í•˜ì§€ë§Œ, í˜„ì¬ Featureë¥¼ ë½‘ì„ ë•ŒëŠ” ë°©í•´ê°€ ë  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ìŠê²Œí•˜ëŠ” ì—­í• ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ë˜ì™€ ê°™ì€ ìƒí™©ì¼ë•Œ, ë§ˆì§€ë§‰ ë°• ì•„ë¬´ê°œì˜ ë‹µì„ ì¶”ë¡ í•˜ê³ ì í•œë‹¤ë©´, ë¨¼ì € â€œë‚˜ëŠ” ì‚¬ê³¼ê°€ ì¢‹ë‹¤.â€, â€œë„ˆëŠ” ê³¼ì¼ì„ ì‹«ì–´í•œë‹¤.â€ë¼ëŠ” ë¬¸ì¥ 2ê°œëŠ” \u001cë¶„ë¦¬ê°€ ëœ ë¬¸ì¥ì´ì§€ë§Œ â€œë‚˜ëŠ” ì‚¬ê³¼ê°€ ì¢‹ë‹¤â€ ë‚´ì—ì„œëŠ” â€˜ë‚˜â€™í•˜ê³  â€˜ì‚¬ê³¼â€™ëŠ” ì˜ ê¸°ì–µì´ ë˜ì–´ì•¼ í•˜ì§€ë§Œ â€œë„ˆëŠ” ê³¼ì¼ì„ ì‹«ì–´í•œë‹¤â€ë¼ëŠ” ë¬¸ì¥ì€ ë‹¤ë¥¸ ë¬¸ì¥ì´ë¯€ë¡œ ê¸°ì–µì´ ì•ˆë˜ì–´ì•¼ í•  ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, â€œë‚˜ëŠ” ì–´ë–¤ ê³¼ì¼ì´ ë¨¹ê³  ì‹¶ì„ê¹Œ?â€ì— ë‹µì„ í•˜ë ¤ë©´, ìµœê·¼ì— ë¬¸ì¥ì¸ â€œë„ˆëŠ” ê³¼ì¼ì„ ì‹«ì–´í•œë‹¤â€ì—ì„œëŠ” ì¶”ë¡ í•  ë•Œ í•„ìš”í•œ ì •ë³´ê°€ ì—†ê¸° ë•Œë¬¸ì— ê·¸ ì´ì „ ë¬¸ì¥ì¸ â€œë‚˜ëŠ” ì‚¬ê³¼ê°€ ì¢‹ë‹¤.â€ëŠ” contextë¥¼ ê³„ì†í•´ì„œ ê°€ì§€ê³  ìˆì–´ì•¼í•œë‹¤. ì´ ì •ë³´ê°€ Hidden stateë¥¼ íƒ€ê³  ì›€ì§ì—¬ì•¼ í•˜ëŠ” ì •ë³´ì´\u001dê³ , ì—¬ê¸°ì„œ â€œë‚˜ëŠ” ì‚¬ê³¼ê°€ ì¢‹ë‹¤.â€ì™€ â€œë„ˆëŠ” ê³¼ì¼ì„ ì‹«ì–´í•œë‹¤.â€ë¼ëŠ” ë¬¸ì¥ì„ êµ¬ë¶„í•˜ì—¬ ë‹¨ê³„ì ìœ¼ë¡œ í™œìš©í•˜ì§€ ì•Šê¸° ìœ„í•œ ì‘ì—…ì´ Reset gateë¥¼ í†µí•œ ì‘ì—…ì´ë‹¤. Reset gateì™€ ë‹¤ë¥´ê²Œ Hidden stateì— ì§ì ‘ì ìœ¼ë¡œ ê³±í•´ì ¸ì„œ ì´ì „ time step Hidden Stateì—ì„œ ê¸°ì–µì„ ìŠì–´ë²„ë¦¬ê²Œ í•˜ëŠ” ì—­í• ì„ í•œë‹¤. ìŠì–´ë²„ë¦° ë¶€ë¶„ë§Œí¼ì„ ë‹¤ì‹œ ìƒˆë¡œìš´ ì •ë³´ë¡œ ë³´ì¶©í•˜ê¸° ìœ„í•´ 1ì—ì„œ ëº€ ë§Œí¼ì„ ìƒˆë¡œìš´ ì…ë ¥ì˜ ê²°ê³¼ì— ê³±í•´ì¤€ë‹¤. ì´ì „ time stepì˜ Hidden stateê°€ ë“¤ì–´ì™”ì„ ë•Œ reset gateë¥¼ í†µí•´ ì œì–´ê°€ ëœ ê²ƒì„ ê°€ì§€ê³  í˜„ì¬ Featureë“¤ì„ ë½‘ì•„ì£¼ê²Œ ë˜ê³ , Forget gateì—ì„œ ì˜í•´ì„œ ì œì–´ê°€ ëœ ë§Œí¼ ë„˜ì–´ì˜¤ê³  Forget gateì— ì˜í•´ì„œ ìƒë³´ì ì¸ ë§Œí¼ ë‹¤ì‹œ ìƒˆë¡œ ë½‘ì€ Featureë¥¼ ì…ë ¥ì„ ë°›ì•„ì„œ ë‹¤ìŒ ì¶œë ¥ìœ¼ë¡œ ë‚˜ê°€ê²Œ ëœë‹¤. ê·¸ë ‡ê¸°ì— ê°’ì´ -1~1ë¡œ boundë˜ì–´ìˆì–´ LSTMê³¼ ë‹¤ë¥´ê²Œ tanhí•¨ìˆ˜ê°€ í•„ìš”í•˜ì§€ ì•Šë‹¤. ìˆœì°¨ ì‹ ê²½ë§ì—ì„œ Tensorì˜ ì´í•´ ë°ì´í„°ê°€ Featureê°™ì€ ê²½ìš°ì—ëŠ” í•­ìƒ ê½‰ì°¨ê²Œ ë˜ëŠ”ë°, ìˆœì°¨ë°ì´í„° ê°™ì€ ê²½ìš°ì—ëŠ” ê¸¸ì´ê°€ Lë³´ë‹¤ ì§§ì„ ìˆ˜ ìˆë‹¤. ê·¸ëŸ° ê²½ìš°ì—ëŠ” ì•ì„ 0ìœ¼ë¡œ ì±„ì›Œì¤€ë‹¤.(pre-padding) ì¶œë ¥ì´ ë‚˜ì˜¤ëŠ” ì‹œì ì€ ê³ ì •ë˜ë¯€ë¡œ ì¼ê´€ë˜ê²Œ ì•ìª½ìœ¼ë¡œ ì •ë ¬ëœ ì¶œë ¥ì´ ë‚˜ì˜¬ ìˆ˜ ìˆê²Œ í•˜ê¸° ìœ„í•´ì„œ ë’·ë¶€ë¶„ì„ 0ìœ¼ë¡œ ì±„ìš´ë‹¤. ìˆœí™˜ ì‹ ê²½ë§ì˜ í•™ìŠµë²• ì‹œê°„ì— ëŒ€í•´ì„œ í¼ì³ìˆê³ , ì¶”ê°€ì ìœ¼ë¡œ Batchë¡œë„ í¼ì³ ì£¼ì–´ì•¼ í•˜ëŠ”ë°, ì¦‰, ì•„ë˜ì™€ ê°™ì€ êµ¬ì¡°ê°€ Batch sizeë§Œí¼ ë” ìˆì–´ì£¼ì–´ì•¼ í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ê·¸ë˜ì„œ ì‹œê°„ì ìœ¼ë¡œ í¼ì¹  ë•Œ ì—­ì „íŒŒë¥¼ ìœ„í•œ ì¶”ê°€ì ì¸ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ë‹¤. ì¼ë°˜ì ì¸ CNNì´ë‚˜ DNNì€ ì‹œê°„ì ìœ¼ë¡œ í¼ì¹˜ëŠ” ê²ƒì´ ì—†ê¸° ë•Œë¬¸ì— Batchì— ëŒ€í•´ì„œ í¬ê²Œ ì—„ê²©í•˜ì§€ ì•Šë‹¤. í•˜ì§€ë§Œ RNNì€ ì•„ë˜ì™€ ê°™ì´ ì‹œê°„ì ìœ¼ë¡œ í¼ì¹˜ê¸° ë•Œë¬¸ì— Batch sizeë¥¼ ëŠ˜ë¦¬ëŠ”ë° ì—„ê²©í•˜ë‹¤. ìˆœì°¨ ë°ì´í„°ì˜ ê¸¸ì´ Lì´ ë§¤ìš° í´ ê²½ìš°, ì‹œê°„ í¼ì¹¨ì´ ëŠ˜ì–´ë‚˜ë©´ì„œ í•„ìš” ë©”ëª¨ë¦¬ê°€ Lë°° ì¦ê°€í•œë‹¤. ê·¸ ì´ìœ ëŠ” ê¸¸ì´ê°€ 1ê°œ ì”© ëŠ˜ì–´ë‚  ë•Œë§ˆ\u001cë‹¤ í¼ì¹¨ì„ í•˜ë‚˜ì”© ë” í•´ì•¼í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ ë•Œ B(Batch)ë¥¼ í•œë²ˆì— ê³„ì‚°í•˜ë¯€ë¡œ, ì–•ì€ ì‹ ê²½ë§ì— ë¹„í•´ í›¨ì”¬ í° ë©”ëª¨ë¦¬ê°€ í•„ìš”. ê¸¸ì´ Lì˜ ì…ë ¥ì„ ê¸¸ì´ Të¡œ ìª¼ê°œì–´ ìˆœì„œëŒ€ë¡œ í•™ìŠµí•œë‹¤. ì¦‰, Time stepì´ T ì´ìƒ ë–¨ì–´ì§„ ì…-ì¶œë ¥ ê´€ê³„ëŠ” í•™ìŠµë˜ì§€ ì•ŠëŠ”ë‹¤. Hidden stateì™€ Cell stateë¥¼ í†µí•´ Forward propagationì—ì„œëŠ” ì˜ ì¶”ë¡  í•  ìˆ˜ ìˆë„ë¡ ë„˜ê²¨ ì£¼ì§€ë§Œ, Back propagationì—ì„œëŠ” ê·¸ ê´€ê³„ë¥¼ ì„œë¡œ ë„˜ê²¨ì£¼ì§€ ëª»í•œë‹¤. ë§Œì•½ ì „ë¶€ë‹¤ ì—°ê²°ì‹œí‚¨ ê´€ê³„ë¥¼ í•™ìŠµì‹œì¼œì•¼ í•œë‹¤ë©´ Truncated BPTTê°€ ì•„ë‹Œ ê¸¸ì´ê°€ Lì¸ ëª¨ë“  ë°ì´í„°ë¥¼ í•™ìŠµì‹œì¼œì•¼ í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ Truncated BPTTë¥¼ ì‚¬ìš©í•  ì‹œ ë°˜ë“œì‹œ ì˜í–¥ì„ ì£¼ëŠ” ë°ì´í„° ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì¹¨í•´í•˜ì§€ ì•Šê²Œ Të¡œ ì ì ˆí•˜ê²Œ ë‚˜ëˆ„ì–´ì¡ŒëŠ”ì§€, ìš°ë¦¬ê°€ í•™ìŠµí•˜ê³ ì í•˜\bëŠ” ê²ƒì´ ì–´ëŠ ì •ë„ì˜ ì‹œê°„ì°¨ì´ê¹Œì§€ ìš°ë¦¬ê°€ ì—°ê´€ì„±ì„ ë´ì•¼ í•˜ëŠ”ì§€ë¥¼ ì—¼ë‘í•´ ë‘ê³  í•™ìŠµì„ ì‹œì¼œì•¼ í•œë‹¤. ë§Œì•½ ì—°ê´€ì„±ì´ ìˆëŠ” ë°ì´í„°ì˜ ì£¼ê¸°(ë°ì´í„°ê°„ì˜ ì‹œì  ì°¨ì´)ê°€ í¬ê³  Gradientê°€ ëŠê¸°ì§€ ì•Šê³  ì—°ê²°ë˜ì–´ ì—…ë°ì´íŠ¸ê°€ ì´ë£¨ì–´ì ¸ì•¼ í•œë‹¤ë©´, ìµœëŒ€í•œ Batch Sizeë¥¼ ê·¹ë‹¨ì  ë‚®ì¶”ê³ , ìµœëŒ€í•œ Memoryê°€ í° GPUë¥¼ ì‚¬ìš©í•´ì„œ ìµœëŒ€í•œ ê¸´ ê¸¸ì´ë¥¼ í•™ìŠµí•´ ì£¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•´ì•¼ í•  ê²ƒì´ë‹¤.","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"DenseNet êµ¬í˜„ ë° í•™ìŠµ","slug":"deep_learning_07","date":"2020-01-12T06:23:36.000Z","updated":"2020-01-20T16:27:52.245Z","comments":true,"path":"2020/01/12/deep_learning_07/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/12/deep_learning_07/","excerpt":"","text":"DenseNetwork êµ¬í˜„ ë° í•™ìŠµ í•„ìëŠ” êµ¬ê¸€ colabì„ í†µí•´ í•™ìŠµì‹œì¼°ìœ¼ë©°, @tf.functionì„ ì‚¬ìš©í•˜ê¸°ìœ„í•´ tensorflow 2.0 ë²„ì ¼ì„ ì‹œìš©í•˜ì˜€ë‹¤. 1!pip install tensorflow==2.0.0-beta1 ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ import 12import tensorflow as tfimport numpy as np ìœ„ì—ì„œ ì„¤ì¹˜í•œ tensorflowì˜ ë²„ì „ì´ 2.0ì¸ì§€ í™•ì¸ 1print(tf.__version__) Hyper parameterë¥¼ ì„¤ì • 1EPOCHS = 10 DenseUnit êµ¬í˜„1234567891011121314151617class DenseUnit(tf.keras.Model): def __init__(self, filter_out, kernel_size): super(DenseUnit, self).__init__() # batch normalization -&gt; ReLu -&gt; Conv Layer # ì—¬ê¸°ì„œ ReLu ê°™ì€ ê²½ìš°ëŠ” ë³€ìˆ˜ê°€ ì—†ëŠ” Layerì´ë¯€ë¡œ ì—¬ê¸°ì„œ êµ³ì´ initialize í•´ì£¼ì§€ ì•ŠëŠ”ë‹¤. (callìª½ì—ì„œ ì‚¬ìš©í•˜ë©´ ë˜ë¯€ë¡œ) # Pre-activation êµ¬ì¡°ëŠ” ë˜‘ê°™ì´ ê°€ì ¸ê°€ë˜, concatenate êµ¬ì¡°ë¡œ ë§Œë“¤ì–´ì£¼ì–´ì•¼í•¨ì— ì£¼ì˜í•˜ì!! self.bn = tf.keras.layers.BatchNormalization() self.conv = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same') self.concat = tf.keras.layers.Concatenate() def call(self, x, training=False, mask=None): # x : (Batch ê°¯ìˆ˜, Height, width, Channel_in) # training ê¼­ ìŠì–´ë²„ë¦¬ì§€ ë§ì!! h = self.bn(x, training=training) h = tf.nn.relu(h) h = self.conv(h) # h : (Batch, height, width, filter_output) zero-paddingì„ í–ˆìœ¼ë¯€ë¡œ filterë§Œ ë°”ë€œ return self.concat([x, h]) # (Batch, height, width, (channel_in + filter_output)) DenseLayer êµ¬í˜„1234567891011class DenseLayer(tf.keras.Model): def __init__(self, num_unit, growth_rate, kernel_size): super(DenseLayer, self).__init__() self.sequence = list() for idx in range(num_unit): self.sequence.append(DenseUnit(growth_rate, kernel_size)) def call(self, x, training=False, mask=None): for unit in self.sequence: x = unit(x, training=training) return x Transition Layer êµ¬í˜„ Maxpoolingì„ í•´ì¤„ ë•Œ í•„ìš”í•¨. ì˜ˆë¥¼ ë“¤ì–´, DenseLayerë¥¼ ì‚¬ìš©ì„ í•˜ê²Œ ë˜ë©´, Growth_rate=32, num_unit=8ì¸ ê²½ìš°ì—ëŠ”, 32X8 ë§Œí¼ channelì˜ ìˆ˜ê°€ ê¸‰ê²©í•˜ê²Œ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì— ë„ˆë¬´ ì»¤ì§ˆ ìˆ˜ ìˆë‹¤. ì´ëŸ´ ê²½ìš° ì´ Transition Layerë¥¼ í†µí•´ Pooling ì „ì— channelì˜ ìˆ˜ë¥¼ ì¡°ì ˆí•´ì£¼ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ë‹¤. 123456789101112class TransitionLayer(tf.keras.Model): def __init__(self, filters, kernel_size): super(TransitionLayer, self).__init__() # transitionì„ í•  ê²½ìš°ì—ëŠ” ì´ë ‡ê²Œ convolutionì„ í•´ì„œ ë‹¨ìˆœíˆ filter ê°œìˆ˜ë¥¼ ë³€ê²½ë§Œ í•´ì¤€ë’¤ # ê·¸ ë‹¤ìŒì— Maxpoolingì„ í•´ì£¼ëŠ” ì‹ìœ¼ë¡œ êµ¬í˜„ì´ ëœë‹¤. self.conv = tf.keras.layers.Conv2D(filters, kernel_size, padding='same') self.pool = tf.keras.layers.MaxPool2D() def call(self, x, training=False, mask=None): # ì—¬ê¸°ì„œëŠ” Batch normalizationì´ ì—†ê¸° ë•Œë¬¸ì— trainingì„ ì•ˆì¨ì¤˜ë„ ëœë‹¤. x = self.conv(x) return self.pool(x) ëª¨ë¸ ì •ì˜123456789101112131415161718192021222324252627282930313233class DenseNet(tf.keras.Model): def __init__(self): super(DenseNet, self).__init__() self.conv1 = tf.keras.layers.Conv2D(8, (3, 3), padding='same', activation='relu') # 28x28x8 # num_unit=2, growth_rate=4, kernel_size=(3,3) # num_unitì€ ResNetê³¼ ë™ì¼ self.dl1 = DenseLayer(2, 4, (3, 3)) # 28x28x(8+2*4) self.tr1 = TransitionLayer(16, (3, 3)) # 14x14x16 self.dl2 = DenseLayer(2, 8, (3, 3)) # 14x14x(16 + 2*8) self.tr2 = TransitionLayer(32, (3, 3)) # 7x7x32 self.dl3 = DenseLayer(2, 16, (3, 3)) # 7x7x(32+2*16) self.flatten = tf.keras.layers.Flatten() self.dense1 = tf.keras.layers.Dense(128, activation='relu') self.dense2 = tf.keras.layers.Dense(10, activation='softmax') def call(self, x, training=False, mask=None): x = self.conv1(x) x = self.dl1(x, training=training) x = self.tr1(x) x = self.dl2(x, training=training) x = self.tr2(x) x = self.dl3(x, training=training) x = self.flatten(x) x = self.dense1(x) return self.dense2(x) í•™ìŠµ, ë°ìŠ¤íŠ¸ ë£¨í”„ ì •ì˜1234567891011121314151617181920# Implement training loop@tf.functiondef train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy): with tf.GradientTape() as tape: predictions = model(images, training=True) loss = loss_object(labels, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train_loss(loss) train_accuracy(labels, predictions)# Implement algorithm test@tf.functiondef test_step(model, images, labels, loss_object, test_loss, test_accuracy): predictions = model(images, training=False) t_loss = loss_object(labels, predictions) test_loss(t_loss) test_accuracy(labels, predictions) ë°ì´í„°ì…‹ ì¤€ë¹„12345678910mnist = tf.keras.datasets.mnist(x_train, y_train), (x_test, y_test) = mnist.load_data()x_train, x_test = x_train / 255.0, x_test / 255.0x_train = x_train[..., tf.newaxis].astype(np.float32)x_test = x_test[..., tf.newaxis].astype(np.float32)train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) í•™ìŠµ í™˜ê²½ ì •ì˜ëª¨ë¸ ìƒì„±, ì†ì‹¤í•¨ìˆ˜, ìµœì í™” ì•Œê³ ë¦¬ì¦˜, í‰ê°€ì§€í‘œ ì •ì˜12345678910111213# ëª¨ë¸ ìƒì„±model = DenseNet()# ì†ì‹¤í•¨ìˆ˜ ì •ì˜ ë° ìµœì í™” ê¸°ë²• ì •ì˜loss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()# í‰ê°€ì§€í‘œ ì •ì˜train_loss = tf.keras.metrics.Mean(name='train_loss')train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')test_loss = tf.keras.metrics.Mean(name='test_loss')test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy') í•™ìŠµ ë£¨í”„ ë™ì‘ ì–´ë–¤ ê²ƒì€ DenseNetì´ ì„±ëŠ¥ì´ ë” ì¢‹ê²Œ ë‚˜ì˜¤ê³  ì–´ë–¤ ê²ƒì€ ResNetì´ ë” ì¢‹ê²Œ ë‚˜ì˜¤ê¸°ë„ í•œë‹¤. ë˜í•œ parameterì— ë”°ë¼ ë‹¤ë¥´ë‹¤. Resnetì€ ì¢€ ë” ì•ˆì •ì ì´ê²Œ ìˆ˜ë ´í•˜ì§€ë§Œ Densenetì€ ì¢€ fluctuateí•˜ë‹¤ëŠ” ì ì„ ìœ ì˜í•˜ì. 123456789101112131415161718for epoch in range(EPOCHS): for images, labels in train_ds: train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy) for test_images, test_labels in test_ds: test_step(model, test_images, test_labels, loss_object, test_loss, test_accuracy) template = \"Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;\" print(template.format(epoch+1, train_loss.result(), train_accuracy.result() * 100, test_loss.result(), test_accuracy.result() * 100))#train_loss.reset_states()#train_accuracy.reset_states()#test_loss.reset_states()#test_accuray.reset_states()","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"Residual Network êµ¬í˜„ ë° í•™ìŠµ","slug":"deep_learning_06","date":"2020-01-12T06:17:16.000Z","updated":"2020-01-20T15:58:35.374Z","comments":true,"path":"2020/01/12/deep_learning_06/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/12/deep_learning_06/","excerpt":"","text":"Residual Network GoogLeNet ì´í›„ì— ë‚˜ì˜¨ ëª¨ë¸ë¡œ Residual êµ¬ì¡°ë¥¼ Skip connection êµ¬ì¡°ë¥¼ ê°–ìœ¼ë©°, pre-activationì„ ê°–ëŠ” Residual unitì„ ë¨¼ì € ë§Œë“  í›„ì—ì„œ Resnet Unitì„ ì—°ê²°í•˜ì•¼ ë§Œë“¤ ResnetLayerë¥¼ ë§Œë“¤ì–´ Residual Layerë¥¼ êµ¬í˜„í•  ê²ƒì´ë‹¤. ê·¸ í›„ ì „ì²´ì ì¸ ResNet Modelë¥¼ ìƒì„±í•˜ì—¬ ì•ì„œ êµ¬í˜„í•´ë³¸ VGG-16ê³¼ ì„±ëŠ¥ì„ ë¹„êµí•˜ê¸° ìœ„í•´ ë™ì¼í•œ mnist ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµê³¼ ê²€ì¦ì„ í•´ë³¼ ê²ƒì´ë‹¤. DenseNetwork êµ¬í˜„ ë° í•™ìŠµ í•„ìëŠ” êµ¬ê¸€ colabì„ í†µí•´ í•™ìŠµì‹œì¼°ìœ¼ë©°, @tf.functionì„ ì‚¬ìš©í•˜ê¸°ìœ„í•´ tensorflow 2.0 ë²„ì ¼ì„ ì‹œìš©í•˜ì˜€ë‹¤. 1!pip install tensorflow==2.0.0-beta1 ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ import 12import tensorflow as tfimport numpy as np ìœ„ì—ì„œ ì„¤ì¹˜í•œ tensorflowì˜ ë²„ì „ì´ 2.0ì¸ì§€ í™•ì¸ 1print(tf.__version__) Hyper parameterë¥¼ ì„¤ì • 1EPOCHS = 10 Residual Unit êµ¬í˜„123456789101112131415161718192021222324252627282930313233class ResidualUnit(tf.keras.Model): def __init__(self, filter_in, filter_out, kernel_size): super(ResidualUnit, self).__init__() # batch normalization -&gt; ReLu -&gt; Conv Layer # ì—¬ê¸°ì„œ ReLu ê°™ì€ ê²½ìš°ëŠ” ë³€ìˆ˜ê°€ ì—†ëŠ” Layerì´ë¯€ë¡œ ì—¬ê¸°ì„œ êµ³ì´ initialize í•´ì£¼ì§€ ì•ŠëŠ”ë‹¤. (callìª½ì—ì„œ ì‚¬ìš©í•˜ë©´ ë˜ë¯€ë¡œ) self.bn1 = tf.keras.layers.BatchNormalization() self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding=\"same\") self.bn2 = tf.keras.layers.BatchNormalization() self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding=\"same\") # identityë¥¼ ì–´ë–»ê²Œ í• ì§€ ì •ì˜ # ì›ë˜ Residual Unitì„ í•˜ë ¤ë©´ ìœ„ì˜ ìˆœì„œë¡œ ì§„í–‰í•œ ë’¤, ë°”ë¡œ Xë¥¼ ë”í•´ì„œ ë‚´ë³´ë‚´ë©´ ë˜ëŠ”ë°, # ì´ Xì™€ ìœ„ì˜ ê³¼ì •ì„ í†µí•´ ì–»ì€ Feature mapê³¼ ì°¨ì›ì´ ë™ì¼í•´ì•¼ ë”í•˜ê¸° ì—°ì‚°ì´ ê°€ëŠ¥í•  ê²ƒì´ë¯€ë¡œ # ì¦‰, ìœ„ì—ì„œ filter_inê³¼ filter_outì´ ê°™ì•„ì•¼ í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. # í•˜ì§€ë§Œ, ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì€ ì‘ì—…ì„ ê±°ì¹œë‹¤. if filter_in == filter_out: self.identity = lambda x: x else: self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding=\"same\") # ì•„ë˜ì—ì„œ batch normalizationì€ trainí• ë•Œì™€ inferenceí•  ë•Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë‹¬ë¼ì§€ë¯€ë¡œ ì˜µì…˜ì„ ì¤„ê²ƒì´ë‹¤. def call(self, x, training=False, mask=None): h = self.bn1(x, training=training) h = tf.nn.relu(h) h = self.conv1(h) h = self.bn2(h, training=training) h = tf.nn.relu(h) h = self.conv2(h) return self.identity(x) + h Residual Layer êµ¬í˜„12345678910111213141516171819class ResnetLayer(tf.keras.Model): # ì•„ë˜ arg ì¤‘ filter_in : ì²˜ìŒ ì…ë ¥ë˜ëŠ” filter ê°œìˆ˜ë¥¼ ì˜ë¯¸ # Resnet LayerëŠ” Residual unitì´ ì—¬ëŸ¬ê°œê°€ ìˆê²Œë”í•´ì£¼ëŠ”ê²ƒì´ë¯€ë¡œ # filters : [32, 32, 32, 32]ëŠ” 32ì—ì„œ 32ë¡œ Residual unitì´ ì—°ê²°ë˜ëŠ” í˜•íƒœ def __init__(self, filter_in, filters, kernel_size): super(ResnetLayer, self).__init__() self.sequnce = list() # [16] + [32, 32, 32] # ì•„ë˜ëŠ” listì˜ lengthê°€ ë” ì‘ì€ ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ zipì´ ë˜ì–´ì„œ ëŒì•„ê°€ê¸° ë•Œë¬¸ì— # ì•ì˜ listì˜ ë§ˆì§€ë§‰ element 32ëŠ” ë¬´ì‹œëœë‹¤. # zip([16, 32, 32, 32], [32, 32, 32]) for f_in, f_out in zip([filter_in] + list(filters), filters): self.sequnce.append(ResidualUnit(f_in, f_out, kernel_size)) def call(self, x, training=False, mask=None): for unit in self.sequnce: # ìœ„ì˜ batch normalizationì—ì„œ trainingì´ ì“°ì˜€ê¸°ì— ì—¬ê¸°ì„œ ë„˜ê²¨ ì£¼ì–´ì•¼ í•œë‹¤. x = unit(x, training=training) return x ëª¨ë¸ ì •ì˜1234567891011121314151617181920212223242526272829class ResNet(tf.keras.Model): def __init__(self): super(ResNet, self).__init__() self.conv1 = tf.keras.layers.Conv2D(8, (3,3), padding=\"same\", activation=\"relu\") # 28X28X8 self.res1 = ResnetLayer(8, (16, 16), (3, 3)) # 28X28X16 self.pool1 = tf.keras.layers.MaxPool2D((2,2)) # 14X14X16 self.res2 = ResnetLayer(16, (32, 32), (3, 3)) # 14X14X32 self.pool2 = tf.keras.layers.MaxPool2D((2,2)) # 7X7X32 self.res3 = ResnetLayer(32, (64, 64), (3, 3)) # 7X7X64 self.flatten = tf.keras.layers.Flatten() self.dense1 = tf.keras.layers.Dense(128, activation=\"relu\") self.dense2 = tf.keras.layers.Dense(10, activation=\"softmax\") def call(self, x, training=False, mask=None): x = self.conv1(x) x = self.res1(x, training=training) x = self.pool1(x) x = self.res2(x, training=training) x = self.pool2(x) x = self.res3(x, training=training) x = self.flatten(x) x = self.dense1(x) return self.dense2(x) í•™ìŠµ,í…ŒìŠ¤íŠ¸ ë£¨í”„ ì •ì˜12345678910111213141516171819202122# Implement training loop@tf.functiondef train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy): with tf.GradientTape() as tape: # training=True ê¼­ ë„£ì–´ì£¼ê¸°!! predictions = model(images, training=True) loss = loss_object(labels, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train_loss(loss) train_accuracy(labels, predictions)# Implement algorithm test@tf.functiondef test_step(model, images, labels, loss_object, test_loss, test_accuracy): # training=False ê¼­ ë„£ì–´ì£¼ê¸°!! predictions = model(images, training=False) t_loss = loss_object(labels, predictions) test_loss(t_loss) test_accuracy(labels, predictions) ë°ì´í„°ì…‹ ì¤€ë¹„12345678910mnist = tf.keras.datasets.mnist(x_train, y_train), (x_test, y_test) = mnist.load_data()x_train, x_test = x_train / 255.0, x_test / 255.0x_train = x_train[..., tf.newaxis].astype(np.float32)x_test = x_test[..., tf.newaxis].astype(np.float32)train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) í•™ìŠµ í™˜ê²½ ì •ì˜ëª¨ë¸ ìƒì„±, ì†ì‹¤ í•¨ìˆ˜, ìµœì í™” ì•Œê³ ë¦¬ì¦˜, í‰ê°€ì§€í‘œ ì •ì˜12345678910111213# ëª¨ë¸ ìƒì„±model = ResNet()# ì†ì‹¤í•¨ìˆ˜ ì •ì˜ ë° ìµœì í™” ê¸°ë²• ì •ì˜loss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()# í‰ê°€ì§€í‘œ ì •ì˜train_loss = tf.keras.metrics.Mean(name='train_loss')train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')test_loss = tf.keras.metrics.Mean(name='test_loss')test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy') í•™ìŠµ ë£¨í”„ ë™ì‘12345678910111213for epoch in range(EPOCHS): for images, labels in train_ds: train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy) for test_images, test_labels in test_ds: test_step(model, test_images, test_labels, loss_object, test_loss, test_accuracy) template = \"Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;\" print(template.format(epoch+1, train_loss.result(), train_accuracy.result() * 100, test_loss.result(), test_accuracy.result() * 100))","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"Scrapy ì›¹ í¬ë¡¤ë§ 02 - Spider, Scrapy selectors, Items","slug":"Crawling_01","date":"2020-01-10T18:39:47.000Z","updated":"2020-01-28T08:26:14.810Z","comments":true,"path":"2020/01/11/Crawling_01/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/11/Crawling_01/","excerpt":"","text":"Spider Spiderì˜ ì¢…ë¥˜ (ì°¸ê³ ë¡œ, ì•„ë˜ 3ê°€ì§€ ì¢…ë¥˜ì˜ SpiderëŠ” ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤.) CrawlSpider XMLFeedSpider CSVFeedSpider SitemapSpider 12# ì—¬ëŸ¬ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ê¸° ìœ„í•œ spiderë¥¼ ìƒì„±scrapy genspider many_site hub.scraping.com spider í´ë”ì˜ many_site.pyíŒŒì¼ì—ì„œ ì½”ë“œë¥¼ ì‘ì„±í•˜ê¸°ì— ì•ì„œ settings.pyì—ì„œ naverì™€ daumì€ robots.txtì—ì„œ í¬ë¡¤ë§ì„ ë¶ˆí—ˆí•˜ê¸°ì— ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì •ì‘ì—…ì„ í•´ì£¼ì–´ì•¼ í¬ë¡¤ë§ì´ ê°€ëŠ¥í•˜ë‹¤. ì•„ë˜ì˜ ì½”ë“œì²˜ëŸ¼ ì—¬ëŸ¬ ë„ë©”ì¸ì„ í¬ë¡¤ë§í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ í¬ê²Œ 3ê°€ì§€ ì •ë„ê°€ ìˆë‹¤. ë‹¤ìŒê³¼ ê°™ì´ 1ë²ˆì§¸ ë°©ë²•ì„ ì‚¬ìš©í•´ì„œ ë¡œê¹… ë° ë¶„ê¸°ì²˜ë¦¬ë¡œ ì—¬ëŸ¬ ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•  ìˆ˜ ìˆê²Œë” ì½”ë“œë¥¼ ìˆ˜ì •í•´ ì£¼ì—ˆë‹¤. Selectorxpath selector ë„ì›€ ì‚¬ì´íŠ¸ https://docs.scrapy.org/en/latest/topics/selectors.html#working-with-xpath www.nextree.co.kr/p6278 css selector ë„ì›€ ì‚¬ì´íŠ¸ https://docs.scrapy.org/en/latest/topics/selectors.html#extension-to-css-selectors crawlingì‹œ í™œìš© tip íƒ€ê²Ÿ ë°ì´í„°ëŠ” í¬ë¡¬ ê°œë°œì ë„êµ¬ ì‚¬ìš© ì„ íƒì ì—°ìŠµ íŒ : scrapy shell ì—ì„œ í…ŒìŠ¤íŠ¸(íš¨ìœ¨ì„±) scrapy shell ë„ë©”ì¸ ì¤‘ìš”(ì™„ì „ ë™ì¹˜ëŠ” ì•„ë‹ˆë‹¤!) get() == extract_first() getall() == extract() CSS ì„ íƒì div#chan div : (ìì†) chanì´ë¼ëŠ” classì†ì„±ê°’ìœ¼ë¡œ ê°–ëŠ” div tagì˜ ì•„ë˜ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  div div#chan &gt; div : (ìì‹) chanì´ë¼ëŠ” classì†ì„±ê°’ìœ¼ë¡œ ê°–ëŠ” div tagì˜ ì§ê³„ìì‹ divë“¤ ::text -&gt; ë…¸ë“œì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ ::attr(name) -&gt; ë…¸ë“œ ì†ì„±ê°’ ì¶”ì¶œ get(default=â€™â€™) : getìœ¼ë¡œ ì¶”ì¶œí•  ë•Œ í•´ë‹¹ì‚¬í•­ì´ ì—†ë‹¤ë©´ ê³µë°±ìœ¼ë¡œ ì¶œë ¥ ì˜ˆì‹œ) response.css(â€˜title::textâ€™).get() : title tagì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ response.css(â€˜div &gt; a::attr(href)â€™).getall() : div tagì˜ ìì‹ a tagì˜ hrefì†ì„±ê°’ ì „ë¶€ ì¶”ì¶œ Xpath ì„ íƒì nodename : ì´ë¦„ì´ nodename ì„ íƒ text() -&gt; ë…¸ë“œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ / : ë£¨íŠ¸ë¶€í„° ì‹œì‘ // : í˜„ì¬ node ë¶€í„° ë¬¸ì„œìƒì˜ ëª¨ë“  ë…¸ë“œ ì¡°íšŒ . : í˜„ì¬ node .. : í˜„ì¬ nodeì˜ ë¶€ëª¨ë…¸ë“œ @ : ì†ì„± ì„ íƒì ì˜ˆì‹œ) response.xpath(â€˜/divâ€™) : ë£¨íŠ¸ë…¸ë“œë¶€í„° ëª¨ë“  div tag ì„ íƒ response.xpath(â€˜//div[@id=â€idâ€]/a/text()â€™).get() : div tag ì¤‘ idê°€ â€˜idâ€™ì¸ ìì‹ a tagì˜ í…ìŠ¤íŠ¸ í•˜ë‚˜ë§Œ ì¶”ì¶œ í˜¼í•© ì‚¬ìš© ê°€ëŠ¥!! response.css(â€˜imgâ€™).xpath(â€˜@srcâ€™).getall() ì‹¤ìŠµ) w3school(ì›¹ì— ê´€í•œ ì •ë³´ë“¤ì´ ìˆëŠ” ì‚¬ì´íŠ¸) ì‹¤ìŠµ ëª©í‘œ : nav ë©”ë‰´ ì´ë¦„ í¬ë¡¤ë§ ì‹¤ìŠµ ê³¼ì • : shell ì‹¤í–‰ -&gt; ì„ íƒì í™•ì¸ -&gt; ì½”ë”© -&gt; ë°ì´í„° ì €ì¥(í”„ë¡œê·¸ë¨ í…ŒìŠ¤íŠ¸) Items êµ¬ì¡°ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ë‚´ê°€ í¬ë¡¤ë§í•  ë°ì´í„°ë¥¼ ì •í™•í•˜ê²Œ êµ¬ë¶„(ì‹ ë¬¸ê¸°ì‚¬ì˜ ì´ë¦„, ë³¸ë¬¸, ì´ë¯¸ì§€ ì´ë ‡ê²Œ êµ¬ì¡°ì ìœ¼ë¡œ êµ¬ë¶„)í•˜ê²Œ êµ¬ì¡°ì ìœ¼ë¡œ ê·œì¹™ì„ ì •í•˜ê³  ê·¸ ê·œì¹™ë“¤ì„ Itemsë¼ëŠ” íŒŒì¼ì•ˆì— ì‘ì„±í•˜ì—¬ ë‚˜ì¤‘ì— Itemsë¥¼ returní•˜\u001dë©´ ëª…í™•í•˜ê²Œ êµ¬ë¶„ëœ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ì—¬ëŸ¬ê°€ì§€ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆë‹¤. spiderëŠ” ì§ì ‘ í¬ë¡¤ë§ì„ í•˜ëŠ” ì—­í• , ItemsëŠ” í¬ë¡¤ë§ ë  íƒ€ê²Ÿ ë°ì´í„°ë¥¼ ëª…í™•íˆ í•´ì£¼ëŠ” ì—­í• ì´ë¼ê³  ìƒê°í•˜ë©´ ë  ê²ƒì´ë‹¤. Scrapy Itemì¥ì  1) ìˆ˜ì§‘ ë°ì´í„°ë¥¼ ì¼ê´€ì„±ìˆê²Œ ê´€ë¦¬ ê°€ëŠ¥ 2) ë°ì´í„°ë¥¼ ì‚¬ì „í˜•(Dict)ë¡œ ê´€ë¦¬, ì˜¤íƒ€ ë°©ì§€ 3) ì¶”í›„ ê°€ê³µ ë° DB ì €ì¥ ìš©ì´ Itemsë¥¼ ì‚¬ìš©í•œ scrapyëŠ” ìƒˆë¡œìš´ ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•  ê²ƒì´ë¯€ë¡œ ìƒˆë¡œìš´ spiderë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤. 1scrapy genspider using_items itnews.com items.py íŒŒì¼ì—ì„œ ìš°ë¦¬ì˜ íƒ€ì¼“ ë°ì´í„°ë¥¼ ì •ì˜í•´ì¤€ë‹¤. items.pyë¥¼ í™œìš©í•˜ê¸° ìœ„í•´ importë¥¼ í•  ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ ì ˆëŒ€ê²½ë¡œë¥¼ ì‚¬ìš©í•œ path ì¶”ê°€ ë°©ë²•ì„ ì‚¬ìš©í•´ì•¼í•œë‹¤. ë‹¤ìŒê³¼ ê°™ì´ itemsì˜ ItArticle classë¥¼ í™œìš©í•˜ì—¬ spiderë¥¼ ì¢€ ë” ê¹”ë”í•˜ê²Œ ì‘ì„±í•  ìˆ˜ ìˆë‹¤.","categories":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/categories/crawling/"}],"tags":[]},{"title":"Scrapy ì›¹ í¬ë¡¤ë§ 01 - í™˜ê²½ì„¤ì • ë° ê¸°ì´ˆ","slug":"Crawling_00","date":"2020-01-09T12:08:12.000Z","updated":"2020-01-23T17:17:25.112Z","comments":true,"path":"2020/01/09/Crawling_00/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/09/Crawling_00/","excerpt":"","text":"Scrapy VS Beautiful SoupBeautiful Soup Beautiful SoupëŠ” ì›¹ ìƒì˜ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ í¬ë¡¤ë§ í•˜ê¸°ìœ„í•œ ë„êµ¬ì´ë©°, ì •ì ì¸ ì •ë³´ë¥¼ ê°€ì ¸ ì˜¬ ìˆ˜ ìˆë‹¤. ì¦‰, í•´ë‹¹ API(URL)ì— ìš”ì²­í–ˆì„ë•Œ ë°”ë¡œ ê°€ì ¸ì˜¬ìˆ˜ ìˆëŠ” ì •ë³´ë“¤ë§Œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤. ì‹œê°„ì´ ì¢€ ë” ê±¸ë¦° í›„ì— ë‚˜ì˜¤ëŠ” ì •ë³´ë“¤ì€ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ë‹¤. ì§„ì… ì¥ë²½ì´ ë§¤ìš° ë‚®ê³  ê°„ê²°í•´ì„œ, ì…ë¬¸ ê°œë°œìì—ê²Œ ì•ˆì„±ë§ì¶¤ì´ë‹¤. ê·¸ë¦¬ê³  ì´ ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ìŠ¤ìŠ¤ë¡œ í¬ë¡¤ë§ì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ urlib2 ë˜ëŠ” requests ëª¨ë“ˆì„ í†µí•´ HTML ì†ŒìŠ¤ë¥¼ ê°€ì ¸ì™€ì•¼ í•œë‹¤. Scrapy ScrapyëŠ” Pythonìœ¼ë¡œ ì‘ì„±ëœ Frameworkì´ë©°, spider(bot)ì„ ì‘ì„±í•´ì„œ í¬ë¡¤ë§ì„ í•œë‹¤. Scrapyì—ì„œëŠ” ì§ì ‘ Beautiful Soup ì´ë‚˜ lxmlì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ Beautiful Soupì—ì„œëŠ” ì§€ì›í•˜ì§€ ì•ŠëŠ” Xpathë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, Xpathë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡´ì¨ ë³µì¡í•œ HTMLì†ŒìŠ¤ë¥¼ ì‰½ê²Œ í¬ë¡¤ë§ í•  ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤. ë˜í•œ Xpathë¥¼ í†µí•œ crawlingì´ ê°€ëŠ¥í•œ ëª¨ë“ˆë¡œëŠ” seleniumë„ ì¡´ì¬í•œë‹¤. seleniumë„ Scrapyì™€ ì—°ë™í•´ì„œ ê°€ëŠ¥í•˜ë‹¤. Anaconda env ë¨¼ì € ì‚¬ì „ì— anacondaë¥¼ í†µí•´ ê°€ìƒí™˜ê²½ì„ì„ ë§Œë“¤ì–´ì¤€ë‹¤. 1234567891011121314# env ìƒì„±conda create -n env_name python=3.5# env ë¦¬ìŠ¤íŠ¸ ë³´ê¸°conda env list# env í™œì„±í™”conda activate env_name# env ë¹„í™œì„±í™”conda deavtivate# env ì‚­ì œconda env remove -n env_name Scrapy í™˜ê²½ì„¤ì • ë¨¼ì € ê°€ìƒí™˜ê²½ì„ í™œì„±í™”ì‹œì¼œì¤€ í›„ì—, spider botì„ ë§Œë“¤ í´ë”ì˜ ìƒìœ„ í´ë”ì—ì„œ ë‹¤ìŒì˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰ì‹œì¼œì¤€ë‹¤. 12345conda activate env_namecd ..scrapy startproject project_name ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì •í•œ projectëª…ì„ ê°–ëŠ” í´ë”ê°€ ë§Œë“¤ì–´ì§€ë©°, í•„ìëŠ” section01_2ë¼ê³  ëª…ëª…í–ˆë‹¤. ìœ„ì˜ ë‹¨ê³„ê¹Œì§€ ì‹¤í–‰í–ˆë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì€ ì¶œë ¥ì´ ë³´ì¼ ê²ƒì´ë‹¤. ë¹¨ê°„ì¤„ ì•„ë˜ì— ë‚˜ì™€ìˆëŠ” ì˜ˆì‹œ ëª…ë ¹ì–´ë¥¼ ë”°ë¼ì„œ ì‹¤í–‰ì‹œí‚¤ë©´ spider botì„ ë§Œë“¤ìˆ˜ ìˆë‹¤. ë˜í•œ, ëª¨ë“  ì•ìœ¼ë¡œì˜ ëª¨ë“  ëª…ë ¹ì–´ëŠ” scrapy.cfgë¼ëŠ” íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” directory pathì—ì„œ í•´ì•¼í•œë‹¤. genspider scrapyì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ spider classë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤. 123456789# spiderë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ scrapy.cfgíŒŒì¼ì˜ ê²½ë¡œë¡œ ì´ë™í•´ì•¼í•˜ê¸° ë•Œë¬¸ì—cd section01_2# scrapy.cfgíŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ ë‹¤ì‹œ í•œë²ˆ í™•ì¸ls# https://blog.scrapinghub.com/ì€ crawling í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì‚¬ì´íŠ¸ë¡œ ìœ ëª…í•˜ë‹¤.# https://blog.scrapinghub.com/ì´ë¼ëŠ” ì‚¬ì´íŠ¸ë¥¼ í¬ë¡¤ë§í•  testspiderë¼ëŠ” ì´ë¦„ìœ¼ë¡œ spider ì„ ë§Œë“¤ì–´ë¼ëŠ” ëª…ë ¹ì–´scrapy genspider testspider blog.scrapinghub.com ë‹¤ìŒê³¼ ê°™ì€ ì¶œë ¥ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìœ¼ë©°, section01_2ì— spidersë¼ëŠ” í´ë”ì˜ testspiderë¼ê³  ë§Œë“¤ì–´ì¡Œë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. 12345678# ë§Œë“¤ì–´ì§„ spider íŒŒì¼ í™•ì¼ì„ ìœ„í•´ ì´ë™cd section01_2/spiders# ìœ„ì—ì„œ ë§Œë“¤ì–´ ë†“ì•˜ë˜ testspiderë¼ëŠ” spiderê°€ ìˆëŠ”ì§€ í™•ì¸ls# testspider.py í™•ì¸vim testspider.py ë¨¼ì € straturlê³¼ ìš°ë¦¬ê°€ í¬ë¡¤ë§í•˜ë ¤ëŠ” URL endpointê°€ httpsì¸ì§€ í™•ì¸í•œ í›„ ê³ ì³ì¤€ë‹¤.(ì—¬ê¸°ì„œ í•„ìëŠ” vimìœ¼ë¡œ ìˆ˜ì •í•˜ì˜€ê¸°ì— pep8ì— ì˜ê±°í•˜ì—¬ space 4ë²ˆìœ¼ë¡œ indentë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. spaceì™€ tapì„ ë²ˆê°ˆì•„ê°€ë©° ì‚¬ìš©í•˜ë©´ python interpreterê°€ ë‹¤ë¥´ê²Œ ì¸ì‹í•˜ë¯€ë¡œ ì—ëŸ¬ë¥¼ ë°œìƒì‹œí‚¨ë‹¤!) ì•ìœ¼ë¡œì˜ ì‹¤ìŠµì— í—·ê°ˆë¦¼ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ nameì„ test1ìœ¼ë¡œ ë³€ë™í•´ì£¼ì—ˆê³ , allowed_domainsê³¼ start_urlsë¥¼ ë³´ë©´ ì„¤ì •í•´ ë†“ì€ ëŒ€ë¡œ ë“¤ì–´ê°€ ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ scrapyëŠ” allowed_domainsê³¼ start_urlsê°€ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°ë¡œ ë˜ì–´ìˆëŠ”ë° ë‹¤ë¥¸ URLê³¼ ë„ë©”ì¸ë“¤ì„ ì¶”ê°€í•˜ë©´ í•´ë‹¹ ì‚¬ì´íŠ¸ë“¤ì„ ëŒì•„ê°€ë©° í¬ë¡¤ë§ì„ í•  ìˆ˜ ìˆëŠ” ë³‘ë ¬ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì´ ê°€ì¥ í° ì¥ì ì´ë‹¤. ì¶”í›„ì— ì„¤ëª…í•˜ê² ì§€ë§Œ, ëˆˆì¹˜ ë¹ ë¥´ì‹  ë¶„ë“¤ì€ ì•„ë˜ parseí•¨ìˆ˜ì—ì„œ responseë¥¼ parameterë¡œ ë°›ëŠ” í•¨ìˆ˜ì´ë¯€ë¡œ ì´ í•¨ìˆ˜ì— í¬ë¡¤ë§í•˜ê³  ì‹¶ì€ ë¶€ë¶„ì— ëŒ€í•œ ì½”ë“œë¥¼ ë§Œë“¤ë©´ í¬ë¡¤ë§ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ê²ƒì´ë‹¤!! í˜¹ì‹œ responseì—ì„œ ì–´ë–¤ ëª…ë ¹ì–´ê°€ ì‚¬ìš©ê°€ëŠ¥í•œì§€ ë³´ê³  ì‹¶ë‹¤ë©´ runspider vs crawl runspiderì™€ crawlì˜ ì°¨ì´ì ì€ runspiderëŠ” spidersí´ë”ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆê³ , crawlì€ scrapy.cfgíŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” í´ë”ì—ì„œ ì‹¤í–‰í•˜ì—¬í– í•œë‹¤ëŠ” ì ì´ ì°¨ì´ì ì´ë‹¤!! runspider ëª…ë ¹ì–´ë¥¼ í†µí•´ spider botì„ ì‹¤í–‰ì‹œí‚¤ëŠ” ê²ƒì€ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë¼ê³  ì†Œìœ„ ë¶ˆë¦¬ëŠ” ë°©ì‹ì„ í•  ë•Œ ìœ ìš©í•˜ê³  crawlì€ ìš°ë¦¬ê°€ ì›í•˜ëŠ” êµ¬ì¡°ë¥¼ ë‹¤ ë§Œë“¤ì–´ ë†“ì€ í›„ í…ŒìŠ¤íŠ¸ë¥¼ í•  ë•Œë‚˜ ì‹¤ì œë¡œ í¬ë¡¤ë§ì„ í•  ê²½ìš° ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìœ ìš©í•˜ë‹¤. 12345# runspiderëŠ” spiders í´ë”ì—ì„œ ì‹¤í–‰í•˜ì—¬ì•¼í•œë‹¤.scrapy runspider testspider.py# crawlì€ scrapy.cfgíŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” pathì—ì„œ ì‹¤í–‰ì‹œì¼œì£¼ì–´ì•¼í•œë‹¤.scrapy crawl test1 --nolog settings.py spiderì˜ ì†ì„±ì— ê´€ë ¨ëœ parameterë“¤ì´ ìˆëŠ” íŒŒì¼ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ì•„ë˜ì˜ ê·¸ë¦¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ SPIDER MODULESëŠ” í˜„ì¬ SPIDERì˜ ìœ„ì¹˜ë¥¼ ì˜ë¯¸í•˜ê³ , NEWSPIDER MODULEì€ Spiderë¥¼ ìƒˆë¡œ ìƒì„±ì‹œ ì–´ëŠ ìœ„ì¹˜ì— ì¶”ê°€ë˜ëŠ”ì§€ë¥¼ ì˜ë¯¸í•œë‹¤. ROBOTSTXT_OBEYëŠ” robots.txtì˜ ê·œì¹™ì— ì˜ê±°í•˜ì—¬ crawlingì„ í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì´ë©°, DOWNLOAD_DELAYëŠ” ëª‡ì´ˆê°„ê²©ìœ¼ë¡œ ì„œë²„ì— ìš”ì²­ì„ í• ì§€ì— ëŒ€í•œ ìˆ˜ì¹˜ì´ë‹¤. í•„ìëŠ” 1ë¡œ ì •í–ˆëŠ”ë° ì—¬ê¸°ì„œëŠ” 1ì´ˆë§ˆë‹¤ë¼ëŠ” ì˜ë¯¸ì´ë‹¤. ë§Œì•½ì— 0.2ë¼ê³  í•˜ê²Œ ë˜ë©´ 0.2ì´ˆë§ˆë‹¤ ì„œë²„ì— ìš”ì²­í•˜ê²Œ ë˜ì–´ ì„œë²„ì— ë¶€í•˜ë¥¼ ì¼ìœ¼í‚¤ê²Œ ë˜ë©´ ì‹¬í• ê²½ìš° ì˜êµ¬ vanì„ ë‹¹í•  ìˆ˜ë„ ìˆê¸°ì— ê°„ê²©ì„ 1ì´ˆì´ìƒìœ¼ë¡œ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤. ì‹¤ìŠµ)blog.scrapinghub.comì—ì„œ ê¸°ì‚¬ ì œëª©ë“¤ë§Œ í¬ë¡¤ë§ í•˜ê¸°! ìœ„ì˜ ì‹¤ìŠµì£¼ì œë¡œ ì‹¤ìŠµì„ ì§„í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” ì•ì„œ ë§Œë“¤ì–´ë³¸ spider íŒŒì¼ì—ì„œ parseí•¨ìˆ˜ë¥¼ ìˆ˜ì •í•´ì•¼í•  ê²ƒì´ë‹¤. ê·¸ì— ì•ì„œ í¬ë¡¤ë§í•  blog.scrapinghub.comì˜ ì œëª©ì— í•´ë‹¹í•˜ëŠ” css pathë¥¼ ë³´ë©´ ì „ì²´ htmlì˜ body ë¶€ë¶„ì—ì„œ div element ì¤‘ classì˜ ì´ë¦„ì´ post-headerì¸ ë¶€ë¶„ì—ë§Œ ì¡´ì¬í•˜ëŠ” ê²ƒì„ ê°œë°œì ë„êµ¬ë¥¼ í†µí•´ ì•Œì•„ë‚´ì—ˆë‹¤. ë‹¤ë¥¸ ë¶€ë¶„ì— ë™ì¼í•œ elementë‚˜ classëª…ì„ ê°€ì§ˆ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ findë¥¼ í•´ë³´ì•„ì•¼í•œë‹¤! ë‹¤ìŒê³¼ ê°™ì´ ì œëª©ì„ í¬ë¡¤ë§í•˜ê¸° ìœ„í•´ testspider.pyì„ ìˆ˜ì •í•´ ì£¼ì—ˆë‹¤. ì°¸ê³ ë¡œ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í• ë•Œ scrapyê°€ ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹ì€ json, jsonlines, jl, csv, xml, marshal, pickleì´ë‹¤. ê²°ê³¼ë¥¼ ì €ì¥í• ë•Œ ë™ì¼í•œ íŒŒì¼ëª…ê³¼ í™•ì¥ìëª…ì„ ê°€ì§„ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•œë‹¤ë©´ ê·¸ íŒŒì¼ì— ë°ì´í„°ë¥¼ ì¶”ê°€í•´ì£¼ë¯€ë¡œ ì£¼ì˜í•˜ì! 12345# í•„ìëŠ” spider í´ë”ì—ì„œ ì‹¤í–‰í•¨.scrapy runspider testspider.py -o result.csv -t csv# result íŒŒì¼ì¸ result.csvê°€ ë§Œë“¤ì–´ì§„ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.ls Requestsë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ìˆœíšŒí•˜ë©° í¬ë¡¤ë§ ìš°ë¦¬ê°€ ì˜ˆë¥¼ ë“¤ì–´ ì–´ë–¤ í˜ì´ì§€ë‚´ì—ì„œ ì—¬ëŸ¬ í•­ëª©ì— ëŒ€í•´ í•´ë‹¹ urlë¡œ ì´ë™ í›„ í¬ë¡¤ë§í•˜ê³  ë‹¤ì‹œ ê·¸ ì „ í˜ì´ì§€ë¡œ ëŒì•„ê°€ì„œ ë‹¤ìŒ í•­ëª©ì˜ urlë¡œ ì´ë™ í›„ í¬ë¡¤ë§í•˜ëŠ” ì´ëŸ° ìˆœíšŒë¥¼ ê±°ì³ì•¼í•˜ëŠ” ì‘ì—…ì€ requestë‚˜ seleniumìœ¼ë¡œ í•˜ê²Œë˜ë©´ iterableí•œ ì½”ë“œë¥¼ í†µí•´ ê°€ëŠ¥í•˜ê²Œ ë˜ë©°, ê·¸ë ‡ì§€ ì•Šë‹¤ë©´, ë™ì¼í•œ êµ¬ì„±ì„ ì§€ë‹Œ í˜ì´ì§€ë“¤ì´ë¼ë©´ í•¨ìˆ˜ë¥¼ ì—¬ëŸ¬ë²ˆ ì‹¤í–‰í•˜ëŠ” ë“±ì˜ multiprocessingì„ í†µí•´ ë³‘ë ¬ì²˜ë¦¬ë¥¼ ë”°ë¡œ í•´ì£¼ì–´ì—¬í•˜ëŠ” ë¶ˆí¸í•¨ì´ ìˆë‹¤. í—ˆë‚˜, scrapyëŠ” ì½”ë“œ ëª‡ ì¤„ë¡œ ê°€ëŠ¥í•˜ë‹¤. ë¨¼ì € ì•ì˜ spiderë§ê³  ìƒˆë¡œìš´ spiderë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©í•  ê²ƒì´ë‹¤. spiderë¥¼ ë§Œë“  ì„¤ì •ì€ ìœ„ì—ì„œ ë§Œë“  ê²ƒê³¼ ë™ì¼í•˜ë‹¤. spider ì´ë¦„ë§Œ pagerutineì´ë¼ê³  ëª…ëª…í–ˆì„ ë¿ì´ë‹¤. ì´ì „ì— blog.scrapinghub.comì˜ í˜ì´ì§€ì— í•´ë‹¹ 10ê°œì˜ ê¸°ì‚¬ë“¤ì— ëŒ€í•œ pathê°€ â€œdiv.post-header h2 &gt; aâ€ ì´ë©°, a tagì˜ href ì†ì„± ê°’ë“¤ì„ í†µí•´ ê°ê°ì˜ ê¸°ì‚¬ì— í•´ë‹¹ í˜ì´ì§€ë¡œ ì´ë™ì´ ê°€ëŠ¥í•  ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ì²«ë²ˆì§¸ parse í•¨ìˆ˜ì—ì„œ ë¯¸ë¦¬ ì¶”ì¶œí•œ urlì„ requestí•˜ì—¬ ì–»ì€ responseë¥¼ ë‹¤ë¥¸ í•¨ìˆ˜ë¡œ ì „ë‹¬í•´ì£¼ê¸° ìœ„í•´ Request ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°, urljoinì„ ì‚¬ìš©í•œ ì´ìœ ëŠ” ì ˆëŒ€ì£¼ì†Œê°€ ì•„ë‹Œ ìƒëŒ€ì£¼ì†Œë¡œ ë˜ì–´ìˆëŠ” ê²½ìš° ìœ„\u001dì˜ start_urlsì— ì„¤ì •í•´ ë†“ì€ ì£¼ì†Œë¥¼ ì•ì— ë¶™ì—¬ ì ˆëŒ€ ì£¼ì†Œë¡œ ë°”ê¿”ì£¼ëŠ” ê¸°ëŠ¥ì´ë‹¤. ë¬¼ë¡  ì ˆëŒ€ì£¼ì†Œì¸ ê²½ìš°ëŠ” ì´ëŸ° ì‘ì—…ì„ ìƒëµí•œë‹¤. Scrapy Shell ì‚¬ìš©ë²• ì‰½ê²Œ ë§í•´ ì´ì „ì— í¬ë¡¤ë§ì„ í• ë•Œ Spider í´ë”ì—ì„œ íŒŒì¼ì„ ìˆ˜ì •í•˜ê³  í…ŒìŠ¤íŠ¸í•´ë³´ëŠ” ë°©ì‹ìœ¼ë¡œëŠ” ì‘ì—…ì˜ íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§€ë¯€ë¡œ ê·¸ ì „ì— cssë‚˜ xpath selectorë¥¼ í…ŒìŠ¤íŠ¸í•´ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì´ Shell modeì´ë‹¤. 123456789101112131415161718192021222324252627282930313233343536# shell ëª¨ë“œ ì ‘ì†scrapy shell################ shell ëª¨ë“œ ì ‘ì†í–ˆë‹¤ê³  ê°€ì • ################ url ì„¤ì •(requestí•˜ëŠ” ëŒ€ìƒì„ ë°”ê¾¸ëŠ” ì—­í• )fetch('url')quit################# ë‹¤ë¥¸ ë°©ë²• ############################# ìœ„ì˜ ë‹¨ê³„ë¥¼ í•œë²ˆì— í•˜ëŠ” ë°©ë²•scrapy shell https://blog.scrapinghub.com# response dataê°€ ë¬´ì—‡ì´ ìˆëŠ”ì§€ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ì†ŒìŠ¤í˜ì´ì§€ë¥¼ í˜„ì¬ ë‚´ ì»´í“¨ì €ì— ê°€ì ¸ì™€ì„œ ë³´ì—¬ì£¼ëŠ” ë°©ì‹view(response)# ì˜ˆì‹œfetch(https://daum.net)view(response)# responseê°€ ì‚¬ìš©í• ìˆ˜ìˆëŠ” method í™•ì¸dir(response)# í˜„ì¬ reponseì˜ url í™•ì¸response.url# í˜„ì¬ responseì˜ bodyì •ë³´ í™•ì¸response.body# í˜„ì¬ reponseì˜ status í™•ì¸response.status# robot.txt\u001dì— í¬ë¡¤ë§ì´ í—ˆìš©ë˜ì§€ ì•Šì•˜ìœ¼ë©´ shell scriptê°€ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤.# settings íŒŒì¼ì—ì„œì˜ ì„¤ì • íŒŒë¼ë¯¸í„°ë“¤ì„ ë™ì ìœ¼ë¡œ ì„¤ì •í•˜ë©° ì‹¤í–‰ ê°€ëŠ¥!scrapy shell https://daum.net --set=\"ROBOTSTXT_OBEY=False\"","categories":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/categories/crawling/"}],"tags":[]},{"title":"ëª¨í˜• ì„±ëŠ¥ í‰ê°€ ì§€í‘œ","slug":"machine_learning_03","date":"2020-01-09T06:12:42.000Z","updated":"2020-01-15T09:24:04.397Z","comments":true,"path":"2020/01/09/machine_learning_03/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/09/machine_learning_03/","excerpt":"","text":"íšŒê·€(regression) í‰ê°€ ì§€í‘œ íšŒê·€ì˜ í‰ê°€ë¥¼ ìœ„í•œ ì§€í‘œëŠ” ì‹¤ì œ ê°’ê³¼ íšŒê·€ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§€í‘œê°€ ì¤‘ì‹¬ì´ë‹¤. ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ë¥¼ ê·¸ëƒ¥ ë”í•˜ë©´ ì”ì°¨ì˜ í•©ì€ 0ì´ë¯€ë¡œ ì§€í‘œë¡œ ì“¸ ìˆ˜ ì—†ë‹¤. ì´ ë•Œë¬¸ì— ì”ì°¨ì˜ ì ˆëŒ€ê°’ í‰ê· ì´ë‚˜ ì œê³±, ë˜ëŠ” ì œê³±í•œ ë’¤ ë‹¤ì‹œ ë£¨íŠ¸ë¥¼ ì”Œìš´ í‰ê· ê°’ì„ ì„±ëŠ¥ ì§€í‘œë¡œ ì‚¬ìš©í•œë‹¤. í‰ê°€ ì§€í‘œ ìˆ˜ì‹ MAE(Mean Absolute Error) $MAE = \\frac{1}{n} \\sum_{i=1}^{n} \\lvert Y_{i} - \\hat{Y_{i}} \\rvert$ MSE(Mean Squared Error) $MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_{i} - \\hat{Y_{i}})$ RMSE(Root Mean Squared Error) $RMSE = $\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (Y_{i} - \\hat{Y_{i}})^{2}}$ $R^{2}$ $R^{2} = \\frac{ì˜ˆì¸¡ê°’ì˜ Variance}{ì‹¤ì œê°’ Variance} = \\frac{SSR}{SST}$ ì´ì „ì—ë„ ì–¸ê¸‰í–ˆë˜ ê²ƒì²˜ëŸ¼ ë³€ìˆ˜ê°€ ì¶”ê°€ëœë‹¤ë©´ ë‹¹ì—°íˆ SSRì˜ ìˆ˜ì¹˜ê°€ ë†’ì•„ì§€ê¸° ë•Œë¬¸ì— $R^{2}$ê°’ì€ ì˜¬ë¼ê°ˆ ìˆ˜ ë°–ì— ì—†ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ë³€ìˆ˜ ìˆ˜ì— ì˜í–¥ì„ ë°›ì§€ ì•Šê³  ì„œë¡œ ë¹„êµí•  ìˆ˜ ìˆê²Œë” ë§Œë“¤ì–´ ì¤€ ê²ƒì´ ìˆ˜ì •ëœ ê²°ì •ê³„ìˆ˜ì´ë‹¤. ë¶„ë¥˜(classification) ì„±ëŠ¥ ì§€í‘œ íŠ¹íˆ imbalanced dataì—ì„œ ëª¨í˜•ì˜ ì„±ëŠ¥ì„ ì •í™•ë„ í•˜ë‚˜ë§Œì„ ê°€ì§€ê³  ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤ë©´, ì˜ˆë¥¼ ë“¤ì–´, 100ê°œì¤‘ 90ê°œëŠ” ì„¸ëª¨ê³  10ê°œëŠ” ë„¤ëª¨ë¼ê³  í•  ë•Œ 100ê°œ ëª¨ë‘ ì„¸ëª¨ë¼ê³  ì˜ˆì¸¡í•´ë²„ë¦¬ê²Œ ë˜ë©´ ì •í™•ë„ëŠ” 90%ì´ë¯€ë¡œ ì¢‹ì€ ì„±ëŠ¥ ì§€í‘œë¼ê³  í•  ìˆ˜ ì—†ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ imbalanced dataì—ì„œì˜ ì„±ëŠ¥ ì§€í‘œëŠ” ì •í™•ë„(accuarcy) ë³´ë‹¤ëŠ” ì •ë°€ë„(precision), ì¬í˜„ìœ¨(Recall)ë¥¼ ë” ì„ í˜¸í•œë‹¤. ì •ë°€ë„(precision)ì™€ ì¬í˜„ìœ¨(recall) ì§€í‘œ ì¤‘ì— ë¶„ë¥˜ ëª¨ë¸ì˜ ì—…ë¬´ íŠ¹ì„±ì— ë”°ë¼ì„œ íŠ¹ì • í‰ê°€ ì§€í‘œê°€ ë” ì¤‘ìš”í•œ ì§€í‘œë¡œ ê°„ì£¼ ë  ìˆ˜ ìˆë‹¤. ì¬í˜„ìœ¨(recall)ì´ ì¤‘ìš” ì§€í‘œì¸ ê²½ìš°ëŠ” ì‹¤ì œ Positive ì–‘ì„± ë°ì´í„°ë¥¼ Negativeë¡œ ì˜ëª» íŒë‹¨í•˜ê²Œ ë˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ë°œìƒí•˜ëŠ” ê²½ìš°(FNì´ Criticalí•œ ê²½ìš°)ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•” íŒë‹¨ ëª¨í˜•ì€ ì¬í˜„ìœ¨(recall)ì´ í›¨ì”¬ ì¤‘ìš”í•œ ì§€í‘œì´ë‹¤. ì™œëƒí•˜ë©´ ì‹¤ì œ Positiveì¸ ì•” í™˜ìë¥¼ Positive ì–‘ì„±ì´ ì•„ë‹Œ Negative ìŒì„±ìœ¼ë¡œ ì˜ëª» íŒë‹¨í–ˆì„ ê²½ìš° ì˜¤ë¥˜ì˜ ëŒ€ê°€ê°€ ìƒëª…ì„ ì•—ì•„ê°ˆ ì •ë„ë¡œ ì‹¬ê°í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë°˜ë©´ì— ì‹¤ì œ Negativeì¸ ê±´ê°•í•œ í™˜ìë¥¼ ì•” í™˜ìì¸ Positiveë¡œ ì˜ˆì¸¡í•œ ê²½ìš°ë©´ ë‹¤ì‹œ í•œë²ˆ ì¬ê²€ì‚¬ë¥¼ í•˜ëŠ” ìˆ˜ì¤€ì˜ ë¹„ìš©ì´ ì†Œëª¨ë  ê²ƒì´ë‹¤. ë˜ ë‹¤ë¥¸ ì˜ˆë¡œëŠ”, ê¸ˆìœµ ì‚¬ê¸° ì ë°œ ëª¨ë¸ì„ ë“¤ ìˆ˜ ìˆë‹¤. ë¬¼ë¡  ê³ ê°ì—ê²Œ ê¸ˆìœµ ì‚¬ê¸° í˜ì˜ë¥¼ ì˜ëª» ì”Œìš°ë©´ ë¬¸ì œê°€ ë  ìˆ˜ ìˆê¸°ì— ì •ë°€ë„(Precision)ë„ ì¤‘ìš” í‰ê°€ ì§€í‘œì§€ë§Œ, ì—…ë¬´ì ì¸ íŠ¹ì„±ì„ ê³ ë ¤í•˜ë©´ ì¬í˜„ìœ¨(Recall)ì´ ìƒëŒ€ì ìœ¼ë¡œ ë” ì¤‘ìš”í•œ ì§€í‘œì…ë‹ˆë‹¤. ë³´í†µì€ ì¬í˜„ìœ¨(Recall)ì´ ì •ë°€ë„(Precision)ë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì—…ë¬´ê°€ ë§ì§€ë§Œ, ì •ë°€ë„ê°€ ë” ì¤‘ìš”í•œ ì§€í‘œì¸ ê²½ìš°ë„ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìŠ¤íŒ¸ë©”ì¼ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ëª¨í˜•ì˜ ê²½ìš° ì‹¤ì œ Positiveì¸ ìŠ¤íŒ¸ ë©”ì¼ì„ Negativeì¸ ì¼ë°˜ ë©”ì¼ë¡œ ë¶„ë¥˜í•˜ë”ë¼ë„ ì‚¬ìš©ìê°€ ë¶ˆí¸í•¨ì„ ëŠë¼ëŠ” ì •ë„ì´ì§€ë§Œ, ì‹¤ì œ Negativeì¸ ì¼ë°˜ ë©”ì¼ì„ Positiveì¸ ìŠ¤íŒ¸ë©”ì¼ë¡œ ë¶„ë¥˜í•  ê²½ìš°ì—ëŠ” ë©”ì¼ì„ ì•„ì˜ˆ ë°›ì§€ ëª»í•˜ê²Œ ë¼ ì—…ë¬´ì— ì°¨ì§ˆì´ ìƒê¸´ë‹¤. ì •ë°€ë„(Precision)ì´ ìƒëŒ€ì ìœ¼ë¡œ ë” ì¤‘ìš”í•œ ì§€í‘œì¸ ê²½ìš°ëŠ” ì‹¤ì œ Negative ìŒì„±ì¸ ë°ì´í„° ì˜ˆì¸¡ì„ Positive ì–‘ì„±ìœ¼ë¡œ ì˜ëª» íŒë‹¨í•˜ê²Œ ë˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ë°œìƒí•˜ëŠ” ê²½ìš°(FPê°€ Criticalí•œ ê²½ìš°)ì´ë‹¤. ì¬í˜„ìœ¨(Recall)ê³¼ ì •ë°€ë„(Precision) ëª¨ë‘ TPë¥¼ ë†’ì´ëŠ” ë° ë™ì¼í•˜ê²Œ ì´ˆì ì„ ë§ì¶”ì§€ë§Œ, ì¬í˜„ìœ¨(Recall)ì€ FNë¥¼ ë‚®ì¶”ëŠ”ë°, ì •ë°€ë„(Precision)ëŠ” FPë¥¼ ë‚®ì¶”ëŠ”ë° ì´ˆì ì„ ë§ì¶˜ë‹¤. ì´ ê°™ì€ íŠ¹ì„± ë•Œë¬¸ì— ì¬í˜„ìœ¨(Recall)ê³¼ ì •ë°€ë„(Precision)ì€ ì„œë¡œ ë³´ì™„ì ì¸ ì§€í‘œë¡œ ë¶„ë¥˜ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ”ë° ì ìš©ëœë‹¤. ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ í‰ê°€ëŠ” ì¬í˜„ìœ¨(Recall)ê³¼ ì •ë°€ë„(Precision) ëª¨ë‘ ë†’ì€ ìˆ˜ì¹˜ë¥¼ ì–»ëŠ” ê²ƒì´ë‹¤. ë°˜ë©´ì— ë‘˜ ì¤‘ ì–´ëŠ í•œ í‰ê°€ ì§€í‘œë§Œ ë§¤ìš° ë†’ê³ , ë‹¤ë¥¸ ìˆ˜ì¹˜ëŠ” ë§¤ìš° ë‚®ì€ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²½ìš°ì—ëŠ” ë°”ëŒì§í•˜ì§€ ì•Šë‹¤. ì •ë°€ë„(Precision)/ ì¬í˜„ìœ¨(Recall) Trade-off ë¶„ë¥˜í•˜ë ¤ëŠ” ì—…ë¬´ì˜ íŠ¹ì„±ìƒ ì •ë°€ë„(Precision) ë˜ëŠ” ì¬í˜„ìœ¨(Recall)ì´ íŠ¹ë³„íˆ ê°•ì¡°ë¼ì•¼ í•  ê²½ìš° ë¶„ë¥˜ì˜ ê²°ì • ì„ê³„ê°’(Threshold)ì„ ì¡°ì •í•´ ì •ë°€ë„(Precision) ë˜ëŠ” ì¬í˜„ìœ¨(Recall)ì˜ ìˆ˜ì¹˜ë¥¼ ë†’ì¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall)ì€ ìƒí˜¸ ë³´ì™„ì ì¸ í‰ê°€ ì§€í‘œì´ê¸° ë•Œë¬¸ì— ì–´ëŠ í•œìª½ì„ ê°•ì œë¡œ ë†’ì´ë©´ ë‹¤ë¥¸ í•˜ë‚˜ì˜ ìˆ˜ì¹˜ëŠ” ë–¨ì–´ì§€ê¸° ì‰½ë‹¤. ì´ë¥¼ ì •ë°€ë„(Precision)/ì¬í˜„ìœ¨(Recall)ì˜ Trade-offë¼ê³  ë¶€ë¥¸ë‹¤. scikit-learnì—ì„œ ê°ê°ì˜ ë¶„ë¥˜ëª¨ë¸ë“¤ì€ predict_probaì˜ ê²°ê³¼ë¥¼ Threshold(ë³´í†µì€ 0.5)ë³´ë‹¤ ê°™ê±°ë‚˜ ì‘ìœ¼ë©´ 0ê°’ìœ¼ë¡œ, í¬ë©´ 1ê°’ìœ¼ë¡œ ë³€í™˜í•´ ë°˜í™˜í•˜ëŠ” Binarizer í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ predictì˜ ê²°ê³¼ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•´ ì¤€ë‹¤. ë§Œì•½ ì„ê³„ê°’ì„ ë‚®ì¶”ë©´ ì¬í˜„ìœ¨(Recall)ê°’ì´ ì˜¬ë¼ê°€ê³  ì •ë°€ë„(Precision)ê°€ ë–¨ì–´ì§ˆ ê²ƒì´ë‹¤. ê·¸ ì´ìœ ëŠ” ì„ê³„ê°’ì€ Positive ì˜ˆì¸¡ê°’ì„ ê²°ì •í•˜ëŠ” í™•ë¥ ì˜ ê¸°ì¤€ì´ ë˜ëŠ”ë° ì„ê³„ê°’ì„ 0.5ì—ì„œ 0.4ë¡œ ë‚®ì¶”ë©´ ê·¸ë§Œí¼ Positive ì˜ˆì¸¡ì„ ë” ë„ˆê·¸ëŸ½ê²Œ í•˜ê¸° ë–„ë¬¸ì— Trueë¡œ ì˜ˆì¸¡í•˜ëŠ” ê°’ì´ ë§ì•„ì§€ê²Œ ëœë‹¤. Positive ì˜ˆì¸¡ì„ ë§ì´ í•˜ë‹¤ë³´ë‹ˆ ì‹¤ì œ ì–‘ì„±ì„ ìŒì„±ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” íšŸìˆ˜ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì´ë‹¤. ì •ë°€ë„(Precision)ê³¼ ì¬í˜„ìœ¨(Recall)ì˜ ë§¹ì  Positive ì˜ˆì¸¡ì˜ ì„ê³„ê°’ì„ ë³€ê²½í•¨ì— ë”°ë¼ ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall)ì˜ ìˆ˜ì¹˜ê°€ ë³€ê²½ëœë‹¤. ì„ê³„ê°’ì˜ ì´ëŸ¬í•œ ë³€ê²½ì€ ì—…ë¬´ í™˜ê²½ì— ë§ê²Œ ë‘ ê°œì˜ ìˆ˜ì¹˜ë¥¼ ìƒí˜¸ ë³´ì™„í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì—ì„œ ì ìš©ë¼ì•¼ í•œë‹¤. ê·¸ë ‡ì§€ ì•Šê³  ë‹¨ í•˜ë‚˜ì˜ ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì¹˜ë¥¼ ë†’ì´ê¸° ìœ„í•œ ìˆ˜ë‹¨ìœ¼ë¡œ ì‚¬ìš©ë¼ì„œëŠ” ì•ˆëœë‹¤. ê°ê°ì˜ ì§€í‘œë¥¼ ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ì¼ ìˆ˜ëŠ” ìˆê³ , ì •ë°€ë„(Precision) ë˜ëŠ” ì¬í˜„ìœ¨(Recall) ì¤‘ í•˜ë‚˜ì— ìƒëŒ€ì ì¸ ì¤‘ìš”ë„ë¥¼ ë¶€ì—¬í•´ ê° ì˜ˆì¸¡ ìƒí™©ì— ë§ëŠ” ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì„ íŠœë‹í•  ìˆ˜ ìˆì§€ë§Œ, ê·¸ë ‡ë‹¤ê³  ì •ë°€ë„(Precision)/ì¬í˜„ìœ¨(Recall) ì¤‘ í•˜ë‚˜ì— ìƒëŒ€ì ì¸ ì¤‘ìš”ë„ë¥¼ ë¶€ì—¬í•´ ê° ì˜ˆì¸¡ ìƒí™©ì— ë§ëŠ” ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì„ íŠœë‹í•  ìˆ˜ ìˆì§€ë§Œ, ê·¸ë ‡ë‹¤ê³  ì •ë°€ë„(Precision)/ì¬í˜„ìœ¨(Recall) í•˜ë‚˜ë§Œ ê°•ì¡°í•˜ëŠ” ìƒí™©ì´ ë¼ì„œëŠ” ì•ˆëœë‹¤. F1-scoreëŠ” ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall)ì„ ê²°í•©í•œ ì§€í‘œë¡œ ì–´ëŠ í•œìª½ìœ¼ë¡œ ì¹˜ìš°ì¹˜ì§€ ì•ŠëŠ” ìˆ˜ì¹˜ë¥¼ ë‚˜íƒ€ë‚¼ ë•Œ ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ ê°’ì„ ê°€ì§„ë‹¤. ì—¬ê¸°ì„œ ë˜ í•œê°€ì§€ ì£¼ì˜í•  ì ì€ ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall)ì˜ ì¡°í™”í‰ê· ê°’ì´ë¼ í•´ì„œ ë¬´ì¡°ê±´ F1-scoreê°€ ë†’ì€ ê²ƒì´ ì¢‹ì€ ëª¨í˜•ì€ ì•„ë‹ˆë¼ëŠ” ì ì´ë‹¤. ì •ë°€ë„(Precision)ê³¼ ì¬í˜„ìœ¨(Recall) ê·¸ë¦¬ê³  F1-score ëª¨ë‘ êµ¬í•œ í›„ ë¹„êµí•˜ì—¬ ì í•©í•œ ëª¨í˜•ì„ ì„ ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤! F1 = \\frac{2}{\\frac{1}{Recall} + \\frac{1}{Precision}} ROC Curveì™€ ì´ì— ê¸°ë°˜í•œ AUC scoreëŠ” ì´ì§„ ë¶„ë¥˜ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ ì¸¡ì •ì—ì„œ ì¤‘ìš”í•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ì§€í‘œì´ë‹¤. ROC Curve(Receiver Operation Characteristic Curve)ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì˜í•™ë¶„ì•¼ì—ì„œ ë§ì´ ì‚¬ìš©ë˜ì§€ë§Œ, ë¨¸ì‹  ëŸ¬ë‹ì˜ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ íŒë‹¨í•˜ëŠ” ì¤‘ìš”í•œ ì§€í‘œì´ë‹¤. ROC CurveëŠ” FRR(False Positive Rate)ì´ ë³€í•  ë•Œ TPR(True Positive Rate)ì´ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê³¡ì„ ì´ë‹¤. FPRì„ Xì¶•ìœ¼ë¡œ í•˜ê³  FPRì„ 0ë¶€í„° 1ê¹Œì§€ ë³€ê²½í•˜ë©´ì„œ, TPRì„ Yì¶•ìœ¼ë¡œ ì¡ì•„ FPRì— ë³€í™”ì— ë”°ë¥¸ TPRì˜ ë³€í™”ê°€ ê³¡ì„  í˜•íƒœë¡œ ë‚˜íƒ€ë‚œë‹¤. ë¶„ë¥˜ê²°ì • ì„ê³„ê°’ì€ Positive ì—ì¸¡ê°’ì„ ê²°ì •í•˜ëŠ” ê°’ì´ë¯€ë¡œ FPRì„ 0ìœ¼ë¡œ ë§Œë“¤ë ¤ë©´ 1ë¡œ ì§€ì •í•˜ë©´ ëœë‹¤. TPRì€ ì¬í˜„ìœ¨(Recall)ê³¼ ë™ì¼í•˜ë©°, ë¯¼ê°ë„ë¼ê³ ë„ ë¶ˆë¦°ë‹¤. ê°€ìš´ë° ì§ì„ ì€ ROC Curveì˜ ìµœì €ê°’(AUCëŠ” 0.5)ì´ë‹¤. ROC ê³¡ì„ ì´ ê°€ìš´ë° ì§ì„ ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì´ë©°, ë©€ì–´ì§ˆìˆ˜ë¡ ì„±ëŠ¥ì´ ë›°ì–´ë‚˜ë‹¤ëŠ” ê²ƒì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ROC Curve ìì²´ëŠ” FPRê³¼ TPRì˜ ë³€í™”ê°’ì„ ë³´ëŠ” ë° ì´ìš©í•˜ë©° ë¶„ë¥˜ì˜ ì„±ëŠ¥ ì§€í‘œë¡œ ì‚¬ìš©ë˜ëŠ” ê²ƒì€ ROC Curve ë©´ì ì— ê¸°ë°˜í•œ AUC ê°’ìœ¼ë¡œ ê²°ì •í•œë‹¤. AUC(Area Under Curve)ê°’ì€ ROC Curve ë°‘ì˜ ë©´ì ì„ êµ¬í•œ ê²ƒìœ¼ë¡œì„œ ì¼ë°˜ì ìœ¼ë¡œ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ì€ ìˆ˜ì¹˜ì´ë‹¤. AUC ìˆ˜ì¹˜ê°€ ì»¤ì§€ë ¤ë©´ FPRì´ ì‘ìŒ ìƒíƒœì—ì„œ ì–¼ë§ˆë‚˜ í° TPRì„ ì–»ì„ ìˆ˜ ìˆëŠëƒê°€ ê´€ê±´ì´ë‹¤. ê°€ìš´ë° ì§ì„ ì„ ëœë¤ ìˆ˜ì¤€ì˜(ë™ì „ ë˜ì§€ê¸° ìˆ˜ì¤€) ì´ì§„ ë¶„ë¥˜ AUC ê°’ìœ¼ë¡œ 0.5ì´ë‹¤. ë”°ë¼ì„œ ë³´í†µì˜ ë¶„ë¥˜ëŠ” 0.5ì´ìƒì˜ AUCê°’ì„ ê°€ì§€ë‚Ÿ. TPR(ë¯¼ê°ë„) = \\frac{TP}{TP+FN}TNR(íŠ¹ì´ì„±) = \\frac{TN}{FP+TN})FPR = 1 - TNR = \\frac{FP}{FP+TN}","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"Regression(02) - ë‹¤ì¤‘ì„ í˜•íšŒê·€ ë° ë‹¤ì¤‘ê³µì„ ì„±","slug":"machine_learning_02","date":"2020-01-08T14:22:36.000Z","updated":"2020-03-25T14:31:00.947Z","comments":true,"path":"2020/01/08/machine_learning_02/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/08/machine_learning_02/","excerpt":"","text":"ë‹¤ì¤‘ ì„ í˜• íšŒê·€ ë‹¤ì¤‘íšŒê·€ë°©ì •ì‹ì—ì„œ íšŒê·€ê³„ìˆ˜ì— ëŒ€í•œ í•´ì„ì€ ìì£¼ í˜¼ë™ë˜ëŠ” ê²ƒ ì¤‘ í•˜ë‚˜ì´ë‹¤. ë‹¨ìˆœíšŒê·€ë°©ì •ì‹ì€ ì§ì„ ì„ í‘œí˜„í•˜ì§€ë§Œ ë‹¤ì¤‘íšŒê·€ë°©ì •ì‹ì€ í‰ë©´(ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ê°€ ë‘ê°œì¸ ê²½ìš°) í˜¹ì€ ì´ˆí‰ë©´(ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ê°€ ë‘ê°œë³´ë‹¤ ë§ì€ ê²½ìš°)ì„ í‘œí˜„í•œë‹¤. ìœ„ì˜ ì˜ˆì—ì„œ íšŒê·€ê³„ìˆ˜ì˜ í•´ì„ì€ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì´ ê³ ì •ë˜ì–´ ìˆì„ë•Œ TVê°€ 1ë‹¨ìœ„ ì¦ê°€í•  ë•Œ ë§¤ì¶œì•¡ì€ 0.046ë‹¨ìœ„ ì¦ê°€í•œë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆë‹¤. íšŒê·€ ê³„ìˆ˜ $\\beta_{j}$ëŠ” $X_{j}$ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ëª¨ë“  ì˜ˆì¸¡ ë³€ìˆ˜ë“¤ì„ ìƒìˆ˜ë¡œ ê³ ì •ì‹œí‚¨ ìƒíƒœì—ì„œ $X_{j}$ì˜ í•œ ë‹¨ìœ„ ì¦ê°€ì— ë”°ë¥¸ Yì˜ ì¦ë¶„ìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆë‹¤. ë³€í™”ì˜ í¬ê¸°ëŠ” ë‹¤ë¥¸ ì˜ˆì¸¡ ë³€ìˆ˜ë“¤ì´ ì–´ë–¤ ê°’ìœ¼ë¡œ ê³ ì •ë˜ì–´ ìˆëŠ”ì§€ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ”ë‹¤. ë˜ ë‹¤ë¥¸ í•´ì„ì€ $\\beta_{j}$ê°€ ë‹¤ë¥¸ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ì— ì˜í•˜ì—¬ ì¢…ì†(ë°˜ì‘)ë³€ìˆ˜ Yê°€ ì¡°ì •ëœ í›„ì— Yì— ëŒ€í•œ $X_{j}$ì˜ ê³µí—Œë„ë¥¼ ì˜ë¯¸í•œë‹¤. ì´ëŠ” ì˜ˆë¥¼ ë“¤ì–´ $Y = \\beta{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\epsilon $ì¼ ë•Œ, $[Y~X_{1}ë¡œ ì–»ì€ ì”ì°¨] ~ [X_{2}~X_{1}]$ì—ì„œì˜ ê³„ìˆ˜ì™€ ë™ì¼í•˜ë‹¤. Yì™€ $X_{2}$ ê°ê°ìœ¼ë¡œë¶€í„° $X_{1}$ì˜ ì„ í˜•íš¨ê³¼ë¥¼ ì œê±°í•œ í›„ Yì— ë¯¸ì¹˜ëŠ” $X_{2}$ì˜ íš¨ê³¼ë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì´ë‹¤.ë‹¤ì¤‘ ì„ í˜• íšŒê·€ ê³„ìˆ˜ ê²€ì • ë‹¨ìˆœ ì„ í˜• íšŒê·€ì™€ ë™ì¼í•˜ê²Œ ê°ê°ì˜ íšŒê·€ê³„ìˆ˜ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œì§€ë¥¼ ê²€ì •í•˜ëŠ” ê²ƒì€ ë™ì¼í•˜ë‹¤. í•˜ì§€ë§Œ, ë‹¤ì¤‘ ì„ í˜• íšŒê·€ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì „ì²´ íšŒê·€ê³„ìˆ˜ê°€ ì˜ë¯¸ìˆëŠ”ì§€ì— ëŒ€í•œ ê²€ì •ë„ í•˜ê²Œ ëœë‹¤. ì—¬ê¸°ì„œ íšŒê·€ê³„ìˆ˜ê°€ 0ì— ê°€ê¹ê³  standard errorë¥¼ ë”í•˜ê³  ëº€ ë²”ìœ„ë‚´ì— 0ì´ í¬í•¨ëœë‹¤ë©´ ê·¸ ë³€ìˆ˜ ë˜í•œ ìœ ì˜ë¯¸í•˜ë”ë¼ë„ ì œê±°í•´ì•¼í•  ê²ƒì´ë‹¤. ë˜í•œ ì—¬ê¸°ì„œ ì”ì°¨ì˜ ì •ê·œì„±ì´ë¼ëŠ” ê°€ì •ì„ ë§Œì¡±í•  ë•Œ t í†µê³„ëŸ‰ì˜ ì ˆëŒ€ê°’ì´ í¬ê±°ë‚˜ ëŒ€ì‘ë˜ëŠ” p-valueê°€ ë” ì‘ë‹¤ë©´ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ì™€ ì¢…ì†(ë°˜ì‘)ë³€ìˆ˜ ì‚¬ì´ì˜ ì„ í˜•ê´€ê³„ê°€ ë” ê°•í•¨ì„ ì˜ë¯¸í•œë‹¤. ìœ„ì—ì„œ ì–¸ê¸‰í•˜ê³  ìˆëŠ” ê°œë³„ì ì¸ íšŒê·€ê³„ìˆ˜ $\\beta$ì— ëŒ€í•œ ê²€ì • ì´ì™¸ì—, ì—¬ëŸ¬ ê°€ì§€ ë‹¤ë¥¸ í˜•íƒœì˜ ê°€ì„¤ë“¤ì´ ì„ í˜•ëª¨í˜•ì˜ ë¶„ì„ê³¼ ê´€ë ¨í•˜ì—¬ ê³ ë ¤ë  ìˆ˜ ìˆë‹¤. í†µìƒì ìœ¼ë¡œ ê³ ë ¤ë  ìˆ˜ ìˆëŠ” ê°€ì„¤ë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 1) ë…ë¦½ ë³€ìˆ˜ì˜ ëª¨ë“  íšŒê·€ ê³„ìˆ˜ë“¤ì´ 0ì´ë‹¤. 2) ë…ë¦½ ë³€ìˆ˜ì˜ íšŒê·€ ê³„ìˆ˜ë“¤ ì¤‘ ì¼ë¶€ë¶„ì´ 0ì´ë‹¤. 3) íšŒê·€ê³„ìˆ˜ë“¤ ì¤‘ ì¼ë¶€ë¶„ì´ ì„œë¡œ ê°™ì€ ê°’ì„ ê°€ì§„ë‹¤. 4) íšŒê·€ëª¨ìˆ˜ë“¤ì´ íŠ¹ì •í•œ ì œì•½ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤. ì´ëŸ° ê°€ì„¤ë“¤ì€ í•˜ë‚˜ì˜ í†µí•©ëœ ì ‘ê·¼ë°©ë²•ì„ í†µí•´ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ê²€ì •ë  ìˆ˜ ìˆë‹¤. ë¨¼ì €, ëª¨ë“  ë…ë¦½ë³€ìˆ˜ë¥¼ í¬í•¨í•œ ëª¨í˜•ì„ ì™„ì „ëª¨í˜•(FM)ì´ë¼ê³  í•˜ì. ê·¸ë¦¬ê³  ê·€ë¬´ê°€ì„¤ì— ê°€ì •ëœ ë‚´ìš©ë“¤ì„ ì™„ì „ëª¨í˜•ì— ëŒ€ì…í•´ì„œ ì–»ì€ ëª¨í˜•ì„ ì¶•ì†Œëª¨í˜•(RM)ì´ë¼ê³  í•˜ì. ì™„ì „ëª¨í˜•ì˜ ë³€ìˆ˜ë“¤ì´ ìƒëŒ€ì ìœ¼ë¡œ ì¶•ì†Œëª¨í˜•ì— ë¹„í•´ ë§ìœ¼ë¯€ë¡œ SSRê°’ì´ ì»¤ì ¸ ì”ì°¨ì œê³±í•©(SSE)ì„ ê°ì†Œì‹œí‚¬ ê²ƒì´ë¯€ë¡œ $SSE(RM) \\geq SSE(FM)$ì´ ëœë‹¤. ë”°ë¼ì„œ ì°¨ì´ $SSE(RM) - SSE(FM)$ì€ ì¶•ì†Œëª¨í˜•ì„ ì í•©í•¨ìœ¼ë¡œì¨ ì¦ê°€í•˜ëŠ” ì”ì°¨ì œê³±í•©(SSE)ì„ ì˜ë¯¸í•œë‹¤. ë§Œì•½ ì´ ì°¨ì´ê°€ í¬ë‹¤ë©´ ì¶•ì†Œëª¨í˜•ì€ ì ì ˆí•˜ì§€ ì•Šë‹¤. F = \\frac{[SSE(RM) - SSE(FM) ]/(p + 1 - k)}{SSE(FM)/(n-p-1)} ìœ„ì˜ ì‹ì„ í†µí•´ ë‹¤ ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ ê°€ì„¤ë“¤ì„ ëª¨ë‘ ê²€ì •í•  ìˆ˜ ìˆë‹¤. ê°€ì„¤ 1)ë…ë¦½ ë³€ìˆ˜ì˜ ëª¨ë“  íšŒê·€ ê³„ìˆ˜ë“¤ì´ 0ì´ë‹¤.ì˜ ê·€ë¬´ê°€ì„¤ì€ $H_{0}:\\beta_{1} = \\beta_{2} = \\cdots = \\beta_{p} = 0$ì´ë©° ëŒ€ë¦½ê°€ì„¤ì€ $H_{1}: ìµœì†Œí•œ í•˜ë‚˜ì˜ ê³„ìˆ˜ëŠ” 0ì´ ì•„ë‹ˆë‹¤. $ì´ë‹¤. ì´ ê°€ì„¤ì€ ì¶•ì†Œëª¨í˜•ì˜ ë³€ìˆ˜ëŠ” 1ê°œì´ë¯€ë¡œ í•´ë‹¹ ëª¨í˜•ì„ fittingí•œ í›„ì— ë‚˜ì˜¤ëŠ” ë¶„ì‚°ë¶„ì„í‘œì—ì„œì˜ F í†µê³„ëŸ‰ ê°’ì„ ë³´ê³  ê²€ì • í•  ìˆ˜ ìˆë‹¤. ê°€ì„¤ 2)ë…ë¦½ ë³€ìˆ˜ì˜ íšŒê·€ ê³„ìˆ˜ë“¤ ì¤‘ ì¼ë¶€ë¶„ì´ 0ì´ë‹¤.ì˜ ê·€ë¬´ê°€ì„¤ì€ $H_{0}:\\beta_{1} = \\beta_{3} = \\beta_{5} = 0 $ ì´ê³ , ëŒ€ë¦½ê°€ì„¤ì€ $H_{1}:\\beta_{1}, \\beta_{3}, \\beta_{5} ì¤‘ ìµœì†Œí•œ í•˜ë‚˜ëŠ” 0ì´ ì•„ë‹ˆë‹¤. $ ì´ë‹¤. ì´ ê°€ì„¤ì€ ìœ„ì—ì„œ F í†µê³„ëŸ‰ì„ êµ¬í•˜ëŠ” ë°©ì‹ì— ë³€í˜•ì„ ì£¼ì–´ ìƒê°í•´ë³´ë©´ $R^{2}$ê°’ì„ í†µí•´ êµ¬í•  ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. F = \\frac{({R_{p}}^{2}-{R_{q}}^{2})/(p-q)}{(1 - {R_{p}}^{2})/(n-p-q)}, df=(p-q,n-p-1) ì—¬ê¸°ì„œ ì£¼ëª©í•  ì ì€ SSTëŠ” ì •í•´ì ¸ ìˆì–´ ê³ ì •ë˜ì–´ ìˆëŠ”ë°, SSRì€ ë³€ìˆ˜ë¥¼ ì¶”ê°€í• ìˆ˜ë¡ ì ì  ë” ì»¤ì§€ë¯€ë¡œ ê·€ë¬´ê°€ì„¤ì„ ê¸°ê°í•˜ê¸° ë” ì‰¬ì›Œì§„ë‹¤ëŠ” ê²ƒì´ë©°, ìš°ë¦¬ê°€ ì¶”í›„ì— ë§í• $R^{2}$ê°’ë„ ë³€ìˆ˜ë¥¼ ì¶”ê°€í• ìˆ˜ë¡ ë†’ì•„ì§€ë¯€ë¡œ ì´ê°’ìœ¼ë¡œ ëª¨í˜•ì˜ ì„±ëŠ¥ì„ í‰ê°€í• ë•Œ ë¬´ì¡°ê±´ ì´ê°’ì´ ë†’ë‹¤ê³  ì¢‹ì€ ëª¨í˜•ì´ë¼ê³  ìƒê°í•˜ì§€ ì•Šì•„ì•¼ í•œë‹¤. ë˜í•œ ìˆ˜ì •ê²°ì •ê³„ìˆ˜(adjusted R-squared) ${R_{adj}}^{2}$ë„ ì í•©ë„ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. ${R_{adj}}^{2}$ì€ ëª¨í˜•ì•ˆì— ìˆëŠ” ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ì˜ ìˆ˜ê°€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ ì¡°ì •í•˜ë¯€ë¡œ Fê°’ê³¼ ê°™ì´ ì„œë¡œ ë‹¤ë¥¸ ëª¨í˜•ë“¤(í¬í•¨ëœ ë…ë¦½ë³€ìˆ˜ê°€ ë‹¤ë¥´ê±°ë‚˜ ê°¯ìˆ˜ê°€ ë‹¤ë¥¸)ì„ ë¹„êµí•˜ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤. ì´ ê°’ì€ ê²°ì •ê³„ìˆ˜ ê°’ê³¼ ë‹¤ë¥´ê²Œ Yì˜ ì „ì²´ ë³€ì´ ì¤‘ì—ì„œ ë…ë¦½ë³€ìˆ˜ë“¤ì— ì˜í•˜ì—¬ ì„¤ëª…ë˜ëŠ” ë¹„ìœ¨ë¡œ í•´ì„ ë  ìˆ˜ ì—†ë‹¤! {R_{adj}}^{2} = 1 - \\frac{SSE/(n-p-1)}{SST/(n-1)}ì›ì ì„ í†µê³¼í•˜ëŠ” íšŒê·€ì„  ì¼ë°˜ì ìœ¼ë¡œ ê³ ë ¤ë˜ëŠ” ë‹¨ìˆœì„ í˜•íšŒê·€ëª¨í˜•ì€ $ Y = \\beta_{0} + \\beta_{1}X + \\epsilon $ê³¼ ê°™ì´ ì ˆí¸í•­ì„ ê°€ì§€ê³  ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ì›ì ì„ í†µê³¼í•˜ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ëª¨í˜• $ Y = \\beta_{1}X + \\epsilon $ ì— ë°ì´í„°ë¥¼ ì í•©ì‹œí‚¬ í•„ìš”ê°€ ìˆì„ ë•Œë„ ìˆë‹¤. ì´ ëª¨í˜•ì€ ì ˆí¸í•­ì´ ì—†ëŠ” ëª¨í˜•ìœ¼ë¡œ ë¶ˆë¦°ë‹¤. ë¬¸ì œì˜ ì„±ê²©ì´ë‚˜ ì™¸ì  ìƒí™©ì— ì˜í•´ íšŒê·€ì„ ì´ ì›ì ì„ ì§€ë‚˜ì•¼ë§Œí•˜ëŠ” ê²½ìš°ê°€ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‹œê°„(X)ì˜ í•¨ìˆ˜ë¡œì„œ ì—¬í–‰ ê±°ë¦¬(Y)ëŠ” ìƒìˆ˜í•­ì„ ê°€ì§€ì§€ ì•Šì•„ì•¼ í•œë‹¤. ì´ë•ŒëŠ” SSEì˜ ììœ ë„ê°€ í™•ë¥  ë³€ìˆ˜ì¸ ì ˆí¸í•­ì´ í•˜ë‚˜ ë¹ ì§€ë¯€ë¡œ N-p(ì „ì²´ í™•ë¥ ë³€ìˆ˜ê°€ ì„¤ëª…ë³€ìˆ˜pê°œ ì ˆí¸í•­ 1ê°œ ì´ì—ˆë˜ N-p-1ì—ì„œ)ë¡œ ë°”ë€Œê²Œ ëœë‹¤. ë˜í•œ ì´ë•ŒëŠ” ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” SST=SSR+SSEë¼ëŠ” ê³µì‹ì´ ë” ì´ìƒ ì„±ë¦½ë˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ $R^{2}$ì™€ ê°™ì€ ì ˆí¸í•­ì„ ê°–ëŠ” ëª¨í˜•ì— ëŒ€í•œ ëª‡ëª‡ ì„±ëŠ¥ í‰ê°€ ì§€í‘œë“¤ì€ ì ˆí¸í•­ì´ ì—†ëŠ” ëª¨í˜•ì— ëŒ€í•´ì„œëŠ” ë” ì´ìƒ ì ì ˆí•˜ì§€ ì•Šë‹¤. ì ˆí¸í•­ì´ ì—†ëŠ” ëª¨í˜•ì— ëŒ€í•œ ì ì ˆí•œ í•­ë“±ì‹ì€ yì˜ í‰ê· ì„ 0ìœ¼ë¡œ ëŒ€ì²´í•¨ìœ¼ë¡œì¨ ì–»ì–´ì§„ë‹¤. \\sum_{i = 1}^{n} y_{i}^{2} = \\sum_{i = 1}^{n} \\hat{y_{i}^{2}} + \\sum_{i = 1}^{n} e_{i}^{2} ê·¸ëŸ¬ë¯€ë¡œ $R^{2}$ ë˜í•œ ì¬ì •ì˜ ëœë‹¤. R^{2} = \\frac{\\sum \\hat{y_{i}^{2}}}{\\sum {y_{i}^{2}}} = 1 - \\frac{\\sum e_{i}^{2}}{\\sum y_{i}^{2}} ì ˆí¸í•­ì„ ê°€ì§„ ëª¨í˜•ì˜ ê²½ìš° $R^{2}$ê°€ Yë¥¼ ê·¸ì˜ í‰ê· ìœ¼ë¡œ ì¡°ì •í•œ í›„ì— Yì˜ ì „ì²´ ë³€ë™ì„± ì¤‘ì—ì„œ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ Xì— ì˜í•˜ì—¬ ì„¤ëª…ë˜ëŠ” ë¹„ìœ¨ë¡œ í•´ì„ë  ìˆ˜ ìˆë‹¤. ì ˆí¸í•­ì´ ì—†ëŠ” ëª¨í˜•ì˜ ê²½ìš°ì—ëŠ” Yì— ëŒ€í•œ ì¡°ì •ì´ ì—†ë‹¤. ì´ ì²˜ëŸ¼ ì ˆí¸í•­ì´ ì—†ëŠ” ëª¨í˜•ì€ í’€ê³ ìí•˜ëŠ” ë¬¸ì œì™€ ê´€ë ¨ëœ ì´ë¡  í˜¹ì€ ë¬¼ë¦¬ì  ìƒí™©ì— ë¶€í•©ë˜ëŠ” ê²½ìš°ì—ë§Œ ì‚¬ìš©ë˜ì–´ì•¼ë§Œ í•œë‹¤! ê·¸ëŸ¬ë‚˜ ëª‡ëª‡ ì‘ìš©ì—ì„œëŠ” ì–´ë–¤ ëª¨í˜•ì„ ì‚¬ìš©í•´ì•¼ í• ì§€ê°€ ë¶„ëª…í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ê²½ìš° 1) ê´€ì¸¡ê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ê°€ê¹Œìš´ ì •ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì´ ì”ì°¨ì œê³±í‰ê· ì´ë¯€ë¡œ ë‘ ëª¨í˜•ì— ì˜í•´ ì‚°ì¶œë˜ëŠ” ì”ì°¨í‰ê· ì œê³±(SSEë¥¼ ê°ê°ì˜ ëª¨í˜•ì— ëŒ€í•œììœ ë„ë¡œ ë‚˜ëˆˆ ê°’)ì„ ë¹„êµí•˜ì—¬ í‰ê°€í•œë‹¤. 2) ë°ì´í„° ëª¨í˜•ì„ ì í•©í•˜ê³  ì ˆí¸í•­ì˜ ìœ ì˜ì„±ì„ ê²€ì •í•˜ì—¬(tí†µê³„ëŸ‰ì„ ë°”íƒ•ìœ¼ë¡œ) ê²€ì •ì´ ìœ ì˜í•˜ë‹¤ë©´ ì ˆí¸í•­ì„ ê°€ì§„ ëª¨í˜•ì„ ì‚¬ìš©í•˜ê³  ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì ˆí¸í•­ì´ ì—†ëŠ” ëª¨í˜•ì„ ì‚¬ìš©í•œë‹¤. ê·¸ëŸ¬ë‚˜, ì¼ë°˜ì ìœ¼ë¡œ íšŒê·€ëª¨í˜•ì—ì„œëŠ” ìƒìˆ˜í•­ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ ì•Šë”ë¼ë„, ê°•í•œ ì´ë¡ ì  ê·¼ê±°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ìƒìˆ˜í•­ì€ ëª¨í˜•ì— í¬í•¨ë˜ì–´ì•¼ í•œë‹¤. íŠ¹íˆ ë¶„ì„ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ê°€ ì›ì ì„ í¬í•¨í•˜ì§€ ì•ŠëŠ” ê²½ìš° ë”ìš± ê°•ì¡°ë˜ëŠ”ë° ê·¸ ì´ìœ ëŠ” ìƒìˆ˜í•­ì´ ì¢…ì†(ë°˜ì‘)ë³€ìˆ˜ì˜ ê¸°ë³¸ì ì¸ ìˆ˜ì¤€(í‰ê· )ì„ì„ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì´ë‹¤. ì°¸ê³ ë¡œ í•„ìëŠ” ANCOVA ë¶„ì„ ì¦‰, íšŒê·€ì‹ì—ì„œ ì„¤ëª…ë³€ìˆ˜ë“¤ ì¤‘ ì§ˆì ì¸ ë³€ìˆ˜(í˜¹ì€ ë”ë¯¸ë³€ìˆ˜)ê°€ í¬í•¨ë˜ì–´ ìˆì–´ ê·¸ëŸ° ì§ˆì ì¸ ë³€ìˆ˜ì™€ ë”ë¯¸ë³€ìˆ˜ê°€ ì ˆí¸í•­(ìƒìˆ˜í•­)ì„ ëŒ€ì‹ í•´ì¤„ ê²ƒì´ë¼ê³  ì°©ê°í•˜ì—¬ ìƒìˆ˜í•­ì„ ìƒì„±í•˜ì§€ ì•Šê³ , ëª¨í˜•ì„ ì í•©ì‹œì¼°ë˜ ê²½í—˜ì´ ìˆë‹¤. í”„ë¡œì íŠ¸ì˜€ëŠ”ë° ë©˜í† ë¶„ê»˜ì„œ ì™œ ì ˆí¸í•­ì„ í¬í•¨í•˜ì§€ ì•Šì•˜ëƒê³  ë¬¼ì–´ë³´ë³´ì…¨ëŠ”ë° ìœ„ì™€ ê°™ì€ ë‹µë³€ì„ í–ˆì—ˆëŠ”ë° ì˜ëª»ëœ ì ‘ê·¼ë²•ì´ë¼ê³  ì¡°ì–¸ì„ í•´ì£¼ì…¨ì—ˆë‹¤. ê·¸ ë‹¹ì‹œì—ëŠ” ì´í•´ê°€ ê°€ì§€ ì•Šì•˜ì§€ë§Œ ì´ì œëŠ” ë‚˜ì˜ ì ‘ê·¼ë²•ì´ ë§ì´ ì•ˆëœë‹¤ëŠ” ê²ƒë¶€í„° ê¹¨ë‹¬ì•˜ë‹¤. ì™œëƒí•˜ë©´ SGD ë°©ë²•ìœ¼ë¡œ í™•ë¥ ë³€ìˆ˜ì¸ ì ˆí¸í•­ê³¼ ê³„ìˆ˜í•­ë“¤ì„ ì—…ë°ì´íŠ¸í•´ ë‚˜ê°€ëŠ” ë°©ì‹ìœ¼ë¡œ íšŒê·€ëª¨í˜•ì„ ì§œëŠ”ë° í•„ìëŠ” ì´ë¯¸ ìƒìˆ˜í•­ ì·¨ê¸‰ì„ í•˜ëŠ” ì§ˆì ë³€ìˆ˜ë‚˜ ë”ë¯¸ë³€ìˆ˜ ìì²´ë¥¼ ì ˆí¸í•­ì´ë¼ê³  ìƒê°í–ˆìœ¼ë‹ˆ ë§ ìì²´ê°€ ì•ˆë˜ëŠ” ê²ƒì´ë‹¤. ëª¨í˜•ì—ì„œ ì¤‘ì‹¬í™”(centering)ì™€ ì²™ë„í™”(scaling) íšŒê·€ë¶„ì„ì—ì„œëŠ” íšŒê·€ê³„ìˆ˜ì˜ í¬ê¸°ê°€ ë³€ìˆ˜ì˜ ì¸¡ì • ë‹¨ìœ„ì— ì˜í–¥ì„ ë°›ê²Œ ë˜ë¯€ë¡œ ì¤‘ì‹¬í™”ì™€ ì²™ë„í™”ë¥¼ í•´ì•¼í•œë‹¤.ì˜ˆë¥¼ ë“¤ì–´ ë‹¬ëŸ¬ ë‹¨ìœ„ë¡œ ì¸¡ì •ëœ ì†Œë“ì˜ íšŒê·€ê³„ìˆ˜ê°€ 5.123ì´ë¼ë©´, ì†Œë“ì´ 1,000ë‹¬ëŸ¬ ë‹¨ìœ„ë¡œ ì¸¡ì • ë˜ì—ˆì„ë•ŒëŠ” 5123ìœ¼ë¡œ ë°”ë€Œê²Œ ëœë‹¤. ì ˆí¸í•­(ìƒìˆ˜í•­)ì´ ìˆëŠ” ëª¨í˜•ì„ ë‹¤ë£° ë•ŒëŠ” ë³€ìˆ˜ì— ëŒ€í•œ ì¤‘ì‹¬í™”ì™€ ì²™ë„í™”ê°€ í•„ìš”í•˜ì§€ë§Œ, ì ˆí¸ì´ ì—†ëŠ” ëª¨í˜•ì„ ë‹¤ë£° ë•ŒëŠ” ë³€ìˆ˜ì˜ ì²™ë„í™”ë§Œ í•„ìš”í•˜ë‹¤. ë˜í•œ ì´ëŠ” ë‹¤ë¥¸ ì„ í˜•ì„±ì„ ê°€ì •í•˜ëŠ” ëª¨ë¸(RBF kernelì„ ì‚¬ìš©í•˜ëŠ” SVM, logistic regression)ì—ì„  í”¼ì²˜ë¥¼ ì •ê·œì„±ì„ ë„ê²Œ í•´ì£¼ì–´ì•¼í•˜ëŠ” ëª¨í˜• ë¿ë§Œì•„ë‹ˆë¼ í”¼ì²˜ scalingì„ í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë°©ë²•ì´ë¯€ë¡œ ì•Œê³ ìˆì–´ì•¼í•œë‹¤. ì¤‘ì‹¬í™”(centering) ë³€ìˆ˜ëŠ” ê° ê´€ì¸¡ê°’ì—ì„œ ëª¨ë“  ê´€ì¸¡ê°’ì˜ í‰ê· ì„ ë¹¼ëŠ” ê²ƒìœ¼ë¡œ ì–»ì–´ì§„ë‹¤. ì¤‘ì‹¬í™”ëœ ë³€ìˆ˜ ì²™ë„í™” ë˜í•œ ê°€ëŠ¥í•˜ë‹¤. ë‘ ê°€ì§€ í˜•íƒœì˜ ì²™ë„í™”(scaling)ê°€ í†µìƒì ìœ¼ë¡œ ê°€ëŠ¥í•œë°, ë‹¨ìœ„ ê¸¸ì´ ì²™ë„í™”(unit length scaling or normalization)ì™€ í‘œì¤€í™”(Standardization)ì´ë‹¤. ë‹¨ìœ„ê¸¸ì´ ì²™ë„í™”ëŠ” í”¼ì²˜ ë²¡í„°ì˜ ê¸¸ì´ë¡œ ë‚˜ëˆ„ì–´ì£¼ê±°ë‚˜ min-max scaling ê°™ì€ ê²ƒì„ ì˜ë¯¸í•œë‹¤. í‘œì¤€í™”ëŠ” ë§ ê·¸ëŒ€ë¡œ í¸ì°¨ë¥¼ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ì–´ í‘œì¤€ì •ê·œë¶„í¬ë¥¼ ë„ê²Œë”í•´ì£¼ëŠ” ì‘ì—…ì„ ì˜ë¯¸í•œë‹¤. ë‹¤ì¤‘ ê³µì„ ì„±(Multi-collinearity) Ordinary Least Squares(OLS) ì¦‰ ìµœì†Œ ì œê³±ë²• ê¸°ë°˜ì˜ íšŒê·€ ê³„ìˆ˜ ê³„ì‚°ì€ ë…ë¦½ ë³€ìˆ˜(ì…ë ¥ í”¼ì²˜)ì˜ ë…ë¦½ì„±ì— ë§ì€ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. í”¼ì²˜ê°„ì˜ ìƒê´€ê´€ê³„ê°€ ë§¤ìš° ë†’ì€ ê²½ìš° ë¶„ì‚°ì´ ë§¤ìš° ì»¤ì ¸ì„œ ì˜¤ë¥˜ì— ë§¤ìš° ë¯¼ê°í•´ì§€ë©° ì„ í˜•ëŒ€ìˆ˜ì˜ ê´€ì ì—ì„œ ë³´ë©´, ëª¨ë“  ì»¬ëŸ¼ë“¤ì´ linearly independentí•´ì•¼ ìµœì†Œí•œ í•˜ë‚˜ ì´ìƒì˜ í•´ê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë§Œì•½ ìœ„ì˜ ë§ì´ ì´í•´ê°€ ê°€ì§€ ì•ŠëŠ”ë‹¤ë©´, í•„ìê°€ ì¶”ì „í•˜ëŠ” ì„ í˜•ëŒ€ìˆ˜í•™ ê°•ì˜ë¥¼ ë“£ëŠ” ê²ƒì„ ê¶Œí•œë‹¤. ë‹¤ìŒ í˜ì´ì§€ë¥¼ ê°€ë©´ ì°¾ì„ ìˆ˜ ìˆë‹¤. ì„ í˜•ëŒ€ìˆ˜í•™ ê°•ì˜ ì¶”ì²œ ìœ„ì™€ ê°™ì€ ë‹¤ì¤‘ ê³µì„ ì„± ë¬¸ì œê°€ ìˆì„ ê²½ìš°, ì¼ë°˜ì ìœ¼ë¡œ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ë…ë¦½ ë³€ìˆ˜(ì…ë ¥ í”¼ì²˜)ê°€ ë§ì€ ê²½ìš° ë…ë¦½ì ì¸ ì¤‘ìš”í•œ ë…ë¦½ ë³€ìˆ˜(ì…ë ¥ ë³€ìˆ˜)ë§Œì„ ë‚¨ê¸°ê³  ì œê±°í•˜ê±°ë‚˜ ê·œì œë¥¼ ì ìš©í•œë‹¤. ë˜í•œ ë§¤ìš° ë§ì€ í”¼ì²˜ê°€ ë‹¤ì¤‘ ê³µì„ ì„± ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆë‹¤ë©´ PCAë¥¼ í†µí•´ ì°¨ì› ì¶•ì†Œë¥¼ ìˆ˜í–‰í•˜ëŠ” ê²ƒë„ ê³ ë ¤í•´ ë³¼ ìˆ˜ ìˆë‹¤. ë‹¤ì¤‘ ê³µì„ ì„± ê²€ì‚¬í•˜ëŠ” ë°©ë²•ë“¤ ìœ„ì—ì„œ VIFê°€ 10ì´ìƒì¸ ê²½ìš° ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆëŠ” ë³€ìˆ˜ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤ê³  í–ˆëŠ”ë°, ê·¸ë ‡ë‹¤ë©´ ë‹¤ì¤‘ê³µì„ ì„±ì´ ìˆë‹¤ê³  íŒë‹¨ë˜ëŠ” ë³€ìˆ˜ë¥¼ ë¬´ì¡°ê±´ì ìœ¼ë¡œ ì œê±°í•´ì•¼ í•˜ë‚˜ë¼ëŠ” ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ê·¸ì— ëŒ€í•œ ë‹µì€ ë¬´ì¡°ê±´ì ìœ¼ë¡œ ì œê±°í•˜ë©´ ì•ˆëœë‹¤ë¼ëŠ” ê²ƒì´ë‹¤. VIFê°€ ë†’ë”ë¼ë„ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ë³€ìˆ˜(p-valueê°€ ìœ ì˜ìˆ˜ì¤€ ë³´ë‹¤ ë‚®ì€ ë³€ìˆ˜)ë¼ë©´ ì œê±°í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì ì ˆí•˜ë‹¤. ì¶”ê°€ì ìœ¼ë¡œ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ ì¤‘ì— VIFê°€ ë†’ê³ , ìœ ì˜ë¯¸í•˜ì§€ ì•Šì€ ë³€ìˆ˜ê°€ ìˆë‹¤ë©´ ê·¸ ë³€ìˆ˜ë“¤ì„ ì œê±°í•´ ë³¸ ë’¤ VIFë¥¼ ê³„ì‚°í•´ ë³´ì•„ì•¼ í•  ê²ƒì´ë‹¤. ì´ ê³¼ì •ì„ ê±°ì³ì„œë„ ì•„ë§ˆë„ VIFëŠ” ë†’ì€ ìˆ˜ì¹˜ì´ê² ì§€ë§Œ, ì œê±°ë¥¼ í•´ì„œëŠ” ì•ˆëœë‹¤. ë‹¤ì¤‘ ê³µì„ ì„±ì„ ê²€ì‚¬í•˜ëŠ” ë°©ë²•ì—ëŠ” VIFì™¸ì—ë„ ìƒê´€ê³„ìˆ˜ í–‰ë ¬ì„ êµ¬í•´ì„œ ìœ„ì™€ ê°™ì´ ì‚°ì ë„ì™€ ê°™ì´ ê·¸ë ¤ì„œ ë³´ì•„ì•¼í•œë‹¤. ìƒê´€ê³„ìˆ˜ëŠ” ê³µë¶„ì‚°ì„ ê°ê°ì˜ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ì–´ì¤€ ìˆ˜ì¹˜ì¸ë°, ê³µë¶„ì‚°ì€ ì˜ˆë¥¼ ë“¤ì–´ ë‘ ë³€ìˆ˜ Xì™€ Yê°€ ìˆë‹¤ë©´, Yì™€ X ì‚¬ì´ì˜ ì„ í˜• ê´€ê³„ì— ëŒ€í•œ ë°©í–¥ì„ ë‚˜íƒ€ë‚¸ë‹¤. Cov(X, Y)ëŠ” ì¸¡ì •ë‹¨ìœ„ì˜ ë³€í™”ì— ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì— ìš°ë¦¬ì—ê²Œ ê´€ê³„ì˜ ê°•ë„ê°€ ì–¼ë§ˆë‚˜ ë˜ëŠ” ì§€ë¥¼ ì•Œë ¤ì£¼ì§€ëŠ” ì•Šê³  ë°©í–¥ë§Œì„ ì•Œë ¤ì¤€ë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ì–´ Standardizationì„ í•´ì£¼ì–´ ë‹¨ìœ„ì— ëŒ€í•œ ì˜í–¥ì„ ì—†ì• ì¤€ ê²ƒì´ ìƒê´€ê³„ìˆ˜ì´ë‹¤. ì°¸ê³ ë¡œ ì—¬ê¸°ì„œ Corr(X, Y)=0 ê°€ ë°˜ë“œì‹œ Yì™€ X ì‚¬ì´ì— ê´€ê³„ê°€ ì—†ìŒì„ ì˜ë¯¸í•˜ëŠ” ê²ƒì´ ì•„ë‹˜ì„ ì£¼ì˜í•˜ì! ìƒê´€ê³„ìˆ˜ëŠ” ì˜¤ì§ ì„ í˜• ê´€ê³„ë¥¼ ì¸¡ì •í•˜ê¸°ì— ì„ í˜•ì ìœ¼ë¡œ ê´€ê³„ê°€ ì—†ìŒì„ ì˜ë¯¸í•œë‹¤. ì¦‰, Xì™€ Yê°€ ë¹„ì„ í˜•ì ìœ¼ë¡œ ê´€ë ¨ë˜ì–´ ìˆì„ ë•Œì—ë„ Corr(X, Y)ê°€ 0ì´ ë  ìˆ˜ ìˆë‹¤.ë˜í•œ ìƒê´€ê³„ìˆ˜ë„ í‰ê· ê³¼ ë¶„ì‚°ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ê·¹ë‹¨ê°’ì— ë¯¼ê°í•˜ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì´ëŸ¬í•œ ìš”ì•½ í†µê³„ëŸ‰ì—ë§Œ ì˜ì¡´í•˜ëŠ” ë¶„ì„ìœ¼ë¡œëŠ” ì „ì²´ì ì¸ íŒ¨í„´ì„ ë³´ëŠ”ë°ì— ìˆì–´ì„œ ì°¨ì´ë¥¼ ë°œê²¬í•  ìˆ˜ ì—†ê²Œ í•  ê²ƒì´ë‹¤. ë”°ë¼ì„œ í•„ìëŠ” ê°œì¸ì ìœ¼ë¡œ ë°ì´í„° EDA ê³¼ì •ì—ì„œ ë…ë¦½(ì„¤ëª…)ë³€ìˆ˜ë“¤ê³¼ ë°˜ì‘ ë³€ìˆ˜ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ê¼­ ì‚°ì ë„ë¡œ ê·¸ë ¤ í™•ì¸í•œ ë’¤, ìƒê´€ê³„ìˆ˜ì™€ ì˜ë¯¸ê°€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ëŠ” ì‘ì—…ì´ í•„ìˆ˜ë¼ê³  ì—¬ê¸´ë‹¤. í†µê³„ë¥¼ ê³µë¶€í•˜ëŠ” Beginnerë“¤ì´ ë§ì´ë“¤ ì˜¤í•´í•  ë§Œí•œ ì‚¬ì‹¤ì€ Corr(X, Y)ëŠ” í•œ ë³€ìˆ˜ì˜ ê°’ì´ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ë‹¤. ë‹¨ì§€ ëŒ€ì‘(pairwise)ê´€ê³„ë§Œ ì¸¡ì •í•œë‹¤. ì˜ˆì¸¡ì„ í•˜ê³  ê´€ê³„ë¥¼ ì„¤ëª…í•˜ê¸° ìœ„í•´ì„œ ìš°ë¦¬ëŠ” íšŒê·€ë¶„ì„ì„ í•˜ëŠ” ê²ƒ!!!!","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"NLPë¥¼ ê³µë¶€í•˜ëŠ”ë° ë„ì›€ë˜ëŠ” ì‚¬ì´íŠ¸ ëª¨ìŒ","slug":"NLP_00","date":"2020-01-07T11:57:26.000Z","updated":"2020-01-18T07:37:27.407Z","comments":true,"path":"2020/01/07/NLP_00/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/07/NLP_00/","excerpt":"","text":"ìì—°ì–´ ì²˜ë¦¬ ê´€ë ¨ ìë£Œ ìì—°ì–´ ì²˜ë¦¬ì— ëŒ€í•´ ê³µë¶€í•  ìˆ˜ ìˆê²Œ ë„ì›€ì´ ë  ë§Œí•œ ì‚¬ì´íŠ¸ ìì—°ì–´ ì²˜ë¦¬ ê°•ì˜ ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬:https://www.edwith.org/deepnlp ìì—°ì–´ ì²˜ë¦¬ ì˜¤í”„ë¼ì¸ ìŠ¤í„°ë”” ëª¨ì„ DeepNLP(ëª¨ë‘ì˜ì—°êµ¬ì†Œ ìì—°ì–´ ì²˜ë¦¬ ìŠ¤í„°ë””):http://www.modulabs.co.kr/information ë°”ë²¨í”¼ì‰¬(ì‹¸ì´ê·¸ë˜ë¨¸ ìŠ¤í„°ë””) : https://www.facebook.com/groups/babelPish/ ì˜¨ë¼ì¸ ì°¸ê³  ìë£Œ ìŠ¤íƒ í¼ë“œ ìì—°ì–´ ì²˜ë¦¬ ê°•ì˜ : http://web.stanford.edu/class/cs224n Jacob Eisenstein êµìˆ˜ë‹˜ì˜ ìì—°ì–´ ì²˜ë¦¬ ê°•ì˜ : https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf YSDA ìì—°ì–´ ì²˜ë¦¬ : https://github.com/yandexdataschool/nlp_course ì¡°ê²½í˜„ êµìˆ˜ë‹˜ì˜ ìì—°ì–´ ì²˜ë¦¬ ê°•ì˜ ë…¸íŠ¸ : https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf íŒŒì´ì¬ ì •ê·œ í‘œí˜„ì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ reíŒŒì´ì¬ ì •ê·œ í‘œí˜„ì‹ . ì¤„ ë°”ê¿ˆì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì ^ ë¬¸ìì—´ì˜ ì‹œì‘ $ ë¬¸ìì—´ì˜ ë * ì•ì— ìˆëŠ” ë¬¸ìê°€ 0íšŒ ì´ìƒ ë°˜ë³µëœ ë¬¸ìì—´ + ì•ì— ìˆëŠ” ë¬¸ìê°€ 1íšŒ ì´ìƒ ë°˜ë³µëœ ë¬¸ìì—´ {m} ì• ë¬¸ìë¥¼ míšŒ ë°˜ë³µí•˜ëŠ” ë¬¸ìì—´ {m, n} ì• ë¬¸ìë¥¼ m~níšŒ ë°˜ë³µí•˜ëŠ” ë¬¸ìì—´ ? ì• ë¬¸ìê°€ ë‚˜ì˜¤ê±°ë‚˜ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ë¬¸ìì—´ ({0, 1}ì™€ ë™ì¼) \\d ìˆ«ì \\D ìˆ«ìê°€ ì•„ë‹Œ ë¬¸ì \\w ë¬¸ì í˜¹ì€ ìˆ«ì \\W ë¬¸ì í˜¹ì€ ìˆ«ìê°€ ì•„ë‹Œê²ƒ (â€¦) ê´„í˜¸ ì•ˆì˜ ëª¨ë“  ì •ê·œ í‘œí˜„ì‹ì„ ë§Œì¡±í•˜ëŠ” ë¬¸ì [abc] a, b, c ì¤‘ í•œ ê°œì˜ ë¬¸ìì™€ ì¼ì¹˜ re í•¨ìˆ˜ re ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ í•¨ìˆ˜ 4ê°€ì§€ë¥¼ ì‚´í´ ë³¼ ê²ƒì´ë‹¤. ì´ ë°–ì—ë„ re ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì—¬ëŸ¬ ê°€ì§€ ì •ê·œ í‘œí˜„ì‹ì„ ì´ìš©í•´ ë¬¸ìì—´ì„ ë‹¤ë£° ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•œë‹¤. re.compile(pattern) compile í•¨ìˆ˜ëŠ” íŠ¹ì • ê¸°í˜¸ë¥¼ ì •ê·œí‘œí˜„ì‹ ê°ì²´ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤. re ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì •ê·œí‘œí˜„ì‹ íŒ¨í„´ì„ ë§¤ë²ˆ ì‘ì„±í•´ì•¼í•˜ëŠ”ë°, ì´ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ íŒ¨í„´ì„ ì»´íŒŒì¼í•˜ë©´ í•„ìš”í•  ë•Œë§ˆë‹¤ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. 123#ìˆ«ìë‚˜ ë¬¸ìê°€ ì•„ë‹Œ ê²ƒì´ 1íšŒì´ìƒ ë°˜ë³µë˜ëŠ” ë¬¸ìì—´pattern = ' \\W+'re_pattern = re.compile(pattern) re.search(pattern, string) search í•¨ìˆ˜ëŠ” í•´ë‹¹ ë¬¸ìì—´ì—ì„œ ì •ê·œ í‘œí˜„ì‹ì— í•´ë‹¹í•˜ëŠ” ì²« ë¶€ë¶„ì„ ì°¾ëŠ”ë‹¤. 1234567# ë¬¸ìë‚˜ ìˆ«ìì¸ ê²ƒì´ 1íšŒ ì´ìƒ ë°˜ë³µë˜ëŠ” ë¬¸ìì—´# ì¦‰, íƒ­, ì¤„ë°”ê¿ˆ, ê³µë°±ì´ ì•„ë‹Œ ë¬¸ìë¥¼ ëª¨ë‘ ì°¾ëŠ” ê³¼ì •re.search(\"(\\w+)\"), \"wow, it is awesome\")# ê²°ê³¼# ë²”ìœ„ê°€ (0,3), ì°¾ì€ ë¬¸ìëŠ” 'wow'ë¡œ ê·¸ ë’¤ì— ,ì™€ ê³µë°±ì´ ìˆìœ¼ë¯€ë¡œ ê·¸ì „ê¹Œì§€ì˜ ë¬¸ìë¥¼ ì¶œë ¥&lt;_sre.SRE_Match object; span=(0,3), match='wow'&gt; re.split(pattern, string) split í•¨ìˆ˜ëŠ” í•´ë‹¹ ë¬¸ìì—´ì—ì„œ íŠ¹ì • íŒ¨í„´ìœ¼ë¡œ ë¬¸ìì—´ì„ ë‚˜ëˆ ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“ ë‹¤. 12345# ë¬¸ì í˜¹ì€ ìˆ«ìê°€ ì•„ë‹Œ ê²ƒìœ¼ë¡œ ë¬¸ìì—´ì„ ë‚˜ëˆ ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ì¶œë ¥re.split('\\W', \"wow, it is world of word\")# ê²°ê³¼['wow', '', 'it', 'is', 'world', 'of', 'word'] re.sub(pattern, repl, string) ë¬¸ìì—´ì—ì„œ íŠ¹ì • íŒ¨í„´ì„ ë§Œì¡±ì‹œí‚¤ëŠ” ë¬¸ìë¥¼ ì‚¬ìš©ìê°€ ì •ì˜í•œ ë¬¸ì(repl)ë¡œ ì¹˜í™˜í•œë‹¤. 12345# ìˆ«ìì¸ ê²ƒì„ numberë¡œ ì¹˜í™˜í•œ ë¬¸ìì—´ ë°˜í™˜re.sub('\\d', 'number', '7 candy')# ê²°ê³¼'number candy' Kaggle Kaggleì—ì„œ APIë¥¼ í™œìš©í•´ì„œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë°›ëŠ” ë°©ë²•ì„ ì†Œê°œí•˜ë ¤ê³  í•œë‹¤. ìºê¸€ API ì—°ë™ì„ ìœ„í•´ì„œëŠ” ë‘ê°€ì§€ ë‹¨ê³„ê°€ í•„ìš”í•˜ë‹¤. ë‹¨ ì´ ë°©ë²•ì€ Local PC í™˜ê²½ì—ì„œì˜ ë°©ë²•ì´ë¯€ë¡œ Colabê³¼ ì—°ë™í•´ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ì¶”ê°€ì ì¸ ì‘ì—…ë“¤ì´ í•„ìš”í•˜ë‹¤. 1conda install kaggle APIê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì¹˜ë˜ë©´ ê³„ì •ì„ ì—°ë™í•´ì•¼ í•œë‹¤. ìºê¸€ í™ˆí˜ì´ì§€ì—ì„œ íšŒì›ê°€ì… í›„ Account íƒ­ìœ¼ë¡œ ê°€ì„œ â€˜Create API Tokenâ€™ì„ ì„ íƒí•œ í›„ kaggle.json íŒŒì¼ì„ ë‚´ë ¤ë°›ëŠ”ë‹¤. ì´ íŒŒì¼ì—ëŠ” ë³¸ì¸ì˜ ì¸ì¦ì„œê°€ ìˆê³  ì´ íŒŒì¼ì„ ë‹¤ìŒì˜ ìœ„ì¹˜ë¡œ ì´ë™ì‹œí‚¨ë‹¤. 12345# ìœˆë„ì›…C:\\Users\\&lt;ì‚¬ìš©ìëª…&gt;\\.kaggle# macOS, Linux$ /&lt;ì‚¬ìš©ì í™ˆ ë””ë ‰í† ë¦¬&gt;/.kaggle ì´ì œ APIë¥¼ í™œìš©í•´ ë°ì´í„°ë¥¼ ë‚´ë ¤ ë°›ì„ ìˆ˜ ìˆë‹¤. ë°ì´í„°ë¥¼ ë‚´ë ¤ë°›ëŠ” ë°©ë²•ì€ Data(ë°ì´í„°) íƒ­ì˜ API ëª…ë ¹ì–´ë¥¼ ë³µì‚¬í•œ í›„ ì»¤ë§¨ë“œ ë¼ì¸ì—ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì‹¤í–‰í•˜ë©´ ëœë‹¤. 1$ kaggle competitions download -c &lt;competition-name&gt; ë°ì´í„° ëª©ë¡ í™•ì¸ 1$ kaggle competitions files -c &lt;competition-name&gt; ë°ì´í„° ì œì¶œ 1kaggle competions submit &lt;competition-name&gt; -f &lt;file-name&gt; -m &lt;message&gt; ëŒ€íšŒ ëª©ë¡ í™•ì¸ 1kaggle competition list ë³´ë‹¤ ë” ìì„¸í•œ ê¸°ëŠ¥ì€ ìºê¸€ë¬¸ì„œì—ì„œ í™•ì¸í•´ ë³´ì.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"Regression(01) - íšŒê·€ì˜ ì¢…ë¥˜ ë° íšŒê·€ê³„ìˆ˜","slug":"machine_learning_01","date":"2020-01-04T02:44:20.000Z","updated":"2020-03-24T09:01:44.489Z","comments":true,"path":"2020/01/04/machine_learning_01/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/04/machine_learning_01/","excerpt":"","text":"íšŒê·€ë¶„ì„ì´ë€? ì§€ë„ í•™ìŠµì€ ë‘ ê°€ì§€ ìœ í˜•ìœ¼ë¡œ ë‚˜ë‰˜ëŠ”ë°, ë°”ë¡œ ë¶„ë¥˜(classification)ì™€ íšŒê·€(regression)ì´ë‹¤. ì´ ë‘ ê°€ì§€ ê¸°ë²•ì˜ ê°€ì¥ í° ì°¨ì´ëŠ” ë¶„ë¥˜ëŠ” ì˜ˆì¸¡ê°’ì´ ì¹´í…Œê³ ë¦¬ì™€ ê°™ì€ ì´ì‚°í˜• í´ë˜ìŠ¤ ê°’ì´ê³ , íšŒê·€ëŠ” ì—°ì†í˜• ìˆ«ì ê°’ì´ë¼ëŠ” ê²ƒì´ë‹¤. íšŒê·€(regression)ì€ í˜„ëŒ€ í†µê³„í•™ì„ ë– ë°›ì¹˜ê³  ìˆëŠ” ì£¼ìš” ê¸°ì¤‘ ì¤‘ í•˜ë‚˜ì´ë‹¤. ì—¬ëŸ¬ë¶„ì´ íšŒê·€ë¶„ì„ì‹œì— ë§ì´ ë“¤ì–´ë´¤ì„ ì˜ˆì‹œëŠ” ë¶€ëª¨ì˜ í‚¤ì™€ ìì‹ì˜ í‚¤ì—ëŒ€í•œ ì˜ˆì‹œê°€ ìˆì„ ê²ƒì´ë‹¤. ë¶€ëª¨ì˜ í‚¤ê°€ ì•„ì£¼ í¬ë”ë¼ë„ ìì‹ì˜ í‚¤ê°€ ë¶€ëª¨ë³´ë‹¤ ë” ì»¤ì„œ ì„¸ëŒ€ë¥¼ ì´ì–´ê°€ë©´ì„œ ë¬´í•œì • ì»¤ì§€ëŠ” ê²ƒì€ ì•„ë‹ˆë©°, ë¶€ëª¨ì˜ í‚¤ê°€ ì•„ì£¼ ì‘ë”ë¼ë„ ìì‹ì˜ í‚¤ê°€ ë¶€ëª¨ë³´ë‹¤ ë” ì‘ì•„ì„œ ì„¸ëŒ€ë¥¼ ì´ì–´ê°€ë©° ë¬´í•œì • ì‘ì•„ì§€ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ëŠ” ê²ƒì´ë‹¤. ì¦‰, ì‚¬ëŒì˜ í‚¤ëŠ” í‰ê·  í‚¤ë¡œ íšŒê·€í•˜ë ¤ëŠ” ê²½í–¥ì„ ê°€ì§„ë‹¤ëŠ” ìì—°ì˜ ë²•ì¹™ì´ë¼ëŠ” ì˜ë¯¸ì´ë©°, íšŒê·€ë¶„ì„ì€ ì´ì²˜ëŸ¼ ë°ì´í„° ê°’ì´ í‰ê· ê³¼ ê°™ì€ ì¼ì •í•œ ê°’ìœ¼ë¡œ ëŒì•„ê°€ë ¤ëŠ” ê²½í–¥ì„ ì´ìš©í•œ í†µê³„í•™ ê¸°ë²•ì´ë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ê´€ì ì—ì„œ ë³´ë©´ ë…ë¦½ë³€ìˆ˜ëŠ” í”¼ì²˜ì— í•´ë‹¹ë˜ë©°, ì¢…ì†ë³€ìˆ˜ëŠ” ê²°ì • ê°’ì´ë‹¤. ë¨¸ì‹ ëŸ¬ë‹ íšŒê·€ ì˜ˆì¸¡ì˜ í•µì‹¬ì€ ì£¼ì–´ì§„ í”¼ì²˜ì™€ ê²°ì • ê°’ ë°ì´í„° ê¸°ë°˜ì—ì„œ í•™ìŠµì„ í†µí•´ ìµœì ì˜ íšŒê·€ ê³„ìˆ˜ë¥¼ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ë‹¤. íšŒê·€ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ë°”ë¡œ íšŒê·€ ê³„ìˆ˜ì´ë‹¤. ì´ íšŒê·€ ê³„ìˆ˜ê°€ ì„ í˜•ì´ë‚˜ ì•„ë‹ˆëƒì— ë”°ë¼ ì„ í˜•íšŒê·€ì™€ ë¹„ì„ í˜• íšŒê·€ë¡œ ë‚˜ëˆŒìˆ˜ ìˆìœ¼ë©°, ë…ë¦½ë³€ìˆ˜ì˜ ê°œìˆ˜ê°€ í•œê°œ ì¸ì§€ ì—¬ëŸ¬ê°œì¸ì§€ì— ë”°ë¼ ë‹¨ì¼ íšŒê·€, ë‹¤ì¤‘ íšŒê·€ë¡œ ë‚˜ë‰œë‹¤. ì„ í˜•(ë¹„ì„ í˜•)ì´ë¼ëŠ” ìš©ì–´ëŠ” Yì™€ $X_{1},X_{2},â€¦,X_{p}ì˜ ê´€ê³„ë¥¼ ë¬˜ì‚¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ëŠ” ê²ƒì— ì£¼ëª©í•˜ì—¬ë¼! íšŒê·€ê³„ìˆ˜ê°€ ë°©ì •ì‹ì— ì„ í˜•ì (ë¹„ì„ í˜•ì )ìœ¼ë¡œ ì‚½ì…ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒê³¼ ê´€ë ¨ì´ ìˆë‹¤. ì„ í˜• í•¨ìˆ˜ì˜ ì˜ˆ Y = \\beta_{0} + \\beta_{1}X_{1} + \\epsilon- `Yì™€ X ì‚¬ì´ì˜ ê´€ê³„ëŠ” ë¹„ì„ í˜•ì´ì§€ë§Œ, ëª¨ìˆ˜ë“¤ì´ ì„ í˜•ì ìœ¼ë¡œ ì‚½ì…ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì„ í˜• í•¨ìˆ˜` Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}{X_{2}}^{2} + \\epsilonY = \\beta_{0} + \\beta_{1}\\log X_{1} + \\epsilon ë¹„ì„ í˜• í•¨ìˆ˜ì˜ ì˜ˆ Y = \\beta_{0} + e^{\\beta_{1}X_{1}} + \\epsilon ê° ë…ë¦½ë³€ìˆ˜ë“¤ì€ ì–‘ì (quantitative) í˜¹ì€ ì§ˆì (qualitative)ìœ¼ë¡œ ë¶„ë¥˜ ë  ìˆ˜ ìˆë‹¤. ì–‘ì  ë³€ìˆ˜ì˜ ì˜ˆ) : ì£¼íƒ ê°€ê²©, ì¹¨ì‹¤ì˜ ê°œìˆ˜, ì—°ìˆ˜, ì„¸ê¸ˆ ë“± ì§ˆì  ë³€ìˆ˜ì˜ ì˜ˆ) : ì´ì›ƒì˜ í˜•íƒœ(ì¢‹ì€ í˜¹ì€ ë‚˜ìœ ì´ì›ƒ), ì§‘ì˜ í˜•íƒœ(ì •ì›ì´ìˆëŠ”, ê³ í’ìŠ¤ëŸ¬ìš´ ë“±) ë…ë¦½ë³€ìˆ˜ë“¤ì€ ì–‘ì  ì§ˆì  ë³€ìˆ˜ ëª¨ë‘ ì·¨í•  ìˆ˜ ìˆëŠ”ë°, ì§ˆì  ë³€ìˆ˜ë“¤ì´ ìˆë‹¤ë©´, ê³„ì‚°ìƒì˜ ì´ìœ ë¡œ ë”ë¯¸ ë³€ìˆ˜(dummy variable)ë¡œ ì½”ë”©ì„ í•´ì£¼ì–´ì•¼ í•œë‹¤. ë‹¨, ì§ˆì ì¸ ë³€ìˆ˜ë“¤ë„ ì˜ˆë¥¼ ë“¤ì–´ ì „ë¬¸ê°€ë“¤ì— ì˜í•´ ì´ë¯¸ ê·œì •ë˜ì–´ ì‹ ë¢°ì„± ìˆëŠ” ê³µì‹ì´ë‚˜ ê·œì¹™ì„ í†µí•´ ì—°ì†ì ì¸ ìˆ˜ì¹˜ë¡œ ë³€í™˜ë  ìˆ˜ ìˆë‹¤ë©´ ë”ë¯¸ ë³€ìˆ˜ë¡œ ë§Œë“¤ì–´ ì£¼ì§€ ì•Šê³  ì‚¬ìš©í•´ë„ ëœë‹¤. ëª¨ë“  ë…ë¦½ë³€ìˆ˜ë“¤ì´ ì§ˆì ì¸ ê²½ìš° ë¶„ì‚° ë¶„ì„(ANOVA : analysis of variance)ê¸°ë²•ì´ë¼ê³  í•œë‹¤. ë¶„ì‚° ë¶„ì„ì€ ê·¸ ìì‹ ì˜ ê³ ìœ í•œ ë°©ë²•ìœ¼ë¡œì¨ ì†Œê°œë˜ê³  í†µê³„í•™ë¶€ìƒë“¤ì´ í†µê³„ì  ìë£Œë¶„ì„ì´ë¼ëŠ” ì£¼ì œë¡œ ìˆ˜ì—…ì„ ìˆ˜ê°•í• ë•Œ ë‚˜ì˜¤ëŠ” ê°œë…ìœ¼ë¡œì¨ ì„¤ëª…ë˜ê³  ìˆëŠ”ë° íšŒê·€ë¶„ì„ì˜ íŠ¹ë³„í•œ ê²½ìš°ì„ì„ ì•Œê³ ìˆì–´ë¼!!!! ë˜í•œ, ì–´ë–¤ ì˜ˆì¸¡ë³€ìˆ˜ë“¤ì´ ì–‘ì ì´ê³  ë°˜ë©´ì— ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì´ ì§ˆì ì´ë¼ë©´, ì´ëŸ¬í•œ ê²½ìš°ì˜ íšŒê·€ë¶„ì„ì„ ê³µë¶„ì‚°ë¶„ì„(ANCOVA : analysis of covariance)ì´ë¼ê³  í•œë‹¤. íšŒê·€ì˜ ìœ í˜• ì¡°ê±´ - ì¼ë³€ëŸ‰(Univariate) - ì˜¤ì§ í•˜ë‚˜ì˜ ì–‘ì  ë…ë¦½ë³€ìˆ˜(ì„¤ëª…ë³€ìˆ˜) - ë‹¤ë³€ëŸ‰(Multivariate) - ë‘ ê°œ ì´ìƒì˜ ì–‘ì  ë…ë¦½ë³€ìˆ˜(ì„¤ëª…ë³€ìˆ˜) - ë‹¨ìˆœ(Simple) - ì˜¤ì§ í•˜ë‚˜ì˜ ì¢…ì†ë³€ìˆ˜(ë°˜ì‘ë³€ìˆ˜) - ë‹¤ì¤‘(Multiple) - ë‘ ê°œ ì´ìƒì˜ ì¢…ì†ë³€ìˆ˜(ë°˜ì‘ë³€ìˆ˜) - ì„ í˜•(Linear) - ë°ì´í„°ì— ëŒ€í•˜ì—¬ ê°€ëŠ¥í•œ ë³€í™˜ì„ ì·¨í•œ í›„, ëª¨ë“  ê³„ìˆ˜ë“¤ì´ ë°©ì •ì‹ì— ì„ í˜•ì ìœ¼ë¡œ ì‚½ì…ë˜ì–´ ìˆìŒ. - ë¹„ì„ í˜•(Nonlinear) - ì¢…ì†ë³€ìˆ˜(ë°˜ì‘ë³€ìˆ˜)ì™€ ì¼ë¶€ ë…ë¦½ë³€ìˆ˜ë“¤ì˜ ê´€ê³„ê°€ ë¹„ì„ í˜•ì´ê±°ë‚˜ ì¼ë¶€ ê³„ìˆ˜ë“¤ì´ ë¹„ì„ í˜•ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¨. ê³„ìˆ˜ë“¤ì„ ì„ í˜•ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ê²Œ í•˜ëŠ” ì–´ë–¤ ë³€í™˜ë„ ê°€ëŠ¥í•˜ì§€ ì•ŠìŒ. - ë¶„ì‚°ë¶„ì„(ANOVA) - ëª¨ë“  ë…ë¦½ë³€ìˆ˜ë“¤ì´ ì§ˆì  ë³€ìˆ˜ì„. - ê³µë¶„ì‚°ë¶„ì„(ANCOVA) - ì–´ë–¤ ë…ë¦½ë³€ìˆ˜ë“¤ì€ ì–‘ì ë³€ìˆ˜ì´ê³  ë‹¤ë¥¸ ë…ë¦½ë³€ìˆ˜ë“¤ì€ ì§ˆì ë³€ìˆ˜ì„. - ë¡œì§€ìŠ¤í‹±(Logistic) - ì¢…ì†ë³€ìˆ˜(ë°˜ì‘ë³€ìˆ˜)ê°€ ì§ˆì ë³€ìˆ˜ì„. ëŒ€í‘œì ì¸ ì„ í˜• íšŒê·€ ëª¨í˜•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.ì¼ë°˜ ì„ í˜• íšŒê·€ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì”ì°¨ ì œê³±í•©ì„ ìµœì†Œí™”í•  ìˆ˜ ìˆë„ë¡ íšŒê·€ ê³„ìˆ˜ë¥¼ ìµœì í™”í•˜ë©°, ê·œì œ(Regularization)ë¥¼ ì ìš©í•˜ì§€ ì•Šì€ ëª¨ë¸ì´ë‹¤. ë¦¿ì§€(Ridge) Ridge íšŒê·€ëŠ” ì„ í˜• íšŒê·€ì— L2 Regularizationì„ ì¶”ê°€í•œ ëª¨í˜•ì´ë‹¤. Ridge íšŒê·€ëŠ” L2 Regularizationì„ ì ìš©í•˜ëŠ”ë°, L2 Regularizationì€ ìƒëŒ€ì ìœ¼ë¡œ í° íšŒê·€ ê³„ìˆ˜ ê°’ì˜ ì˜ˆì¸¡ ì˜í–¥ë„ë¥¼ ê°ì†Œ ì‹œí‚¤ê¸° ìœ„í•´ì„œ íšŒê·€ ê³„ìˆ˜ê°’ì„ ë” ì‘ê²Œ ë§Œë“œëŠ” Regularization ëª¨í˜•ì´ë‹¤. ë¼ì˜(Lasso) Lasso íšŒê·€ëŠ” ì„ í˜• íšŒê·€ì— L1 Regularizationì„ ì ìš©í•œ ë°©ì‹ì´ë‹¤. L2 Regularizationì´ íšŒê·€ ê³„ìˆ˜ ê°’ì˜ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ë° ë°˜í•´, L1 Regularizationì€ ì˜ˆì¸¡ ì˜í–¥ë ¥ì´ ì‘ì€ í”¼ì²˜ì˜ íšŒê·€ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ íšŒê·€ ì˜ˆì¸¡ ì‹œ í”¼ì²˜ê°€ ì„ íƒë˜ì§€ ì•Šê²Œ í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ íŠ¹ì„± ë•Œë¬¸ì— L1 Regularizationì€ í”¼ì²˜ ì„ íƒ ê¸°ëŠ¥ìœ¼ë¡œë„ ë¶ˆë¦°ë‹¤. ì—˜ë¼ìŠ¤í‹±ë„·(ElasticNet) L2, L1 Regularizationì„ í•¨ê»˜ ê²°í•©í•œ ëª¨í˜•ì´ë‹¤. ì£¼ë¡œ í”¼ì²˜ê°€ ë§ì€ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì ìš©ë˜ë©°, L1 Regularizationìœ¼ë¡œ í”¼ì²˜ì˜ ê°œìˆ˜ë¥¼ ì¤„ì„ê³¼ ë™ì‹œì— L2 Regularizationìœ¼ë¡œ ê³„ìˆ˜ì˜ ê°’ì˜ í¬ê¸°ë¥¼ ì¡°ì •í•œë‹¤. ë¡œì§€ìŠ¤í‹±(Logistic) ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” íšŒê·€ë¼ëŠ” ì´ë¦„ì´ ë¶™ì–´ ìˆì§€ë§Œ, ì‚¬ì‹¤ì€ ë¶„ë¥˜ì— ì‚¬ìš©ë˜ëŠ” ì„ í˜• ëª¨í˜•ì´ë‹¤. ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ë§¤ìš° ê°•ë ¥í•œ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ ë¿ë§Œì•„ë‹ˆë¼ í¬ì†Œ ì˜ì—­ì˜ ë¶„ë¥˜, ì˜ˆë¥¼ ë“¤ì–´ í…ìŠ¤íŠ¸ ë¶„ë¥˜ì™€ ê°™ì€ ì˜ì—­ì—ì„œ ë›°ì–´ë‚œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ê°€ì •ì—ì„œ ì”ì°¨( $\\epsilon_{i}$ )ì™€ target ê°’ì¸ Yê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. íšŒê·€ ê³„ìˆ˜ ì¶”ì • íšŒê·€ ê³„ìˆ˜ì˜ ì˜ë¯¸ íšŒê·€ ê³„ìˆ˜ì˜ ê²€ì •","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"ë¨¸ì‹ ëŸ¬ë‹ì˜ ê°œìš”","slug":"machine_learning_00","date":"2019-12-30T08:20:19.000Z","updated":"2020-01-08T07:45:51.536Z","comments":true,"path":"2019/12/30/machine_learning_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/30/machine_learning_00/","excerpt":"","text":"Machine learning Machine Learningìœ¼ë¡œ í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤ ë…ë¦½ë³€ìˆ˜ ë°˜ì‘ë³€ìˆ˜ ëª¨í˜• ê³ ê°ë“¤ì˜ ê°œì¸ ì •ë³´ ë° ê¸ˆìœµ ê´€ë ¨ ì •ë³´ ëŒ€ì¶œ ì—°ì²´ ì—¬ë¶€ ëŒ€ì¶œ ì—°ì²´ì ì˜ˆì¸¡ íƒì§€ ëª¨ë¸, ëŒ€ì¶œ ì—°ì²´ ê´€ë ¨ ì£¼ìš” feature ì¶”ì¶œ ê²Œì„ ìœ ì €ë“¤ì˜ ê²Œì„ ë‚´ í™œë™ ì •ë³´ ê²Œì„ ì´íƒˆ ì—¬ë¶€. ì–´ë·°ì§• ì—¬ë¶€ ì´ìƒ íƒì§€ ëª¨ë¸(anomaly detection model) ìˆ«ì ì† ê¸€ì”¨ ë°ì´í„° ìˆ«ì ë¼ë²¨(0~9) ìˆ«ì ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ ìƒí’ˆ êµ¬ë§¤ ê³ ê° íŠ¹ì„± ì •ë³´ êµ°ì§‘í™”ë¥¼ í†µí•œ ê³ ê° íŠ¹ì„±ì— ë”°ë¥¸ Segmentation Segmentation ëª¨ë¸ ê³ ê°ë“¤ì˜ ìƒí’ˆ êµ¬ë§¤ë‚´ì—­ ë§¤ì¥ë‚´ ìƒí’ˆ ì§„ì—´ ìœ„ì¹˜ ë¦¬ë‰´ì–¼ì„ í†µí•œ ë§¤ì¶œ ì¦ëŒ€ ì‡¼í•‘ëª° í˜ì´ì§€ ê²€ìƒ‰ ë° í´ë¦­ ë¡œê·¸ ê¸°ë¡ ë§ì¶¤ ìƒí’ˆ ì¶”ì²œ ì‹œìŠ¤í…œ recommendation system SNS ë°ì´í„° ë° ë‰´ìŠ¤ ë°ì´í„° ì†Œì…œ ë° ì‚¬íšŒ ì´ìŠˆ íŒŒì•… Supervised Learning VS Unspervised Learning ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì´ ì‹¤ë¬´ì—ì„œ ì‚¬ìš©ë˜ì–´ì§€ëŠ” ê²½ìš°ì— íŒ€ì˜ êµ¬ë¶„ì„ ë‘ê³  íšŒì‚¬ì—ì„œ ìš´ì˜í•˜ëŠ” ê²½ìš°ê°€ ë§ì§€ë§Œ, ì„œë¡œ êµ¬ë¶„í•´ì„œ ê³µë¶€í•˜ê±°ë‚˜ ì´í•´í•˜ì§„ ì•Šì•˜ìœ¼ë©´ ì¢‹ê² ë‹¤ëŠ”ê²ƒì´ í•„ìì˜ ë°”ëŒì´ë‹¤. ë¬¼ë¡ , ìµœê·¼ì—ëŠ” ë”¥ëŸ¬ë‹ì˜ ìœ í–‰ìœ¼ë¡œ ëŒ€ë¶€ë¶„ì˜ ë¶„ì„ì§êµ°ì—ì„œ ë”¥ëŸ¬ë‹ì„ ìœ„í•œ ì¸ì›ì„ ì±„ìš©í•˜ê³  ìˆê¸´ í•˜ì§€ë§Œ ë¨¸ì‹ ëŸ¬ë‹ë˜í•œ ì¤‘ìš”í•˜ì§€ ì•Šì€ ê²ƒì´ ì•„ë‹ˆë©´, ì•„ì§ë„ ë§ì€ ê³³ì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ë“¤ì„ ë½‘ê³  ìˆë‹¤. ëª¨í˜•ì˜ ì í•©ì„± í‰ê°€ ë° ì‹¤í—˜ ì„¤ê³„ ì „ì²´ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì‘ì—…ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. ì „ì²˜ë¦¬ Raw ë°ì´í„°ë¥¼ ëª¨ë¸ë§ í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„°ë¥¼ ë³‘í•© ë° íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì‹¤í—˜ ì„¤ê³„ ìœ„ì—ì„œ ì‹¤ì œë¡œ ìš°ë¦¬ê°€ ëª¨ë¸ì„ ì ìš©ì„ í•œë‹¤ëŠ” ê²ƒì€ ì˜ˆë¥¼ ë“¤ì–´ ê¸°ì—…ì—ì„œ ìƒìš©í™”ë¥¼ í•œë‹¤ëŠ” ê°€ì •ì´ë¼ëŠ” ì˜ë¯¸ì´ë‹¤. testì •ë³´ê°€ Trainê³¼ validation ë°ì´í„°ì— ì—†ì–´ì•¼ í•œë‹¤ëŠ” ì ì€ ì‰½ê²Œ ë¹„ìœ í•˜ë©´ ì‹œí—˜ì„ ë³´ëŠ”ë° ìš°ë¦¬ê°€ ì´ë¯¸ í•™ìŠµí•œ ë‚´ìš©(training data)ì´ ì‹œí—˜ì— ë˜‘ê°™ì´ ë‚˜ì˜¨ë‹¤ë©´ ì‹œí—˜ì„ ì˜ ë³¼ í™•ë¥ ì´ ë†’ì•„ì§€ê¸° ë•Œë¬¸ì´ë‹¤. ìš°ë¦¬ê°€ dataë¥¼ ë‚˜ëˆ„ì—ˆë˜ ì´ìœ ëŠ” training dataë¥¼ í†µí•´ í•™ìŠµëœ ì•Œê³ ë¦¬ì¦˜ ëª¨í˜•ì´ í•™ìŠµí•˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œë„ ì˜ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•œ ì‘ì—…ì´ì—ˆë‹¤ëŠ” ê²ƒì„ ìŠì§€ ë§ì•„ë¼! ë˜í•œ, validation dataëŠ” parameterë“¤ì„ ì¡°ì ˆí•˜ë©´ì„œ ìµœì ì˜ ëª¨í˜•ì„ ì„ íƒí•˜ê¸° ìœ„í•œ ë°ì´í„°ì…‹ì´ë¼ê³  ìƒê°í•˜ë©´ëœë‹¤. ë°ì´í„°ê°€ ì˜ ë‚˜ëˆ„ì–´ì¡ŒëŠ”ì§€ ì–´ëŠ í•œ ìª½ìœ¼ë¡œ ì¹˜ìš°ì³ì ¸ ìˆëŠ”ì§€ í™•ì¸í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ìœ„ì˜ ì˜¤ë¥¸ìª½ ê·¸ë˜í”„ì²˜ëŸ¼ í™•ì¸ í•  ìˆ˜ë„ ìˆì„ ê²ƒ ê°™ë‹¤! ë¬´ì¡°ê±´ ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ k-hold cross validationìœ¼ë¡œ training data setê³¼ validation setì„ ë‚˜ëˆ ì„œ ì§„í–‰í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆê³  ë°ì´í„°ì˜ ì„±ê²©ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì„¤ê³„ë¥¼ í•´ì•¼í•œë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ìœ„ì˜ ë°˜ë„ì²´ ë‘ê»˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œì— ìˆì–´ì„œëŠ” ë°˜ë„ì²´ ë‘ê»˜ê°€ ê° ê°œì²´ê°€ ë§Œë“¤ì–´ì§€ëŠ” ìˆœì„œì— ì˜í•´ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ì¦‰ Y2ëŠ” Y1ì— ì˜í•´ ì˜í–¥ì„ ë°›ê³  Y3ëŠ” Y1,Y2ì— ì˜í•´ ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì´ëŸ¬í•œ ë°ì´í„°ì˜ ê²½ìš°ì—ëŠ” ë¬´ì‘ì • k-hold cross validationìœ¼ë¡œ ë‚˜ëˆ„ì–´ì£¼ë©´ ì•ˆëœë‹¤. Y2ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë° Y3ì— ê´€í•œ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. ë˜ ë‹¤ë¥¸ ì˜ˆë¡œëŠ” Imbalanced dataì— ëŒ€í•´ ë§í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. Imbalanced dataëŠ” target variableì´ ë§ ê·¸ëŒ€ë¡œ ë¶ˆê· í˜•í•œ ë°ì´í„°ë¥¼ ì˜ë¯¸í•œë‹¤. ì¡°ê¸ˆì´ë¼ë„ ë¶ˆê· í˜•í•˜ë©´ Imbalanced datasëƒëŠ” ì˜ë¬¸ì´ ë“¤ê² ì§€ë§Œ ê·¸ëŸ°ì˜ë¯¸ê°€ ì•„ë‹ˆë¼ ì˜ˆë¥¼ë“¤ë©´ ë³´í—˜íšŒì‚¬ì˜ ë³´í—˜ì‚¬ê¸°ë¼ë˜ì§€, ê¸ˆìœµì‚¬ê¸° ê°™ì´ ì „ì²´ ë°ì´í„°ì—ì„œ target variableì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ 10% ì •ë„ë¡œ í¬ë°•í•˜ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ë°ì´í„°ë¥¼ ì˜ë¯¸í•œë‹¤. ì´ëŸ° ë°ì´í„°ì—ì„œëŠ” ë¨¼ì € ì²˜ìŒì˜ ë¹„ìœ¨ëŒ€ë¡œ trainê³¼ validation setìœ¼ë¡œ ë‚˜ëˆ„ì–´ì¤€ë‹¤. ê·¸ ë‹¤ìŒ train dataì—ì„œë§Œ resamplingì„ í•˜ì—¬ modelì„ í•™ìŠµì‹œí‚¨í›„ì— ì¶”í›„ì— validation dataë¡œ ì˜ˆì¸¡í•´ë³´ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ê·¸ëƒ¥ k-holdê°€ ì•„ë‹Œ stratified k-Foldë¥¼ í†µí•´ ë°ì´í„°ë¥¼ trainê³¼ validation setìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì£¼ì–´ì•¼í•œë‹¤. ëª¨í˜• í•™ìŠµ ë° ì„ íƒ ê³¼ì í•©(Overfitting) ìœ„ì˜ í¸í–¥ê³¼ ë¶„ì‚°ì˜ íŠ¸ë ˆì´ë“œ ì˜¤í”„ ê´€ê³„ì— ì˜í•´ ìš°ë¦¬ëŠ” ë‘˜ ì¤‘ í•˜ë‚˜ë¥¼ ì¢€ë” ìƒê°í•´ì•¼ë§Œí•˜ëŠ” ìƒí™©ì— ë†“ì´ê²Œë  ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ìœ„ì˜ 4ê°€ì§€ ê·¸ë˜í”„ ì¤‘ ì–´ë–¤ ëª¨í˜•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ì€ ê²ƒì¸ê°€? ë‚˜ì˜ ê°œì¸ì ì¸ ìƒê°ì€ ëª¨í˜•ì˜ ë¶„ì‚°ì€ ì ê³  í¸í–¥ì´ ë†’ì€ ëª¨í˜• ê°™ì€ ê²½ìš°ëŠ” ì¹˜ìš°ì³ ìˆëŠ” ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„±ëŠ¥ë§Œ ë†’ê³  ì—¬ëŸ¬ ë‹¤ì–‘í•œ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ì„±ëŠ¥ì€ ë‚®ì•„ì§€ê¸° ë•Œë¬¸ì— ê¶ê·¹ì ìœ¼ë¡œ ëª©í‘œí•´ì•¼í•  ëª¨í˜•ì€ ë¶„ì‚°ì€ ë†’ì§€ë§Œ í¸í–¥ì´ ë‚®ì€ ëª¨í˜•ì´ë‹¤.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"data engineering (DBì— table ë§Œë“¤ê¸°)","slug":"data_engineering_05","date":"2019-12-17T08:30:11.000Z","updated":"2020-02-21T16:14:12.176Z","comments":true,"path":"2019/12/17/data_engineering_05/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/17/data_engineering_05/","excerpt":"","text":"Spotifyê°€ êµ­ë‚´ì— ìŒì› ì§„ì¶œì„ í™•ì •ì§€ì—ˆë‹¤ëŠ” ê¸°ì‚¬ë¥¼ ë³´ë©´ì„œ ë‹¤ì‹œ í•œë²ˆ ì´ í† ì´í”„ë¡œì íŠ¸ì— ëŒ€í•´ ë™ê¸°ë¶€ì—¬ê°€ ë˜ì—ˆë‹¤. Spotify APIë¥¼ í†µí•´ AWSì— ë§Œë“¤ì–´ ë†“ì€ DBì— ì…ë ¥í•´ ë³¼ ê²ƒì´ë‹¤. Spotify APIë¥¼ ì´ìš©í•´ì„œ DB êµ¬ì¶•í•˜ê¸° ë¨¼ì €, í˜¹ì‹œë¼ë„ í•„ìì˜ í† ì´ í”„ë¡œì íŠ¸ì˜ ëª©í‘œê°€ ë¬´ì—‡ì¸ì§€ ëª¨ë¥´ì‹¤ ë¶„ë“¤ì„ ìœ„í•´ ë§í•˜ìë©´, Spotify dataë¥¼ í†µí•´ì„œ ìì‹ ì˜ ì·¨í–¥ì„ ì…ë ¥í•˜ë©´ ê·¸ ì·¨í–¥ê³¼ ë¹„ìŠ·í•œ ì¥ë¥´ì˜ ìŒì•…ê³¼ ë®¤ì§€ì…˜ì„ ì¶”ì²œí•´ì£¼ëŠ” chat botì„ Facebook APIë¥¼ í†µí•´ì„œ ë§Œë“¤ê³ ì í•œë‹¤. ìœ„ì™€ ê°™ì€ ì„œë¹„ìŠ¤ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € í•„ìš”í•œ ë°ì´í„°ë¥¼ ì–»ê¸° ìœ„í•´ Spotify APIì˜ responseë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” ê°’ë“¤ì´ ë¬´ì—‡ì´ ìˆê³ , ì–´ë–¤ methodë¥¼ í†µí•´ ì–´ë–¤ ë°ì´í„°ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ ì •ë¦¬ë¥¼ í•´ ë³´ê³  ê°„ë‹¨í•œ ERDë¥¼ í†µí•´ ë„ì‹í™” í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. Spotify developerë¥¼ í´ë¦­í•´ì„œ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ DOCS íƒ­ì—ì„œ WEB API referenceë¥¼ í´ë¦­í•˜ë©´ ì—¬ëŸ¬ í•­ëª©ë“¤ì´ ë‚˜ì˜¬ ê²ƒì´ë‹¤. ì—¬ëŸ¬ íƒ­ë“¤ ì¤‘ Artistì™€ ê´€ë ¨ëœ ì‚¬í•­ë“¤ê³¼ trackì— ë”°ë¥¸ Audio featureë“¤ì„ í•„ìš”ë¡œ í•˜ê¸° ë•Œë¬¸ì— Artists íƒ­ê³¼ Tracks íƒ­ì„ ì‚´í´ë³´ë©´ ë  ê²ƒì´ë‹¤. í™•ì¸í•´ ë³¸ ê²°ê³¼ í•„ìš”í•œ ë¶€ë¶„ë“¤ì„ ì •í•´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ERDë¥¼ ë„ì‹í™” í•  ìˆ˜ ìˆë‹¤. Get an Artist íƒ­ì—ì„œ ì–»ì„ ìˆ˜ ìˆëŠ” í…Œì´ë¸”ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. íŠ¹íˆ genreëŠ” ê°€ì ¸ì˜¬ë•ŒëŠ” pythonì˜ listí˜•ì‹ìœ¼ë¡œ ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ ì‚¬ìš©í•  RDBì— ì €ì¥í•  ë•ŒëŠ” listë¡œ ì €ì¥í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë˜ ë‹¤ë¥¸ í…Œì´ë¸”ì„ ë§Œë“¤ì–´ ì£¼ì–´ í•˜ë‚˜ì˜ idì— ì—¬ëŸ¬ê°€ì§€ genre ê°’ì„ ê°–ì„ ìˆ˜ ìˆê²Œë” ì‚¬ìš©í•  ê²ƒì´ë‹¤. artistë¼ëŠ” í…Œì´ë¸”ì€ â€˜idâ€™, â€˜Nameâ€™, â€˜Followersâ€™, â€˜Popularityâ€™, â€˜External_Urlsâ€™(í•´ë‹¹ ê°€ìˆ˜ í˜ì´ì§€ì˜ url)ì„ ë„£ì–´ ì¤„ ê²ƒì´ë‹¤. ê·¸ ì¤‘ â€˜idâ€™ëŠ” Not NuLLê³¼ Uniqueí•œ ê²ƒì„ ë™ì‹œì— ë§Œì¡±ì‹œí‚¤ëŠ” Primary Keyë¡œ ì§€ì •í•  ê²ƒì´ë‹¤. ì™œëƒí•˜ë©´ ê³ ìœ í•œ ì‹ë³„ìì´ë©°, ì´ë¥¼ í†µí•´ ë‹¤ë¥¸ í…Œì´ë¸”ë“¤ê³¼ì˜ joiní•˜ëŠ” ê²ƒì´ë‚˜ ê²€ìƒ‰ì— ìˆì–´ì„œ í›¨ì”¬ ë¹ ë¥¸ ê²€ìƒ‰ì„ í•  ìˆ˜ ìˆê²Œ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. artist genres í…Œì´ë¸”ì€ â€˜Artist_Idâ€™, â€˜Genreâ€™ ê°’ì„ ê°–ê²Œ í•  ê²ƒì´ë©°, ì—¬ê¸°ì„œëŠ” Artist_Idê°€ Foreign Key ì—­í• ì„ í•  ê²ƒì´ë‹¤. Get an Artist&#39;s Top Tracksì—ì„œëŠ” top_tracks í…Œì´ë¸”ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. â€˜idâ€™(track id), â€˜Artist IDâ€™, â€˜Nameâ€™, â€˜Popularityâ€™(spotifyì—ì„œ ê³„ì‚°í•œ íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ ì‚°ì¶œëœ 0~100ì‚¬ì´ì˜ score), â€˜URLâ€™, â€˜image_urlâ€™ì„ ë„£ì–´ ì¤„ ê²ƒì´ë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ìœ ì¼í•˜ê²Œ Tracksíƒ­ì—ì„œ Get Audio Features for Several Tracksë¥¼ í†µí•´ audio featureì¸ â€˜track_idâ€™, â€˜Keyâ€™, â€˜Modeâ€™, â€˜Acousticnessâ€™, â€˜Danceabilityâ€™, â€˜Energyâ€™, â€˜Instrumentalnessâ€™, â€˜Livenessâ€™, â€˜Loudnessâ€™, â€˜Speechinessâ€™, â€˜Valenceâ€™, â€˜Tempoâ€™ë¥¼ ë„£ì–´ ì¤„ ê²ƒì´ë‹¤. â€˜Keyâ€™ : ìŒì˜ ë†’ë‚®ì´ë¥¼ ì˜ë¯¸í•˜ë©°, ê°ê°ì— í•´ë‹¹í•˜ëŠ” ìŒì˜ ë†’ë‚®ì´ë¥¼ ì •ìˆ˜ì— mappingí–ˆë‹¤. E.g. 0 = C, 1 = Câ™¯/Dâ™­, 2 = Dë“± â€˜Modeâ€™ : ì¥ì¡°(ë„ë ˆë¯¸íŒŒì†”ë¼ì‹œ\b)ë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤ë©´ 1, ë‹¨ì¡°(ë¼ì‹œë„ë ˆë¯¸íŒŒì†”)ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤ë©´ 0ì„ ê°–ëŠ”ë‹¤. â€˜Acousticnessâ€™ : Acousticì ì¸ íŠ¸ë™ì¸ì§€ë¥¼ ë§í•´ì£¼ëŠ” ì§€í‘œë¡œì„œ, 0~1ê°’ì„ ê°–ê³ , í•´ë‹¹ íŠ¸ë™ì´ Acousticì ì¸ ìŒì•…ì¼ìˆ˜ë¡ 1ê°’ì„ ê°–ëŠ”ë‹¤. â€˜Danceabilityâ€™ : ìŒì•…ì  ìš”ì†Œì¸ í…œí¬, ë¦¬ë“¬ì˜ ì•ˆì •ì„±ë“±ì˜ ì¡°í•©ì„ ê¸°ë°˜ìœ¼ë¡œí•˜ì—¬ ì¶¤ì— ì í•©í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì´ë‹¤. 0~1ì‚¬ì´ì˜ ê°’ì„ ê°–ìœ¼ë©°, ì¶¤ì— ì í•©í•œ íŠ¸ë™ì¼ìˆ˜ë¡ 1ì˜ ê°’ì„ ê°–ëŠ”ë‹¤. â€˜Energyâ€™ : í™œë™ì ì´ê³  ê¸´ì¥ì„ ì¤„ ìˆ˜ ìˆëŠ” íŠ¸ë™ì¸ì§€ì— ëŒ€í•œ ì§€í‘œë¡œì„œ 0~1ì‚¬ì´ì˜ ê°’ì„ ê°–ëŠ”ë‹¤. â€˜Instrumentalnessâ€™ : íŠ¸ë™ì´ ëª©ì†Œë¦¬ê°€ ì£¼ì¸ì§€ë¥¼ íŒŒì•…í•˜ëŠ” ì§€í‘œë¡œì„œ, ì—¬ê¸°ì„œ ë§í•˜ëŠ” ëª©ì†Œë¦¬ë€, ì¶”ì„ìƒˆì ì¸ ë¶€ë¶„ë“¤ì„ ì•…ê¸°ì ì¸ ìš”ì†Œë¡œ ë³´ê³  ì´ëŸ° ì•…ê¸°ì ì¸ ìš”ì†Œê°€ ë§ì„ìˆ˜ë¡ 1ì— ê°€ê¹ìš´ ê°’ì„ ê°–ëŠ”ë‹¤. â€˜Livenessâ€™ : í•´ë‹¹ íŠ¸ë™ì´ liveìŒì›ì¸ì§€ë¥¼ íŒŒì•…í•˜ëŠ” ìš”ì†Œë¡œì„œ, 0.8ì„ ë„˜ëŠ”ë‹¤ë©´ live íŠ¸ë™ì¼ í™•ë¥ ì´ ë†’ë‹¤. â€˜Loudnessâ€™ : íŠ¸ë™ ì „ë°˜ì ì¸ í‰ê·  ë°ì‹œë²¨(dB)ë¡œì„œ, ìƒëŒ€ì ìœ¼ë¡œ ë‹¤ë¥¸ íŠ¸ë™ë“¤ê³¼ ë¹„êµí•  ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ì¸ ê°’ì€ -60ì—ì„œ 0 db ì‚¬ì´ì´ë‹¤. â€˜Speechinessâ€™ : í† í¬ì‡¼, ì˜¤ë””ì˜¤ë¶, ì‹œê°™ì´ êµ¬ì–´ì²´ ë‹¨ì–´ë“¤ì˜ ì¡´ì¬ë¥¼ íƒì§€í•˜ëŠ” ì§€í‘œë¡œì„œ 0.66 ê°’ ì´ìƒì€ ì•„ë§ˆë„ ì™„ì „íˆ êµ¬ì–´ì²´ë¡œ ë§Œë“¤ì–´ì§„ íŠ¸ë™ì„ ì˜ë¯¸í•œë‹¤ê³  í•˜ë©°, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ êµ¬ì–´ì²´ë¥¼ ë§ì´ ê°€ì§€ê³  ìˆë‹¤. â€˜Valenceâ€™ : íŠ¸ë™ì˜ ê°ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œë¡œì„œ, 0~1ì‚¬ì´ì˜ ê°’ì„ ê°–ëŠ”ë‹¤. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë°ê³  ê¸ì •ì ì¸ ëŠë‚Œì˜ íŠ¸ë™ì„ ì˜ë¯¸í•œë‹¤. â€˜Tempoâ€™ : ì „ë°˜ì ì¸ íŠ¸ë™ì˜ BPMì„ ì¶”ì •í•˜ëŠ” ì§€í‘œì´ë‹¤. DB Table ìƒì„± Python Script íŒŒì¼ì„ ì‘ì„±í•˜ê¸°ì— ì•ì„œ, ë¨¼ì € SQLì— ì ‘ì†í•œ í›„ ë°ì´í„°ë¥¼ insertí•  ê²½ìš° ì–´ë–¤ êµ¬ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ì•¼ ì í•©í• ì§€ì— ëŒ€í•´ ì•Œì•„ ë³¼ ê²ƒì´ë‹¤. 123456789101112## artistsì— ê´€í•œ table# ì´ëª¨ì§€ê¹Œì§€ ì»¤ë²„ í•˜ê³  ì‹¶ì€ ê²½ìš°mysql&gt; create table artists (id VARCHAR(255), name VARCHAR(255), followers INTEGER, popularity INTEGER, url VARCHAR(255), image_url VARCHAR(255), PRIMARY KEY(id)) ENGINE=InnoDB DEFAULT CHARSET='utf8mb4' COLLATE 'utfmb4_unicode_ci'# ì´ëª¨ì§€ëŠ” ì œì™¸í•˜ê³  ë¬¸ìë§Œ ì»¤ë²„í•˜ëŠ” ê²½ìš°mysql&gt; create table artists (id VARCHAR(255), name VARCHAR(255), followers INTEGER, popularity INTEGER, url VARCHAR(255), image_url VARCHAR(255), PRIMARY KEY(id)) ENGINE=InnoDB DEFAULT CHARSET='utf8'## artist_genresì— ê´€í•œ í…Œì´ë¸”mysql&gt; create table artist_genres (artist_id VARCHAR(255), genre VARCHAR(255)) ENGINE=InnoDB DEFAULT CHARSET='utf8';# ìœ„ì—ì„œ ë§Œë“  artists tableì— ëŒ€í•œ infoë¥¼ ë³´ê³  ì‹¶ì€ ê²½ìš°mysql&gt; show create table artists; ì•ìœ¼ë¡œ artist_genres í…Œì´ë¸”ì— ì–´ë–¤ artistì˜ ì¥ë¥´ê°€ ì¶”ê°€ëœë‹¤ë©´, ë°ì´í„°ë¥¼ ì¶”ê°€í•´ì£¼ì–´ì•¼ í•˜ëŠ”ë° ì§€ì†ì ìœ¼ë¡œ ì¶”ê°€í•´ì£¼ì–´ì•¼í•˜ë¯€ë¡œ ìë™í™”ë¥¼ í•  ê²ƒì´ë‹¤. ì¶”ê°€í•´ ì£¼ëŠ” ê°’ì´ í…Œì´ë¸”ì— ì´ë¯¸ ìˆëŠ” ê°’ì„ ê°–ëŠ” ë°ì´í„°ê°€ ë“¤ì–´ì˜¨ë‹¤ë©´ ë¬´ì˜ë¯¸í•  ê²ƒì´ë‹¤. ì•„ë¬´ëŸ° columnì— ì œì•½ì„ ì£¼ì§€ ì•Šì•˜ê¸°ì— insertë¥¼ í†µí•œ ë°ì´í„° ì¶”ê°€ ë°©ì‹ì€ ë¬´ì˜ë¯¸í•˜ë‹¤. 123456789101112131415161718# ë°ì´í„° ì¶”ê°€mysql&gt; insert into artist_genres (artist_id, genre) values ('1234', 'pop');# í…Œì´ë¸” í™•ì¸mysql&gt; select * from artist_genre;# ë°ì´í„° ì¶”ê°€mysql&gt; insert into artist_genres (artist_id, genre) values ('1234', 'pop');# í…Œì´ë¸” í™•ì¸mysql&gt; select * from artist_genre;# í…Œì´ë¸” ê°’ë§Œ ì‚­ì œmysql&gt; delete from artist_genres;# í…Œì´ë¸” ìì²´ë¥¼ ì‚­ì œ# ì‹¤ë¬´ì—ì„œëŠ” dropì€ ì˜ ì‚¬ìš©í•˜ì§€ ì•Šê³ , Alterë¥¼ ì‚¬ìš©í•œë‹¤.mysql&gt; drop table artist_genres; ì•ì—ì„œ ë§í•œ ì¶”í›„ì— ë°ì´í„° ì…ë ¥ì‹œ ë™ì¼í•œ ë°ì´í„°ë¥¼ ê³„ì† ì¶”ê°€í•˜ëŠ” ë¬¸ì œë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ columnì— unique key ì†ì„±ì„ ì¶”ê°€í•´ ì£¼ì—ˆë‹¤. ì´ì „ì˜ ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ ì¶”ê°€í–ˆì„ ê²½ìš° insert intoêµ¬ë¬¸ì€ ì˜¤ë¥˜ë¥¼ ë°œìƒí•˜ì—¬ì„œ ì¶”ê°€ ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§„ ì•Šì§€ë§Œ, Python scriptê°€ ë„ì¤‘ì— ë©ˆì¶”ê²Œ ë˜ëŠ” ë¬¸ì œê°€ ìƒê¸´ë‹¤ëŠ” ì ì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. 12345678910111213141516# Unique key ì†ì„± ë¶€ì—¬mysql&gt; create table artist_genres (artist_id VARCHAR(255), genre VARCHAR(255), unique key(artist_id, genre)) ENGINE=InnoDB DEFAULT CHARSET='utf8';# ì†ì„± ì¶”ê°€ í™•ì¸mysql&gt; show create table artist_genres;# ë°ì´í„° ì¶”ê°€mysql&gt; insert into artist_genres (artist_id, genre) values ('1234', 'pop');# ë°ì´í„° í™•ì¸mysql&gt; select * from artist_genres;# ë°ì´í„° ì¶”ê°€mysql&gt; insert into artist_genres (artist_id, genre) values ('1234', 'pop');ERROR 1062 (23000): Duplicate entry '1234-pop' for key 'artist_id' insert into êµ¬ë¬¸ ëŒ€ì‹  update set êµ¬ë¬¸ì„ ì‚¬ìš©í•´ë³´ì•˜ë‹¤. ìš°ì„  í‚¤ê°’ì´ ìˆê¸° ë•Œë¬¸ì— ì¤‘ë³µë˜ëŠ” ê°’ì´ ì €ì¥ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. update set êµ¬ë¬¸ì€ NULLë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆì—ˆë‹¤ë©´ ë³€ê²½ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ë¬¸ì œì ì´ ìˆë‹¤. replace into êµ¬ë¬¸ì„ í†µí•´ ê°’ì´ ë³€ê²½ë˜ê¸° í–ˆì§€ë§Œ ì„±ëŠ¥ì ì¸ ì¸¡ë©´ì—ì„œ, ë°ì´í„°ê°€ ë§ì„ë• ë¨¼ì € í‚¤ê°’ì´ ìˆëŠ”ì§€ ì°¾ê³ , í‚¤ ê°’ì´ ì¡´ì¬í•œë‹¤ë©´ ì§€ê¸ˆì²˜ëŸ¼ ê·¸ í–‰ì„ ì§€ìš°ê³  ìƒˆë¡œìš´ í–‰ìœ¼ë¡œ ë°”ê¿”ì¤€ë‹¤. ë˜í•œ, primary keyì™€ auto incrementë¥¼ í†µí•´ ì„¤ì •ë˜ì–´ìˆë˜ í…Œì´ë¸”ì´ë¼ë©´ ì›ë˜ primary keyë¡œ ì¸í•´ ë¶€ì—¬ ë°›ì€ ê°’ì´ ì•„ë‹Œ ìƒˆë¡œìš´ ê°’ì„ ë¶€ì—¬ë°›ê²Œëœë‹¤ëŠ” ë¬¸ì œì ì´ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, auto incrementë¡œ DBì— ì…ë ¥ë˜ì–´ ì§€ëŠ”ëŒ€ë¡œ ë²ˆí˜¸ë¥¼ ë¶€ì—¬í–ˆëŠ”ë°, DBì— ì…ë ¥ëœì§€ ì˜¤ë˜ëœ idì— genreë¥¼ ì¶”ê°€í•˜ë ¤ê³  í•œë‹¤ë©´ ê¸°ì¡´ì˜ í–‰ë²ˆí˜¸ë¥¼ ì§€ìš°ê³  tableì˜ ë§¨ë’¤ì— ìƒˆë¡œ ì…ë ¥ëœë‹¤ëŠ” ê²ƒì´ë‹¤. on duplicate key updateë¥¼ í†µí•´ ë¬¸ì œì ì„ í•´ê²° í•  ìˆ˜ ìˆë‹¤! 1234567891011121314151617181920212223242526272829303132333435363738# ë°ì´í„° ì¶”ê°€mysql&gt; update artist_genres set genre='pop' where artist_id='1234';# columns ì¶”ê°€mysql&gt; alter table artist_genres add column country VARCHAR(255);# update ë˜ëŠ” ì‹œì ì„ ê°–ëŠ” column ì¶”ê°€# ìë™ì ìœ¼ë¡œ ë°ì´í„°ê°€ ì¶”ê°€ ë ë•Œë§ˆë‹¤ ê·¸ ì‹œì ì´ ì €ì¥ë¨mysql&gt; alter table artist_genres add column updated_at timestamp default current_timestamp on update current_timestamp;# ë™ì¼í•œ í‚¤ê°’ì„ ê°–ëŠ” ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•˜ë¯€ë¡œ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚´.mysql&gt; INSERT INTO artist_genres (artist_id, genre, country) VALUES ('1234', 'pop', 'UK');# replace into êµ¬ë¬¸ì„ í†µí•´ ê°’ì´ ë³€ê²½ë˜ê¸° í–ˆì§€ë§Œ ì„±ëŠ¥ì ì¸ ì¸¡ë©´ì—ì„œ, ë°ì´í„°ê°€ ë§ì„ë• ë¨¼ì € í‚¤ê°’ì´ ìˆëŠ”ì§€ ì°¾ê³ , ì¡´ì¬í•œë‹¤ë©´ ì§€ê¸ˆì²˜ëŸ¼ ê·¸ í–‰ì„ ì§€ìš°ê³  ìƒˆë¡œìš´ í–‰ìœ¼ë¡œ ë°”ê¿”ì¤€ë‹¤.mysql&gt; REPLACE INTO artist_genres (artist_id, genre, country) VALUES ('1234', 'pop', 'UK');# ê·¸ëŸ¬ë¯€ë¡œ ìœ„ì˜ ì¡°ê±´ì— ë¶€í•©í•˜ëŠ” í–‰ìœ„ì¸ í‚¤ê°’ì¸ artist_idì™€ genreê°’ì´ ìˆëŠ” í–‰ì„ ì°¾ê¸° ë•Œë¬¸ì— 2rowsê°€ëœë‹¤.Query OK, 2 rows affected (0.28 sec)# ìœ„ì—ì„œ replace into êµ¬ë¬¸ì„ í†µí•´ ê°’ì´ ë³€ê²½ë˜ì—ˆìŒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.mysql&gt; select * from artist_genres;# artist_id, genreê°€ Unique keyì¸ë° ë‘ ì»¬ëŸ¼ì„ ë™ì‹œì— ë™ì¼í•œ ê°’ì„ ê°–ëŠ” rowê°€ ì—†ìœ¼ë¯€ë¡œ ìƒˆë¡œ ì¶”ê°€ í•´ì¤€ë‹¤.mysql&gt; REPLACE INTO artist_genres (artist_id, genre, country) VALUES ('1234', 'rock', 'UK');mysql&gt; select * from artist_genres;# ë§ ê·¸ëŒ€ë¡œ ë™ì¼í•œ í‚¤ê°’ì´ ìˆìœ¼ë©´ ê·¸ëƒ¥ insert into êµ¬ë¬¸ì€ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œì¼°ì§€ë§Œ, insert ignore into êµ¬ë¬¸ì€ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šê³  ë¬´ì‹œí•œë‹¤.# uniqueí•œ keyê°’ì¸ artist_idì™€ genreê°€ ë™ì¼í•œ í–‰ì´ ì´ë¯¸ í…Œì´ë¸”ì— ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤.mysql&gt; insert ignore into artist_genres (artist_id, genre, country) VALUES ('1234', 'rock', 'FR');mysql&gt; select * from artist_genres;# on duplicate key updatemysql&gt; insert into artist_genres (artist_id, genre, country) values ('1234', 'rock', 'FR') on duplicate key update artist_id='1234', genre='rock', country='FR'# ìš°ë¦¬ê°€ ì…ì˜ë¡œ ë„£ì–´ ì£¼ì—ˆë˜ countryëŠ” ì§€ì›Œì¤„ ê²ƒì´ë‹¤.mysql&gt; alter table artist_genres drop column country; ê²°ë¡ ì€ ë‹¤ìŒ êµ¬ë¬¸ì„ queryë¬¸ìœ¼ë¡œ ì‚¬ìš©í•˜ê² ë‹¤ëŠ” ê²ƒì´ë‹¤.1mysql&gt; insert into artist_genres (artist_id, genre, country) values ('1234', 'rock', 'FR') on duplicate key update artist_id='1234', genre='rock', country='FR' create_artist_table.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141import sysimport requestsimport base64import jsonimport loggingimport pymysqlimport csvimport sys, os, argparsedef main(host, user, passwd, db, port, client_id, client_secret): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") sys.exit(1) headers = get_headers(client_id, client_secret) ## Spotify Search API artists = [] with open('artist_list.csv') as f: raw = csv.reader(f) for row in raw: artists.append(row[0]) for a in artists: params = &#123; \"q\": a, \"type\": \"artist\", \"limit\": \"1\" &#125; r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) raw = json.loads(r.text) artist = &#123;&#125; try: artist_raw = raw['artists']['items'][0] if artist_raw['name'] == params['q']: artist.update( &#123; 'id': artist_raw['id'], 'name': artist_raw['name'], 'followers': artist_raw['followers']['total'], 'popularity': artist_raw['popularity'], 'url': artist_raw['external_urls']['spotify'], 'image_url': artist_raw['images'][0]['url'] &#125; ) insert_row(cursor, artist, 'artists') except: logging.error('something worng') continue conn.commit() sys.exit(0) try: r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) except: logging.error(r.text) sys.exit(1) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) if r.status_code != 200: logging.error(r.text) if r.status_code == 429: retry_after = json.loads(r.headers)['Retry-After'] time.sleep(int(retry_after)) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) ## access_token expired elif r.status_code == 401: headers = get_headers(client_id, client_secret) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) else: sys.exit(1)def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" # base64 encode ë° decode í•¨ìˆ˜ëŠ” ëª¨ë‘ byte í˜• ê°ì²´ë¥¼ í•„ìš”ë¡œ í•œë‹¤. # ë¬¸ìì—´ì„ byteë¡œ ê°€ì ¸ ì˜¤ë ¤ë©´, íŒŒì´ì¬ì˜ ë‚´ì¥ ëœ encode í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ìì—´ì„ encodingí•´ì£¼ì–´ì•¼ í•œë‹¤. encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) # r.textê°€ typeì´ stringì´ë¯€ë¡œ jsoní˜•íƒœë¡œ ë§Œë“¤ì–´ ì£¼ê¸° ìœ„í•´ json íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•œë‹¤. access_token = json.loads(r.text)['access_token'] # Spotif Web APIë¥¼ ì ‘ê·¼ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” Access Tokenì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì‚¬ìš©ê°€ëŠ¥í•˜ê³  Authrization í˜ì´ì§€ì— ë‚˜ì™€ìˆë‹¤. headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersdef insert_row(cursor, data, table): placeholders = ', '.join(['%s'] * len(data)) # \"%s, %s, %s, %s, %s, %s\" columns = ', '.join(data.keys()) # \"id, name, follwers, popularity, url, image_url\" key_placeholders = ', '.join(['&#123;0&#125;=%s'.format(k) for k in data.keys()]) # \"id=%s, name=%s, follwers=%s, popularity=%s, url=%s, image_url=%s\" sql = \"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s\" % (table, columns, placeholders, key_placeholders) cursor.execute(sql, list(data.values())*2)if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) ê²°ê³¼ ì´ë¯¸ì§€ python scriptë¥¼ ì§œê³  ì¶”í›„ì— scriptë¥¼ ì‹¤í–‰í•˜ì—¬ ë°”ë¡œ RDSì— ì €ì¥í•˜ê³  tableì´ ì œëŒ€ë¡œ ìƒì„±ë¬ëŠ”ì§€ í™•ì¸í•˜ì˜€ë‹¤. artist genre table artist genre tableì€ ìµœëŒ€ë¡œ 50ê°œë¥¼ ê²€ìƒ‰í• ìˆ˜ ìˆëŠ” Spotify search API ë©”ì„œë“œë¥¼ í†µí•´ batch ë‹¨ìœ„ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ Python scriptë¥¼ êµ¬ì„±í•˜ì˜€ë‹¤. ë¨¼ì € artist tableì— ìˆëŠ” idë“¤ì„ ëª¨ë‘ fetch_allí•œ í›„ì— batch sizeì¸ 50ê°œì”© ë¬¶ì–´ì„œ listë¡œ ë§Œë“¤ì–´ ë‘” í›„ joinì„ í†µí•´ í•œêº¼ë²ˆì— search í•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ì˜€ë‹¤. search API ì‚¬ìš©ë²• 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141import sysimport requestsimport base64import jsonimport loggingimport pymysqlimport csvimport sys, os, argparsedef main(host, user, passwd, db, port, client_id, client_secret): try: # use_unicode=Trueë¥¼ ì¨ì•¼ í•œê¸€ê°™ì€ ê²½ìš°ëŠ” ê¹¨ì§€ì§€ ì•ŠëŠ”ë‹¤. conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") # ë³´í†µ ë¬¸ì œê°€ ì—†ìœ¼ë©´ 0 # ë¬¸ì œê°€ ìˆìœ¼ë©´ 1ì„ ë¦¬í„´í•˜ë„ë¡ ì•ˆì— ìˆ«ìë¥¼ ë„£ì–´ì¤€ë‹¤. sys.exit(1) headers = get_headers(client_id, client_secret) cursor.execute(\"SELECT id FROM artists\") artists = [] # ì»¤ì„œì˜ fetchall() ë©”ì„œë“œëŠ” ëª¨ë“  ë°ì´íƒ€ë¥¼ í•œêº¼ë²ˆì— í´ë¼ì´ì–¸íŠ¸ë¡œ ê°€ì ¸ì˜¬ ë•Œ ì‚¬ìš©ëœë‹¤. # ë˜ë‹¤ë¥¸ fetch ë©”ì„œë“œë¡œì„œ fetchone()ì€ í•œë²ˆ í˜¸ì¶œì— í•˜ë‚˜ì˜ Row ë§Œì„ ê°€ì ¸ì˜¬ ë•Œ ì‚¬ìš©ëœë‹¤. for (id, ) in cursor.fetchall(): artists.append(id) artist_batch = [artists[i: i+50] for i in range(0, len(artists), 50)] artist_genres = [] for i in artist_batch: ids = ','.join(i) URL = \"https://api.spotify.com/v1/artists/?ids=&#123;&#125;\".format(ids) r = requests.get(URL, headers=headers) raw = json.loads(r.text) for artist in raw['artists']: for genre in artist['genres']: artist_genres.append( &#123; 'artist_id': artist['id'], 'genre': genre &#125; ) for data in artist_genres: insert_row(cursor, data, 'artist_genres') # commitì„ í•´ì¤˜ì•¼ recordsë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. conn.commit() cursor.close() sys.exit(0) try: r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) except: logging.error(r.text) sys.exit(1) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) if r.status_code != 200: logging.error(r.text) if r.status_code == 429: retry_after = json.loads(r.headers)['Retry-After'] time.sleep(int(retry_after)) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) ## access_token expired elif r.status_code == 401: headers = get_headers(client_id, client_secret) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) else: sys.exit(1)def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersdef insert_row(cursor, data, table): \"\"\" insert into queryë¬¸ ì‘ì„±ë° ì‹¤í–‰ì„ ì¢€ ë” ê°„í¸í•˜ê²Œ í•˜ê¸° ìœ„í•´ ë§Œë“  í•¨ìˆ˜ \"\"\" placeholders = ', '.join(['%s'] * len(data)) columns = ', '.join(data.keys()) key_placeholders = ', '.join(['&#123;0&#125;=%s'.format(k) for k in data.keys()]) sql = \"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s\" % (table, columns, placeholders, key_placeholders) cursor.execute(sql, list(data.values())*2)if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) ê²°ê³¼ ì´ë¯¸ì§€ python scriptë¥¼ ì§œê³  ì¶”í›„ì— scriptë¥¼ ì‹¤í–‰í•˜ì—¬ ë°”ë¡œ RDSì— ì €ì¥í•˜ê³  tableì´ ì œëŒ€ë¡œ ìƒì„±ë¬ëŠ”ì§€ í™•ì¸í•˜ì˜€ë‹¤. APIë¥¼ í†µí•´ í¬ë¡¤ë§í•œ ë°ì´í„° DBì—ì„œ ë¶„ì„í•˜ê¸° DBë¥¼ í†µí•´ ê°„ë‹¨í•œ ë¶„ì„ì„ ì‹¤í–‰í•´ ë³¼ ê²ƒì´ë‹¤. ë¨¼ì € artists tableì—ì„œ ìƒìœ„ 10ê°œì˜ recordsë¥¼ ë³´ì—¬ì£¼ëŠ” SQLêµ¬ë¬¸ì„ í†µí•´ ë°ì´í„° êµ¬ì„±ì„ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì¶œë ¥ë˜ë©°, ì „ì²´ columnì´ id, name, followers, popularity, image_urlë“¤ì´ ìˆë‹¤. 1SELECT * FROM artists LIMIT 10; artistë‹¹ í‰ê· ì ìœ¼ë¡œ ëª‡ê°œì˜ ì¥ë¥´ë¥¼ ê°–ê³  ìˆëŠ”ì§€ë¥¼ ì‚´í´ë³´ê¸° ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ ê³„ì‚°ì„ í–ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ì „ì²´ artistsëŠ” 488ëª…ì„ searchí•´ì„œ ê°€ì ¸ì™”ê³ , artist_genres tableì—ì„œëŠ” ì „ì²´ artistsì˜ ëª…ìˆ˜ì¸ 489ë¡œ ë‚˜ëˆ„ì–´ì ¸ í‰ê· ì ì¸ artist 1ëª…ë‹¹ ê°–ëŠ” genreëŠ” 6ê°œì¸ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. 123SELECT COUNT(*) FROM artists;SELECT COUNT(*)/489 FROM artist_genres; ê²°ê³¼12345678910111213+----------+| COUNT(*) |+----------+| 488 |+----------+1 row in set (0.01 sec)+--------------+| COUNT(*)/489 |+--------------+| 6.4376 |+--------------+1 row in set (0.01 sec) artist_genres table ì—ì„œ genreì™€ countë¥¼ ë³´ì—¬ì£¼ëŠ”ë°, genreë¡œ GROUPINGì„ í•˜ê³  ê·¸ì— ë”°ë¥¸ ê°¯ìˆ˜ì— ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ë³´ì—¬ë‹¬ë¼ê³  Queryë¬¸ì„ ì‘ì„±í•˜ì—¬ ì‚´í´ ë³´ì•˜ë”ë‹ˆ, ê°€ì¥ ë§ì€ genreëŠ” rockì´ë©°, ê·¸ ë‹¤ìŒì€ classic rockì´ë‹¤. 1234SELECT genre, COUNT(*) AS count_num FROM artist_genres GROUP BY genre ORDER BY count_num DESC LIMIT 20;# ë™ì¼í•œ ê²°ê³¼ë¥¼ ì¶œë ¥# SELECT genre, COUNT(*) FROM artist_genres GROUP BY 1 ORDER BY 2 DESC LIMIT 20; ê²°ê³¼12345678910111213141516171819202122232425+-------------------+-----------+| genre | count_num |+-------------------+-----------+| rock | 155 || classic rock | 93 || album rock | 90 || mellow gold | 87 || folk rock | 70 || soft rock | 70 || art rock | 66 || singer-songwriter | 61 || adult standards | 51 || dance rock | 50 || hard rock | 50 || blues rock | 49 || soul | 45 || new wave pop | 44 || roots rock | 43 || permanent wave | 40 || folk | 37 || psychedelic rock | 37 || pop rock | 35 || pop | 35 |+-------------------+-----------+20 rows in set (0.16 sec) ê°€ì¥ popularityê°€ ë†’ì€ ê°€ìˆ˜ëŠ” ì•„ë˜ì™€ ê°™ì´ Drakeì´ë‹¤. ê·¸ ë‹¤ìŒì€ Ariana Grande, Taylor Swift ìˆœì´ë‹¤. 1SELECT popularity, name FROM artists ORDER BY 1 DESC LIMIT 20; ê²°ê³¼12345678910111213141516171819202122232425+------------+-----------------------+| popularity | name |+------------+-----------------------+| 98 | Drake || 97 | Ariana Grande || 95 | Taylor Swift || 95 | Justin Bieber || 95 | Kanye West || 94 | Eminem || 93 | Maroon 5 || 93 | Nicki Minaj || 92 | Queen || 92 | Sam Smith || 92 | Imagine Dragons || 92 | Rihanna || 91 | Kendrick Lamar || 91 | The Beatles || 91 | Lil Wayne || 89 | Lana Del Rey || 88 | Frank Sinatra || 88 | Katy Perry || 87 | Red Hot Chili Peppers || 87 | Snoop Dogg |+------------+-----------------------+20 rows in set (0.01 sec) ì´ë²ˆì—ëŠ” artist tableì—ì„œ followersì™€ name ì¤‘ followersê°€ ë§ì€ ìˆœìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœí•˜ì—¬ ìƒìœ„ 20ê°œì˜ recordsë§Œ ë³¼ ê²ƒì´ë‹¤. ì´ ë˜í•œ, Drakeê°€ ë¶€ë™ì˜ 1ìœ„ì´ë‹¤. 1SELECT followers, name FROM artists ORDER BY 1 DESC LIMIT 20; ê²°ê³¼12345678910111213141516171819202122232425+-----------+-----------------------+| followers | name |+-----------+-----------------------+| 42943467 | Drake || 39573173 | Ariana Grande || 35366176 | Rihanna || 31737977 | Justin Bieber || 31374657 | Eminem || 25029508 | Taylor Swift || 23906032 | Imagine Dragons || 22042155 | Queen || 21244706 | Maroon 5 || 16648595 | Nicki Minaj || 15813453 | Demi Lovato || 14486822 | The Beatles || 14319668 | Katy Perry || 13656683 | Kendrick Lamar || 12980433 | Michael Jackson || 12587845 | Metallica || 12205939 | Red Hot Chili Peppers || 11002859 | Pink Floyd || 10840912 | Kanye West || 10800211 | Sam Smith |+-----------+-----------------------+20 rows in set (0.02 sec) ì´ë²ˆì—ëŠ” artists tableì—ì„œ popularityê°€ 80 ì´ˆê³¼ì¸ idë¥¼ ê°–ëŠ” artistë“¤ì˜ ì¥ë¥´ë“¤ì´ ë¬´ì—‡ì¸ì§€ ì‚´í´ ë³´ì•˜ë‹¤. ê·¸ ì¤‘ popì´ 17ê±´ìœ¼ë¡œ ì œì¼ ë§ì•˜ê³ , ê·¸ ë‹¤ìŒì€ rockì´ì—ˆë‹¤. ì´ ë¶€ë¶„ì€ í•„ìì—ê²ŒëŠ” ì•½ê°„ì˜ ì˜ì™¸ì˜€ë‹¤. ë¬¼ë¡  ì•½ 500ëª…ì˜ artistë§Œì„ ê°€ì ¸ì™”ê¸°ì— ê°ê´€ì ì´ì§€ëŠ” ëª»í•˜ê² ì§€ë§Œ, í•„ìê°€ ê°œì¸ì ìœ¼ë¡œ rockìŒì•…ì„ ì•ˆë“¤ì–´ì„œ ì¸ì§€ ì•„ì§ê¹Œì§€ rockë„ ì¸ê¸°ê°€ ìˆëŠ” ê²ƒìœ¼ë¡œ ë¯¸ë£¨ì–´ë³´ì•„ ë“£ëŠ” ë¶„ë“¤ì´ ë§ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œê²Œ ë˜ì—ˆë‹¤. 1SELECT genre, COUNT(*) FROM artist_genres tb1 JOIN artists tb2 ON tb1.artist_id = tb2.id WHERE tb2.popularity &gt; 80 GROUP BY 1 ORDER BY 2 DESC LIMIT 20 ê²°ê³¼12345678910111213141516171819202122232425+------------------+----------+| genre | COUNT(*) |+------------------+----------+| pop | 17 || rock | 16 || rap | 9 || dance pop | 9 || post-teen pop | 7 || hip hop | 7 || permanent wave | 7 || album rock | 6 || classic rock | 6 || adult standards | 5 || pop rap | 5 || modern rock | 4 || neo mellow | 4 || hard rock | 3 || alternative rock | 3 || g funk | 3 || pop rock | 3 || gangster rap | 3 || post-grunge | 3 || west coast rap | 3 |+------------------+----------+20 rows in set (0.01 sec)","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering (AWSë¡œ DB ë§Œë“¤ê¸°)","slug":"data_engineering_04","date":"2019-12-15T06:16:36.000Z","updated":"2020-02-20T07:34:45.492Z","comments":true,"path":"2019/12/15/data_engineering_04/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/15/data_engineering_04/","excerpt":"","text":"AWS RDB ë§Œë“¤ê¸° Spotify dataë¥¼ í¬ë¡¤ë§ í•˜ê³ ë‚œ í›„ì— AWS RDBì— ì €ì¥í•˜ê¸° ìœ„í•´ì„œ ë¨¼ì € DBë¥¼ ë§Œë“¤ì–´ ì¤„ ê²ƒì´ë‹¤.Youtube ì´ˆë³´ìë¥¼ ìœ„í•œ AWS ì‹œì‘í•˜ê¸°! AWS RDS ìƒì„± ê°œë°œìë‚˜ í•„ì ì²˜ëŸ¼ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ë¶„ë“¤ì„ ì œì™¸í•œ ë¶„ë“¤ì€ ì•„ë§ˆ Amazonì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ë“£ê²Œ ëœë‹¤ë©´, ë¬¼ê±´ì„ ì‚¬ê³ íŒŒëŠ” ë­ ê·¸ëŸ° ì›¹ì‚¬ì´íŠ¸ í˜ì´ì§€ë¥¼ ë– ì˜¬ë¦¬ëŠ” ë¶„ë“¤ì´ ë§ì„ ê²ƒì´ë‹¤. í—ˆë‚˜, Amazon Web Service(AWS)ëŠ” Amazonì˜ ê·¸ëŸ° ì´ë¯¸ì§€ì™€ëŠ” ë‹¤ë¥´ë‹¤. Cloud Serviceë¥¼ ì œê³µí•´ì£¼ëŠ” ê²ƒì´ë‹¤. ìš°ì„  ê°€ì…ì„ í•´ì•¼í•œë‹¤. ì°¸ê³ ë¡œ ëŒ€í•™ìƒì¸ ë¶„ë“¤ì€ AWS educateë¡œ ê°€ì…í•˜ë©´ Creditì„ ë°›ëŠ” ë°©ë²•ì´ ìˆëŠ”ë°, ë­ ê¼­ í˜„ì¬ ì¬í•™ì¤‘ì´ì§€ ì•Šì•„ë„ ìì‹ ì˜ ëŒ€í•™êµ ì´ë©”ì¼ë¡œ ì¸ì¦ì´ ê°€ëŠ¥í•˜ë‹¤ë©´ AWS Educateì— Studentì‹ ë¶„ìœ¼ë¡œ ê°€ì…ì´ ê°€ëŠ¥í•˜ë‹¤. ë§Œì•½ ëŒ€í•™ìƒì´ ì•„ë‹Œ ë¶„ë“¤ì€ ê·¸ëƒ¥ AWS(AWSì™€ AWS educateëŠ” ë‹¤ë¥´ë‹¤.)ë¥¼ ê°€ì…í•´ì„œ ì‚¬ìš©í•˜ë©´ ëœë‹¤. ì°¸ê³ ë¡œ 1ë…„ ë™ì•ˆì€ ì–´ëŠì •ë„ free tierë¥¼ ì£¼ì–´ì„œ ëª‡ëª‡ ì„œë¹„ìŠ¤ë“¤ì€ ë¬´ë£Œë¡œ ì˜¤ë˜ ì´ìš©ê°€ëŠ¥í•  ê²ƒì´ë‹¤. í•„ìë„ ì‘ë…„ ì´ìš©í–ˆì—ˆëŠ”ë° ê¸°ê°„ì´ ë§Œë£Œë˜ì–´ ì´ë²ˆì— ë‹¤ì‹œ ë‹¤ë¥¸ ê³„ì •ì„ ë§Œë“¤ì—ˆëŠ”ë°, ìƒˆë¡œìš´ ê³„ì •ì„ ë§Œë“¤ë©´ ë˜ free tier ì´ìš©ì´ ê°€ëŠ¥í•œ ê²ƒ ê°™ë‹¤.(ê°œì¸ì ì¸ ìƒê°ì´ì§€ë§Œ ì•„ë§ˆë„ ê°€ì…ì‹œì— ì ëŠ” ì‹ ìš©ì¹´ë“œ ë²ˆí˜¸ê°€ ë‹¤ë¥¸ ê²ƒì´ë©´ ê°€ëŠ¥í•œë“¯ í•˜ë‹¤.) ë˜í•œ, ê°€ì…ì‹œì— ì ëŠ” ì‹ ìš©ì¹´ë“œëŠ” ê²°ì œ ì¹´ë“œë¡œ ì„¤ì •ë˜ë©° free tierë¡œ ì´ìš©í•˜ëŠ” ê²ƒì„ ì œì™¸í•œ ë‹¤ë¥¸ ì´ìš©ë£Œë¥¼ ê²°ì œí•  ìˆ˜ ìˆë‹¤. ë¬¼ë¡ , ìë™ê²°ì œëŠ” ì•„ë‹ˆê³  ìì‹ ì´ ê²°ì œí•´ì¤˜ì•¼ í•˜ë©°, ê²°ì œë¥¼ í•´ì£¼ì§€ ì•ŠëŠ”ë‹¤ë©´ íœ´ë©´ ê³„ì •ìœ¼ë¡œ ì „í™˜ì‹œì¼œ ì„œë¹„ìŠ¤ ì´ìš©ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.(ì²˜ìŒ ê²°ì œë˜ëŠ” $1ëŠ” ê²°ì œê°€ ë˜ëŠ” ì¹´ë“œì¸ì§€ í™•ì¸í•˜ëŠ” í™•ì¸ìš©ìœ¼ë¡œ ì•Œê³ ìˆëŠ”ë° ë‚˜ì¤‘ì— ê²°ì œ ì·¨ì†Œ í•´ì£¼ë¯€ë¡œ ê±±ì •í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.) ë‹¤ìŒì˜ ì„œë¹„ìŠ¤ ì¤‘ì— ìš°ì„  RDMì„ ìƒì„±í•´ ì¤„ ê²ƒì´ë‹¤. step 1) Create databaseë¥¼ í´ë¦­, methodì—ì„œ ì§ì ‘ Customizeí•˜ë ¤ë©´ Standardë¥¼ ì²´í¬!(easy ë°©ë²•ì€ ì´ë¯¸ Instancd Sizeì™€ ram ë“± ì‚¬ì–‘ë“¤ì„ AWS Imageì²˜ëŸ¼ ë§Œë“¤ì–´ ë†“ì€ í˜•íƒœë¡œ ë˜ì–´ìˆë‹¤.) ë˜í•œ, Pythonì„ í†µí•´ ì‚¬ìš©í•  ê²ƒì´ë¯€ë¡œ MySQLë¡œ ë§Œë“¤ ê²ƒì´ë‹¤. versionì€ ì œì¼ stableí•œ 5.7.22 versionì„ ì„ íƒ! Templeteì€ Free-Tierë¥¼ ì„ íƒ! ë§Œì•½ ë°”ë¡œ ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•´ì•¼ í•œë‹¤ë©´ Productionì„ ì‚¬ìš©í•˜ë©´ ëœë‹¤. í•„ìëŠ” ì—°ìŠµìš©ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì´ë¯€ë¡œ Freeë¥¼ ì„ íƒ !!! DB instance identifierëŠ” DBì˜ ì´ë¦„ì´ê³ , ê·¸ ì•„ë˜ Credentials Settingsì˜ Master usernameì€ DB ì ‘ì†ì‹œ Master ê¶Œí•œì„ ì¸ì¦í•  IDì™€ passwordì´ë‹¤. ì•„ë˜ë¡œ ë‚´ë ¤ ê°ˆìˆ˜ë¡ ì €ì‚¬ì–‘ DBì´ë©°, ì•„ë§ˆ í•„ìì™€ ë™ì¼í•˜ê²Œ Free-Tierë¥¼ ì„ íƒí–ˆë‹¤ë©´, ì´ë¯¸ ë§¨ ì•„ë˜ ë‹¨ê³„ë¡œ ì„¤ì • ë˜ì–´ìˆì„ ê²ƒì´ë‹¤. Free-Tierì˜ ê²½ìš°ì—ëŠ” ë‹¤ë¥¸ ì‚¬ì–‘ì„ ì„ íƒí•  ìˆ˜ ì—†ë‹¤. Storage type General Purpose : ì£¼ë¡œ ì €ì¥í•  ë•Œ ì‚¬ìš© Provisioned IOPS(Input Output Per Second): ë°ì´í„°ì˜ ì…ì¶œë ¥ì„ ë¹ ë¥´ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆê²Œ í•´ì•¼í•  ê²½ìš° ì‚¬ìš© Storage autoscaling Enable storage autoscalingì€ í• ë‹¹í•œ ìì›ì´ ì´ˆê³¼ë˜ì–´ ë‹¤ë¥¸ ì—¬ìœ ìì›ì´ ìˆë‹¤ë©´, ìë™ìœ¼ë¡œ ì—¬ìœ ìì›ì„ ê°€ì ¸ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œë”í•´ì£¼ëŠ” ì„¤ì •ì´ë‹¤. ì•„ë˜ì— ìˆëŠ” ì„¤ì • ì‚¬í•­ë“¤ì€ ëª¨ë‘ ê¸°ë³¸ê°’ì„ ì„¤ì •í–ˆë‹¤. ì´ì œ ë§¨ ì•„ë˜ë¡œ ê°€ì„œ ìƒì„±ì„ í´ë¦­í•˜ë©´ ëœë‹¤. ì°¸ê³ ) Multi-AZ deployment ì ‘ì†í•˜ëŠ” Userì˜ ì§€ì—­ì— ìƒê´€ì—†ì´ ë™ì¼í•˜ê²Œ Performanceë¥¼ ë‚´ë„ë¡ í•  ë•Œ ì‚¬ìš© ì™¼ìª½ì˜ Database íƒ­ì„ í´ë¦­í•˜ë©´, ë‹¤ìŒê³¼ ê°™ì´ ë³¸ì¸ì˜ DBì— ëŒ€í•œ ì°½ì´ ë‚˜ì˜¬ ê²ƒì´ë‹¤. ì•„ì§ ìƒì„±ì¤‘ì¼ ê²ƒì´ë‹¤. ìš°ì„ , ë³¸ì¸ì˜ DBëª…ì„ í´ë¦­í•˜ì. Connectivity &amp; Security íƒ­ì„ ì‚´í´ë³´ë©´ Public accessibilityê°€ Noë¡œ ë˜ì–´ìˆì„ í…ë° ì´ê²ƒì„ Yesë¡œ ì„¤ì •ì„ ë°”ê¿”ì£¼ì–´ì•¼ ì ‘ì†ì´ ê°€ëŠ¥í•˜ë‹¤. ì•„ë˜ Connectivity &amp; Security íƒ­ì—ì„œ Securityì˜ ë¹¨ê°„ë„¤ëª¨ì¹¸ ë¶€ë¶„ì„ ëˆ„ë¥´ë©´ ì•ìœ¼ë¡œ DBì— ì ‘ì†ì´ ê°€ëŠ¥í•œ í”„ë¡œí† ì½œ ì„¤ì •í•˜ê±°ë‚˜ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” í˜ì´ì§€ë¡œ ì´ë™í•œë‹¤. edit ë²„íŠ¼ì„ ëˆŒëŸ¬ DBì— connect í•  ë•Œ ì ‘ì†ê°€ëŠ¥í•œ í”„ë¡œí† ì½œì„ ì„¤ì •í•´ì¤€ë‹¤. MySQLë¡œ ì ‘ì†ì´ ê°€ëŠ¥í•˜ê²Œë” ì„¤ì •ì„ ì¶”ê°€í•´ì£¼ê³  ì €ì¥í•œë‹¤. ë‹¤ì‹œ Database íƒ­ìœ¼ë¡œ ëŒì•„ê°€ë©´, ì•„ë§ˆë„ ì—¬ëŸ¬ë¶„ì˜ DBê°€ ë§Œë“¤ì–´ì¡Œì„ ê²ƒì´ë‹¤. Command Lineìœ¼ë¡œ DB ì ‘ì†í•˜ê¸°command ë„ì›€ë§ì„ ë¨¼ì € í™•ì¸í•˜ì—¬ ì ‘ì†ì‹œ í•„ìš”í•œ ì˜µì…˜ë“¤ì„ ì•Œì•„ë³´ì.1mysql --help ì ‘ì†ì‹œì— í•„ìš”í•œ ê°„ë‹¨í•œ ì˜µì…˜ë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. -h, â€”host=name =&gt; Connect to host. -p,â€”password[=name] =&gt; Password to use when connecting to server. If password is not given itâ€™s asked from the tty. -P, â€”port=# =&gt; Port number to use for connection or 0 for default to, in order of preference, my.cnf, $MYSQL_TCP_PORT, /etc/services, built-in default (3306). -u, â€”user=name =&gt; User for login if not current user. ì ‘ì†!! -p ì˜µì…˜ê¹Œì§€ë§Œ ì¹˜ë©´ passwordë¥¼ ì…ë ¥í•˜ë¼ê³  í•  í…ë°, ì…ë ¥í•˜ë©´ ì ‘ì†ì´ ëœë‹¤. ì•„ë˜ì—ì„œì˜ end-pointëŠ” DB ìƒì„±í•œ í›„ í•´ë‹¹ DBí˜ì´ì§€ë¥¼ ë³´ë©´ ë‚˜ì™€ìˆë‹¤. í˜¹ì‹œë¼ë„ ì˜ ì´í•´ê°€ ì•ˆê°€ì‹œëŠ” ë¶„ë“¤ì„ ìœ„í•´ ì•„ë˜ í•„ìì˜ ì ‘ì† ì»¤ë§¨ë“œë¥¼ ì˜ˆì‹œë¡œ ë“¤ ê²ƒì´ë‹¤. 12345678910111213141516# mysql -h end-point -P 3306 -u userId -pmysql -h spotify.cgaj5rvtgf25.ap-northeast-2.rds.amazonaws.com -P 3306 -u hb0619 -p# ----- MySQL DB ì ‘ì† í›„ -----------# ìƒì„±ë˜ì–´ ìˆëŠ” DBëª©ë¡ í™•ì¸SHOW TABLES;# Create TableCREATE TABLE people(first_name VARCHAR(20), last_name VARCHAR(20), age INT);# ìƒì„±ëœ DB ì‚¬ìš©USE people# ì ‘ì†ì‹œ ì´ë¯¸ ë§Œë“¤ì–´ì ¸ ìˆëŠ” í…Œì´ë¸”ë¡œ ë°”ë¡œ ì‚¬ìš©ê°€ëŠ¥í•˜ê²Œ ë” í•˜ëŠ” ë°©ë²•# ì´ ë°©ë²•ì€ ìƒì„±ë˜ì–´ ìˆëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— í•œí•´ì„œë§Œ ì‘ë™ ê°€ëŠ¥í•˜ë‹¤.mysql -h spotify.cgaj5rvtgf25.ap-northeast-2.rds.amazonaws.com -P 3306 -D people -u hb0619 -p","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"ì„ í˜• ëŒ€ìˆ˜ ê³µë¶€í•  ë•Œ ë„ì›€ë˜ëŠ” ì‚¬ì´íŠ¸","slug":"linear_algebra_00","date":"2019-12-14T07:24:18.000Z","updated":"2020-02-04T17:54:33.939Z","comments":true,"path":"2019/12/14/linear_algebra_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/14/linear_algebra_00/","excerpt":"","text":"ê¸°ì´ˆ ì„ í˜• ëŒ€ìˆ˜ ê³µë¶€í•  ë•Œ ë„ì›€ë˜ëŠ” ì‚¬ì´íŠ¸ ì•„ë˜ì˜ ì„¤ëª…ë“¤ì€ ì‚¬ì´íŠ¸ë“¤ì— ëŒ€í•œ ì£¼ê´€ì ì¸ ì˜ê²¬ì´ë¯€ë¡œ, ê°œì¸ë§ˆë‹¤ ì°¨ì´ê°€ ìˆì„ ê²ƒì´ë‹¤. ì˜ì–´ê°€(ë„) í¸í•˜ì‹  ë¶„ë“¤ì„ ìœ„í•œ ìë£Œ ì„ í˜•ëŒ€ìˆ˜ cheat sheet ì—„ì²­ ê¸°ë³¸ì ì¸ ê°œë…ë“¤ì„ ëª¨ë¥´ëŠ” ë¶„ë“¤ê»˜ë§Œ ì¶”ì²œ Gilbert Strang êµìˆ˜ë‹˜ ê°•ì˜ ê¸°ë³¸ì ì¸ ê°œë…ë¶€í„° ì°¨ê·¼ì°¨ê·¼ ê·¸ë¦¬ê³  ì¡°ê¸ˆì€ ì§ê´€ì ìœ¼ë¡œ ì„ í˜•ëŒ€ìˆ˜ë¥¼ ê³µë¶€í•˜ê³  ì‹¶ë‹¤ë©´ ì¶”ì²œ CS231n Numpy tutorial ê¸°ë³¸ ê°œë…ì„ ì¡°ê¸ˆì€ ìµíŒ í›„ Pythonì„ í†µí•´ ì‹¤ìŠµí•´ ë³´ê³  ì‹¶ì€ ë¶„ë“¤ê»˜ ì¶”ì²œ í•œêµ­ì–´ê°€ í¸í•˜ì‹  ë¶„ë“¤ì„ ìœ„í•œ ìë£Œ ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ ì„ í˜•ëŒ€ìˆ˜ in edwith ìœ„ì˜ ê°œë…ì„ ìœ„ì£¼ë¡œ í•œ ê°•ì˜ë¥¼ ë“¤ì€ í›„ì— ë³µìŠµì°¨ì›ì—ì„œ ë¹ ë¥´ê²Œ ë“£ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤. ê°œë…ì— ëŒ€í•œ ì„¤ëª…ì´ ë¶€ì¡±í•œ ê°•ì˜ëŠ” ì ˆëŒ€ì ìœ¼ë¡œ ì•„ë‹ˆì§€ë§Œ ìœ„ì˜ ê°œë…ì ì¸ ê°•ì˜ë¥¼ ë“¤ìœ¼ë©´ì„œ ìŠ¤ìŠ¤ë¡œ ë¨¼ì € ìƒê°í•˜ê³  ê³ ë¯¼í•œ ë’¤ì— ì´ ê°•ì˜ë¥¼ ìˆ˜ê°•í•˜ë©´ ë” íš¨ê³¼ì ì¼ ê²ƒ ì´ë‹¤. ë˜í•œ ì¤‘ê°„ ì¤‘ê°„ Pythonì— ì˜í•œ ì‹¤ìŠµë„ ì œê³µí•œë‹¤. ì¹¸ ì•„ì¹´ë°ë¯¸ ê¸°ë³¸ì ì¸ ê°œë…ë¶€í„° ì‹œì‘í•´ì„œ ê·¸ë˜í”„ë¥¼ í†µí•´ ê¸°í•˜í•™ì ì¸ ë¶€ë¶„ì„ ë§ì´ ë³´ì—¬ì£¼ëŠ” ê°•ì˜.(ì˜ì–´ ê°•ì˜ë„ ê°€ëŠ¥) í•œì–‘ëŒ€ ì´ìƒí™” êµìˆ˜ë‹˜ ì„ í˜•ëŒ€ìˆ˜ ê°•ì˜(Kocw) ê°œì¸ì ìœ¼ë¡œ ì´ ê°•ì˜ë¥¼ ì œì¼ ë¨¼ì € ì ‘í•˜ê²Œ ë˜ì—ˆì—ˆê³ , ì¶”í›„ì— ê¸¸ë²„íŠ¸ êµìˆ˜ë‹˜ì˜ ê°•ì˜ì™€ ë¹„êµí•˜ë©´ ê°œë…ì— ëŒ€í•œ ì„¤ëª…ì´ ì¢€ ë” ê³µí•™ì ì´ì§€ë§Œ, ì˜ˆì‹œë¥¼ í†µí•œ ì„¤ëª…ìœ¼ë¡œ ê·¹ë³µì´ ê°€ëŠ¥í•˜ë‹¤. í•„ìëŠ” ë¨¸ë¦¬ê°€ ë›°ì–´ë‚œ í¸ì€ ì•„ë‹ˆì–´ì„œ ì²˜ìŒ ë“¤ì—ˆì„ ë• ì†”ì§íˆ ë…¸íŠ¸ì— ì ìœ¼ë©´ì„œ ê³µë¶€í–ˆì–´ë„ ì´í•´ë¥¼ í•˜ì§€ ëª»í–ˆìœ¼ë‚˜, ë‘ë²ˆì§¸ ë“¤ìœ¼ë©´ì„œ ê° ê°œë…ë“¤ì˜ ì—°ê²°ê³ ë¦¬ë¥¼ ìƒê°í•˜ê³  ì´í•´í•˜ë©° ë“£ê²Œë˜ì–´ í›¨ì”¬ ì¢‹ì•˜ë‹¤.","categories":[{"name":"linear algebra","slug":"linear-algebra","permalink":"https://heung-bae-lee.github.io/categories/linear-algebra/"}],"tags":[]},{"title":"ì¶”ì²œì‹œìŠ¤í…œ(Recommendation System)","slug":"Recommendation_System_00","date":"2019-12-14T02:34:02.000Z","updated":"2019-12-15T05:43:25.915Z","comments":true,"path":"2019/12/14/Recommendation_System_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/14/Recommendation_System_00/","excerpt":"","text":"1) ì¶”ì²œ ì‹œìŠ¤í…œ(Recommendation System)ì´ë€?ìœ„í‚¤ë°±ê³¼ì˜ ì •ì˜ë¥¼ í†µí•´ ë¨¼ì € ì •ë¦¬í•´ë³´ì! ì •ë³´ í•„í„°ë§ (IF) ê¸°ìˆ ì˜ ì¼ì¢… íŠ¹ì • ì‚¬ìš©ìê°€ ê´€ì‹¬ì„ ê°€ì§ˆë§Œí•œ ì •ë³´ (ì˜í™”, ìŒì•…, ì±…, ë‰´ìŠ¤, ì´ë¯¸ì§€, ì›¹ í˜ì´ì§€ ë“±)ë¥¼ ì¶”ì²œí•˜ëŠ” ê²ƒ ã„´.ì¢…ë¥˜(Different Types of Recommendation Engines) :1) í˜‘ì—… í•„í„°ë§ ê¸°ë²•(Collaborative filtering) ê¸°ë³¸ì ì¸ ê°€ì •ì´ ê³¼ê±°ì— ë™ì˜í•œ ì‚¬ëŒë“¤ì´ ë¯¸ë˜ì—ë„ ë™ì˜í•˜ê³  ê·¸ë“¤ì´ ê³¼ê±°ì— ì¢‹ì•„í–ˆë˜ ê²ƒë“¤ì„ ì¢‹ì•„í•  ê²ƒì´ë¼ëŠ” ê°€ì •ì„ ë°”íƒ•ìœ¼ë¡œ ë‹¤ë¥¸ ì‚¬ìš©ìì™€ì˜ ë¹„ìŠ·í•¨ì— ê¸°ì´ˆë¥¼ ë‘ê³  ì‚¬ìš©ìë“¤ì´ ë¬´ì—‡ì„ ì¢‹ì•„í•  ì§€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì— ê¸°ì´ˆì— ë‘ê³  ìˆë‹¤. Linkedin, facebookê³¼ ê°™ì€ SNSëŠ” collaboprative filteringì„ ì¹œêµ¬ ì¶”ì²œ ë“±ì— ì‚¬ìš©í•œë‹¤. ê°€ì¥ í° ì¥ì ì¸ machine analyzable contentì— ì˜ì¡´í•˜ê³  ìˆì§€ ì•Šë‹¤ëŠ” ì ìœ¼ë¡œ ì¸í•´ ì •í™•í•˜ê²Œ item ê·¸ ìì²´ë¥¼ ì´í•´í•˜ì§€ ì•Šê³ ë„ ì˜í™”ì™€ ê°™ì€ ë³µì¡í•œ itemë“¤ì„ ì¶”ì²œ í•  ìˆ˜ ìˆë‹¤. ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ : KNN, Pearson Correlation ëª¨ë¸ì„ ë§Œë“¤ ë•Œ, featureëŠ” Explicití•˜ê±°ë‚˜ Implicití•œ data collection ì‚¬ì´ì—ì„œ ë§Œë“¤ì–´ì§„ë‹¤. Explicit data collectionì˜ ì˜ˆ ì‚¬ìš©ìì—ê²Œ itemì„ í‰ê°€í•˜ê²Œí•˜ê¸°, ê²€ìƒ‰í•˜ê²Œ í•˜ê¸°, ê°€ì¥ ì„ í˜¸í•˜ëŠ” ê²ƒê³¼ ê°€ì¥ ëœ ì„ í˜¸í•˜ëŠ” ê²ƒì„ ìˆœìœ„ë§¤ê¸°ê²Œ í•˜ê¸° ë“± Implicit data collectionì˜ ì˜ˆ ì‚¬ìš©ìê°€ ë³¸ itemì„ ê´€ì°°í•˜ê³  ë¶„ì„í•˜ê¸°, ì‚¬ìš©ìê°€ êµ¬ë§¤í•œ itemì„ ê¸°ë¡í•˜ê¸°, ì‚¬ìš©ìì˜ SNSë¥¼ ë¶„ì„í•˜ê³  ë¹„ìŠ·í•œ likesì™€ dislikeë¥¼ ì°¾ì•„ë‚´ê¸°! 2) ì»¨í…ì¸  ê¸°ë°˜ í•„í„°ë§(Content-based filtering) Keyword(itemì„ ì„¤ëª…(describe)í•˜ëŠ”ë° ì‚¬ìš©í•¨.), Profile(ì‚¬ìš©ìê°€ ì¢‹ì•„í•˜ëŠ” typeì˜ itemì„ ê°€ë¦¬í‚¤ê²Œ(indicate) ë§Œë“¤ì–´ì§.)ì„ í†µí•´ ê³¼ê±°ì— ì‚¬ìš©ììê°€ ì¢‹ì•„í–ˆë˜ ê²ƒë“¤ (ë˜ëŠ” í˜„ì¬ ë³´ê³  ìˆëŠ” ê²ƒë“¤)ê³¼ ë¹„ìŠ·í•œ itemsì„ ì¶”ì²œí•˜ë ¤ê³  í•œë‹¤. ë‹¤ì–‘í•œ í›„ë³´ itemsëŠ” ì‚¬ìš©ìì— ì˜í•´ í‰ê°€ë˜ëŠ”(rated) itemsì™€ ë¹„êµë˜ê³  ê·¸ ì¤‘ best-matching itemsë¥¼ ì¶”ì²œí•œë‹¤. Pandora RadioëŠ” ì²« seedì™€ ê°™ì´ ì‚¬ìš©ìì— ì˜í•´ ì œê³µëœ ë…¸ë˜ì™€ ë¹„ìŠ·í•œ íŠ¹ì§•ì˜ ìŒì•…ì„ ì¬ìƒí•´ ì£¼ëŠ” content-based recommendation Systemì´ë‹¤. ì´ ì ‘ê·¼ë²•ì€ ì§‘í•©ì  ì •ë³´ë¡œë¶€í„° ì›í•˜ëŠ” ë‚´ìš©ì´ë‚˜ ê´€ë ¨ëœ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” Inforamtion retrievalê³¼ í•„ìš”ì—†ëŠ” ì •ë³´ë¥¼ ì œê±°í•˜ëŠ” Information filteringì— ë¿Œë¦¬ë¥¼ ë‘ê³  ìˆë‹¤. Itemsì˜ íŠ¹ì§•(Keyword)ì„ ëŒì–´ë‚´ê¸° ìœ„í•´ TF-IDF(Term frequency-inverse document frequency)ë¥¼ ì‚¬ìš©í•œë‹¤ Userì˜ profilì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ, ê·¸ ì‹œìŠ¤í…œì€ ëŒ€ê²Œ ë‘ê°€ì§€ ì •ë³´ì— ì§‘ì¤‘í•œë‹¤. 1) ì‚¬ìš©ìì˜ ì„ í˜¸ì˜ model 2) ì¶”ì²œì‹œìŠ¤í…œê³¼ ì‚¬ìš©ìì˜ ìƒí˜¸ì‘ìš© ì •ë³´(history) ê¸°ë³¸ì ìœ¼ë¡œ ì´ëŸ° ë°©ë²•ë“¤ì€ ì‹œìŠ¤í…œ ì•ˆì—ì„œ itemì— íŠ¹ì„±ì„ ë¶€ì—¬í•˜ë©´ì„œ item profile(ì´ì‚°ì  featureì™€ attributes)ì„ ì‚¬ìš©í•œë‹¤. ê·¸ ì‹œìŠ¤í…œì€ item íŠ¹ì„±ì˜ weighted vectorì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‚¬ìš©ìì˜ content-based profileì„ ë§Œë“ ë‹¤. WeightsëŠ” ì‚¬ìš©ìì—ê²Œ ê°ê°ì˜ featureì˜ ì¤‘ìš”ë„ë¥¼ ë‚˜íƒ€ë‚´ê³  ê°œë³„ì ìœ¼ë¡œ ì ìˆ˜ ë§¤ê²¨ì§„(rated) content vectorë¡œ ë¶€í„° ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ê³„ì‚°ë  ìˆ˜ ìˆë‹¤. ì‚¬ìš©ìê°€ ì¢‹ì•„í•  ê²ƒ ê°™ í™•ë¥ ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ ë³µì¡í•œ ë°©ë²•ë“¤(ë² ì´ì§€ì•ˆ ë¶„ë¥˜, í´ëŸ¬ìŠ¤í„° ë¶„ì„, ê²°ì •íŠ¸ë¦¬, ê·¸ë¦¬ê³  ì¸ê³µ ì‹ ê²½ë§ ë„¤íŠ¸ì›Œí¬ì™ ê°™ì€ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´ì—, ê°„ë‹¨í•œ ì ‘ê·¼ë²•ë“¤ì€ ê·¸ ì ìˆ˜ ë§¤ê²¨ì§„ item vectorì˜ í‰ê·  ê°’ì„ ì‚¬ìš©í•œë‹¤. ë³´í†µ â€˜ì¢‹ì•„ìš”â€™ì™€ â€˜ì‹«ì–´ìš”â€™ì™€ ê°™ì€ í˜•íƒœë¡œ ì‚¬ìš©ìë¡œë¶€í„° ì§ì ‘ì ì¸ í”¼ë“œë°±ì€ íŠ¹ì •í•œ ì†ì„±(attribute)ì˜ ì¤‘ìš”ë„ì— ëŒ€í•œ ë” ë†’ê±°ë‚˜ ë‚®ì€ weightë¥¼ í• ë‹¹í•˜ëŠ”ë° ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤. Content-based filteringì˜ ì¤‘ìš”í•œ ë¬¸ì œì ì€, í•˜ë‚˜ì˜ content sourceì— ê´€ë ¨ëœ ì‚¬ìš©ìë“¤ í–‰ë™ìœ¼ë¡œë¶€í„° ì‚¬ìš©ì ì„ í˜¸ë„ë¥¼ ë°°ìš¸ ìˆ˜ ìˆê³  ë‹¤ë¥¸ content ì¢…ë¥˜(type)ì— ëŒ€í•´ì„œë„ ë°°ìš´ ì‚¬ìš©ì ì„ í˜¸ë„ë“¤ì„ ì ìš©ì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ì—¬ë¶€ì´ë‹¤. ê·¸ ì‹œìŠ¤í…œì´ ë‹¤ë¥¸ ì„œë¹„ìŠ¤ì˜ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ contentë¥¼ ì¶”ì²œí•  ìˆ˜ ìˆëŠ” ê²ƒë³´ë‹¤ ì‚¬ìš©ìê°€ ì´ë¯¸ ì‚¬ìš©í•œ ê²ƒê³¼ ê°™ì€ ì¢…ë¥˜ì˜ contentë¥¼ ì¶”ì²œí•˜ëŠ” ê²ƒì— í•œì •ë¼ ìˆë‹¤ë©´ í•´ë‹¹ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ê°€ì¹˜ëŠ” ìƒë‹¹íˆ ë‚®ì„ ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, news browsingì— ê¸°ë°˜í•œ ì¶”ì²œ ë‰´ìŠ¤ ê¸°ì‚¬ëŠ” ìœ ìš©í•˜ì§€ë§Œ, news browsingì— ê¸°ë°˜í•´ ì¶”ì²œë  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ì„œë¹„ìŠ¤ì˜ ìŒì•…, ë¹„ë””ì˜¤, ì œí’ˆ í† ë¡ ì—ì„œ ë” ìœ ìš©í•˜ë‹¤. ì°¸ê³ ) TF-IDF ì •ë³´ê²€ìƒ‰ê³¼ í…ìŠ¤íŠ¸ ë§ˆì´ë‹ì—ì„œ ì´ìš©í•˜ëŠ” ê°€ì¤‘ì¹˜ë¡œ, ì—¬ëŸ¬ ë¬¸ì„œë¡œ ì´ë£¨ì–´ì§„ ë¬¸ì„œêµ°ì´ ìˆì„ ë•Œ ì–´ë–¤ ë‹¨ì–´ê°€ íŠ¹ì • ë¬¸ì„œ ë‚´ì—ì„œ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œ ê²ƒì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í†µê³„ì  ìˆ˜ì¹˜ì´ë‹¤. ë¬¸ì„œì˜ í•µì‹¬ì–´ë¥¼ ì¶”ì¶œí•˜ê±°ë‚˜ ê²€ìƒ‰ì—”ì§„ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ì˜ ìˆœìœ„ë¥¼ ê²°ì •í•˜ê±°ë‚˜,ë¬¸ì„œë“¤ ì‚¬ì´ì˜ ë¹„ìŠ·í•œ ì •ë„ë¥¼ êµ¬í•˜ëŠ” ë“±ì˜ ìš©ë„ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. IDF ê°’ì€ ë¬¸ì„œêµ°ì˜ ì„±ê²©ì— ë”°ë¼ ê²°ì •ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´, â€˜ì›ìâ€™ë¼ëŠ” ë‚±ë§ì€ ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì„œë“¤ ì‚¬ì´ì—ì„œëŠ” ì˜ ë‚˜ì˜¤ì§€ ì•Šê¸° ë•Œë¬¸ì— IDF ê°’ì´ ë†’ì•„ì§€ê³  ë¬¸ì„œì˜ í•µì‹¬ì–´ê°€ ë  ìˆ˜ ìˆì§€ë§Œ, ì›ìì— ëŒ€í•œ ë¬¸ì„œë¥¼ ëª¨ì•„ë†“ì€ ë¬¸ì„œêµ°ì˜ ê²½ìš° ì´ ë‚±ë§ì€ ìƒíˆ¬ì–´ê°€ ë˜ì–´ ê° ë¬¸ì„œë“¤ì„ ì„¸ë¶„í™”í•˜ì—¬ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ë‚±ë§ë“¤ì´ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì–»ê²Œ ëœë‹¤. íŠ¹ì • ë¬¸ì„œ ë‚´ì—ì„œ ë‹¨ì–´ ë¹ˆë„ê°€ ë†’ì„ìˆ˜ë¡, ê·¸ë¦¬ê³  ì „ì²´ ë¬¸ì„œë“¤ ì¤‘ ê·¸ ë‹¨ì–´ë¥¼ í¬í•¨í•œ ë¬¸ì„œê°€ ì ì„ìˆ˜ë¡ TF-IDFê°’ì´ ë†’ì•„ì§„ë‹¤. ë”°ë¼ì„œ ì´ ê°’ì„ ì´ìš©í•˜ë©´ ëª¨ë“  ë¬¸ì„œì— í”í•˜ê²Œ ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ë¥¼ ê±¸ëŸ¬ë‚´ëŠ” íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. IDFì˜ ë¡œê·¸ í•¨ìˆ˜ê°’ì€ í•­ìƒ 1ì´ìƒì´ë¯€ë¡œ, IDFê°’ê³¼ TF-IDFê°’ì„ í•­ìƒ 0 ì´ìƒì´ ëœë‹¤. íŠ¹ì • ë‹¨ì–´ë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì„œë“¤ì´ ë§ì„ ìˆ˜ë¡ ë¡œê·¸ í•¨ìˆ˜ ì•ˆì˜ ê°’ì´ 1ì— ê°€ê¹Œì›Œì§€ê²Œ ë˜ê³ , ì´ ê²½ìš° IDFê°’ê³¼ TF-IDê°’ì€ 0ì— ê°€ê¹Œì›Œì§€ê²Œ ëœë‹¤. TF-IDF = TF \\times IDF TF(Term Frequency, ë‹¨ì–´ ë¹ˆë„) íŠ¹ì •í•œ ë‹¨ì–´ê°€ ë¬¸ì„œ ë‚´ì— ì–¼ë§ˆë‚˜ ìì£¼ ë“±ì¥í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ ì‚°ì¶œ ë°©ì‹ ã„´. TF :tf(t, d) = f(t, d) (f(t, d) : ë¬¸ì„œ d ë‚´ì—ì„œ ë‹¨ì–´ tì˜ ì´ ë¹ˆë„) ã„´. Boolean TF :tf(t, d) = tê°€ dì— í•œ ë²ˆì´ë¼ë„ ë‚˜íƒ€ë‚˜ë©´ 1, ì•„ë‹ˆëª… 0 ã„´. log scale TF :tf(t, d) = \\log(f(t, d) +1) ã„´. ì¦ê°€ ë¹ˆë„ TF : ì¼ë°˜ì ìœ¼ë¡œëŠ” ë¬¸ì„œì˜ ê¸¸ì´ê°€ ìƒëŒ€ì ìœ¼ë¡œ ê¸¸ ê²½ìš°, ë‹¨ì–´ ë¹ˆë„ê°’ì„ ì¡°ì •í•˜ê¸° ìœ„í•´ ì‚¬ìš© tf(t, d) = 0.5 + \\frac{0.5 \\times f(t, d)}{max{f(w, d) : w \\in d}} = 0.5 + \\frac{0.5 \\times target ë‹¨ì–´ì— ëŒ€í•œ TF}{ë™ì¼ ë¬¸ì„œ(ë¬¸ì¥)ë‚´ì—ì„œì˜ ìµœë¹ˆë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜} IDF(Inverse Document Frequency, ì—­ë¬¸ì„œ ë¹ˆë„) TF ê°’ì´ ë†’ì„ ìˆ˜ë¡ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°ë  ìˆ˜ë„ ìˆì§€ë§Œ ë‹¨ìˆœíˆ í”í•˜ê²Œ ë“±ì¥í•˜ëŠ” ê²ƒì¼ ìˆ˜ë„ ìˆë‹¤. ì´ê°’ì„ DF(Document Frequency, ë¬¸ì„œ ë¹ˆë„)ë¼ê³ í•œë‹¤. ì˜ì–´ë¬¸ì¥ì—ì„œ ì˜ˆë¥¼ ë“¤ìë©´ ê°€ë ¹ I, you ê°™ì€ ë‹¨ì–´ë“¤ì„ ì˜ˆë¡œ ë“¤ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. DFì˜ ì—­ìˆ˜ë¥¼ IDF(Inverse Document Frequency, ì—­ë¬¸ì„œ ë¹ˆë„)ë¼ê³  í•œë‹¤. í•œ ë‹¨ì–´ê°€ ë¬¸ì„œ ì§‘í•© ì „ì²´ì—ì„œ ì–¼ë§ˆë‚˜ ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ IDF ê°’ì€ ë¬¸ì„œêµ°ì˜ ì„±ê²©ì— ë”°ë¼ ê²°ì •ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´, â€˜ì›ìâ€™ë¼ëŠ” ë‚±ë§ì€ ì¼ë°˜ì ìœ¼ë¡œ ë¬¸ì„œë“¤ ì‚¬ì´ì—ì„œëŠ” ì˜ ë‚˜ì˜¤ì§€ ì•Šê¸° ë•Œë¬¸ì— IDF ê°’ì´ ë†’ì•„ì§€ê³  ë¬¸ì„œì˜ í•µì‹¬ì–´ê°€ ë  ìˆ˜ ìˆì§€ë§Œ, ì›ìì— ëŒ€í•œ ë¬¸ì„œë¥¼ ëª¨ì•„ë†“ì€ ë¬¸ì„œêµ°ì˜ ê²½ìš° ì´ ë‚±ë§ì€ ìƒíˆ¬ì–´ê°€ ë˜ì–´ ê° ë¬¸ì„œë“¤ì„ ì„¸ë¶„í™”í•˜ì—¬ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ë‚±ë§ë“¤ì´ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì–»ê²Œ ëœë‹¤. ì‚°ì¶œ ë°©ì‹ $ \\mid D \\mid$ : ë¬¸ì„œ ì§‘í•© Dì˜ í¬ê¸°, ë˜ëŠ” ì „ì²´ ë¬¸ì„œì˜ ìˆ˜ $ \\mid d \\in D : t \\in d \\mid$ : ë‹¨ì–´ tê°€ í¬í•¨ëœ ë¬¸ì„œì˜ ìˆ˜(ì¦‰, $tf(t,0) \\neq 0$). ë‹¨ì–´ê°€ ì „ì²´ ë§ë­‰ì¹˜(Corpus)ì•ˆì— ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš° ì´ëŠ” ë¶„ëª¨ê°€ 0ì´ ë˜ëŠ” ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¨ë‹¤. ì´ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ $1 + \\mid d \\in D : t \\in d \\mid $ë¡œ ì“°ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. idf(t, D) = \\log \\frac{ \\mid D \\mid}{ \\mid d \\in D : t \\in d \\mid} = \\log \\frac{ì „ì²´ ë¬¸ì„œì˜ ìˆ˜}{í•´ë‹¹ Targetë‹¨ì–´ë¥¼ í¬í•¨í•œ ë¬¸ì„œì˜ ìˆ˜}3) Hybrid Recommendation Systems ìœ„ì˜ 2ê°€ì§€ Recommendation Systemë“¤ì€ ê°ê°ì˜ ì¥ì ê³¼ ë‹¨ì ì´ ì¡´ì¬í•¨ì„ ì‚´í´ë³´ì•˜ë‹¤. ê·¸ë˜ì„œ ìµœê·¼ ì—°êµ¬ëŠ” Collaborative filteringê³¼ content-based filteringì„ ì„ì€ Hybrid ì ‘ê·¼ë²•ì´ ëª‡ëª‡ì˜ ìƒí™©(Cold start(ì¶©ë¶„í•œ ì •ë³´ê°€ ì—†ì–´ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì–»ì§€ ëª»í•˜ëŠ” ê²ƒ)) Sparsity)ì—ì„œ ë” íš¨ê³¼ì ì¼ ìˆ˜ ìˆë‹¤ê³  ì„¤ëª…í•œë‹¤. Hybrid ì¶”ì²œ ì‹œìŠ¤í…œì´ë€ ìš©ì–´ëŠ” ì•„ë˜ì˜ ê° ì‹œìŠ¤í…œë³„ ë‹¨ì ë“¤ì„ ë³´ì™„í•˜ê¸° ìœ„í•´ ë‹¤ì¤‘ì˜ ì¶”ì²œ ê¸°ìˆ ì„ í•¨ê»˜ ì„ëŠ” ì–´ë– í•œ ì¶”ì²œ ì‹œìŠ¤í…œì„ ì˜ë¯¸í•˜ë©° ë‹¤ì¤‘ì˜ ì¶”ì²œ ê¸°ìˆ ì´ ë‚´í¬í•˜ê³  ìˆëŠ” ì˜ë¯¸ëŠ” ë™ì¼í•œ ê¸°ìˆ ì„ ì—¬ëŸ¬ê°œ ê²¹ì¹˜ëŠ” ê²ƒë„ í¬í•¨ëœë‹¤. Collaborative ì´ ì‹œìŠ¤í…œì€ ë‹¤ë¥¸ ì‚¬ìš©ìë“¤ê³¼ itemsì— ëŒ€í•œ profilesì„ í‰ê°€í•˜ëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ë©´ì„œ ì¶”ì²œì„ í•œë‹¤. ì´ ì‹œìŠ¤í…œì€ í˜„ì¬ì˜ ì‚¬ìš©ìë‚˜ itemsê³¼ ë¹„ìŠ·í•œ í‰ê°€ ê¸°ë¡(history)ì™€ í•¨ê»˜ ë¹„ìŠ·í•œ(peer) ì‚¬ìš©ì ë˜ëŠ” itemsì„ ë°°ì¹˜í•˜ê³ , ì´ ê·¼ì ‘ì´ì›ƒ(neighborhood)ë¥¼ ì´ìš”í•´ì„œ ì¶”ì²œì„ ë§Œë“ ë‹¤. ì‚¬ìš©ì ê¸°ë°˜ê³¼ itemê¸°ë°˜ì˜ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ì•Œê³ ë¦¬ì¦˜ì€ cold-startë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•©ì³ì§ˆ ìˆ˜ ìˆê³  ì¶”ì²œ ê²°ê³¼ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤. Content-based ì´ ì‹œìŠ¤í…œì€ ì‚¬ìš©ìê°€ ê·¸ë“¤ì—ê²Œ ì¤€ í‰ê°€ì™€ ì œí’ˆë“¤ê³¼ ê´€ë ¨ëœ íŠ¹ì§•ì´ë¼ëŠ” ë‘ê°€ì§€ Sourcesë¡œë¶€í„° ì¶”ì²œì„ ë§Œë“ ë‹¤. Content-based ì¶”ì²œìëŠ” ì¶”ì²œì„ user-specific ë¶„ë¥˜ ë¬¸ì œì²˜ëŸ¼ ë‹¤ë£¨ê³  ì œí’ˆì˜ íŠ¹ì§•ì— ê¸°ë°˜í•œ ì‚¬ìš©ìì˜ ì¢‹ì•„ìš”ì™€ ì‹«ì–´ìš”ì˜ ë¶„ë¥˜ìë¥¼ í•™ìŠµí•œë‹¤. Demographic Demographic(ì¸êµ¬ í†µê³„í•™ì ) ì¶”ì²œì€ ì‚¬ìš©ìì˜ ì¸êµ¬í†µê³„í•™ì  ì •ë³´(profile)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œì„ ì œê³µí•œë‹¤. ì¶”ì²œëœ ì œí’ˆì€ ê·¸ ì˜ì—­ì˜ ì‚¬ìš©ìë“¤ì˜ í‰ê°€ë“¤ì„ í•©ì¹¨ìœ¼ë¡œì¨ ë‹¤ë¥¸ ì¸êµ¬í†µê³„í•™ì  ì˜ì—­ì„ ìœ„í•´ ë§Œë“¤ì–´ ì§ˆ ìˆ˜ ìˆë‹¤. Knowledge-based ì´ ì¶”ì²œìëŠ” ì‚¬ìš©ìì˜ ì„ í˜¸ì™€ ìš”êµ¬(needs)ì— ëŒ€í•œ ì¶”ë¡ ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì œí’ˆì„ ì œì•ˆí•œë‹¤. ì´ ì§€ì‹(knowledge)ëŠ” ë•Œë•Œë¡œ ì–¼ë§ˆë‚˜ íŠ¹ì •í•œ ì œí’ˆ íŠ¹ì§•ì´ ì‚¬ìš©ìì˜ ìš”êµ¬ë¥¼ ì¶©ì¡±ì‹œí‚¤ëŠ” ì§€ì— ëŒ€í•œ ëšœë ·í•œ ê¸°ëŠ¥ì (functional) ì§€ì‹ì„ í¬í•¨í•œë‹¤. NetflixëŠ” Hybrid ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì¢‹ì€ ì˜ˆì´ë‹¤. ì‚¬ìš©ìê°€ ë†’ê²Œ í‰ê°€í–ˆë˜(Content-based)ì˜í™”ì™€ ë¹„ìŠ·í•œ featureë¥¼ ë„ëŠ” ì˜í™”ë¥¼ ì¶”ì²œí•˜ê³ , ë¹„ìŠ·í•œ ì‚¬ìš©ì(collaborate)ë“¤ì˜ ê²€ìƒ‰ ìŠµê´€ê³¼ ì‹œì²­ì„ ë¹„êµí•¨ìœ¼ë¡œì„œ ì¶”ì²œì„ í•œë‹¤. ì •í™•ë„ë¥¼ ë„˜ì–´ì„œì „í˜•ì ìœ¼ë¡œ, ì¶”ì²œ ì‹œìŠ¤í…œì— ëŒ€í•œ ì—°êµ¬ëŠ” ê°€ì¥ ì •í™•í•œ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜ì„ ì°¾ëŠ” ê²ƒì— ê´€ì‹¬ì„ ë‘”ë‹¤. í•˜ì§€ë§Œ, ë§ì€ ì¤‘ìš”í•œ ìš”ì†Œë“¤ì´ ìˆë‹¤. Diversity ì‚¬ìš©ìë“¤ì€ ìì‹ ì´ ì„ íƒí•œ Itemê³¼ ìœ ì‚¬ì„±ì´ ë†’ì€ intra-listì— í¬í•¨ëœ ë‹¤ë¥¸ ì•„í‹°ìŠ¤ë“¤ì„ ë³´ì´ëŠ” ë‹¤ì–‘ì„±ì„ ê°–ì¶˜ ì¶”ì²œ ì‹œìŠ¤í…œì— ë” ë§Œì¡±í•˜ëŠ” ê²½í–¥ì„ ë³´ì¸ë‹¤. Recommender persistence ì–´ë–¤ ìƒí™©ì—ì„œ, ì¶”ì²œì‹œìŠ¤í…œì´ ê·¸ ì´ì „ì˜ ì¶”ì²œê³¼ ë™ì¼í•œ ì¶”ì²œì„ ë‹¤ì‹œ ë³´ì—¬ì£¼ê±°ë‚˜ ì‚¬ìš©ìê°€ ë‹¤ì‹œ itemsì„ í‰ê°€í•˜ê²Œ í•˜ëŠ” ê²ƒì´ ë” íš¨ê³¼ì ì´ë‹¤. Privacy ì¶”ì²œ ì‹œìŠ¤í…œì€ ëŒ€ê²Œ privacy ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•œë‹¤. ì™œëƒí•˜ë©´ ì‚¬ìš©ìë“¤ì€ ë¯¼ê°í•œ ì •ë³´ë¥¼ ê³µê°œí•´ì•¼í•˜ê¸° ë•Œë¬¸ì´ë‹¤. Collaborative filteringì„ ì‚¬ìš©í•´ ì‚¬ìš©ìì˜ Profilesì„ ë§Œë“œëŠ” ê²ƒì€ privacyì˜ ê´€ì ì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆë‹¤. ë§ì€ ìœ ëŸ½ êµ­ê°€ë“¤ì€ data privacyì— ëŒ€í•œ ê°•í•œ ë¬¸í™”ë¥¼ ê°€ì§€ê³  ìˆê³ , ì‚¬ìš©ìì˜ profileì„ ë§Œë“œëŠ” ì–´ë– í•œ ë‹¨ê³„ë¥¼ ì†Œê°œí•˜ë ¤ëŠ” ì‹œë„ëŠ” ë¶€ì •ì ì¸ ì‚¬ìš©ì ë°˜ì‘ì„ ì´ˆë˜í•  ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ NetflixëŠ” ë°ì´í„° ëŒ€íšŒë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ê³µê°œí–ˆë‹¤ê°€ ë¹„ì‹ë³„í™”ëœ ë°ì´í„°ì™€ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ì—°ê²°í•¨ì„ì¨ ê°œì¸ì„ ì‹ë³„í•  ìˆ˜ ìˆê²Œ ë¨ì„ í™•ì¸í–ˆê³ , ê³ ì†Œê¹Œì§€ ë‹¹í–ˆì—ˆë‹¤. ê·¸ ì´ì™¸ì˜ ì£¼ì˜ì‚¬í•­ ì°¸ê³  ë¬¸í—Œ ë° ì‚¬ì´íŠ¸ ì›¹ ì‚¬ì´íŠ¸ ì”ì¬ë¯¸ ì½”ë”© leebaro blog svdì˜ í™œìš©ì— ê´€í•œ darkpgmrë‹˜ì˜ ë¸”ë¡œê·¸ NMF ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ìœ ì‚¬í•œ ë¬¸ì„œê²€ìƒ‰ê³¼ êµ¬í˜„ Pythonì„ ì´ìš©í•œ í–‰ë ¬ì˜ ë¶„í•´ ì˜ˆì œ naver ê²€ìƒ‰ì—”ì§„ ì¶”ì²œì‹œìŠ¤í…œ airsê°œë°œê¸°(2017 Deview) Movie recommendation system ì˜ˆì‹œ sanghyukchunë‹˜ì˜ github blog(Recommendation System) ì¶”ì²œì‹œìŠ¤í…œì„ ìœ„í•œ Deep learning ë¬¸í—Œ Building Recommendation Engines Recommender Systems in E-CommerceJ. Ben Schafer, Joseph Konstan, John Riedl State of the Art Recommender System. Laurent Candillier, Kris Jack Recommender Systems in E-Commerce. Sanjeevan Sivapalan, Alireza Sadeghian A Survey of e-Commerce Recommender Systems Farida Karimova, PhD Low-Rank Matrix Completion (2013) by Ryan Kennedy Exact Matrix Completion via Convex Optimization Emmanuel J. Cand`es","categories":[{"name":"Recommendation System","slug":"Recommendation-System","permalink":"https://heung-bae-lee.github.io/categories/Recommendation-System/"}],"tags":[]},{"title":"data engineering (APIëŠ” ë¬´ì—‡ì¸ê°€?!?)","slug":"data_engineering_03","date":"2019-12-13T04:25:42.000Z","updated":"2020-02-20T07:34:05.544Z","comments":true,"path":"2019/12/13/data_engineering_03/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/13/data_engineering_03/","excerpt":"","text":"REST APIì˜ ì •ì˜ì™€ ì˜ˆì œë“¤API(Application Programming Interface) ë‘ ê°œì˜ ì‹œìŠ¤í…œì´ ì„œë¡œ ìƒí˜¸ ì‘ìš©í•˜ê¸° ìœ„í•œ ì¸í„°í˜ì´ìŠ¤ ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ëŠ” ì¸í„°í˜ì´ìŠ¤ APIë¼ê³  í•˜ë©´ ë³´í†µ REST APIë¥¼ ì§€ì¹­ ì›¹ì‚¬ì´íŠ¸ëŠ” HTTP(S)í”„ë¡œí† ì½œì„ ì‚¬ìš©í•˜ëŠ” REST API ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶• API ì ‘ê·¼ ê¶Œí•œAuthentication VS Authorization Authentication : Identity(ì •ì²´)ê°€ ë§ë‹¤ëŠ” ì¦ëª… Authorization : APIë¥¼ í†µí•œ ì–´ë– í•œ ì•¡ì…˜ì„ í—ˆìš© APIê°€ Authenticationìœ¼ë¡œ í•˜ì—¬ë„ ì–´ë– í•œ ì•¡ì…˜ì— ëŒ€í•´ì„œëŠ” Authorizationì„ í˜€ìš©í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒAPIì˜ í•„ìˆ˜ëŠ” ì²«ì§¸ë„ ë‘˜ì§¸ë„ Security ì–´ë– í•œ Security ë°©ì•ˆì´ ì—†ì„ ê²½ìš° DELETE requestë¥¼ í†µí•´ì„œ ë‹¤ë¥¸ ì´ìš©ìì˜ ì •ë³´ë¥¼ ì§€ìš¸ ìˆ˜ë„ ìˆìŒ ì œ 3ìì—ê²Œ ë°ì´í„° ìœ ì¶œë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŒ ëˆ„ê°€ APIë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€, ì–´ë–¤ ì •ë³´ë¥¼ ê°€ì ¸ê°€ëŠ”ì§€ íŠ¸ë˜í‚¹ í•  ìˆ˜ê°€ ì—†ìŒ API Keyë€ ë¬´ì—‡ì¸ê°€? API Keyë€ ë³´í†µ Request URLí˜¹ì€ Request í—¤ë”ì— í¬í•¨ë˜ëŠ” ê¸´ String Basic Auth OAuth 2.0 ì„¤ëª…í•˜ìë©´ Web APIëŠ” ìš°ë¦¬ê°€ ì§ì ‘ ì–´ë–¤ Actionì„ í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— í•´ë‹¹ Webì—ë§Œ ì ‘ê·¼ ê¶Œí•œì„ ë°›ìœ¼ë©´ ë˜ì§€ë§Œ, ê·¸ì™€ëŠ” ë‹¤ë¥´ê²Œ ì–´ë–¤ Actionì„ ì·¨í•  Webì„ ë‹¤ë¥¸ ì•±ì—ê²Œ ì ‘ê·¼ ê¶Œí•œì„ ì£¼ì–´ End Userì¸ ìš°ë¦¬ ëŒ€ì‹  ì •ë³´ë¥¼ ì œê³µí•˜ê²Œ í•´ì£¼ëŠ” ë°©ì‹ì´ë‹¤. ì˜ˆë¥¼ë“¤ë©´ ìš°ë¦¬ê°€ ì–´ë–¤ ì„œë¹„ìŠ¤ë¥¼ ê°€ì…í•˜ë ¤ê³  í• ë•Œ SNSë¡œ ê°€ì…ì´ ê°€ëŠ¥í•˜ê²Œ í•  ìˆ˜ ìˆëŠ” ì ‘ê·¼ ê¶Œí•œ ë°©ì‹ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. Endpoints &amp; Methods ResourceëŠ” APIë¥¼ í†µí•´ ë¦¬í„´ëœ ì •ë³´ì´ë©°, í•˜ë‚˜ì˜ Resource ì•ˆì— ì—¬ëŸ¬ê°œì˜ Endpointsê°€ ì¡´ì¬í•œë‹¤. Parameters ParametersëŠ” Endpointë¥¼ í†µí•´ Requests í• ë•Œ ê°™ì´ ì „ë‹¬í•˜ëŠ” ì˜µì…˜ë“¤ Request Bodyì•ˆì— í¬í•¨ë˜ëŠ” Parameterë“¤ì€ post ë°©ì‹ì—ì„œ ì£¼ë¡œ ë§ì´ ì‚¬ìš©í•œë‹¤. Spotify í•„ìì˜ í”„ë¡œì íŠ¸ì˜ ì£¼ëœ dataë¥¼ ì œê³µë°›ì„ Spotifyë¥¼ ë¨¼ì € ì†Œê°œí•˜ê¸° ì „ì—, ì™œ êµ­ë‚´ì˜ Melonê³¼ Genieë¥¼ íƒí•˜ì§€ ì•Šì•˜ëŠ”ì§€ë¥¼ ë§í•˜ë ¤ê³  í•œë‹¤. SKí”Œë˜ë‹›ì´ 2012ë…„ë¶€í„° ìš´ì˜í•˜ë˜ ê°œë°œìì„¼í„° ë‚´ ì˜¤í”ˆ API ì„œë¹„ìŠ¤ë¥¼ 2018ë…„ 3ì›”ë¶€í„° ì¤‘ë‹¨í•œë‹¤ê³  ë°œí‘œí•¨ìœ¼ë¡œì¨ Melonì˜ API ì„œë¹„ìŠ¤ë¥¼ ì‹œê¸°ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ëª»í•˜ê²Œ ë˜ì—ˆìœ¼ë©°, Genie ë˜í•œ API ì„œë¹„ìŠ¤ë¥¼ ë” ì´ìƒ ì œê³µí•˜ì§€ ì•Šê³  ìˆë‹¤. ë‹¨ìˆœíˆ ê³¡ëª…ê³¼ ì•„í‹°ìŠ¤íŠ¸ëª…, í•´ë‹¹ ê³¡ì— ëŒ€í•œ ëŒ“ê¸€ë“± ì´ëŸ° ê²ƒë“¤ì€ Seleniumì´ë‚˜ ê·¸ëƒ¥ ê¸°ë³¸ì ì¸ requests ëª¨ë“ˆì„ í†µí•´ ê°€ëŠ¥í•˜ì§€ë§Œ, ì´ë²ˆ í† ì´ í”„ë¡œì íŠ¸ì˜ ëª©í‘œëŠ” Spotify APIë¥¼ í†µí•˜ì—¬ ì—”í„°í‹°ê°„ì˜ ê´€ê³„ë¥¼ ë‚´ê°€ ì§ì ‘ ì„¤ê³„í•´ ë³´ê³  ì´ë¯¸ ë§Œë“¤ì–´ ìˆê¸´ í•˜ì§€ë§Œ ê° ê³¡ë“¤ì˜ íŠ¹ì§•ì„ ìˆ˜ì¹˜ì ìœ¼ë¡œ ë¶„ë¥˜í•´ ë†“ì€ íŠ¹ì§•ë“¤ë¡œ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ì—¬ Userì—ê²Œ ì¶”ì²œí•˜ëŠ” Facebook ì•±ì„ ë§Œë“¤ì–´ ë³´ê³  ì‹¶ì–´ì„œì´ë‹¤. ê·¸ë ‡ë‹¤ë©´, SpotifyëŠ” ë¬´ì—‡ì¸ê°€? ìŠ¤í¬í‹°íŒŒì´ëŠ” í”„ë¦¬ì›¨ì–´ì´ë‹¤. ë³¸ë˜ì—ëŠ” ë¬´ë£Œë¡œ ì´ìš©í•˜ë©´ ì‹œê°„ ì œí•œì´ ìˆì—ˆìœ¼ë‚˜, 2014ë…„ íì§€ë˜ì—ˆë‹¤. ìŠ¤í¬í‹°íŒŒì´ëŠ” Spotify ì›¹ì‚¬ì´íŠ¸ì—ì„œ ë°”ë¡œ ë‹¤ìš´ë°›ì„ ìˆ˜ ìˆë‹¤. ì œê³µë˜ëŠ” ê³¡ë“¤ì€ ìŒë°˜ì‚¬ë“¤ì´ ë¼ì´ì„ ìŠ¤í•˜ì—¬ í•©ë²•ì ìœ¼ë¡œ ì œê³µí•œ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ, ì‚¬ìš©ìê°€ í•œ ë‹¬ 9.99 ìœ ë¡œì˜ ì„œë¹„ìŠ¤ ì‚¬ìš©ë£Œë¥¼ ë‚´ì§€ ì•ŠëŠ”ë‹¤ë©´, ì†Œí”„íŠ¸ì›¨ì–´ ìƒì— ê´‘ê³ ê°€ í‘œì‹œë˜ë©°, ê³¡ê³¼ ê³¡ ì‚¬ì´ì— ê´‘ê³ ê°€ ì‚½ì…ëœë‹¤. ê°€ì…ìê°€ ì„œë¹„ìŠ¤ ì‚¬ìš©ë£Œë¥¼ ëƒˆë‹¤ë©´, ê°€ì…ìëŠ” ìë™ì ìœ¼ë¡œ â€œí”„ë¦¬ë¯¸ì—„ ì‚¬ìš©ìâ€ ìƒíƒœê°€ ëœë‹¤. í”„ë¦¬ë¯¸ì—„ ì‚¬ìš©ìë“¤ì€ íŠ¹ë³„íˆ ë‰´ìŠ¤ë‚˜ í”„ë¦¬ë·°ë¥¼ ë“¤ì„ ìˆ˜ ìˆë‹¤. ë˜í•œ developer APIë¥¼ ì œê³µí•¨ìœ¼ë¡œì¨, API ì‚¬ìš©ë²•ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…ì´ ìˆë‹¤.Spotify developerì•ìœ¼ë¡œ ì´ ì‚¬ì´íŠ¸ì— ìˆëŠ” API ì‚¬ìš©ë²•ì„ í™œìš©í•˜ì—¬ ë°ì´í„°ë¥¼ crawlingí•œ í›„ì— RDSì— ì €ì¥í•  ê²ƒì´ë‹¤. Spotify for Developers APIë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ì ‘ê·¼ ê¶Œí•œì´ ìˆëŠ” Access IDì™€ passwordë¥¼ ë°œê¸‰ë°›ì•„ì•¼ í•˜ë¯€ë¡œ ë¨¼ì €, APPì„ ë§Œë“¤ê²ƒì´ë‹¤. ìœ„ì˜ íƒ­ë€ì—ì„œ DashBoardë¥¼ í´ë¦­í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ í˜ì´ì§€ë¡œ ì´ë™í•  ê²ƒì´ë‹¤. loginì´ í•„ìš”í•˜ë¯€ë¡œ ë¨¼ì € ê°€ì…ì„ í•´ì•¼ í•  ê²ƒì´ë‹¤. ì˜†ì— ìˆëŠ” Sign up for free Spotify account here ë²„íŠ¼ì„ ëˆŒëŸ¬ í•„ìê°€ ì‚¬ìš©í•  Access ë°©ë²•ì€ ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë˜ Oauth 2.0ì„ í™œìš©í•œ ë°©ì‹ì€ ì•„ë‹ˆê³ , ê·¸ëƒ¥ Access IDì™€ Passwordë¥¼ ë°œê¸‰ë°›ì€ í›„ Accessí•˜ëŠ” ë°©ì‹ì„ íƒí–ˆë‹¤. ì•½ê°„ì˜ ì£¼ì˜ì‚¬í•­ì€ ë°œê¸‰ë°›ì€ í›„ 1ì‹œê°„ ê²½ê³¼ í›„ì—ëŠ” Passwordë¥¼ ì¬ë°œê¸‰ë°›ì€ í›„ ì‚¬ìš©í•˜ì—¬ì•¼ í•œë‹¤. Client Credentials Flow ë°©ì‹ì€ ë§Œë“¤ì–´ ë†“ì€ application(dash board)ì„ í†µí•´ì„œ client idì™€ client secretì„ spotifyì—ê²Œ ì£¼ê²Œ ë˜ëŠ”ë°, spotifyëŠ” ë‹¤ì‹œ í•œë²ˆ Access Tokenì„ ë°˜í™˜í•´ ì£¼ê²Œëœë‹¤. ì´ Access Tokenì„ í†µí•´ dataë¥¼ ê°€ì ¸ ì˜¬ ìˆ˜ ìˆê²Œ ëœë‹¤. Client Credentials FlowëŠ” POST https://accounts.spotify.com/api/tokenë¥¼ í†µí•´ request í•  ìˆ˜ ìˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ì„œ ì¡°ê¸ˆ ë” ìì„¸í•˜ê²Œ ë§í•˜ìë©´, requestsë¥¼ postë°©ì‹ìœ¼ë¡œ ì–»ê²Œ ë˜ëŠ”ë°, request body parameterì—ëŠ” client credentialê°’ì„ ê¼­ í•„ìš”ë¡œ í•˜ë©°, header parameterì—ëŠ” Basic ë‹¤ìŒ ë¬¸ìì—´ë¡œ ì„ ë„£ì–´ì£¼ì–´ì•¼í•œë‹¤. ê·¸ë¦¬ê³  ë°›ì€ Access Tokenì„ ì´ìš©í•´\bì„œ â€œAuthorizationâ€: â€œBearer {Access token}â€í˜•íƒœë¡œ headersì— ë„£ì–´ requestë¥¼ get ë°©ì‹ìœ¼ë¡œ í•´ì£¼\u001cë©´ í•˜ê³ ì‹¶ì€ ë©”ì„œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ë¬¸ì œê°€ ìƒê²¼ë‹¤. êµ­ì œì ì¸ ì„œë¹„ìŠ¤ì—¬ì„œ êµ­ë‚´ì—ì„œë„ ì´ìš© ì œí•œì´ ì—†ì„ ê²ƒì´ë¼ê³  ìƒê°í–ˆì§€ë§Œ, êµ­ë‚´ì—ì„œëŠ” ì„œë¹„ìŠ¤ë¥¼ ì•„ì§ í•˜ì§€ ì•Šê³  ìˆë‹¤í•œë‹¤â€¦â€¦. ê²°êµ­ ë°©ë²•ì„ ì°¾ì•„ë³´ë‹¤ VPNì„ ì‚¬ìš©í•˜ì—¬ ìš°íšŒí•œ í›„ì— ê°€ì…ì„ í•˜ë©´ ì„œë¹„ìŠ¤ ì´ìš©ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œê²Œ ë˜ì—ˆê³ , VPNì„ ì´ìš©í•˜ì—¬ ê°€ì…í•˜ì˜€ë‹¤. ê°€ì…í•œ í›„ì— Applicationì„ ë“±ë¡í•´ë³´ì. Appì˜ ì´ë¦„ê³¼ ì„¤ëª… ê°œë°œ ìš©ë„ë¥¼ ì…ë ¥í•´ì£¼ëŠ”ë°, App ê°œë°œ ìš©ë„ëŠ” ë¯¸ì •ì´ë¯€ë¡œ ëª¨ë¥¸ë‹¤ê³  ì„¤ì •í–ˆë‹¤. ë‹¤ ë§Œë“¤ì–´ ì¡Œë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ ìƒˆë¡œìš´ ì•±ì´ ìƒì„±ë˜ì—ˆìŒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ìš°ë¦¬ê°€ APIì— ì ‘ê·¼í•  ë•Œ í•„ìš”í•œ IDì™€ password ì •ë³´ë¥¼ ë³´ë ¤ë©´ ì•±ì„ í´ë¦­í•˜ë©´ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. passwordëŠ” hideë˜ì–´ìˆëŠ”ë° hideë¥¼ í’€ë©´ ë³¼ ìˆ˜ ìˆë‹¤. 1ì‹œê°„ ë§ˆë‹¤ passwordëŠ” resetí•´ì„œ ì‚¬ìš©í•´ì•¼í•œë‹¤. APIë¥¼ ì‚¬ìš©ë²•ì„ í†µí•´ ê²°ê³¼ì ìœ¼ë¡œ ë§Œë“¤ ERD(Entity Relationship Diagram)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ìœ„ì˜ ERDë¥¼ ë§Œë“¤ê¸°ì— ì•ì„œì„œ Spotify APIë¥¼ í†µí•´ Access Tokenì„ ì–»ê³  searchí•  ìˆ˜ ìˆë„ë¡ script íŒŒì¼ì„ ë§Œë“¤ì–´ ë³¸ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²ƒì´ë‹¤. ì§€ì†ì ìœ¼ë¡œ ìœ ì§€ë¥¼ í•˜ê¸° ìœ„í•´ì„œ status codeë§ˆë‹¤ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì„ ë‹¬ë¦¬ í•´ì£¼ì–´ì•¼ í•  ê²ƒì´ë‹¤.íŠ¹íˆ ê·¸ ì¤‘ì— status code ì¤‘ 429ëŠ” ë„ˆë¬´ ë§ì€ ë°ì´í„°ë¥¼ ìš”ì²­ í–ˆì„ ë•Œ ì¶œë ¥ê°’ìœ¼ë¡œ ë°›ëŠ” status codeì´ë©°, ë°›ì€ ì¶œë ¥ì•ˆì— ì œí•œì‹œê°„ì´ ê°™ì´ ì¡´ì¬í•˜ë¯€ë¡œ ê·¸ ì‹œê°„ë™ì•ˆ python ë™ì‘ìœ¼ë¡œ ë©ˆì¶”ë„ë¡ sleepì„ ê±¸ì–´ë†“ê²Œë” ì½”ë“œë¥¼ ì‘ì„±í•  ê²ƒì´ë‹¤. Spotifyì˜ status code ë˜í•œ, status code = 401ì¸ ê²½ìš°ì—ëŠ” access tokenì´ expiredë˜ì—ˆê¸° ë•Œë¬¸ì— ì¼ì–´ë‚˜ëŠ” ê²½ìš°ì´ê¸° ë•Œë¬¸ì— ìƒˆë¡­ê²Œ client secretì„ ë°›ì•„ì•¼ í•  ê²ƒì´ë‹¤.","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"Basic ConvNN(VGG-16ëª¨ë°©í•œ ê¸°ë³¸)êµ¬í˜„","slug":"deep_learning_05","date":"2019-12-13T03:58:02.000Z","updated":"2020-01-12T16:26:13.503Z","comments":true,"path":"2019/12/13/deep_learning_05/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/13/deep_learning_05/","excerpt":"","text":"Basic ConvNN êµ¬í˜„ ì°¸ê³ ë¡œ ì €ëŠ” macì„ ì‚¬ìš©í•˜ê¸°ì— localì—ì„œë§ê³  GPUë¥¼ ì‚¬ìš©í•˜ê²Œë” Google Colabì„ ì‚¬ìš©í•˜ì˜€ë‹¤. ì œê°€ êµ¬í˜„í•œ ë°©ì‹ì€ tensorflow 2.0 versionì´ë¯€ë¡œ(tf.functionì„ ì‚¬ìš©í•˜ëŠë¼) colabì˜ tensorflowì˜ versionì´ ë­”ì§€ ë¨¼ì € í™•ì¸í–ˆìŠµë‹ˆë‹¤. 1.15 versionì´ì–´ì„œ 2.0ìœ¼ë¡œ ì„¤ì¹˜ë¥¼ ì§„í–‰í•œ í›„ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì˜€ìŠµë‹ˆë‹¤. ì°¸ê³ ë¡œ 2.0ìœ¼ë¡œ ì„¤ì¹˜í•˜ê³  ë‚œ í›„ì—ëŠ” ê¼­ ë°˜ë“œì‹œ ëŸ°íƒ€ì„ì„ ì¬ì‹œì‘ í•´ì£¼ì…”ì•¼ ì—…ë°ì´íŠ¸ í•œ 2.0 versionìœ¼ë¡œ ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 123import tensorflow as tfimport numpy as npprint(tf.__version__) ëŸ°íƒ€ì„ ì¬ì‹œì‘ í›„1!pip install tensorflow==2.0.0-beta1 ê¸°ë³¸ í•©ì„±ê³± ì‹ ê²½ë§ êµ¬í˜„12import tensorflow as tfimport numpy as np í•˜ì´í¼ íŒŒë¼ë¯¸í„°1EPOCHS = 10 ì°¸ê³ ë¡œ conv layerì„ í†µê³¼í•œ ì¶œë ¥ì˜ dimensionì„ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ë‹¤ìŒê³¼ ê°™ë‹¤.padding : 2N+1 = kernel_size(Filter_size)ë¡œ Nì„ êµ¬í•œë‹¤.output dimension :\\frac{input\\hspace{0.1cm} size + (padding\\hspace{0.1cm} size * 2) - filter\\hspace{0.1cm}size}{strid} + 1ëª¨ë¸ ì •ì˜123456789101112131415161718192021222324252627282930313233class ConvNet(tf.keras.Model): def __init__(self): super(ConvNet, self).__init__() self.sequence = [] conv2d = tf.keras.layers.Conv2D max_pool = tf.keras.layers.MaxPool2D flatten = tf.keras.layers.Flatten # filters = 16 (ì¶œë ¥ë˜ëŠ” channelì˜ ìˆ˜) # kernel_size = 3 * 3 # paddingì˜ defaultê°’ì¸ 'valid'ëŠ” zero-paddingì„ í•´ì£¼ì§€ ì•ŠìŒìœ¼ë¡œì¨ ì˜ìƒì˜ í¬ê¸°ê°€ Conv layerë¥¼ í†µê³¼í•¨ìœ¼ë¡œì¨ ì¤„ì–´ë“¤ ìˆ˜ ìˆë‹¤. # 'same'ì€ zero-paddingì„ ì˜ë¯¸ ì—¬ê¸°ì„œëŠ” ë™ì¼í•œ í¬ê¸°ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ # input dataëŠ” (28x28x1)ì„ ê°–ëŠ” MNISTì´ë‹¤. # VGG-16ì˜ ê°€ì¥ í° íŠ¹ì§•ì€ Poolingì„ í•˜ê¸° ì „ì— ë™ì¼í•œ Conv Layerë¥¼ ë°˜ë³µí•´ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. self.sequence.append(conv2d(16, (3,3), padding='same', activation='relu')) # output dimension (28x28x16) self.sequence.append(conv2d(16, (3,3), padding='same', activation='relu')) # output dimension (28x28x16) # 2x2 poolingì„ í•œë‹¤. maxpoolingì„ ì´ìš©í•˜ì—¬ ì˜ìƒì˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì¤€ë‹¤. self.sequence.append(max_pool((2,2))) # output dimension (14x14x16) self.sequence.append(conv2d(32, (3,3), padding='same', activation='relu')) # output dimension (14x14x32) self.sequence.append(conv2d(32, (3,3), padding='same', activation='relu')) # output dimension (14x14x32) self.sequence.append(max_pool((2,2))) # output dimension (7x7x32) self.sequence.append(conv2d(64, (3,3), padding='same', activation='relu')) # output dimension (7x7x64) self.sequence.append(conv2d(64, (3,3), padding='same', activation='relu')) # output dimension (7x7x64) self.sequence.append(flatten()) # 1568x1 self.sequence.append(tf.keras.layers.Dense(2028, activation='relu')) self.sequence.append(tf.keras.layers.Dense(10, activation='softmax')) def call(self, x, training=False, mask=None): for layer in self.sequence: x = layer(x) return x í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë£¨í”„ ì •ì˜1234567891011121314151617181920# Implement training loop@tf.functiondef train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy): with tf.GradientTape() as tape: predictions = model(images) loss = loss_object(labels, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train_loss(loss) train_accuracy(labels, predictions)# Implement algorithm test@tf.functiondef test_step(model, images, labels, loss_object, test_loss, test_accuracy): predictions = model(images) t_loss = loss_object(labels, predictions) test_loss(t_loss) test_accuracy(labels, predictions) ë°ì´í„°ì…‹ ì¤€ë¹„1234567891011121314151617181920212223mnist = tf.keras.datasets.mnist# ì…ë ¥ ì˜ìƒì´ ì´ 8bit ì¦‰, 0~255 ì‚¬ì´ì˜ ê°’ë“¤ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë¯€ë¡œ(x_train, y_train), (x_test, y_test) = mnist.load_data()# 0~1í‘œí˜„ìœ¼ë¡œ ë°”ê¿”ì¤€ë‹¤.x_train, x_test = x_train / 255.0, x_test / 255.0# ì…ë ¥ ì˜ìƒ í•˜ë‚˜ì˜ ì‚¬ì´ì¦ˆëŠ” 28x28ì´ë¯€ë¡œ channelì„ í•˜ë‚˜ ë” ëŠ˜ë ¤ ì£¼ì–´ì•¼í•œë‹¤.print(x_train.shape)print(x_train[0].shape)# x_train : (NUM_SAMPLE, 28, 28) -&gt; (NUM_SAMPLE, 28, 28 , 1)# ...ì€ í•´ë‹¹ ë°ì´í„° ê°ì²´ì˜ ëª¨ë“  axisë¥¼ í‘œí˜„í•˜ëŠ” ê²ƒì´ë‹¤.# ìœ„ì—ì„œ 255.0ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì£¼ê²Œ ë˜ë©´ float64ë¡œ ë˜ë¯€ë¡œ ìë£Œí˜•ì„ float32ë¡œ í•´ì•¼ errorê°€ ì—†ë‹¤.## x_train[:,:,:, tf.newaxis]x_train = x_train[..., tf.newaxis].astype(np.float32)x_test = x_test[..., tf.newaxis].astype(np.float32)# Numpy objectë‚˜ Tensorë¡œ ë¶€í„° ë°ì´í„°ì…‹ì„ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤.train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)# test dataëŠ” suffleí•  í•„ìš”ì—†ë‹¤.test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) í•™ìŠµ í™˜ê²½ ì •ì˜ëª¨ë¸ ìƒì„±, ì†ì‹¤í•¨ìˆ˜, ìµœì í™” ì•Œê³ ë¦¬ì¦˜, í‰ê°€ì§€í‘œ ì •ì˜12345678910111213# Create modelmodel = ConvNet()# Define loss and optimizerloss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()# Define performance metricstrain_loss = tf.keras.metrics.Mean(name='train_loss')train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')test_loss = tf.keras.metrics.Mean(name='test_loss')test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy') í•™ìŠµ ë£¨í”„ ë™ì‘123456789101112131415161718for epoch in range(EPOCHS): for images, labels in train_ds: train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy) for test_images, test_labels in test_ds: test_step(model, test_images, test_labels, loss_object, test_loss, test_accuracy) template = 'Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;' print(template.format(epoch + 1, train_loss.result(), train_accuracy.result() * 100, test_loss.result(), test_accuracy.result() * 100)) # reset_stateëŠ” ìƒˆë¡œìš´ ê°’ë“¤ì„ ë°›ê¸° ìœ„í•´ í•˜ëŠ” ê±´ê°€? train_loss.reset_states() train_accuracy.reset_states() test_loss.reset_states() test_accuracy.reset_states()","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"Convolution Neural Network(1)","slug":"deep_learning_04","date":"2019-12-10T04:50:24.000Z","updated":"2020-01-20T05:53:41.787Z","comments":true,"path":"2019/12/10/deep_learning_04/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/10/deep_learning_04/","excerpt":"","text":"í•©ì„±ê³± ì—°ì‚°ê³¼ ì´ë¯¸ì§€ í•„í„° ì•„ë‚ ë¡œê·¸ ì‹ í˜¸ì²˜ë¦¬ëŠ” ì„ í˜•ì´ê³  ì‹œë¶ˆë³€ì¸ ì‹œìŠ¤í…œì— ì˜ì¡´í•´ì„œ ê°œë°œì´ ë˜ê²Œ ë˜ëŠ”ë°, Noiseê°€ ìˆëŠ” ì…ë ¥ì´ ë“¤ì–´ì™”ì„ ë•Œ ë„£ì–´ì£¼ë©´ Noiseê°€ ì œê±°ëœ ì¶œë ¥ì´ ë‚˜ì˜¤ëŠ” ì´ëŸ° ì‹œìŠ¤í…œì„ ëª¨ë‘ LTI systemì´ë¼ê³  ë¶€ë¥¸ë‹¤. ë””ì§€í„¸ ì‹ í˜¸ê°€ ì•„ë‹Œ ì•„ë‚ ë¡œê·¸ ì‹ í˜¸ë¡œë¶€í„° LTI systemì´ ì •ì˜ë˜ì–´ìˆë‹¤. ì„ í˜•ì´ë¼ëŠ” ê²ƒì€ ëŒ€ë¶€ë¶„ ì•Œê³  ìˆë“¯ì´ ì„ í˜•ëŒ€ìˆ˜ì—ì„œ ë‚˜ì˜¤ëŠ” linearityë¥¼ ë§Œì¡±ì‹œí‚¤ë©´ ë˜ëŠ” ê²ƒì´ê³ , ì‹œë¶ˆë³€ì´ë¼ëŠ” ì˜ë¯¸ëŠ” ì‹œê°„ì´ ì§€ë‚˜ë„ ë™ì¼í•œ ê²°ê³¼ë¥¼ ë‚´ë³´ë‚´ì¤€ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. í™•ë¥ ê³¼ì •ì—ì„œ stepì— ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë‹¤ë¼ê³  ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. ì‚¬ëŒì€ ëŒ€í‘œì ìœ¼ë¡œ LTI ì‹œìŠ¤í…œì´ ì•„ë‹Œ ì‹œìŠ¤í…œì´ë‹¤. ìˆ˜í•™ì ìœ¼ë¡œëŠ” ì—„ë°€í•˜ì§„ ì•Šì§€ë§Œ, ê³µí•™ì—ì„  ë§ì´ ì‚¬ìš©í•œë‹¤. ì™¼ìª½ì˜ ì‚¼ê°í˜•ì„ ëª¨ë“  êµ¬ê°„ì— ëŒ€í•´ ì „ë¶€í•´ì¤€ë‹¤ë©´ ê°’ì€ 1ì´ ë  ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ $h\\to\\infty$ê°€ ë˜ë©´, Dirac ë¸íƒ€ í•¨ìˆ˜ê°€ ëœë‹¤. ì‹œê°„ t=0ë§Œ ì„ì˜ì˜ ê°’ì„ ê°–ê³ , ë‚˜ë¨¸ì§€ êµ¬ê°„ì€ 0ì„ ê°–ëŠ”ë‹¤. ëª¨ë“  êµ¬ê°„ì—ì„œ ì ë¶„í•œ ê°’ì´ 1 convolutionì„ í•œë‹¤ëŠ” ê²ƒì€ ì„ì˜ì˜ ë‘ í•¨ìˆ˜ ì¤‘ í•œ í•¨ìˆ˜ë¥¼ ì¢Œìš°ë¡œ ë’¤ì§‘ê³  ì´ë™ì‹œí‚¤ë©´ì„œ ë‘í•¨ìˆ˜ì˜ ê³±ì„ ì ë¶„í•˜ì—¬ ê³„ì‚°í•œë‹¤. í•©ì„±ê³± ê³„ì‚° animation vertical Sobel Filterê°€ ì™œ ë¯¸ë¶„ í•„í„°ì´ëƒê³  ì˜ë¬¸ì´ ë“ ë‹¤ë©´, 1ì°¨ì› ì‹ í˜¸ë¥¼ dataë¡œ ìƒê°í•˜ê³  ì•ì„œ í–ˆì—ˆë˜ ìˆ˜ì¹˜ ë¯¸ë¶„ì„ ë– ì˜¬ë ¤ ë³´ì. ê·¸ë ‡ë‹¤ë©´, ë‹¨ìˆ¨ì— ì´í•´ê°€ ê°”ì„ ê²ƒì´ë‹¤. ì§€ê¸ˆì€ vertical Sobel Filterì´ë¯€ë¡œ ê°€ë¡œì˜ EdgeëŠ” ì¶”ì¶œí•˜ì§€ ëª»í•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ê·¸ì— ë°˜í•´ ì„¸ë¡œ ì„±ë¶„ë“¤ì€ ì˜ ê²€ì¶œëœ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ë§Œì¼ ìœ„ì˜ í•„í„°ë¥¼ 90ë„ rotateí•´ì£¼ê²Œë˜ë©´ ê°€ë¡œë¡œ ë¯¸ë¶„í•˜ëŠ” horizonal Sobel Filterê°€ ë˜ì–´ ìœ„ì˜ ê²°ê³¼ì™€ëŠ” ë°˜ëŒ€ì˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤„ ê²ƒì´ë‹¤. í•©ì„±ê³± ê³„ì¸µ ì…ë ¥ì´ ì´ì œëŠ” ì˜ìƒìœ¼ë¡œ ì—¬ëŸ¬ê°œ ë“¤ì–´ì˜¤ê²Œ ë˜ì–´ ê°ê°ì˜ ì…ë ¥ì¸µì˜ ë‰´ëŸ° í•˜ë‚˜ í•˜ë‚˜ê°€ channelì´ë¼ê³  ë¶ˆë¦°ë‹¤. í•„í„° ê°€ì¤‘ì¹˜ëŠ” ë³´í†µ 3x3, 5x5, 7x7ë“±ì„ ì£¼ë¡œ ì‚¬ìš©í•œë‹¤. 2D signalê³¼ 2D signalì„ ê³±(element-wise product or Hadamard product)í•´ì•¼í•˜ë¯€ë¡œ í•©ì„±ê³±ì„ ì‚¬ìš©í•œë‹¤. $kernel_{Height} \\times kernel_{Width} \\times channel_{in} \\times channel_{out}$ ë§Œí¼ì˜ parameterê°€ í•„ìš”í•˜ë‹¤. filterëŠ” $Channel_{in} \\times Channel_{out}$ê°œ ë§Œí¼ ìˆì„ ê²ƒì´ë‹¤. kernel(Filter)ì— ë‚˜íƒ€ë‚˜ëŠ” ëª¨ì–‘ê³¼ ìœ ì‚¬í•œ ëª¨ì–‘ì„ í•œ ìœ„ì¹˜ê°€ ë†’ì€ ê°’ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ê²Œ ëœë‹¤. ê¸°ë³¸ì ì¸ í•©ì„±ê³± ì‹ ê²½ë§ strideìš”ì†Œë¥¼ ë„£ì§€ ì•Šìœ¼ë©´, í•©ì„±ê³± ê³„ì¸µì—ì„œëŠ” ì˜ìƒì˜ í¬ê¸°ëŠ” ê·¸ëŒ€ë¡œì´ë©°, ì˜ìƒì˜ ì±„ë„ ìˆ˜ê°€ ë‹¬ë¼ì§„ë‹¤. kernel(filter)ê°€ ëŒì•„ë‹¤ë‹ˆë©´ì„œ í¬ì°©í•˜ëŠ” í˜•íƒœì´ê¸° ë•Œë¬¸ì— ê³µê°„ì ì¸ íŠ¹ì§•ì´ ìˆê³ , ë”°ë¼ì„œ Feature Mapì´ë¼ê³  í•œë‹¤. classificationì—ì„œëŠ” Max Poolingì´ ì£¼ë¡œ ì˜ ë¨¹íŒë‹¤! -Convolutional Layerì™€ FC Layerë¥¼ ì—°ê²°í•´ì£¼ê¸° ìœ„í•´ í•„ìš”í•˜ë‹¤ ë¨¼ì €, ë§¨ ì²˜ìŒ ì–¸ê¸‰í–ˆë˜ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ê°€ì¥ í° ì°¨ì´ëŠ” ì‚¬ëŒì´ featureë¥¼ ë„£ì–´ì£¼ëŠëƒ ê·¸ë ‡ì§€ ì•ŠëŠëƒì˜ ì°¨ì´ë¼ê³  í–ˆë‹¤. í¬ê²Œ ë³´ë©´ ìœ„ì˜ ê·¸ë˜í”„ì—ì„œ í•©ì„±ê³± ê³„ì¸µê³¼ í™œì„±í•¨ìˆ˜ì˜ ê³¼ì •ì„ Në²ˆ ë°˜ë³µí•˜ëŠ” ê²ƒì€ shallowNNì˜ inputìœ¼ë¡œ ë„£ì–´ ì¤„ Featureë¥¼ ë½‘ëŠ” ê³¼ì •ì´ë¼ê³  ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤. ì•ì˜ í•©ì„±ê³± ê³„ì¸µì—ì„œ activation function ê¹Œì§€ë¥¼ ê³„ì†í•´ì„œ ì§„í–‰ í• ìˆ˜ë¡ Feature Mapì˜ í¬ê¸°(widthì™€ height)ëŠ” Kernelê³¼ Poolingì— ì˜í•´ì„œ ì¤„ì–´ë“¤ê³  channel(depth)ëŠ” ëŠ˜ì–´ë‚˜ê²Œ ë  ê²ƒì´ë‹¤. ë˜í•œ ì²˜ìŒë¶€í„° ëê¹Œì§€ ë™ì¼í•œ í¬ê¸°ì˜ kernel(Filter)ì„ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í•œë‹¤ë©´, ì˜ìƒì—ì„œ ë” ë„“ì€ ì˜ì—­ì„ ì»¤ë²„í•˜ëŠ” íš¨ê³¼ë¥¼ ì£¼ëŠ” ê²ƒê³¼ ë™ì¼í•˜ë‹¤. ê·¸ë˜\u001dì„œ Feature Mapì„ í•œë²ˆ ë½‘ì„ ë•Œë§ˆë‹¤ Poolingì„ í•´ì£¼ë©´\u001cì„œ ì²˜ìŒì—ëŠ” ì¢ì€ ì˜ì—­ì„ ì ì  ë” ë„“ì€ ì˜ì—­ì„ ë³¸ë‹¤. ì ì  ë” ë„“ì€ ì˜ì—­ì„ ë³¸ë‹¤ëŠ” ì˜ë¯¸ëŠ” Poolingì„ í•¨ìœ¼ë¡œì¨ ê²°êµ­ì—ëŠ” ë” ë„“ì€ ë²”ìœ„ë¥¼ ëŒ€í‘œí•˜ëŠ” ê°’ë“¤ì„ ê°€ì§„ 2-D Matrixì¸ Feature Mapì´ ë  ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. 98ë…„ë„ì˜ ë¥´ì¿¤ êµìˆ˜ë‹˜ì˜ LeNet-5ëŠ” pooling ëŒ€ì‹  subsamplingì„ ì‚¬ìš©í•˜ì—¬ ê°™ì€ Feature Mapì˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì£¼ì—ˆë‹¤. í•©ì„±ê³± ì‹ ê²½ë§ì˜ ì‹¬í™” ì´í•´ ê°„ë‹¨íˆ ìƒê°í•˜ë©´ $kernel_{height} \\times kernel_{width} \\times Channel_{in} \\times Channel_{out}$ ë§Œí¼ ì–´ë§ˆì–´ë§ˆí•˜ê²Œ ë§ì€ Parameterê°€ í•„ìš”í•˜ë¯€ë¡œ ê³„ì‚°í•´ì•¼ í•  Para\u001cmeterê°€ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ FC Layerë¡œ í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ë°©ë²•ì´ì§€ ì•Šì„ê¹Œë¼ê³  ìƒê°í•˜ì‹œëŠ” ë¶„ë“¤ì´ ìˆì„ ê²ƒì´ë‹¤. í—ˆë‚˜, ê·¸ê²ƒì€ ì˜ëª»ëœ ìƒê°ì´ë‹¤. Convolutional Layerë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ìš°ë¦¬ê°€ Imageë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. FC Layerë¥¼ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ì˜¤íˆë ¤ ê³„ì‚°í•´ì•¼ í•  Parameterì˜ ê°œìˆ˜ê°€ ì–´ë§ˆì–´ë§ˆí•˜ê²Œ ëŠ˜ì–´ë‚œë‹¤.$(Height_{in} \\times Width_{in} \\times Channel_{in}) \\times (Height_{out} \\times Width_{out} \\times C_{out})$ ì–¼í• ë³´ê¸°ì—” ë¹„ìŠ·í•´ë³´ì´ê² ì§€ë§Œ, ì˜ˆë¥¼ ë“¤ì–´ë³´ì. ì…ë ¥ìœ¼ë¡œ RGB channelì„ ê°–ëŠ” 1024 * 1024 imageë¥¼ ë°›ëŠ”ë‹¤ë©´, FC Layerë¥¼ ì‚¬ìš©í•œë‹¤ë©´, $(1024 \\times 1024 \\times 16) \\times (1024 \\times 1024 \\times 32)$ì´ì§€ë§Œ Convolutional Layerë¥¼ ì‚¬ìš©í•˜ê³  $3 \\times 3$ kernelì„ ì‚¬ìš©í•œë‹¤ë©´ $(3 \\times 3 \\times 16 \\times 32)$ë¡œ í›¨ì”¬ ì ì€ ì—°ì‚°ì„ í•œë‹¤. ì´ëŸ¬í•œ ì´ìœ ë¡œ ìš°ë¦¬ê°€ ì˜ìƒì„ ì…ë ¥ìœ¼ë¡œ í•˜ëŠ” ê²ƒì€ ì ˆëŒ€ë¡œ FC Layerë¥¼ í†µí•´ í•´ê²°í•  ìˆ˜ ì—†ë‹¤. ìœ„ì—ì„œ WëŠ” kernelë“¤ì„ $C_{in} \\times C_{out}$ Matrixë¡œ ì´ë£¨ì–´ì§„ tensorì´ë‹¤. ì¦‰, $W_{i,j}$ë“¤ì´ ê°ê°ì˜ kernelì„ ë‚˜íƒ€ë‚´ê³  $X_{i}$ì™€ convolution operationì„ í•´ì£¼ë¯€ë¡œ í¸í–¥ì€ FC Layerì™€ ë™ì¼í•˜ê²Œ channel 1ê°œë§ˆë‹¤ 1ê°œì”© ì¡´ì¬í•œë‹¤. ìœ„ì˜ ê·¸ë¦¼ì˜ ì˜ˆë¥¼ ë³´ë©´ kernel sizeê°€ $3 = 2N+1$ì´ë¯€ë¡œ ì…ë ¥ì— ìƒí•˜ì¢Œìš° 1ê°œì˜ Zero-Paddingì„ í•´ì¤€ ê²ƒì´ë‹¤. \bStrideë¥¼ í•˜ëŠ” ê²ƒì€ ê²°ê³¼ë¥¼ ë¯¸ë¦¬ Convolutionì„ Fullë¡œ ë‹¤ ì—°ì‚°ì„ í•œ ë‹¤ìŒì— í•˜ë‚˜ì”© Subsampleí•˜ëŠ” ê²ƒê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¨ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ë‹¤ ì—°ì‚°í•œ í›„ì— subsamplingì„ í•˜ë©´ ì—°ì‚°ì€ ë‹¤í•˜ì§€ë§Œ ê²°êµ­ì—” ë²„ë¦¬ëŠ” ê°’ì´ ìƒê¸°ê¸° ë•Œë¬¸ì— Strideë¥¼ ì‚¬ìš©í•œë‹¤. í•™ìŠµ ì´ˆë°˜ì—ëŠ” ìœ„ìª½ì˜ Feature Mapë“¤ ì²˜ëŸ¼ ì¢ì€ ë²”ìœ„ì˜ Featureë“¤ì„ ì¶”ì¶œí•˜ì§€ë§Œ, í•™ìŠµì˜ í›„ë°˜ ë¶€ì—ëŠ” ë„“ì€ ë²”ìœ„ì˜ Featureë“¤ì„ í•™ìŠµí•œë‹¤. Batch Normalization(ë°°ì¹˜ ì •ê·œí™”) ì¼ë°˜ ê²½ì‚¬ í•˜ê°•ë²•ì˜ ê²½ìš°, Gradientë¥¼ í•œë²ˆ ì—…ë°ì´íŠ¸ í•˜ê¸° ìœ„í•´ ëª¨ë“  í•™ìŠµ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œë‹¤. í•˜ì§€ë§Œ ë°ì´í„°ê°€ ì—„ì²­ë‚˜ê²Œ ë§ë‹¤ë©´?? ê·¸ë ‡ë‹¤ë©´ Gradientë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ”ë° ì˜¤ëœì‹œê°„ì´ ì†Œìš”ë  ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ SGDëŠ”??!! Stochasticì€??? Epochë§ˆë‹¤ ë°ì´í„° ìˆœì„œë¥¼ ì„ì–´ì£¼ê¸°ë„ í•˜ëŠ” ì´ìœ ëŠ” randomì„±ì„ ë” ê°•í•˜ê²Œ í•´ì£¼ê¸° ìœ„í•´ì„œì´ë‹¤. ì´ëŸ° í˜„ìƒì„ í•´ê²°í•˜ê¸° ìœ„í•œ ê²ƒì´ batch normalizationì´ë‹¤. ë˜í•œ, ë™ì¼í•œ scaleê³¼ ë™ì¼í•œ zero-meanì„ ê°€ì§€ê²Œ ë˜ê¸° ë•Œë¬¸ì— í•™ìŠµë¥  ê²°ì •ì— ìœ ë¦¬í•˜ë‹¤ ë§ì˜ ì˜ë¯¸ëŠ” í•™ìŠµì„ í•  ë•Œ ë” scaleì´ í° ê²½ìš°ì—ëŠ” í•™ìŠµì´ ë§ì´ ë˜ê³ , scaleì´ ì‘\u001dìœ¼ë©´ í•™ìŠµì´ ì ê²Œ ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. í•™ìŠµë¥ ì„ ë„ˆë¬´ í¬ê²Œ í•  ê²½\u001cìš° Gradientê°€ í¬ê²Œ ë‚˜ì˜¤ëŠ” ê³³ì— Gradient explodingì´ ë°œ\u001dìƒí•  ìˆ˜ê°€ ìˆê³ , ë°˜ëŒ€ë¡œ í•™ìŠµë¥ ì„ ë„ˆë¬´ ì‘ê²Œ í•  ê²½ìš° Gradient Vanishingì´ ë°œìƒë˜ì„œ í•™ìŠµì´ ì•ˆë˜ëŠ” ê³³ì´ ë°œìƒ\u001cë˜ëŠ” ë¬¸ì œê°€ ìˆëŠ”ë° Normalizationì„ í•´ì£¼ê²Œ ë˜\u001c\u001cë©´ ëª¨ë“  ê°ê°ì˜ ê³„ì¸µë“¤ì´ ë™ì¼í•œ scaleë¡œ í•™ìŠµë˜ê¸° ë•Œë¬¸ì— í•™ìŠµë¥  ê²°ì •ì— ìœ ë¦¬í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.(ë¯¸ë¶„ì„ í• ë•Œ ì…ë ¥ì— ëŒ€í•´ì„œ ì¶œë ¥ê°’ì´ ì»¤ì§€ê²Œ ë˜ë©´, Gradientê°’ë„ ì»¤ì§ˆ ê²ƒì´ë‹¤.) ê°ê°ì˜ batchë¥¼ normalizationí•˜ë©´, ë§ ê·¸ëŒ€ë¡œ normalizationì´ ëœ ê²ƒì´ë¯€ë¡œ ëª¨ìˆ˜ê°€ $\\mu=0, \\sigma^2=1$ì¸ gaussian distributionì„ ê°–ê²Œ ë  ê²ƒì´ë‹¤. ê·¸ëŸ°ë° activationí•¨ìˆ˜ëŠ” LeRuë¥¼ ì‚¬ìš©í•œë‹¤ë©´ 0ë¯¸ë§Œì¸ ê²ƒë“¤ì€ ëª¨ì¡°ë¦¬ 0ê°’ìœ¼ë¡œ ë°˜í™˜ë ê²ƒì´ë‹¤. ì´ë¯¸ ì—°ì‚°ì„ í•´ë†“ì€ ê°’ë“¤ì„ ì—°ì‚° í•˜ê¸° ì „ì´ ì•„ë‹Œ ì—°ì‚°í›„ì— 0ìœ¼ë¡œ ë§Œë“¤ì–´ ì˜ë¯¸ ì—†ê²Œ ë§Œë“œëŠ” ê²ƒ ë³´ë‹¤ ê·¸ë ‡ê²Œ 0ìœ¼ë¡œ ë°˜í™˜ë˜ëŠ” ê°œìˆ˜ë¥¼ ì¡°ì ˆí•˜ê¸° ìœ„í•´ ì¶”ê°€ ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜ì¸ $\\gamma$ì™€ $\\beta$ë¥¼ ë§Œë“¤ê³ , ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµì‹œì¼œì¤€ë‹¤. í•™ìŠµê³¼ì •ì—ì„œ ì´ë™í‰ê· ì„ êµ¬í•´ë†“ëŠ”ë°, ìµœê·¼ Nê°œì— ëŒ€í•œ ì´ë™í‰\u001cê· ì„ ì‚¬ìš©í•œë‹¤. ìµœê·¼ Nê°œë§Œ ì‚¬ìš©í•˜ê³  ê·¸ ì „ì— ê²ƒë“¤ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‚ ë¼ê°€ê¸° ë•Œë¬¸\u001cì— ìµœê·¼ Nê°œê°€ ì¶©ë¶„í•œ sampleì´ ì•„ë‹ ê²½\u001cìš°$\\mu$,$\\sigma$ê°€ ì ì ˆí•˜ì§€ ì•Šê²Œ ê²°ì •ë˜ëŠ” ë¬¸ì œê°€ ìˆëŠ”\u001cë° ì´ëŸ° ìƒí™©ì„ í•´ê²°í•˜ëŠ” ê²ƒì€ ì§€ìˆ˜í‰ê· ì„ ì‚¬ìš©í•œë‹¤. ì‹¬í™” í•©ì„±ê³± ì‹ ê²½ë§ GoogLeNet 2014ë…„ë„ì— GoogLeNetê³¼ VGG-19ê°€ ë‚˜ì™”ëŠ”ë°, GoogLeNetì´ ì—ëŸ¬ìœ¨ì´ ì¢€ ë” ë‚®ê³  ì¸µì´ ë” \bê¹Šì€ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. GoogLeNetì´ ì¢€ ë” ë³µì¡í•´ì„œ VGGê°€ ë” ë§ì´ ì•Œë ¤ì ¸ ìˆì§€ë§Œ, ë‹¤ì–‘í•œ ìŠ¤í‚¬ë“¤ì„ ê³µë¶€í•˜ë ¤ë©´ GoogLeNetì„ ì¡°ê¸ˆ ì‚´í´ë³´ëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒì´ë‹¤. Let&#39;s Go Deeper and Deeperë¼ëŠ” ëª¨í† ë¥¼ ê°€ì§€ê³  ë§Œë“¤ì–´ì§„ ê²ƒê³¼ ê°™ì´ ì¢€ë” hidden_layerê°€ ê¹Šì–´ì§„ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. 1x1, 3x3, 5x5ì˜ featureë“¤ì„ ë‹¤ ë‚˜ëˆ„ì–´ì„œ í•™ìŠµ\bí•œë‹¤. ì¦‰, ë‹¤ì–‘í•œ í¬ê¸°ì˜ Filterë“¤ì´ ì˜ í•™ìŠµëœë‹¤. ë˜í•œ 3x3 Max pooling ê°™ì€ ê²½ìš°ëŠ” convolution Layerë¥¼ ê±°ì¹˜ì§€ì•Šê³ ë„ ë‹¨ìˆœíˆ max poolingì„ í†µí•œ í›„ì—ë„ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì˜ë¯¸ìˆëŠ” featureë¡œ ì‘ë™ëœë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆë‹¤!! naive í•œ Inception ëª¨ë“ˆ êµ¬ì¡°ì—ì„œ ë¨¼ì € ë‹¨ìˆœíˆ 1x1 convë¥¼ í†µê³¼ì‹œì¼œ ë™ì¼í•œ channel ì˜ì—­(Receptive Field)ì„ ê°€ì ¸ê°€ë©´ì„œë„ channelì„ ì¤„ì—¬ ì—°ì‚°ëŸ‰ì„ ì¤„ì—¬ ì£¼ëŠ” êµ¬\u001cì¡°ì¸ Bottleneckë¥¼ êµ¬í˜„í•˜ê³  ìˆë‹¤. ë§¨ ë§ˆì§€ë§‰ ì¶œë ¥ì¸µì—ì„œë§Œ inferenceë¥¼ í•œë‹¤ë©´ Inputì— ê°€ê¹Œìš´ ì¸µì¼ìˆ˜ë¡ ì ì  ë” Vanishing Gradientë¬¸ì œë¡œ ì¸í•´ í•™ìŠµì´ ì €í•˜ ë  ê²ƒì„ ìš°ë ¤í•˜ì—¬ ì¤‘ê°„ featureë“¤ë¡œë„ classificationì„ í•˜ë„ë¡ í•˜ì˜€ë‹¤. GoogLeNet ì¤‘ê°„ ìš”ì•½ Inception êµ¬ì¡° Battleneck êµ¬ì¡° ì¤‘ê°„ ì¤‘ê°„ì— inference Residual Network ì´ì œëŠ” ê±°ì˜ ì¼ë°˜ì ì´ê³ , ê¸°ë³¸êµ¬ì¡°ë¡œ ë§ì´ ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°ì´ë‹¤. ì™¼ìª½ êµ¬ì¡°ì—ì„œ í‘œí˜„ê°€ëŠ¥í•œ ê²ƒì€ ì˜¤ë¥¸ìª½ êµ¬ì¡°ì¸ Residual êµ¬ì¡°ì—ì„œë„ í‘œí˜„ ê°€ëŠ¥í•¨ì´ ì¦ëª… ë˜ì–´ ìˆë‹¤. ì§ê´€ì ìœ¼ë¡œ ë´¤ì„ë•ŒëŠ” Featureë¥¼ ë½‘ì•„ì„œ ì´ì „ Featureì™€ ë”í•œ ë‹¤ëŠ” ê²ƒì´ ì˜ ì´í•´ê°€ ì•ˆê°ˆ ìˆ˜ ë„ ìˆê² ì§€ë§Œ, ì´ëŸ°ì‹ìœ¼ë¡œ í–ˆì„ë•Œë„ ì¢Œì¸¡ì˜ ì¼ë°˜ì ì¸ Conv Layerì˜ êµ¬ì¡°ì™€ ìˆ˜í•™ì ìœ¼ë¡œ ë™ì¹˜ë¥¼ ì´ë£¬ë‹¤ëŠ” ê²ƒì„ ì•Œê³  ìˆì. - ë§¨ ì™¼ìª½ì´ Original ResidualNNì˜ êµ¬ì¡°ì´ê³  ê°€ìš´ë°ê°€ Pre-Activationì„ ì‚¬ìš©í•˜ëŠ” êµ¬ì¡°ì´ë‹¤. Densely Connected ConvNets(DenseNet) ê°„ë‹¨íˆ ë§í•˜ìë©´, ëª¨ë“  Layerë“¤ì´ ë‹¤ ì—°ê²°ë˜ì–´ ìˆëŠ” êµ¬ì¡°ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì²˜ìŒì— ì¼ë°˜ì ì¸ Conv Layerë¥¼ í†µí•´ Feature Mapì„ ë§Œë“¤ê³  ê·¸ëŸ° ë’¤ì— Dense Blockì„ ì´ìš©í•´ì„œ ë‹¤ë¥¸ ëª¨ë“  Conv Layerë“¤ê³¼ Denseí•˜ê²Œ ì—°ê²°ì‹œì¼œì¤€ë‹¤. ê·¸ ë‹¤ìŒ Conv Layerë¥¼ ì´ìš©í•´ì„œ channel ê°œìˆ˜ë¥¼ ì¡°ì •í•´ì£¼ê³ , Max Poolingì„ ì´ìš©í•´ì„œ ì˜ìƒí¬ê¸°ë¥¼ ì¤„ì—¬ì¤€ë‹¤. ì´ëŸ° ê³¼ì •ì„ ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•´ì„œ Featureë¥¼ ì¶”ì¶œí•œ í›„, ë§¨ ë§ˆì§€ë§‰ì€ FC Layerë¡œ êµ¬ì„±í•´ì£¼ì—ˆë‹¤. ìœ„ì˜ êµ¬ì¡°ì—ì„œ Dense Blockë“¤ì´ residual blockìœ¼ë¡œ ë°”ë€ë‹¤ë©´ ResNetì¸ ê²ƒì´ë‹¤. Pre-Activationêµ¬ì¡°ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì´ ResNetì„ ê³„ìŠ¹í•˜ê³  ìˆëŠ”ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆëŠ” ëª…í™•í•œ ë‚´ìš©ì´ë‹¤. ìœ„ì˜ ê·¸ë¦¼ì„ ì„¤ëª…í•˜ìë©´, ë¨¼ì € ì…ë ¥ìœ¼ë¡œ 3X3XChannelì˜ Feature mapì´ ë§Œë“¤ì–´ì§€ë©´ ê·¸ê²ƒì„ í˜„ì¬ Growth rateê°€ 4ì´ë¯€ë¡œ 3X3XChannelX4ë¡œ Feature mapì„ ìƒˆë¡­ê²Œ ë§Œë“¤ì–´ ì£¼ê²Œ ëœë‹¤. ê·¸ í›„ ë˜ë‹¤ë¥¸ Convolution Layerë¥¼ í†µí•´ ìƒˆë¡œì´ ë§Œë“¤ì–´ì§„ Feature mapì€ 3X3X4X4ê°€ ë  ê²ƒì´ë‹¤. ì™œëƒí•˜ë©´ input channelì€ ì´ì „ì˜ ë§Œë“¤ì–´ì§„ Feature mapì´ output channelì˜ ìˆ˜ë¥¼ 4ë¡œ ì •í–ˆê¸° ë•Œë¬¸ì— 4ì´ê³  output channelì€ ë§Œë“¤ê¸° ë‚˜ë¦„ì´ì§€ë§Œ ì²˜ìŒë¶€í„° 4ë¡œ ê³ ì •ì‹œì¼œ ë†“ì•˜ë‹¤ëŠ” ê°€ì •ì„ í•´ë³´ì. ê·¸ë ‡ë‹¤ë©´ ì´ì œ ì´ ë‘ê°œì˜ Feature mapì„ concatenateí•´ë³´ë©´ 3X3XChannelX4 + 3X3X4X4 = 3X3X(Channel+4)X4 ê°€ ë  ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ê·¸ ë’¤ì˜ Dense Block ë‚´ì˜ ëª¨ë“  convolution Layerê°€ ì§„í–‰ë  ë•Œë§ˆë‹¤ í•´ì£¼ë©´ ìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ë  ê²ƒì´ë©° ê²°êµ­ ì´ì „ Feature mapë“¤ì„ ëˆ„ì í•˜ëŠ” ì˜ë¯¸ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ìœ„ì˜ ê·¸ë¦¼ì—ì„œ C:ì²˜ìŒ ì…ë ¥ëœ channelì˜ ìˆ˜, k:growth rate, l:ëª‡ë²ˆì§¸ ê¹Œì§€ Growthë¥¼ í–ˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•œë‹¤. $C_{Bottle neck}$ì€ (C+K*l)ì´ ì»¤ì¡Œì„ ë•Œë¥¼ ëŒ€ë¹„í•´ ì¤„ì´ê¸°ìœ„í•œ ëª©ì ìœ¼ë¡œ ê³±í•´ì£¼ëŠ” ê²ƒì´ë‹¤.","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"data engineering basic(SQL Basic)","slug":"data_engineering_02","date":"2019-12-10T02:23:24.000Z","updated":"2019-12-13T06:46:53.388Z","comments":true,"path":"2019/12/10/data_engineering_02/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/10/data_engineering_02/","excerpt":"","text":"SQL(Structured Query Language)DB (Database) ë°ì´í„°ë¥¼ í†µí•©í•˜ì—¬ ê´€ë¦¬í•˜ëŠ” ë°ì´í„°ì˜ ì§‘í•© DBMS (Database Management system) DBë¥¼ ê´€ë¦¬í•˜ëŠ” ë¯¸ë“¤ì›¨ì–´ ì‹œìŠ¤í…œì„ ì˜ë¯¸ Database ë¶„ë¥˜ RDBMS(Relational Database Management System) NoSQL - ë°ì´í„° í…Œì´ë¸” ì‚¬ì´ì— í‚¤ê°’ìœ¼ë¡œ ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ë² ì´ìŠ¤ ex) Oracle, Mysql, Postgresql, Sqlite -ë°ì´í„° ì‚¬ì´ì˜ê´€ê³„ ì„¤ì •ìœ¼ë¡œ ìµœì í™”ëœ ìŠ¤í‚¤ë§ˆë¥¼ ì„¤ê³„ ê°€ëŠ¥ - ë°ì´í„° í…Œì´ë¸” ì‚¬ì´ì— ê´€ê³„ê°€ ì—†ì´ ì €ì¥í•˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ - ë°ì´í„° ì‚¬ì´ì˜ ê´€ê³„ê°€ ì—†ìœ¼ë¯€ë¡œ ë³µì¡ì„±ì´ ì¤„ê³  ë§ì€ ë°ì´í„°ë¥¼ ì €ì¥ ê°€ëŠ¥ RDBMS table í–‰(row)ê³¼ ì—´(column)ë¡œ ì´ë£¨ì–´ì ¸ ìˆëŠ” ë°ì´í„° ë² ì´ìŠ¤ë¥¼ ì´ë£¨ëŠ” ê¸°ë³¸ ë‹¨ìœ„ Storage Engine MyISAM : full text index ì§€ì›, table ë‹¨ìœ„ lock, selectê°€ ë¹ ë¦„, êµ¬ì¡° ë‹¨ìˆœ InnoDB : transaction ì§€ì›, row ë‹¨ìœ„ lock, ìì›ì„ ë§ì´ ì‚¬ìš©, êµ¬ì¡° ë³µì¡ Column í…Œì´ë¸”ì˜ ì„¸ë¡œì¶• ë°ì´í„° Field, Attribute ë¼ê³ ë„ ë¶ˆë¦¼ Row í…Œì´ë¸”ì˜ ê°€ë¡œì¶• ë°ì´í„° Tuple, Recode ë¼ê³ ë„ ë¶ˆë¦¼ Value í–‰(row)ê³¼ ì—´(column)ì— í¬í•¨ë˜ì–´ìˆëŠ” ë°ì´í„° Key í–‰(row)ì˜ ì‹ë³„ìë¡œ ì‚¬ìš© Relationship Schema ìŠ¤í‚¤ë§ˆ(schema)ëŠ” ë°ì´í„° ë² ì´ìŠ¤ì˜ êµ¬ì¡°ë¥¼ ë§Œë“œëŠ” ë””ìì¸ NoSQL NoSQL(Not Only SQL) RDBMSì˜ ì˜ì¡´ì ì¸ ê´€ê³„ê°€ ê°–ëŠ” í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì§„ ë°ì´í„°ë² ì´ìŠ¤ í™•ì¥ì„±ì´ ì¢‹ìŒ ë°ì´í„° ë¶„ì‚°ì²˜ë¦¬ ìš©ì´ ë°ì´í„° ì €ì¥ì´ ìœ ì—°í•¨ RDMBSì™€ ë‹¤ë¥´ê²Œ êµ¬ì¡°ì˜ ë³€ê²½ì´ ë¶ˆí•„ìš” Schema ë° Joinì´ ì—†ìŒ Join ê¸°ëŠ¥ì´ ì—†ìœ¼ë¯€ë¡œ ê°ê°ì˜ í…Œì´ë¸”ë§Œ ì‚¬ìš©ê°€ëŠ¥ collection ë³„ë¡œ ê´€ê³„ê°€ ì—†ê¸° ë•Œë¬¸ì— ëª¨ë“  ë°ì´í„°ê°€ ë“¤ì–´ìˆì–´ì•¼ í•˜ë¯€ë¡œ RDBMSë³´ë‹¤ ì €ì¥ê³µê°„ì´ ë” í•„ìš” ì €ì¥ë˜ëŠ” ë°ì´í„°ëŠ” Key-value í˜•íƒœì˜ JSON í¬ë©§ì„ ì‚¬ìš© selectëŠ” RDBMSë³´ë‹¤ ëŠë¦¬ì§€ë§Œ insertê°€ ë¹¨ë¼ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë² ì´ìŠ¤ì— ë§ì´ ì‚¬ìš© íŠ¸ëœì ì…˜(transaction)ì´ ì§€ì›ë˜ì§€ ì•ŠìŒ(ë™ì‹œìˆ˜ì •ì— ëŒ€í•œ ì‹ ë¢°ì„±ì´ ì§€ì›ë˜ì§€ ì•ŠìŒ) https://db-engines.com/en/ranking_trend Install MySQL(for Mac OS) ì£¼ì˜) 2ê°€ì§€ ë°©ë²•ì„ ì†Œê°œí•˜ì§€ë§Œ, Pythonì—ì„œ MySQLì„ í™œìš©í•  Userë“¤ì—ê²ŒëŠ” 1ë²ˆ ë°©ë²•ìœ¼ë¡œ ì„¤ì¹˜ë¥¼ í•´ì•¼í•œë‹¤ëŠ” ê²ƒì„ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤!!! brew(1ë²ˆë°©ë²•)ë¡œ ì„¤ì¹˜í•´ì•¼ pythonì˜ mysql clientë¥¼ ì‚¬ìš©í• ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°©ë²• 1) reference https://gist.github.com/operatino/392614486ce4421063b9dece4dfe6c21 Install12345$ brew install mysql@version_num$ brew tap homebrew/services $ brew services start mysql@version_num$ brew services list$ brew link mysql@version_num --force$ mysql -V ì•ìœ¼ë¡œ SQL ì ‘ì†ì‹œ ì‚¬ìš©í•  Password!!1$ mysqladmin -u root password 'yourpassword' Connect mysql server1$ mysql -u root -p ë°©ë²• 2) dmg íŒŒì¼ ë°›ì•„ì„œ install step 1) https://dev.mysql.com/downloads/mysql/5.7.html#downloadsì—ì„œ DMG íŒŒì¼ ë‹¤ìš´ë¡œë“œ step 2) ë‹¤ìš´ ë°›ì€ DMG íŒŒì¼ì„ ì‹¤í–‰ ì„¤ì¹˜ ì¤‘ê°„ì— ì„ì‹œ íŒ¨ìŠ¤ì›Œë“œë¥¼ ê¸°ì–µ step 3) ì‹œìŠ¤í…œ í™˜ê²½ì„¤ì •ì— ê°€ë©´ MySQLì´ ì„¤ì¹˜ ëœê²ƒì„ í™•ì¸ MySQL ì„œë²„ì˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì •ì§€ ë° ì‹¤í–‰, ì´ˆê¸°í™”, ì œê±°ë“±ì„ í• ìˆ˜ ìˆë‹¤. Start MySQL Server ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ì‹¤í–‰ ì•„ë˜ì˜ ê²½ë¡œë¡œ ì´ë™1$ cd /usr/local/mysql/bin Mysql ì„œë²„ì— ì ‘ì†1$ sudo ./mysql -p Password: (ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰ì„ ìœ„í•œ PCì˜ íŒ¨ìŠ¤ì›Œë“œ)Enter password: (ì„ì‹œë¡œ ë°œê¸‰ë°›ì€ DBì˜ íŒ¨ìŠ¤ì›Œë“œ ì…ë ¥) ì•„ë˜ì˜ mysql í”„ë¡¬í”„íŠ¸ê°€ ë‚˜ì˜¤ë©´ ì •ìƒ!! ì„¤ì¹˜ ì™„ë£Œ!!1mysql&gt; íŒ¨ìŠ¤ì›Œë“œ ë³€ê²½ ( qwer1234 ë¡œ ë³€ê²½í•  ê²½ìš° )12mysql&gt; ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;qwer1234&apos;;mysql&gt; FLUSH PRIVILEGES; mysql&gt; quit; ë³€ê²½í•œ íŒ¨ìŠ¤ì›Œë“œë¡œ ë‹¤ì‹œ ë¡œê·¸ì¸Mysql Basic Commandsystem12345678910111213# mysqlëª…ë ¹ì–´ ë¦¬ìŠ¤íŠ¸ í™•ì¸mysql&gt; help# í˜„ì¬ ìƒíƒœ ë³´ê¸°mysql&gt; status# mysql ì ‘ì† ì¢…ë£Œmysql&gt; exitmysql&gt; quit# íŒ¨ìŠ¤ì›Œë“œ ë³€ê²½ ( qwer1234 ë¡œ ë³€ê²½í•˜ëŠ” ê²½ìš° )mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'qwer1234'2.2 Database Database1234567891011121314151617# DB ëª©ë¡ ë³´ê¸°mysql&gt; show databases;# DB ë§Œë“¤ê¸° ( DBì´ë¦„ì„ testë¼ê³  í•˜ë ¤ë©´ )mysql&gt; create database test;# DB ì ‘ì†í•˜ê¸° ( DB ì´ë¦„ test )mysql&gt; use test;# í˜„ì¬ ì ‘ì†ì¤‘ì¸ DB í™•ì¸í•˜ê¸°mysql&gt; select database();# DB ì§€ìš°ê¸°mysql&gt; drop database test;# DB ì‚­ì œ í™•ì¸mysql&gt; show databases; Table1234567891011121314151617181920212223242526272829# í…Œì´ë¸” ë§Œë“¤ê¸°# ë¬¸ìì—´ name 20ì, age ìˆ«ì 3ì ì»¬ëŸ¼ì´ ìˆëŠ” í…Œì´ë¸”ì´ ìƒì„±mysql&gt; create table user ( name char(20), age int(3) );# í…Œì´ë¸” ëª©ë¡ í™•ì¸mysql&gt; show tables;# í…Œì´ë¸” êµ¬ì¡° í™•ì¸mysql&gt; desc user;mysql&gt; describe user;mysql&gt; explain user;# í…Œì´ë¸” ì´ë¦„ ë°”ê¾¸ê¸°(anotherë¡œ ë°”ê¾¸ê¸°)mysql&gt; rename table user to another;# í…Œì´ë¸” ì´ë¦„ ë°”ë€ê²ƒ í™•ì¸ mysql&gt; show tables;# í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€í•˜ê¸°mysql&gt; insert into another(name, age) values(\"alice\", 23);mysql&gt; insert into another(name, age) values(\"peter\", 30);# ì¶”ê°€ëœ ë°ì´í„° í™•ì¸í•˜ê¸°mysql&gt; select * from anther;# í…Œì´ë¸” ì§€ìš°ê¸°mysql&gt; drop table anther;# í…Œì´ë¸” ì‚­ì œëœê²ƒ í™•ì¸mysql&gt; show tables; Database Management Application for Mac OSstep 1) Install Sequel Pro https://www.sequelpro.com/ ê²½ë¡œì—ì„œ sequelproë¥¼ ë‹¤ìš´ ë°›ì•„ì„œ ì„¤ì¹˜ step 2) Connect Database Server ì•„ë˜ì™€ ê°™ì´ Host, Username, Passwordë¥¼ ì„¤ì •í•˜ì—¬ ì—°ê²° Sample Database Download https://dev.mysql.com/doc/index-other.htmlë§í¬ì—ì„œ Sample database ë¥¼ ë‹¤ìš´ í˜¹ì‹œë¼ë„ ì•ìœ¼ë¡œ ì €ì˜ ë¸”ë¡œê·¸ë¥¼ ë³´ì‹œë©´ì„œ ë”°ë¼í•´ë³´ì‹¤ ë¶„ë“¤ì€ world database, sakila database ë¥¼ ë‹¤ìš´ë°›ì•„ ì£¼ì„¸ìš”. sql íŒŒì¼ ì¶”ê°€1234/usr/local/mysql/bin ë””ë ‰í† ë¦¬ì—ì„œ ì•„ë˜ì™€ ê°™ì´ ì‹¤í–‰í•˜ë©´ sql íŒŒì¼ì„ import - import í•˜ê¸° ì „ì— world ë°ì´í„° ë² ì´ìŠ¤ê°€ ìˆì–´ì•¼ í•¨$ sudo ./mysql -p world &lt; (sql íŒŒì¼ ê²½ë¡œ)- brewë¡œ ì„¤ì¹˜í•œ ê²½ìš° ì•„ë˜ì™€ ê°™ì´ ì¶”ê°€$ mysql -u root -p world &lt; (sql íŒŒì¼ ê²½ë¡œ)","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering basic(Unixí™˜ê²½ ë° ì»¤ë§¨ë“œ)","slug":"data_engineering_01","date":"2019-12-09T08:48:07.000Z","updated":"2019-12-17T04:17:10.108Z","comments":true,"path":"2019/12/09/data_engineering_01/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/09/data_engineering_01/","excerpt":"","text":"Pipes and Filterscat : í•´ë‹¹ íŒŒì¼ ì „ì²´ë¥¼ printhead : í•´ë‹¹ íŒŒì¼ ì•ì˜ 10ì¤„ ì •ë„ë¥¼ printtail : í•´ë‹¹ íŒŒì¼ ë’¤ì˜ 20ì¤„ ì •ë„ë¥¼ print command &gt; file : ê¸°ì¡´ì˜ íŒŒì¼ ë‚´ìš©ì€ ì§€ìš°ê³  í˜„ì¬ commandí•œ ê²°ê³¼ íŒŒì¼ì— ì €ì¥command &gt;&gt; file : ê¸°ì¡´ì˜ íŒŒì¼ì— ë®ë¶™ì—¬ì„œ ê²°ê³¼ë¥¼ ì €ì¥(python appendê°™ì€ ëŠë‚Œ!) 12345# example.pyë¥¼ python3ë¡œ runí•˜ê³  ê·¸ ê²°ê³¼ë¥¼ result.txtíŒŒì¼ë¡œ ì €ì¥python3 example.py &gt; result.txt# example.pyë¥¼ python3ë¡œ runí•˜ê³  ê·¸ ê²°ê³¼ë¥¼ result.txtíŒŒì¼ì— ë®ë¶™ì—¬ì„œ ì €ì¥python3 example.py &gt;&gt; result.txt Shell script terminalì—ì„œ ë°”ë¡œ ëª…ë ¹ì–´ë¥¼ ì—¬ëŸ¬ê°œ ì‚¬ìš©í•˜ê³  ì‹¶ì„ë•Œ shell scriptë¥¼ ì‚¬ìš©í•˜ë©´ ëœë‹¤. ì˜ˆë¥¼ë“¤ì–´ ì•„ë˜ì˜ example.pyë¥¼ ì‹¤í–‰ì‹œì¼œ ìœ„ì—ì„œ commandë¥¼ í•œë²ˆì— ì‹¤í–‰ì‹œí‚¤ê³  ì‹¶ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë¨¼ì € example.pyë¥¼ ì‘ì„±í•œ í›„ì— command.sh íŒŒì¼ì—ëŠ” commandë“¤ì„ ì‘ì„±í•œ í›„ì— shell script íŒŒì¼ì„ runí•˜ë©´ ëœë‹¤. example.py12345678import sysdef main(): # command ë’¤ì— ë”°ë¼ì˜¤ëŠ” ì²«ë²ˆì§¸ ê¸€ìë¥¼ print print(sys.argv[1])if __name__==\"__main__\": main() command.sh123456#!/usr/bin/env bashpython3 example.py 1 &gt; result.txtpython3 example.py 2 &gt;&gt; result.txthead result.txtrm -rf result.txt example.py terminalì°½123#ê¶Œí•œì„ ì„¤ì •chmod +x command.sh./command.sh ë³´í†µì€ ìš°ë¦¬ê°€ deploy.shë¼ëŠ” íŒŒì¼ë¡œ ë§Œë“¤ì–´ ê·¸ ì•ˆì—ì„œ ì‘ì—…ì„ í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ,1234567891011121314# zipí˜•ì‹ìœ¼ë¡œ ë˜ì–´ìˆëŠ” ëª¨ë“ (*)íŒŒì¼ì„ ì‚­ì œí•´ë¼rm *.zip# ëª¨ë“  íŒŒì¼ì„ lisztfeverë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì••ì¶•í•´ë¼. -r ì˜µì…˜ì€ íŒŒì¼ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¶™ì—¬ì¤€ë‹¤.zip lisztfever.zip -r *# aws s3ë¼ëŠ” storageì— s3://areha/lisztfever/lisztfever.zip ì— í•´ë‹¹ pathì˜ íŒŒì¼ì„ ì‚­ì œaws s3 rm s3://areha/lisztfever/lisztfever.zip# s3ì—ê²Œ ë‹¤ì‹œ copyí•´ë¼aws s3 cp ./listzfever.zip s3://areha/lisztfever/listzfever.zip# aws lambda functionì„ updateí•´ë¼.aws lambda update-function-code --function-name listzfever --s3-buket areha --s3-key listzfever/listzfever.zip AWS Cloud Serviceë¨¼ì €, IAM(Identity and Access Management)ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ê² ë‹¤. ë‚´ê°€ ëˆ„êµ¬ì´ê³  ì–´ë–¤ Accessë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ë¥¼ ê´€ë¦¬í•˜ëŠ” ê³³ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ìƒˆë¡œìš´ Userë¥¼ ë“±ë¡ í•  ìˆ˜ ìˆë‹¤. ìœ„ì˜ Add Userë¥¼ í†µí•´ì„œ ìƒˆë¡œìš´ Userë¥¼ ë“±ë¡í•´ë³´ì. Access typeì€ ìš°ë¦¬ê°€ AWS clië¥¼ í†µí•´ì„œë„ ê´€ë¦¬í•˜ë¯€ë¡œ ì²«ë²ˆì§¸ì¸ Programmatic accessë¡œ ì„¤ì •í•œë‹¤. Permissionì„ ì£¼ëŠ” ë°©ì‹ì— ëŒ€í•œ ì„¤ì •í•˜ëŠ” ë¶€ë¶„ì´ë‹¤. Add user to group : í•œ Projectë¥¼ ì—¬ëŸ¬ëª…ì´ ê°™ì´ ì§„í–‰í•˜ì—¬ ì—¬ëŸ¬ëª…ì´ ê´€ë¦¬í•  ê²½ìš° ì‚¬ìš©. Copy permissions from existing user : ë§ ê·¸ëŒ€ë¡œ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” userì˜ permissionë“¤ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ Copyí•  ê²½ìš° ì‚¬ìš© Attach existing policies directly : AWSì— ì¡´ì¬í•˜ëŠ” ì •ì±…ë“¤ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ë°”ë¡œ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì‚¬ìš© ì˜ˆì „ì— ìˆë˜ ê³„ì •ì´ ë§Œë£Œëœê±¸ ëª¨ë¥´ê³  ìˆë‹¤ê°€ ê²°ì œë¥¼ ì•ˆí•´ë²„ë ¤ì„œâ€¦. ã… ã… ã…  ìƒˆë¡­ê²Œ ë§Œë“  ê³„ì •ìœ¼ë¡œ í•˜ëŠë¼ ë“±ë¡ëœ Userë“¤ì´ ì—†ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ 3ë²ˆì§¸ ì„¤ì •ìœ¼ë¡œ ë“¤ì–´ê°€ì„œ ëª¨ë“  ìµœìƒìœ„ permissionì„ ê°–ëŠ” AdministratorAccessë¥¼ ì£¼ì—ˆë‹¤. ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°€ê²Œ ë˜ë©´, tagë¥¼ ì„¤ì •í•˜ê²Œ ë˜ëŠ”ë°, ìš°ì„  ë„˜ì–´ê°€ê² ë‹¤.(ì´ ë¶€ë¶„ì€ ë‚˜ì¤‘ì— ì„¤ì •í•  ê²ƒì´ë‹¤.) ì•ì—ì„œ ì„¤ì •í•œ ì‚¬í•­ë“¤ì„ í™•ì¸í•˜ê³ , Create Userë¥¼ ëˆ„ë¥´ê²Œ ë˜ë©´ ì„¤ì •í•œëŒ€ë¡œ Userë¥¼ ìƒì„±í•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤. Access key ID, Secret access keyê°€ ìƒì„±ë˜ì–´ ë‚˜ì˜¤ëŠ”ë°, ì´ ì°½ì´ ë‹«íˆë©´, ë³¼ìˆ˜ ì—†ìœ¼ë¯€ë¡œ Download csvë¥¼ í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•œë‹¤. ì„¤ì¹˜ í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•œë‹¤. /usr/local/awsì— AWS CLIë¥¼ ì„¤ì¹˜í•˜ê³  /usr/local/bin ë””ë ‰í„°ë¦¬ì— symlink awsë¥¼ ìƒì„±í•œë‹¤. -b ì˜µì…˜ì„ ì‚¬ìš©í•˜ì—¬ symlinkë¥¼ ìƒì„±í•˜ë©´ ì‚¬ìš©ìì˜ $PATH ë³€ìˆ˜ì— ì„¤ì¹˜ ë””ë ‰í„°ë¦¬ë¥¼ ì§€ì •í•  í•„ìš”ê°€ ì—†ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë“  ì‚¬ìš©ìê°€ ì„ì˜ ë””ë ‰í„°ë¦¬ì—ì„œ awsë¥¼ ì…ë ¥í•˜ì—¬ AWS CLIë¥¼ í˜¸ì¶œ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤. 123curl \"https://s3.amazonaws.com/aws-cli/awscli-bundle.zip\" -o \"awscli-bundle.zip\"unzip awscli-bundle.zipsudo ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws ìœ„ì˜ ì„¤ì¹˜ê°€ ë‹¤ ëë‚˜ë©´, ì´ì œ aws cliì˜ configureë¥¼ ì„¤ì •í•´ ë³¼ ê²ƒì´ë‹¤. ì´ë¥¼ í†µí•´ ìš°ë¦¬ê°€ consoleì— ì ‘ì†í•˜ì§€ ì•Šê³ ë„ cli í™˜ê²½ì—ì„œë„ awsë¥¼ ì¡°ì‘í•  ìˆ˜ ìˆê²Œ ëœë‹¤. 12345aws configureAWS Access Key ID [None]: Access key IDAWS Secret Access Key [None]: Secret access keyDefault region name [None]: ap-northeast-2Default output format [None]:","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"ì‹¬ì¸µ ì‹ ê²½ë§ì˜ êµ¬ì¡°","slug":"deep_learning_03","date":"2019-12-07T15:00:00.000Z","updated":"2019-12-20T04:38:06.937Z","comments":true,"path":"2019/12/08/deep_learning_03/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/08/deep_learning_03/","excerpt":"","text":"ì‹¬ì¸µ ì‹ ê²½ë§ì˜ êµ¬ì¡° ì€ë‹‰ ê³„ì¸µ ì¶”ê°€ = íŠ¹ì§•ì˜ ë¹„ì„ í˜• ë³€í™˜ ì¶”ê°€!!ì„ í˜• ë³€í™˜ì˜ ì´í•´ ì„ í˜•ëŒ€ìˆ˜ì˜ ì„ í˜• ë³€í™˜ì„ í•¨ìˆ˜ì˜ ê°œë…ì—ì„œ ë³´ì•˜ì„ë•Œ, ì…ë ¥ ì°¨ì›(n)ì´ ì¶œë ¥ ì°¨ì›(m)ë³´ë‹¤ í¬ë‹¤ë©´ Onto(ì „ì‚¬í•¨ìˆ˜: ëª¨ë“  ê³µì—­ì´ ì¹˜ì—­ì´ ë˜ìˆëŠ” ìƒíƒœ)ê°€ ë  ìˆ˜ ìˆì§€ë§Œ, ê·¸ ë°˜ëŒ€ì¸ ê²½ìš°ëŠ” ì ì€ ì°¨ì›ì„ ê°–ëŠ” ì…ë ¥ë²¡í„°ì˜ ì°¨ì›ìœ¼ë¡œ ì¼ë¶€ë¶„ì˜ ì¶œë ¥ ë²¡í„°ì˜ ì°¨ì›ì„ ì»¤ë²„í•˜ëŠ” ê²ƒì´ ë˜ëŠ” ê²ƒì´ë¯€ë¡œ, ì „ì‚¬í•¨ìˆ˜ê°€ ë  ìˆ˜ ì—†ë‹¤. ë˜í•œ, ì´ëŸ° ì…ë ¥ì°¨ì›(n)ì´ ì¶œë ¥ ì°¨ì›(m)ë³´ë‹¤ ì‘ì€ ê²½ìš°ì˜ êµ¬ì¡°ë¥¼ ìš°ë¦¬ëŠ” ë”¥ëŸ¬ë‹ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì—ì„œë„ ë³¼ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ GANì´ë‚˜ Auto Encoderì˜ decoderêµ¬ì¡°ê°€ ê°€ì¥ ì‰¬ìš´ ì˜ˆì‹œì¼ ê²ƒì´ë‹¤. ì—¬ê¸°ì„œì˜ ì˜ë¬¸ì€ ê·¸ë ‡ë‹¤ë©´, ì¼ë¶€ë¶„ì˜ ì°¨ì›ìœ¼ë¡œ í”¼ì²˜ë¥¼ ì˜ ë°°ìš¸ ìˆ˜ ìˆëŠ”ì§€ê°€ ì˜ë¬¸ì¼ ê²ƒì´ë‹¤. í—ˆë‚˜, ê·¸ ì¼ë¶€ì˜ ì°¨ì›ì´ ì›ë˜ ê°–ê³  ìˆë˜ íŠ¹ì„±ì—ì„œ ë‚˜ì˜¬ë²•í•œ íŠ¹ì„±ë“¤ë§Œì„ ìƒì„±í•´ ì£¼ë¯€ë¡œ ê±±ì •í•˜ì§€ ì•Šì•„ë„ ëœë‹¤. ë˜í•œ, ì„ í˜•ì‹œìŠ¤í…œì˜ ê³±ìœ¼ë¡œ ë…¸ë“œë“¤ì˜ ì—°ì‚°ì„ í‘œí˜„í•  ìˆ˜ ìˆëŠ”ë°, ì—¬ê¸°ì„œ, ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì§•ë²¡í„°1ê³¼ íŠ¹ì§•ë²¡í„°2ê°„ì˜ ë°©í–¥ì´ ë¹„ìŠ·í•œ ì¦‰, Orthogonalí•˜ì§€ ì•Šê³  ë°©í–¥ì´ ë¹„ìŠ·í•œ ë²¡í„°ë¥¼ í†µí•´ ì—°ì‚°ì„ ì§„í–‰í•˜ë©´ ë‹¤ìŒ ì¸µì—ì„œëŠ” ë…¸ë“œë“¤ ì¤‘ì— ë¹„ìŠ·í•œ íŠ¹ì§•ì— ëŒ€í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆì„ ê²ƒì´ë‹¤. Inner productë¥¼ projectionì˜ ê°œë…ì—ì„œ ì‚´í´ë³´ë©´, ì–´ë– í•œ ë²¡í„°ê°€ ë‹¤ë¥¸ ë°©í–¥ì˜ ë²¡í„°ì— projectionì„ í•˜ëŠ” ê²ƒì€ ê·¸ projectioní•œ ë²¡í„°ê°€ ê·¸ ë°©í–¥ì˜ ë²¡í„°ê°€ ì–´ëŠ ì •ë„ì˜ ì„±ë¶„ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•˜ë¯€ë¡œ ì„ í˜•ëŒ€ìˆ˜ ì¸¡ë©´ì—ì„œ ìœ„ì—ì„œ ê° í”¼ì²˜ë“¤ê°„ì˜ ê³±ì˜ ì—°ì‚°ë“¤ì— ì˜í•œ ìƒˆë¡œìš´ í”¼ì²˜ë“¤ì˜ ìƒì„±ì€ projectionëœ ê¸¸ì´ë¥¼ ë¹„êµí•˜ëŠ” í–‰ìœ„ì™€ ë™ì¼í•  ê²ƒì´ë‹¤. ì—­ì „íŒŒ í•™ìŠµë²•ì˜ ê°œë… yë¥¼ êµ¬í•˜ë ¤ë©´ xì™€ zë¥¼ ì•Œì•„ì•¼ í•˜ëŠ”ë°, xì™€ zì—ëŠ” ì¤‘ë³µëœ ì—°ì‚°ì´ ìˆì–´ì„œ ë¹„íš¨ìœ¨ì ì´ë‹¤. ì²˜ìŒ ê³„ì‚°í•  ë•Œ ê°’ì„ ì €ì¥í•´ì£¼ì–´ì„œ ì¤‘ë³µê³„ì‚°ì´ ë°œìƒí•˜ì§€ ì•Šë„ë¡ í•´ì¤€ë‹¤. í•™ìŠµì„ ë§ˆì¹œ í›„ validation setì´ë‚˜ test setì— ì ìš©í•  ë•ŒëŠ” ë” ì´ìƒ í•™ìŠµì„ í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì´ ìˆœë°©í–¥ ì¶”ë¡ ë§Œì„ ì‚¬ìš©í•œë‹¤. ì‹¬ì¸µ ì‹ ê²½ë§ì˜ ìˆ˜í•™ì  ì´í•´ ì—­ì „íŒŒ í•™ìŠµì˜ í•„ìš”ì„± (N+1ë²ˆ) ì†ì‹¤í•¨ìˆ˜ë¥¼ í‰ê°€í•œë‹¤ê³  í•˜ëŠ”ë° ê·¸ ì´ìœ ëŠ” ê¸°ì¤€ì ì´ ë˜ëŠ” ì†ì‹¤í•¨ìˆ˜ë¥¼ ë¨¼ì € í•œë²ˆ ê³„ì‚°í•˜ê³  ë‚˜ë¨¸ì§€ í¸ë¯¸ë¶„ì‹œì— ê°€ê° Në²ˆ í‰ê°€í•˜ê¸° ë•Œë¬¸ì´ë‹¤. í•©ì„±í•¨ìˆ˜ì™€ ì—°ì‡„ ë²•ì¹™ ì—­ì „íŒŒ í•™ìŠµë²•ì˜ ìˆ˜ì‹ì  ì´í•´ ë¯¸ë¶„í•˜ê³ ì í•˜ëŠ” ê²½ë¡œ ì‚¬ì´ì— ìˆëŠ” ëª¨ë“  ë¯¸ë¶„ê°’ì„ ì•Œì•„ì•¼ ì›í•˜ëŠ” ë¯¸ë¶„ì„ êµ¬í•  ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ìˆ˜ì¹˜ì  ë¯¸ë¶„ì—ì„œëŠ” N+1ë²ˆì„ ê³„ì‚°í•˜ì—¬ì•¼ í–ˆì§€ë§Œ, ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¸í•´ ë‹¨ í•œë²ˆì˜ ì†ì‹¤í•¨ìˆ˜ í‰ê°€ë¡œ ë¯¸ë¶„ì„ êµ¬í•  ìˆ˜ ìˆë‹¤. ìˆ˜ì¹˜ ë¯¸ë¶„ì„ ì´ìš©í•œ ì‹¬ì¸µ ì‹ ê²½ë§ í•™ìŠµ123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134## ìˆ˜ì¹˜ ë¯¸ë¶„ì„ ì´ìš©í•œ ì‹¬ì¸µ ì‹ ê²½ë§ í•™ìŠµimport timeimport numpy as np## ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜epsilon = 0.0001def _t(x): return np.transpose(x)def _m(A, B): return np.matmul(A, B)def sigmoid(x): return 1 / (1 + np.exp(-x))def mean_squared_error(h, y): return 1 / 2 * np.mean(np.square(h - y))## ë‰´ëŸ° êµ¬í˜„class Neuron: def __init__(self, W, b, a): self.W = W self.b = b self.a = a # Gradient self.dW = np.zeros_like(self.W) self.db = np.zeros_like(self.b) def __call__(self, x): return self.a(_m(_t(self.W), x) + self.b) # activation((W^T)x + b)## ì‹¬ì¸µì‹ ê²½ë§ êµ¬í˜„class DNN: \"\"\" hidden_depth : hidden_layerì˜ ê°¯ìˆ˜ num_neuron : hidden_layer í•˜ë‚˜ë‹¹ neuronì˜ ê°¯ìˆ˜ num_input : input_layerì˜ neuronì˜ ê°¯ìˆ˜ num_output : output_layerì˜ neuronì˜ ê°¯ìˆ˜ activation : activation funtionìœ¼ë¡œ ì‚¬ìš©í•  í•¨ìˆ˜ \"\"\" def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid): # W, b initialize def init_var(i, o): return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,)) self.sequence = list() # First hidden layer W, b = init_var(num_input, num_neuron) self.sequence.append(Neuron(W, b, activation)) # Hidden layers for _ in range(hidden_depth - 1): W, b = init_var(num_neuron, num_neuron) self.sequence.append(Neuron(W, b, activation)) # Output layer # ë‹¨ìˆœíˆ ì‹¬ì¸µì‹ ê²½ë§ êµ¬í˜„ í›„ì— ìˆ˜ì¹˜ë¯¸ë¶„ì„ ì‚¬ìš©í•œ ì—­ì „íŒŒí•™ìŠµì„ ë³´ì´ê¸° ìœ„í•œ ì½”ë“œì´ë¯€ë¡œ # Output layerì˜ activation functionì„ ë”°ë¡œ ë°”ê¾¸ì§€ ì•Šê³  sigmoidë¡œ ì‚¬ìš©í•˜ê² ë‹¤. W, b = init_var(num_neuron, num_output) self.sequence.append(Neuron(W, b, activation)) def __call__(self, x): # layerë¥¼ callí•˜ëŠ” ê²ƒì€ ê²°êµ­ ìœ„ì—ì„œ ì •ì˜í•œ Neuronì˜ callì´ ë  ê²ƒì´ê³  # xëŠ” activation((W^T)x + b)ì´ ë  ê²ƒì´ë‹¤. for layer in self.sequence: x = layer(x) return x def calc_gradient(self, x, y, loss_func): def get_new_sequence(layer_index, new_neuron): # íŠ¹ì •í•œ ë³€ìˆ˜í•˜ë‚˜(weightë‚˜ bias)ë§Œ ë³€í™”ë¥¼ ì¤˜ì„œ ê·¸ ë•Œ lossê°€ ì–¼ë§ˆë‚˜ ë³€í•˜ëŠ”ì§€ë¥¼ ë³´ê³  # numerical gradientë¥¼ ê³„ì‚°í•˜ë ¤í•˜ê¸° ë•Œë¬¸ì— ë³€í™”ëœ ë³€ìˆ˜ê°€ ìˆëŠ” ìƒˆë¡œìš´ Sequenceê°€ í•„ìš”í•˜ë‹¤. new_sequence = list() for i, layer in enumerate(self.sequence): if i == layer_index: new_sequence.append(new_neuron) else: new_sequence.append(layer) return new_sequence def eval_sequence(x, sequence): for layer in sequence: x = layer(x) return x loss = loss_func(self(x), y) for layer_id, layer in enumerate(self.sequence): # iterate layer for w_i, w in enumerate(layer.W): # iterate W (row) for w_j, ww in enumerate(w): # iterate W (col) W = np.copy(layer.W) W[w_i][w_j] = ww + epsilon new_neuron = Neuron(W, layer.b, layer.a) new_seq = get_new_sequence(layer_id, new_neuron) h = eval_sequence(x, new_seq) num_grad = (loss_func(h, y) - loss) / epsilon # (f(x+eps) - f(x)) / epsilon layer.dW[w_i][w_j] = num_grad for b_i, bb in enumerate(layer.b): # iterate b b = np.copy(layer.b) b[b_i] = bb + epsilon new_neuron = Neuron(layer.W, b, layer.a) new_seq = get_new_sequence(layer_id, new_neuron) h = eval_sequence(x, new_seq) num_grad = (loss_func(h, y) - loss) / epsilon # (f(x+eps) - f(x)) / epsilon layer.db[b_i] = num_grad # gradientë¥¼ ê³„ì‚°í•  ë•Œ lossë¥¼ returní•´ì•¼ í•™ìŠµê³¼ì •ì— lossê°€ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ë¥¼ ì•Œ ìˆ˜ ìˆê¸°ë•Œë¬¸ì— return í•´ì¤€ë‹¤. return loss## ê²½ì‚¬í•˜ê°•ë²•def gradient_descent(network, x, y, loss_obj, alpha=0.01): loss = network.calc_gradient(x, y, loss_obj) for layer in network.sequence: layer.W += -alpha * layer.dW layer.b += -alpha * layer.db return loss## ë™ì‘ í…ŒìŠ¤íŠ¸x = np.random.normal(0.0, 1.0, (10,))y = np.random.normal(0.0, 1.0, (2,))dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=sigmoid)t = time.time()for epoch in range(100): loss = gradient_descent(dnn, x, y, mean_squared_error, 0.01) print('Epoch &#123;&#125;: Test loss &#123;&#125;'.format(epoch, loss))print('&#123;&#125; seconds elapsed.'.format(time.time() - t)) ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ì‹¬ì¸µ ì‹ ê²½ë§ í•™ìŠµ123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137## ì—­ì „íŒŒ í•™ìŠµì„ ì´ìš©í•œ ì‹¬ì¸µ ì‹ ê²½ë§ í•™ìŠµimport timeimport numpy as np## ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜def _t(x): return np.transpose(x)def _m(A, B): return np.matmul(A, B)## Sigmoid êµ¬í˜„class Sigmoid: def __init__(self): # ê³±ì˜ í˜•íƒœë¡œ ë‚˜ì˜¤ê²Œ ë˜ë¯€ë¡œ ì²˜ìŒì— 1ë¡œí•´ì„œ ì¶”í›„ì— ì…ë ¥ë  ìˆ˜ì¹˜ì— ì˜í–¥ì„ ëœ ì£¼ê²Œ í•´ì¤€ë‹¤. self.last_o = 1 def __call__(self, x): self.last_o = 1 / (1.0 + np.exp(-x)) return self.last_o def grad(self): # sigmoid(x) * (1- sigmoid(x)) return self.last_o*(1-self.last_o)## Mean Squared Error êµ¬í˜„class MeanSquaredError: def __init__(self): # chain ruleì„ í•  ë•Œ MSEë¡œ ë¶€í„° gradientë¥¼ ê³„ì†í•´ì„œ ê°€ì ¸ì™€ì•¼í•˜ë¯€ë¡œ ì €ì¥í•´ë†“ê¸° ìœ„í•´ self.dh = 1 self.last_diff = 1 def __call__(self, h, y): # 1/2 * mean((h - y)^2) self.last_diff = h - y return 1 / 2 * np.mean(np.square(h - y)) def grad(self): # h - y return self.last_diff## ë‰´ëŸ° êµ¬í˜„class Neuron: def __init__(self, W, b, a_obj): self.W = W self.b = b # activationì´ ì´ì „ê³¼ ë‹¤ë¥´ê²Œ classë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë¯€ë¡œ instanctiationì„ í•´ì£¼ì–´ì•¼í•œë‹¤. self.a = a_obj() # gradient self.dW = np.zeros_like(self.W) self.db = np.zeros_like(self.b) self.dh = np.zeros_like(_t(self.W)) ## ì•„ë˜ì˜ grad_Wë¥¼ ìœ„í•´ ì €ì¥í•´ë†“ëŠ”ë‹¤. ## Wë¡œ ë¯¸ë¶„í–ˆì„ ê²½ìš° ì´ì „ ì…ë ¥ì„ ê°–ê³  ìˆì–´ì•¼ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ self.last_x = np.zeros((self.W.shape[0])) self.last_h = np.zeros((self.W.shape[1])) def __call__(self, x): self.last_x = x self.last_h = _m(_t(self.W), x) + self.b return self.a(self.last_h) def grad(self): # dy/dh = W return self.W * self.a.grad() def grad_W(self, dh): grad = np.ones_like(self.W) grad_a = self.a.grad() for j in range(grad.shape[1]): # dy/dw = x grad[:, j] = dh[j] * grad_a[j] * self.last_x return grad def grad_b(self, dh): # dy/db = 1 return dh * self.a.grad() * 1## ì‹¬ì¸µì‹ ê²½ë§ êµ¬í˜„class DNN: def __init__(self, hidden_depth, num_neuron, input, output, activation=Sigmoid): def init_var(i, o): return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,)) self.sequence = list() # First hidden layer W, b = init_var(input, num_neuron) self.sequence.append(Neuron(W, b, activation)) # Hidden Layers for index in range(hidden_depth): W, b = init_var(num_neuron, num_neuron) self.sequence.append(Neuron(W, b, activation)) # Output Layer W, b = init_var(num_neuron, output) self.sequence.append(Neuron(W, b, activation)) def __call__(self, x): for layer in self.sequence: x = layer(x) return x def calc_gradient(self, loss_obj): loss_obj.dh = loss_obj.grad() # forë¬¸ì—ì„œ í•œë²ˆì— ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œ loss objectë¥¼ ë„£ì–´ì¤€ë‹¤. self.sequence.append(loss_obj) # back_propagation loop for i in range(len(self.sequence) -1, 0 , -1): l1 = self.sequence[i] l0 = self.sequence[i - 1] l0.dh = _m(l0.grad(), l1.dh) l0.dw = l0.grad_W(l1.dh) l0.db = l0.grad_b(l1.dh) # loss objectê°€ ë“¤ì–´ ìˆìœ¼ë©´ ì¶œë ¥ì„ ì–»ì§€ ëª»í•˜ê³  loss ë§Œ ì–»ê²Œ ë  ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. self.sequence.remove(loss_obj)## ê²½ì‚¬í•˜ê°• í•™ìŠµë²•def gradient_descent(network, x, y, loss_obj, alpha=0.01): loss = loss_obj(network(x), y) # Forward inference network.calc_gradient(loss_obj) # Back-propagation for layer in network.sequence: layer.W += -alpha * layer.dW layer.b += -alpha * layer.db return loss## ë™ì‘ í…ŒìŠ¤íŠ¸x = np.random.normal(0.0, 1.0, (10,))y = np.random.normal(0.0, 1.0, (2,))t = time.time()dnn = DNN(hidden_depth=5, num_neuron=32, input=10, output=2, activation=Sigmoid)loss_obj = MeanSquaredError()for epoch in range(100): loss = gradient_descent(dnn, x, y, loss_obj, alpha=0.01) print('Epoch &#123;&#125;: Test loss &#123;&#125;'.format(epoch, loss))print('&#123;&#125; seconds elapsed.'.format(time.time() - t))","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"ì‰½ê²Œ ë°°ìš°ëŠ” ê²½ì‚¬í•˜ê°• í•™ìŠµë²•","slug":"deep_learning_02","date":"2019-12-07T15:00:00.000Z","updated":"2019-12-13T03:55:29.137Z","comments":true,"path":"2019/12/08/deep_learning_02/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/08/deep_learning_02/","excerpt":"","text":"ì‰½ê²Œ ë°°ìš°ëŠ” ê²½ì‚¬í•˜ê°• í•™ìŠµë²• ì–´ë–¤ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠëƒì— ë”°ë¼ì„œ í•™ìŠµì´ ì–´ë–»ê²Œ ì´ë£¨ì–´ì§ˆ ê²ƒì¸ì§€, ê·¸ë¦¬ê³  í•™ìŠµì„ í•  ë•Œ ì •ë‹µì˜ í˜•íƒœë¥¼ ê²°ì •í•˜ê¸° ë•Œë¬¸ì— ì†ì‹¤ í•¨ìˆ˜ëŠ” ì¤‘ìš”í•˜ë‹¤! Traning Dataë¥¼ Modelì— ì…ë ¥í•´ ìš°ë¦¬ê°€ í•™ìŠµì‹œí‚¤ê³ ì í•˜ëŠ” Trainable Parametersë¥¼ ì–»ê²Œ ë˜ëŠ”ë° Trainable Parametersë“¤ì„ inputsìœ¼ë¡œ ë³´ê³  outputsì„ í•™ìŠµê²°ê³¼ì¸ Loss Functionìœ¼ë¡œ ìƒê°í•˜ë©´, ì•Œê³ ë¦¬ì¦˜ í•™ìŠµì€ ì…ë ¥ì„ ë°”ê¿”ê°€ë©´ì„œ, ì¶œë ¥ê°’ì´ ì ì  ì‘ì•„ì§€ê²Œ í•˜ëŠ” ê²ƒì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ê²°êµ­ ì•Œê³ ë¦¬ì¦˜ í•™ìŠµì€ ì…ë ¥ì„ ë°”ê¿”ê°€ë©´ì„œ, ì¶œë ¥ê°’ì´ ì ì  ì‘ì•„ì§€ê²Œ í•˜ëŠ” ê²ƒì´ë¼ëŠ” ê´€ì ì—ì„œ ìµœì í™” ì´ë¡ ì˜ ëª©í‘œì™€ ë™ì¼í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤. ê²½ì‚¬ í•˜ê°• í•™ìŠµë²• ë¬´ì°¨ë³„ ëŒ€ì…ë²•ì€ ë²”ìœ„ë¥¼ ì•Œì•„ì•¼í•˜ê³  ë²”ìœ„ë¥¼ ì•ˆë‹¤í•´ë„ stepì„ ì´˜ì´˜íˆ ì¡°ì‚¬í•´ì•¼ í•˜ë¯€ë¡œ ê³„ì‚° ë³µì¡ë„ê°€ ë†’ë‹¤. ì ê²Œ ëŒ€ì…í•´ ë³´ê³  ë‹µì„ ì°¾ì„ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ìƒê°í•˜ë‹¤ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ ë°œì „ í•˜ê²Œ ë˜ì—ˆë‹¤. ìµœì í™” ì´ë¡ ê³¼ ìˆ˜í•™ì  í‘œí˜„ ìˆ˜ì¹˜ì  ë°©ë²•ì˜ ëŒ€í‘œì ì¸ ë°©ë²•ì´ ê²½ì‚¬í•˜ê°•ë²•ì´ë‹¤. ì‹¬í™” ê²½ì‚¬ í•˜ê°• í•™ìŠµë²• ê²½ì‚¬í•˜ê°• í•™ìŠµë²•ì˜ ë‹¨ì ë“¤ì„ ê·¹ë³µí•œ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ì. ê²½ì‚¬í•˜ê°•ë²•ì€ ì•ˆì¥ì ì—ì„œ ê¸°ìš¸ê¸°ê°€ 0ì´ ë˜ë¯€ë¡œ ë²—ì–´ë‚˜ì§€ ëª»í•˜ê²Œ ë˜ëŠ” ë¬¸ì œì ì´ ìˆë‹¤. ì´ë™ ë²¡í„°ê°€ ì´ì „ ê¸°ìš¸ê¸°ì— ì˜í–¥ì„ ë°›ë„ë¡ í•˜ëŠ” ë°©\u001dë²• ì´ì „ì˜ ì†ë„ì— ì˜í–¥ì„ ë°›ëŠ” ë°©ë²•ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì¥ì  : Local minimumê³¼ noiseì— ëŒ€ì²˜ ê°€ëŠ¥ ë‹¨ì  : ê²½ì‚¬í•˜ê°•ë²•ì€ ë‹¨ìˆœíˆ$x_{t-1}$ì´ë™ë²¡í„°($v_{t}$)ë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ, ê²½ì‚¬ í•˜ê°•ë²• ëŒ€ë¹„ 2ë°°ì˜ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš© ë³€ìˆ˜ë³„ë¡œ learning rateê°€ ë‹¬ë¼ì§€ê²Œ ì¡°ì ˆí•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ $x=[x_{1}, x_{2}, x_{3},â€¦,x_{n}]$ì´ ì¡´ì¬í• ë•Œ ì–´ë–¤ ë³€ìˆ˜ëŠ” ê¸°ìš¸ê¸°ë¥¼ í¬ê²Œ ê°€ì ¸ê°€ê³  ì–´ë–¤ ë³€ìˆ˜ëŠ” ê¸°ìš¸ê¸°ë¥¼ ì‘ê²Œ ê°€ì ¸ê°ˆ ê²½ìš° ì²˜ìŒì— ê¸°ìš¸ê¸°ë¥¼ í¬ê²Œ ê°€ì ¸ê°€ì§€ ëª»í•œë‹¤ë©´ local minimumì— ë¹ ì§€ê¸° ì‰¬ìš´ ë¬¸ì œì ì´ ìˆë‹¤. ì´ëŸ° ë¬¸ì œì ì„ í•´ê²°í•˜ê³ ì ë³€ìˆ˜ë³„ë¡œ learning rateë¥¼ ë‹¤ë¥´ê²Œ ê°€ì ¸ê°€ëŠ” ì•Œê³ ë¦¬ì¦˜ì¸ Ada Grad íƒ„ìƒëœ ê²ƒì´ë‹¤. ì¥ì  : $g_{t}$ê°€ ëˆ„ì ë˜ì–´ ì»¤ì§„ ê²ƒì€ í•™ìŠµì´ ê·¸ë§Œí¼ ë§ì´ ëœ ê²ƒì´ë¯€ë¡œ í•™ìŠµì´ ë§ì´ ë³€ìˆ˜ëŠ” í•™ìŠµìœ¨ì„ ê°ì†Œì‹œì¼œ, ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì´ ì˜ í•™ìŠµë˜ë„ë¡ í•œë‹¤. ë‹¨ì  : $g_{t}$ ê°€ ê³„ì†í•´ì„œ ì»¤ì ¸ì„œ í•™ìŠµì´ ì˜¤ë˜ ì§„í–‰ë˜ë©´ learning rateê°€ 0ã…‡ì— ê°€ê¹Œì›Œì§€ë¯€ë¡œ ë”ì´ìƒ í•™ìŠµì´ ì´ë£¨ì–´ì§€ì§€ ì•ŠëŠ” ë‹¨ì ì´ ìˆë‹¤. gradientì˜ í¬ê¸°ë¥¼ ì œê³±í•œ ë²¡í„°(gradientë²¡í„°ì˜ L2-norm)ë¥¼ ëˆ„ì í•©ì„ í•´ì„œ ì ê²Œ í•™ìŠµë˜ëŠ” ë³€ìˆ˜ë“¤ì„ ë” í•™ìŠµì‹œì¼œ ì£¼ë„ë¡í–ˆì§€ë§Œ epochë‚˜ batchsizeë“± ë°˜ë³µ ì‹œí‚¤ëŠ” parameterì˜ valueê°€ ë†’ì•„ì§ˆìˆ˜ë¡ ì˜¤ë˜ ì§„í–‰ë˜ì–´ ëˆ„ì í•©ì´ ì»¤ì§€ê²Œ ë˜ë©´ ë” ì´ìƒ í•™ìŠµì´ ë˜ì§€ ì•ŠëŠ” ë¬¸ì œì ì„ ê°œì„ í•œ ë°©ë²•ì´ë‹¤. ìœ„ì˜ ì‹ì—ì„œ $\\gamma$ê°’ì€ 0~1ê°’ì„ ê°–ê²Œ ë˜ë©°, ì´ ê°’ì„ í†µí•´ ì´ì „ì˜ gradient ëˆ„ì í•©ì„ ê°ì†Œì‹œí‚¤ëŠ” íš¨ê³¼ë¥¼ ì£¼ë©´ì„œ ìƒˆë¡œìš´ gradientì˜ ê°’ì„ ì«“ì•„ê°ˆ ìˆ˜ ìˆë„ë¡ ê°œì„ í•˜ì˜€ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, ë³€ìˆ˜ ê°„ì˜ ìƒëŒ€ì ì¸ í•™ìŠµìœ¨ ì°¨ì´ëŠ” ìœ ì§€í•˜ë©´ì„œ$g_{t}$ê°€ ë¬´í•œì • ì»¤ì§€ì§€ ì•Šì•„ í•™ìŠµì„ ì˜¤ë˜ í•  ìˆ˜ ìˆë‹¤. RMSpropê³¼ Momentumì˜ ì¥ì ì„ ê²°í•©í•œ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ëŒ€ë¶€ë¶„ì˜ ì½”ë“œì— ì´ Adam optimizationì„ ì‚¬ìš©í•œë‹¤. ê²½ì‚¬ í•˜ê°•ë²•\u001dì„ ì´ìš©í•œ ì–•ì€ ì‹ ê²½ë§ í•™ìŠµ123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112# ê²½ì‚¬ í•˜ê°•ë²•ì„ ì´ìš©í•œ ì–•ì€ ì‹ ê²½ë§ í•™ìŠµimport tensorflow as tfimport numpy as np## í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •epochs = 1000## ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì •ì˜### ì–•ì€ ì‹ ê²½ë§#### ì…ë ¥ ê³„ì¸µ : 2, ì€ë‹‰ ê³„ì¸µ : 128 (Sigmoid activation), ì¶œë ¥ ê³„ì¸µ : 10 (Softmax activation)# kerasì˜ ëª¨ë“ˆì„ ìƒì†í•´ì„œ Modelì„ êµ¬í˜„class MyModel(tf.keras.Model): def __init__(self): # ìƒì†ì„ í•œ ê²½ìš°ì—ëŠ” ìƒì†ì„ í•œ ìƒìœ„ classë¥¼ initializeí•˜ëŠ” ê²ƒì„ ìŠì–´ë²„ë¦¬ì§€ ë§ì! super(MyModel, self).__init__() # ì•„ë˜ì˜ input_dimì„ ì ì–´ì¤„ í•„ìš”ëŠ” ì—†ë‹¤. ì‹¤ì œ ë°ì´í„°ê°€ ë“¤ì–´ì˜¬ë•Œ ì •ì˜ ë˜ê¸° ë–„ë¬¸ì´ë‹¤. self.d1 = tf.keras.layers.Dense(128, input_dim=2, activation=\"sigmoid\") self.d2 = tf.keras.layers.Dense(10, input_dim=128, activation=\"softmax\") # Modelì´ ì‹¤ì œ callì´ ë ë•Œ ì…ë ¥ì—ì„œ ì¶œë ¥ìœ¼ë¡œ ì–´ë–»ê²Œ ì—°ê²°ì´ ë  ê²ƒì¸ì§€ë¥¼ ì •ì˜ def call(self, x, training=None, mask=None): x = self.d1(x) return self.d2(x)## í•™ìŠµ ë£¨í”„ ì •ì˜@tf.function# tensorflowì˜ Auto Graphë¥¼ í†µí•´ ì‰½ê²Œ êµ¬í˜„ê°€ëŠ¥í•˜ë‹¤.# function ë‚´ì˜ python ë¬¸ë²•ìœ¼ë¡œ ì…ë ¥ëœ ëª¨ë“  tensor ì—°ì‚°ë“¤ì„ tf.functionì— ì˜í•´ì„œ# ìµœì í™”ëœë‹¤.def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_metric): # Gradientë¥¼ ê³„ì‚°í•˜ê¸°ìœ„í•œ with tf.GradientTape() as tape: predictions = model(inputs) loss = loss_object(labels, predictions) # lossë¥¼ modelì˜ trainable_variables(W,b)ë¡œ ê°ê° ë¯¸ë¶„í•´ì„œ gradientë¥¼ êµ¬í•œê²ƒ. # lossëŠ” scalarì´ê³ , model.trainable_variablesëŠ” ë²¡í„°ì´ë¯€ë¡œ ê²°ê³¼ ë˜í•œ ë²¡í„°ê°€ ë  ê²ƒì´ë‹¤. gradients = tape.gradient(loss, model.trainable_variables) # ê° gradientì™€ trainable_variablesë“¤ì´ optimizerë¡œ í•™ìŠµ optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # lossë¥¼ ì¢…í•© train_loss(loss) # matric train_metric(labels, predictions)## ë°ì´í„°ì…‹ ìƒì„±, ì „ì²˜ë¦¬np.random.seed(0)pts = []labels = []center_pts = np.random.uniform(-8.0, 8.0, size=(10, 2))for label, center_pt in enumerate(center_pts): for _ in range(100): pts.append(center_pt + np.random.randn(*center_pt.shape)) labels.append(label)# GPUë¥¼ ì‚¬ìš©í•˜ê²Œ ëœë‹¤ë©´ ìœ„ì˜ MyModel classì—ì„œ initialize í• ë•Œ# Layerì— ë”°ë¡œ dtypeì„ ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ float32ë¡œ ì„¤ì •ë˜ë¯€ë¡œ ë™ì¼í•˜ê²Œ í•´ì£¼ê¸° ìœ„í•´ type ì¬ì„¤ì •pts = np.stack(pts, axis=0).astype(np.float32)# ì´ë¯¸ integerì´ë¯€ë¡œ ë°”ê¿€ í•„ìš”ê°€ ì—†ìŒ.labels = np.stack(labels, axis=0)# ìœ„ì—ì„œ ë§Œë“  ë°ì´í„°ë¥¼ train data setìœ¼ë¡œ ë³€í˜•# train_dsëŠ” iterableí•œ objectê°€ ëœë‹¤.# 1000ê°œë¥¼ ì„ì–´ batch_sizeë¥¼ 32ê°œë¡œ í•´ì„œ êµ¬ì„±í•´ì¤€ë‹¤.train_ds = tf.data.Dataset.from_tensor_slices((pts, labels)).shuffle(1000).batch(32)print(pts.shape)print(labels.shape)## ëª¨ë¸ ìƒì„±model = MyModel()## ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì„¤ì •### CrossEntropy, Adam Optimizerloss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()## í‰ê°€ ì§€í‘œ ì„¤ì •### Accuracytrain_loss = tf.keras.metrics.Mean(name='train_loss')train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')## í•™ìŠµ ë£¨í”„for epoch in range(epochs): #ìœ„ì—ì„œ batch_sizeë¥¼ 32ë¡œ í–ˆìœ¼ë¯€ë¡œ í•œë²ˆ ì‹¤í–‰ì‹œ 32ê°œì”© ë‚˜ì˜´. for x, label in train_ds: train_step(model, x, label, loss_object, optimizer, train_loss, train_accuracy) template = 'Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;' print(template.format(epoch+1, train_loss.result(), train_accuracy.result()*100))## ë°ì´í„°ì…‹ ë° í•™ìŠµ íŒŒë¼ë¯¸í„° ì €ì¥# ì••ì¶•í•´ì„œ ì—¬ëŸ¬ê°œì˜ Numpy Objectë“¤ì„ ì €ì¥í•  ìˆ˜ ìˆë‹¤.np.savez_compressed('ch2_dataset.npz', inputs=pts, labels=labels)W_h, b_h = model.d1.get_weights()W_o, b_o = model.d2.get_weights()# weightëŠ” tensorflowì—ì„œ ì‚¬ìš©í•˜ê³  ìˆëŠ” conventionì´ë‘# shallowNNì„ êµ¬í˜„í•  ë•Œ ì‚¬ìš©í–ˆë˜ conventionì´ ì¢€ ë‹¤ë¥´ë‹¤.W_h = np.transpose(W_h)W_o = np.transpose(W_o)np.savez_compressed('ch2_parameters.npz', W_h=W_h, b_h=b_h, W_o=W_o, b_o=b_o)","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"ê°€ì¥ ë‹¨ìˆœí•œ ì‹ ê²½ë§ì„ í†µí•œ ì‘ë™ì›ë¦¬","slug":"deep_learning_01","date":"2019-12-05T15:00:00.000Z","updated":"2019-12-08T12:33:35.852Z","comments":true,"path":"2019/12/06/deep_learning_01/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/06/deep_learning_01/","excerpt":"","text":"Nodeê°€ ë‹¨ì¼ ë‰´ëŸ° ì—°ì‚°ì„ ì˜ë¯¸í•œë‹¤ê³  í–ˆëŠ”ë° ì—¬ê¸°ì„œì˜ ë‹¨ì¼ ë‰´ëŸ° ì—°ì‚°ì´ë€ inputì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•˜ê³  í•©ê³„ë¥¼ ë‚¸ í›„ì— activation functionê¹Œì§€ í†µê³¼ì‹œí‚¤ëŠ” ê³¼ì •ì„ ì˜ë¯¸í•œë‹¤. ìœ„ì˜ ì‹ì—ì„œ í¸í–¥ì„ ìŠì–´ë²„ë¦¬ì§€ ë§ì!! ì˜ˆë¥¼ë“¤ë©´, í¸í–¥ì´ ì—†ë‹¤ë©´ ì›ì ì„ ì§€ë‚˜ëŠ” ì„ ë§Œ í‘œí˜„í•  ìˆ˜ ìˆì§€ë§Œ í¸í–¥ì„ í†µí•´ ì›ì ì„ ì§€ë‚˜ì§€ ì•ŠëŠ” ì„ ë“¤ë„ í‘œí˜„í•  ìˆ˜ ìˆê²Œ í•  ìˆ˜ ìˆë‹¤. ì°¸ê³ ë¡œ íŠ¹ë³„íˆ í¸í–¥ì´ ì—†ëŠ” ê²½ìš°ë„ ìˆì„ ìˆœ ìˆë‹¤. íšŒê·€ ë¬¸ì œ ì–´ë–¤ ì…ë ¥ì´ ë“¤ì–´ì™”ì„ ë–„ ì¶œë ¥ì´ ì—°ì†ì ì¸ ê°’ì„ ê°€ì§ˆ ë•Œ Regressionì„ ì‚¬ìš©í•œë‹¤. ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œ Softmaxì˜ ë¶„ëª¨ì— ì˜í•´ì„œ ë‹¤ë¥¸ í´ë˜ìŠ¤ì— ëŒ€í•œ í•™ìŠµì—ë„ ì˜í–¥ì„ ì¤€ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ë¶„ëª¨ëŠ” ë‹¤ë¥¸ í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í•œ í™•ë¥ ë˜í•œ ë”í•´ì£¼ê¸° ë•Œë¬¸ì´ë‹¤. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# ì–•ì€ ì‹ ì…©ë§ì„ ì´ìš©í•œ ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œimport numpy as npimport matplotlib.pyplot as plt## í•¨ìˆ˜ êµ¬í˜„# Sigmoid í•¨ìˆ˜def sigmoid(x): return 1/(1+np.exp(-x))# Softmax í•¨ìˆ˜def softmax(x): return np.exp(x)/np.sum(np.exp(x))# ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ì •ì˜class ShallowNN: # ì•„ë˜ì˜ Wì™€ bì— ì ì ˆí•œ ê°’ì€ ì¶”í›„ì— ë„£ì–´ì£¼ê¸° ë•Œë¬¸ì— í˜„ì¬ëŠ” 0ìœ¼ë¡œ ì¡ìŒ def __init__(self, num_input, num_hidden, num_output): self.W_h = np.zeros((num_hidden, num_input), dtype=np.float32) self.b_h = np.zeros((num_hidden, 1), dtype=np.float32) self.W_o = np.zeros((num_output, num_hidden), dtype=np.float32) self.b_o = np.zeros((num_output, 1), dtype=np.float32) # NNì˜ ì—°ì‚°ì„ call í˜•íƒœë¡œ í•´ì„œ ì‘ì„± def __call__(self, x): h = sigmoid(np.matmul(self.W_h, x) + self.b_h) return softmax(np.matmul(self.W_o, h) + self.b_o)# ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°dataset = np.load('ch2_dataset.npz')inputs = dataset['inputs']labels = dataset['labels']print(labels.shape)print(inputs.shape)# ShallowNN Model ìƒì„±model=ShallowNN(num_input=inputs.shape[1], num_hidden=128, num_output=10)# ì‚¬ì „ì— í•™ìŠµëœ íŒŒë¼ë¯¸í„° ë¶ˆëŸ¬ì˜¤ê¸°weights = np.load('ch2_parameters.npz')model.W_h = weights['W_h']model.b_h = weights['b_h']model.W_o = weights['W_o']model.b_o = weights['b_o']# ëª¨ë¸ ê²°ê³¼ í”„ë¦°íŠ¸outputs = []for point, label in zip(inputs, labels): output = model(point) outputs.append(np.argmax(output)) print(np.argmax(output), label)outputs = np.stack(outputs, axis=0)# ì •ë‹µ í´ë˜ìŠ¤ scatter plotplt.figure()for idx in range(10): mask = labels == idx plt.scatter(inputs[mask, 0], inputs[mask, 1])plt.title('True Label')# plt.grid()plt.show()# ëª¨ë¸ ì¶œë ¥ í´ë˜ìŠ¤ scatter plotplt.figure()for idx in range(10): mask = outputs == idx plt.scatter(inputs[mask, 0], inputs[mask, 1])plt.title('Model output')# plt.grid()plt.show()","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"ë”¥ëŸ¬ë‹ì´ ë¬´ì—‡ì¸ê°€?","slug":"deep_learning00","date":"2019-12-04T15:00:00.000Z","updated":"2019-12-08T12:41:32.567Z","comments":true,"path":"2019/12/05/deep_learning00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/05/deep_learning00/","excerpt":"","text":"ë”¥ëŸ¬ë‹ì˜ ì´í•´ ê¸°ê³„í•™ìŠµì˜ ê²½ìš°ì—ëŠ” ìœ„ì˜ ê³ ì–‘ì´ì™€ ê°œë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ì„œ ì´ì§„ ë¶„ë¥˜ê¸°ë¥¼ êµ¬í˜„í•  ê²ƒì¸ë°, ì´ëŸ° ì´ì§„ ë¶„ë¥˜ê¸°ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œëŠ” Feature Extractor\u001dê°€ í•„ìš”í•˜ë‹¤. ì—¬ê¸°ì„œ ë§í•˜ëŠ” Feature Extractor\u001d ë€ êµ¬ë¶„ì— ìš©ì´í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ì—¬ feature vectorë¥¼ ë§Œë“œëŠ” ë° ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë ‡ê²Œ ì˜ ì¶”ì¶œí•œ íŠ¹ì§• ë²¡í„°ë¥¼ ê°€ì§€ê³  ë¶„ë¥˜ê¸°ë¥¼ ê°œì™€ ê³ ì–‘ì´ë¥¼ êµ¬ë¶„í•œë‹¤. íŠ¹ì§• ì¶”ì¶œê¸°ë¥¼ í†µí•´ ì‚¬ëŒì´ ì§ì ‘ Feature vectorë“¤ì„ ë§Œë“¤ê³  Classifier ë¶€ë¶„ë§Œ ê¸°ê³„ê°€ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ Machine Learningì´ë‹¤. ë°˜ë©´ì— ë”¥ëŸ¬ë‹ì€ ê°œì™€ ê³ ì–‘ì´ì˜ row dataë¥¼ ë°›ì•„ì„œ Feature Extractor\u001dê°€ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ë‚´ë¶€ì— í¬í•¨ë˜ì–´ ìˆë‹¤. íŠ¹ì§• ì¶”ì¶œë„ ì»´í“¨í„°ê°€ í•˜ê³  classifier ë¶€ë¶„ë„ ì»´í“¨í„°ê°€ ì•Œì•„ì„œ ë¶„ë¥˜í•˜ë¯€ë¡œ ì „ì²´ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ê°€ í•™ìŠµëŒ€ìƒì´ ëœë‹¤. ë”¥ëŸ¬ë‹ì€ ê³¼ê±° ëª‡ë²ˆì˜ ê³ ë¹„(XORë¬¸ì œë¥¼ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œ ê·¹ë³µ, ê¸°ìš¸ê¸° ì†Œì‹¤ë¬¸ì œëŠ” ì‹¬ì¸µë¯¿ìŒ ì‹ ê²½ë§ì„ í†µí•´ ê·¹ë³µ)ì„ ê·¹ë³µí•˜ê³  í˜„ì¬ëŠ” ë§ì€ ì´ë“¤ì˜ ê´€ì‹¬ ì†ì— ë°œì „í•´ê°€ê³  ìˆë‹¤. ë”¥ëŸ¬ë‹ì˜ ëŒ€ì¤‘í™”ë¥¼ ì´ëˆ ìš”ì†Œë“¤ì„ ë‹¤ìŒ ê·¸ë¦¼ë“¤ì—ì„œ ë³¼ ìˆ˜ìˆë‹¤.","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"data engineering basic","slug":"data_engineering_basic","date":"2019-11-29T10:57:36.000Z","updated":"2019-12-08T14:31:09.223Z","comments":true,"path":"2019/11/29/data_engineering_basic/","link":"","permalink":"https://heung-bae-lee.github.io/2019/11/29/data_engineering_basic/","excerpt":"","text":"ë°ì´í„° ë¶„ì„ê°€ì™€ ì—”ì§€ë‹ˆì–´ë§ ì°¨ì´ì  ë°ì´í„° ë¶„ì„ê°€ëŠ” ê°–ì¶°ì§„ ë°ì´í„° ì‹œìŠ¤í…œê³¼ ë°ì´í„°ë¥¼ í†µí•´ì„œ ë‹¤ì–‘í•œ ë¶„ì„ì„ í•˜ëŠ” ì—…ë¬´ì´ë©°, ì—”ì§€ë‹ˆì–´ë§ì€ ê·¸ì™€ ë‹¤ë¥´ê²Œ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë§ëŠ” ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ê·¸ì— ë”°ë¼ ë¶„ì„í•˜ëŠ” í™˜ê²½ì„ ë§Œë“¤ì–´ ë‚˜ê°€ëŠ” ì—…ë¬´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. íŠ¹íˆ, ë°ì´í„° ì „ì²˜ë¦¬ë‚˜ ì¶”ì¶œ, ì •ì œë¥¼ ë‹´ë‹¹í•˜ëŠ” ì—…ë¬´ì´ë‹¤. ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì´ ì¤‘ìš”í•œ ì´ìœ  ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ê³¼ ê°€ì¥ ì—°ê´€ì´ ê¹Šì€ ì—…ë¬´ì´ë‹¤. ì™œëƒí•˜ë©´ íšŒì‚¬ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ì— ë§ëŠ” ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì•¼í•˜ê³  ê°€ì ¸ì˜¨ ë°ì´í„°ë¥¼ í†µí•´ ì–´ë–¤ í™˜ê²½ì„ ê°–ì¶œ ê²ƒì¸ì§€, ê·¸ì—ë”°ë¼ ë°ì´í„° ë¶„ì„ê°€ë“¤ì´ ì „ëµì„ ì§¤ ë•Œ ê¸°ë°˜ì„ ê°–ì¶œ ìˆ˜ ìˆë„ë¡ í•´ì£¼ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ì§€ê°€ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì´ë‹¤.ê·¸ë˜ì„œ ì—”ì§€ë‹ˆì–´ë§ì„ ë½‘ì„ ê²½ìš° í•´ë‹¹ ë¹„ì¦ˆë‹ˆìŠ¤ì˜ Knowledgeê°€ ì–´ëŠ ì •ë„ ìˆëŠ” ê²ƒì´ ì¢‹ì„ ê±°ë¼ê³  ìƒê°ì´ë“¤ê³  ì‹¤ì œë¡œ ê·¸ë ‡ê²Œ ë©´ì ‘ë„ ë³´ëŠ”(?)ê²ƒ ê°™ë‹¤. í˜ì´ìŠ¤ë¶ì€ Userì™€ ê´€ë ¨ ì„¸ë°€í•œ ë°ì´í„°ê°€ ì¤‘ìš”í–ˆì§€ë§Œ e-commerceëŠ” User ê´€ë ¨ ë°ì´í„° ë³´ë‹¤ëŠ” ë§ˆì¼€íŒ…, CRM, ë¬¼ë¥˜ ë°ì´í„°ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë” ì¤‘ìš”í•  ìˆ˜ë„ ìˆë‹¤. ë°ì´í„° ì•„í‚¤í…ì³ì‹œ ê³ ë ¤ì‚¬í•­1.ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ìƒ ê°€ì¥ ì¤‘ìš”í•œ ë°ì´í„°ëŠ” ë¬´ì—‡ì¸ê°€? ë°œìƒë˜ëŠ” ë°ì´í„° ì–‘ ëŒ€ë¹„ ì´ˆì ì„ ë§ì¶°ì•¼ í•˜ëŠ” ë°ì´í„°ëŠ” ì–´ë–¤ ê²ƒì¸ì§€ ì¦‰, ë¹„ìš© ëŒ€ë¹„ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ê°€ ê°€ì¥ ë†’ìœ¼ ë°ì´í„°ë¥¼ í™•ë³´í•˜ëŠ” ê²ƒì´ ì œì¼ ì¤‘ìš”í•˜ë‹¤. 2.Data Governance 3.ìœ ì—°í•˜ê³  ë³€í™” ê°€ëŠ¥í•œ í™˜ê²½ êµ¬ì¶• íŠ¹ì • ê¸°\bìˆ  ë° ì†”ë£¨ì…˜ì— ì–½ë§¤ì—¬ì ¸ ìˆì§€ ì•Šê³  ìƒˆë¡œìš´ í…Œí¬ë¥¼ ë¹ ë¥´ê²Œ ì ìš©í•  ìˆ˜ ìˆëŠ” ì•„í‚¤í…ì³ë¥¼ ë§Œë“œëŠ” ê²ƒ ìƒì„±ë˜ëŠ” ë°ì´í„°ì˜ í˜•ì‹ì´ ë³€í™”í•  ìˆ˜ ìˆëŠ” ê²ƒì²˜ëŸ¼ ê·¸ì— ë§ëŠ” Toolë“¤ê³¼ solutionë“¤ë„ ë¹ ë¥´ê²Œ ë³€í™”í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” \u001dê²ƒ 4. Real Time(ì‹¤ì‹œê°„) ë°ì´í„° í•¸ë“¤ë§ì´ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œ ë°€ë¦¬ì„¸ì»¨ ë‹¨ìœ„ì˜ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ê°€ ëê±´ í•˜ë£¨ì— í•œë²ˆ ì—…ë°ì´íŠ¸ ë˜ëŠ” ë°ì´í„°ë“  ë°ì´í„° ì•„í‚¤í…ì³ëŠ” ëª¨ë“  ìŠ¤í”¼ë“œì˜ ë°ì´í„°ë¥¼ í•¸ë“¤ë§ í•´ì•¼í•œë‹¤. Real Time Streaming Data Processing Cronjob Serverless Triggered Data Processing 5. ì‹œíë¦¬í‹° ë‚´ë¶€ì™€ ì™¸ë¶€ ëª¨ë“  ê³³ì—ì„œë¶€í„° ë°œìƒí•  ìˆ˜ ìˆëŠ” ìœ„í—˜ìš”ì†Œë“¤ì„ íŒŒì•…í•˜ì—¬ ì–´ë–»ê²Œ ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ”ì§€ ì•„í‚¤í…ì³ ì•ˆì— í¬í•¨ 6. ì…€í”„ ì„œë¹„ìŠ¤ í™˜ê²½ êµ¬ì¶• ë°ì´í„° ì—”ì§€ë‹ˆì–´ í•œëª…ë§Œ ì—‘ì„¸ìŠ¤ê°€ ê°€ëŠ¥í•œ ë°ì´í„° ì‹œìŠ¤í…œì€ í™•ì¥ì„±ì´ ì—†ëŠ” ë°ì´í„° ë¶„ì„ í™˜ê²½ì´ë‹¤. ì´ëŸ° í™˜ê²½ì—ì„œëŠ” ì˜ˆë¥¼ ë“¤ì–´, ë°ì´í„° ë¶„ì„ê°€ë“¤ì´ë¼ë˜ì§€, ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ë“¤, ë¹„ì¦ˆë‹ˆìŠ¤íŒ€ë“¤ ë“± ë‹¤ë¥¸ ì‚¬ëŒë“¤ë„ BI Tool, Query System for Analysis, Front-end applicationë“± ì´ ê°€ëŠ¥í•˜ê²Œë” í™•ì¥ì„±ì´ ìˆë„ë¡ í™˜ê²½ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ë°ì´í„° ì‹œìŠ¤í…œì˜ ì˜µì…˜ë“¤APIì‹œëŒ€ í˜„ì¬ ë§ˆì¼€íŒ…, CRM, ERPë“± ë‹¤ì–‘í•œ í”Œë«í¼ ë° ì†Œí”„íŠ¸ì›¨ì–´ë“¤ì€ APIë¼ëŠ” ì†¡ì‹ ë°©ë²•ì„ í†µí•´ ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ì„ ìˆ˜ ìˆëŠ” í™˜ê²½ì„ êµ¬ì¶•í•˜ì—¬ ìƒíƒœê³„ë¥¼ ìƒì„±ë˜ì–´ìˆë‹¤. ì˜ˆë¥¼ë“¤ë©´, facebook, google, Spotifyê°™ì€ ì„œë¹„ìŠ¤ë“¤ì´ íšŒì‚¬ìì²´ì— DBì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ê³  ìˆëŠ”ë°, ì´ëŸ° ë°ì´í„°ë“¤ì„ APIë¥¼ í†µí•´ ë°”ë¡œ DBì— ì—‘ì„¸ìŠ¤í•´ì„œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ë„ ìˆê³ , ì•„ë‹ˆë©´ DBë¥¼ ìƒˆë¡œ ìƒì„±í•´ ê±°ê¸°ì— ë°›ì•„ì„œ ì €ì¥í•´ë†“ì€ í›„ ì •ì œ ë° ë¶„ì„ í™˜ê²½ì„ êµ¬ì¶•í•˜ì—¬ ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆë‹¤. ì´ëŸ° í™˜ê²½ì—ì„œ í˜„ì¬ ë§ì€ ì„œë¹„ìŠ¤ë“¤ì´ ìˆìœ¼ë©°, ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ëŠ” ì…ì¥ì—ì„œëŠ” í•„ìš”í•œ ì—¬ëŸ¬ê°€ì§€ ì„œë¹„ìŠ¤ë“¤ì´ ìˆëŠ”ë°, ì¼ì¼ì´ ë‹¤ ë§Œë“¤ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë§Œë“¤ì–´ì ¸ ìˆëŠ” ê²ƒë“¤, ì¨ë“œ íŒŒí‹°ë¼ê³  í•˜ëŠ” ì„œë¹„ìŠ¤ë“¤ì„ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ëŠ” ë°ì´í„°ë¥¼ ê°€ì§€ê³ ë„ ë˜ë‹¤ë¥¸ ë¶„ì„í™˜ê²½ì„ êµ¬\bì¶•í•´ì•¼ í•œë‹¤. ex) CaFe24(í˜¸ìŠ¤íŒ…ì—…ì²´), facebook Ads,Google Ads(ë§ˆì¼€íŒ…ë¶„ì•¼) Relational Databases ë°ì´í„°ì˜ ê´€ê³„ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë””ì§€í„¸ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë°ì´í„°ì˜ ì €ì¥ì„ ëª©ì ìœ¼ë¡œ ìƒê²¨ë‚¬ë‹¤. SQLì´ë¼ê³  í•˜ëŠ” ìŠ¤íƒ ë‹¤ë“œ ë°©ì‹ì„ í†µí•´ ìë£Œë¥¼ ì—´ëŒí•˜ê³  ìœ ì§€í•œë‹¤. í˜„ì¬ ëŒ€ë¶€ë¶„ì˜ ì„œë¹„ìŠ¤ë“¤ì´ ê°€ì¥ ë§ì´ ì“°ê³  ìˆëŠ” ë°ì´í„° ì‹œìŠ¤í…œ. NoSQL Databases ê´€ê³„í˜• ë°ì´í„° ë² ì´ìŠ¤ì—ì„œëŠ” Schema í˜•ì‹ì— ë§ì¶° ë°ì´í„°ë¥¼ ì¶”ì¶œ ë° ì €ì¥í–ˆë‹¤ë©´, ì´ì œëŠ” ë„ˆë¬´ë‚˜ ë‹¤ì–‘í•œ í˜•ì‹ì´ ì—†ëŠ” ë°ì´í„° ë¶€í„° í‹€ì— ë§ì¶œ ìˆ˜ ì—†ëŠ” ë°ì´í„°ë“¤ì´ ìƒì„±ë˜ì–´ NoSQLì´ ëŒ€ë‘ë˜ì—ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ë©”ì‹ ì €ì—ì„œ ë§ì´ ì‚¬ìš©ëœë‹¤. Not Only NoSQL Unstructured, Schema Less Databases Scale horizontally Highly scalable Haddop / Spark / Presto ë“± ë¹…ë°ì´í„° ì²˜ë¦¬Distribtion Storage System / MapReduceë¥¼ í†µí•œ ë³‘ë ¬ ì²˜ë¦¬ Spark Hadoopì˜ ì§„í™”ëœ ë²„ì „ìœ¼ë¡œ ë¹…ë°ì´í„° ë¶„ì„ í™˜ê²½ì—ì„œ Real Time ë°ì´í„°ë¥¼ í”„ë¡œì„¸ì‹±í•˜ê¸°ì— ë” ìµœì  java, Python, Scalaë¥¼ í†µí•œ APIë¥¼ ì œê³µí•˜ì—¬ Application ìƒì„± SQL Query í™˜ê²½ì„ ì„œí¬íŠ¸í•˜ì—¬ ë¶„ì„ê°€ë“¤ì—ì„¸ ë” ê°ê´‘ ì„œë²„ë¦¬ìŠ¤ í”„ë ˆì„ì›Œí¬ Triggered by http requests, database events, queuing services DBê°€ ëê±´, ì–´ë–¤ serverê°€ ë\u001dê±´ ì–´ë– í•œ í•˜ë‚˜ì˜ ê°€ìƒ í´ë¼ìš°ë“œìƒì—ì„œ serverê°€ í•„ìš”í•˜ê²Œ ë˜ëŠ”ë°, ì„œë²„ë¥¼ ìƒì„±í•˜ê³  ìœ ì§€ë° ê´€ë¦¬í•  ë•Œ ë°ì´í„°ê°€ ë°œìƒí•˜ëŠ” eventê°€ ë°œìƒí•  ë•Œ Triggerê°€ ë˜ëŠ” ë¶€ë¶„ë“¤ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤. Pay as you User í•­ìƒ Serverë¥¼ ë„ì›Œë†“ê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ì“°ëŠ” ë§Œí¼ë§Œ ë¹„ìš©ì„ ì§€ë¶ˆí•˜ê¸°ì— ì¢‹ë‹¤. Form of functions í•˜ë‚˜ì˜ Functionì´ë¼ê³  ìƒê°í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ, ì„œë²„ë¦¬ìŠ¤ í”„ë ˆì„ ì›Œí¬ë¥¼ í†µí•´ì„œ ì–´ë– í•œ eventê°€ ë“¤ì–´ì™”ì„ ê²½ìš°, ì–´ë–¤ ê²ƒìœ¼ë¡œ Triggerê°€ ëì„ë•Œ, ì–´ë– í•œ Algorithmì„ ì‹¤í–‰ì‹œí‚¤ëŠ” functionì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. 3rd Party ì•±ë“¤ ë° ë‹¤ì–‘í•œ APIë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ì •ì œí•˜ëŠ”ë° ìœ ìš© ë°ì´í„° íŒŒì´í”„ë¼ì¸ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë°ì´í„°ë¥¼ í•œ ì¥ì†Œì—ì„œ ë‹¤ë¥¸ ì¥ì†Œë¡œ ì˜®ê¸°ëŠ” ê²ƒì„ ì˜ë¯¸ ex) API -&gt; DB, DB -&gt; DB, DB -&gt; BI Tool ë°ì´í„° íŒŒì´í”„ë¼ì¸ì´ í•„ìš”í•œ ê²½ìš° 1) ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ë“¤ë¡œë¶€í„° ë§ì€ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ì €ì¥í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•  ê²½ìš° í•„ìš”í•˜ë‹¤! 2) ë°ì´í„° ì‚¬ì¼ë¡œ: ë§ˆì¼€íŒ…, ì–´ì¹´ìš´íŒ…, ì„¸ì¼ì¦ˆ, ì˜¤í¼ë ˆì´ì…˜ ë“± ê° ì˜ì—­ì˜ ë°ì´í„°ê°€ ì„œë¡œ ê³ ë¦½ë˜ì–´ ìˆëŠ” ê²½ìš° (ex)ëŒ€ê¸°ì—…ì˜ ê° ë¶€ì„œë¥¼ ìƒê°í•´ë³´ë©´ ì´í•´í•˜ê¸° ì‰¬ìš¸ ê²ƒì´ë‹¤.ì¦‰, ê°ê°ì˜ íŒ€ë“¤ì´ ë”°ë¡œ ì¡´ì¬í•˜ì—¬ ê³µìœ ê°€ ì–´ë ¤ìš´ê²½ìš°) 3) ì‹¤ì‹œê°„ í˜¹ì€ ë†’ì€ ìˆ˜ì¤€ì˜ ë°ì´í„° ë¶„ì„ì´ í•„ìš”í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ ex)facebookë“± 4) í´ë¼ìš°ë“œ í™˜ê²½ìœ¼ë¡œ ë°ì´í„° ì €ì¥ ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ì‹œ ê³ ë ¤ì‚¬\u001dí•­ Scalability : ë°ì´í„°ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚¬ì„ë•Œë„ ì‘ë™í•˜ëŠ”ê°€? Stability : ì—ëŸ¬, ë°ì´í„°í”Œë¡œìš° ë“± ë‹¤ì–‘í•œ ëª¨ë‹ˆí„°ë§ ê´€ë¦¬ Security : ë°ì´í„° ì´ë™ê°„ ë³´ì•ˆì— ëŒ€í•œ ë¦¬ìŠ¤í¬ëŠ” ë¬´ì—‡ì¸ê°€? ë°ì´í„° í”„ë¡œì„¸ì‹± ìë™í™”ë€? ë°ì´í„° í”„ë¡œì„¸ì‹± ìë™í™”ë€ í•„ìš”í•œ ë°ì´í„°ë¥¼ ì¶”ì¶œ, ìˆ˜ì§‘, ì •ì œí•˜ëŠ” í”„ë¡œì„¸ì‹±ì„ ìµœì†Œí•œì˜ ì‚¬ëŒ ì¸í’‹ìœ¼ë¡œ ë¨¸ì‹ ì´ ìš´ì˜í•˜ëŠ” ê²ƒì„ ì˜ë¯¸ ex) Spotify ë°ì´í„°ë¥¼ í•˜ë£¨ì— í•œë²ˆ APIë¥¼ í†µí•´ì„œ í´ë¼ìš°ë“œ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ê°€ì ¸ì˜¨ë‹¤ê³  í–ˆì„ ë•Œ ë§¤ë²ˆ ì‚¬ëŒì´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ì‘ë™í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ Crontab ë“± ë¨¸ì‹  ìŠ¤ì¼€ì¥´ë§ì„ í†µí•´ ìë™í™” ìë™í™”ë¥¼ ìœ„í•´ ê³ ë ¤í•  ì‚¬í•­ errorê°€ ëœ¨ëŠ” ê²ƒì´ë“ , ì¶”ì¶œì„ í–ˆìœ¼ë©´ ë¶„ì„ì„ í•œë‹¤ë˜ì§€ ì‚¬ëŒì´ í•˜ë©´ ìˆœì„œë‚˜ ì—¬ëŸ¬ê°€ì§€ ê³ ë´ë¥¼ í•  ìˆ˜ ìˆì§€ë§Œ, ìë™ìœ¼ë¡œ í—€ì„ê²½ìš°ëŠ” ë¨¸ì‹ ì´ ëª¨ë¥´ê¸° ë–„ë¬¸ì— ë‹¤ìŒê³¼ ê°™ì€ ì‚¬í•­ë“¤ì„ ê³ ë ¤í•´ì•¼í•œë‹¤. 1) ë°ì´í„° í”„ë¡œì„¸ì‹± ìŠ¤í…ë“¤ Spotify APIì—ì„œ ì–´ë– í•œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì•¼ë˜ê³ , ê·¸ ì¤‘ì—ì„œ ì–´ë– í•œ ê²ƒë“¤ì€ ê±¸ëŸ¬ë‚´ê³ , ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ì„ ëŒë¦¬ê³ , ê·¸ í›„ì— ì‹œê°í™”ë¥¼ í•œê² ë‹¤ëŠ” ë§ ê·¸ëŒ€ë¡œ í”„ë¡œì„¸ì‹± ìŠ¤í…ì„ ì˜ë¯¸. 2) ì—ëŸ¬ í•¸ë“¤ë§ ë° ëª¨ë‹ˆí„°ë§ ì—ëŸ¬ê°€ ìƒì„±ì´ ëì„ë•Œ, ì–´ë–»ê²Œ ë°˜ì‘ì„ í•˜ê²Œë”í•  ê²ƒì¸ì§€, ì—ëŸ¬ë‚˜ í¼í¬ë¨¼ìŠ¤ ë˜ëŠ” ë°ì´í„° ì¶”ì¶œì´ ì–¼ë§ˆë‚˜ ê±¸ë ¸ëŠ”ì§€ ê°™ì€ ì‚¬í•­ì„ ëª¨ë‹ˆí„°ë§ í• ìˆ˜ ìˆê²Œë” êµ¬ì¶•í•´ì•¼í•œë‹¤. exampl.logë¼ëŠ” íŒŒì¼ì— ë‹¤ì–‘í•œ ë¡œê·¸ë“¤ì„ ì €ì¥í•˜ëŠ”ë°, ê·¸ ìƒì„±ëœ ë¡œê·¸ë“¤ë„ CloudWatchì—ë„ ìƒì„±ë˜ì–´ ëª¨ë‹ˆí„°ë§ì´ ê°€ëŠ¥í•˜ë‹¤. 3) Trigger/ Scheduling ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ê¸° ìœ„í•´ ì–´ë–»ê²Œ Triggerê°€ ë˜ì–´ì„œ ì‹¤í–‰ì„ ì‹œí‚¬ì§€, í•˜ë£¨ì— í•œë²ˆ ëŒë¦´ì§€, ì•„ë‹ˆë©´ í•œë‹¬ì— í•œë²ˆ ëŒë¦´ì§€ ë“±ì— ê´€í•œ ìŠ¤ì¼€ì¤„ì„ ê³ ë ¤í•´ì•¼í•œë‹¤. Spotify í”„ë¡œì íŠ¸ ë°ì´í„° ì•„í‚¤í…ì³Ad hoc VS Automated Ad hoc ë¶„ì„ í™˜ê²½ êµ¬ì¶•ì€ ì„œë¹„ìŠ¤ë¥¼ ì§€ì†ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ ë³€í™”ì‹œí‚¤ê¸° ìœ„í•´ í•„ìˆ˜ì ì¸ ìš”ì†Œ Ad hoc ë¶„ì„ì€ ì‰½ê²Œ ë§í•´ ë¶„ì„ì„ í•˜ê³  ì‹¶ì„ ë•Œë§Œ í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° Ad hoc ë¶„ì„ì´ í•„ìˆ˜ì ì¸ ì´ìœ ëŠ” êµ¬ì¶•í•œ ë¶„ì„í™˜ê²½ì„ í†µí•´ì„œ ë‹¤ì–‘í•œ ì‚¬ëŒë“¤ì´ ë¶„ì„ì„ í•  ìˆ˜ ìˆê²Œë”í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì´ë‹ˆì…œ ë°ì´í„° ì‚½ì…, ë°ì´í„° Backfill ë“±ì„ ìœ„í•´ Ad hoc ë°ì´í„° í”„ë¡œì„¸ì‹± ì‹œìŠ¤í…œ êµ¬ì¶• í•„ìš” Automated : ì´ë²¤íŠ¸, ìŠ¤ì¼€ì¥´ ë“± íŠ¸ë¦¬ê±°ë¥¼ í†µí•´ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"Requests ëª¨ë“ˆ ì‚¬ìš©í•˜ê¸°(HTTP í†µì‹ )","slug":"Request","date":"2019-09-28T07:20:55.000Z","updated":"2019-11-21T10:39:17.411Z","comments":true,"path":"2019/09/28/Request/","link":"","permalink":"https://heung-bae-lee.github.io/2019/09/28/Request/","excerpt":"","text":"Requests ëª¨ë“ˆ http request/responseë¥¼ ìœ„í•œ ëª¨ë“ˆ HTTP methodë¥¼ ë©”ì†Œë“œ ëª…ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ request ìš”ì²­ ì˜ˆ)GET, POST ê°€ì¥ ìš°ë¦¬ê°€ í”í•˜ê²Œ í¬ë¡¤ë§ì„ í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ë©°, APIë§Œ ì•Œê³  ìˆë‹¤ë©´ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤!","categories":[],"tags":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/tags/crawling/"}]},{"title":"ì›¹ ê¸°ë³¸ ì§€ì‹ ì´í•´í•˜ê¸° 01(chromeì„ ì´ìš©í•˜ì—¬ ì›¹í˜ì´ì§€ ë¶„ì„í•˜ê¸°)","slug":"HTTP_method","date":"2019-09-28T05:52:18.000Z","updated":"2019-12-06T04:56:15.576Z","comments":true,"path":"2019/09/28/HTTP_method/","link":"","permalink":"https://heung-bae-lee.github.io/2019/09/28/HTTP_method/","excerpt":"","text":"HTTP HyperText Transfer Protocol: HTML(HyperText Markup language) ë¬¸ì„œ ë“±ì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ì „ì†¡í•˜ëŠ” í”„ë¡œí† ì½œ(ê·œì•½) í´ë¼ì´ì–¸íŠ¸ì—ì„œ ì„œë²„ë¡œ HTTP ìš”ì²­ì„ í•˜ëŠ”ë° ì´ ìš”ì²­ ë°©ì‹ìœ¼ë¡œëŠ” Get, Post, Put, delete, Optionë“±ì´ ìˆëŠ”ë° ì´ ì¤‘ í¬ë¡¤ë§ì—ì„œ ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ë‘ê°€ì§€(Get, Post)ë¥¼ ì•Œì•„ë³¼ ê²ƒì´ë‹¤. GET ìš”ì²­ : ë°ì´í„°ë¥¼ URLì— í¬í•¨í•˜ì—¬ ì „ë‹¬ì´ ê°€ëŠ¥í•˜ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì •ë³´ì˜ ê³µìœ ê°€ ê°€ëŠ¥í•˜ë‹¤. ê³µìœ ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì˜ë¯¸ëŠ” ê°€ì¥ ì‰¬ìš´ ì˜ˆë¥¼ ë“¤ìë©´ URL í´ë¦­í•˜ë©´ ê·¸ ì •ë³´ë¥¼ ë‹´ê³ ìˆëŠ” ì›¹ í˜ì´ì§€ë¥¼ ìš”ì²­í•˜ì—¬ ìš°ë¦¬ì˜ ì›¹ ë¸Œë¼ìš°ì ¸ì— ëœë”ë§ì„ ê±°ì³ ë³´ì—¬ì£¼ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤.(ì£¼ë¡œ ë¦¬ì†ŒìŠ¤ ìš”ì²­ì— ì‚¬ìš©) ex)https://search.naver.com/search.naver?sm=top hty&amp;fbm=1 POST ìš”ì²­ : ë°ì´í„°ë¥¼ Form dataì— í¬í•¨í•˜ì—¬ ì „ë‹¬ ê·¸ë˜ì„œ ëŒ€ë¶€ë¶„ ìš°ë¦¬ ëˆˆì— ì•ˆë³´ì´ëŠ” ìš”ì²­ì´ë‹¤.(ì£¼ë¡œ ë¡œê·¸ì¸ì— ì‚¬ìš©) ì˜ˆë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ìë©´, ë¨¼ì € ë¡œê·¸ì¸ í˜ì´ì§€ê°€ ìˆë‹¤ë©´, ì°¸ê³ ë¡œ ì´ ë¡œê·¸ì¸ í˜ì´ì§€ëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ ìš”ì²­í•˜ëŠ” ê²ƒì´ë¯€ë¡œ GET ë°©ì‹ì´ê³  ë¡œê·¸ì¸ í˜ì´ì§€ì—ì„œ ë¡œê·¸ì¸ì„ í•˜ëŠ” í–‰ìœ„ë¥¼ í•  ê²½ìš° ì´ ë–„ ì‚¬ìš©ëœë‹¤. ex) https://www.kangco.com/meber/member_check.asp ì–´ë– í•œ ë°©ì‹ìœ¼ë¡œ í•´ë‹¹ ì›¹ í˜ì´ì§€ê°€ ì„œë²„ì— ìš”ì²­ì„ í•˜ëŠ”ì§€ëŠ” ê°œë°œì ë„êµ¬ì˜ Network íƒ­ì—ì„œ íŠ¹ì • Nameì„ í´ë¦­ í›„ Request Methodë¥¼ ë³´ë©´ì„œ í™•ì¸í•  ìˆ˜ ìˆë‹¤. HTML element ì´í•´í•˜ê¸°(tag, attribute, value) HTML(Hyper Text Markup Language) ì›¹ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ ì–¸ì–´ë¡œ ë¬¸ì„œì™€ ë¬¸ì„œê°€ ë§í¬ë¡œ ì—°ê²°ë˜ì–´ ìˆê³ , tagë¥¼ ì‚¬ìš©í•˜ëŠ” ì–¸ì–´ íƒœê·¸(Tag)ë€? HTML ë¬¸ì„œì˜ ê¸°ë³¸ ë¸”ë½ &lt;íƒœê·¸ëª… ì†ì„±1=&quot;ì†ì„±ê°’1&quot; ì†ì„±2=&quot;ì†ì„±ê°’2&quot;&gt;Value&lt;/íƒœê·¸ëª…&gt; (Valueê°€ ìˆëŠ” ê²½ìš°) &lt;íƒœê·¸ëª… ì†ì„±1=&quot;ì†ì„±ê°’1&quot; ì†ì„±2=&quot;ì†ì„±ê°’2/&quot;&gt; (Valueê°€ ì—†ëŠ” ê²½ìš°) í¬ë¡¤ë§ì„ ë‹¨ í•œë²ˆì´ë¼ë„ ì§ì ‘ í•´ë³´ì‹  ë¶„ë“¤ì€ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê°’ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ëŠ ì •ë„ì˜ HTML ì§€ì‹ì´ ìˆì–´ì•¼í•œë‹¤. ì¦‰, Valueë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•´ í•´ë‹¹ Valueê°€ í¬í•¨ë˜ì–´ ìˆëŠ” tagì˜ êµ¬ì¡°ë¥¼ ì•Œì•„ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ìœ„ì˜ ë‹¨ìˆœí•œ tagì˜ êµ¬ì¡°ëŠ” ê·¸ëŸ° ê´€ì ì—ì„œ í˜¹ì‹œë¼ë„ HTMLì˜ êµ¬ì¡°ë¥¼ ëª¨ë¥´ì‹œë¶„ë“¤ì„ ìœ„í•´ ê°„ë‹¨íˆ ì„¤ëª…í•˜ê³  ë„˜ì–´ê°€ëŠ” ê²ƒì´ë‹¤. Requests ëª¨ë“ˆ http request/responseë¥¼ ìœ„í•œ ëª¨ë“ˆ HTTP methodë¥¼ ë©”ì†Œë“œ ëª…ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ request ìš”ì²­ ì˜ˆ)GET, POST ê°€ì¥ ìš°ë¦¬ê°€ í”í•˜ê²Œ í¬ë¡¤ë§ì„ í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ë©°, APIë§Œ ì•Œê³  ìˆë‹¤ë©´ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤!","categories":[],"tags":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/tags/crawling/"}]},{"title":"colab & Kaggle ì—°ë™ ë° ê¸°ì´ˆ ì‚¬ìš©ë²•","slug":"kaggle_00","date":"2019-08-01T07:26:46.000Z","updated":"2019-09-28T05:51:41.381Z","comments":true,"path":"2019/08/01/kaggle_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/08/01/kaggle_00/","excerpt":"","text":"Kaggle Koreaì—ì„œ ì§„í–‰ì¤‘ì¸ ëŒ€íšŒì—ì„œ Kaggle Kernelì„ ì‚¬ìš©í•˜ë‹¤ ë³´ë‹ˆ ì»¤ë„ì´ ìê¾¸ ì£½ëŠ” ì´ìœ ëŠ” ë„ëŒ€ì²´ ë¬´ì—‡ì¸ì§€â€¦ competitionì— ëŠ¦ê²Œ ì°¸ì—¬í•œ ê´€ê³„ë¡œ ë” ì‹œê°„ì´ ì´‰ë°•í•˜ê¸°ë§Œ í•œë°â€¦. ê·¸ë˜ì„œ google colabìœ¼ë¡œ ë°”ê¾¸ë ¤ê³  ìƒê°í•˜ì˜€ë‹¤. ìºê¸€ì€ ì˜ˆì¸¡ëª¨ë¸ ë° ë¶„ì„ ëŒ€íšŒë¥¼ í•˜ëŠ” í”Œë«í¼ì´ë‹¤. ê°œì¸ ë° ë‹¨ì²´ì—ì„œ í•´ê²°í•˜ê³  ì‹¶ì€ ê³¼ì œì™€ ë°ì´í„°ë¥¼ ë“±ë¡í•˜ë©´, ìºê¸€ì— ê°€ì…í•œ ë°ì´í„° ê³¼í•™ìë“¤ì´ ëª¨ë¸ì„ ê°œë°œí•˜ê³  ê²°ê³¼ë¥¼ ë“±ë¡í•œë‹¤. ì˜ˆì¸¡ë ¥ì„ ìˆœìœ„ë¡œ í•˜ì—¬ ê°€ì¥ ì¢‹ì€ ìˆœìœ„ì—ê²ŒëŠ” ìƒê¸ˆë„ ì£¼ì›Œì§„ë‹¤. ê·¸ë§Œí¼ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ì— ê´€í•œ ë¶„ë“¤ì€ ëª¨ë¥¼ ìˆ˜ ì—†ëŠ” ì‚¬ì´íŠ¸ë¼ê³  ìƒê°í•œë‹¤. Google ColaboratoryëŠ” Google Drive + Jupyter Notebookì˜ ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆìœ¼ë©°, Google Driveì²˜ëŸ¼ í˜‘ì—… ê°€ëŠ¥(ë™ì‹œì— ìˆ˜ì • ê°€ëŠ¥)í•˜ë‹¤ê³  í•œë‹¤. https://colab.research.google.com/ë¡œ ì ‘ì†ì‹œ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤. ë¬´ì—‡ë³´ë‹¤ ê°€ì¥ ì¢‹ì•˜ë˜ ì ì€ ìºê¸€ì˜ ì»¤ë„ì€ 9ì‹œê°„ì´ ìµœì¥ ì´ìš©ì‹œê°„ì¸ ë°˜ë©´ì—, colabì€ 12ì‹œê°„ì´ë‹¤. 3ì‹œê°„ ì°¨ì´ì— ì–¼ë§ˆë‚˜ ë” ë°”ë€Œê² ëƒë¼ëŠ” ë¶„ë“¤ë„ ê³„ì‹œ ê² ì§€ë§Œ GPUê°€ ì—†ëŠ” ë‚˜ì—ê² 3ì‹œê°„ì€ ì—„ì²­ë‚œ ì‹œê°„ì´ë‹¤. ë” ìì„¸í•œ ì‚¬í•­ì€ [https://zzsza.github.io/data/2018/08/30/google-colab/][https://zzsza.github.io/data/2018/08/30/google-colab/] ì´ ë¸”ë¡œê·¸ë¥¼ ì°¸ì¡°í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤! ê°œì¸ì ìœ¼ë¡œ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ì— ê´€í•´ ë§ì€ ê²ƒì— ëŒ€í•´ ìì„¸íˆ ë‹¤ë£¨ê³  ìˆë‹¤ê³  ìƒê°í•˜ë©° ê°•ì¶”í•œë‹¤!(ì ˆëŒ€ í™ë³´ê¸€ ì•„ë‹˜.) êµ¬ê¸€ ë“œë¼ì´ë¸Œì™€ Colab ì—°ë™ ë§¤ë²ˆ sessionì´ ëŠê¸°ê±°ë‚˜ ì¢…ë£Œë˜ë©´ ì´ ì‘ì—…ì„ í•´ì£¼ì–´ì•¼ í•œë‹¤. ê·¸ë˜ë„ kaggle ë³´ë‹¨ ë‚´ ì»´í“¨í„°ì—ì„  ëœ ëŠê¸´ë‹¤. ë¨¼ì €, êµ¬ê¸€ ë“œë¼ì´ë¸Œì™€ ì—°ë™ì„ ì‹œí‚¤ëŠ” ì´ìœ ëŠ” ë¡œì»¬ì—ì„œ Colab working directoryë¡œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê²Œ ë˜ë©´ ì°¨í›„ ë‹¤ì‹œ ì ‘ì†í•  ë•Œ ë‹¤ì‹œ ì—…ë¡œë“œë¥¼ í•´ì£¼ì–´ì•¼í•˜ì§€ë§Œ êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„  ë°”ë¡œ ì½ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. 1234567from google.colab import authauth.authenticate_user()# colabì—ì„œ driveë€ í´ë”ë¥¼ ë§Œë“  í›„, ìš°ë¦¬ êµ¬ê¸€ ë“œë¼ì´ë¸Œì˜ rootì™€ drive í´ë”ë¥¼ ì—°ê²°(mount)from google.colab import drivedrive.mount('/content/gdrive') êµ¬ê¸€ ë“œë¼ì´ë¸Œì™€ ë¡œì»¬ ì—°ë™ íŒŒì¼ì„ í•˜ë‚˜ì”© ì—…ë¡œë“œí•˜ì§€ ë§ê³  ëŒ€ëŸ‰ì˜ íŒŒì¼ì„ í•œêº¼ë²ˆì— ì—…ë¡œë“œí•˜ê³  ì‹¶ì€ ê²½ìš° [BackupAndSync](https://www.google.com/drive/download/)ë¥¼ ì‚¬ìš©í•´ ë¡œì»¬ê³¼ êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ ì—°ë™ 1) ìœ„ ë§í¬ë¥¼ í´ë¦­í•´ ë°±ì—… ë° ë™ê¸°í™” ë‹¤ìš´ë¡œë“œ 2) InstallBackupAndSync.dmgë¼ëŠ” íŒŒì¼ì„ í´ë¦­í•œ í›„, (ì—´ë¦¬ì§€ ì•Šìœ¼ë©´ ìš°í´ë¦­ í›„ ì—´ê¸°) í”„ë¡œê·¸ë¨ ì„¤ì¹˜ 3)ë§¥ë¶ í™˜ê²½ì´ í•œê¸€ì´ì‹  ë¶„ì€ Googleì—ì„œ ë°±ì—… ë° ë™ê¸°í™”ë¼ëŠ” ì‘ìš© í”„ë¡œê·¸ë¨ì´ ì¶”ê°€ë¨(ì´ê²ƒë„ ì‹¤í–‰ì´ ì•ˆë˜ë©´ í´ë¦­ í›„ ì‹¤í–‰) í™˜ê²½ ì„¤ì •ì—ì„œ ë™ê¸°í™”í•  í´ë” ì„ íƒ (ë‹¨, í¬ê¸°ê°€ í° íŒŒì¼ì€ ë™ê¸°í™” ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ) Kaggle ì—°ë™í•˜ê¸°- 1) Kaggle beta API Json Key ë‹¤ìš´- Kaggle - My Account - Dataset ì˜†ì— ìˆëŠ” â€¦ì„ í´ë¦­í•œ í›„, Accountë¡œ ì´ë™ - í•˜ë‹¨ì— API ë¶€ë¶„ì— Create New API Tokenì„ í´ë¦­í•˜ë©´ Json Keyê°€ ë‹¤ìš´ë¡œë“œ ë¨ - ì´ Json í‚¤ë¥¼ ë§¤ë²ˆ Colabì—ì„œ ì˜¬ë ¤ì„œ í•  ìˆ˜ë„ ìˆì§€ë§Œ, ë” í¸í•˜ê²Œ ì‚¬ìš©í•˜ê³  ì‹¶ì–´ì„œ Google Storageì— Json íŒŒì¼ì„ ì˜¬ë¦¬ê³ , ê±°ê¸°ì„œ í‚¤ë¥¼ ë³µì‚¬í•´ì˜¤ëŠ” ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤ - 2) Google Storageì— Json Key ì €ì¥- Google Storageë¡œ ì´ë™í•œ í›„, Storage ë²„í‚· ì„ íƒ (ë²„í‚·ì´ ì—†ë‹¤ë©´ ìƒì„±!) - Colabì—ì„œ ì•„ë˜ ëª…ë ¹ì–´ ì…ë ¥ 123456from google.colab import authauth.authenticate_user()!mkdir -p ~/.kaggle!mv ./kaggle.json ~/.kaggle/!chmod 600 ~/.kaggle/kaggle.json ìš°ì„  ì—¬ê¸°ê¹Œì§€! ë‚´ì¼ ë‹¤ì‹œ ì‹œì‘í•©ë‹ˆë‹¤!!!!!","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"https://heung-bae-lee.github.io/categories/Kaggle/"}],"tags":[]},{"title":"[CS231n]Lecture10-Recurrent Neural Networks","slug":"cs231n_10","date":"2019-07-29T07:22:55.000Z","updated":"2019-07-31T13:40:54.829Z","comments":true,"path":"2019/07/29/cs231n_10/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/29/cs231n_10/","excerpt":"","text":"","categories":[],"tags":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/tags/CS231n/"}]},{"title":"[ìˆ˜ë¦¬í†µê³„í•™] ë‚˜í˜¼ì ì •ë¦¬í•˜ëŠ” í†µê³„ìƒì˜ ìˆ˜ë¦¬í†µê³„í•™ 00","slug":"Statistics_00","date":"2019-07-29T07:12:38.000Z","updated":"2019-07-31T13:40:58.126Z","comments":true,"path":"2019/07/29/Statistics_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/29/Statistics_00/","excerpt":"","text":"ë¨¼ì €, ì´ ê¸€ì€ data scienceë¼ëŠ” ë¶„ì•¼ë¥¼ ê³µë¶€í•˜ë©° ì›ë˜ í†µê³„í•™ì„ ì „ê³µí–ˆë˜ ë‚˜ì˜€ì§€ë§Œ í•™ë¶€ê³¼ì •ì—ì„œ ë°°ì› ë˜ ê·¸ë¦¬ê³  ë¶€ë„ëŸ½ì§€ë§Œ ì„ì‚¬ ë•Œ ë°°ì› ë‹¤ê³  ê¸°ì–µí•˜ê³  ìˆëŠ” ì¡°ê°ë“¤ì„ ëª¨ì•„ ìˆ˜ë¦¬í†µê³„í•™ì„ ê°„ë‹¨íˆ ì •ë¦¬í•˜ê¸° ìœ„í•´ ì“°ëŠ” ê¸€ì´ë‹¤. í™•ë¥ ê³¼ í™•ë¥ ë¶„í¬ì—¬ëŸ¬ ê°€ì§€ ì¡°ì‚¬ ì—°êµ¬ë“¤ì€ ë™ì¼í•œ ì¡°ê±´ì—ì„œì˜ ë°˜ë³µ ì‹¤í—˜ì´ ëŒ€ì²´ì ìœ¼ë¡œ í‘œì¤€ì´ ëœë‹¤ëŠ” ê²ƒì´ íŠ¹ì§•ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì˜í•™ ì—°êµ¬ì—ì„œëŠ” ë³µìš©ëœ ì•½ì˜ íš¨ê³¼ì— ê´€ì‹¬ì´ ìˆì„ ê²ƒì´ë‹¤. ìš°ë¦¬ê°€ ì–´ë–¤ ê²°ë¡ ì„ ë‚´ë¦¬ê¸° ê¹Œì§€ ê·¸ì— ë”°ë¥¸ ê·¼ê±°ë¥¼ ë’·ë°›ì¹¨í•˜ê¸° ìœ„í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ìœ ì¼í•œ ë°©ë²•ì€ ì‹¤í—˜ì´ë‹¤. ì´ëŸ° ì‹¤í—˜ì„ í†µí•´ ì–»ì€ ì‹¤í—˜ê°’ë“¤ì€ ì˜ˆì¸¡í•  ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ ì‹¤í—˜ì˜ íŠ¹ì„±ì´ë‹¤. ì—¬ê¸°ì„œ í†µê³„ë¥¼ ê³µë¶€í•˜ë‹¤ ë³´ë©´ ì œì¼ ë¨¼ì € ì•Œê²Œë˜ëŠ” sample spaceë¥¼ ì •ì˜í•  ê²ƒì´ë‹¤. sample space : ê°™ì€ ì¡°ê±´ì—ì„œ ë°˜ë³µí•  ìˆ˜ ìˆëŠ” ì‹¤í—˜ì—ì„œ ì‹¤í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ì˜ ì§‘í•©","categories":[{"name":"Statistics - Mathematical Statistics","slug":"Statistics-Mathematical-Statistics","permalink":"https://heung-bae-lee.github.io/categories/Statistics-Mathematical-Statistics/"}],"tags":[]},{"title":"[CS231n]Lecture09-CNN_Architectures","slug":"cs231n_09","date":"2019-07-28T03:43:10.000Z","updated":"2019-07-31T13:40:52.676Z","comments":true,"path":"2019/07/28/cs231n_09/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/28/cs231n_09/","excerpt":"","text":"LeNet ì‚°ì—…ì— ì•„ì£¼ ì„±ê³µì ìœ¼ë¡œ ì ìš©ëœ ìµœì´ˆì˜ ConvNetì´ë‹¤. ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ Stride = 1ì¸ 5x5í•„í„°ë¥¼ ê±°ì¹˜ê³  ëª‡ ê°œì˜ Conv Layerì™€ Pooling Layerë¥¼ ê±°ì¹œë‹¤. ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ ì¶œë ¥ ë…¸ë“œ ì „ì— Fully Connected Layerê°€ ë¶™ëŠ”ë‹¤. ì—„ì²­ ê°„ë‹¨í•œ ëª¨ë¸ì´ì§€ë§Œ ìˆ«ì ì¸ì‹ì—ì„œ ì—„ì²­ë‚œ ì„±ê³µì„ ê±°ë‘ì—ˆë‹¤. AlexNet 2012ë…„ì— ë“±ì¥í•œ ìµœì´ˆì˜ Large scale CNNì´ë©° Image Classification Benchmarkì˜ ìš°ìŠ¹ ëª¨ë¸ì´ë‹¤. ConvNet ì—°êµ¬ì˜ ë¶€í¥ì„ ì¼ìœ¼í‚¨ ì¥ë³¸ì¸ì´ë‹¤. ìˆ˜ë…„ ì „ê¹Œì§€ ëŒ€ë¶€ë¶„ì˜ CNN ì•„í‚¤í…ì³ì˜ ë² ì´ìŠ¤ ëª¨ë¸ë¡œ ì‚¬ìš©ë˜ì–´ ì™”ë‹¤. AlexNetì€ ë‹¤ì–‘í•œ Taskì˜ transfer learningì— ë§ì´ ì‚¬ìš©ë˜ì—ˆë‹¤. AlexNetì€ ê¸°ë³¸ì ìœ¼ë¡œ conv - pool - normalization êµ¬ì¡°ê°€ 2ë²ˆ ë°˜ë³µëœë‹¤. ê·¸ë¦¬ê³  conv layerê°€ ì¡°ê¸ˆ ë” ë¶™ê³  (CONV 3,4,5) ê·¸ ë’¤ì— pooling layerê°€ ìˆë‹¤.(Max POOL3) ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ì—ëŠ” Fully connected layerê°€ ëª‡ ê°œ ë¶™ëŠ”ë‹¤.(FC 6,7,8) ìƒê¸´ ê²ƒë§Œ ë´ì„œëŠ” ê¸°ì¡´ì˜ LeNetê³¼ ìƒë‹¹íˆ ìœ ì‚¬í•˜ë©° layerë§Œ ëŠ˜ì–´ ë‚¬ë‹¤ê³  ìƒê°ì´ ë“¤ ê²ƒì´ë‹¤. AlexNetì€ 5ê°œì˜ Conv Layerì™€ 2ê°œì˜ FC-Layer(ë§ˆì§€ë§‰ FC Layer ì „ê¹Œì§€)ë¡œ êµ¬ì„±ëœë‹¤. ë‹¤ë¥¸ Conv Netì˜ ë‹¤ì´ì–´ê·¸ë¨ê³¼ ìœ ì‚¬í•˜ê¸´ í•˜ì§€ë§Œ í•œ ê°€ì§€ ì°¨ì´ì ì´ ìˆë‹¤. ëª¨ë¸ì´ ë‘ê°œë¡œ ë‚˜ëˆ„ì–´ì ¸ì„œ ì„œë¡œ êµì°¨í•œë‹¤ëŠ” ê²ƒì´ë‹¤. AlexNetì„ í•™ìŠµí•  ë‹¹ì‹œì— 3GB GTX850ìœ¼ë¡œ í•™ìŠµì‹œì¼°ë‹¤. ê·¸ë˜ì„œ ì „ì²´ Layerë¥¼ GPUì— ë‹¤ ë„£ì„ ìˆ˜ ì—†ì–´ì„œ ë¶„ì‚° ì‹œì¼œ ë„£ì„ ìˆ˜ ë°–ì— ì—†ì—ˆë‹¤. ê° GPUê°€ ëª¨ë¸ì˜ ë‰´ëŸ°ê³¼ Feature Mapì„ ë°˜ë°˜ì”© ë‚˜ëˆ ê°€ì§„ë‹¤. Conv 1,2,4,5ë¥¼ ì‚´í´ë³´ë©´ ê°™ì€ GPU ë‚´ì— ìˆëŠ” Feature Mapë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì¦‰, ì „ì²´ 96ê°œì˜ feature Mapì„ ë³¼ ìˆ˜ ì—†ë‹¤. ê·¸ë˜ì„œ ë‹¤ì´ì–´ê·¸ë¨ì„ ë³´ë©´ ê° Conv Layerì˜ Depthê°€ 48ì¸ ê²ƒì´ë‹¤. Conv 3, FC 6,7,8ë¥¼ ë³´ë©´ ì´ Layerë“¤ì€ ì´ì „ ê³„ì¸µì˜ ì „ì²´ Feature Mapê³¼ ì—°ê²°ë˜ì–´ ìˆë‹¤. ì´ Layerë“¤ì—ì„œëŠ” GPUê°„ì˜ í†µì‹ ì„ í•˜ê¸° ë•Œë¬¸ì— ì´ì „ ì…ë ¥ Layerì˜ ì „ì²´ Depthë¥¼ ì „ë¶€ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. AlexNet ë…¼ë¬¸ì˜ ì•„í‚¤í…ì³ì™€ ê´€ë ¨í•œ ì¡°ê·¸ë§Œ ì´ìŠˆëŠ” ê·¸ë¦¼ì„ ìì„¸íˆ ë³´ë©´ ì²« Layerê°€ 224x224ë¼ê³  ë˜ì–´ ìˆëŠ”ë°, ì‹¤ì œ ì…ë ¥ì€ 227x227ì´ë‹¤. ì§ˆë¬¸ Pooling layerì—ëŠ” íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ”ê°€? íŒŒë¼ë¯¸í„°ëŠ” ìš°ë¦¬ê°€ í•™ìŠµì‹œí‚¤ëŠ” ê°€ì¤‘ì¹˜ì´ë‹¤. Conv Layerì—ëŠ” í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê°€ì¤‘ì¹˜ê°€ ìˆë‹¤. ë°˜ë©´ Poolingì˜ ê²½ìš°ì—ëŠ” ê°€ì¤‘ì¹˜ê°€ ì—†ê³  ê·¸ì € íŠ¹ì • ì§€ì—­ì—ì„œ í° ê°’ì„ ë½‘ì•„ë‚´ëŠ” ì—­í• ë§Œ í•œë‹¤. ë”°ë¼ì„œ í•™ìŠµì‹œí‚¬ íŒŒë¼ë¯¸í„°ê°€ ì—†ëŠ” ê²ƒì´ë‹¤. ê°ê°ì˜ Layerì˜ íŒŒë¼ë¯¸í„° ì‚¬ì´ì¦ˆë¥¼ ê³„ì‚°í•´ ë³´ì!!!! ZFNet 2013ë…„ ìš°ìŠ¹ ëª¨ë¸ì´ë©° AlexNetì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ê°œì„ í•œ ëª¨ë¸ì´ë‹¤. AlexNetê³¼ ê°™ì€ Layer ìˆ˜ì´ê³ , ê¸°ì¡´ì˜ êµ¬ì¡°ë„ ê°™ë‹¤. ë‹¤ë§Œ, stride size, í•„í„° ìˆ˜ ê°™ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì ˆí•´ì„œ AlexNetì˜ Error rateë¥¼ ì¢€ ë” ê°œì„ ì‹œì¼°ë‹¤. ì•ìœ¼ë¡œ ì–¸ê¸‰í•  ë‘ê°€ì§€ ëª¨ë¸ì€ Batch normalizationì´ ì—†ë˜ ì‹œì ˆì´ì—ˆê¸° ë•Œë¬¸ì— ê¹Šì€ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ì¼ì´ ì–´ë ¤ì› ë‹¤. ê·¸ë ‡ê¸°ì— ê¹Šì€ ëª¨ë¸ì„ ìˆ˜ë ´ì‹œí‚¤ê¸° ìœ„í•´ì„œ ê°ì¢… í…Œí¬ë‹‰ì„ ì‚¬ìš©í•´ì•¼ í–ˆë‹¤.\u001d ë¨¼ì €, VGGëŠ” ì´ˆê¸°ì— 11 Layerì´ì—ˆëŠ” 11 Layerê°€ ëª¨ë¸ì´ ì˜ ìˆ˜ë ´í•˜ëŠ” í•œê³„ì˜€ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ 11 Layer ì¤‘ê°„ì— Layerë¥¼ ë¬´ì‘ìœ„ë¡œ ì¶”ê°€í•´ì„œ VGG-16, VGG-19ë¥¼ í•™ìŠµì‹œì¼°ë‹¤. GoogLeNetì˜ ê²½ìš°ì—ëŠ” auxiliary classifiersë¥¼ ë„ì…í•˜ì—¬ ë‹¨ì§€ ë„¤íŠ¸ì›Œí¬ì˜ ì´ˆê¸° Layerì— gradientë¥¼ ì§ì ‘ í˜ë ¤ ë³´ë‚´ê¸° ìœ„í•œ ìˆ˜ë‹¨ì´ì—ˆë‹¤.(ì„±ëŠ¥ì„ ì˜¬ë¦¬ê¸° ìœ„í•´ ë„ì…í•œê²ƒì´ ì•„ë‹ˆë‹¤! ê·¸ë¦¬ê³  Batch Normì´ ìˆë‹¤ë©´ êµ³ì´ ì´ëŸ°ì‹ì˜ í…Œí¬ë‹‰ë“¤ì€ ë”ì´ìƒ í•„ìš”ì¹˜ ì•Šë‹¤!!!) VGGNet í›¨ì”¬ ë” ê¹Šì–´ì¡Œê³  ë” ì‘ì€ í•„í„°ë¥¼ ì‚¬ìš©í•œë‹¤. ë” ê¹Šê²Œ ìŒ“ì´ë¯€ë¡œì¨ Non-Linearityë¥¼ ë” ì¶”ê°€ í•  ìˆ˜ ìˆê³  íŒŒë¼ë¯¸í„° ìˆ˜ë„ ë” ì ì–´ì§€ê²Œ ë˜ì—ˆë‹¤. AlexNetì—ì„œëŠ” 8ê°œì˜ Layer -&gt; VGGNet 16~19ê°œì˜ Layer 3x3í•„í„°ë§Œ ì‚¬ìš© (ì´ì›ƒí”½ì…€ì„ í¬í•¨í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ì‘ì€ í•„í„°) ì´ìœ  : í•„í„°ì˜ í¬ê¸°ê°€ ì‘ìœ¼ë©´ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ë” ì ì–´ì„œ Depthë¥¼ ë” í‚¤ìš¸ ìˆ˜ ìˆë‹¤. 3x3 í•„í„°ë¥¼ ì—¬ëŸ¬ ê°œ ìŒ“ì€ ê²ƒì€ ê²°êµ­ 7x7 í•„í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ì‹¤ì§ˆì ìœ¼ë¡œ ë™ì¼í•œ Receptive Filterë¥¼ ê°€ì§€ëŠ” ê²ƒì´ë‹¤. ì‘ì€ í•„í„°ë¥¼ ìœ ì§€í•´ ì£¼ê³  ì£¼ê¸°ì ìœ¼ë¡œ Poolingì„ ìˆ˜í–‰í•˜ë©´ì„œ ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì„±í•˜ê²Œ ëœë‹¤. fc7 ì€ 4096 ì‚¬ì´ì¦ˆì˜ Layerì¸ë° ì•„ì£¼ ì¢‹ì€ feature representationì„ ê°€ì§€ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì¡Œìœ¼ë©° \bë‹¤ë¥¸ ë°ì´í„°ì—ì„œë„ feature ì¶”ì¶œì´ ì˜ë˜ë©° ë‹¤ë¥¸ Taskì—ì„œë„ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ë›°ì–´ë‚œ ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ìˆë‹¤. VGG19ì˜ ê²½ìš° VGG16ê³¼ ìœ ì‚¬í•œ ì•„í‚¤í…ì³ì´ì§€ë§Œ Conv Layerê°€ ì¡°ê¸ˆ ë” ì¶”ê°€ ë˜ì—ˆë‹¤. VGG19ê°€ ì•„ì£¼ ì¡°ê¸ˆ ë” ì¢‹ë‹¤. ê·¸ëŸ¬ë‚˜ ë³´í†µ VGG16ì„ ë” ë§ì´ ì‚¬ìš©í•œë‹¤. AlexNetì—ì„œ ì²˜ëŸ¼ ëª¨ë¸ ì„±ëŠ¥ì„ ìœ„í•´ì„œ ì•™ìƒë¸” ê¸°ë²•ì„ ì‚¬ìš©í–ˆë‹¤. ì§ˆë¬¸) 3x3ì¸ Strideê°€ 1ì¸ í•„í„° 3ê°œë¥¼ ìŒ“ê²Œ ë˜ë©´ ì‹¤ì§ˆì ì¸ Receptive Fieldê°€ ì–´ë–»ê²Œ ë ê¹Œ? Receptive Fieldì€ í•„í„°ê°€ í•œë²ˆì— ë³¼ ìˆ˜ ìˆëŠ” ì…ë ¥ì˜ Spatial areaì´ë‹¤. ì²«ë²ˆì§¸ Layerì˜ Receptive FieldëŠ” 3x3ì´ë‹¤. ë‘ ë²ˆì§¸ Layerì˜ ê²½ìš°ëŠ” ê° ë‰´ëŸ°ì´ ì²« ë²ˆì§¸ Layer ì¶œë ¥ì˜ 3x3 ë§Œí¼ì„ ë³´ê²Œ ë  ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  3x3 ì¤‘ì— ê° ì‚¬ì´ë“œëŠ” í•œ í”½ì…€ì”© ë” ë³¼ ìˆ˜ ìˆê²Œ ëœë‹¤. ë”°ë¼ì„œ ë‘ë²ˆì§¸ Layerì˜ ê²½ìš°ëŠ” ì‹¤ì œë¡œ 5x5ì˜ receptive filedë¥¼ ê°€ì§€ê²Œ ëœë‹¤. 3ë²ˆì§¸ Layerì˜ ê²½ìš° 2ë²ˆì§¸ Layerì˜ 3x3ì„ ë³´ê²Œëœë‹¤. ê·¸ë¦¬ê³  ì´ ê³¼ì •ì„ í”¼ë¼ë¯¸ë“œì²˜ëŸ¼ ê·¸ë ¤ë³´ë©´ ì…ë ¥ Layerì˜ 7x7ì„ ë³´ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.ë”°ë¼ì„œ ì‹¤ì§ˆì ì¸ Receptive FieldëŠ” ì—¬ê¸°ì—ì„œ 7x7ì´ ëœë‹¤. í•˜ë‚˜ì˜ 7x7 í•„í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•˜ë‹¤. ì§ˆë¬¸) í•˜ë‚˜ì˜ Conv Layer ë‚´ì— ì—¬ëŸ¬ê°œì˜ í•„í„°ê°€ ì¡´ì¬í•˜ëŠ” ì´ìœ ëŠ”? ê° í•„í„°ê°€ ì¡´ì¬í•˜ëŠ” ì´ìœ ëŠ” ì„œë¡œ ë‹¤ë¥¸ íŒ¨í„´ì„ ì¸ì‹í•˜ê¸° ìœ„í•´ì„œë¼ê³  í•  ìˆ˜ ìˆë‹¤. ê° í•„í„°ëŠ” ê°ê°ì˜ Feature Mapì„ ë§Œë“¤ê²Œ ëœë‹¤. ì§ˆë¬¸) Localizationì€ ë¬´ì—‡ì¸ê°€? task ì¤‘ì—ì„œ ì˜ˆë¥¼ ë“¤ë©´ â€œì´ë¯¸ì§€ì— ê³ ì–‘ì´ê°€ ìˆëŠ”ê°€?â€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê²ƒ ë¿ë§Œ ì•„ë‹ˆë¼ ì •í™•íˆ ê³ ì–‘ì´ê°€ ì–´ë””ì— ìˆëŠ”ì§€ ë„¤ëª¨ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ëŠ” ê²ƒì´ë‹¤. Detectionì€ ì´ë¯¸ì§€ ë‚´ì— ë‹¤ìˆ˜ì˜ ê°ì²´ê°€ ì¡´ì¬í•  ìˆ˜ ìˆë‹¤. ê·¸ì— ë°˜í•´ localizationì€ ì´ë¯¸ì§€ì— ê°ì²´ê°€ í•˜ë‚˜ë§Œ ìˆë‹¤ê³  ê°€ì •í•˜ê³  ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ê³  ì¶”ê°€ì ìœ¼ë¡œ ë„¤ëª¨ë°•ìŠ¤ë„ ì³ì•¼í•œë‹¤. ì§ˆë¬¸) ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ Layerì˜ í•„í„° ê°¯ìˆ˜ë¥¼ ëŠ˜ë ¤ì•¼ í•˜ëŠ”ì§€?(Channel Depthë¥¼ ëŠ˜ë ¤ì•¼ í•˜ëŠ”\u001dì§€) ë””ìì¸í•˜ê¸° ë‚˜ë¦„ì´ê³  ë°˜ë“œì‹œ ê·¸ëŸ´ í•„ìš”ëŠ” ì—†ë‹¤. í•˜ì§€ë§Œ, ì‹¤ì œë¡œ ì‚¬ëŒë“¤ì´ Depthë¥¼ ë§ì´ ëŠ˜ë¦¬ëŠ” ê²½ìš°ê°€ ë§ë‹¤. Depthë¥¼ ëŠ˜ë¦¬ëŠ” ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ” ê³„ì‚°ëŸ‰ì„ ì¼ì •í•˜ê²Œ ìœ ì§€ì‹œí‚¤ê¸° ìœ„í•´ì„œì´ë‹¤. ì™œëƒí•˜ë©´ ë³´í†µ ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ê° Layerì˜ ì…ë ¥ì„ Down samplingí•˜ê²Œ ëœë‹¤. ì¦‰, ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ë‚˜ê°€ë©´ì„œ ì ì  ì •ë³´ë¥¼ ìƒì–´ë‚˜ê°€ëŠ” í˜„ìƒì´ ë°œìƒë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ Spatial areaê°€ ì‘ì•„ì§ˆìˆ˜ë¡ í•„í„°ì˜ depthë¥¼ ì¡°ê¸ˆì”© ëŠ˜ë ¤ì£¼ê²Œ ëœë‹¤. Widthë‚˜ Heightê°€ ì‘ì•„ì§€ê¸° ë•Œë¬¸ì— Depthë¥¼ ëŠ˜ë ¤ë„ ë¶€ë‹´ì´ ì—†ë‹¤. GoogLeNet 2014ë…„ Classification Challengeì—ì„œ ìš°ìŠ¹í•œ ëª¨ë¸ì´ë‹¤. 22ê°œì˜ Layerë¥¼ ê°€ì§„ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì´ë‹¤. ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ íš¨ìœ¨ì ì¸ ê³„ì‚°ì— ê´€í•œ ê·¸ë“¤ì˜ íŠ¹ë³„í•œ ê´€ì ì´ ìˆë‹¤ëŠ” ê²ƒê³¼ ë†’ì€ ê³„ì‚°ëŸ‰ì„ ì•„ì£¼ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ë„ë¡ ë„¤íŠ¸ì›Œí¬ë¥¼ ë””ìì¸í–ˆë‹¤ëŠ” ì ì´ë‹¤. â€œa good local network typologyâ€ë¥¼ ë””ìì¸ í•˜ê³  ì‹¶ì—ˆë‹¤. ê·¸ë¦¬ê³  â€œnetwork within a networkâ€ë¼ëŠ” ê°œë…ìœ¼ë¡œ local topologyë¥¼ êµ¬í˜„í–ˆê³  ì´ë¥¼ ìŒ“ì•„ì˜¬ë ¸ë‹¤. Inception moduleì„ ì—¬ëŸ¬ê°œ ìŒ“ì•„ì„œ ë§Œë“ ë‹¤. ë˜í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì¤„ì´ê¸° ìœ„í•´ FC-Layerë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ 60Mì¸ AlexNetì— ë¹„í•´ GoogLeNetì€ ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ 5M ì •ë„ì´ë‹¤. Inception Module ë‚´ë¶€ì—ëŠ” ë™ì¼í•œ ì…ë ¥ì„ ë°›ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë‹¤ì–‘í•œ í•„í„°ë“¤ì´ ë³‘ë ¬ë¡œ ì¡´ì¬í•œë‹¤. ì´ì „ Layerì˜ ì…ë ¥ì„ ë°›ì•„ì„œ ë‹¤ì–‘í•œ Conv ì—°ì‚°ì„ ìˆ˜í–‰ í•œ í›„ ê° Layerì—ì„œ ê°ê°ì˜ ì¶œë ¥ ê°’ë“¤ì´ ë‚˜ì˜¨ë‹¤. ê·¸ ì¶œë ¥ë“¤ì„ ëª¨ë‘ Depth ë°©í–¥ìœ¼ë¡œ í•©ì¹œë‹¤.(concatenate) ê·¸ë ‡ê²Œ í•©ì¹˜ë©´ í•˜ë‚˜ì˜ tensorë¡œ ì¶œë ¥ì´ ê²°ì •ë˜ê³  ì´ í•˜ë‚˜ì˜ ì¶œë ¥ì„ ë‹¤ìŒ ë ˆì´ì–´ë¡œ ì „ë‹¬í•˜ëŠ” ê²ƒì´ë‹¤. ì§ˆë¬¸) ì´ëŸ¬í•œ ë‹¤ì–‘í•œ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³  ì´ë¥¼ í•˜ë‚˜ë¡œ í•©ì³ì£¼ëŠ” ì•„ì£¼ ë‹¨ìˆœí•œ ë°©ì‹ì´ ê°–ëŠ” ë¬¸ì œì ì€ ë¬´ì—‡ì¼ê¹Œ? ê³„ì‚° ë¹„ìš©ì— ë¬¸ì œê°€ ìˆë‹¤. 1x1 convì˜ ê²½ìš° ì…ë ¥ì— ë§ì¶° depthëŠ” 256ì´ë‹¤. ê·¸ë¦¬ê³  128ê°œì˜ í•„í„° í•˜ë‚˜ë‹¤. ê·¸ë¦¬ê³  128ê°œì˜ í•„í„° í•˜ë‚˜ ë‹¹ 28x28 Feature mapì„ ìƒì„±í•˜ê²Œ ë  ê²ƒì´ë‹¤. ì´ëŸ°ì‹ìœ¼ë¡œ ë‹¤ë¥¸ Layerì˜ ì¶œë ¥ê°’ì„ ê³„ì‚°í•´ ë³´ë©´ ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì„ ê²ƒì´ë‹¤. ì°¸ê³ ë¡œ ì´ëŸ° ê³„ì‚°ì´ ë‚˜ì˜¨ ì´ìœ ëŠ” spatial dimensionì„ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ zero paddingì„ í•˜ì˜€ê¸° ë•Œë¬¸ì´ë‹¤.Strideë¥¼ ì˜ ì¡°ì ˆí•´ì„œ Spatial dimensionë¥¼ ìœ ì§€í•˜ë©´ ì…ë ¥ê³¼ ì¶œë ¥ì˜ í¬ê¸°ëŠ” ê°™ê²Œ ëœë‹¤. ì¦‰ 28x28ì€ ë™ì¼í•˜ê³  depthê°€ ì ì  ìŒ“ì´ê²Œ ëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ê·¸ë¦¼ì—ì„œëŠ” ìµœì¢…ì ìœ¼ë¡œ 28 x 28 x 672 ê°€ ëœë‹¤. Inception moduleì˜ ì…ë ¥ì€ 28x28x256 ì´ì—ˆìœ¼ë‚˜ ì¶œë ¥ì€ 28x28x672ì´ ëœ ê²ƒì´ë‹¤. Spatial dimensionì€ ë³€í•˜ì§€ ì•Šì•˜ì§€ë§Œ depthê°€ ì—„ì²­ë‚˜ê²Œ ë¶ˆì–´ë‚œ ê²ƒì´ë‹¤. ì—°ì‚°ëŸ‰ì´ \u001cë§ë‹¤ëŠ” ê²ƒì´ ë¬¸ì œì´ë©°, Pooling LayerëŠ” Depthë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ê¸° ë•Œë¬¸ì— ë¬¸ì œë¥¼ ì•…í™” ì‹œí‚¨ë‹¤. ìœ„ì™€ ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ key insight bottleneck layerë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì´ë‹¤. Conv ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê¸°ì— ì•ì„œ ì…ë ¥ì„ ë” ë‚®ì€ ì°¨ì›ìœ¼ë¡œ ë³´ë‚´ëŠ” ê²ƒì´ë‹¤.(depthë¥¼ ë” ë‚®ì€ ì°¨ì›ìœ¼ë¡œ projectioní•˜ëŠ” \u001dê²ƒ) input feature mapë“¤ ê°„ì˜ ì„ í˜• ê²°í•©(linear combination)ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì£¼ìš” ì•„ì´ë””ì–´ëŠ” ë°”ë¡œ ì…ë ¥ì˜ depthë¥¼ ì¤„ì´ëŠ” ê²ƒì´ë‹¤.ê° Layerì˜ ê³„ì‚°ëŸ‰ì€ 1x1 convë¥¼ í†µí•´ ì¤„ì–´ë“ ë‹¤. ì§ˆë¬¸) 1x1 Convë¥¼ ìˆ˜í–‰í•˜ë©´ ì¼ë¶€ ì •ë³´ì†ì‹¤ì´ ë°œìƒí•˜ì§€ ì•ŠëŠ”ë‹¤? ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•  ìˆœ ìˆì§€ë§Œ ë™ì‹œì— redundancyê°€ ìˆëŠ” imput featuresë¥¼ ì„ í˜•ê²°í•© í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. 1x1 convë¡œ ì„ í˜•ê²°í•©ì„ í•˜ê³  non-Linearity(ReLUê°™ì€)ë¥¼ ì¶”ê°€í•˜ë©´ ë„¤íŠ¸ì›Œí¬ê°€ ë” ê¹Šì–´ì§€ëŠ” íš¨ê³¼ë„ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ 1x1 conë¥¼ ì¶”ê°€í•˜ë©´ ì—¬ëŸ¬ëª¨ë¡œ ë„ì›€ì´ ë˜ê³  ë” ì˜ ë™ì‘í•œë‹¤. ìœ„ì˜ ê·¸ë¦¼ì—ì„œ íŒŒë€ìƒ‰ ë„¤ëª¨ ë°•ìŠ¤ëŠ” ì¶”ê°€ì‹œí‚¨ ë³´ì¡° ë¶„ë¥˜ê¸°(auxiliary classifier)ì´ë‹¤. ê·¸ êµ¬ì¡°ëŠ” Average poolingê³¼ 1x1 convê°€ ìˆê³  FC-layerë„ ëª‡ê°œ ë¶™ëŠ” ìš°ë¦¬ê°€ ì•Œê³ ìˆëŠ” ì‘ì€ ë„¤íŠ¸ì›Œí¬ë“¤ì´ë‹¤. SoftMaxë¡œ 1000ê°œì˜ ImageNet classë¥¼ êµ¬ë¶„í•œë‹¤. ë˜í•œ ë„¤íŠ¸ì›Œí¬ê°€ ê¹Šê¸° ë•Œë¬¸ì•  ì´ê³³ì—ì„œë„ lossë¥¼ ê³„ì‚°í•˜\bê³  ì¶”ê°€ì ì¸ gradientë¥¼ ì–»ì„ ìˆ˜ ìˆê³  ë”°ë¼ì„œ ì¤‘ê°„ Layerì˜ í•™ìŠµì„ ë„ìš¸ ìˆ˜ ìˆë‹¤. ì§ˆë¬¸) ë³´ì¡°ë¶„ë¥˜ê¸°ì—ì„œ ë‚˜ì˜¨ ê²°ê³¼ë¥¼ ìµœì¢… ë¶„ë¥˜ ê²°ê³¼ì— ì´ìš©í•  ìˆ˜ ìˆëŠ”ê°€?? GoogLeNet í•™ìŠµ ì‹œ, ê° ë³´ì¡°ë¶„ë¥˜ê¸°ì˜ Lossë¥¼ ëª¨ë‘ í•©ì¹œ í‰ê· ì„ ê³„ì‚°í•œë‹¤. ì•„ë§ˆë„ ë„ì›€ì´ ë  ê²ƒì´ë‹¤. ì§ˆë¬¸) bottleneck layerë¥¼ êµ¬ì„±í•  ë•Œ 1x1 conv ë§ê³  ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì°¨ì›ì„ ì¶•ì†Œì‹œì¼œë„ ë˜ëŠ”ê°€?? ì—¬ê¸°ì—ì„œ 1x1 convë¥¼ ì“´ ì´ìœ ëŠ” ì°¨ì› ì¶•ì†Œì˜ íš¨ê³¼ë„ ìˆê³  ë‹¤ë¥¸ Layerë“¤ ì²˜ëŸ¼ conv Layerì´ê¸° ë•Œë¬¸ì´ë‹¤. ì°¨ì› ì¶•ì†Œ ê³¼ì •ì—ì„œ ì´ì „ì˜ feature mapê³¼ ì—°ê´€ì´ ìˆëŠ”ì§€ í•™ìŠµí•˜ë ¤ë©´ ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ Backpropìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆì–´ì•¼ í•œë‹¤. ë„¤íŠ¸ì›Œí¬ê°€ ì—„ì²­ ê¹Šì€ ê²½ìš°ì—ì„œëŠ” gradient ì‹ í˜¸ê°€ ì ì  ì‘ì•„ì§€ê²Œ ë˜ê³  ê²°êµ­ì—ëŠ” 0ì— ê°€ê¹ê²Œ ë  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë³´ì¡° ë¶„ë¥˜ê¸°ë¥¼ ì´ìš©í•´ì„œ ì¶”ê°€ì ì¸ gradient ì‹ í˜¸ë¥¼ í˜ë ¤ì¤€ë‹¤. ì´ ë•Œ backpropì€ ê° ë³´ì¡°ë¶„ë¥˜ê¸° ë³„ë¡œ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ ë„¤íŠ¸ì›Œí¬ ì „ì²´ê°€ í•œë²ˆì— ì‘ë™ì‹œí‚¨ë‹¤. ì§ˆë¬¸) ê° Layerê°€ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ í•˜ëŠ”ê°€ ì•„ë‹Œê°€?? ëª¨ë“  Layerë¥¼ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ í•˜ì§€ ì•ŠëŠ”ë‹¤. ResNet 2015ë…„ë„ ìš°ìŠ¹ ëª¨ë¸ì´ë©°, í˜ëª…ì ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ì˜ ê¹Šì´ê°€ ê¹Šì–´ì§„ ëª¨ë¸ì´ë‹¤.(152ê°œì˜ Layer) residual connection ì´ë¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. residual blockë“¤ì„ ìŒ“ì•„ì˜¬ë¦¬ëŠ” êµ¬ì¡°ì´ë‹¤. short connectionê³¼ residual blockì´ ë„ì…ëœ ëª¨\u001dë¸ ëª¨ë¸ Depthê°€ 50ì´ìƒì¼ ë•Œ Bottleneck Layersë¥¼ ë„ì… â€˜CNNì„ ê¹Šê³  ë” ê¹Šê²Œ ìŒ“ê²Œ ë˜ë©´ ì–´ë–¤ ì¼ì´ ë°œìƒí• ê¹Œ?â€™ë¼ëŠ” ì˜ë¬¸ì—ì„œ ë¶€í„° ì‹œì‘ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´, VGGì— conv pool Layerë¥¼ ê¹Šê²Œë§Œ ìŒ“ëŠ”ë‹¤ê³  ê³¼ì—° ì„±ã„´ëŠ¥ì´ ë” ì¢‹ì•„ì§€ëŠ” ê²ƒì´ ë§ëŠ”ì§€ë¥¼ ë³´ìë©´ ì•„ë‹ˆë¼ëŠ” ê²ƒì´ë‹¤. ë‹¤ìŒ ê·¸ë¦¼ì—ì„œ 56-Layerì™€ 20-Layerë¥¼ ë¹„êµí•´ì„œ ì„¤ëª…í•˜ê³  ìˆë‹¤. ìš°ë¦¬ëŠ” 56-LayerëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë” ë§ì€ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ê³  ìˆê¸°ì— 20-Layerë³´ë‹¤ ì¢€ë” overfittingì´ ì¼ì–´ë‚  í™•ë¥ ì´ ë†’ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. í—ˆë‚˜ ìš°ë¦¬ ì˜ˆìƒì²˜ëŸ¼ test errorëŠ” 56-layerê°€ ë” ë‚®ì§€ë§Œ training error ë˜í•œ ë” ë‚®ê¸°ì— overfittingì´ ì›ì¸ì´ ì•„ë‹ˆë¼ëŠ” ê²ƒì„ í™•ì‹¤íˆ ì•Œ ìˆ˜ ìˆë‹¤. ResNet ì €ìë“¤ì•„ ë‚´ë¦° ê°€ì„¤ì€ ë” ê¹Šì€ ëª¨ë¸ í•™ìŠµ ì‹œ Optimizationì— ë¬¸ì œê°€ ìƒê¸´ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ëª¨ë¸ì´ ë” ê¹Šë‹¤ë©´ ì ì–´ë„ ë” ì–•ì€ ëª¨ë¸ë§Œí¼ì€ ì„±ëŠ¥ì´ ë‚˜ì™€ì•¼ í•˜ì§€ ì•ŠëŠ”ê°€ë¼ëŠ” ìƒê°ìœ¼ë¡œ ì¸í•´ ë” ì–•ì€ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê¹Šì€ ëª¨ë¸ì˜ ì¼ë¶€ Layerì— ë³µì‚¬í•œë‹¤. ê·¸ë¦¬ê³  ë‚˜ë¨¸ì§€ LayerëŠ” identity mappingì„ í•˜ì—¬ Deeper Modelì˜ í•™ìŠµì´ ì œëŒ€ë¡œ ì•ˆë˜ë”ë¼ë„ ì ì–´ë„ shallower Model ë§Œí¼ì˜ ì„±ëŠ¥ì„ ë³´ì¥í•˜ê²Œë” ë””ìì¸ í•œë‹¤. ì´ëŸ° ì•„ì´ë””ì–´ë¥¼ ëª¨ë¸ì— ì ìš©ì‹œí‚¤ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ê°€ ì—†ìœ¼ë©´ ì…ë ¥ì„ identity mappingì„ ì‹œì¼œ ì¶œë ¥ìœ¼ë¡œ ë‚´ë³´ë‚´ëŠ” Skip Connectionì„ ë„ì…í•˜ê²Œ ëœë‹¤. ì‹¤ì œ LayerëŠ” ë³€í™”ëŸ‰(delta)ë§Œ í•™ìŠµí•˜ë©´ ëœë‹¤. ì…ë ¥ Xì— ëŒ€í•œ ì”ì°¨(residual)ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì§ˆë¬¸) Layerì˜ ì¶œë ¥ê³¼ Skip Connectionì˜ ì¶œë ¥ì´ ê°™ì€ ì°¨ì›ì¸ê°€? ê·¸ë ‡ë‹¤. ë‘ ê²½ë¡œì˜ ì¶œë ¥ ê°’ ëª¨ë‘ ê°™ì€ ì°¨ì›ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œëŠ” ê°™ì€ ì°¨ì›ì´ ë§ì§€ë§Œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” Depth-wise paddingìœ¼ë¡œ ì°¨ì›ì„ ë§ì¶°ì¤€ë‹¤. ì§ˆë¬¸) Layerì˜ ì¶œë ¥ì¸ Residualì˜ ì˜ë¯¸ëŠ” ë¬´ì—‡ì¸ê°€? ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´, ì „ì²´ ì¶œë ¥ ê°’ì€ F(x)+X ì´ê³ , F(x)ëŠ” Layerì˜ ì¶œë ¥ ê°’ì´ë‹¤. XëŠ” ê·¸ì € ì…ë ¥ê°’ì´ë‹¤. ì™¼ìª½ì˜ í‰ë²”í•œ ë„¤íŠ¸ì›Œí¬ëŠ” H(x)ë¥¼ í•™ìŠµì‹œí‚¤ê³  ìˆì§€ë§Œ ì•„ì£¼ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” H(x)ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì€ ë„ˆë¬´ ì–´ë µë‹¤. ê·¸ë˜ì„œ ResNetì˜ ì•„ì´ë””ì–´ëŠ” H(x)=F(x)+xì´ë¯€ë¡œ F(x)ë¥¼ í•™ìŠµì‹œì¼œë³´ë©´ ì–´ë–¨ê¹Œë¼ëŠ” ê²ƒì´ë‹¤. ì¦‰, H(x)ë¥¼ ì§ì ‘ ë°°ìš°ëŠ” ëŒ€ì‹ ì— Xì— ì–¼ë§ˆì˜ ê°’ì„ ë”í•˜ê³  ë¹¼ì•¼í• ê¹Œë¥¼ ë°°ìš°ëŠ” ê²ƒì´ ì‰¬ìš¸ê²ƒì´ë¼ê³  ìƒê°í•œ ê²ƒì´ë‹¤. ì´ê²ƒì€ ë‹¨ì§€ ê°€ì„¤ì¼ ë¿ì´ì§€ë§Œ ê°€ì„¤ì´ ì°¸ì´ë¼ë©´ ëª¨ë¸ì˜ ì¼ë¶€ê°€ í•™ìŠµëœ shallow layersì´ê³  ë‚˜ë¨¸ì§€ layerë“¤ì€ identityë¡œ êµ¬ì„±ë˜ì–´ì§„ ìƒí™©ì—ì„œëŠ” ì˜ ë™ì‘í•  ê²ƒì´ë‹¤. ì´ëŠ” ëŒ€ë¶€ë¶„ì˜ layerê°€ ì˜ ë™ì‘í•˜ë ¤ë©´ layerì˜ ì¶œë ¥ì´ identityì— ê°€ê¹Œì›Œì•¼ í• ì§€ ëª¨ë¥¸ë‹¤ëŠ” ê²ƒì„ ì•”ì‹œí•œë‹¤. ì´ ë•Œë¬¸ì— Identity(Input) + ë³€í™”ëŸ‰(delta)ë§Œ í•™ìŠµì‹œí‚¤ë©´ ëœë‹¤. ì˜ˆë¥¼ë“¤ì–´, Output = Input(Identity)ì´ì–´ì•¼ë§Œ í•˜ëŠ” ìƒí™©ì´\u001dë©´ F(x) = 0 ì´ ë˜ë©´ ê·¸ë§Œì´ë‹¤. ì´ëŠ” ìƒëŒ€ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê¸° ì‰½ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ResNetì—ì„œëŠ” Layerì˜ ì¶œë ¥ì€ ì…ë ¥ + residual blockì˜ ì¶œë ¥ì´ë‹¤. ìš°ì„  residual blockì˜ ê°€ì¤‘ì¹˜ê°€ 0ì´ë©´ ì´ blockì€ identity mappingì„ í•œë‹¤. ì´ëŸ¬í•œ ì†ì„±ìœ¼ë¡œ ëª¨ë¸ì´ í•„ìš”ì—†ëŠ” Layerë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ í•™ìŠµí•˜ëŠ”ë° ì•„ì£¼ ìœ ìš©í•˜ë‹¤. ResNetì˜ ê´€ì ì—ì„œ L2 Regularizationì„ í•´ì„í•´ ë³¼ ìˆ˜ë„ ìˆë‹¤. Layerì— L2 Regularizationì„ ì¶”ê°€ì‹œí‚¤ë©´ L2ëŠ” ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ 0ì´ ë˜ë„ë¡ ë…¸ë ¥í•  ê²ƒì´ë‹¤. ì‚¬ì‹¤ CNN Architecturesì˜ ê´€ì ì—ì„œ ë³´ë©´ ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ 0ì´ë©´ ì´ìƒí•˜ë‹¤. í•˜ì§€ë§Œ ResNetì˜ ê´€ì ì—ì„œëŠ” íŒŒë¼ë¯¸í„°ë¥¼ 0ìœ¼ë¡œ ë§Œë“œë ¤ëŠ” ì†ì„±ì€ ëª¨ë¸ì´ ë¶ˆí•„ìš”í•œ Layerë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šë„ë¡ í•´ì¤„ ìˆ˜ ìˆë‹¤.","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"ë‚´ê°€ ì •ë¦¬í•˜ëŠ” C/C++ 00","slug":"data_structure_00","date":"2019-07-24T08:03:14.000Z","updated":"2020-03-19T11:07:46.234Z","comments":true,"path":"2019/07/24/data_structure_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/24/data_structure_00/","excerpt":"","text":"ê°œë°œí™˜ê²½ êµ¬ì¶•í•˜ê¸°Cì™€ C++1) C++ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ Cì˜ ê¸°ëŠ¥ì„ í™•ì¥í•œ í˜•íƒœì˜ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì´ë‹¤.2) ë”°ë¼ì„œ C ì–¸ì–´ì˜ ê¸°ëŠ¥ì„ í¬í•¨í•˜ê³  ìˆë‹¤ëŠ” ì ì—ì„œ C++í”„ë¡œì íŠ¸ë¡œ .C í™•ì¥ìë¥¼ ê°–ëŠ” íŒŒì¼ì„ ìƒì„±í•˜ì—¬ ì½”ë”©í•´ë„ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•œë‹¤. ê°œë°œí™˜ê²½ êµ¬ì¶•í•˜ê¸° ë‚˜ëŠ” ê°œì¸ì ìœ¼ë¡œ IDE(Integrated Development Environment) ì¤‘ì—ì„œ Atomì„ ì´ë¯¸ ì„¤ì¹˜í•˜ê³  ìˆê¸°ì— ë”°ë¡œ ëŒ€í‘œì ì¸ Visual Studioë¥¼ ì„¤ì¹˜í•˜ì§„ ì•Šì•˜ë‹¤. Atomì—ì„œëŠ” ë”°ë¡œ gpp-compiler ë¼ëŠ” íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•´ì£¼ë©´ ëë‚œë‹¤! ë‹¨ì¶•í‚¤ ë˜í•œ ìì‹ ì´ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•  ìˆ˜ ìˆëŠ”ë°, ë‚˜ëŠ” defaultì¸ f5ê°€ compile f6ì´ ë””ë²„ê¹…ìœ¼ë¡œ ë˜ì–´ìˆëŠ” ìƒíƒœì—ì„œ ì‚¬ìš©í•  ê²ƒì´ë‹¤. Development_environment ì „í†µì ì¸ í”„ë¡œê·¸ë¨ì€ ì „ì²˜ë¦¬ê¸° -&gt; ì»´íŒŒì¼ëŸ¬ -&gt; ë§ì»¤ë¥¼ ê±°ì³ ì‹¤í–‰íŒŒì¼ë¡œ ë§Œë“¤ì–´ì§„ë‹¤. ì—­ì‹œ! ëª¨ë“  ì–¸ì–´ì˜ ê¸°ì´ˆë¥¼ ë°°ìš¸ë•Œ í•˜ëŠ” Hello Worldë¥¼ ì–¸ê¸‰í•˜ë©° ì‹œì‘í•´ë³´ì! 12345678#include &lt;stdio.h&gt;int main(void) &#123; printf(\"Hello world\\n\"); // system(\"pause\"); system( \"read -n 1 -s -p \\\"Press any key to continue...\\\"\" ); return 0;&#125; ëª…ë ¹ë¬¸ í•˜ë‚˜í•˜ë‚˜ì”© ì„¤ëª…ì„ í•˜ìë©´ ì²˜ìŒ, #include ëª…ë ¹ì–´ë¥¼ ì´ìš©í•´ ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ ì˜¬ ìˆ˜ ìˆë‹¤. ìœ„ì—ì„œ ë¶ˆëŸ¬ì˜¨ stdio.h(standard io)ëŠ” ì—¬ëŸ¬ ê¸°ë³¸ì ì¸ ê¸°ëŠ¥ì„ í¬í•¨í•˜ê³  ìˆì§€ë§Œ ê·¸ ì¤‘ ëŒ€í‘œì ìœ¼ë¡œ ìœ„ì—ì„œ ì‚¬ìš©í•œ printfê°€ ìˆë‹¤.mainí•¨ìˆ˜ëŠ” ë‹¤ì–‘í•œ í•¨ìˆ˜ê°€ ì‚¬ìš© ë  ìˆ˜ ìˆê² ì§€ë§Œ ì²˜ìŒ ì‹œì‘í•  ë•ŒëŠ” mainí•¨ìˆ˜ ì´í›„ì— ì‚¬ìš©í•œë‹¤. ë˜í•œ ê°€ì¥ í° íŠ¹ì§•ì€ í•¨ìˆ˜ëŠ” ë°˜í™˜ê°’ì´ ì—†ì„ ìˆ˜ë„ ìˆì§€ë§Œ main í•¨ìˆ˜ì—ì„œëŠ” í•­ìƒ 0ì„ ë°˜í™˜í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ë‚˜ì™€ ê°™ì´ Rê³¼ pythonìœ¼ë¡œ í”„ë¡œê·¸ë˜ë°ì„ ë°°ìš´ ì‚¬ëŒì´ë¼ë©´ ;ì´ ë‚¯ì„¤ì„ ê²ƒì´ë‹¤. C/C++ì—ì„œëŠ” í•˜ë‚˜ì˜ ëª…ë ¹ì–´ê°€ ëë‚¬ìŒì„ ì•Œë¦¬ê¸° ìœ„í•´ ;ì„ ë¶™ì¸ë‹¤. ìœ„ì˜ ëª…ë ¹ í”„ë¡¬í”„íŠ¸ì—ì„œ pause ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰ì‹œí‚¤ë©´ í‚¤ë³´ë“œë¥¼ ì…ë ¥ ì „ê¹Œì§€ ëŒ€ê¸°í•˜ëŠ” ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. system í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ìš´ì˜ì²´ì œì˜ ê¸°ë³¸ì ì¸ ê¸°ëŠ¥ì„ ì´ìš©í•  ìˆ˜ ìˆë‹¤. í—ˆë‚˜, ë‚˜ì˜ macë¶ì€ windowì˜ í”„ë¡¬í”„íŠ¸ ì°½ê³¼ëŠ” ë‹¤ë¥¸ ìš´ì˜ì²´ì œì´ë¯€ë¡œ ë‹¹ì—°íˆ pauseê°€ ê±¸ë¦¬ì§€ ì•ŠëŠ”ë‹¤! ì‹¤í–‰í•˜ë©´ ì°¾ì„ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ë¼ê³  ë‚˜ì˜¤ë¯€ë¡œ ì•½ê°„ì˜ í¸ë²•ìœ¼ë¡œ êµ¬ì‚¬í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ë‹¤! í˜¹ì‹œë¼ë„ macì—ì„œ pauseë¥¼ ê±¸ë ¤í•œë‹¤ë©´ ì €ëŸ°ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ ë³´ëŠ” ë°©ë²•ë„ ìˆë‹¤.","categories":[{"name":"C/C++/ìë£Œêµ¬ì¡°","slug":"C-C-ìë£Œêµ¬ì¡°","permalink":"https://heung-bae-lee.github.io/categories/C-C-ìë£Œêµ¬ì¡°/"}],"tags":[]},{"title":"[CS231n]Lecture08-Deep learning Software","slug":"cs231n_08","date":"2019-07-23T05:13:30.000Z","updated":"2019-07-24T07:35:38.400Z","comments":true,"path":"2019/07/23/cs231n_08/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/23/cs231n_08/","excerpt":"","text":"GPU Graphics card ë˜ëŠ” Graphics Processing Unitì´ë¼ê³  í•˜ëŠ”ë° ê²°êµ­ì—” ìš°ë¦¬ê°€ ì•„ëŠ” ì‚¬ì‹¤ ì²˜ëŸ¼ computer graphicsë¥¼ ëœë”ë§í•˜ê¸° ìœ„í•´ ë” ì™€ë‹¿ê²Œ ë§í•˜ìë©´ ê²Œì„ì„ ë” ìµœì ì˜ í™˜ê²½ì—ì„œ í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ ì¡Œë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. # Cores Clock speed Memory Price CPU 4 4.4 GHz Shared with system $339 CPU 10 3.5 GHz Shared with system $1723 GPU 3840 1.6 GHz 12GB GDDR5X $1200 GPU 1920 1.68 GHz 8GB GDDR5 $399 ìœ„ì˜ í‘œì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ CPUì˜ ê²½ìš° coreì˜ ìˆ˜ê°€ ì ë‹¤. GPUëŠ” CPUë³´ë‹¤ í›¨ì”¬ ë” ë§ì€ coreë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ ê°ê°ì˜ ì½”ì–´ê°€ ë” ëŠë¦° clock speedì—ì„œ ë™ì‘í•˜ë©° ê·¸ ì½”ì–´ë“¤ì´ CPUì²˜ëŸ¼ ë…ë¦½ì ìœ¼ë¡œ ë™ì‘í•˜ì§€ ì•Šìœ¼ë©° ë§ì€ ì¼ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ë‹¤. GPUëŠ” ì½”ì–´ë§ˆë‹¤ ë…ë¦½ì ì¸ í…ŒìŠ¤í¬ê°€ ìˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ë§ì€ ì½”ì–´ë“¤ì´ í•˜ë‚˜ì˜ í…ŒìŠ¤í¬ë¥¼ ë³‘ë ¬ì ìœ¼ë¡œ ìˆ˜í–‰í•œë‹¤. GPUì˜ ì½”ì–´ì˜ ìˆ˜ê°€ ë§ë‹¤ëŠ” ê²ƒì€ ì–´ë–¤ í…ŒìŠ¤í¬ê°€ ìˆì„ ë•Œ ê·¸ í…ŒìŠ¤í¬ì— ëŒ€í•´ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ê¸° ì•„ì£¼ ì í•©í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. CPUì—ë„ ìºì‹œê°€ ìˆì§€ë§Œ ë¹„êµì  ì‘ë‹¤. ëŒ€ë¶€ë¶„ì˜ memoryëŠ” RAMì—ì„œ ëŒì–´ë‹¤ ì“´ë‹¤. ì‹¤ì œ RAMê³¼ GPUê°„ì˜ í†µì‹ ì€ ìƒë‹¹í•œ ë³´í‹€ë„¥ì„ ì´ˆë˜í•œë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— GPUëŠ” ë³´í†µ RAMì´ ë‚´ì¥ë˜ì–´ ìˆë‹¤. GPUëŠ” ë‚´ì¥ë˜ì–´ìˆëŠ” ë©”ëª¨ë¦¬ì™€ ì½”ì–´ ì‚¬ì´ì˜ ìºì‹±ì„ í•˜ê¸° ìœ„í•œ ì¼ì¢…ì˜ ë‹¤ê³„ì¸µ ìºì‹± ì‹œìŠ¤í…œì„ ê°€ì§€ê³  ìˆë‹¤. ì´ëŠ” CPUì˜ ìºì‹±êµ¬ì¡°ì™€ ë§¤ìš° ìœ ì‚¬í•˜ë‹¤. CPUëŠ” ë²”ìš©ì²˜ë¦¬ì— ì í•©í•˜ê³ , GPUëŠ” ë³‘ë ¬ì²˜ë¦¬ì— ë” íŠ¹í™”ë˜ì–´ ìˆë‹¤. GPUì—ì„œ ì •ë§ ì˜ ë™ì‘í•˜ê³  ì•„ì£¼ ì í•©í•œ ì•Œê³ ë¦¬ì¦˜ì€ ë°”ë¡œ í–‰ë ¬ê³± ì—°ì‚°ì´ë‹¤. ì‹¤ì œë¡œ GPUë¡œ í•™ìŠµì„ í•  ë•Œ ìƒê¸°ëŠ” ë¬¸ì œ ì¤‘ í•˜ë‚˜ëŠ” ë°”ë¡œ Modelê³¼ Modelì˜ ê°€ì¤‘ì¹˜ëŠ” ì „ë¶€ GPU RAMì— ìƒì£¼í•˜ê³  ìˆëŠ” ë°˜ë©´ì— Train dataëŠ” í•˜ë“œë“œë¼ì´ë¸Œ(SSD)ì— ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ë•Œë¬¸ì— Train timeì— ë””ìŠ¤í¬ì— ë””ìŠ¤í¬ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ë“¤ì´ëŠ” ì‘ì—…ì„ ì„¸ì‹¬í•˜ê²Œ ì‹ ê²½ì“°ì§€ ì•Šìœ¼ë©´ ë³´í‹€ë„¥ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ì¦‰,GPUëŠ” forward/backward ê°€ ì•„ì£¼ ë¹ ë¥¸ ê²ƒì€ ì‚¬ì‹¤ì´ì§€ë§Œ, ë””ìŠ¤í¬ì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ë“¤ì´ëŠ” ê²ƒì´ ë³´í‹€ë„¥(ë³‘ëª©í˜„ìƒ)ì´ ë˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í•´ê²°ì±… ì¤‘ í•˜ë‚˜ëŠ” ë°ì´í„°ì…‹ì´ ì‘ë‹¤ë©´ ë°ì´í„° ì „ì²´ë¥¼ RAMì— ì˜¬ë ¤ ë†“ëŠ” ê²ƒì´ë‹¤. ë˜ëŠ” ë°ì´í„°ì…‹ì´ ì‘ì§€ ì•Šë”ë¼ë„, ì„œë²„ì— RAM ìš©ëŸ‰ì´ í¬ë‹¤ë©´ ê°€ëŠ¥ í•  ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤. ë˜í•œ ê¸°ë³¸ì ìœ¼ë¡œ HDD ëŒ€ì‹  SSDë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ë˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” CPUì˜ multiple CPU threads(CPUì˜ ë‹¤ì¤‘ìŠ¤ë ˆë“œ)ë¥¼ ì´ìš©í•´ì„œ ë°ì´í„°ë¥¼ RAMì— ë¯¸ë¦¬ ì˜¬ë ¤ ë†“ëŠ” ê²ƒì´ë‹¤.(Pre-fetching)GPUëŠ” ë¹ ë¥¸ë° ë°ì´í„° ì „ì†¡ ìì²´ê°€ ì¶©ë¶„íˆ ë¹ ë¥´ì§€ ëª»í•˜ë©´ ë³´í‹€ë„¥ì´ ìƒê¸¸ìˆ˜ ë°–ì— ì—†ë‹¤. Deep learning frameworkDeep learning frameworkë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ 1) ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¥¼ ì´ìš©í•˜ë©´ ì—„ì²­ ë³µì¡í•œ ê·¸ë˜í”„ë¥¼ ìš°ë¦¬ê°€ ì§ì ‘ ë§Œë“¤ì§€ ì•Šì•„ë„ ëœë‹¤.2) forward passë§Œ ì˜ êµ¬í˜„í•´ ë†“ëŠ”ë‹¤ë©´ Back propagationì€ ì•Œì•„ì„œ êµ¬ì„±ë˜ì–´ gradientë¥¼ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.3) cuBLAS, cuDNN, CUDA ê·¸ë¦¬ê³  memoryë“±ì„ ì§ì ‘ ì„¸ì‹¬í•˜ê²Œ ë‹¤ë£¨ì§€ ì•Šê³  GPUë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. frameworkì˜ ì¡´ì¬ ëª©í‘œëŠ” forward pass ì½”ë“œë¥¼ NumpyìŠ¤ëŸ½ê²Œ ì‘ì„œì„ í•´ ë†“ìœ¼ë©´ GPUì—ì„œë„ ë™ì‘í•˜ê³  gradientë„ ì•Œì•„ì„œ ê³„ì‚°í•´ ì£¼ëŠ” ê²ƒì´ë‹¤. [ê·¸ë¦¼1][ê·¸ë¦¼2] Tensorflow placeholderëŠ” ê·¸ë˜í”„ ë°–ì—ì„œ ë°ì´í„°ë¥¼ ë„£ì–´ì£¼ëŠ” ë³€ìˆ˜ì´ê³ , variableì€ ê·¸ë˜í”„ ë‚´ë¶€ì— ìˆëŠ” ë³€ìˆ˜ì´ë‹¤. TensorflowëŠ” ë¶„ì‚°ì²˜ë¦¬ë„ ì§€ì›í•˜ê¸° ë–„ë¬¸ì— ì„œë¡œ ë‹¤ë¥¸ ë¨¸ì‹ ì„ ì´ìš©í•´ graphë¥¼ ìª¼ê°œì„œ ì‹¤í–‰ì‹œí‚¬ ìˆ˜ë„ ìˆë‹¤. í˜¹ ë¶„ì‚°ì²˜ë¦¬ë¥¼ ê³„íší•œë‹¤ë©´ Tensorflowê°€ ìœ ì¼í•œ ì„ íƒì§€ê°€ ë  ê²ƒì´ë‹¤. Pytorch Facebookì—ì„œ ë‚˜ì˜¨ PyTorchëŠ” TensorFlowì™€ëŠ” ë‹¤ë¥´ê²Œ 3ê°€ì§€ ì¶”ìƒí™” ë ˆë²¨ì„ ì •ì˜í•´ ë†“ì•˜ë‹¤. ì´ë¯¸ ê³ ìˆ˜ì¤€ì˜ ì¶”ìƒí™”ë¥¼ ë‚´ì¥í•˜ê³  ìˆê¸°ì— (Module ê°ì²´) TensorFlow ì²˜ëŸ¼ ì–´ë–¤ ëª¨ë“ˆì„ ì„ íƒí•  ì§€ ê³ ë¯¼í•  í•„ìš”ê°€ ì—†ë‹¤. tensor : Numpy arrayì™€ ìœ ì‚¬í•œ tensor objectê°€ ìˆìœ¼ë©° GPUì—ì„œ ì‘ë™í•œë‹¤. tensorflowì˜ Numpy array variable : ê·¸ë˜í”„ì˜ ë…¸ë“œ(ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ê³  gradient ë“±ì„ ê³„ì‚°) tensorflowì˜ Tensor, Variable, Placeholder Module : ì „ì²´ Neural networkë¥¼ êµ¬ì„± tensorflowì˜ tf.layers, TFSlim, TFLearn ë“± Static computational graph vs Dynamic graphPytorchì™€ TensorFlowì˜ ì£¼ëœ ì°¨ì´ì  ì¤‘ í•˜ë‚˜ì´ë‹¤. TensorFlowëŠ” ë‘ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ì§„ë‹¤.(Static computational graph - ê·¸ë˜í”„ê°€ ë‹¨ í•˜ë‚˜ë§Œ ê³ ì •ì ìœ¼ë¡œ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤.) 1) ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•˜ëŠ” ë‹¨ê³„ 2) êµ¬ì„±ëœ ê·¸ë˜í”„ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ëŒë¦¬ëŠ” ë‹¨ê³„ ê·¸ë˜í”„ë¥¼ í•œë²ˆ êµ¬ì„±í•´ ë†“ìœ¼ë©´ í•™ìŠµì‹œì—ëŠ” ë™ì¼í•œ ê·¸ë˜í”„ë¥¼ ì•„ì£¼ ë§ì´ ì¬ì‚¬ìš©í•˜ê²Œ ëœë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ê·¸ëŸ° ê·¸ë˜í”„ë¥¼ ìµœì í™”ì‹œí‚¬ ê¸°íšŒê°€ ì£¼ì–´ì§ˆ ìˆ˜ ìˆë‹¤. ì²˜ìŒ ìµœì í™” ì‹œí‚¬ ë•Œ ê¹Œì§€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤ í•˜ë”ë¼ë„ ìµœì í™”ëœ ê·¸ë˜í”„ë¥¼ ì—¬ëŸ¬ë²ˆ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ê³ ë ¤í•´ë³´ë©´ ê·¸ì— ë”°ë¥¸ ì†Œìš”ëœ ì‹œê°„ì€ ì¤‘ìš”ì¹˜ ì•Šì„ ìˆ˜ ë„ ìˆë‹¤. ë˜í•œ ë©”ëª¨ë¦¬ë‚´ì— ê·¸ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ ê°–ê³  ìˆë‹¤ëŠ” ê²ƒì´ë˜ë¯€ë¡œ \bë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ íŒŒì¼ í˜•íƒœë¡œ ì €ì¥í•  ìˆ˜ ìˆë‹¤. ê·¸ë˜í”„ì˜ ëª¨ë“  ì „ì²´ì ì¸ ì—°ì‚°ë“¤ì„ ë‹¤ ê³ ë ¤í•´ì„œ ë§Œë“¤ì–´ ì£¼ì–´ì•¼í•œë‹¤.(ex.loopë¬¸) TensorFlow Foldë¼ëŠ” TF ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ static graphìœ¼ë¡œ ë§Œë“  íŠ¸ë¦­ìœ¼ë¡œ dynamic graphsë¥¼ ì‘ì„±í•˜ê²Œ í•´ì¤€ë‹¤. PytorchëŠ” í•˜ë‚˜ì˜ ë‹¨ê³„ì´ë‹¤.(Dynamic computational graph) ë§¤ë²ˆ forward pass í•  ë•Œ ë§ˆë‹¤ ìƒˆë¡œìš´ ê·¸ë˜í”„ë¥¼ êµ¬ì„±í•œë‹¤. ë˜í•œ, ê·¸ë˜í”„ êµ¬ì„±ê³¼ ê·¸ë˜í”„ ì‹¤í–‰í•˜ëŠ” ê³¼ì •ì´ ì–½í˜€ ìˆê¸°ì— ëª¨ë¸ì„ ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” í•­ìƒ ì›ë³¸ ì½”ë“œê°€ í•„ìš”í•˜ë‹¤. ì½”ë“œê°€ í›¨ì”¬ ê¹”ë”í•˜ê³  ì‘ì„±í•˜ê¸° ë” ì‰½ë‹¤. tensorflowì™€ëŠ” ë‹¤ë¥´ê²Œ python ëª…ë ¹ì–´ë“¤ì„ í™œìš©í•  ìˆ˜ ìˆë‹¤. ë‹¤ì–‘í•œ ë°ì´í„°ì—ë„ ìœ ë™ì ì¸ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤.(RNNì‚¬ìš© - NLPì—ì„œ ë¬¸ì¥ì„ íŒŒì‹±í•˜ëŠ” ë¬¸ì œ ì¤‘ íŠ¸ë¦¬ë¥¼ íŒŒì‹±í•˜ê¸° ìœ„í•´ recursiveí•œ ë„¤íŠ¸ì›Œí¬ê°€ í•„ìš”í•  ìˆ˜ ìˆë‹¤.) Recurrent network, Recursive network, Modular Networks(ì´ë¯¸ì§€ì™€ ì§ˆë¬¸ì„ ë˜ì§€ë©´ ì ì ˆí•œ ë‹µì„ í•˜ëŠ” êµ¬ì¡°)ë¥¼ êµ¬ì„±í•  ë•Œ ì¡°ê¸ˆ ë” í¸í•  ìˆ˜ ìˆë‹¤.","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"[CS231n]Lecture07-Training Neural Networks2","slug":"cs231n_07","date":"2019-07-22T03:24:40.000Z","updated":"2019-07-23T04:21:56.948Z","comments":true,"path":"2019/07/22/cs231n_07/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/22/cs231n_07/","excerpt":"","text":"ì§€ë‚œ 6ê°•ì—ì„œëŠ” activation functionì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¨ì–´ ë³´ì•˜ëŠ”ë°, 10ë…„ì „ ê¹Œì§€ë§Œ í•´ë„ sigmoidê°€ ì•„ì£¼ ìœ ëª…í–ˆë‹¤. í—ˆë‚˜, Vanishing gradientê°€ ìƒê¸°ëŠ” ë¬¸ì œë¡œ ì¸í•´ ìµœê·¼ì—ëŠ” Sigmoidì™€ tanh ë³´ë‹¤ëŠ” ReLUë¥¼ ì“´ë‹¤ë¼ê³  í–ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê²½ìš° normalizeë‚˜ zero-centeredë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ ì£¼ì§€ ì•Šìœ¼ë©´, Lossê°€ íŒŒë¼ë¯¸í„°ì— ë„ˆë¬´ ë¯¼ê°í•˜ê¸° ë•Œë¬¸ì— í•™ìŠµì‹œí‚¤ê¸°ì— ì–´ë µë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ëª‡ ê°œì”© ì„ íƒí•˜ëŠ”ì§€ì— ë”°ë¥¸ ê³ ë¯¼ì€ ë³´í†µ ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥´ë©°, í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ë§ì„ ìˆ˜ë¡ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ê²½ìš°ì˜ ìˆ˜ê°€ ëŠ˜ì–´ë‚œë‹¤. ë§ì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì¤‘ learning rateê°€ ê°€ì¥ ì¤‘ìš”í•  ê²ƒì´ë¼ê³  ë³¸ë‹¤. regularization, learning rate decay, model size ê°™ì€ ê²ƒë“¤ì€ Learning rateë³´ë‹¨ ëœ ì¤‘ìš”í•˜ë‹¤. ê·¸ë ‡ê¸°ì— Block Coordinate Descent(BCD) ê°™ì€ ë°©ë²•ì„ ì“¸ ìˆ˜ë„ ìˆë‹¤. ìš°ì„  learning rateë¥¼ ì •í•´ë†“ì€ ë‹¤ìŒì— ë‹¤ì–‘í•œ ëª¨ë¸ ì‚¬ì´ì¦ˆë¥¼ ì‹œë„í•´ ë³´ëŠ” ê²ƒì´ë‹¤. ì´ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´ë‚˜ëŠ” Search spaceë¥¼ ì¡°ê¸ˆì€ ì¤„ì¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì •í™•íˆ ì–´ë–¤ ìˆœì„œë¡œ ì–´ë–»ê²Œ ì°¾ì•„ì•¼ í• ì§€ ì •í•´ì•¼ í•˜ëŠ” ê²ƒì´ ê°€ì¥ í° ë¬¸ì œì´ë‹¤. ìš°ë¦¬ê°€ ì–´ë–¤ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°’ì„ ë³€ê²½í•  ì‹œì— ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ìµœì  ê°’ì´ ë³€í•´ë²„ë¦¬ëŠ” ê²½ìš°ëŠ” ê°€ë” ë°œìƒí•œë‹¤. ì´ëŸ° ê²½ìš° ë” ì¢‹ì€ ìµœì í™” ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ì‰ learning rateì— ëœ ë¯¼ê°í•˜ë„ë¡ í•  ìˆ˜ ìˆë‹¤. Fancier OptimizationSGD(Stochastic Gradient Descent)ì˜ ë¬¸ì œì  1)ê°€ì¤‘ì¹˜ê°€ ì›€ì§ì¼ìˆ˜ ìˆëŠ” ë°©í–¥ ì¤‘ ë¶ˆê· í˜•í•œ ë°©í–¥ì´ ì¡´ì¬í•œë‹¤ë©´ SGDëŠ” ì˜ ë™ì‘í•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì„ ë³´ê³  ìˆ˜í‰ì¶•ê³¼ ìˆ˜ì§ì¶• ì´ ë‘ê°€ì§€ì— ëŒ€í•´ ê°€ì¤‘ì¹˜ì˜ ë³€í™”ì— ë”°ë¥¸ ì†ì‹¤í•¨ìˆ˜ì˜ ë³€í™”ëŸ‰ì´ë¼ê³  ìƒê°í•´ ë³´ì.(ìš°ë¦¬ê°€ ì‰½ê²Œ ì–´ë¦´ì  ë³´ì•˜ë˜ ì§€ë„ì—ì„œ ë“±ê³ ì„ ì„ ë– ì˜¬ë¦°ë‹¤ë©´ ë” ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.) ê·¸ë ‡ë‹¤ë©´, ìˆ˜í‰ì¶•ì˜ ê°€ì¤‘ì¹˜ ë³´ë‹¤ ìˆ˜ì§ì¶•ì˜ ê°€ì¤‘ì¹˜ê°€ í›¨ì”¬ ë” ì†ì‹¤í•¨ìˆ˜ì˜ ë³€í™”í•˜ëŠ” ì†ë„ê°€ ë¹ ë¥¼ ê²ƒì´ë‹¤.(ì™œ? ê¸°ìš¸ê¸°ê°€ ë” ê°€íŒŒë¥´ë‹ˆê¹Œ!!!) ì¦‰, LossëŠ” ìˆ˜ì§ ë°©í–¥ì˜ ê°€ì¤‘ì¹˜ ë³€í™”ì— í›¨ì”¬ ë” ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•œë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì—ì„œ red pointê°€ í˜„ì¬ Lossë¼ê³  ê°€ì •í–ˆì„ë•Œ, í˜„ì¬ ì§€ì ì˜ Hessian matrixì˜ ìµœëŒ€/ìµœì†Œ singular valuesê°’ì˜ ë¹„ìœ¨ì´ ë§¤ìš° ì•ˆì¢‹ë‹¤ëŠ” ëœ»ì´ë¯€ë¡œ LossëŠ” bad condition numberë¥¼ ì§€ë‹ˆê³  ìˆë‹¤ê³  ë§í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.[ê·¸ë¦¼0] ì•„ë˜ì™€ ê°™ì€ ì†ì‹¤í•¨ìˆ˜ì—ì„œ SGDë¥¼ ì‹œí–‰í•œë‹¤ë©´, gradientì˜ ë°©í–¥ì´ ê³ ë¥´ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ì§€ê·¸ì¬ê·¸ ëª¨ì–‘ìœ¼ë¡œ gradientì˜ ë°©í–¥ì´ ê·¸ë ¤ì§€ê²Œ ëœë‹¤. Lossì— ì˜í–¥ì„ ëœ ì£¼ëŠ” ìˆ˜í‰ë°©í–¥ ì°¨ì›ì˜ ê°€ì¤‘ì¹˜ëŠ” ì—…ë°ì´íŠ¸ê°€ ì•„ì£¼ ëŠë¦¬ê²Œ ì§„í–‰ëœë‹¤. ì¦‰, ì´ë ‡ê²Œ ê°€ì¤‘ì¹˜ê°€ ì›€ì§ì¼ìˆ˜ ìˆëŠ” ë°©í–¥ ì¤‘ ë¶ˆê· í˜•í•œ ë°©í–¥ì´ ì¡´ì¬í•œë‹¤ë©´ SGDëŠ” ì˜ ë™ì‘í•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤. [ê·¸ë¦¼1] 2)local minimaì™€ saddle points Xì¶•ì€ ì–´ë–¤ í•˜ë‚˜ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‚˜íƒ€ë‚´ê³ , Yì¶•ì€ Lossë¥¼ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤. ìœ„ì˜ ê·¸ë¦¼ì€ local minimaì— ê´€í•œ ê·¸ë¦¼ì´ê³ , ì•„ë˜ì˜ ê·¸ë¦¼ì€ saddle pointì™€ ê´€ë ¨ëœ ê·¸ë¦¼ì´ë‹¤. saddle pointê°€ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ì–´ë–¤ ë°©í–¥ì€ Lossê°€ ì¦ê°€í•˜ê³  ëª‡ëª‡ ë°©í–¥ì€ Lossê°€ ê°ì†Œí•˜ê³  ìˆëŠ” ê³³ì„ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ì— ë°˜í•´ local minimaëŠ” í•œ ë°©í–¥ìœ¼ë¡œ Lossê°€ ìƒìŠ¹í•˜ëŠ” ë°©í–¥ì´ë‹¤. ì´ëŸ° saddle point ë¬¸ì œëŠ” ê³ ì°¨ì› ê³µê°„ì—ì„œëŠ” ë”ìš± ë” ë¹ˆë²ˆí•˜ê²Œ ë°œìƒí•˜ë©°, ì§€ë‚œ ëª‡ ë…„ê°„ ì•Œë ¤ì§„ ì‚¬ì‹¤ì€ very large neural networkê°€ local minima ë³´ë‹¤ëŠ” saddle pointì— ì·¨ì•½í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. saddle point ë¿ë§Œ ì•„ë‹ˆë¼ saddle point ê·¼ì²˜ì—ì„œë„ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ”ë° ê·¼ì²˜ì—ì„œ gradientê°€ 0ì€ ì•„ë‹ˆì§€ë§Œ ê¸°ìš¸ê¸°ê°€ ì•„ì£¼ ì‘ì€ ê³³ë“¤ì´ ë³´ì¼ ê²ƒì´ë‹¤. ê·¸ê²ƒì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ” gradientë¥¼ ê³„ì‚°í•´ì„œ ì—…ë°ì´íŠ¸ë¥¼ í•´ë„ ê¸°ìš¸ê¸°ê°€ ì•„ì£¼ ì‘ê¸° ë•Œë¬¸ì— í˜„ì¬ ê°€ì¤‘ì¹˜ì˜ ìœ„ì¹˜ê°€ saddle point ê·¼ì²˜ë¼ë©´ ì—…ë°ì´íŠ¸ëŠ” ì•„ì£¼ ëŠë¦¬ê²Œ ì§„í–‰ëœë‹¤ëŠ” ì  ë˜í•œ ë¬¸ì œì ì´ë‹¤. 3) mini-batchë¡œ ì¸í•œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ëŠ” ì¶”ì •ê°’ì´ë‹¤. ì†ì‹¤í•¨ìˆ˜ë¥¼ ê³„ì‚°í•  ë•ŒëŠ” ì—„ì²­ ë§ì€ Training Set ê°ê°ì˜ lossë¥¼ ì „ë¶€ ê³„ì‚°í•´ì•¼ í•œë‹¤. ë§¤ë²ˆ ì´ë ‡ê²Œ ì „ë¶€ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ì–´ë µê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œëŠ” mini-batchì˜ ë°ì´í„°ë“¤ë§Œ ê°€ì§€ê³  ì‹¤ì œ Lossë¥¼ ì¶”ì •í•˜ê¸°ë§Œ í•œë‹¤. ì´ëŠ” gradientì˜ ë¶€ì •í™•í•œ ì¶”ì •ê°’ë§Œì„ êµ¬í•  ë¿ì´ë¼ëŠ” ê²ƒì´ë‹¤. ìœ„ì—ì„œ ë§í•œ ìœ„í—˜ìš”ì†Œë“¤ì„ ë‹¤ë£¨ê¸° ìœ„í•´ì„œ ë” ì¢‹ì€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ í•„ìš”í•˜ë‹¤. ì•„ë˜ì—ì„œ ì†Œê°œí•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ë“¤ì˜ velocityì€ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ê°€ ì•„ë‹ˆë©° ì´ˆê¸°ê°’ì„ í•­ìƒ 0ìœ¼ë¡œ ë‘”ë‹¤!1) SGD + momentum ì•„ì´ë””ì–´ëŠ” gradientì˜ ë°©í–¥ìœ¼ë¡œë§Œ ì›€ì§ì´ëŠ” SGDì— velocityë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰, gradientë¥¼ ê³„ì‚°í•  ë•Œ velocityë¥¼ ì´ìš©í•œë‹¤. í˜„ì¬ mini-batchì˜ gradient ë°©í–¥ë§Œ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ velocityë¥¼ ê°™ì´ ê³ ë ¤í•˜ëŠ” ê²ƒì´ë‹¤.ì•„ë˜ì˜ ìˆ˜ì‹ì„ ë³´ë©´ velocityì˜ ì˜í–¥ë ¥ì„ rhoì˜ ë¹„ìœ¨ë¡œ ë§ì¶°ì£¼ëŠ”ë° ë³´í†µ 0.9 ë˜ëŠ” 0.99 ê°™ì€ ë†’ì€ ê°’ìœ¼ë¡œ ë§ì¶°ì¤€ë‹¤. gradient vector ê·¸ëŒ€ë¡œì˜ ë°©í–¥ì´ ì•„ë‹Œ velocity vectorì˜ ë°©í–¥ìœ¼ë¡œ ë‚˜ì•„ê°€ê²Œ ëœë‹¤. [ê·¸ë¦¼2]local minimaì™€ saddle points ë¬¸ì œëŠ” local minimaì— ë„ë‹¬í•´ë„ ì—¬ì „íˆ velocityë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— gradientê°€ 0ì´ë¼ë„ ì›€ì§ì¼ ìˆ˜ ìˆìœ¼ë©° ê³„ì†í•´ì„œ ë‚´ë ¤ê°ˆ ìˆ˜ ìˆë‹¤. saddle point ì£¼ë³€ì˜ gradientê°€ ì‘ë”ë¼ë„, êµ´ëŸ¬ë‚´ë ¤ì˜¤ëŠ” ì†ë„ê°€ ìˆê¸° ë•Œë¬¸ì— velocityë¥¼ ê°€ì§€ê²Œ ë˜ì–´ ì´ ë˜í•œ ì˜ ê·¹ë³µí•´ ë‚´ê³  ê³„ì† ë°‘ìœ¼ë¡œ ë‚´ë ¤ì˜¬ ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤. ê¸°ì¡´ì˜ SGDë§Œì„ ì‚¬ìš©í–ˆì„ ê²½ìš°ì²˜ëŸ¼ ì§€ê·¸ì¬ê·¸ë¡œ ì›€ì§ì´ëŠ” ìƒí™©ì„ momentumìœ¼ë¡œ ì¸í•´ ê·¸ëŸ¬í•œ ë³€ë™ì„ ì„œë¡œ ìƒì‡„ì‹œì¼œ ë²„ë¦°ë‹¤. noiseê°€ í‰ê· í™” ë˜ë²„ë¦¬ëŠ” ì˜ë¯¸ë¥¼ ê°–ëŠ”ë‹¤. ì´ë¥¼ í†µí•´ì„œ Lossì— ë¯¼ê°í•œ ìˆ˜ì§ ë°©í–¥ì˜ ë³€ë™ì€ ì¤„ì—¬ì£¼ê³  ìˆ˜í‰ë°©í–¥ì˜ ì›€ì§ì„ì€ ì ì°¨ ê°€ì†í™” ë  ê²ƒì´ë‹¤. momentumì„ ì¶”ê°€í•˜ê²Œ ë˜ë©´ high condition number problemì„ í•´ê²°í•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” ê²ƒì´ë‹¤! ì§ê´€ì ìœ¼ë¡œ ë³´ë©´ velovityëŠ” ì´ì „ gradientsì˜ weighted sumì´ë‹¤. ë” ìµœê·¼ì˜ gradientsì— ê°€ì¤‘ì¹˜ê°€ ë” í¬ê²Œ ë¶€ì—¬ë˜ê³  ê³„ì‚°ë˜ëŠ” ê³¼ì •ì´ ì¼ì¢€ì˜ smooth moving averageë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì‹œê°„ì´ ì§€ë‚ ìˆ˜ë¡ ì´ì „ì˜ gradientë“¤ì€ exponentiallyí•˜ê²Œ ê°ì†Œí•œë‹¤.[ê·¸ë¦¼3] 2) Nesterov momentumSGD momentumì€ í˜„ì¬ì§€ì ì—ì„œì˜ gradientë¥¼ ê³„ì‚°í•œ ë’¤ì— velocityì™€ ê³±í•´ì£¼ì—ˆì§€ë§Œ Nesterov momentumì€ ê³„ì‚° ìˆœì„œë§Œ ë³€í˜•ì„ ì‹œì¼œ ì£¼ì—ˆë‹¤ê³  ë³´ë©´ ëœë‹¤. ì•„ë˜ì˜ ì™¼ìª½ ê·¸ë¦¼ì„ ë³´ë©´, ë¹¨ê°„ ì ì—ì„œ ì‹œì‘í•´ì„œ ìš°ì„ ì€ Velocity ë°©í–¥ìœ¼ë¡œ ì›€ì§ì¸ë‹¤. ê·¸ë¦¬ê³  ê·¸ ì§€ì ì—ì„œì˜ gradientë¥¼ ê³„ì‚°í•œ í›„ ë‹¤ì‹œ ì›ì ìœ¼ë¡œ ëŒì•„ê°€ì„œ ì´ ë‘˜ì„ í•©ì¹˜ëŠ” ê²ƒì´ë‹¤. velocityì˜ ë°©í–¥ì´ ì˜ëª»ë˜ì—ˆì„ ê²½ìš°ì— í˜„ì¬ gradientì˜ ë°©í–ì„ ì¢€ ë” í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•´ì¤€ë‹¤. NesterovëŠ” Convex Optimization ë¬¸ì œì—ì„œëŠ” ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ í•˜ì§€ë§Œ Neural networkì™€ ê°™ì€ Non-convex problemì—ì„œëŠ” ì„±ëŠ¥ì´ ë³´ì¥ë˜ëŠ” ì•ŠëŠ”ë‹¤.[ê·¸ë¦¼4]Nesterovì˜ ì²«ë²ˆì§¸ ìˆ˜ì‹ì€ ê¸°ì¡´ì˜ momentumê³¼ ë™ì¼í•˜ë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ ì¬ë°°ì—´í•œ ìˆ˜ì‹ì„ ë³´ë©´ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ velocityì™€ ê³„ì‚°í•œ gradientë¥¼ ì¼ì • ë¹„ìœ¨ë¡œ ì„ì–´ì£¼ëŠ” ì—­í• ì„ í•œë‹¤. ê·¸ë¦¬ê³  ë‘ ë²ˆì§¸ ìˆ˜ì‹ì—ì„œ ë§ˆì§€ë§‰ ë¶€ë¶„ì„ ë³´ë©´ í˜„ì¬ ì ê³¼ velocityë¥¼ ë”í•´ì£¼ë©°, í˜„ì¬ velocity - ì´ì „ velocityë¥¼ ê³„ì‚°í•´ì„œ ì¼ì • ë¹„ìœ¨(rho)ì„ ê³±í•˜ê³  ë”í•´ì¤ë‹ˆë‹¤. í˜„ì¬/ì´ì „ì˜ velocityê°„ì˜ ì—ëŸ¬ ë³´ì •(error-correcting term)ì´ ì¶”ê°€ë˜ì—ˆë‹¤.[ê·¸ë¦¼5]ì´ì „ì˜ velocityì˜ ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì— momentum ë°©ë²•ë“¤ì€ minimaë¥¼ ê·¸ëƒ¥ ì§€ë‚˜ì³ ë²„ë¦¬ëŠ” ê²½í–¥ì´ ìˆë‹¤. í•˜ì§€ë§Œ ìŠ¤ìŠ¤ë¡œ ê²½ë¡œë¥¼ ìˆ˜ì •í•˜ê³ ëŠ” ê²°êµ­ minimaì— ìˆ˜ë ´í•œë‹¤. 3) AdaGrad í•™ìŠµ ë„ì¤‘ì— ê³„ì‚°ë˜ëŠ” gradientì— ì œê³±ì„ í•´ì„œ ê³„ì† ë”í•´ì¤€ë‹¤. ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í• ë•Œ gradientë¡œ ë‚˜ëˆ ì£¼ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•œë‹¤. í•œ ì°¨ì›ì€ í•­ìƒ gradientê°€ ë†’ì€ ì°¨ì›ì´ê³  ë‹¤ë¥¸ í•˜ë‚˜ëŠ” í•­ìƒ ì‘ì€ gradientë¥¼ ê°€ì§€ëŠ” 2ì°¨ì› ì¢Œí‘œê°€ ìˆë‹¤ê³  ê°€ì •í•˜ì. small dimensionì—ì„œëŠ” gradientì˜ ì œê³± ê°’ í•©ì´ ì‘ì€ë° ì´ ì‘ì€ ê°’ìœ¼ë¡œ ë‚˜ì›Œì§€ë¯€ë¡œ ê°€ì†ë„ê°€ ë¶™ê²Œëœë‹¤. Large dimensionì—ì„œëŠ” gradientê°€ í° ê°’ ì´ë¯€ë¡œ í° ê°’ì´ ë‚˜ì›Œì§€ê²Œ ë˜ì–´ ì†ë„ê°€ ì ì  ì¤„ì–´ë“ ë‹¤. í•˜ì§€ë§Œ í•™ìŠµì´ ê³„ì† ì§„í–‰ë ìˆ˜ë¡ í•™ìŠµ íšŸìˆ˜ê°€ ëŠ˜ì–´ë‚œë‹¤ëŠ” ë¬¸ì œê°€ ìˆë‹¤. í•™ìŠµ íšŸìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ AdaGradì˜ ê°’ì€ ì ì  ì‘ì•„ì§„ë‹¤. ì´ëŸ¬í•œ ì ì€ Convexí•œ Lossì¸ ê²½ìš°ì— ì¢‹ì€ íŠ¹ì§•ì´ ë  ìˆ˜ ìˆë‹¤. minimumì— ê·¼ì ‘í•˜ë©´ ì„œì„œíˆ ì†ë„ë¥¼ ì¤„ì—¬ì„œ ìˆ˜ë ´í•  ìˆ˜ ìˆê²Œ í•´ ì¤„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ saddle point problemê³¼ ê°™ì€ non-convex ë¬¸ì œì—ì„œëŠ” AdaGradê°€ ë©ˆì¶° ë²„ë¦¬ëŠ” ìƒí™©ì´ ë°œìƒí•  ìˆ˜ ë„ ìˆì–´ ë¬¸ì œê°€ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ NNì„ í•™ìŠµì‹œí‚¬ ë•ŒëŠ” ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤.[ê·¸ë¦¼6] 4) RMSProp ìœ„ì™€ ê°™ì€ ë¬¸ì œë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. AdaGradì˜ gradient ì œê³± í•­ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤. ì ì  ì†ë„ê°€ ì¤„ì–´ë“œëŠ” ë¬¸ì œë¥¼ ì œê³±í•­ì„ ê·¸ì € ëˆ„ì ì‹œí‚¤ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê¸°ì¡´ì˜ ëˆ„ì  ê°’ì— decay_rateë¥¼ ê³±í•´ì£¼ëŠ” ë°©ì‹ì„ í†µí•´ í•´ê²°í•˜ì˜€ë‹¤. decay_rateëŠ” ë³´í†µ 0.9 ë˜ëŠ” 0.99ë¥¼ ìì£¼ ì‚¬ìš©í•œë‹¤. gradient ì œê³±ì„ ê³„ì† ë‚˜ëˆ ì¤€ë‹¤ëŠ” ì ì€ AdaGradì™€ ìœ ì‚¬í•˜ë‹¤. ì´ë¥¼ í†µí•´ stepì˜ ì†ë„ë¥¼ ê°€ì†/ê°ì† ì‹œí‚¬ ìˆ˜ ìˆë‹¤. [ê·¸ë¦¼7] 5) Adam momentum + RMSProp ìœ¼ë¡œ ìœ„ì—ì„œ ì¢…í•©í•œ momentumê³„ì—´ì˜ ì•Œê³ ë¦¬ì¦˜ê³¼ Adaê³„ì—´ì˜ ì•Œê³ ë¦¬ì¦˜ì˜ íŠ¹ì§•ì„ í•©í•œ ê²ƒì´ë‹¤. ë¹¨ê°„ìƒ‰ ë¶€ë¶„ì€ gradientì˜ ê°€ì¤‘í•©ì´ë‹¤. íŒŒë€ìƒ‰ ë¶€ë¶„ì€ AdaGradë‚˜ RMSPropì²˜ëŸ¼ gradientsì˜ ì œê³±ì„ ì´ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ˆê¸° Stepì´ ì—„ì²­ ì»¤ì ¸ ë²„ë¦´ ìˆ˜ ìˆê³  ì´ë¡œ ì¸í•´ ì˜ëª»ë  ìˆ˜ë„ ìˆë‹¤. ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³´ì •í•˜ëŠ” í•­ì„ ì¶”ê°€í•œë‹¤.(bias correction term) Adamì€ ë‹¤ì–‘í•œ ë¬¸ì œì—ë„ ì •ë§ ì˜ ë™ì‘í•œë‹¤. í•˜ì§€ë§Œ ì˜ˆë¥¼ë“¤ì–´ ì†ì‹¤í•¨ìˆ˜ê°€ íƒ€ì›í˜•ì´ê³  ì¶• ë°©í–¥ìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆì§€ ì•Šê³  ê¸°ìš¸ì–´ì ¸ ìˆë‹¤ê³  ìƒê°í•´ ë³´ì. íšŒì „ëœ íƒ€ì›(poor conditioning) ë¬¸ì œëŠ” Adamì„ ë¹„ë¡¯í•œ ë‹¤ë¥¸ ì—¬ëŸ¬ ì•Œê³ ë¦¬ì¦˜ë“¤ë„ ë‹¤ë¥¼ ìˆ˜ ì—†ëŠ” ë¬¸ì œì´ë‹¤.[ê·¸ë¦¼8] learning rates decay ì „ëµ ì²˜ìŒì—ëŠ” learning rateë¥¼ ë†’ê²Œ ì„¤ì •í•œ ë‹¤ìŒì— í•™ìŠµì´ ì§„í–‰ë ìˆ˜ë¡ learning ratesë¥¼ ì ì  ë‚®ì¶”ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ë©´, 100,000 iterì—ì„œ learning ratesë¥¼ ë‚®ì¶”ê³  í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ë‹¤.(step decay) ë˜ëŠ” exponential decay ì²˜ëŸ¼ í•™ìŠµê³¼ì • ë™ì•ˆì— ê¾¸ì¤€íˆ learning rateë¥¼ ë‚®ì¶œ ìˆ˜ë„ ìˆë‹¤. learning rateê°€ ë„ˆë¬´ ë†’ì•„ì„œ ë” ê¹Šê²Œ ë“¤ì–´ê°€ì§€ ëª»í•˜ëŠ” ìƒí™©ì— learning rateë¥¼ ë‚®ì¶”ê²Œ ë˜ë©´ ì†ë„ê°€ ì¤„ì–´ë“¤ë©° ì§€ì†í•´ì„œ Lossë¥¼ ë‚´ë ¤ê°ˆ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. learning rate decayëŠ” ë¶€ì°¨ì ìœ¼ë¡œ ìƒê°í•´ë³´ëŠ” ê²ƒì´ì§€ í•™ìŠµ ì´ˆê¸°ë¶€í„° ê³ ë ¤í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ë˜í•œ Adam ë³´ë‹¤ëŠ” SGD Momentumì„ ì‚¬ìš©í•  ë•Œ ìì£¼ ì‚¬ìš©í•œë‹¤.[ê·¸ë¦¼9] ìœ„ì—ì„œ ì–¸ê¸‰í•œ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ 1ì°¨ ë¯¸ë¶„ê°’ì„ ì‚¬ìš©í•˜ì—¬ 1ì°¨ ê·¼ì‚¬í•¨ìˆ˜ë¥¼ ì‹¤ì œ ì†ì‹¤í•¨ìˆ˜ë¼ê³  ê°€ì •í•˜ê³  ê°€ì¤‘ì¹˜ì˜ ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•˜ì˜€ë‹¤. ì´ëŸ° ë°©ì‹ì€ ê·¼ì‚¬ ì‹œí‚¨ ê°’ì´ë¯€ë¡œ ì •í™•ì„±ì´ ë–¨ì–´ì ¸ ìŠ¤í…ì˜ ì‚¬ì´ì¦ˆë¥¼ í‚¤ì›Œ ë©€ë¦¬ ê°ˆìˆ˜ê°€ ì—†ë‹¤. 2ì°¨ ë¯¸ë¶„ê°’ì„ í™œìš©í•˜ì—¬ ê·¼ì‚¬ ì‹œí‚¤ëŠ” ë°©ë²•ì„ í†µí•´ ê·¸ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤.[ê·¸ë¦¼10][ê·¸ë¦¼11] Second-Order Optimization ìœ„ì˜ 2ì°¨ì›ì„ ë‹¤ì°¨ì›ìœ¼ë¡œ í™•ì¥ì‹œì¼œë³´ë©´ ì´ë¥¼ Newton stepì´ë¼ê³  í•œë‹¤. Hessian matrixì˜ inverse matrixë¥¼ ì´ìš©í•˜ê²Œ ë˜ë©´ ì‹¤ì œ ì†ì‹¤í•¨ìˆ˜ì˜ 2ì°¨ ê·¼ì‚¬ë¥¼ ì´ìš©í•´ ê³§ë°”ë¡œ minimaë¡œ ì´ë™í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ë“¤ê³¼ ë‹¬ë¦¬ learning rateë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ì‹¤ì œë¡œëŠ” 2ì°¨ ê·¼ì‚¬ë„ ì™„ë²½í•˜ì§€ ì•Šê¸°ì— learning rateê°€ í•„ìš”í•˜ë‹¤. ì–´ë””ê¹Œì§€ë‚˜, minimaë¡œ ì´ë™í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ minimaì˜ ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ê¸° ë•Œë¬¸ì´ë‹¤. í—ˆë‚˜, inverse matrixë¥¼ êµ¬í•˜ê¸° í˜ë“¤ê³  ë©”ëª¨ë¦¬ì— ëŒ€ëŸ‰ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì €ì¥í•  ë°©ë²•ì´ ì—†ê¸°ì— ì´ ì•Œê³ ë¦¬ì¦˜ì€ NNì—ì„œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤. 1) Quasi-Newton methods(BGFS most popular) Full Hessianì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê¸° ë³´ë‹¤ ê·¼ì‚¬ì‹œí‚¨ë‹¤. Low-rank approximationsí•˜ëŠ” ë°©ë²•ì´ë‹¤. 2) L-BFGS Hessianì„ ê·¼ì‚¬ì‹œì¼œ ì‚¬ìš©í•˜ëŠ” second-order optimizerì´ë‹¤. ì‚¬ì‹¤ìƒ DNNì—ì„œëŠ” ì˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ì™œëƒí•˜ë©´ L-BFGSì—ì„œ 2ì°¨ ê·¼ì‚¬ê°€ stochastic caseì—ì„œ ì˜ ë™ì‘í•˜ì§€ëŠ” ì•Šìœ¼ë©°, Non-convex ë¬¸ì œì—ë„ ì í•©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. ë‹¨ì§€ full batch updateê°€ ê°€ëŠ¥í•˜ê³  stochasticityê°€ ì ì€ ê²½ìš°ë¼ë©´, L-BFGSê°€ ì¢‹ì€ ì„ íƒì´ ë  ìˆ˜ ìˆë‹¤. NNì„ í•™ìŠµì‹œí‚¤ëŠ”ë° ë§ì´ ì‚¬ìš©ë˜ì§€ëŠ” ì•Šì§€ë§Œ Style transfer ê°™ì€ stochasticityì™€ íŒŒë¼ë¯¸í„°ê°€ ì ì€ ê²½ìš°ì—ì„œ Optimizationì„ í•´ì•¼í•  ê²½ìš°ì— ì¢…ì¢… ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ìœ„ì—ì„œ ì‚´í´ë³¸ ë°©ë²•ë“¤ì€ í•™ìŠµê³¼ì •ì˜ errorë¥¼ ì¤„ì´ê¸° ìœ„í•œ ë°©ë²•ë“¤ì´ì—ˆë‹¤. í—ˆë‚˜ ìš°ë¦¬ê°€ ì§„ì •ìœ¼ë¡œ ê´€ì‹¬ì„ ê°–ê³  ë³´ì•„ì•¼ í•  ê²ƒì€ test setì˜ errorì´ë‹¤. ê·¸ëŸ° test setì˜ errorë¥¼ ì¤„ì´ê¸° ìœ„í•œ ë°©ì•ˆë“¤ì„ ë‹¤ìŒì—ì„œ ì œì‹œí•œë‹¤. Model Ensemble modelì„ train ì‹œí‚¨ í›„ ìš°ë¦¬ê°€ ê°€ì¥ ê¸°ëŒ€í•˜ëŠ” ë°”ëŠ” ìƒˆë¡­ê²Œ ë“¤ì–´ì˜¨ test setì— ëŒ€í•œ ì„±ëŠ¥. ì¦‰, test setì— ëŒ€í•œ errorê°€ ì‘ê¸°ë¥¼ ê¸°ëŒ€í•œë‹¤. ê·¸ë ‡ê²Œ í•˜ëŠ” ê°€ì¥ ì‰½ê³  ë¹ ë¥¸ ë°©ë²•ì´ ë°”ë¡œ Model Ensembleì´ë‹¤. machine learning ë¶„ì•¼ì—ì„œ ì¢…ì¢… ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ ì˜ˆë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ìë©´ ëª¨ë¸ì„ í•˜ë‚˜ë§Œ í•™ìŠµì‹œí‚¤ì§€ ë§ê³  10ê°œì˜ ëª¨ë¸ì„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ë‹¤. ê²°ê³¼ëŠ” 10ê°œ ëª¨ë¸ ê²°ê³¼ì˜ í‰ê· ì„ ì´ìš©í•œë‹¤. ëª¨ë¸ì˜ ìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ Overfittingì´ ì¤„ì–´ë“¤ê³  ì„±ëŠ¥ì´ ì¡°ê¸ˆì”© í–¥ìƒëœë‹¤. ë³´í†µ 2%ì •ë„ ì¦ê°€í•œë‹¤. ImageNetì´ë‚˜ Kaggle competition ê°™ì´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœëŒ€í™” ì‹œí‚¤ëŠ” ê²ƒì´ ì£¼ëœ ëª©í‘œì¼ ê²½ìš° ë§ì´ ì‚¬ìš©í•œë‹¤. í—ˆë‚˜ ê°œì¸ì ìœ¼ë¡œë‚˜ ì£¼ë³€ì˜ ì¡°ì–¸ë“¤ì„ ì¢…í•©í•´ë³´ìë©´ ìš°ì„  ê¸°ë³¸ì ì¸ baseëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ëŠ”ë° ì£¼ë ¥í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì„ ë“¯í•˜ë‹¤. ì‹¤ì œë¡œ ì‹¤ë¬´ì—ì„œëŠ” Ensembleì´ë‚˜ Stackingê°™ì€ ê¸°ë²•ì„ ìì£¼ ì‚¬ìš©í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤ê³  í•˜ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ í•œë²ˆ ì¯¤ì€ ë§Œë“¤ì–´ ë³´ëŠ” ê²ƒë„ ì¢‹ì€ ê²ƒ ê°™ë‹¤. ë˜í•œ í•™ìŠµ ë„ì¤‘ ì¤‘ê°„ ëª¨ë¸ë“¤ì„ ì €ì¥í•˜ê³  ì•™ìƒë¸”ë¡œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆë‹¤. ê·¸ë¦¬ê³  Testì‹œì—ëŠ” ì—¬ëŸ¬ ì¤‘ê°„ ëª¨ë¸ë“¤ì„ í†µí•´ ë‚˜ì˜¨ ì˜ˆì¸¡ê°’ë“¤ì„ í‰ê· ì„ ë‚´ì„œ ì‚¬ìš©í•œë‹¤. ë§Œì•½ ëª¨ë¸ê°„ì˜ Loss ì°¨ì´ê°€ í¬ë©´ í•œìª½ì´ Overfitting ì¼ìˆ˜ ìˆê³ , ì°¨ì´ê°€ ì‘ì•„ë„ ì•ˆì¢‹ì€ ê²ƒì€ ì•„ë‹ê¹Œë¼ëŠ” ìƒê°ì— ì˜í•´ ì¢‹ì€ ì•™ìƒë¸” ê²°ê³¼ë¥¼ ìœ„í•´ì„œë¼ë©´ ëª¨ë¸ ê°„ì˜ ìµœì €ê·¸ì´ ê°­ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ì§€ ì•ŠëŠ”ëƒëŠ” ìƒê°ì´ ë“¤ ìˆ˜ë„ ìˆê² ì§€ë§Œ, ì–¸ì œë‚˜ ë§í•˜ë“¯ ìš°ë¦¬ì—ê²Œ ì¤‘ìš”í•œ ê²ƒì€ validation setì˜ ì„±ëŠ¥ì„ ìµœëŒ€í™”ì‹œí‚¤ëŠ” ê²ƒì´ë‹¤. ì•™ìƒë¸”ì‹œì— ë‹¤ì–‘í•œ ëª¨ë¸ ì‚¬ì´ì¦ˆ, learning rate, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ regularization ê¸°ë²• ë“±ì„ ì•™ìƒë¸” í•  ìˆ˜ ìˆë‹¤. ì´ëŸ° ëª¨ë¸ì„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì™¸ì—ë„ í•™ìŠµí•˜ëŠ” ë™ì•ˆì— íŒŒë¼ë¯¸í„°ì˜ exponentially decaying averageë¥¼ ê³„ì† ê³„ì‚°í•œë‹¤. ì´ ë°©ë²•ì€ í•™ìŠµì¤‘ì¸ ë„¤íŠ¸ì›Œí¬ì˜ smooth ensemble íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì¦‰, checkpointsì—ì„œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³  smoothly decaying averageë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì´ë‹¤.(Polyak averaging) Regularization ëª¨ë¸ì— ì–´ë–¤ ì¡°ê±´ë“¤ì„ ì¶”ê°€í•  í…ë° ê·¸ termë“¤ì€ ëª¨ë¸ì´ Training dataì— fití•˜ëŠ” ê²ƒì„ ë§‰ì•„ì¤„ ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  í•œë²ˆë„ ë³´ì§€ ëª»í•œ ë°ì´í„°ì—ì„œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤. L2 regularizationì€ NNì—ëŠ” ì˜ ì–´ìš¸ë¦¬ì§€ ì•ŠëŠ”ë‹¤. ì™œ??? NNì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” Regularizationì€ ë°”ë¡œ dropoutì´ë‹¤!!! Dropout Forward pass ê³¼ì •ì—ì„œ í•œ layerì”© ì¶œë ¥(activation = previous activation * weight)ì„ ì „ë¶€ êµ¬í•œ í›„ì— ì„ì˜ë¡œ ì¼ë¶€ ë‰´ëŸ°ì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ëŠ”ë° ë§¤ë²ˆ Forward passë§ˆë‹¤ 0ì´ ë˜ëŠ” ë‰´ëŸ°ì„ ë°”ê¿”ì£¼ì–´ íŠ¹ì§•ë“¤ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ë°©ì§€í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì¦‰, ë„¤íŠ¸ì›Œí¬ê°€ ì–´ë–¤ ì¼ë¶€ featureì—ë§Œ ì˜ì¡´í•˜ì§€ ëª»í•˜ê²Œ í•´ì¤€ë‹¤. ë‹¤ì–‘í•œ featureë¥¼ ê³¨ê³ ë£¨ ì´ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ Overfittingì„ ë°©ì§€í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤..ë³´í†µì€ 0.5ë¡œ ì¤€ë‹¤. [ê·¸ë¦¼12] ë‹¨ì¼ ëª¨ë¸ë¡œ ì•™ìƒë¸” íš¨ê³¼ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì„œë¡œ ë‹¤ë¥¸ íŒŒë¼ë¯¸í„°ë¥¼ ê³µìœ í•˜ëŠ” ì„œë¸Œë„¤íŠ¸ì›Œí¬ ì•™ìƒë¸”ì„ ë™ì‹œì— í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ë¼ê³ ë„ ìƒê°í•  ìˆ˜ë„ ìˆë‹¤. ê·¸ëŸ¬ë‚˜, ë‰´ëŸ°ì˜ ìˆ˜ì— ë”°ë¼ì„œ ì•™ìƒë¸” ê°€ëŠ¥í•œ ì„œë¸Œë„¤íŠ¸ì›Œí¬ì˜ ìˆ˜ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•œ ëª¨ë“  ì„œë¸Œë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ì‚¬ì‹¤ìƒ ë¶ˆê°€ëŠ¥í•˜ë‹¤. Dropoutì„ ì‚¬ìš©í•˜ë©´ Test timeì—ëŠ” ì–´ë–¤ ì¼ì´ ë°œìƒë˜ë‚˜? Dropoutì„ ì‚¬ìš©í•˜ë©´ ê¸°ë³¸ì ìœ¼ë¡œ NNì˜ ë™ì‘ìì²´ê°€ ë³€í•˜ê²Œ ëœë‹¤. ê¸°ì¡´ì˜ NNì€ wì™€ xì— ëŒ€í•œ í•¨ìˆ˜ì˜€ë‹¤. ê·¸ëŸ¬ë‚˜ Dropoutì„ ì‚¬ìš©í•œë©´ Networkì— zë¼ëŠ” ì…ë ¥ì´ ì¶”ê°€ëœë‹¤. zëŠ” random dropout maskì´ë‹¤. testì‹œì—ëŠ”(ì˜ˆì¸¡ì‹œ) ì„ì˜ì˜ ê°’ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì€ ì¢‹ì§€ ì•Šë‹¤. ì™œëƒí•˜ë©´ ì˜ˆì¸¡í• ë•Œë§ˆë‹¤ ê²°ê³¼ê°€ ë°”ë€”ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŸ° randomnessë¥¼ average outì‹œí‚¤ëŠ”ë° ì ë¶„ì„ í†µí•´ marginalize outì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ ìƒê°í•´ë³¼ ìˆ˜ ìˆë‹¤. í—ˆë‚˜ ì‹¤ì œë¡œëŠ” ê¹Œë‹¤ë¡œìš´ ë¬¸ì œì´ë¯€ë¡œ zë¥¼ ì—¬ëŸ¬ë²ˆ ìƒ˜í”Œë§í•´ì„œ ì˜ˆì¸¡ì‹œì— ì´ë¥¼ average outì‹œí‚¤ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì´ ë°©ë²•ë„ test timeì—ì„œì˜ randomnessì„ ë§Œë“¤ì–´ ë‚´ê¸° ë•Œë¬¸ì— ì¢‹ì§€ ì•Šì€ ë°©ë²•ì´ë‹¤. í—ˆë‚˜ ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ì´ test timeì—ì„œ stochasticityë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  í•  ìˆ˜ ìˆëŠ” ê°’ ì‹¼ ë°©ë²• ì¤‘ í•˜ë‚˜ëŠ” dropout probabilityë¥¼ ë„¤íŠ¸ì›Œí¬ì˜ ì¶œë ¥ì— ê³±í•˜ì—¬ test timeê³¼ train timeì˜ ê¸°ëŒ€ê°’ì„ ê°™ê²Œ í•´ì£¼ëŠ” ê²ƒì´ë‹¤. [ê·¸ë¦¼13] ì‹¤ì œë¡œ ì½”ë“œì—ì„œëŠ” ì•„ë˜ì™€ ê°™ì´ ì˜ˆì¸¡ì‹œì— dropout probabilityë¥¼ ê³±í•´ì£¼ê±°ë‚˜ tipìœ¼ë¡œ trainì—ì„œëŠ” ì—°ì‚°ì´ GPUì— ì˜í•´ ê³„ì‚°ë˜ì–´ ì¶”ê°€ë˜ëŠ” ê²ƒì— ë³„ë¡œ ì‹ ê²½ì“°ì§€ ì•Šì§€ë§Œ Test timeì—ì„œëŠ” íš¨ìœ¨ì ìœ¼ë¡œ ë™ì‘í•˜ê¸¸ ë°”ë¼ë¯€ë¡œ trainì‹œì— ì˜¤íˆë ¤ ì—­ìœ¼ë¡œ dropout probabilityë¥¼ ë‚˜ëˆ„ì–´ì£¼ëŠ” ì‹ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. [ê·¸ë¦¼14] dropoutì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ Train timeì—ì„œ gradientì—ëŠ” ì–´ë–¤ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ê¶ê¸ˆí•  ê²ƒì´ë‹¤. ê²°ë¡ ì€ ìš°ë¦¬ê°€ ìƒê°í•˜ë˜ Dropoutì´ 0ìœ¼ë¡œ ë§Œë“¤ì§€ ì•Šì€ ë…¸ë“œì—ì„œë§Œ Backpropagationì´ ë°œìƒí•˜ê²Œ ëœë‹¤. Dropoutì„ ì‚¬ìš©í•˜ê²Œ ë˜ë©´ ê° ìŠ¤í…ë§ˆë‹¤ ì—…ë°ì´íŠ¸ë˜ëŠ” íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸ì— ì „ì²´ í•™ìŠµì‹œê°„ì€ ëŠ˜ì–´ë‚˜ì§€ë§Œ ëª¨ë¸ì´ ìˆ˜ë ´í•œ í›„ì—ëŠ” ë” ì¢‹ì€ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ Dropoutì€ ì¼ë°˜ì ì¸ regularizationì „ëµì„ êµ¬ì²´í™”ì‹œí‚¨ í•˜ë‚˜ì˜ ì˜ˆì‹œì— ë¶ˆê³¼í•˜ë‹¤. ì´ ì „ëµì€ Train timeì—ëŠ” ë„¤íŠ¸ì›Œí¬ì— randomnessë¥¼ ì¶”ê°€í•´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë§ˆêµ¬ì¡ì´ë¡œ í©ëœ¨ë ¤ ë†“ìœ¼ë¯€ë¡œì¨ Training dataì— ë„ˆë¬´ fití•˜ì§€ ì•Šê²Œ í•´ì¤€ë‹¤. ê·¸ë¦¬ê³  Test timeì—ì„œëŠ” randomnessë¥¼ í‰ê· í™” ì‹œì¼œì„œ generalization íš¨ê³¼ë¥¼ ì£¼ëŠ” ê²ƒì´ë‹¤. Dropoutì´ Regularizationì— ê°€ì¥ ëŒ€í‘œì ì¸ ì˜ˆì´ê¸´ í•˜ì§€ë§Œ Batch normalization ë˜í•œ ë¹„ìŠ·í•œ ë™ì‘ì„ í•  ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´ mini-batchë¡œ í•˜ë‚˜ì˜ ë°ì´í„°ê°€ ìƒ˜í”Œë§ ë  ë•Œ ë§¤ë²ˆ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„°ë“¤ê³¼ ë§Œë‚˜ê²Œ ëœë‹¤. Train timeì—ì„œëŠ” ê° ë°ì´í„°ì— ëŒ€í•´ì„œ ì´ ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ì–´ë–»ê²Œ ì •ê·œí™”ì‹œí‚¬ ê²ƒì¸ì§€ì— ëŒ€í•œ stochasticityì´ ì¡´ì¬í–ˆë‹¤. í•˜ì§€ë§Œ test timeì—ì„œëŠ” ì •ê·œí™”ë¥¼ mini-batch ë‹¨ìœ„ê°€ ì•„ë‹Œ global ë‹¨ìœ„ë¡œ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨ stochasticityë¥¼ í‰ê· í™” ì‹œí‚¨ë‹¤. ì´ëŸ¬í•œ íŠ¹ì„± ë•Œë¬¸ì— Batch-Normalizationì€ Dropoutê³¼ ìœ ì‚¬í•œ Regularization íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ Batch-Normalizationì„ ì‚¬ìš©í•  ë•ŒëŠ” Dropoutì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. Batch-Normalizationì—ë„ ì¶©ë¶„íˆ regularization íš¨ê³¼ê°€ ìˆê¸° ë•Œë¬¸ì´ë‹¤.[ê·¸ë¦¼15] Batch-Normalizationê³¼ëŠ” ë‹¤ë¥´ê²Œ ììœ ë¡­ê²Œ ì¡°ì ˆí•  ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„° pê°€ ìˆê¸° ë•Œë¬¸ì— Batch-Normalizationì€ ì—¬ì „íˆ ì“¸ëª¨ìˆë‹¤. data augmentation Regularization íŒ¨ëŸ¬ë‹¤ì„ì— ë¶€í•©í•˜ëŠ” ì „ëµ ì¤‘ í•˜ë‚˜ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê³ ì–‘ì´ ì‚¬ì§„ì„ classification ë¬¸ì œë¡œ í’€ë ¤ê³  í• ë•Œ, ì´ë¯¸ì§€ì˜ ë°˜ì „ì„ ì£¼ì–´ ì…ë ¥í•œë‹¤ë˜ì§€ ì•„ë‹ˆë©´, ì„ì˜ì˜ ë‹¤ì•¼í•œ ì‚¬ì´ì¦ˆë¡œ ì˜ë¼ì„œ(crop) ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, color jitteringë„ ìˆëŠ”ë° ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œëŠ” í•™ìŠµì‹œ ì´ë¯¸ì§€ì˜ contrast ë˜ëŠ” brightnessë¥¼ ë°”ê¿”ì¤€ë‹¤. ë³µì¡í•œ ë°©ë²•ìœ¼ë¡œëŠ” PCAì˜ ë°©í–¥ì„ ê³ ë ¤í•˜ì—¬ color offsetì„ ì¡°ì ˆí•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ëŸ° ë°©ë²•ì€ color jitteringì„ ì¢€ ë” data-dependentí•œ ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•˜ëŠ” ê²ƒìœ¼ë¡œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ì•„ë‹ˆë‹¤. data augmentationì€ ì–´ë–¤ ë¬¸ì œì—ë„ ì ìš©í•´ ë³¼ ìˆ˜ ìˆëŠ” ì•„ì£¼ ì¼ë°˜ì ì¸ ë°©ë²•ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì–´ë–¤ ë¬¸ì œë¥¼ í’€ë ¤ê³  í•  ë•Œ, ì´ë¯¸ì§€ì˜ labelì„ ë°”ê¾¸ì§€ ì•Šìœ¼ë©´ì„œ ì´ë¯¸ì§€ë¥¼ ë³€í™˜ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë§ì€ ë°©ë²•ë“¤ì„ ìƒê°í•´ ë³¼ ìˆ˜ ìˆë‹¤.train timeì— ì…ë ¥ ë°ì´í„°ì— ì„ì˜ì˜ ë³€í™˜ì„ ì‹œì¼œì£¼ê²Œ ë˜ë©´ ì¼ì¢…ì˜ regularization íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ê·¸ ì´ìœ ëŠ” ìœ„ì—ì„œ ì–¸ê¸‰í•˜ê³  ê°•ì¡°í•œ ê²ƒê³¼ ê°™ì´ train timeì—ëŠ” stochasticityê°€ ì¶”ê°€ë˜ê³  test timeì—ëŠ” marginalize out ë˜ê¸° ë•Œë¬¸ì´ë‹¤. DropConnect Dropoutê³¼ ë‹¤ë¥´ê²Œ activationì´ ì•„ë‹Œ weight matrixë¥¼ ì„ì˜ì ìœ¼ë¡œ 0ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ë°©ë²•ì´ë‹¤. fractional max pooling ë³´í†µì˜ ê²½ìš°, 2x2 max pooling ì—°ì‚°ì€ ê³ ì •ëœ 2x2 ì§€ì—­ì—ì„œë§Œ ìˆ˜í–‰í•˜ì§€ë§Œ fractional max poolingì—ì„œëŠ” ê·¸ë ‡ê²Œ í•˜ì§€ ì•Šê³  pooling ì—°ì‚°ì„ ìˆ˜í–‰í•  ì§€ì—­ì´ ì„ì˜ë¡œ ì„ ì •ëœë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ì•„ë˜ì˜ ê·¸ë¦¼ì—ì„œ Train timeì— ìƒ˜í”Œë§ ë  ìˆ˜ ìˆëŠ” ì„ì˜ì˜ pooling regionì„ ë³¼ ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  test timeì— stochasticityë¥¼ average out ì‹œí‚¤ë ¤ë©´ pooling regionsë¥¼ ê³ ì •ì‹œì¼œ ë²„ë¦¬ê±°ë‚˜ í˜¹ì€ ì—¬ëŸ¬ê°œì˜ pooling regionsë¥¼ ë§Œë“¤ê³  averaging overë¥¼ ì‹œí‚¨ë‹¤. ë§ì´ ì‚¬ìš©í•˜ì§€ëŠ” ì•Šì§€ë§Œ ì¢‹ì€ ë°©ë²•ì´ë‹¤.[ê·¸ë¦¼16] Stochastic Depth Train timeì—ëŠ” layer ì¤‘ ì¼ë¶€ë¥¼ ì œê±°í•´ ë²„ë¦¬ê³  ì¼ë¶€ë§Œ ì‚¬ìš©í•´ì„œ í•™ìŠµí•œë‹¤. Test timeì—ëŠ” ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‹¤ ì‚¬ìš©í•œë‹¤. ìµœì‹ ì˜ ì—°êµ¬ì´ë©° ì‹¤ì œë¡œ ì˜ ì‚¬ìš©í•˜ì§„ ì•Šì§€ë§Œ ì•„ì£¼ ì¢‹ì€ ì•„ì´ë””ì–´ì´ë‹¤. ë³´í†µ í•˜ë‚˜ ì´ìƒì˜ regularization ë°©ë²•ì„ ì‚¬ìš©í•˜ëŠ”ë° ê·¸ ì¤‘ì—ì„œë„ Batch-Normalizationë§Œìœ¼ë¡œë„ ì¶©ë¶„í•˜ë‹¤. ë‹¤ë§Œ Overfittingì´ ë°œìƒí•œë‹¤ ì‹¶ìœ¼ë©´ Dropoutê³¼ ê°™ì€ ë‹¤ì–‘í•œ ë°©ë²•ì„ ì¶”ê°€í•´ ë³¼ ìˆ˜ ìˆë‹¤. ì´ë¥¼ ê°€ì§€ê³  blind cross-validationì„ ìˆ˜í–‰í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ëŒ€ì‹ ì— ë„¤íŠ¸ì›Œí¬ì— overfitì˜ ì¡°ì§ì´ ë³´ì¼ë•Œ í•˜ë‚˜ì”© ì¶”ê°€ì‹œì¼œ ë³¸ë‹¤. Transfer Learning overfittingì´ ì¼ì–´ë‚  ìˆ˜ ìˆëŠ” ìƒí™© ì¤‘ í•˜ë‚˜ëŠ” ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ì„ ë•Œì´ë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ° ìƒí™©ì—ì„œ Transfer learningì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²° í•  ìˆ˜ ìˆë‹¤. ë˜í•œ, í”íˆë“¤ CNN í•™ìŠµì—ëŠ” ì—„ì²­ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤ê³  ìƒê°í•  ìˆ˜ ìˆëŠ”ë° ê·¸ëŸ°í•œ ì‚¬ê³ ë¥¼ ë¬´ë„ˆëœ¨ë ¤ ë²„ë ¤ì¤€ë‹¤. 1) ë¨¼ì € ImageNet ê°™ì€ ì•„ì£¼ í° ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµì„ í•œë²ˆ ì‹œí‚¨ë‹¤.ImageNetì—ì„œ í•™ìŠµëœ featureë¥¼ ìš°ë¦¬ê°€ ê°€ì§„ ì‘ì€ ë°ì´í„°ì…‹ì— ì ìš©í•˜ëŠ” ê²ƒì´ë‹¤.ì´ì œëŠ” 1000ê°œì˜ ImageNet ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ 10ì¢…ì˜ ê°•ì•„ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œì´ë‹¤. ë°ì´í„°ëŠ” ì—„ì²­ ì ë‹¤. ì´ ë°ì´í„° ì…‹ì€ ì˜¤ì§ C(ì˜ˆ:10ê°œ)ê°œì˜ í´ë˜ìŠ¤ë§Œ ê°€ì§€ê³  ìˆë‹¤. 2) ê°€ì¥ ë§ˆì§€ë§‰ì˜ Fully connected layerëŠ” ìµœì¢… featureì™€ class scoresê°„ì˜ ì—°ê²°ì¸ë° ì´ë¥¼ ì´ˆê¸°í™”ì‹œí‚¨ë‹¤. ë˜í•œ, ê¸°ì¡´ì˜ ImageNetì€ 4,096 x 1,000 ì°¨ì›ì˜ matrixì˜€ì§€ë§Œ ì´ì œëŠ” ìš°ë¦¬ê°€ 10ê°œì˜ í´ë˜ìŠ¤ë¥¼ ê°–ìœ¼ë¯€ë¡œ 4,096 x 10 ì°¨ì›ì˜ matrixë¡œ ë°”ê¿”ì¤€ë‹¤. ë‚˜ë¨¸ì§€ ì´ì „ì˜ ëª¨ë“  ë ˆì´ì–´ë“œë¥´ì´ ê°€ì¤‘ì¹˜ëŠ” ê·¸ëŒ€ë¡œ ë‘”ë‹¤. ì´ë ‡ê²Œ ë˜ë©´ linear classifierë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒê³¼ ê°™ë‹¤. ì™œëƒí•˜ë©´ ì˜¤ë¡œì§€ ë§ˆì§€ë§‰ ë ˆì´ì–´ë§Œ ê°€ì§€ê³  ìš°ë¦¬ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. ì´ëŸ¬í•œ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ì•„ì£¼ ì‘ì€ ë°ì´í„° ì…‹ì¼ì§€ë¼ë„ ì•„ì£¼ ì˜ ë™ì‘í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ë§Œì¼, ë°ì´í„°ê°€ ì¡°ê¸ˆ ë” ìˆë‹¤ë©´ ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ fine-tuning í•  ìˆ˜ ìˆë‹¤. ìµœì¢… ë ˆì´ì–´ë“¤ì„ í•™ìŠµì‹œí‚¤ê³  ë‚˜ë©´, ë„¤íŠ¸ì›Œí¬ì˜ ì¼ë¶€ë§Œì´ ì•„ë‹Œ ë„¤íŠ¸ì›Œí¬ ì „ì²´ì˜ í•™ìŠµì„ ê³ ë ¤í•´ ë³¼ ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤ ë°ì´í„°ê°€ ë” ë§ì´ ìˆë‹¤ë©´ ë„¤íŠ¸ì›Œí¬ì˜ ë” ë§ì€ ë¶€ë¶„ì„ ì—…ë°ì´íŠ¸ ì‹œí‚¬ ìˆ˜ ìˆì„ì§€ë„ ëª¨ë¥¸ë‹¤. ì´ ë¶€ë¶„ì—ì„œëŠ” ë³´í†µ ê¸°ì¡´ì˜ Learning rateë³´ë‹¤ëŠ” ë‚®ì¶°ì„œ í•™ìŠµì‹œí‚¨ë‹¤. ì™œëƒí•˜ë©´ ê¸°ì¡´ì˜ ê°€ì¤‘ì¹˜ë“¤ì´ ì´ë¯¸ ImageNetìœ¼ë¡œ ì˜ í•™ìŠµë˜ì–´ ìˆê³  ì´ ê°€ì¤‘ì¹˜ë“¤ì´ ëŒ€ê²ŒëŠ” ì•„ì£¼ ì˜ ë™ì‘í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ìš°ë¦¬ê°€ ê°€ì§„ ë°ì´í„°ì…‹ì—ì„œì˜ ì„±ëŠ¥ì„ ë†’íˆê¸° ìœ„í•´ì„œë¼ë©´ ê·¸ ê°€ì¤‘ì¹˜ë“¤ì„ ì•„ì£¼ ì¡°ê¸ˆì”©ë§Œ ìˆ˜ì €ì •í•˜ë©´ ë  ê²ƒì´ë‹¤. transfer learningì€ ê±°ì˜ ì¼ìƒì ì¸ ìˆ˜ì¤€ì´ ë˜ì—ˆë‹¤. ëŒ€ë¶€ë¶„ì€ ImageNet pretrained modelì„ ì‚¬ìš©í•˜ê³  í˜„ì¬ ë³¸ì¸ì˜ taskì— ë§ë„ë¡ fine tuningí•œë‹¤. captioningì˜ ê²½ìš° word vectorsë¥¼ pretrainí•˜ê¸°ë„ í•œë‹¤. pretrained CNN ë¿ë§Œ ì•„ë‹ˆë¼ í° ê·œëª¨ì˜ ì½”í¼ìŠ¤ë¡œ ë¶€í„° í•™ìŠµëœ pretrained word vectorsë„ í•¨ê»˜ ì´ìš©í•  ìˆ˜ ìˆë‹¤. í—ˆë‚˜, captioning taskì—ì„œëŠ” pretrained word vectorsì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ê°€ ë§ì§€ ì•Šê³  í¬ê²Œ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤. ë¬¸ì œì— ëŒ€í•œ ë°ì´í„°ì…‹ì´ í¬ì§€ ì•Šì€ ê²½ìš°ë¼ë©´ í’€ë ¤ëŠ” ë¬¸ì œì™€ ìœ ì‚¬í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµëœ pretrained modelì„ ë‹¤ìš´ë¡œë“œ ë°›ì•„ë¼. ê·¸ë¦¬ê³  ì´ ëª¨ë¸ì˜ ì¼ë¶€ë¥¼ ì´ˆê¸°\bí™”ì‹œí‚¤ê³  ê°€ì§€ê³ ìˆëŠ” ë°ì´í„°ë¡œ ëª¨ë¸ì„ fine-tuningí•œë‹¤. TensorFlow : https://github.com/tensorflow/modelsPytorch : https://github.com/pytorch/vision","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"[CS231n]Lecture06-Training Neural Networks","slug":"cs231n_06","date":"2019-07-19T14:15:00.000Z","updated":"2019-07-23T04:18:10.010Z","comments":true,"path":"2019/07/19/cs231n_06/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/19/cs231n_06/","excerpt":"","text":"Optimizationì„ í†µí•´ì„œ ë„¤íŠ¸ì›Œí¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤. Lossê°€ ì¤„ì–´ë“œëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ëŠ”ë° ì´ê²ƒì€ gradientì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. Mini-batch SGD ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë“¤(ë„¤íŠ¸ì›Œí¬ì˜ íŒŒë¼ë¯¸í„°)ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. Mini-batch SGDë¡œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ê³¼ì •Loop: ë°ì´í„°ì˜ batch sizeë¥¼ ì •í•œ í›„ ê·¸ë§Œí¼ì˜ í¬ê¸°ì˜ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¨ë‹¤. Forward propì„ í†µí•´ Lossê°’ì„ ì‚°ì¶œí•œë‹¤. gradientë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ Backpropagationì„ ì‹œí–‰í•œë‹¤. ê³„ì‚°í•˜ì—¬ ì–»ì€ gradientë¥¼ ì´ìš©í•´ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•œë‹¤. Training Neural NetworksNeural Networksì˜ í•™ìŠµì„ ì²˜ìŒ ì‹œì‘í• ë•Œ í•„ìš”í•œ ê¸°ë³¸ì„¤ì •ì— ëŒ€í•´ ë§í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. activation function ì„ íƒ, preprocessing, weight initialization, regularization, gradient checking Activation function1)Sigmoid $\\sigma(x) = \\frac{1}{1+e^{-x}}$ ê° ì…ë ¥ì„ ë°›ì•„ì„œ ê·¸ ì…ë ¥ì„ [0,1]ì‚¬ì´ì˜ ê°’ì´ ë˜ë„ë¡ í•´ì¤€ë‹¤. ì…ë ¥ì˜ ê°’ì´ í¬ë©´ Sigmoidì˜ ì¶œë ¥ê°’ì´ 1ì— ê°€ê¹Œìš¸ ê²ƒì´ê³  ì‘ìœ¼ë©´ 0ì— ê°€ê¹Œìš¸ ê²ƒì´ë‹¤. neuronì˜ firing rateë¥¼ saturation ì‹œí‚¤ëŠ” ê²ƒìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´ ì–´ë–¤ ê°’ì´ 0ì—ì„œ 1ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ë©´ ì´ë¥¼ fireing rateë¼ê³  ìƒê°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ReLuê°€ ìƒë¬¼í•™ì  íƒ€ë‹¹ì„±ì´ ë” í¬ê¸°ì— ìµœê·¼ì—ëŠ” ëŒ€ë¶€ë¶„ ReLuë¥¼ ì‚¬ìš©í•œë‹¤.(ì´ ë¶€ë¶„ì€ ìƒë¬¼í•™ì ì¸ ë‚´ìš©ì¸ê²ƒ ê°™ë‹¤.) ì°¸ì¡° firing rate ë¬¸ì œì  Saturationë˜ëŠ”ê²ƒì´ gradientë¥¼ ì—†ì•¤ë‹¤. Sigmoidì—ì„œ xê°€ 0ì´ë©´ ì˜ ì‘ë™í•  ê²ƒì´ê³ , gradientë„ ì˜ ì–»ê²Œ ë  ê²ƒì´ë‹¤. í—ˆë‚˜, ìŒì˜ í° ê°’ì´ê±°ë‚˜ ì–‘ì˜ í°ê°’ì´ë©´ sigmoidê°€ flatí•˜ê²Œ ë˜ê³  gradientê°€ 0ì´ ë  ê²ƒì´ë‹¤. ê±°ì˜ 0ì— ê°€ê¹Œìš´ ê°’ì´ backprobì´ ë˜ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ê²°êµ­ ì¶œë ¥ ë…¸ë“œì—ì„œëŠ” ê°€ì¤‘ì¹˜ì˜ ì—…ë°ì´íŠ¸ ì˜ë¯¸ê°€ ìˆëŠ” ê²ƒì´ ë˜ê² ì§€ë§Œ ë°‘ìœ¼ë¡œ ê³„ì† Backpropagation ê³¼ì •ì„ í•˜ë‹¤ë³´ë©´ 0ì´ ê³„ì† ì „ë‹¬ë˜ê²Œ ë˜ë¯€ë¡œ ì˜ë¯¸ê°€ ì—†ì–´ì§„ë‹¤. sigmoidì˜ ì¶œë ¥ì´ zero centered í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì´ë‹¤. sigmoidì˜ ì…ë ¥ì´ í•­ìƒ ì–‘ìˆ˜ë¼ê³  ê°€ì •í•´ë³´ì. ê·¸ëŸ° layerì—ì„œ dL/dfë¥¼ ê³„ì‚°í•˜ë©´ì„œ local gradientë¥¼ ìƒê°í•´ë³´ì. local gradientëŠ” ì „ë¶€ ì–‘ìˆ˜ê°€ ë˜ê±°ë‚˜ ì „ë¶€ ìŒìˆ˜ê°€ ëœë‹¤. ê²°ë¡ ì ìœ¼ë¡œëŠ” gradientì˜ ë¶€í˜¸ê°€ ê³„ì† ë™ì¼í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì— ê°€ì¤‘ì¹˜ê°€ ëª¨ë‘ ê°™ì€ ë°©í–¥ìœ¼ë¡œë§Œ ì›€ì§ì¼ ê²ƒì„ì„ ì˜ë¯¸í•œë‹¤. íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•  ë•Œ ë‹¤ ê°™ì´ ì¦ê°€í•˜ê±°ë‚˜ ë‹¤ê°™ì´ ê°ì†Œí•˜ê±°ë‚˜ í•  ìˆ˜ ë°–ì— ì—†ë‹¤. ì´ëŸ° gradient ì—…ë°ì´íŠ¸ëŠ” ì•„ì£¼ ë¹„íš¨ìœ¨ì ì´ë‹¤. ìœ„ì—ì„œ ì „ë¶€ ì–‘ìˆ˜ ë˜ëŠ” ìŒìˆ˜ë¡œ ì—…ë°ì´íŠ¸ëœë‹¤ëŠ” ê²ƒì„ í•´ì„í•´ë³´ë©´, gradientê°€ ì´ë™í•  ìˆ˜ ìˆëŠ” ë°©í–¥ì€ 4ë¶„ë©´ ì¤‘ ë‘ ì˜ì—­ë§Œ í•´ë‹¹ ë  ê²ƒì´ë‹¤. ì´ëŸ¬í•œ ì´ìœ ê°€ zero-mean dataë¥¼ ìš°ë¦¬ê°€ ë§Œë“¤ì–´ì£¼ëŠ” ì´ìœ ì´ë‹¤. ì…ë ¥ Xê°€ ì–‘ìˆ˜/ìŒìˆ˜ë¥¼ ëª¨ë‘ ê°€ì§€ê³  ìˆìœ¼ë©´ ì „ë¶€ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ì›€ì§ì´ëŠ” ì¼ì€ ë°œìƒí•˜ì§€ ì•Šì„ ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. exponentialë¡œ ì¸í•´ ê³„ì‚°ë¹„ìš©ì´ í¬ë‹¤ëŠ” ê²ƒì´ë‹¤.(ë‚´ì ì˜ ê³„ì‚°ì´ ë¹„ì‹¸ë‹¤.) 2)tanh sigmoidì™€ëŠ” ë¹„ìŠ·í•œ ëª¨ì–‘ì„ ê°€ì§€ì§€ë§Œ ì¶œë ¥ê°’ì˜ ë²”ìœ„ê°€ [-1,1]ì´ë‹¤. ê°€ì¥ í° ì°¨ì´ë¼ë©´ zero-centeredë¼ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ í†µí•´ sigmoidì˜ ë‘ë²ˆì§¸ ë¬¸ì œëŠ” í•´ê²°ë˜ì§€ë§Œ, saturation ë•Œë¬¸ì— ì—¬ì „íˆ GradientëŠ” ì£½ëŠ”ë‹¤. ì—¬ì „íˆ flatí•œ êµ¬ê°„ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤.-3)ReLU f(x) = max(0,x) ì ì–´ë„ ì–‘ì˜ ê°’ì—ì„œëŠ” saturetionë˜ì§€ ì•ŠëŠ”ë‹¤. sigmoidì™€ëŠ” ë‹¤ë¥´ê²Œ exponentialê°™ì€ ì—°ì‚°ì´ ì—†ëŠ” ë‹¨ìˆœí•œ max ì—°ì‚°ì´ë¯€ë¡œ ê³„ì‚°ì´ ë§¤ìš° ë¹ ë¥´ë‹¤.(sigmoidë‚˜ tanhë³´ë‹¤ ìˆ˜ë ´ì†ë„ê°€ 6ë°°ì •ë„ë¡œ í›¨ì”¬ ë¹ ë¥´ë‹¤.) ìƒë¬¼í•™ì  íƒ€ë‹¹ì„±ë„ ReLUê°€ sigmoidë³´ë‹¤ í¬ë‹¤. ImageNet 2012ì—ì„œ ìš°ìŠ¹í•œ AlexNetì´ ì²˜ìŒ ReLUë¥¼ ì‚¬ìš©í•˜ê¸° ì‹œì‘í–ˆë‹¤. ë¬¸ì œì  ë” ì´ìƒ zero-centeredê°€ ì•„ë‹ˆë¼ëŠ” ì ì´ë‹¤. tanhê°€ ì´ ë¬¸ì œëŠ” í•´ê²°í–ˆëŠ”ë° ReLUëŠ” ë‹¤ì‹œ ì´ ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆê²Œ ëœë‹¤. ì–‘ì˜ ìˆ˜ì—ì„œëŠ” saturationì´ ë˜ì§€ ì•Šì§€ë§Œ ìŒì˜ ê²½ìš°ì—ì„œëŠ” ê·¸ë ‡ì§€ ì•Šë‹¤. Dead ReLUê°€ ë°œìƒë˜ëŠ” ë¬¸ì œì˜ ì´ìœ  initializationì„ ì˜ëª»í•œ ê²½ìš° learning rateê°€ ì§€ë‚˜ì¹˜ê²Œ ë†’ì€ ê²½ìš° 4)Leaky ReLU f(x) = max(0.01x, x) ReLUì™€ ìœ ì‚¬í•˜ì§€ë§Œ ìŒìˆ˜ ì˜ì—­ì—ì„œ ë”ì´ìƒ 0ì´ ì•„ë‹ˆë©°, ì—¬ì „íˆ ê³„ì‚°ì´ íš¨ìœ¨ì ì´ì–´ì„œ sigmoidë‚˜ tanhë³´ë‹¤ ìˆ˜ë ´ì„ ë¹¨ë¦¬ í•  ìˆ˜ ìˆë‹¤. Dead ReLUê°€ ì—†ë‹¤! 5)Parametric ReLU f(x) = max(alpha*x, x) Leaky ReLUì™€ ìœ ì‚¬í•˜ê²Œ ìŒì˜ ì˜ì—­ì—ì„œë„ ê¸°ìš¸ê¸°ë¥¼ ê°€ì§€ê³  ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ alphaë¼ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì •í•´ ë†“ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, backprobìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” ì ì´ ë‹¤ë¥´ë‹¤. 6)Exponential Linear Units(ELU) ReLUì˜ ì¥ì ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ëŠ”ë°, ì¶”ê°€ì ìœ¼ë¡œ zero-meanì— ê°€ê¹Œìš´ ì¶œë ¥ì„ ë³´ì—¬ì¤€ë‹¤. zero-meanì— ê°€ê¹Œìš´ ì¶œë ¥ì€ ì•ì„œ Leaky ReLU, PReLUê°€ ê°€ì§„ ì´ì ì´ìˆë‹¤. í•˜ì§€ë§Œ Leaky ReLUì™€ ë¹„êµí•´ë³´ë©´ ELUëŠ” negativeì—ì„œ ê¸°ìš¸ê¸°ë¥¼ ê°€ì§€ëŠ” ê²ƒ ëŒ€ì‹  ë˜ ë‹¤ì‹œ saturationì´ ëœë‹¤.ELUê°€ ì£¼ì¥í•˜ëŠ”ê±´ ì´ëŸ° saturationì´ ì¢€ë” noiseì— ê°•ì¸í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ReLUì™€ Leaky ReLUì˜ ì¤‘ê°„ ì •ë„ì´ë‹¤. Leaky ReLUì²˜ëŸ¼ zero-meanì˜ ì¶œë ¥ì„ ë‚´ì§€ë§Œ Saturationì˜ ê´€ì ì—ì„œ ReLUì˜ íŠ¹ì„±ë„ ê°€ì§€ê³  ìˆë‹¤. 6)Maxout â€œNeuronâ€ maxoutëŠ” ReLUì™€ leaky ReLUì˜ ì¢€ ë” ì¼ë°˜í™”ëœ í˜•íƒœì´ë‹¤. ì„ í˜•í•¨ìˆ˜ì´ê¸° ë•Œë¬¸ì— saturationë˜ì§€ ì•Šìœ¼ë©° gradientê°€ ì£½ì§€ ì•Šì„ ê²ƒì´ë‹¤. ë¬¸ì œì ì€ ë‰´ëŸ°ë‹¹ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ë‘ë°°ê°€ ëœë‹¤ëŠ” ê²ƒì´ë‹¤. w1ê³¼ w2ë¥¼ ì§€ë‹ˆê³  ìˆì–´ì•¼í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì‹¤ì œë¡œ ê°€ì¥ ë§ì´ë“¤ ì“°ëŠ” ê²ƒì€ ë°”ë¡œ ReLUì´ë‹¤. ë‹¤ë§Œ ReLUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ learning rateë¥¼ ì•„ì£¼ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ê²°ì •í•´ì•¼ í•  ê²ƒì´ë‹¤. Leaky ReLU, Maxout, ELUì™€ ê°™ì€ ê²ƒë“¤ë„ ì¨ë³¼ ìˆ˜ ìˆì§€ë§Œ ì•„ì§ ì‹¤í—˜ë‹¨ê³„ì´ê¸´ í•˜ë‹¤. ì—¬ëŸ¬ë¶„ë“¤ì˜ ë¬¸ì œì— ë§ì¶° ì–´ë–¤ í™œì„±í•¨ìˆ˜ê°€ ì˜ ë™ì‘í•˜ëŠ”ì§€ í™•ì¸í•´ ë³¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. tanhë„ ì¨ë³¼ ìˆ˜ ìˆì§€ë§Œ, ëŒ€ê²ŒëŠ” ReLU, ReLUì˜ ë³€ì¢…ë“¤ì´ ì¢€ ë” ì˜ ë™ì‘í•œë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤. Data Preprocessingê°€ì¥ ëŒ€í‘œì ì¸ ì „ì²˜ë¦¬ ê³¼ì •ì€ zero-meanìœ¼ë¡œ ë§Œë“¤ê³  normalizeí•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° ê³¼ì •ì€ ì™œ ê±°ì¹˜ëŠ” ê²ƒì¼ê¹Œ? zero-centeringì€ ê°€ì¤‘ì¹˜ì˜ ì—…ë°ì´íŠ¸ì‹œì— ì¢€ ë” ë‹¤ì–‘í•œ ë°©í–¥ì˜ gradientë¥¼ ì–»ê¸° ìœ„í•¨ì´ë©°, normalizationì„ í•´ì£¼ëŠ” ì´ìœ ëŠ” ëª¨ë“  ì°¨ì›ì´ ë™ì¼í•œ ë²”ìœ„í•œì— ìˆê²Œ í•´ì¤˜ ë™ë“±í•œ ê¸°ì—¬ë¥¼ í•˜ê²Œ í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. ì´ë¯¸ì§€ì˜ ê²½ìš° ì‹¤ì œë¡œëŠ” ì „ì²˜ë¦¬ë¡œ zero-centering ì •ë„ë§Œ í•´ì¤€ë‹¤. ì™œëƒí•˜ë©´ ì´ë¯¸ì§€ëŠ” ì´ë¯¸ ê° ì°¨ì›ê°„ì— ìŠ¤ì¼€ì¼ì´ ì–´ëŠì •ë„ ë§ì¶°ì ¸ ìˆê¸° ë•Œë¬¸ì— normalizationì´ í•„ìš”ì¹˜ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. ë‹¤ë¥¸ PCAë‚˜ whiteningê°™ì€ ë” ë³µì¡í•œ ì „ì²˜ë¦¬ ê³¼ì •ë„ ì˜ ì‚¬ìš©ì¹˜ ì•ŠëŠ”ë‹¤. -AlexNetì€ ì „ì²´ ì´ë¯¸ì§€ì˜ í‰ê· ì„ ë¹¼ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ zero-centeringì„ í•´ì£¼ì§€ë§Œ VGGNetì€ ê° ì±„ë„ë³„ í‰ê· ì„ ë¹¼ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•œë‹¤. ì´ëŸ° ë°©ì‹ì˜ ê²°ì €ì„ ê°ìì˜ íŒë‹¨ì— ë§¡ê²¨ì§„ë‹¤. weight Initializationëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì„¸íŒ…í•œë‹¤ë©´ ì–´ë–»ê²Œ ë ê¹Œ? ê°€ì¤‘ì¹˜ê°€ 0ì´ë¼ì„œ ëª¨ë“  ë‰´ëŸ°ì€ ëª¨ë‘ ë‹¤ ê°™ì€ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤. ì¶œë ¥ë„ ê°™ì„ ê²ƒì´ê³ , ê²°êµ­ gradientë„ ì„œë¡œ ê°™ì„ ê²ƒì´ë‹¤. ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ ë˜‘ê°™ì€ ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸ ëœë‹¤. ì´ê²ƒì´ ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ ë™ì¼í•˜ê²Œ ì´ˆê¸°í™”ì‹œí‚¤ë©´ ë°œìƒí•˜ëŠ” ì¼ì´ë‹¤. ìœ„ì™€ ê°™ì€ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ë“¤ì„ ì œì‹œí•œë‹¤. í‘œì¤€ ì •ê·œ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì—ì„œ ì„ì˜ì˜ ì‘ì€ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•œë‹¤. ì´ ë°©ë²•ì€ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ”ë° ìˆì–´ì„œ ê°€ì¤‘ì¹˜ì˜ ê°’ì´ ì‘ê¸° ë•Œë¬¸ì— ê°€ì¤‘ì¹˜ê°€ 0ìœ¼ë¡œ ìˆ˜ë ´ë˜ëŠ” ìƒí™©ì´ ë°œìƒë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ì ì ˆí•œ ê°€ì¤‘ì¹˜ë¥¼ ì–»ëŠ” ê²ƒì€ ë„ˆë¬´ ì–´ë µë‹¤. ë„ˆë¬´ ì‘ìœ¼ë©´ ì‚¬ë¼ì ¸ë²„ë¦¬ê³  ë„ˆë¬´ í¬ë©´ saturation(ê°€ì¤‘ì¹˜ì˜ gradientê°€ 0)ì´ ë˜ì–´ë²„ë¦°ë‹¤. 1)Xavier Initialization-Standard gaussianìœ¼ë¡œ ë½‘ì€ ê°’ì„ ì…ë ¥ì˜ ìˆ˜ë¡œ ìŠ¤ì¼€ì¼ë§í•´ì¤€ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ Xavier initializationê°€ í•˜ëŠ” ì¼ì€ ì…/ì¶œë ¥ì˜ ë¶„ì‚°ì„ ë§ì¶°ì£¼ëŠ” ê²ƒì´ë‹¤. ì…ë ¥ì˜ ìˆ˜ê°€ ì‘ìœ¼ë©´ ë” ì‘ì€ ê°’ìœ¼ë¡œ ë‚˜ëˆ„ê³  ì¢€ ë” í° ê°’ì„ ì–»ëŠ”ë‹¤. ë” í° ê°€ì¤‘ì¹˜ë¥¼ ì–»ëŠ” ì´ìœ ëŠ” ì‘ì€ ì…ë ¥ì˜ ìˆ˜ì™€ ê°€ì¤‘ì¹˜ê°€ ê³±í•´ì§€ê¸° ë•Œë¬¸ì— ê°€ì¤‘ì¹˜ê°€ ì»¤ì•¼ë§Œ ì¶œë ¥ì˜ ë¶„ì‚° ë§Œí¼ í° ê°’ì„ ì–»ì„ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë°˜ëŒ€ë¡œ ì…ë ¥ì˜ ìˆ˜ê°€ ë§ì€ ê²½ìš°ì—ëŠ” ë” ì‘ì€ ê°€ì¤‘ì¹˜ê°€ í•„ìš”í•˜ë‹¤. ê° ë ˆì´ì–´ì˜ ì…ë ¥ì´ Unit gaussianì´ê¸¸ ì›í•œë‹¤ë©´ ì´ëŸ° ë¥˜ì˜ ì´ˆê¸°í™” ê¸°ë²•ì„ ì‚¬ìš©í•´ ë³¼ ìˆ˜ìˆë‹¤. ì´ëŸ° ê²°ê³¼ëŠ” Linear activationì´ ìˆë‹¤ê³  ê°€ì •í•˜ëŠ” ê²ƒì´ë‹¤. tanhì˜ ê²½ìš°ë¥¼ ì˜ˆë¥¼ ë“¤ë©´ active regionì•ˆì— ìˆë‹¤ê³  ê°€ì •í•˜ëŠ” ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ReLUë¥¼ ì“°ë©´ ì˜ ì‘ë™í•˜ì§€ ì•ŠëŠ”ë‹¤. ReLUëŠ” ì¶œë ¥ì˜ ì ˆë°˜ì´ 0ì´ë˜ì–´ ì£½ëŠ”ë‹¤. ê²°êµ­ ì¶œë ¥ì˜ ë¶„ì‚°ì„ ë°˜í† ë§‰ ë‚´ë²„ë¦°ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ê°’ì´ ë„ˆë¬´ ì‘ì•„ì§€ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¶”ê°€ì ìœ¼ë¡œ 2ë¡œ ë‚˜ëˆ ì£¼ëŠ” ì¦‰ ì…ë ¥ì˜ ë°˜ë°–ì— ë“¤ì–´ê°€ì§€ ì•ŠëŠ” ì ì„ ì¶”ê°€í•´ì£¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ê¸°ë„ í•œë‹¤. 2)Batch Normalizationgaussianì˜ ë²”ìœ„ë¡œ activationì„ ìœ ì§€ì‹œí‚¤ëŠ” ê²ƒì— ê´€ë ¨í•œ ì•„ì´ë””ì–´ ì¤‘ í•˜ë‚˜ì´ë‹¤. ìš°ë¦¬ëŠ” layerë¡œ ë¶€í„° ë‚˜ì˜¨ activationì˜ ê°’ë“¤ì´ Unit gaussianì´ê¸°ë¥¼ ë°”ë€ë‹¤. ê°€ì¤‘ì¹˜ë¥¼ ì˜ ì´ˆê¸°í™” ì‹œí‚¤ëŠ” ê²ƒ ëŒ€ì‹ ì— í•™ìŠµ í•  ë•Œ ë§ˆë‹¤ ê° ë ˆì´ì–´ì— ì´ëŸ° ì¼ì„ í•´ì¤˜ì„œ ëª¨ë“  layerê°€ Unit gaussianì´ ë˜ë„ë¡ í•´ì¤€ë‹¤. Batch size ë§Œí¼ì˜ ë°ì´í„°ì—ì„œ ê° í”¼ì²˜ë³„ë¡œ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•œí›„ Normalizationì„ í•´ì¤€ë‹¤. ì´ëŸ¬í•œ ì—°ì‚°ì€ Fully connected layerë‚˜ Conv Layerì§í›„ì— ë„£ì–´ì¤€ë‹¤. ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ê° layerì˜ ê°€ì¤‘ì¹˜ê°€ ì§€ì†ì ìœ¼ë¡œ ê³±í•´ì ¸ì„œ Bad scaling effectê°€ ë°œìƒí–ˆì§€ë§Œ, Normalizationì€ ê·¸ëŸ° Bad effectë¥¼ ìƒì‡„ì‹œì¼œ ë²„ë¦°ë‹¤. Batch Normalizationì€ ì…ë ¥ì˜ ìŠ¤ì¼€ì¼ë§Œ ì‚´ì§ ì¡°ì •í•´ ì£¼ëŠ” ì—­í• ì´ê¸° ë•Œë¬¸ì— Fully connectedì™€ Conv layer ì–´ë””ì—ë“  ì ìš©ê°€ëŠ¥í•˜ë‹¤. Conv layerì—ì„œ ì°¨ì´ì ì´ ìˆë‹¤ë©´ Normalizationì„ ì°¨ì›ë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ê°™ì€ Activation Mapì˜ ê°™ì€ ì±„ë„ì— ìˆëŠ” ìš”ì†Œë“¤ì€ ê°™ì´ Normalizeí•´ ì¤€ë‹¤. ì™œëƒí•˜ë©´ Conv íŠ¹ì„±ìƒ ê°™ì€ ë°©ì‹ìœ¼ë¡œ normalize ì‹œì¼œì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì¦‰, Conv Layerì˜ ê²½ìš° Activation map(ì±„ë„, Depth)ë§ˆë‹¤ í‰ê· ê³¼ ë¶„ì‚°ì„ í•˜ë‚˜ë§Œ êµ¬í•œë‹¤. ê·¸ë¦¬ê³  í˜„ì¬ Batchì— ìˆëŠ” ëª¨ë“  ë°ì´í„°ë¡œ Normalizeë¥¼ í•´ì¤€ë‹¤. ì´ì²˜ëŸ¼ layerì˜ ì…ë ¥ì´ unit gaussianì´ ë˜ë„ë¡ ê°•ì œí•˜ëŠ” ê²ƒì´ë‹¤. tanhë¥¼ ì˜ˆì‹œë¡œ ìƒê°í•´ ë³´ë©´ ì…ë ¥ë°ì´í„°ê°€ tanhë¡œ ì¸í•´ ì–¼ë§ˆë‚˜ saturation ë ì§€ë¥¼ ì¡°ì ˆí•˜ê³  ì‹¶ì€ ê²½ìš° ê°™ì´ ìœ ì—°ì„±ì„ ì¤„ ìˆ˜ë„ ìˆë‹¤. í•™ìŠµê°€ëŠ¥í•œ ê°ë§ˆì™€ ë² íƒ€ë¥¼ í†µí•´ normalization ë‹¨ê³„ ì´ì „ìœ¼ë¡œ ë‹¤ì‹œ ë³µì› ì‹œì¼œì£¼ëŠ” ê²ƒê³¼ ê°™ì€ ì‘ì—…ì„ í†µí•´ ìœ ì—°ì„±ì„ ê°–ê²Œí•  ìˆ˜ ìˆë‹¤. ë•Œë¬¸ì— batch ë‹¨ìœ„ë¡œ normalizationì„ ì¼ë‹¨ í•´ì£¼ê³  íŒŒë¼ë¯¸í„°ë¥¼ ë‹¤ì‹œ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ë‹¤. ë˜í•œ Batch Normalizationì€ regularizationì˜ ì—­í• ë„ í•œë‹¤. ê° layerì˜ ì¶œë ¥ì€ í•´ë‹¹ ë°ì´í„° í•˜ë‚˜ ë¿ë§Œ ì•„ë‹ˆë¼ batch ì•ˆì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  ë°ì´í„°ë“¤ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤. ë” ì´ìƒ layerì˜ ì¶œë ¥ì€ deterministicí•˜ì§€ ì•Šê³  ì¡°ê¸ˆì”© ë°”ë€Œê²Œ ë˜ê³  ì´ëŠ” regularization effectë¥¼ ì¤€ë‹¤. ìš°ë¦¬ê°€ ì…ë ¥ì„ ê°•ì œë¡œ gaussian ë¶„í¬ë¡œ ë§Œë“¤ì–´ ë²„ë¦¬ê²Œ ë˜ë©´ ê¸°ì¡´ì˜ êµ¬ì¡°ë¥¼ ìƒëŠ” ê²ƒì€ ì•„ë‹Œì§€ì— ëŒ€í•œ ì˜ë¬¸ì´ ìƒê¸¸ ìˆ˜ ë„ ìˆë‹¤. íŠ¹íˆ CNNì˜ ê²½ìš° ì…ë ¥ì˜ ê³µê°„ì  íŠ¹ì„±ì„ ìœ ì§€ì‹œì¼œ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ë¯€ë¡œ ë” ê°•í•œ ì˜ë¬¸ì´ ë“¤ìˆ˜ë„ ìˆê² ì§€ë§Œ, ì˜ˆë¥¼ ë“¤ì–´ ì„¤ëª…í•˜ìë©´ ë°ì´í„°ì˜ ì „ì²˜ë¦¬ë¥¼ í•  ë•Œë„ gaussianì„ ì‚¬ìš©í•˜ëŠ”ë° ê·¸ëŸ´ ê²½ìš°ë„ ëª¨ë“  í”¼ì²˜ë“¤ì„ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ë§Œë“ ë‹¤ê³  í•´ë„ ì–´ë– í•œ êµ¬ì¡°ë„ ìƒì–´ë²„ë¦¬ì§€ ì•ŠëŠ”ë‹¤. ë‹¨ì§€, ë°ì´í„°ì— ì—°ì‚°ì´ ì˜ ìˆ˜í–‰ë˜ë„ë¡ ì„ í˜•ë³€í™˜(ìŠ¤ì¼€ì¼ë§, ì‹œí”„íŠ¸)ì„ í•´ì£¼ëŠ” ê²ƒì´ë‹¤. ë˜í•œ, shiftì™€ scaleìš”ì†Œë¥¼ ì¶”ê°€ì‹œì¼œ í•™ìŠµì„ ì‹œì¼œë²„ë¦¬ë©´ ê²°êµ­ identity mappingì´ ë˜ì„œ Batch normalizationì´ ì‚¬ë¼ì§€ëŠ” ê²ƒì´ ì•„ë‹Œì§€ì˜ë¬¸ì´ ë“¤ ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ ê°ë§ˆì™€ ë² íƒ€ë¥¼ í•™ìŠµì‹œí‚¤ê²Œ ë˜ë©´ identityê°€ ë˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. siftì™€ scaleì´ ì¼ì •ëŸ‰ ë³€í•˜ê¸´ í•˜ì§€ë§Œ ë³´í†µì€ identity mappingì´ ë  ì •ë„ëŠ” ì•„ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì—¬ì „íˆ batch normalizationì˜ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. Batch Noramalizationì—ì„œ í‰ê· ê³¼ ë¶„ì‚°ì€ í•™ìŠµë°ì´í„°ì—ì„œ êµ¬í•œê²ƒì´ë©°, testì‹œì—ëŠ” ì¶”ê°€ì ì¸ ê³„ì‚°ì€ í•˜ì§€ì•ŠëŠ”ë‹¤. í•˜ì´í¼ íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì—ëŠ” Log scaleë¡œ ê°’ì„ ì£¼ëŠ” ê²ƒì´ ì¢‹ë‹¤. íŒŒë¼ë¯¸í„° ê°’ì„ ìƒ˜í”Œë§í• ë•Œ 10^-3 ~10^-6ì„ ìƒ˜í”Œë§í•˜ì§€ ë§ê³  10ì˜ ì°¨ìˆ˜ ê°’ë§Œ ìƒ˜í”Œë§í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ìµœì ì˜ ê°’ì´ ë‚´ê°€ ì •í•œ ë²”ìœ„ì˜ ì¤‘ì•™ ì¯¤ì— ìœ„ì¹˜í•˜ë„ë¡ ë²”ìœ„ë¥¼ ì˜ ì„¤ì •í•´ ì£¼ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ì‹¤ì œë¡œëŠ” grid searchë³´ë‹¤ëŠ” random searchë¥¼ í•˜ëŠ” ê²ƒì´ ë” ì¢‹ë‹¤. ì™œëƒí•˜ë©´ ì‹¤ì œë¡œëŠ” ì–´ë–¤ íŒŒë¼ë¯¸í„°ê°€ ë” ì¤‘ìš”í•  ìˆ˜ë„ ìˆëŠ”ë° ê·¸ëŸ¬í•œ important variableì—ì„œ ë” ë‹¤ì–‘í•œ ê°’ì„ ìƒ˜í”Œë§ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"[CS231n]Lecture05-Convolution Neural Network","slug":"cs231n_05","date":"2019-07-18T18:00:00.000Z","updated":"2019-12-13T03:43:27.468Z","comments":true,"path":"2019/07/19/cs231n_05/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/19/cs231n_05/","excerpt":"","text":"CNNì˜ ì—­ì‚¬ëŠ” ìƒëµí•˜ê² ë‹¤. CNNì˜ ê¸°ë³¸ì ì¸ êµ¬ì¡°ê¸°ì¡´ì˜ Fully connected Layerì™€ CNNì˜ ì£¼ëœ ì°¨ì´ì ì€ ê¸°ì¡´ì˜ ì´ë¯¸ì§€ êµ¬ì¡°ë¥¼ ë³´ì¡´ì‹œí‚¨ë‹¤ëŠ” ì ì´ë‹¤. ê·¸ë¦¬ê³  í•„í„°ê°€ ê°€ì¤‘ì¹˜ ì—­í• ì„ í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•˜ë©´ ë  ê²ƒì´ë‹¤. ê°„ë‹¨íˆ í‘œí˜„í•˜ìë©´, í•„í„°ë¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ ìŠ¬ë¼ì´ë”©í•˜ë©´ì„œ ê³µê°„ì ìœ¼ë¡œ ë‚´ì ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ì´ CNN êµ¬ì¡°ì´ë‹¤. ë” ìì„¸íˆ ë§í•˜ìë©´, ìš°ì„  í•„í„°ì˜ í¬ê¸°ëŠ” ì…ë ¥ì˜ í¬ê¸°ë³´ë‹¤ëŠ” ì‘ì§€ë§Œ í•„í„°ì˜ ê¹Šì´ëŠ” í•­ìƒ ì…ë ¥ì˜ ê¹Šì´ ë§Œí¼ í•­ìƒ í™•ì¥ë˜ì–´ì•¼ í•œë‹¤. ê·¸ëŸ¬í•œ í•„í„°ë¥¼ ì „ì²´ ì´ë¯¸ì§€ì˜ ì¼ë¶€ ê³µê°„ì— ê²¹ì³ë†“ê³  ë‚´ì ì„ ìˆ˜í–‰í•œë‹¤. í•„í„°ì˜ W(ê°€ì¤‘ì¹˜)ì™€ ì´ì— ìƒì‘í•˜ëŠ” ìœ„ì¹˜ì— ìˆëŠ” ì…ë ¥ ì´ë¯¸ì§€ì˜ í”½ì…€ì„ ê³±í•´ì¤€ë‹¤. ì´ëŸ¬í•œ ë‚´ì ì„ í•  ë•ŒëŠ” fully connected layerì˜ ì—°ì‚°ê³¼ ë™ì¼í•˜ê²Œ í•„í„° í¬ê¸°ì— ìƒì‘í•˜ëŠ” ì…ë ¥ ë°ì´í„°ì˜ í…ì„œë¥¼ í•˜ë‚˜ì˜ ê¸´ ë²¡í„°ë¡œ ë³´ê³  ë²¡í„° ë¼ë¦¬ì˜ ë‚´ì ìœ¼ë¡œ ìƒê°í•˜ë©´ ëœë‹¤. ì™œëƒí•˜ë©´ ê° ì›ì†Œë¼ë¦¬ Convolutionì„ í•˜ëŠ” ê²ƒê³¼ í…ì„œë¥¼ ì­‰ í´ì„œ ë‚´ì ì„ í•˜ëŠ” ê²ƒì€ ë™ì¼í•œ ì‘ì—…ì˜ ê²°ê³¼ë¬¼ì„ ë³´ì—¬ì£¼ê¸° ë•Œë¬¸ì´ë‹¤. ì‹ í˜¸ì²˜ë¦¬ ë¶„ì•¼ì—ì„œì˜ convolutionì€ ì‹¤ì œë¡œ í•„í„°ë¥¼ ë’¤ì§‘ì€ ë‹¤ìŒì— ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤ëŠ” ë‚´ìš©ì´ ìˆì—ˆëŠ”ë° ì´ ë¶€ë¶„ì€ ì•„ì§ ì •í™•í•œ ì˜ë¯¸ë¥¼ ì•Œì§€ ëª»í•´ ì¢€ ë” ì°¾ì•„ë³´ë©° ê³µë¶€í•œ í›„ ì¶”í›„ì— ë‹¤ì‹œ ë‚´ìš©ì„ ì—…ë°ì´íŠ¸ í•´ì•¼í•  ê²ƒ ê°™ë‹¤. ì´ì œë¶€í„°ëŠ” CNNì—ì„œì˜ í•„í„°ê°€ ì‘ë™í•˜ëŠ” ë°©ì‹ì„ ë§í•´ ë³´ë©´, Convolutionì€ ì´ë¯¸ì§€ì˜ ì¢Œìƒë‹¨ë¶€í„° ì‹œì‘í•˜ê²Œ ëœë‹¤. ê·¸ë¦¬ê³  í•„í„°ì™€ ì…ë ¥ ì´ë¯¸ì§€ ë°ì´í„°ì˜ ë‚´ì ìœ¼ë¡œì¸í•´ ê° í•´ë‹¹ ìœ„ì¹˜ì— activation mapì„ ì‚°ì¶œí•˜ê²Œ ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´ 2ì¹¸ì”© ë„ì–´ì„œ í•„í„°ë¥¼ ìŠ¬ë¼ì´ë”© í•œë‹¤ë˜ì§€ ì¶œë ¥ activation map í–‰ë ¬ì˜ í¬ê¸°ëŠ” í•„í„°ë¥¼ ì–´ë–»ê²Œ ìŠ¬ë¼ì´ë“œë¥¼ í•˜ëŠëƒì— ë”°ë¼ ë‹¤ë¥¼ ê²ƒì´ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œëŠ” í•˜ë‚˜ì”© ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤. ë³´í†µ Convolution Layerì—ì„œëŠ” ì—¬ëŸ¬ê°œì˜ í•„í„°ë¥¼ ì‚¬ìš©í•˜ëŠ”ë° ê·¸ ì´ìœ ëŠ” í•„í„°ë§ˆë‹¤ ë‹¤ë¥¸ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ì‹¶ê¸° ë•Œë¬¸ì´ë‹¤. activation mapì˜ ê°¯ìˆ˜ëŠ” ì‚¬ìš©í•˜ëŠ” í•„í„°ê°¯ìˆ˜ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤. CNN(ConvNet)layerë“¤ ì‚¬ì´ì— ë³´í†µì˜ NN ì²˜ëŸ¼ activation function, Convolution, pooling(ë’¤ì— ë” ìì„¸í•œ ì„¤ëª…ì„ í•˜ê² ì§€ë§Œ poolingì€ activation mapì˜ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ëŠ” ì—­í• ì„ í•œë‹¤.) ê°™ì€ ì²˜ë¦¬ë¥¼ í•œ í›„ ì´ëŸ¬í•œ layer êµ¬ì¡°ë¥¼ ì—¬ëŸ¬ë²ˆ ìŒ“ì•„ì¤€ë‹¤. ì´ëŸ¬í•œ ì—¬ëŸ¬ Layerë“¤ì„ ìŒ“ê³ ë‚˜ì„œ ë³´ë©´ ê²°êµ­ ê° í•„í„°ë“¤ì´ ê³„ì¸µì ìœ¼ë¡œ í•™ìŠµì„ í•˜ëŠ”ê²ƒì„ ë³´ê²Œëœë‹¤. ë³´í†µì€ CNN ì „ì²´ì˜ êµ¬ì¡°ì—ì„œ ì•ìª½ì— ìˆëŠ” í•„í„°ë“¤ì€ low-level feature(Edgeì™€ ê°™ì€)ë¥¼ í•™ìŠµí•˜ê²Œ ë˜ê³ , ì¤‘ê°„ì€ Mid-level featureì„ ê°€ì§€ê²Œ ë˜ì–´ cornorë‚˜ blobsë“±ê³¼ ê°™ì€ íŠ¹ì§•ë“¤ì´ ë³´ì¸ë‹¤. ë’¤ìª½ìœ¼ë¡œ ê°ˆìˆ˜ë¡ high-level feature(ì¢€ ë” ê°ì²´ì™€ ë‹®ì€ ê²ƒë“¤ì´ ì¶œë ¥ìœ¼ë¡œ ë‚˜ì˜¤ëŠ”)ë¥¼ í•™ìŠµí•˜ê²Œ ëœë‹¤. ì´ë ‡ê²Œ í•„í„°ì— ë”°ë¼ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ê³¼ì •ì´ ì¤‘ìš”í•˜ê¸°ì— í•„í„°ì˜ Depthë¥¼ ëŠ˜ë¦¬ëŠ” ë° ì–´ë–¤ ì§ê´€ì„ ê°€ì ¸ì•¼í•˜ëŠ”ì§€ ì˜ë¬¸ì´ ë“¤ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤. ì´ëŠ” ì–´ë–»ê²Œ ëª¨ë¸ì„ ë””ìì¸í•´ì•¼ ë˜ëŠ”ì§€ì˜ ë¬¸ì œì´ë¯€ë¡œ ì‹¤ì œë¡œëŠ” ì–´ë–¤ê²ƒì´ ë” ì¢‹ì€ì§€ë¥¼ ì°¾ì•„ë‚´ì•¼í•œë‹¤. Conv Layerë¥¼ ê³„ì¸µì ìœ¼ë¡œ ìŒ“ì•„ì„œ ë‹¨ìˆœí•œ íŠ¹ì§•ì„ ë½‘ê³  ê·¸ì„œì„ ë˜ ì¡°í•©í•´ì„œ ë” ë³µì¡í•œ íŠ¹ì§•ìœ¼ë¡œ í™œìš©í–ˆë‹¤. ì´ëŠ” ê°•ì œë¡œ í•™ìŠµì‹œí‚¨ ê²ƒì´ ì•„ë‹ˆë¼ ê³„ì¸µì  êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ê³  ì—­ì „íŒŒë¡œ í•™ìŠµì‹œí‚¨ ê²ƒ ë¿ì´ì§€ë§Œ í•„í„°ëŠ” ì´ë ‡ê²Œ í•™ìŠµë˜ëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ê°€ ê° í•„í„°ë¥¼ í†µí•´ ì–»ì€ activation mapì„ ì‹œê°í™”í•˜ë©´ ì´ë¯¸ì§€ê°€ ì–´ë–»ê²Œ ìƒê²¨ì•¼ í•´ë‹¹ ë‰´ëŸ°ì˜ í™œì„±ì„ ìµœëŒ€í™”ì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ëŠ” ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ë‹¤. ì¦‰, ì´ë¯¸ì§€ê°€ í•„í„°ì™€ ë¹„ìŠ·í•˜ê²Œ ìƒê²¼ìœ¼ë©´ ì¶œë ¥ê°’ì´ ì»¤ì§€ê²Œ ëœë‹¤! ê·¸ëŸ¬ë¯€ë¡œ ìš°ë¦¬ëŠ” ì´ëŸ° ì‹œê°í™”ë¥¼ í†µí•´ ì–´ë–¤ í•„í„°ì— ëŒ€í•œ activation mapì¸ì§€ë¥¼ ì—­ìœ¼ë¡œ ì¶”ë¡ í•´ ë³¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ë§ˆì¹˜ ì´ë¯¸ì§€ ì¤‘ ì–´ëŠ ìœ„ì¹˜ì—ì„œ í•„í„°ê°€ í¬ê²Œ ë°˜ì‘í•˜ëŠ” ì§€ë¥¼ ë³¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.ì‹œê°í™”ëŠ” Backpropagationì„ í†µí•´ í•´ë³¼ ìˆ˜ ìˆë‹¤. ë³´í†µ í•„í„°ì˜ ê°¯ìˆ˜ëŠ” 2ì˜ ì œê³±ìˆ˜ë¡œ í•œë‹¤. ex) 32, 64, 128, 512ë“± Spatial dimensionë¨¼ì €, í•œê°€ì§€ ì£¼ì˜í•  ì ì€ ìš°ë¦¬ì˜ ì…ë ¥ë°ì´í„°ë¥¼ ì˜ ì»¤ë²„í•  ìˆ˜ ì—†ëŠ” í•„í„° ì‚¬ì´ì¦ˆë¡œ CNN êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ê²Œ ë˜ë©´, ê·¸ë¡œì¸í•œ ë¶ˆê· í˜•í•œ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— ì˜ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤ëŠ” ë¬¸ì œê°€ ìˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ í•„í„°ì˜ ì‚¬ì´ì¦ˆë¥¼ ì •í• ë–„ëŠ” ì…ë ¥ë°ì´í„°ë¥¼ ì»¤ë²„í•  ìˆ˜ ìˆëŠ” ì‚¬ì´ì¦ˆë¡œ ì •í•´ì£¼ì–´ì•¼ í•  ê²ƒì´ë‹¤. ìš°ë¦¬ê°€ í•„í„°ë¥¼ ìŠ¬ë¼ì´ë”©í•œ í›„ì˜ activation mapì˜ ì‚¬ì´ì¦ˆ(ê°€ë¡œ,ì„¸ë¡œ í¬ê¸°)ëŠ” (ì›ë˜ image dimension - í•„í„° dimension)/stride + 1 ì´ë¥¼ ì´ìš©í•´ì„œ ì–´ë–¤ í•„í„° í¬ê¸°ë¥¼ ì‚¬ìš©í•´ì•¼í•˜ëŠ” ì§€ë¥¼ ì•Œ ìˆ˜ê°€ ìˆë‹¤. ì–´ë–¤ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ ì´ë¯¸ì§€ì— ê¼­ ë§ëŠ”ì§€, ê·¸ë¦¬ê³  ëª‡ ê°œì˜ ì¶œë ¥ê°’ì„ ë‚¼ ìˆ˜ ìˆëŠ”ì§€ë„ ì•Œ ìˆ˜ ìˆë‹¤. Paddingê°€ì¥ í”íˆ ì“°ëŠ” ê¸°ë²•ì€ zero-padì´ë‹¤. ì…ë ¥ image dataì˜ ì½”ë„ˆ ì¦‰ ê°€ì¥ìë¦¬ëŠ” í•„í„°ë¥¼ ì ìš©í•  ë•Œ í•„í„°ì˜ ì¤‘ì‹¬ë³´ë‹¤ëŠ” ëœ ì˜í–¥ì„ ì£¼ì§„ ì•Šì„ì§€ì˜ ì˜ë¬¸ì„ ê°€ì§ˆ ìˆ˜ë„ ìˆì„ ê²ƒì´ë‹¤. ê·¸ëŸ¬í•œ ì˜ë¬¸ì„ í•´ì†Œí•˜ê¸° ìœ„í•´ zero paddingì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ë‹¤ìŒ ê·¸ë¦¼ì²˜ëŸ¼, ê°€ì¥ìë¦¬ì— 0ì„ ì±„ì›Œ ë«ë¶™ì—¬ ë„£ëŠ” ê²ƒì´ë‹¤. ê·¸ë ‡ê²Œí•˜ë©´ ì¢Œìƒë‹¨ì˜ ìë¦¬ì—ì„œë„ í•„í„° ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ ëœë‹¤. í˜¹ì‹œ Paddingì„ ëª¨ë¥´ëŠ” ì‚¬ëŒë“¤ì„ ìœ„í•´ ë§ì„ í•˜ìë©´, ì˜ˆë¥¼ë“¤ì–´ image dataì˜ ê°€ì¥ìë¦¬ì— í”½ì…€ì„ ë«ë¶™ì—¬ì£¼ëŠ” ì‘ì—…ì´ë‹¤. zero paddingì„ í•˜ë©´ ëª¨ì„œë¦¬ì— í•„ìš”ì—†ëŠ” íŠ¹ì§•ì„ ì¶”ê°€í•˜ê²Œ ë˜ëŠ” ê²ƒì€ ì•„ë‹Œì§€ë¼ëŠ” ì˜ë¬¸ì„ ê°–ì„ ìˆ˜ ìˆëŠ”ë°, ìš°ë¦¬ì˜ ë³¸ ëª©ì ì€ ì´ë¯¸ì§€ë‚˜ ì˜ìƒ ë‚´ì—ì„œ ì–´ë–¤ edge ë¶€ë¶„ì˜ ìœ„ì¹˜(ê°’)ì„ ì–»ê³  ì‹¶ì€ ê²ƒì´ê³ , zero-paddingì€ ì´ë¥¼ í•  ìˆ˜ ìˆëŠ” í•˜ë‚˜ì˜ ë°©ë²•ì¼ ë¿ì´ë¼ëŠ” ê²ƒì„ ëª…ì‹¬í•˜ì! ì™œëƒí•˜ë©´ ìš°ë¦¬ëŠ” ì§€ê¸ˆ í•„í„°ê°€ ë‹¿ì§€ ì•ŠëŠ” ëª¨ì„œë¦¬ ë¶€ë¶„ì—ì„œë„ ê°’ì„ ë½‘ì„ ìˆ˜ ìˆê²Œ ë˜ê¸° ë•Œë¬¸ì´ë‹¤. (zeroê°€ ì•„ë‹Œ mirrorë‚˜ extendí•˜ëŠ” ë°©ë²•ë„ ìˆë‹¤!! í—ˆë‚˜, zero-padding ì œë²• ì˜ ë™ì‘í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤.) ë¬¼ë¡  ëª¨ì„œë¦¬ ë¶€ë¶„ì— ì•½ê°„ì˜ artifactê°€ ìƒê¸¸ ìˆœ ìˆë‹¤. ë‹¹ì—°íˆ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ë¶€ë¶„ì´ë‹¤. zero-paddingì„ í•˜ëŠ” ë˜ ë‹¤ë¥¸ ì´ìœ ëŠ” Layerë¥¼ ê±°ì¹˜ë©´ì„œë„ ì…ë ¥ì˜ ì‚¬ì´ì¦ˆë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. paddingì„ í•´ì¤Œìœ¼ë¡œì„œ ì»¬ëŸ¼ì´ ë‘ê°œê°€ ë” ìƒì„±ë˜ì–´ ê²°êµ­ì—ëŠ” ì…ë ¥ë°›ì€ ë°ì´í„°ì˜ í¬ê¸°ì™€ ë™ì¼í•œ ì¶œë ¥ í¬ê¸°ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ì•ì„œ ë°°ìš´ ê³µì‹ì— ì ìš©ì‹œì¼œ ìƒê°í•´ë³´ë©´ ì´í•´ê°€ ê°ˆ ê²ƒì´ë‹¤. ì¦‰, ìš”ì•½í•˜ë©´ Paddingì„ í•˜ê²Œë˜ë©´ ì¶œë ¥ ì‚¬ì´ì¦ˆë¥¼ ìœ ì§€ì‹œì¼œì£¼ê³  í•„í„°ì˜ ì¤‘ì•™ì´ ë‹¿ì§€ ì•ŠëŠ” ê³³ë„ ì—°ì‚°í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë‹¤ìŒì€ ë‚´ê°€ ê°•ì˜ë¥¼ ë“¤ìœ¼ë©´ì„œ ì ‘í–ˆë˜ ì§ˆë¬¸ ì¤‘ ë‚˜ ë˜í•œ ê¶ê¸ˆí–ˆë˜ ì§ˆë¬¸ì´ë‹¤. ë§Œì•½ ì´ë¯¸ì§€ê°€ square matrixê°€ ì•„ë‹ˆê³  rectangular matrixë¼ë©´ ìˆ˜í‰, ìˆ˜ì§ë°©í–¥ì˜ strideë¥¼ ë‹¤ë¥´ê²Œ ì ìš©í•´ì•¼ í•˜ëŠ”ì§€ ì´ë‹¤. ì´ëŸ° ì§ˆë¬¸ì€ ë‚˜ì™ ê°™ì€ ì…ë¬¸ìë“¤ì—ê²ŒëŠ” ê¶ê¸ˆí•  ë§Œí•œ ì§ˆë¬¸ì´ë¼ê³  ìƒê°í•œë‹¤. ë‹´ì€ ë¬¼ë¡  ê°€ëŠ¥í•˜ì§€ë§Œ, ë³´í†µì€ square matrixë¡œ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•œí›„ ì‚¬ìš©í•˜ì—¬ ê°™ì€ strideë¥¼ ì ìš©í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ë¯¸ì§€ í•´ìƒë„ ë¬¸ì œì™€ë„ ê´€ë ¨ì´ ìˆë‹¤. ì•„ë§ˆ ìš°ë¦¬ëŠ” ì…ë ¥ ì´ë¯¸ì§€ ì›ë³¸ ìƒíƒœì˜ ë¹„ìœ¨ì„ ìœ ì§€í•˜ê³  ì‹¶ê² ì§€ë§Œ ëŒ€ë¶€ë¶„ì€ ì •ì‚¬ê°í˜•ìœ¼ë¡œ ì˜ë¼ì„œ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì–´ë–¤ strideì™€ filterë¥¼ ì“¸ê±´ì§€ë¥¼ ì •í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. filter size stride 3x3 1 5x5 2 7x7 3 ë§Œì•½ Layerê°€ ì—¬ëŸ¬ê²¹ ìŒ“ì¸ êµ¬ì¡°ë¥¼ ì„¤ê³„í•œë‹¤ê³  ê°€ì •í–ˆì„ë•Œ, zero-paddingì„ í•˜ì§€ ì•ŠëŠ” ë‹¤ë©´ ì¶œë ¥ ì‚¬ì´ì¦ˆëŠ” ì•„ì£¼ ë¹ ë¥´ê²Œ ì¤„ì–´ ë“¤ê²Œ ë  ê²ƒì´ë‹¤. ê·¸ ì˜ë¯¸ëŠ” ê²°êµ­ ì¼ë¶€ ì •ë³´ë¥¼ ìƒê²Œ ë˜ëŠ” ê²ƒì´ê³  ì›ë³¸ ì´ë¯¸ì§€ë¥¼ í‘œí˜„í•˜ê¸°ì— ë„ˆë¬´ ì‘ì€ ê°’ì„ ì‚¬ìš©í•˜ê²Œ ë  ê²ƒì´ë‹¤. ë˜í•œ, ê·¸ ì´ìœ ëŠ” ë§¤ë²ˆ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ convolutionì„ í•  ë•Œ ê° ì½”ë„ˆì— ìˆëŠ” ê°’ë“¤ì„ ê³„ì‚°í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë³´ì. input volumeì´ 32x32x3ì´ê³ , 10ê°œì˜ 5x5 í•„í„°ë“¤ì„ stride 1, paddingì„ 2ë¡œ ì£¼ì—ˆì„ ê²½ìš°, output volume sizeëŠ” ì–´ë–»ê²Œ ë  ê²ƒì¸ê°€? ì •ë‹µì€ 32x32x10ì´ë‹¤.(32+(2x2)-5/1)+1=32ì´ê³  í•„í„°ì˜ ê°¯ìˆ˜ê°€ 10ì´ë¯€ë¡œ) ê·¸ë ‡ë‹¤ë©´ ì´ Layerì˜ parameterëŠ” ì´ ëª‡ê°œ ì¼ê¹Œ? ì •ë‹µì€ 5x5x3=75ê°œë¼ëŠ” ê°€ì¤‘ì¹˜ì™€ biasì¸ 1ê°œë¥¼ ë”í•œ 75+1=76ê°œë¥¼ ê° í•„í„°ë‹¹ ê°€ì§€ë¯€ë¡œ ì´ 10ê°œì˜ í•„í„°ê°€ ìˆê¸°ì— 76*10=760ê°œì´ë‹¤. 1x1 Convolutionë„ ì˜ë¯¸ê°€ ìˆë‹¤. í•„í„°ì˜ í¬ê¸°ê°€ 1ì´ˆê³¼ì¸ ë‹¤ë¥¸ í•„í„°ë“¤ ì²˜ëŸ¼ ê³µê°„ì ì¸ ì •ë³´ë¥¼ ì´ìš©í•˜ì§„ ì•Šì§€ë§Œ ì—¬ì „íˆ Depthë§Œí¼ ì—°ì‚°ì„ ìˆ˜í–‰í•œë‹¤. ê·¸ëŸ¬ë‹ˆ 1x1 ConvNetëŠ” ì…ë ¥ì˜ ì „ì²´ Depthì— ëŒ€í•œ ë‚´ì ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. &#39;strideë¥¼ ì„ íƒí•˜ëŠ” ë° ìˆì–´ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ì§ê´€ì€ ë¬´ì—‡ì¸ê°€&#39;ë¼ëŠ” ì˜ë¬¸ì— strideë¥¼ í¬ê²Œ ê°€ì ¸ê°ˆìˆ˜ë¡ ì¶œë ¥ì€ ì ì  ì‘ì•„ì§ˆ ê²ƒì´ë¼ëŠ” ê²ƒì´ë‹¤. ì¦‰, ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ìƒ˜í”Œë§í•˜ëŠ” ê²ƒì¸ë° Poolingì„ í•˜ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•˜ë‹¤. ì—„ë°€íˆ ë§í•˜ìë©´ ë‘˜ì€ ë‹¤ë¥¸ ì–˜ê¸°ì´ë©°, Pooling ë³´ë‹¤ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ê¸°ë„ í•œë‹¤. Pooling ì²˜ëŸ¼ ë‹¤ìš´ ìƒ˜í”Œë§í•˜ëŠ” ë™ì¼í•œ íš¨ê³¼ë¥¼ ì–»ìœ¼ë©´ì„œë„ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ë„ ìˆë‹¤. ê·¸ë¦¬ê³  activation mapì˜ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ëŠ” ê²ƒì€ ì¶”í›„ ëª¨ë¸ì˜ ì „ì²´ íŒŒë¼ë¯¸í„°ì˜ ê°¯ìˆ˜ì—ë„ ì˜í–¥ì„ ë¯¸ì¹œë‹¤. ì™œëƒí•˜ë©´ ì¶œë ¥ ë…¸ë“œ ì „ ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ Fully connected Layerë¥¼ ë³´ê²Œ ë˜ë©´ Convì˜ ì¶œë ¥ ëª¨ë‘ì™€ ì—°ê²°ë˜ì–´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰, Conv Layerì˜ ì¶œë ¥ì´ ì‘ì„ìˆ˜ë¡ Fully connected Layerì—ì„œ í•„ìš”í•œ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ê°€ ë” ì‘ì„ ê²ƒì´ë¼ëŠ” ì ì´ë‹¤. íŒŒë¼ë¯¸í„°ì˜ ìˆ˜, ëª¨ë¸ì˜ ì‚¬ì´ì¦ˆ, ê·¸ë¦¬ê³  Overfitting ê³¼ ê°™ì€ ê²ƒë“¤ì—ëŠ” ë‹¤ì–‘í•œ trade-offê°€ ìˆë‹¤. ì´ëŸ¬í•œ trade-offëŠ” strideë¥¼ ëª‡ ìœ¼ë¡œ í• ì§€ë¥¼ ê²°ì •í• ë•Œ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ë¬¸ì œì´ë‹¤. Conv Layerë¥¼ Brain Neuronì˜ ê´€ì ì—ì„œ ì‚´í´ë³´ì. ë‰´ë ¨ê³¼ Conv Layerì˜ ê°€ì¥ í° ì°¨ì´ì ì€ ìš°ë¦¬ì˜ ë‰´ëŸ°ì€ Local connectivityë¥¼ ê°€ì§€ê³  ìˆë‹¤ëŠ” ì ì´ë‹¤. Conv Layerì²˜ëŸ¼ ìŠ¬ë¼ì´ë”©ì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ íŠ¹ì • ë¶€ë¶„ì—ë§Œ ì—°ê²°ë˜ì–´ ìˆë‹¤. í•˜ë‚˜ì˜ ë‰´ëŸ°ì€ í•œ ë¶€ë¶„ë§Œ ì²˜ë¦¬í•˜ê³ , ê·¸ëŸ° ë‰´ëŸ°ë“¤ì´ ëª¨ì—¬ì„œ ì „ì²´ ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ë‹¤. ì´ëŸ° ì‹ìœ¼ë¡œ spatial structureë¥¼ ìœ ì§€í•œ ì±„ë¡œ Layerì˜ ì¶œë ¥ì¸ activation mapì„ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ë˜í•œ, í•„í„°ëŠ” ì´ë¯¸ì§€ì—ì„œ ê°™ì€ ì§€ì—­ì„ ëŒë”ë¼ë„ ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì§•ì„ ë½‘ì•„ë‚¸ë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. Pooling layerPooling LayerëŠ” Representationë“¤ì„ ë” ì‘ê³  ê´€ë¦¬í•˜ê¸° ì‰½ê²Œ í•´ì¤€ë‹¤. ì¦‰ íŒŒë¼ë¯¸í„°ì˜ ìˆ˜ë¥¼ ì¤„ì¸ë‹¤ëŠ” ê²ƒì´ë‹¤.ê·¸ë¦¬ê³  ì¼ì¢…ì˜ ê³µê°„ì ì¸ ë¶ˆë³€ì„±ì„ ì–»ì„ ìˆ˜ ë„ ìˆë‹¤. ê²°êµ­ Pooling Layerë¥¼ ê´€ë¦¬í•˜ëŠ” ì¼ì€ Downsamplingí•˜ëŠ” ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ì ì€ Depthì—ëŠ” ì•„ë¬´ëŸ° ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë¦¬ê³  ì¼ë°˜ì ìœ¼ë¡œ Max poolingì´ ì‚¬ìš©ëœë‹¤. Poolingì—ë„ í•„í„° í¬ê¸°ë¥¼ ì •í•  ìˆ˜ ìˆë‹¤. ì–¼ë§ˆë§Œí¼ì˜ ì˜ì—­ì„ í•œ ë²ˆì— ë¬¶ì„ì§€ë¥¼ ì •í•˜ëŠ” ê²ƒì´ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ Downsamplingì„ í•˜ê³  ì‹¶ì€ ê²ƒì´ê¸° ë•Œë¬¸ì— Poolingì„ í•  ë•ŒëŠ” ê²¹ì¹˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¼ë°˜ì ì´ë‹¤. ë˜í•œ, ìš°ë¦¬ê°€ ë‹¤ë£¨ëŠ” ê°’ë“¤ì€, ì–¼ë§ˆë‚˜ ì´ ë‰´ëŸ°ì´ í™œì„±í™”ë˜ì—ˆëŠ” ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì´ë‹¤. ì¦‰, ì´ í•„í„°ê°€ ê° ìœ„ì¹˜ì—ì„œ ì–¼ë§ˆë‚˜ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ ì´ë‹¤. Max poolingì€ ê·¸ ì§€ì—­ì´ ì–´ë””ë“ , ì–´ë–¤ ì‹ í˜¸ì— ëŒ€í•´ â€œì–¼ë§ˆë‚˜â€ ê·¸ í•„í„°ê°€ í™œì„±í™” ë˜ì—ˆëŠ”ì§€ë¥¼ ì•Œë ¤ì¤€ë‹¤ê³  ì•Œ ìˆ˜ ìˆë‹¤. \bì¸ì‹ì— ëŒ€í•´ ìƒê°í•´ ë³´ì•˜ì„ë•Œ ì¸ì‹ì€ ê·¸ ê°’ì´ ì–´ë””ì— ìˆì—ˆë‹¤ëŠ” ê²ƒ ë³´ë‹¤ëŠ” ê·¸ ê°’ì´ ì–¼ë§ˆë‚˜ í°ì§€ê°€ ì¤‘ìš”í•œ ê²ƒì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ average pooling ë³´ë‹¤ ë” ì¢‹ë‹¤. Poolingë„ ì¼ì¢…ì˜ stride ê¸°ë²•ì´ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤. ì‚¬ëŒë“¤ì€ Downsamplingì„ í• ë•Œ Poolingì„ í•˜ê¸°ë³´ë‹¨ strideë¥¼ ë§ì´ ì‚¬ìš©í•˜ê³  ìˆëŠ” ì¶”ì„¸ì´ê³  ì„±ëŠ¥ë„ ì¢‹ë‹¤. Pooling Layerë„ Conv Layerì—ì„œ ì‚¬ìš©í–ˆë˜ ìˆ˜ì‹ì„ ê·¸ëŒ€ë¡œ ì´ìš©í•´ì„œ Design choiceë¥¼ í•  ìˆ˜ ìˆë‹¤.í•œê°€ì§€ íŠ¹ì§•ì´ ìˆë‹¤ë©´ pooling layerì—ì„œëŠ” ë³´í†µ paddingì„ í•˜ì§€ ì•ŠëŠ”ë‹¤. paddingì˜ ëª©ì ì€ ì£¼ë¡œ ì‚¬ì´ì¦ˆë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ê²ƒì´ì§€ë§Œ poolingì˜ ëª©ì ì€ down samplingì´ê³  Conv layerì—ì„œ ì²˜ëŸ¼ ì½”ë„ˆì˜ ê°’ì„ ê³„ì‚°í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ë„ ì—†ê¸° ë•Œë¬¸ì´ë‹¤. filter size stride 2x2 2 3x3 2","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"[CS231n]Lecture02-Image classification pipeline","slug":"cs231n_02","date":"2019-07-18T07:00:00.000Z","updated":"2019-12-08T05:34:11.115Z","comments":true,"path":"2019/07/18/cs231n_02/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/18/cs231n_02/","excerpt":"","text":"ê¸°ë³¸ì ìœ¼ë¡œ Computer visionì—ì„œ ê°€ì¥ í•µì‹¬ì´ ë  ìˆ˜ ìˆëŠ” ì‘ì—…ì€ Image Classificationì´ë¼ê³  í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. Image classificationì´ ê°€ëŠ¥í•˜ë©´ detection, segmentation, captioning ì‘ì—…ë“¤ì´ ìˆ˜ì›”í•˜ê²Œ ì‘ì—…ì´ ê°€ëŠ¥í•´ì§„ë‹¤. ì´ë¯¸ì§€ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ 3ì°¨ì›ì˜ ë°°ì—´ í˜•íƒœë¡œ [0, 255] ì‚¬ì´ì˜ ìˆ˜ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. 3ì°¨ì›ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì´ë¯¸ì§€ê°€ ì»¬ëŸ¬ì¸ ê²½ìš°ì´ë©°, ê°ê°ì˜ ì°¨ì›ì€ RGBì±„ë„ë¡œ ë¶ˆë¦°ë‹¤. ì´ë¯¸ì§€ëŠ” ì¹´ë©”ë¼ ì•µê¸€ì´ë¼ë˜ì§€ ë³´ëŠ” ì‹œê°ì— ë”°ë¼ ë‹¬ë¦¬ ë³´ì´ë©°, ë°ê¸°ë„ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë©°, í˜•íƒœì˜ ë³€í˜•, ê·¸ë¦¬ê³  ì€í, ì€ë‹‰ ë“±ë“± ì—¬ëŸ¬ ë¬¸ì œë“¤ì´ ì¡´ì¬í•œë‹¤. ê·¸ë ‡ë‹¤ë©´, ìš°ë¦¬ëŠ” ì´ëŸ° ë¬¸ì œë“¤ì„ ê°€ì§„ ìƒíƒœì—ì„œ classificationì„ í•˜ëŠ”ë° ìˆì–´ì„œ ìˆ«ìë¥¼ ì •ë ¬í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ê³¼ ê°™ì€ ëª…ë°±í•œ ì•Œê³ ë¦¬ì¦˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ ë¬¸ì œì ìœ¼ë¡œ ë– ì˜¤ë¥¼ ê²ƒì´ë‹¤. ë¬¼ë¡  ì´ë¯¸ì§€ë¥¼ ë³´ê³  ì´ë¯¸ì§€ì˜ edge, chunkë¥¼ ì°¾ì•„ì„œ libraryí™” í•˜ê³  ì´ë¯¸ì§€ê°€ ì–´ë–»ê²Œ ë°°ì—´ë˜ì–´ ìˆëŠ”ì§€ì™€ ê°™ì€ featureë“¤ì„ ê°€ì§€ê³  ë“¤ì–´ì˜¨ ì´ë¯¸ì§€ë¥¼ classificationí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•´ì™”ì§€ë§Œ, ì´ ì •ë„ë§Œìœ¼ë¡œëŠ” ì—­ë¶€ì¡±ì´ì—ˆë‹¤. ì´ì œëŠ”, ë°ì´í„° ê¸°ë°˜ì˜ ì ‘ê·¼ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤. - 1) Labeling ë˜ì–´ìˆëŠ” dataë¥¼ ì¤€ë¹„í•œë‹¤. - 2) Image classiferë¥¼ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•´ Machine Learningì„ ì‚¬ìš©í•œë‹¤. - 3) test image setì„ í™œìš©í•´ì„œ classifierë¥¼ í‰ê°€í•œë‹¤. ë¨¼ì € ì§€ê¸ˆì€ ì˜ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” Nearest Neighbor Classifierë¥¼ ì„¤ëª…í•´ ë³¼ ê²ƒì´ë‹¤.ë°©ë²•ì€ ê°„ë‹¨í•˜ë‹¤. ë¨¼ì € ëª¨ë“  train dataì™€ ê° image dataì˜ labelì„ ì½ì–´ ë©”ëª¨ë¦¬ìƒì— ê¸°ì–µí•˜ê²Œ í•´ë†“ëŠ”ë‹¤.ê·¸ëŸ° í›„ì— ê°€ì¥ ë¹„ìŠ·í•œ train imageì˜ labelìœ¼ë¡œ ì˜ˆì¸¡í•˜ê²Œë” í•˜ëŠ” ë°©ë²•ì´ë‹¤. ìœ„ì—ì„œ ë§í•œ ê°€ì¥ ë¹„ìŠ·í•œ imageë¼ëŠ” ê²ƒì€ ì†Œìœ„ ìƒê°í•´ë³´ë©´ ë°ì¹¼ì½”ë§ˆë‹ˆ ê°™ì´ ì„œë¡œ ê²¹ì³ ë†“ì•˜ì„ë•Œ ê°™ê²Œ ë˜ë©´ ë‘ imageëŠ” ë™ì¼í•œ imageë¼ê³  ìƒê°í•˜ê¸° ë•Œë¬¸ì— ê°ê°ì˜ ë™ì¼í•œ ìœ„ì¹˜ì˜ í”½ì…€ê°’ì„ ë¹¼ ì ˆëŒ€ê°’ì˜ í•©ì„ ì·¨í•˜ëŠ” L1 normì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ê³¼ ConvNet(CNN)ì€ dataê°€ ì—„ì²­ ë§ì•„ì•¼ í•™ìŠµ ì‹œí‚¬ ìˆ˜ ìˆë‹¤ë¼ëŠ” ìƒê°ì€ ì˜ëª»ëœ ìƒê°ì¼ ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´ ìš°ë¦¬ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì´ ì•„ë‹Œ í•™ìŠµë˜ì–´ ìˆëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì ¸ì™€ ì“°ëŠ” Fine Tuneì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤.","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"growth_hacking_01","slug":"growth_hacking_01","date":"2019-05-17T23:38:10.000Z","updated":"2019-07-20T03:36:16.967Z","comments":true,"path":"2019/05/18/growth_hacking_01/","link":"","permalink":"https://heung-bae-lee.github.io/2019/05/18/growth_hacking_01/","excerpt":"","text":"ê²°êµ­ ë¬¸ì œëŠ” Distribution ëª¨ë°”ì¼ ì•± ì‹œì¥ì„ ìƒê°í•´ ë³´ë©´ ì•Œìˆ˜ ìˆë“¯ì´ ì´ˆì°½ê¸°ì—ëŠ” ì‹ ê¸°í•´ì„œ ì•±ì„ ë‹¤ìš´ë¡œë“œí•˜ë˜ ì‹œì •ì´ ìˆì—ˆì§€ë§Œ, í˜„ì¬ëŠ” ì•±ì„ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•ŠëŠ” íŠ¸ë Œë“œë¥¼ ì•Œ ìˆ˜ ìˆë‹¤. í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ì˜ ì„±ì¥ê³¼ SaaSì˜ ì„±ì¥ìœ¼ë¡œ ì¸ë˜ ë§ì€ ìŠ¤íƒ€íŠ¸ì—…ì´ ìƒê²¼ì§€ë§Œ, ê·¸ ì¤‘ ëŒ€ë¶€ë¶„ì´ ì‹¤íŒ¨ë¥¼ í•œë‹¤. ê·¸ ì´ìœ ëŠ” ìœ í†µë•Œë¬¸ì´ë‹¤. ì¦‰, ë¹ˆì•½í•œ ìœ í†µì±„ë„ì„ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ëŸ¬í•œ ëŒ€í‘œì ì¸ ì‚¬ë¡€ë¡œ TiVoë¥¼ ê¼½ì„ ìˆ˜ ìˆë‹¤. ì¦‰, ì»¨í…ì¸ ê°€ ìˆì–´ì•¼ ë””ì§€í„¸ ë¹„ë””ì˜¤ ë ˆì½”ë“œ ìƒí’ˆì„ íŒë§¤í•˜ëŠ” ê²ƒì´ ì˜ë¯¸ìˆëŠ” ê²ƒì¸ë°, ì»¨í…ì¸ ë“¤ì€ ëª¨ë‘ ì¼€ì´ë¸” TV ì‚¬ì—…ìì™€ ìœ„ì„± TV ì‚¬ì—…ìê°€ ì»¨í…ì¸  ìœ í†µì„ ì¥ê³ ìˆê¸° ë•Œë¬¸ì— ìœ í†µì´ ì–´ë ¤ì› ë‹¤. ê²°êµ­ TivoëŠ” Patent Troll(íŠ¹í—ˆ ì „ìŸ)ë¡œ ì „ë½í•˜ì˜€ë‹¤. ì„±ê³µì ì¸ ê¸°ìˆ  ê¸°ì—…ì˜ ì¼ë°˜ì ì¸ ëª¨ë¸ì€, ì œí’ˆ ì¤‘ì‹¬ê¸°ì—…ì´ ì•„ë‹ˆë¼ ìœ í†µ ì¤‘ì‹¬ ê¸°ì—…ì´ ë˜ëŠ” ê²ƒì´ë‹¤. êµ¬ê¸€, í˜ì´ìŠ¤ë¶ë“±ì˜ ê¸°ì—…ì€ ê·¸ ìì²´ê°€ ìœ í†µ í”Œë«í¼ì´ ë˜ì–´, ê³„ì†í•´ì„œ ìƒˆë¡œìš´ ì œí’ˆì„ ê³ ê°ë“¤ì—ê²Œ ì„ ë³´ì´ê³  ìˆë‹¤. í—ˆë‚˜ ìœ„ì˜ ìœ í†µ ì¤‘ì‹¬ì˜ ê¸°ì—…ìœ¼ë¡œ ì„±ì¥í•˜ê¸°ê°€ ì–´ë ¤ìš´ í˜„ì‹¤ì´ ë˜ì—ˆë‹¤. ì‹œì¥ì— ë§ì€ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ê°€ ë‚˜ì™€ìˆìœ¼ë©° ì†Œë¹„ìì™€ ê³ ê°ë“¤ì´ ê°€ì§„ ì‹œê°„ê³¼ ëˆì€ í•œì •ë˜ì–´ìˆìœ¼ë¯€ë¡œ ëª¨ë“  ì œí’ˆê³¼ ì„œë¹„ìŠ¤ê°€ ê³ ê°ë“¤ì—ê²Œ ë„ë‹¬í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë˜í•œ, í˜ì´ìŠ¤ë¶, ì¸ìŠ¤íƒ€ê·¸ë¨ ë“± ê´‘ê³  í”Œë«í¼ì˜ ì§€ë©´ì—ëŠ” í•œê³„ê°€ ìˆì§€ë§Œ, ê´‘ê³ í•˜ê³  ì‹¶ì€ ê¸°ì—…ì€ ì ì  ëŠ˜ì–´ë‚˜ê¸° ë•Œë¬¸ì— ê´‘ê³ ë¹„ëŠ” ìƒìŠ¹í•œë‹¤. ì‚¬ëŒë“¤ì€ ì ì  ê´‘ê³ ë¥¼ í´ë¦­í•˜ì§€ ì•Šìœ¼ë©°, ì‚¬ëŒë“¤ì´ ê¸°ì—…ì˜ ë§ˆì¼€íŒ… í™œë™ì— ì ì‘í•˜ê³  í”¼ë¡œê°ì„ ëŠë¼ê³ ìˆëŠ” ìƒí™©ì€ ë”ìš± ë” ìœ í†µì´ ì´ë£¨ì–´ì§€ê¸° í˜ë“¤ê²Œ í•˜ê³  ìˆë‹¤.","categories":[{"name":"growth hacking","slug":"growth-hacking","permalink":"https://heung-bae-lee.github.io/categories/growth-hacking/"}],"tags":[]},{"title":"growth_hacking_00","slug":"growth_hacking_00","date":"2019-05-17T22:36:16.000Z","updated":"2019-07-20T03:36:19.296Z","comments":true,"path":"2019/05/18/growth_hacking_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/05/18/growth_hacking_00/","excerpt":"","text":"Growth hacking outlineStart up? ì—ì–´ë¹„ì•¤ë¹„, ìš°ë²„, ë¹„ë°”ë¦¬í¼ë¸”ë¦¬ì¹´(í† ìŠ¤), ìš°ì•„í•œí˜•ì œë“¤(ë°°ë‹¬ì˜ ë¯¼ì¡±), VCNCì˜ íƒ€ë‹¤ ë“±ì˜ íšŒì‚¬ í™•ì¥ê°€ëŠ¥í•˜ê³  ë°˜ë³µê°€ëŠ¥í•œ ì‚¬ì—…ëª¨ë¸ì„ íƒìƒ‰í•˜ê¸° ìœ„í•´ ê³ ì•ˆë˜ì–´ì§„ ì„ì‹œì¡°ì§ ì•„ì§ ì˜ ì‘ë™í•˜ëŠ” ì‚¬ì—… ëª¨ë¸ì„ ì°¾ì•„ë‚´ì§€ ëª»í•œ ê¸°ì—…ì„ ì˜ë¯¸í•˜ë©°, ì‚¬ì—… ëª¨ë¸ì´ ì˜ ì‘ë™í•œë‹¤ë©´ Established companyë¼ê³  ë¶€ë¥¼ ìˆ˜ ìˆë‹¤. ìŠ¤íƒ€íŠ¸ì—…ì˜ ìˆ™ëª…ì€ ê²°êµ­ ì‚¬ì—… ëª¨ë¸ì„ ì°¾ê¸° ì „ê¹Œì§€ ì—¬ëŸ¬ ê°€ì§€ ì‹œë„ë¥¼ í•˜ê³  ì‹œí–‰ì°©ì˜¤ë¥¼ ê²ªì„ ìˆ˜ ë°–ì— ì—†ë‹¤! ìŠ¤íƒ€íŠ¸ì—…ì˜ íŠ¹ì§• í•œì •ëœ ìì›(Runway) ex) ì œí•œëœ ëˆ Runwayì˜ ëì— ë„ë‹¬í•˜ê¸° ì „ì— í•´ì•¼í•  ì¼ ì‹œì¥, ê³ ê° ë“±ì— ëŒ€í•œ ê°€ì„¤ì„ í™•ì¸í•˜ë©° ê³ ê° í™•ë³´ ë§¤ì¶œ, í›„ì† íˆ¬ì ìœ ì¹˜ ë“± ì´ëŸ° ìŠ¤íƒ€íŠ¸ì—…ê³¼ ê°€ì„¤ê²€ì¦ì˜ ê´€ê³„ëŠ” ë¬´ì—‡ì¼ê¹Œ? - ê°€ì„¤ì€, íŒ€ì´ ê°€ì§„ ê²½í—˜ì´ë‚˜ ì§ê´€ì„ ê¸°ë°˜ìœ¼ë¡œí•œ í•™ìŠµëœ ì¶”ì¸¡ì´ë‹¤. ê²°êµ­ ìŠ¤íƒ€íŠ¸ì—…ì€ ëŠì„ì—†ì´ ê°€ì„¤ì„ ê²€ì¦í•´ì•¼í•˜ëŠ” ì¡°ì§ì´ë‹¤!! ê·¸ëŸ¬ë¯€ë¡œ, ìŠ¤íƒ€íŠ¸ì—…ì€ ë¹ ë¥¸ ì†ë„ë¡œ(Leaní•˜ê²Œ) ê°€ì„¤ì„ ê²€ì¦í•´ì•¼ í•œë‹¤. ì™œëƒí•˜ë©´ ë§ì€ ê°€ì„¤ë“¤ì„ ì£¼ì–´ì§„ ìì›ë‚´ì— í•´ê²°í•˜ì—¬ ì´ìµì„ ì°½ì¶œí•´ì•¼í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë¡œìŠ¤ ì—­ì‹œ, ëŠì„ ì—†ì´ ê°€ì„¤ì„ ì„¸ìš°ê³  ê²€ì¦í•˜ëŠ” â€œí…ŒìŠ¤íŠ¸â€ë¥¼ ë°˜ë³µí•˜ëŠ” ê²ƒì´ë‹¤. ë¦¬ìŠ¤í¬ì™€ ê°€ì„¤ ìœ„ì—ì„œ ê°€ì„¤ì˜ ê²€ì¦ ì¤‘ìš”ì„±ì„ ì–¸ê¸‰í•˜ì˜€ë‹¤. ê·¸ë ‡ë‹¤ë©´ ìŠ¤íƒ€íŠ¸ì—…ì´ ê°€ì§€ê³  ìˆëŠ” ê°€ì„¤ ì¤‘ ì–´ë–¤ ê°€ì„¤ ë¶€í„° í™•ì¸í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ ë§í•˜ìë©´, í‹€ë ¸ì„ ê²½ìš°, ì‚¬ì—…ì´ ì‹¤íŒ¨í•  í™•ë¥ ì´ í° ë¦¬ìŠ¤í¬ê°€ ë†’ì€ ê°€ì„¤ì„ ë¨¼ì € í™•ì¸í•´ì•¼ í•¨ì´ ë…¼ë¦¬ì ì¼ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ê·¸ ë¦¬ìŠ¤í¬ê°€ í° ê°€ì„¤ì€ ì œì¼ ë¬´ì—‡ì¼ì§€ë¥¼ ê³ ë¯¼í•´ë³´ë©´ ìˆ˜ìµì„ ì°½ì¶œí•´ë‚¼ìˆ˜ ìˆìœ¼ë ¤ë©´ ê³ ê°ì´ ìˆì–´ì•¼ í•˜ë¯€ë¡œ ì ì¬ì ì¸ ê³ ê°ì´ ì—†ë‹¤ëŠ” ê²ƒì´ ê°€ì¥ ë¦¬ìŠ¤í¬ê°€ ë†’ì„ ê²ƒì´ë‹¤. ê³ ê° ë¦¬ìŠ¤í¬ê³ ê°ì— ëŒ€í•œ ê°€ì„¤ì„ í™•ì¸í•˜ëŠ” ë°©ë²• ê³ ê° ì¸í„°ë·° (ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•) í¬ì»¤ìŠ¤ ê·¸ë£¹ ì¸í„°ë·°(FGI) 5~6ëª… ì •ë„ë˜ëŠ” ì¸ì›ë“¤ì„ í•œ ë°©ì— ëª¨ì•„ë†“ê³  ì¸í„°ë·° ì§„í–‰ì„ í•˜ë¯€ë¡œ ì¸í„°ë·° ì¤‘ ìì‹ ì˜ ì£¼ê´€ì ì¸ ì˜ê²¬ì„ ë§í•˜ê¸° ë³´ë‹¤ëŠ” ê·¸ ì¸í„°ë·°ì˜ íë¦„ì— ë”°ë¼ ì˜ê²¬ì„ ë‚¼ ìˆ˜ ìˆëŠ” ì§‘ë‹¨ ì‚¬ê³ ì˜ ìœ„í—˜ì´ ìˆì„ ìˆ˜ ìˆë‹¤. ë˜í•œ ëª©ì†Œë¦¬ í° ì‚¬ëŒì—ì„¸ ë™ì¡°í•˜ëŠ” ê²½í–¥ì„ ë³´ì´ë¯€ë¡œ ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì „ë¬¸ ëª¨ë”ë ˆì´í„°ë¥¼ ê³ ìš©í•˜ëŠ” ë°©ë²•ì´ ìˆì§€ë§Œ ì´ ë°©ë²•ì€ ë¹„ìš©ì´ ë†’ìœ¼ë¯€ë¡œ ìŠ¤íƒ€íŠ¸ì—…ì—ì„œ ì¶”ì²œí•˜ëŠ” ë°©ë²•ì€ ì•„ë‹ˆë‹¤. 1ëŒ€ 1 ì‹¬ì¸µ ì¸í„°ë·° ìœ„ì˜ í¬ì»¤ìŠ¤ ê·¸ë£¹ ì¸í„°ë·°ì™€ëŠ” ë‹¤ë¥´ê²Œ ì§‘ë‹¨ ì‚¬ê³ ì˜ ìœ„í—˜ì´ ì—†ê³  ë¹„êµì  ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤. ì˜¨ì „íˆ í•œëª…ì˜ ê³ ê°ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤. ì—¬ê¸°ê¹Œì§€ ë³´ê²Œ ë˜ë©´ ê·¸ë¡œìŠ¤ í•´í‚¹ì€ ë°ì´í„°ë¥¼ ë³´ì•„ì•¼ë§Œ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆì§€ ì•ŠëŠëƒëŠ” ì˜ë¬¸ì´ ìƒê¸¸ ê²ƒì´ë‹¤. ì´ì— ëŒ€í•œ ë‹µì€ ì•„ë‹ˆë‹¤ë¼ëŠ” ê²°ë¡ ì„ ë‚´ë¦´ ìˆ˜ ìˆê²Œ ëœë‹¤. ì™œëƒí•˜ë©´ ê·¸ë¡œìŠ¤ë¼ëŠ” ê²ƒì€ ê²°êµ­ ê³ ê°ì„ íŒŒì•…í•˜ì—¬ ê³ ê°ì´ ì–´ë–¤ ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ ê°™ì€ ê³ ê°ì˜ ì ì¬ì ì¸ ë‹ˆì¦ˆë¥¼ íŒŒì•…í•˜ì—¬ ê·¸ë¥¼ í•´ê²°í•˜ë¯€ë¡œì„œ ìˆ˜ìµì„ ì°½ì¶œí•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. ì°¸ê³ ë¡œ ë°ì´í„° ê¸°ë°˜ íšŒì‚¬ì˜ ëíŒì™•ì¸ êµ¬ê¸€ë„ ê³ ê° ì¸í„°ë·°ë¥¼ ì¤‘ì‹œí•œë‹¤. [ì°¸ê³ ] ê³ ê° ì¸í„°ë·° ê³µë¶€í•˜ê¸° ì• ì‰¬ ëª¨ë¦¬ì•„ì˜ ì±… â€œë¦° ìŠ¤íƒ€íŠ¸ì—…â€, â€œìŠ¤ì¼€ì¼ë§â€ Google Ventures - User Research, Quick nâ€™Dirtyhttps://library.gv.com/user-research-quick-and-dirty-1fcfa54c91c4 ê³ ê°ì´ ì¡´ì¬í•œë‹¤ë©´ ì¶”í›„ì— í™•ì¸í•´ì•¼í•  ë¦¬ìŠ¤íŠ¸ ê³ ê°ì´ ì¡´ì¬í•˜ê¸´ í•˜ë”ë¼ë„, ì ì€ ìˆ˜ì˜ ê³ ê°ë§Œ ì¡´ì¬í•œë‹¤ë©´ ì‹œì¥ì˜ íŒŒì´ê°€ í¬ì§€ ì•Šìœ¼ë¯€ë¡œ ì‚¬ì—…ëª¨ë¸ì„ ë§Œë“¤ì–´ë‚´ê¸° ì í•©í•˜ì§€ ì•Šë‹¤. ì´ëŸ° ì‹œì¥ì˜ í¬ê¸°ëŠ” ì¶©ë¶„íˆ ì—¬ëŸ¬ëª…(50ëª…, 100ëª…)ê³¼ ì¸í„°ë·°ë¥¼ í•˜ë©°, ë°˜ë³µì ì¸ íŒ¨í„´ì„ ê´€ì°°í•´ì•¼ í•˜ë©°, ë˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œëŠ” ì‹¤ì œ ì œí’ˆì„ ê°œë°œí•˜ì§€ ì•Šê³  ì•„ì´ë””ì–´ë¥¼ ê²€ì¦í•˜ëŠ” ë°©ë²•ì¸ Smoke Testë¥¼ ì‹¤í–‰í—¤ ë³´ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ë”ë¯¸ í™”ë©´ì„ ë§Œë“¤ì–´ ê·¸ê³³ìœ¼ë¡œ íŠ¸ë˜í”½ì„ ìœ ì…ì‹œ\u001dì¼œì„œ í…ŒìŠ¤íŠ¸ë¥¼ í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ ìš°ë¦¬ ì œí’ˆì˜ ì‹œì¥ì—ì„œì˜ ìˆ˜ìš”ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì†”ë£¨ì…˜ ë¦¬ìŠ¤í¬ Problem-Solution Fitì´ ë§ì§€ ì•Šë‹¤ë¼ëŠ” ë§ì„ ë“¤ì–´ë³¸ì ì´ ìˆì„ ê²ƒì´ë‹¤. ì¦‰, ì†”ë£¨ì…˜ì´ ë¬¸ì œì— ì í•©í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì´ë‹¤. ê³ ê°ì˜ ë¬¸ì œë„ ì¡´ì¬í•˜ê³ , ì§€ê°‘ì„ ì—´ ê³ ê°ë„ ì¶©ë¶„íˆ ë§ì§€ë§Œ ìš°ë¦¬ì˜ ì œí’ˆì´ ê·¸ ë¬¸ì œì— ëŒ€í•œ íš¨ê³¼ì ì¸ í•´ê²°ì±…ì´ ì•„ë‹Œ ê²½ìš°ì— ìœ„ì˜ ë§ì„ ì‚¬ìš©í•œë‹¤. ì†”ë£¨ì…˜ ë¦¬ìŠ¤í¬ì— ëŒ€ë¹„í•˜ë ¤ë©´ í”„ë¡œí† íƒ€ì…ì„ ë§Œë“¤ì–´ì„œ ê³ ê° ë°˜ì„ì„ ë¨¼ì € í™•ì¸í•´ì•¼ í•œë‹¤. ê°€ì„¤ì„ í™•ì¸í•˜ì§€ ì•Šì€ ì±„, ì œí’ˆ ê°œë°œì— ë„ˆë¬´ ë§ì€ ìì›ì„ ë“¤ì´ì§€ ì•Šì•„ì•¼ í•œë‹¤.","categories":[{"name":"growth hacking","slug":"growth-hacking","permalink":"https://heung-bae-lee.github.io/categories/growth-hacking/"}],"tags":[]},{"title":"í”„ë¡œê·¸ë˜ë¨¸ë¥¼ ìœ„í•œ ë² ì´ì§€ì•ˆ with íŒŒì´ì¬(1)","slug":"Bayes","date":"2019-03-08T14:52:20.000Z","updated":"2019-07-20T03:36:10.633Z","comments":true,"path":"2019/03/08/Bayes/","link":"","permalink":"https://heung-bae-lee.github.io/2019/03/08/Bayes/","excerpt":"","text":"ë² ì´ì§€ì•ˆ ì‹¬ë¦¬ ìƒíƒœ ë² ì´ì§€ì•ˆ ì¶”ë¡ ì€ ë¶ˆí™•ì‹¤ì„±ì„ ìœ ì§€í•œë‹¤ëŠ” ì ì—ì„œ ê¸°ì¡´ì˜ ì „í†µì ì¸ í†µê³„ì  ì¶”ë¡ ê³¼ ë‹¤ë¥´ë‹¤. ìœ„ì˜ ë§ì„ ì½ì—ˆì„ë•Œ ê°œì¸ì ìœ¼ë¡œ í†µê²Œí•™ì„ ì „ê³µí•œ ë‚˜ë¡œì¨ëŠ” ì´í•´ê°€ ì˜ ê°€ì§€ ì•Šì•˜ë‹¤. ì™œëƒí•˜ë©´ í†µê³„ë¼ëŠ” í•™ë¬¸ ìì²´ê°€ ë¶ˆí™•ì‹¤ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ ì•„ë‹Œê°€ë¼ëŠ” ì˜ë¬¸ì´ ë“¤ì—ˆê¸° ë•Œë¬¸ì´ë‹¤. randomí•œ ê²ƒì—ì„œ í™•ë¥ ì ìœ¼ë¡œ ë†’ì€ ê²ƒì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì¸ë° ë§ì´ë‹¤. ìš°ì„  í™•ë¥ ì„ í•´ì„í•˜ëŠ” ë°©ë²•ì€ í¬ê²Œ ë‘ê°€ì§€ë¡œ êµ¬ë¶„ ë  ìˆ˜ ìˆë‹¤. ê³ ì „ì ì¸ ë¹ˆë„ì£¼ì˜ì  ê´€ì  : ì‚¬ê±´ì´ ì¥ê¸°ì ìœ¼ë¡œ ì¼ì–´ë‚˜ëŠ” ë¹ˆë„ë¡œ í•´ì„ex) í•­ê³µê¸°ì˜ ì‚¬ê³  í™•ë¥ ì„ ì˜¤ë«ë™ì•ˆ ë°œìƒí•œ ë¹„í–‰ê¸° ì‚¬ê³ ì˜ ë¹ˆë„ë¡œ í•´ì„í•œë‹¤. ìœ„ì˜ ë¹ˆë„ì£¼ì˜ì  ê´€ì ì€ ë…¼ë¦¬ì ìœ¼ë¡œëŠ” ì˜¤ë¥˜ê°€ ì—†ì–´ ë³´ì´ë‚˜, ì˜¤ë«ë™ì•ˆì´ë¼ëŠ” ì „ì œì¡°ê±´ì´ í•„ìš”í•˜ë‹¤. ë§Œì¼ ì˜¤ë«ë™ì•ˆ ì¦‰, ì¶©ë¶„í•œ ì‹œë„ê°€ ì—†ì—ˆì„ ê²½ìš°ì˜ í™•ë¥ ì€ ì–´ë– í•œ ê·¼ê±°ë¡œ ì´ì•¼ê¸° í•´ì•¼í•  ê²ƒì¸ê°€ì˜ ì˜ë¬¸ì„ ê°–ê²Œ ëœë‹¤. ê·¸ê²ƒì´ ë°”ë¡œ ë² ì´ì§€ì•ˆ ë°©ë²•ì´ë‹¤. ë² ì´ì§€ì•ˆ ê´€ì  : ì‚¬ê±´ ë°œìƒì— ëŒ€í•œ ë¯¿ìŒ ë˜ëŠ” í™•ì‹ ì˜ ì²™ë„ë¡œ í•´ì„ ê°„ë‹¨íˆ ë§í•˜ë©´, í™•ë¥ ì€ ì˜ê²¬ì´ë‚˜ ê²¬í•´ë¥¼ ìš”ì•½í•œ ê²ƒì´ë¼ëŠ” ì˜ë¯¸ì¼ ê²ƒì´ë‹¤. ì£¼ëª©í•  ì ì€ ë¯¿ìŒì˜ ì •ë„ë‚˜ í™•ì‹ ì˜ ì²™ë„ê°€ ê°œì¸ì˜ ì£¼ê´€ì— ë‹¬ë ¤ ìˆë‹¤ëŠ” ì ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ê°ê´€ì ì´ì§€ ì•Šì€ë° ì´ê²ƒì„ í™•ë¥ ì´ë¼ê³  ë§í•  ìˆ˜ ìˆëŠ”ê°€? ì¢€ë” ê·¼ë³¸ì ì¸ ì–˜ê¸°ë¥¼ í•´ë³´ìë©´ ë¯¿ìŒì´ê¸° ë•Œë¬¸ì— ëˆ„êµ°ê°€ê°€ í‹€ë ¸ë‹¤ëŠ” ê²ƒì„ ë§í•  ìˆ˜ ì—†ì§€ ì•Šì€ê°€? ë² ì´ì§€ì•ˆê´€ì ì—ì„œì˜ í™•ë¥ ì„ ì´ì•¼ê¸°í•˜ìë©´ ë² ì´ì¦ˆ ì •ë¦¬ ë˜í•œ ë¹¼ë†“ê³  ì´ì•¼ê¸° í• ìˆ˜ ì—†ì„ ê²ƒì´ë‹¤.ë² ì´ì¦ˆ ì •ë¦¬ì˜ ì‚¬ì „í™•ë¥ ê³¼ ì‚¬í›„í™•ë¥ ","categories":[{"name":"Bayes","slug":"Bayes","permalink":"https://heung-bae-lee.github.io/categories/Bayes/"}],"tags":[]},{"title":"ë¹„ì£¼ì–¼ ë””ìì¸ì—ì„œ ì œëª©(Headings), ë‹¨ë½(Paragraph)êµ¬ì¡° ë„ì¶œí•˜ê¸°","slug":"front_end4","date":"2019-03-05T14:05:59.000Z","updated":"2019-07-20T03:36:22.826Z","comments":true,"path":"2019/03/05/front_end4/","link":"","permalink":"https://heung-bae-lee.github.io/2019/03/05/front_end4/","excerpt":"","text":"ì œëª©(Hedings), ë‹¨ë½(Paragraph) ë„ì¶œí•˜ëŠ” ê³¼ì •ë‹¨ë½(Paragraph) ì‚¬ìš©ìê°€ ê°€ì¥ ë§ì´ ì½ëŠ” ì½˜í…ì¸ ëŠ” ë‹¨ë½ìœ¼ë¡œ ë‹¨ë½ì€ pìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤. ì œëª©ì˜ ë‹¨ê³„(Hedings Level 1 - 6) ì‚¬ìš©ìê°€ ê°€ì¥ ë¨¼ì € ì½ì€ ì½˜í…ì¸ ëŠ” ì œëª©ìœ¼ë¡œ ì œëª©ì€ hìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤. h1 ìš”ì†ŒëŠ” ë¬¸ì„œì—ì„œ ë‹¨ í•œë²ˆë§Œ ì‚¬ìš©í•˜ë©° HTML5ì—ì„œëŠ” ì„¹ì…˜ ì½˜í…ì¸  ë§ˆë‹¤ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤. HTML ì œëª© HTML ë‹¨ë½ HTML ì£¼\u001dì„ 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=\"ko-KR\" dir=\"ltr\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"&gt; &lt;title&gt;HTML ì œëª©(Headings) ê·¸ë¦¬ê³  ë‹¨ë½(Paragraph)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; \bì˜¤ëŠ˜ì˜ ë‚˜ì´, ëŒ€ì²´ë¡œ ë§‘ìŒ &lt;h1&gt;ì´ì œ ì¢€ ì‚´ì•„ë³¸ ì‚¬ëŒë“¤ì˜ ë§ˆí”, ìê¸° ìƒì˜ ë‚ ì”¨ë¥¼ ì ì ˆíˆ ëŒ€ì²˜í•˜ê¸° ì•Œë§ì€ ë‚˜ì´.&lt;/h1&gt; &lt;h2&gt;ì´ ë‚˜ì´ì—ë„ ì—¬ì „íˆ ë¯¸ìˆ™í•˜ê³  ê¾¸ì¤€íˆ ì‹¤ìˆ˜í•œë‹¤.&lt;/h2&gt; &lt;img src=\"sbs-drama__do-you-want-to-kiss-first.png\" alt=\"SBS ë“œë¼ë§ˆ &lt;í‚¤ìŠ¤ ë¨¼ì € í• ê¹Œìš”?&gt;ì˜ í•œ ì¥ë©´: ë°°ìš° ê¹€ì„ ì•„ê°€ í™€ë¡œ ê²¨ìš¸ë°”ë‹¤ë¥¼ ê±·ëŠ” ì¤‘...\"&gt; &lt;/body&gt;&lt;/html&gt; HTML ì´ë¯¸ì§€ &amp; í”¼ê·œì–´ &amp; ìº¡ì…˜ HTML ë¬¸ì„œì— ì—°ê²°ë˜ì–´ í™”ë©´ì— í‘œì‹œë˜ëŠ” ì´ë¯¸ì§€ ìš”ì†Œì™€ ë„í‘œ, ì°¨íŠ¸, í‘œ ì´ë¯¸ì§€ ë“±ì„ ìº¡ì…˜ê³¼ í•¨ê»˜ ë¬¶ì–´ì£¼ëŠ” í”¼ê·œì–´ ìš”ì†Œì— ì•Œì•„ë³¸ë‹¤. ìœ„ì—ì„œ img ìš”ì†Œì˜ alt ì†ì„±ì„ í•´ë‘ëŠ” ì´ìœ ëŠ” ë§í¬ê°€ ê¹¨ì§ˆ ê²½ìš°ì—ëŠ” í™”ë©´ì— alt ì†ì„± ê°’ì´ ì¶œë ¥ ë˜ì–´ ì–´ë–¤ ì´ë¯¸ì§€ ì˜€ëŠ”ì§€ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ìˆìœ¼ë©°, ë˜í•œ ì‹œê° ì¥ì• ì¸ì˜ ê²½ìš° ì´ë¯¸ì§€ë¥¼ ë³´ì§€ ëª»í•˜ë¯€ë¡œ ì´ëŸ° ëŒ€ì²´ ì†ì„±ì„ í†µí•´ì„œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë˜í•œ, HTMLì˜ ì´ë¯¸ì§€ ê°™ì€ ê²½ìš°ëŠ” ì„œë²„ë¥¼ í†µí•´ ì—°ê²°ë˜ì–´ ìˆë‹¤. ì¦‰, í•œê¸€ì´ë‚˜ ê¸°íƒ€ ë¬¸ì„œ ì‘ì—…ì˜ í”„ë¡œê·¸ë¨ì—ì„œ ì²˜ëŸ¼ ì´ë¯¸ì§€ë¥¼ ì‹¤ì œë¡œ í¬í•¨í•˜ê³  ìˆëŠ” ê²ƒ(ì„ë² ë”©)ì´ ì•„ë‹ˆë¼ëŠ” ì ì´ í¬ê²Œ ë‹¤ë¥¸ ì ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì›¹ ë¬¸ì„œì— ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ í¬ë©§ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. ë¹„íŠ¸ë§µ ê·¸ë˜í”½ íŒŒì¼ : JPG, GIF, PNG í¬ë©§ì´ ì‚¬ìš© ë²¡í„° ê·¸ë˜í”½ íŒŒì¼ : SVG í¬ë©§ì´ ì‚¬ìš© (ìœ„ì˜ ë¹„íŠ¸ë§µ ê·¸ë˜í”½ íŒŒì¼ì€ í”½ì…€ë¡œ êµ¬ì„±ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— í¬ê¸°ë¥¼ í‚¤ìš¸ ê²½ìš° ë¿Œì˜‡ê²Œ íë ¤ì§„ë‹¤.) JPG ì´ë¯¸ì§€JPG ì´ë¯¸ì§€ëŠ” ì••ì¶•ë¥ ì´ ë†’ê³ , ë‹¤ì–‘í•œ ìƒ‰ìƒì„ ì²˜ë¦¬í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ê·¸ëŸ¬ë¯€ë¡œ ì‚¬ì§„ ë˜ëŠ” ë³µì¡í•œ ê·¸ë˜í”½(ê·¸ë ˆë””ì–¸íŠ¸ì™€ ê°™ì€) ì´ë¯¸ì§€ì— ë§ì´ ì‚¬ìš©ëœë‹¤. í•˜ì§€ë§Œ, GIF, PNGì™€ ë‹¬ë¦¬ íˆ¬ëª…í•œ í”½ì…€ì„ í—ˆìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. PNG ì´ë¯¸ì§€ì‚¬ì§„ì˜ ê²½ìš° ë™ì¼í•œ í’ˆì§ˆì˜ PNG íŒŒì¼ í¬ê¸°ê°€ ì¼ë°˜ì ìœ¼ë¡œ JPGë³´ë‹¤ í¬ê¸° ë•Œë¬¸ì— PNGëŠ” ì‚¬ì§„ì´ë‚˜ ì• ë‹ˆë©”ì´ì…˜ì„ ì œì™¸í•œ ëª¨ë“  ìœ í˜•ì— ì í•©í•˜ë‹¤. í•˜ì§€ë§Œ JPGì™€ ë‹¬ë¦¬ íˆ¬ëª… ì²˜ë¦¬ê°€ ê°€ëŠ¥í•´ ì•„ì´ì½˜, ë¡œê³ , ë‹¤ì´ì–´ê·¸ë¨ ë“±ì— ì‚¬ìš©í•˜ë©´ ì¢‹ë‹¤. GIF ì´ë¯¸ì§€GIFëŠ” í‘œí˜„ ê°€ëŠ¥í•œ ìƒ‰ìƒì´ 256ìƒ‰ìœ¼ë¡œ ì œí•œë˜ì–´ ìˆê¸°ì— ì‚¬ì§„ì—ëŠ” ì í•©í•˜ì§€ ì•Šë‹¤. í•˜ì§€ë§Œ, ì• ë‹ˆë©”ì´ì…˜ì„ ì ìš©í•  ìˆ˜ ìˆëŠ” í¬ë©§ìœ¼ë¡œ ë‹¨ìˆœí•œ ê·¸ë˜í”½ì˜ ì• ë‹ˆë©”ì´ì…˜ì— ì‚¬ìš©í•˜ë©´ ì¢‹ë‹¤. íˆ¬ëª…í•˜ê²Œ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•˜ê¸´ í•˜ì§€ë§Œ, PNG í¬ë©§ ë³´ë‹¤ í‘œí˜„ë ¥ì´ ë–¨ì–´ì§„ë‹¤. SVG ì´ë¯¸ì§€SVGëŠ” ë²¡í„° ê¸°ë°˜ ê·¸ë˜í”½ í¬ë©§ìœ¼ë¡œ í’ˆì§ˆ ì†ì‹¤ì—†ì´ í™•ëŒ€, ì¶•ì†Œ í•  ìˆ˜ ìˆë‹¤. ì˜¤ëŠ˜ë‚  ì²˜ëŸ¼ ë‹¤ì–‘í•œ ìŠ¤í¬ë¦°ì— ëŒ€ì‘í•˜ëŠ” ë°˜ì‘í˜• ì›¹ ë””ìì¸ì— ë§¤ìš° ì í•©í•˜ë‹¤. HTML ì´ë¯¸ì§€HTML í”¼ê·œì–´HTML í”¼ê·œì–´ ìº¡ì…˜ HTML ë¬¸ë²• ìœ íš¨ì„± ê²€ì‚¬ &amp; ì—”í‹°í‹°(Entity) ì½”ë“œHTML ë¬¸ë²• ìœ íš¨ì„± ê²€ì‚¬(Validator) HTML ë¬¸ì„œì˜ ë¬¸ë²•ì˜ ìœ íš¨ì„± ê²€ì‚¬ ë°©ë²•ê³¼ ê° ê´„í˜¸(Angle Bracket,&lt; &gt;) ëª¨ì–‘ì˜ ë¬¸ìë¥¼ ë¸Œë¼ìš°ì €ê°€ íƒœê·¸ë¡œ ì¸ì‹í•˜ì§€ ì•Šë„ë¡ ì´ìŠ¤ì¼€ì´í”„(Escape) ì²˜ë¦¬í•˜ëŠ” ì—”í‹°í‹°(Entity)ì½”ë“œì— ëŒ€í•´ì„œ ì•Œì•„ë³¸ë‹¤. ì°¸ê³ ìë£Œvaildator.w3.orgentitycode.com ìœ„ì—ì„œ ì²«ë²ˆì§¸ ì‚¬ì´íŠ¸ì˜ ê²½ìš° HTML ë¬¸ë²•ì„ ê²€ì‚¬í• ìˆ˜ ìˆëŠ” ì˜¨ë¼ì¸ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³  ìˆì–´ì„œ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ë©´ ì¢‹ë‹¤. ë˜í•œ, ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ë¥¼ í• ë ¤ëŠ” ë¬¸ìë“¤ì€ ë‘ë²ˆì§¸ ì‚¬ì´íŠ¸ì—ì„œ ì°¾ì•„ì„œ ìˆ˜ì •í•˜ë©´ ëœë‹¤. ìœ„ì˜ ì½”ë“œì—ì„œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬ë¥¼ í•´ì¤€ë‹¤ë©´, ë‹¤ìŒê³¼ ê°™ì´ &lt;&gt; ë¶€ë¶„ì´ ë³€ê²½ëœë‹¤. 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=\"ko-KR\" dir=\"ltr\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"&gt; &lt;title&gt;HTML ì œëª©(Headings) ê·¸ë¦¬ê³  ë‹¨ë½(Paragraph)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; \bì˜¤ëŠ˜ì˜ ë‚˜ì´, ëŒ€ì²´ë¡œ ë§‘ìŒ &lt;h1&gt;ì´ì œ ì¢€ ì‚´ì•„ë³¸ ì‚¬ëŒë“¤ì˜ ë§ˆí”, ìê¸° ìƒì˜ ë‚ ì”¨ë¥¼ ì ì ˆíˆ ëŒ€ì²˜í•˜ê¸° ì•Œë§ì€ ë‚˜ì´.&lt;/h1&gt; &lt;h2&gt;ì´ ë‚˜ì´ì—ë„ ì—¬ì „íˆ ë¯¸ìˆ™í•˜ê³  ê¾¸ì¤€íˆ ì‹¤ìˆ˜í•œë‹¤.&lt;/h2&gt; &lt;img src=\"sbs-drama__do-you-want-to-kiss-first.png\" alt=\"SBS ë“œë¼ë§ˆ &amp;ltí‚¤ìŠ¤ ë¨¼ì € í• ê¹Œìš”?&amp;gtì˜ í•œ ì¥ë©´: ë°°ìš° ê¹€ì„ ì•„ê°€ í™€ë¡œ ê²¨ìš¸ë°”ë‹¤ë¥¼ ê±·ëŠ” ì¤‘...\"&gt; &lt;/body&gt;&lt;/html&gt; HTML ëª©ë¡ ë””ìì¸HTML ë¬¸ì„œ ì‘ì„± ì‹œ, ëª©ë¡ì€ ë§¤ìš° ë¹ˆë²ˆí•˜ê²Œ ì‚¬ìš© ë˜ëŠ” ìš”ì†Œì´ë‹¤. ë¹„ìˆœì°¨ ëª©ë¡(Unorderd List) : ìˆœì„œê°€ ì—†ëŠ” ëª©ë¡ ìˆœì°¨ ëª©ë¡(Ordered List) : ìˆœì„œê°€ ì¤‘ìš”í•œ ëª©ë¡ ì—¬ê¸°ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ olì´ë‚˜ ulìš”ì†Œì˜ ìì‹ì€ ê¼­ liìš”ì†Œë¡œ ê°ì‹¸ê³  ì‹œì‘í•´ì•¼í•œë‹¤ëŠ” ì ì´ë‹¤!!! ì°¸ê³  ìë£Œ HTML ì•µì»¤(Anchor) &amp; í•˜ì´í¼ë§í¬(Hyperlink)í•˜ì´í¼ë§í¬(Hyperlink)í˜„ì¬ í˜ì´ì§€ì—ì„œ ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ê²Œ í•´ì£¼ëŠ” ê²ƒì„ í•˜ì´í¼ë§í¬ë¼ê³  í•œë‹¤. ì´ë ‡ê²Œ í•˜ì´í¼ ë§í¬ë¥¼ í• ë•Œ ì‚¬ìš©í•˜ëŠ” ìš”ì†ŒëŠ” ì•µì»¤ìš”ì†Œì´ë©°, ë‹¤ìŒê³¼ ê°™ë‹¤. ì•µì»¤(Anchor) : í˜ì´ì§€ë¥¼ ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì „í™˜í•˜ì§€ ì•Šê³ , í˜„ì¬ í˜ì´ì§€ í•˜ë‹¨ìœ¼ë¡œ ì´ë™í•  ë•Œ ì‚¬ìš©í•œë‹¤.123456789101112131415161718192021222324&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" dir=\"ltr\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;ëª©ì°¨(Table of Contents)&lt;/h2&gt; &lt;!-- Depth 1: ul --&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=\"#intro\"&gt;ì†Œê°œ&lt;/a&gt;&lt;/li&gt; &lt;!-- Depth 2: ul --&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=\"\"&gt;ì˜ìƒì†Œê°œ&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"\"&gt;ì–´ë µì§„ ì•Šì„ê¹Œ ê±±ì •ë˜ì‹œë‚˜ìš”&gt;?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"\"&gt;GSAPì— ëŒ€í•´ ê°„ëµí•˜ê²Œ ì •ë¦¬í•´ë³¼ê¹Œìš”?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"\"&gt;TweenLite&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;li&gt;&lt;a href=\"\"&gt;ë‹¤ìš´ë¡œë“œ &amp;amp; CDN&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=\"intro\"&gt;ì†Œê°œ&lt;/h3&gt; &lt;/body&gt;&lt;/html&gt; ì˜ˆë¥¼ë“¤ì–´, ìœ„ì˜ ì½”ë“œì—ì„œëŠ” ì•µì»¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œê°œë¥¼ ëˆ„ë¥´ê²Œ ë˜ë©´, ì£¼ì†Œê°€ file:///Users/heungbaelee/workspace/myBlog/source/_posts/day.htmlì—ì„œ file:///Users/heungbaelee/workspace/myBlog/source/_posts/day.html#introë¡œ ë°”ë€Œê²Œ ë˜ëŠ” ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ì°¸ê³ ìë£Œí•˜ì´í¼ë§ ìš”ì†Œ ì›¹ì‚¬ì´íŠ¸ëŠ” í´ë”ë¡œ êµ¬ì„±ëœ HTML íŒŒì¼ì˜ ëª¨ìŒì¼ ë¿ì´ë‹¤. ë‹¤ë¥¸ íŒŒì¼ ë‚´ë¶€ì—ì„œ ì´ë“¤ íŒŒì¼ì„ ì°¸ì¡°í•˜ê¸° ìœ„í•´ ì¸í„°ë„·ì€ URL(uniform resource locators)ë¥¼ ì‚¬ìš©í•œë‹¤. URLì€ ì›¹ ì‚¬ì´íŠ¸ì˜ ë¦¬ì†ŒìŠ¤ ìœ„ì¹˜ ê²½ë¡œë¥¼ ë§í•˜ë©° ë‹¤ìŒê³¼ ê°™ì´ êµ¬ì„±ëœë‹¤. ì˜ˆë¥¼ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ ì£¼ì†Œê°€ ìˆì„ë•Œ, https://Developer.mozila.org/EN-US/DOCS/WEB/HTML ì—ì„œ https:// : SCHEME Developer.mozila.org : DOMAIN /EN-US/DOCS/WEB/HTML : PATH ì›¹ ë¬¸ì„œì— URLì„ ì…ë ¥í•˜ëŠ” ë°©ë²•ì€ 3ê°€ì§€ ì •ë„ê°€ ìˆë‹¤. ì ˆëŒ€ ê²½ë¡œ : í˜„ì¬ HTML ë¬¸ì„œì™€ ìƒê´€ì—†ì´ URL ì£¼ì†Œë¥¼ ì‚¬ìš©í•´ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¾ëŠ” ìƒëŒ€ ê²½ë¡œ : í˜„ì¬ HTML ë¬¸ì„œì—ì„œ ìƒëŒ€ì ì¸ ìœ„ì¹˜ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤. ex) â€œ../mics/extra.htmlâ€ê³¼ ê°™ì´ ì‚¬ìš©í•œë‹¤. ë£¨íŠ¸ ìƒëŒ€ê²½ë¡œ : í˜„ì¬ HTML ë¬¸ì„œê°€ ì¡´ì¬í•˜ëŠ” ì˜ì—­ì˜ ìµœìƒìœ„ ë£¨íŠ¸ ê²½ë¡œì—ì„œ ëŒ€ìƒì„ ì°¾ëŠ” ê²ƒ ex)â€/images.htmlâ€","categories":[{"name":"Front end","slug":"Front-end","permalink":"https://heung-bae-lee.github.io/categories/Front-end/"}],"tags":[]},{"title":"HTMLì˜ DOCTYPE?","slug":"frontent3","date":"2019-02-12T01:57:00.000Z","updated":"2019-03-05T13:56:18.570Z","comments":true,"path":"2019/02/12/frontent3/","link":"","permalink":"https://heung-bae-lee.github.io/2019/02/12/frontent3/","excerpt":"","text":"ìš°ì„  ì½”ë“œë¥¼ ì‘ì„±í• ë•Œ HTML ì½”ë“œëŠ” tag nameì— ëŒ€ë¬¸ìë¡œë„ ì‘ì„±ì´ ê°€ëŠ¥í•˜ë‚˜, ì†Œë¬¸ìë¡œ ì‘ì„±í•˜ëŠ” ê²ƒì„ ê¶Œí•˜ë©° ì½”ë“œ ê°„ ë“¤ì—¬ ì“°ê¸°ë¥¼ í•˜ëŠ” ì´ìœ ëŠ” HTML ìš”ì†Œ ê°„ ê´€ê³„(ë¶€ëª¨, ìì‹, í˜•ì œ)ë¥¼ êµ¬ë¶„í•˜ê¸° ìš©ì´ í•¨ì— ë“¤ì—¬ì“°ë‚˜, ë¸Œë¼ìš°ì €ëŠ” ì½”ë“œê°€ í•œ ì¤„ ì´ì–´ë„ í•´ì„í•˜ëŠ”ë° ì•„ë¬´ëŸ° ë¬¸ì œê°€ ì—†ë‹¤. DTD(í‘œì¤€ í˜¸í™˜ëª¨ë“œ)ë¬¸ì„œ ìœ í˜• ì •ì˜(Document Type Definition)ë¥¼ ë§í•˜ë©° ë¸Œë¼ìš°ì €ì˜ ë Œë”ë§ ëª¨ë“œë¥¼ í‘œì¤€ìœ¼ë¡œ ì‘ë™í•˜ê²Œ ë§Œë“¤ì–´ ì¤€ë‹¤. ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê²ƒì¸ DTDëŠ” ë°˜ë“œì‹œ HTML ë¬¸ì„œì˜ ìµœìƒ ìœ„ì— ìœ„ì¹˜í•´ì•¼ í•œë‹¤. ë¹„í‘œì¤€(quirks) ëª¨ë“œì™€ í‘œì¤€(standards) í˜¸í™˜ëª¨ë“œ ë˜í•œ, ë¸Œë¼ìš°ì €ì—ì„œ í•´ë‹¹ ì›¹ì˜ DOCTYPEì„ ì•Œê³  ì‹¶ì€ ê²½ìš°ëŠ” console ì°½ì— document.doctypeì„ ì…ë ¥í•˜ë©´ ì¶œë ¥ë˜ê²Œ ë˜ì–´ìˆì–´ì„œ í™•ì¸ í•  ìˆ˜ ìˆë‹¤. ì›¹ í‘œì¤€ ê¸°ìˆ  ê·œê²©ì— ë”°ë¥´ë©´ html ìš”ì†ŒëŠ” ì˜¤ì§ 1ê°œì˜ headì™€ ë’¤ ë”°ë¥´ëŠ” 1ê°œì˜ body ìš”ì†Œë§Œ ìì‹ìœ¼ë¡œ í¬í•¨í•  ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ê¸°ì–µí•˜ì. ë¬¸ì„œì— ì‚¬ìš©ëœ ì£¼ ì–¸ì–´ê°€ ë¬´ì—‡ì¸ì§€ ì„¤ì •í•˜ëŠ” lang ì†ì„±ë£¨íŠ¸ ìš”ì†Œì— lang ì†ì„±ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œì— ì£¼ìš” ì‚¬ìš©ë˜ëŠ” ì–¸ì–´(language)ë¥¼ ì„¤ì •í•œë‹¤.ì‚¬ìš©ë˜ëŠ” ì–¸ì–´ ì½”ë“œëŠ” ISO 639 (ì „ ì„¸ê³„ ì–¸ì–´ ëª…ì¹­ì— ê³ ìœ  ë¶€í˜¸ë¥¼ ë¶€ì—¬í•˜ëŠ” êµ­ì œ í‘œì¤€)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆë‹¤. 12345678910&lt;!doctype html&gt;&lt;html lang=\"ko-KR\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt; HTML ë¬¸ì„œ ì‘ì„±ì„ ìœ„í•œ ê¸°ë³¸ ë¬¸ë²•&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p title=\"Development Tools\"&gt;ê°œë°œ ë„êµ¬(Devtools)&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; ìœ„ì˜ ì½”ë“œì—ì„œ html íƒœê·¸ì˜ langì´ë¼ëŠ” attributeë¥¼ í†µí•´ ì‘ì„±í•˜ëŠ” ì½”ë“œì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì–¸ì–´ë¥¼ ì„ íƒ í•´ì£¼ë©´ ëœë‹¤. â€œko-KRâ€ì€ ëŒ€í•œë¯¼êµ­ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ì–¸ì–´ëŠ” koreanì´ë¼ëŠ” ì˜ë¯¸ì´ë‹¤. ko, en ì€ korean, english ì˜ ì•½ìë¡œ ì–¸ì–´(language) ì •ë³´ë¥¼ ë§í•œë‹¤. ë°˜ë©´ KRì€ Republic of Korea ì§€ì—­(locale) ì •ë³´ë¥¼ ë§í•œë‹¤. ko ë§Œ ì‚¬ìš©í•˜ë©´ í•œêµ­ì–´ë¥¼ í†µì¹­í•˜ì§€ë§Œ, ì–¸ì–´ë¥¼ ì§€ì—­ë§ˆë‹¤ êµ¬ë¶„í•´ì•¼ í•  ê²½ìš°, ì§€ì—­ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤. ì˜ˆë¥¼ë“¤ì–´, ko-KRì€ ëŒ€í•œë¯¼êµ­(ë‚¨í•œ)ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•œêµ­ì–´ë¥¼ ì˜ë¯¸í•˜ì§€ë§Œ, ko-KPëŠ” ì¡°ì„  ë¯¼ì£¼ì£¼ì˜ ì¸ë¯¼ê³µí™”êµ­(ë¶í•œ, ë¶ì¡°ì„ )ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•œêµ­ì–´ë¥¼ ì˜ë¯¸í•˜ê²Œ ëœë‹¤. ì˜ë¬¸ê¶Œì—ì„œëŠ” ì§€ì—­ë§ˆë‹¤ ì‚¬ìš© ë˜ëŠ” ì˜ì–´ê°€ ë‹¬ë¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œê¸°í•˜ì—¬ êµ¬ë¶„í•œë‹¤. en-GB â‡’ ì˜êµ­ ì˜ì–´en-US â‡’ ë¯¸êµ­ ì˜ì–´en-CA â‡’ ìºë‚˜ë‹¤ ì˜ì–´ ì–¸ì–´ ì¡°íšŒëŠ” Language subtag lookup ì—ì„œ í•  ìˆ˜ ìˆë‹¤.","categories":[{"name":"Front end","slug":"Front-end","permalink":"https://heung-bae-lee.github.io/categories/Front-end/"}],"tags":[]},{"title":"deep learning_01","slug":"deep_learning1","date":"2019-01-16T13:59:30.000Z","updated":"2019-12-08T05:35:02.857Z","comments":true,"path":"2019/01/16/deep_learning1/","link":"","permalink":"https://heung-bae-lee.github.io/2019/01/16/deep_learning1/","excerpt":"","text":"ìš°ì„  ì§€ê¸ˆë¶€í„° ì„¤ëª…í•˜ëŠ” ë‚´ìš©ì€ íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ì˜ êµìœ¡ê³¼ì •ì—ì„œ ë°°ìš´ ë‚´ìš©ê³¼ ì œ ë‚˜ë¦„ëŒ€ë¡œ ì±…ì„ ì½ì–´ê°€ë©° ì •ë¦¬í•˜ëŠ” ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ì„±í•˜ëŠ” ê¸€ì…ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ê¸°ì´ˆë”¥ëŸ¬ë‹? MLë¶„ì•¼ì—ì„œë‚˜ AIë¶„ì•¼ì—ì„œ í™œë°œí•˜ê²Œ ì´ìš©í•˜ê³  ìˆëŠ”ë° ê°„ë‹¨íˆ ë§í•˜ìë©´, ì—¬ê¸°ì„œ ë§í•˜ëŠ” â€˜deepâ€™ì€ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ” ì‹ ê²½ë§(í˜¹ì€ ê·¸ì— ìƒì‘í•˜ëŠ” ë‹¤ë¥¸ ê²ƒ)ì˜ ì¸µìˆ˜ê°€ â€˜ê¹Šë‹¤â€™ë¼ëŠ” ì˜ë¯¸ì´ë‹¤. ì¦‰, ì—¬ëŸ¬ì¸µì„ ê°€ì§„ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•œ í•™ìŠµì„ ë§í•œë‹¤. ì‹ ê²½ë§ ê¸°ì´ˆ ì´ë¡  ì‹ ê²½ë§(neural network) ëª¨í˜•ì€ ê¸°ì € í•¨ìˆ˜(basis function)ì˜ í˜•íƒœë¥¼ ëª¨ìˆ˜(parameter)ê°’ìœ¼ë¡œ ë³€í™” ì‹œí‚¬ ìˆ˜ ìˆëŠ” ì ì‘í˜• ê¸°ì € í•¨ìˆ˜ ëª¨í˜•(adaptive basis function model)ì´ë©° êµ¬ì¡°ì ìœ¼ë¡œëŠ” ë³µìˆ˜ì˜ í¼ì…‰íŠ¸ë¡ ì„ ìŒ“ì•„ë†“ì€ í˜•íƒœì´ë¯€ë¡œ MLP(multi-layer perceptron)ë¡œë„ ë¶ˆë¦°ë‹¤. ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜ ì¼ë°˜ì ìœ¼ë¡œ í™œì„±í™” í•¨ìˆ˜ hë¡œëŠ” ìœ„ì™€ ì•„ë˜ê°€ ë§‰í˜€ìˆëŠ”(bounded) ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ \\( \\sigma \\)ë¥¼ ì‚¬ìš©í•˜ëŠ”ë° ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” í™œì„±í™” í•¨ìˆ˜ëŠ” ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ì´ë‹¤. ë¹„ì„ í˜• ê¸°ì € í•¨ìˆ˜í¼ì…‰íŠ¸ë¡ ì—ì„œ x ëŒ€ì‹  ê¸°ì €í•¨ìˆ˜ \\( \\phi(x) \\)ë¥¼ ì‚¬ìš©í•˜ë©´ XOR ë¬¸ì œ ë“±ì˜ ë¹„ì„ í˜• ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ê³ ì •ëœ ê¸°ì € í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ë¯€ë¡œ ë¬¸ì œì— ë§ëŠ” ê¸°ì € í•¨ìˆ˜ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ë”°ë¼ì„œ ë§ì€ ê¸°ì € í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ë°–ì— ì—†ëŠ” ê²ƒì´ ë³´í†µì´ë‹¤. í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì— ì˜í•´ ëª¨ì–‘ì´ ë°”ë€ŒëŠ” ë¹„ì„ í˜• ê¸°ì € í•¨ìˆ˜ë§Œì•½ ê¸°ì € í•¨ìˆ˜ \\( \\phi(x) \\)ì˜ í˜•íƒœë¥¼ ì¶”ê°€ì ì¸ ëª¨ìˆ˜ \\( \\theta \\)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¡°ì ˆí•  ìˆ˜ ìˆë‹¤ë©´ ì¦‰, ê¸°ì €í•¨ìˆ˜ \\( \\phi(x;\\theta) \\)ë¥¼ ì‚¬ìš©í•˜ë©´ \\( \\theta \\)ê°’ì„ ë°”ê¾¸ëŠ” ê²ƒë§Œìœ¼ë¡œ ë‹¤ì–‘í•œ ì‹œë„ë¥¼ í•˜ì—¬ ë‹¤ì–‘í•œ ëª¨ì–‘ì˜ ê¸°ì €í•¨ìˆ˜ë¥¼ í…ŒìŠ¤íŠ¸í•´ ë³¼ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ì•ìœ¼ë¡œ ì„¤ëª…í•  ì‹ ê²½ë§ ì¦‰, MLP(Multi-Layer-Perceptron)ì€ í¼ì…‰íŠ¸ë¡ ì´ ì‚¬ìš©í•˜ê³  ìˆëŠ” ë¡œì§€ìŠ¤í‹± ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ê¸°ì € í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ëŠ” ëª¨í˜•ì´ë‹¤. ê¸°ì € í•¨ìˆ˜ì˜ í˜•íƒœëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì¸ w","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"HTMLì´ë€?","slug":"Foront_end2","date":"2019-01-15T03:36:00.000Z","updated":"2019-03-05T13:54:47.784Z","comments":true,"path":"2019/01/15/Foront_end2/","link":"","permalink":"https://heung-bae-lee.github.io/2019/01/15/Foront_end2/","excerpt":"","text":"HTML ë¬¸ì„œ ì‘ì„±ì„ ìœ„í•´ ì•Œì•„ì•¼ í•  ê¸°ë³¸ ë¬¸ë²•ì›¹ í˜ì´ì§€ëŠ” head ì˜ì—­ê³¼ bodyì˜ì—­ìœ¼ë¡œ êµ¬ì„± ëœë‹¤. Head Page title(ì›¹ í˜ì´ì§€ì˜ ì œëª©ìœ¼ë¡œ ë¸Œë¼ìš°ì € íƒ­ì— í‘œì‹œëœë‹¤.) ë§Œì•½ ìœ„ì˜ ê°’ì´ í•œê¸€ì¸ë° ê¹¨ì§€ê²Œ ëœ ê²½ìš°ëŠ” consoleì°½ì—ì„œ document.characterSetì„ ì³ë³´ë©´ ì¸ì½”ë”© ë°©ì‹ì„ í™•ì¸ í•  ìˆ˜ ìˆìœ¼ë©° meta íƒœê·¸ì— charset=â€utf-8â€ ì†ì„±ê°’ì„ ì…ë ¥í•´ì£¼ë©´ëœë‹¤. CSS Links Other Abstract things Body Headings Paragraph Other things(ì¶”ìƒì ì´ì§€ ì•Šê³  ìš°ë¦¬ ëˆˆì— ë³¼ìˆ˜ ìˆëŠ” ê²ƒ) HTML ìš©ì–´- element(ìš”ì†Œ) - tag - open tag - close tag - attribute - value ê¸°ë³¸ ë¬¸ë²•HTML ìš”ì†ŒëŠ” ëŒ€ì†Œë¬¸ìë¥¼ êµ¬ë¶„í•˜ì§€ ì•ŠëŠ”ë‹¤!!! í•˜ì§€ë§Œ ëŒ€ê²Œ ê°€ë…ì„±ê³¼ ê¸°íƒ€ ì´ìœ  ë•Œë¬¸ì— ì†Œë¬¸ìë¡œ ì‘ì„±í•œë‹¤.HTMLì€ elementsë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ì´ë“¤ì€ ì ì ˆí•œ ë°©ë²•ìœ¼ë¡œ ë‚˜íƒ€ë‚´ê³  ì‹¤í–‰í•˜ê¸° ìœ„í•´ ê° ì»¨í…ì¸ ì˜ ì—¬ëŸ¬ ë¶€ë¶„ë“¤ì„ ê°ì‹¸ê³  ë§ˆí¬ì—… í•œë‹¤. ë§Œì•½ ë¬¸ì¥ì„ ê·¸ëƒ¥ ìì²´ë¡œ í‘œí˜„í•˜ê³  ì‹¶ë‹¤ë©´?? ë‹¤ìŒê³¼ ê°™ì´ ë¡œ ê°ì‹¸ë©´ ëœë‹¤. 1$ &lt;p&gt;My cat is very grumpy.&lt;/p&gt; ì—¬ê¸°ì„œ ëŠ” opening tag &lt;/p&gt;ëŠ” closing tag ì•ˆì— ê°ì‹¸ì ¸ ìˆëŠ” My cat is very grumpyëŠ” Content ìœ„ì˜ ì½”ë“œ ì „ì²´ë¥¼ Elementë¼ê³  í•œë‹¤. ë‹«ëŠ” íƒœê·¸ê°€ ì—†ëŠ” HTML ìš”ì†Œ Empty ElementëŠ” contentsë¥¼ ê°ì‹¸ì§€ ì•Šì•„ ë¹„ì–´ìˆë‹¤ëŠ” ì˜ë¯¸ë¡œì¨ contentsë¥¼ ê°ì‹¸ì§€ ì•Šê¸° ë•Œë¬¸ì— ë‹«ëŠ” íƒœê·¸ë¥¼ ê°–ì§€ ì•ŠëŠ”ë‹¤. empty element ì¤‘ì²©ëœ ìš”ì†Œ(Nesting elements)ìš”ì†Œ ì•ˆì— ë‹¤ë¥¸ ìš”ì†Œê°€ ë“¤ì–´ê°ˆ ìˆ˜ ìˆë‹¤. ê·¸ëŸ° ìš”ì†ŒëŠ” ì¤‘ì²©ë˜ì–´ ìˆë‹¤ê³  í‘œí˜„í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ â€œê³ ì–‘ì´ê°€ ë§¤ìš° ì‚¬ë‚©ë‹¤â€ë¼ëŠ” ë¬¸ë‹¨ì„ ê°•ì¡°í•˜ê¸° ìœ„í•´ì„œ â€˜ë§¤ìš°â€™ë¼ëŠ” ë‹¨ì–´ë¥¼ ê°•ì¡°í•˜ëŠ” ìš”ì†Œë¥¼ ì¤‘ì²©í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.1$ &lt;p&gt;My cat is &lt;strong&gt;very&lt;/strong&gt; grumpy.&lt;/p&gt; ë¸”ëŸ­ ë ˆë²¨ ìš”ì†Œ vs ì¸ë¼ì¸ ìš”ì†Œ(Block vs inline elements)HTMLì˜ ë‘ ì¢…ë¥˜ì˜ ElementëŠ” Block level elementì™€ Inline elementì´ë‹¤. Block-level elementsëŠ” ì›¹í˜ì´ì§€ ìƒì— Blockì„ ë§Œë“œëŠ” ìš”ì†Œì´ë‹¤. ì•ë’¤ ìš”ì†Œ ì‚¬ì´ì— ìƒˆë¡œìš´ ì¤„(Line)ì„ ë§Œë“¤ê³  ë‚˜íƒ€ë‚œë‹¤. ì¦‰ ë¸”ë¡ ë ˆë²¨ ìš”ì†Œ ì´ì „ê³¼ ì´í›„ ìš”ì†Œì‚¬ì´ì˜ ì¤„ì„ ë°”ê¾¼ë‹¤. ë¸”ë¡ ë ˆë²¨ ìš”ì†ŒëŠ” ì¼ë°˜ì ìœ¼ë¡œ í˜ì´ì§€ì˜ êµ¬ì¡°ì  ìš”ì†Œë¥¼ ë‚˜íƒ€ë‚¼ ë•Œ ì‚¬ìš©ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ê°œë°œìëŠ” ë¸”ë¡ ë ˆë²¨ ìš”ì†Œë¥¼ ì‚¬ìš©í•˜ì—¬ Paragraph, list, Navigation Menus(ë„¤ë¹„ê²Œì´ì…˜ ë©”ë‰´), Footers(ê¼¬ë¦¬ë§)ë“±ì„ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. ë¸”ë¡ ë ˆë²¨ ìš”ì†ŒëŠ” ì¸ë¼ì¸ ìš”ì†Œì— ì¤‘ì²©ë  ìˆ˜ ì—†ë‹¤. ê·¸ëŸ¬ë‚˜ ë¸”ë¡ ë ˆë²¨ ìš”ì†ŒëŠ” ë‹¤ë¥¸ ë¸”ë¡ ë ˆë²¨ ìš”ì†Œì— ì¤‘ì²©ë ìˆ˜ ìˆë‹¤. Inline elementsëŠ” í•­ìƒ ë¸”ë¡ ë ˆë²¨ ìš”ì†Œë‚´ì— í¬í•¨ë˜ì–´ ìˆë‹¤. ì¸ë¼ì¸ ìš”ì†ŒëŠ” ë¬¸ì„œì˜ í•œ ë‹¨ë½ê°™ì€ í° ë²”ìœ„ì—ëŠ” ì ìš© ë  ìˆ˜ ì—†ê³  ë¬¸ì¥, ë‹¨ì–´ ê°™ì€ ì‘ì€ ë¶€ë¶„ì— ëŒ€í•´ì„œë§Œ ì ìš©ë  ìˆ˜ ìˆë‹¤. ê°€ì¥ í° ì°¨ì´ì ì€ ì¸ë¼ì¸ ìš”ì†ŒëŠ” ìƒˆë¡œìš´ ì¤„ì„ ë§Œë“¤ì§€ ì•ŠëŠ”ë‹¤.ì¦‰ ì¸ë¼ì¸ ìš”ì†Œë¥¼ ì‘ì„±í•˜ë©´ ê·¸ê²ƒì„ ì‘ì„±í•œ ë‹¨ë½ë‚´ì— ë‚˜íƒ€ë‚˜ê²Œ ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¸ë¼ì¸ ìš”ì†Œì—ëŠ” í•˜ì´í¼ë§í¬ë¥¼ ì •ì˜í•˜ëŠ” ìš”ì†Œì¸ , í…ìŠ¤íŠ¸(Text)ë¥¼ ê°•ì¡°í•˜ëŠ” ìš”ì†Œì¸ , ë“±ì´ ìˆë‹¤. 123&lt;em&gt;first&lt;/em&gt;&lt;em&gt;second&lt;/em&gt;&lt;em&gt;third&lt;/em&gt;&lt;p&gt;forth&lt;/p&gt;&lt;p&gt;fifth&lt;/p&gt;&lt;p&gt;sixth&lt;/p&gt; ìœ„ì˜ ì½”ë“œë¥¼ ì‹¤í–‰ ì‹œì¼œ ë³´ë©´ ì•Œ ìˆ˜ ìˆë“¯ì´ ì€ ì¸ë¼ì¸ ìš”ì†Œì—¬ì„œ ì„œë¡œ ê°™ì€ ì¤„ì— ê³µë°±ì´ ì—†ì´ ìœ„ì¹˜ í•˜ì§€ë§Œ, ëŠ” ë¸”ë¡ ë ˆë²¨ ìš”ì†Œì´ì–´ì„œ, ê° ìš”ì†Œë“¤ì€ ìƒˆë¡œìš´ ì¤„ì— ë‚˜íƒ€ë‚˜ë©°, ìœ„ì™€ ì•„ë˜ì— ì—¬ë°±ì´ ìˆë‹¤.(ì •í™•íˆëŠ” ì—¬ë°±ì€ ë¸Œë¼ìš°ì €ê°€ ë¬¸ë‹¨ì— ì ìš©í•˜ëŠ” ê¸°ë³¸ CSS styling ë•Œë¬¸ì— ì ìš©ëœë‹¤.) ë¹ˆ ìš”ì†Œ(Empty elements)ëª¨ë“  ìš”ì†Œê°€ ìœ„ì— ì–¸ê¸‰ëœ Open tag, contents, closed tagì˜ íŒ¨í„´ì„ ë”°ë¥´ëŠ” ê²ƒì€ ì•„ë‹ˆë‹¤. ì£¼ë¡œ ë¬¸ì„œì— ë¬´ì–¸ê°€ë¥¼ ì²¨ë¶€í•˜ê¸° ìœ„í•´ ë‹¨ì¼ íƒœê·¸(Single tag)ë¥¼ ì‚¬ìš©í•˜ëŠ” ìš”ì†Œë„ ìˆë‹¤. ì˜ˆë“¤ë“¤ì–´ ìš”ì†ŒëŠ” í•´ë‹¹ ìœ„ì¹˜ì— ì´ë¯¸ì§€ë¥¼ ì‚½ì…í•˜ê¸° ìœ„í•œ ìš”ì†Œì´ë‹¤.(ë¹ˆ ìš”ì†ŒëŠ” ê°€ë” Void ìš”ì†Œë¡œ ë¶ˆë¦¬ê¸°ë„ í•œë‹¤.) 1&lt;img src=\"https://raw.githubusercontent.com/mdn/beginner-html-site/gh-pages/images/firefox-icon.png\"&gt; ì†ì„±(attributes)elementëŠ” Attributeë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤. AttributeëŠ” elementì— ì‹¤ì œë¡  ë‚˜íƒ€ë‚´ê³  ì‹¶ì§€ ì•Šì§€ë§Œ ì¶”ê°€ì ì¸ ë‚´ìš©ì„ ë‹´ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©í•œë‹¤. ì•„ë˜ì˜ ì½”ë“œëŠ” ë‚˜ì¤‘ì— ìŠ¤íƒ€ì¼ì— ê´€ë ¨ëœ ë‚´ìš©ì´ë‚˜ ê¸°íƒ€ ë‚´ìš©ì„ ìœ„í•´ í•´ë‹¹ ëª©í‘œë¥¼ êµ¬ë¶„í•  ìˆ˜ ìˆëŠ” class attributeë¥¼ ë¶€ì—¬ í–ˆë‹¤. Attributeë¥¼ ì‚¬ìš©í•  ë•Œ ì§€ì¼œì•¼ í•  ì  element ì´ë¦„ ë‹¤ë¦„ì— ë°”ë¡œì˜¤ëŠ” attributeëŠ” elementì´ë¦„ê³¼ attribute ì‚¬ì´ì— ê³µë°±ì´ ìˆì–´ì•¼ ë˜ê³ , í•˜ë‚˜ ì´ìƒì˜ attributeê°€ ìˆëŠ” ê²½ìš°ì—” attribute ì‚¬ì´ì— ê³µë°±ì´ ìˆì–´ì•¼ í•œë‹¤. attribute ì´ë¦„ ë‹¤ìŒì—” ë“±í˜¸(=)ê°€ ë¶™ëŠ”ë‹¤. attribute ê°’ì€ ì—´ê³  ë‹«ëŠ” ë”°ì˜´í‘œë¡œ ê°ì‹¸ì•¼ í•œë‹¤.","categories":[{"name":"Front end","slug":"Front-end","permalink":"https://heung-bae-lee.github.io/categories/Front-end/"}],"tags":[]},{"title":"HTMLì´ë€?","slug":"Front_end","date":"2019-01-14T08:36:00.000Z","updated":"2019-03-05T13:54:51.234Z","comments":true,"path":"2019/01/14/Front_end/","link":"","permalink":"https://heung-bae-lee.github.io/2019/01/14/Front_end/","excerpt":"","text":"HTML(Hyper Text Markup Language)ì´ë€?? Hyper Textë€? textì˜ ë°‘ì¤„ì´ ê±°ì§€ê²Œë˜ì–´ ì‚¬ìš©ìê°€ ì´ê±¸ ëˆŒë¥´ë©´ ì—°ê²°ë˜ì–´ìˆëŠ” ë§í¬ë¡œ ì—°ê²°ëœë‹¤. êµ¬ì¡°ë¥¼ ì„¤ê³„í•  ë•Œ ì‚¬ìš©ë˜ëŠ” ì–¸ì–´ì´ë©°, í•˜ì´í¼ë§í¬ ì‹œìŠ¤í…œìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤. í™•ì¥ìëŠ” htm, htmlìœ¼ë¡œ ê°€ì§€ë©°, ì´ íŒŒì¼ì€ ë‹¨ìˆœí•œ í…ìŠ¤íŠ¸ íŒŒì¼ì— ë¶ˆê³¼í•˜ì§€ë§Œ, ì´ íŒŒì¼ì„ ì›¹ë¸Œë¼ìš°ì ¸ê°€ í™”ë©´ì— ë Œë”ë§(ê·¸ë¦¼ì„ ê·¸ë ¤ì¤€ë‹¤ë¼ê³  ìƒê°í•˜ë©´ë¨)ì„ í†µí•´ ì‚¬ìš©ìê°€ ë³¼ìˆ˜ ìˆë„ë¡ í•´ì¤€ë‹¤. HTML í‘œì¤€ ê¸°ìˆ  ì‚¬ì–‘ HTML ë¬¸ì„œ íŒŒì¼ê³¼ ì›¹ ë¸Œë¼ìš°ì €ì˜ í•´ì„ &amp; ì‹œë©˜í‹± ë§ˆí¬ì—… ì‹œë©˜í‹± ë§ˆí¬ì—…(Semantic Markup) HTMLì€ ì›¹ì‚¬ì´íŠ¸ ì½˜í…ì¸ ì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ëŠ” ìœ ì¼í•œ ëª©ì ì„ ê°€ì§„ë‹¤. ì¦‰, ë¹„ì£¼ì–¼ ë””ìì¸(ëª¨ì–‘, ìƒ‰, í¬ê¸° ë“±)ì´ ì•„ë‹ˆë¼(ì´ ìš”ì†Œë“¤ì€ CSSë‚˜ Javascriptì˜ ëª©í‘œ) ì „ì²´ì ì¸ êµ¬ì¡° ì„¤ê³„(Structure Design)ë¥¼ ëª©í‘œë¡œ í•œë‹¤. Semantic Markupì€ ì¢…ì¢… POSH(Plain Old Semantic HTML)ë¼ê³ ë„ ë¶ˆë¦¬ìš°ëŠ”ë° í•´ì„ ê·¸ëŒ€ë¡œ í‰ë²”í•˜ê³  ì˜¤í•¸ëœ ì˜ë¯¸ë¡ ì ì¸ HTMLì´ë¼ëŠ” ëœ»ì´ë‹¤. ì ì ˆí•œ HTMLìš”ì†Œë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•˜ëŠ” ê²ƒì—ì„œ ì‹œì‘í•œë‹¤. 1$ &lt;div id=\"heading\" style=\"font-size: 300%; padding: 10px;\"&gt;ì‹œë©˜í‹± ë§ˆí¬ì—…ì´ë€?&lt;/div&gt; ìœ„ì˜ ë§ˆí¬ì—…ì€ í™•ì‹¤íˆ ì œëª©ê°™ì•„ ë³´ì´ì§€ë§Œ ì˜ë¯¸ì™€ ìš©ë„ë©´ì—ì„œëŠ” ì œëª©ìœ¼ë¡œì„œì˜ ê¸°ëŠ¥ì€ ì—†ë‹¤. ì´ê²ƒì€ ê²€ìƒ‰ì—”ì§„ì˜ ìµœì í™”, ì ‘ê·¼ì„±, ê°œë°œë©´ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì‚¬í•­ë“¤ì„ ê³ ë ¤í•´ì•¼ëœë‹¤. ìµœì í™” : ê²€ìƒ‰ì—”ì§„ì„ ìµœì í™”í•˜ëŠ” ê²ƒì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê²ƒì€ headings ìš”ì†Œ ì•ˆì˜ í‚¤ì›Œë“œì´ë‹¤. ì ‘ê·¼ì„± : ìŠ¤í¬ë¦°ë¦¬ë”ê¸°ëŠ” ë„¤ë¹„ê²Œì´ì…˜ ê¸¸ì¡ì´ë¡œ headingsìš”ì†Œë¥¼ ì°¸ê³ í•œë‹¤. ê°œë°œ : ì ì ˆí•œ ì‹œë©˜í‹± ìš”ì†Œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê²Œ ë ì‹œ ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ì„ ì£¼ê¸° ìœ„í•´ í•´ë‹¹ ìš”ì†Œë¥¼ ì„¤ì •í•˜ëŠ”ë° ë§ì´ ê¹Œë‹¤ë¡­ë‹¤. ì‹œë©˜í‹± ë§ˆí¬ì—…ì€ ê°€ëŠ¥í•œ ê°€ë²¼ì›Œì•¼ í•˜ê¸°ì— ì¤‘ì²©ëœ ëª¨ë“  divìš”ì†Œì™€ ì‹¤íƒ€ë˜ ì²˜ëŸ¼ ì–½í˜€ ìˆëŠ” ìŠ¤íŒŒê²Œí‹° ì½”ë“œëŠ” ì œê±°ë˜ì–´ì•¼ íŒŒì¼ ì‚¬ì´ì¦ˆê°€ ì‘ì•„ì§€ê³  ì½”ë”©ì´ ë” ì‰¬ì›Œì§„ë‹¤. ê²°ë¡ ì ìœ¼ë¡œ HTML, CSS, JavascriptëŠ” ê°ê° ë¶„ë¦¬ë˜ì–´ì•¼ ê°œë°œ ë° ìœ ì§€ ë³´ìˆ˜ ì¸¡ë©´ì—ì„œ ìƒë‹¹í•œ ìˆ˜ê³ ë¥¼ ëœì–´ ì¤„ ìˆ˜ ìˆë‹¤. 1$ &lt;h1&gt;ì‹œë©˜í‹± ë§ˆí¬ì—…ì´ë€?&lt;/h1&gt; 2005ë…„ Andy Clark(ì„¸ê³„ì ì¸ ì›¹ë””ìì´ë„ˆ)ë„ ë°ì´í„° êµ¬ì¡°, ìŠ¤íƒ€ì¼ ì •ë³´, ê·¸ë¦¬ê³  ìŠ¤í¬ë¦½íŠ¸ ì •ë³´ë¥¼ ê°ê° ë¶„ë¦¬ì‹œì¼œì•¼ í•œë‹¤ê³  ë§í•œë‹¤. 1) ì‹œë©˜í‹± HTMLì€ ì •í™•í•˜ê³  ì ‘ê·¼ì´ ìš©ì´í•œ ì»¨í…ì¸ ì˜ ë°ì´íƒ€ êµ¬ì¡°ë¥¼ í˜•ì„±í•œë‹¤. ë°”ë¡œ HTML5ê°€ ì´ëŸ° ë¶€ë¶„ì„ ì˜ ì§€ì›í•˜ê³  ìˆë‹¤. ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” ì–´ë– í•œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì¶”ê°€ í•˜ì§€ ì•Šê³ ë„ ê°€ëŠ¥í•œ ì ‘ê·¼ì´ ìš©ì´í•˜ë©° ì“¸ëª¨ìˆê²Œ ë°ì´í„° êµ¬ì¡°ë¥¼ ë§Œë“¤ì–´ ë‚´ì•¼í•œë‹¤. CSSëŠ” ìŠ¤íƒ€ì¼ ì •ë³´ë¥¼ ì œê³µí•œë‹¤. ì¦‰, ë°ì´í„° êµ¬ì¡°ì— ìš°ë¦¬ê°€ ì›í•˜ëŠ”ëŒ€ë¡œ ì‹œê°ì ì¸ íš¨ê³¼ë¥¼ ë”í•´ ì£¼ëŠ” ê²ƒì´ë‹¤. CSS3ëŠ” ì´ì „ì˜ CSS2ë³´ë‹¤ ë” ê°•ë ¥í•œ íˆ´ì´ë‹¤. HTML5ì™€ ê·¸ ì™¸ ë‹¤ë¥¸ ê²ƒì— ì •ì˜ë˜ì–´ì§„ ìŠ¤í¬ë¦½íŒ… APIsì™€ ë² ì´ìŠ¤ì–¸ì–´ë¥¼ í¬í•¨í•œ ìë°”ìŠ¤í¬ë¦½íŠ¸ëŠ” ìš°ë¦¬ê°€ ì œì‘í•˜ëŠ” ì‚¬ì´íŠ¸ì— í’ë¶€í•œ ê¸°ëŠ¥ê³¼ ìœ ìš©ì„±ì˜ ì¦ëŒ€ë¥¼ ë”í•´ ì£¼ëŠ” ìŠ¤í¬ë¦½íŒ…ì„ ì œê³µí•œë‹¤. í…ìŠ¤íŠ¸ ì—ë””í„°ë¡œ HTML íŒŒì¼ì„ ì—´ì–´ì„œ ë§ˆí¬ì—… í•œ í›„ì— HTML íŒŒì¼ìš¸ ì›¹ ë¸Œë¼ìš°ì €ë¡œ ì—´ì–´ ìƒˆë¡œê³ ì¹¨í•˜ë©´ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë¸Œë¼ìš°ì €ëŠ” ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ê°€?","categories":[{"name":"Front end","slug":"Front-end","permalink":"https://heung-bae-lee.github.io/categories/Front-end/"}],"tags":[]},{"title":"í¬ë¡¤ë§ê³¼ ìŠ¤í¬ë˜í•‘ ë° unixëª…ë ¹ì–´ ê¸°ì´ˆì§€ì‹(1)","slug":"unix","date":"2018-12-23T09:34:53.000Z","updated":"2019-01-16T12:08:35.899Z","comments":true,"path":"2018/12/23/unix/","link":"","permalink":"https://heung-bae-lee.github.io/2018/12/23/unix/","excerpt":"","text":"í¬ë¡¤ë§ê³¼ ìŠ¤í¬ë˜í•‘ ì›¹í˜ì´ì§€ì—ì˜ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ í”„ë¡œê·¸ë¨ì„ ì›¹ í¬ë¡¤ëŸ¬(Web Crawler) ë˜ëŠ” ë‹¨ìˆœí•˜ê²Œ í¬ë¡¤ëŸ¬(Crawler)ë¼ê³  í•œë‹¤. í¬ë¡¤ëŸ¬ëŠ” ìŠ¤íŒŒì´ë”(Spider) ë˜ëŠ” ë´‡(Bot)ì´ë¼ê³  ë¶€ë¥´ê¸°ë„ í•˜ëŠ”ë°, ì˜ˆë¥¼ ë“¤ì–´ êµ¬ê¸€ì—ì„œ ìš°ë¦¬ê°€ ê²€ìƒ‰í•  ê²½ìš° ë¹ ë¥´ê²Œ ê²€ìƒ‰ í•  ìˆ˜ ìˆëŠ” ì´ìœ  ì¤‘ì˜ í•˜ë‚˜ëŠ” ë°”ë¡œ ì›¹ê²€ìƒ‰ ì—”ì§„ì´ ë¯¸ë¦¬ ì „ ì„¸ê³„ì˜ ì›¹ì‚¬ì´íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ê³  ì €ì¥í•¨ìœ¼ë¡œ ê°€ëŠ¥í•œ ì¼ì´ë‹¤. ë˜í•œ RSS ë¦¬ë”ëŠ” ì‚¬ëŒ ëŒ€ì‹  í¬ë¡¤ëŸ¬ê°€ RSS í”¼ë“œë¥¼ í™•ì¸í•˜ê³ , ë³€ê²½ ì‚¬í•­ì´ ìˆëŠ” ê²½ìš°ì— ì—…ë°ì´íŠ¸ëœ í•­ëª©ì´ ìˆë‹¤ê³  ì•Œë ¤ì£¼ê¸°ë„ í•œë‹¤. ê°€ì¥ ê³µê°í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì€ ë°”ë¡œ íŠ¸ìœ„í„°, í˜ì´ìŠ¤ë¶ ë“±ì˜ SNSì—ì„œ ì›¹ í˜ì´ì§€ URLì„ ê³µìœ í•˜ë©´ í˜ì´ì§€ì˜ ì œëª©ê³¼ ì´ë¯¸ì§€ë¥¼ ë¯¸ë¦¬ ë³´ê¸°ë¡œ ì¶œë ¥í•´ ì£¼ëŠ”ë°, ì´ê²ƒë„ í¬ë¡¤ëŸ¬ê°€ í•´ë‹¹ í˜ì´ ì§€ë¥¼ ë°©ë¬¸í•´ì„œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê¸° ë•Œë¬¸ì— ê°€ëŠ¥í•œ ê²ƒì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ í¬ë¡¤ëŸ¬ì™€ ìŠ¤í¬ë˜í•‘ ë‘ ìš©ì–´ì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€??? í¬ë¡¤ë§ ì›¹ í˜ì´ì§€ì˜ í•˜ì´í¼ë§í¬ë¥¼ ìˆœíšŒí•˜ë©´ì„œ ì›¹ í˜ì´ì§€ë¥¼ ë‹¤ìš´ë¡œë“œ í•˜ëŠ” ì‘ì—… ìŠ¤í¬ë˜í•‘ ë‹¤ìš´ë¡œë“œí•œ ì›¹ í˜ì´ì§€ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì‘ì—… ê·¸ë ‡ë‹¤ë©´ ìš°ë¦¬ëŠ” ê²°êµ­ í¬ë¡¤ë§í•œ í›„ ìŠ¤í¬ë˜í•‘ê¹Œì§€ í•˜ê²Œ ë˜ëŠ” ì¼ë ¨ì˜ ì‘ì—…ì„ ì•ìœ¼ë¡œ í•  ê²ƒì´ë¼ê³  ìƒê°í•œë‹¤. Wgetìœ¼ë¡œ í¬ë¡¤ë§í•˜ê¸°ê°€ì¥ ë¨¼ì € Wgetìœ¼ë¡œ í¬ë¡¤ë§í•˜ëŠ” ë²•ì„ ì†Œê°œ í•  ê²ƒì´ë‹¤. í¬ë¡¤ë§ê³¼ ìŠ¤í¬ë˜í•‘ì´ ë¬´ì—‡ì¸ì§€ ê°ì„ ì¡ê¸° ìœ„í•¨ì´ë‹¤. W","categories":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/categories/crawling/"}],"tags":[]},{"title":"hexoë¥¼ ì´ìš©í•œ ë¸”ë¡œê·¸ ë§Œë“¤ê¸°","slug":"1day","date":"2018-12-21T15:00:00.000Z","updated":"2020-01-10T19:58:16.175Z","comments":true,"path":"2018/12/22/1day/","link":"","permalink":"https://heung-bae-lee.github.io/2018/12/22/1day/","excerpt":"","text":"Hexoë¡œ ë‚˜ë§Œì˜ ë¸”ë¡œê·¸ ë§Œë“¤ê¸°! HexoëŠ” ì‰½ê³  ë¹ ë¥´ê³  ê°•ë ¥í•œ ë¸”ë¡œê·¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Node.jsë§Œ ì¡°ê¸ˆ ì•Œê³  ìˆë‹¤ë©´ ìì‹ ì˜ ì·¨í–¥ì— ë§ê²Œ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•  ìˆ˜ ìˆì–´ì„œ ì¢‹ìŠµë‹ˆë‹¤. ë˜í•œ í…œí”Œë¦¿ ì—”ì§„ìœ¼ë¡œëŠ” Swig ë˜ëŠ” EJS ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì‘ì„± ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì–¸ì–´ë¡œëŠ” HTML, Markdown, AsciiDoc ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ ê¸€ì€ ê·¸ ì¤‘ì—ì„œ Markdownìœ¼ë¡œ ì‘ì„±í•  ì˜ˆì •ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ì£¼ì†ŒëŠ” Hexoì˜ ê³µì‹ í™ˆí˜ì´ì§€ë¡œì„œ ê¸°ë³¸ì ì¸ ì‚¬ìš©ë²•ì„ ì•Œë ¤ì£¼ê³  ìˆìŠµë‹ˆë‹¤. Hexo ë§Œì•½, ë°”ë¡œ ì‚¬ìš©ë²•ì„ ì•Œê³  ì‹¶ë‹¤ë©´ documentationì„ í´ë¦­í•´ì£¼ì„¸ìš”! ìš”ì ë§Œ ê°„ëµíˆ ìš”ì•½í•˜ìë©´ HexoëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤. ì»¤ë§¨ë“œë¼ì¸(cmd)ìœ¼ë¡œ ê°„í¸í•˜ê²Œ í¬ìŠ¤íŠ¸ ìƒì„± ë° ê´€ë¦¬ ë§ˆí¬ë‹¤ìš´(Markdown) ì§€ì› SEO, ë°˜ì‘í˜• ì›¹ì„ ì§€ì›í•˜ëŠ” ë‹¤ì–‘í•œ í…Œë§ˆ npm ì„ ì´ìš©í•œ ê°„í¸í•œ í”ŒëŸ¬ê·¸ì¸ ì ìš© Github Pages, Netlify ë“±ì„ ì´ìš©í•œ í˜¸ìŠ¤íŒ…","categories":[{"name":"hexo","slug":"hexo","permalink":"https://heung-bae-lee.github.io/categories/hexo/"}],"tags":[]}]}