{"meta":{"title":"DataLatte's IT Blog","subtitle":"DataLatte's IT Blog using Hexo","description":"DataLatte가 Data Science 공부를 하면서 정리해 놓는 블로그","author":"HeungBae Lee","url":"https://heung-bae-lee.github.io","root":"/"},"pages":[],"posts":[{"title":"내가 정리하는 자료구조 00","slug":"data_structure_01","date":"2020-03-19T09:46:30.000Z","updated":"2020-03-19T12:53:16.920Z","comments":true,"path":"2020/03/19/data_structure_01/","link":"","permalink":"https://heung-bae-lee.github.io/2020/03/19/data_structure_01/","excerpt":"","text":"목표 기본 자료 구조/알고리즘 익히기 알고리즘 풀이를 위해, 기본적으로 알고 있어야 하는 자료구조와 알고리즘 정리 자료구조란? 용어: 자료구조 = 데이터 구조 = data structure 대량의 데이터를 효율적으로 관리할 수 있는 데이터의 구조를 의미 코드상에서 효율적으로 데이터를 처리하기 위해, 데이터 특성에 따라, 체계적으로 데이터를 구조화해야 함. 어떤 데이터 구조를 사용하느냐에 따라, 코드 효율이 달라진다. 효율적으로 데이터를 관리하는 예 우편번호 5자리 우편번호로 국가의 기초구역을 식별 5자리 우편번호에서 앞 3자리는 시,군,자치구를 의미, 뒤 2자리는 일련변호로 구성 학생관리 학년, 반, 번호를 학생에게 부여해서, 학생부를 관리 위같은 기법이 없었다면, 많은 학생 중 특정 학생을 찾기 위해, 전체 학생부를 모두 훑어야 하는 불편함이 생긴다. 대표적인 자료 구조 array(배열), stack(스택), queue(큐), linked list(링크드 리스트), hash table(해쉬 테이블), heap(힙) 등 알고리즘이란? 용어: 알고리즘(algorithm) 어떤 문제를 풀기 위한 절차/방법 어떤 문제에 대해, 특정한 ‘입력’을 넣으면, 원하는 ‘출력’을 얻을 수 있도록 만드는 프로그래밍 자료구조와 알고리즘이 중요한 이유 어떤 자료구조와 알고리즘을 쓰느냐에 따라, 성능이 많이 차이나기 때문이다. 결과적으로, 자료구조와 알고리즘을 통해 프로그래밍을 잘 할 수 있는 기술과 역량을 검증할 수 있다. 자료구조/알고리즘, 그리고 Python 어떤 언어로든 자료구조/알고리즘을 익힐 수 있다. 이전에는 무조건 C 또는 C++로만 작성하도록 하는 경향이 많았다. 최근에는 언어로 인한 제약/평가는 없어졌다고 봐도 무방할 것 같다. 그러므로, 가장 쉽고 진입장벽이 상대적으로 낮은 Python을 통해 필자는 정리해 볼 것이다. Array(배열) 데이터를 나열하고, 각 데이터를 인덱스에 대응하도록 구성한 데이터 구조 Python에서는 리스트 타입이 배열 기능을 제공한다. 1. 배열은 왜 필요할까? 같은 종류의 데이터를 효율적으로 관리하기 위해 사용 같은 종류의 데이터를 순차적으로 저장 장점: 빠른 접근 가능 첫 데이터의 위치에서 상대적인 위치로 데이터 접근(인덱스 번호로 접근) 단점: 데이터 추가/삭제의 어려움 미리 최대 길이를 지정해야 함 C 언어 예: 영어 단어 저장 배열을 생성할 때 먼저 길이를 정해주어야 하기 때문에, 아래와 같이 []안에 최대 길이를 지정해준 것을 확인할 수 있다. 123456789#include &lt;stdio.h&gt;int main(int argc, char * argv[])&#123; char country[3] = \"US\"; printf (\"%c%c\\n\", country[0], country[1]); printf (\"%s\\n\", country); return 0;&#125; 파이썬 언어 예: 영어 단어 저장12country = 'US'print (country) 2. 파이썬과 배열 파이썬에서는 리스트로 배열 구현 가능 1차원 배열: 리스트로 구현시12data_list = [1, 2, 3, 4, 5]data_list 2차원 배열: 리스트로 구현시12data_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]data_list 결과123456789101112131415print (data_list[0])# [1, 2, 3]print (data_list[0][0])# 1print (data_list[0][1])# 2print (data_list[0][2])# 3print (data_list[1][0])# 4print (data_list[1][1])# 5print (data_list[1][2])# 6 3. 프로그래밍 연습연습1: 위의 2차원 배열에서 9, 8, 7 을 순서대로 출력해보기1print(data_list[2][2], data_list[2][1], data_list[2][0]) 연습2: 아래의 dataset 리스트에서 전체 이름 안에 M 은 몇 번 나왔는지 빈도수 출력하기123456789101112131415161718192021222324252627282930dataset = ['Braund, Mr. Owen Harris','Cumings, Mrs. John Bradley (Florence Briggs Thayer)','Heikkinen, Miss. Laina','Futrelle, Mrs. Jacques Heath (Lily May Peel)','Allen, Mr. William Henry','Moran, Mr. James','McCarthy, Mr. Timothy J','Palsson, Master. Gosta Leonard','Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)','Nasser, Mrs. Nicholas (Adele Achem)','Sandstrom, Miss. Marguerite Rut','Bonnell, Miss. Elizabeth','Saundercock, Mr. William Henry','Andersson, Mr. Anders Johan','Vestrom, Miss. Hulda Amanda Adolfina','Hewlett, Mrs. (Mary D Kingcome) ','Rice, Master. Eugene','Williams, Mr. Charles Eugene','Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)','Masselmani, Mrs. Fatima','Fynney, Mr. Joseph J','Beesley, Mr. Lawrence','McGowan, Miss. Anna \"Annie\"','Sloper, Mr. William Thompson','Palsson, Miss. Torborg Danira','Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)','Emir, Mr. Farred Chehab','Fortune, Mr. Charles Alexander','Dwyer, Miss. Ellen \"Nellie\"','Todoroff, Mr. Lalio'] 123456m_count = 0for data in dataset: for index in range(len(data)): if data[index] == 'M': m_count += 1print (m_count)","categories":[{"name":"C/C++/자료구조","slug":"C-C-자료구조","permalink":"https://heung-bae-lee.github.io/categories/C-C-자료구조/"}],"tags":[]},{"title":"data engineering (데이터 모델링 및 챗봇 만들기)","slug":"data_engineering_10","date":"2020-03-03T14:29:20.000Z","updated":"2020-03-09T14:06:21.058Z","comments":true,"path":"2020/03/03/data_engineering_10/","link":"","permalink":"https://heung-bae-lee.github.io/2020/03/03/data_engineering_10/","excerpt":"","text":"Spotify 데이터 유사도 모델링 모든 track을 다 유클리디안 거리를 계산해서 유사도를 측정하기에는 많은 양이기 때문에 해당 Artist의 track들의 audio feature 데이터에 대해 평균을 낸 값을 사용하여 Artist 끼리의 유사도를 계산할 것이다. 해당 유사도를 계산하기 위해 아래와 같이 먼저 RDS에 접속하여 table을 생성해 준다. 123mysql -h spotify.cgaj5rvtgf25.ap-northeast-2.rds.amazonaws.com -P 3306 -u hb0619 -pCREATE TABLE related_artists (artist_id VARCHAR(255), y_artist VARCHAR(255), distance FLOAT, PRIMARY KEY(artist_id, y_artist)) ENGINE=InnoDB DEFAULT CHARSET=utf8; 그 다음은 우리가 미리 만들어놓았던 Athena의 DataBase는 무엇이었는지를 확인하자. 필자는 아래와 같이 따로 Database를 만들지 않고 default로 사용했다. 또한, 입력했었던 날짜를 확인해놓아야 추후에 코드 작성시 Athena로 접속하여 만들어진 테이블들을 참조할 수 있다. 필자는 Athena에 미리 만들어놓았던 두가지 top_tracks와 audio_features 테이블을 이용하여 유사도를 구하고 해당 유사도를 MySQL DB에 insert하는 방식으로 작업을 진행 할 것이다. data_modeling.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import sysimport osimport loggingimport pymysqlimport boto3import timeimport mathhost = \"end-point url\"port = you port numberusername = \"your MYSQL DB ID\"database = \"your MYSQL DB Name\"password = \"your MYSQL DB Password\"def main(): try: conn = pymysql.connect(host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") sys.exit(1) athena = boto3.client('athena') query = \"\"\" SELECT artist_id, AVG(danceability) AS danceability, AVG(energy) AS energy, AVG(loudness) AS loudness, AVG(speechiness) AS speechiness, AVG(acousticness) AS acousticness, AVG(instrumentalness) AS instrumentalness FROM top_tracks t1 JOIN audio_features t2 ON t2.id = t1.id AND CAST(t1.dt AS DATE) = DATE('2020-02-24') AND CAST(t2.dt AS DATE) = DATE('2020-02-24') GROUP BY t1.artist_id \"\"\" # 위에서 DATE를 정하는 부분에서 CAST(t1.dt AS DATE) = CURRENT_DATE - INTERVAL '1' DAY 이렇게 현재날짜를 기준으로 차이나는 기간을 통해 정해줄 수 있다. # 필자는 여러번 Athena에 실행하지 않았기 때문에 최근에 Athena에 만들어 놓은 위의 두 테이블의 데이터를 직접 보고 날짜를 지정했다. r = query_athena(query, athena) results = get_query_result(r['QueryExecutionId'], athena) artists = process_data(results) query = \"\"\" SELECT MIN(danceability) AS danceability_min, MAX(danceability) AS danceability_max, MIN(energy) AS energy_min, MAX(energy) AS energy_max, MIN(loudness) AS loudness_min, MAX(loudness) AS loudness_max, MIN(speechiness) AS speechiness_min, MAX(speechiness) AS speechiness_max, ROUND(MIN(acousticness),4) AS acousticness_min, MAX(acousticness) AS acousticness_max, MIN(instrumentalness) AS instrumentalness_min, MAX(instrumentalness) AS instrumentalness_max FROM audio_features \"\"\" r = query_athena(query, athena) results = get_query_result(r['QueryExecutionId'], athena) avgs = process_data(results)[0] metrics = ['danceability', 'energy', 'loudness', 'speechiness', 'acousticness', 'instrumentalness'] for i in artists: for j in artists: dist = 0 for k in metrics: x = float(i[k]) x_norm = normalize(x, float(avgs[k+'_min']), float(avgs[k+'_max'])) y = float(j[k]) y_norm = normalize(y, float(avgs[k+'_min']), float(avgs[k+'_max'])) dist += (x_norm-y_norm)**2 dist = math.sqrt(dist) ## euclidean distance data = &#123; 'artist_id': i['artist_id'], 'y_artist': j['artist_id'], 'distance': dist &#125; insert_row(cursor, data, 'related_artists') conn.commit() cursor.close()def normalize(x, x_min, x_max): normalized = (x-x_min) / (x_max-x_min) return normalizeddef query_athena(query, athena): response = athena.start_query_execution( QueryString=query, QueryExecutionContext=&#123; 'Database': 'default' &#125;, ResultConfiguration=&#123; 'OutputLocation': \"s3://spotify-chatbot-project/athena-panomix-tables/\", 'EncryptionConfiguration': &#123; 'EncryptionOption': 'SSE_S3' &#125; &#125; ) return response# 아래와 같이 Athena API는 response를 받았다고 해서 결과를 보여주는 것이 아니라 실행을 시킨 후에# 해당 query id를 통해 결과를 가져오는 형식으로 이루어져 있다.def get_query_result(query_id, athena): response = athena.get_query_execution( QueryExecutionId=str(query_id) ) while response['QueryExecution']['Status']['State'] != 'SUCCEEDED': if response['QueryExecution']['Status']['State'] == 'FAILED': logging.error('QUERY FAILED') break time.sleep(5) response = athena.get_query_execution( QueryExecutionId=str(query_id) ) # 중요한 점은 MaxResults가 1000이 Max라는 점이다. response = athena.get_query_results( QueryExecutionId=str(query_id) ) return responsedef process_data(results): columns = [col['Label'] for col in results['ResultSet']['ResultSetMetadata']['ColumnInfo']] listed_results = [] for res in results['ResultSet']['Rows'][1:]: values = [] for field in res['Data']: try: values.append(list(field.values())[0]) except: values.append(list(' ')) listed_results.append(dict(zip(columns, values))) return listed_resultsdef insert_row(cursor, data, table): placeholders = ', '.join(['%s'] * len(data)) columns = ', '.join(data.keys()) key_placeholders = ', '.join(['&#123;0&#125;=%s'.format(k) for k in data.keys()]) sql = \"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s\" % (table, columns, placeholders, key_placeholders) cursor.execute(sql, list(data.values())*2)if __name__=='__main__': main() 위의 파일을 실행시켜보자. 위의 python script 파일이 존재하는 path로 이동하여 아래 명령문을 실행시키면 실행에 완료될때까지 걸린 시간 또한 알 수 있다. 12345time python3 data_modeling.pyreal 28m11.013suser 1m36.141ssys 0m24.518s 이제 MySQL에 접속해서 데이터가 제대로 insert 됬는지 확인해 보자. 12345678910111213141516171819202122232425262728SELECT * FROM related_artists LIMIT 20;# 결과+------------------------+------------------------+-----------+| artist_id | y_artist | distance |+------------------------+------------------------+-----------+| 00FQb4jTyendYWaN8pK0wa | 00FQb4jTyendYWaN8pK0wa | 0 || 00FQb4jTyendYWaN8pK0wa | 01C9OoXDvCKkGcf735Tcfo | 0.366558 || 00FQb4jTyendYWaN8pK0wa | 02rd0anEWfMtF7iMku9uor | 0.327869 || 00FQb4jTyendYWaN8pK0wa | 02uYdhMhCgdB49hZlYRm9o | 0.595705 || 00FQb4jTyendYWaN8pK0wa | 03r4iKL2g2442PT9n2UKsx | 0.632109 || 00FQb4jTyendYWaN8pK0wa | 03YhcM6fxypfwckPCQV8pQ | 0.812604 || 00FQb4jTyendYWaN8pK0wa | 04gDigrS5kc9YWfZHwBETP | 0.498764 || 00FQb4jTyendYWaN8pK0wa | 04tBaW21jyUfeP5iqiKBVq | 0.322017 || 00FQb4jTyendYWaN8pK0wa | 0543y7yrvny4KymoaneT4W | 0.365608 || 00FQb4jTyendYWaN8pK0wa | 05E3NBxNMdnrPtxF9oraJm | 0.958604 || 00FQb4jTyendYWaN8pK0wa | 06HL4z0CvFAxyc27GXpf02 | 0.483454 || 00FQb4jTyendYWaN8pK0wa | 06nevPmNVfWUXyZkccahL8 | 0.0592581 || 00FQb4jTyendYWaN8pK0wa | 06nsZ3qSOYZ2hPVIMcr1IN | 0.39567 || 00FQb4jTyendYWaN8pK0wa | 085pc2PYOi8bGKj0PNjekA | 0.608243 || 00FQb4jTyendYWaN8pK0wa | 08avsqaGIlK2x3i2Cu7rKH | 0.328059 || 00FQb4jTyendYWaN8pK0wa | 09C0xjtosNAIXP36wTnWxd | 0.210568 || 00FQb4jTyendYWaN8pK0wa | 0BvkDsjIUla7X0k6CSWh1I | 0.606556 || 00FQb4jTyendYWaN8pK0wa | 0bvRYuXRvd14RYEE7c0PRW | 0.670187 || 00FQb4jTyendYWaN8pK0wa | 0C0XlULifJtAgn6ZNCW2eu | 0.70478 || 00FQb4jTyendYWaN8pK0wa | 0cc6vw3VN8YlIcvr1v7tBL | 0.716507 |+------------------------+------------------------+-----------+20 rows in set (0.01 sec) JOIN을 통해 각각 이름을 볼수있게 해주면서 가장 distance가 작은 즉 유사성이 큰 데이터 순서로 보여주길 원해 아래와 같은 query를 작성하여 실행시켰다. 그 결과, audio_features로만 모델링을 했음에도 비슷한 장르의 아티스트가 묶여있음을 확인할 수 있다. 123456789101112131415161718192021222324252627SELECT p1.name, p2.name, p1.url, p2.url, p2.distance FROM artists p1 JOIN (SELECT t1.name, t1.url, t2.y_artist, t2.distance FROM artists t1 JOIN related_artists t2 ON t2.artist_id = t1.id) p2 ON p2.y_artist=p1.id WHERE distance != 0 ORDER BY p2.distance ASC LIMIT 20;+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+| name | name | url | url | distance |+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+| Four Tops | Alan Jackson | https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4 | https://open.spotify.com/artist/4mxWe1mtYIYfP040G38yvS | 0.0241995 || Alan Jackson | Four Tops | https://open.spotify.com/artist/4mxWe1mtYIYfP040G38yvS | https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4 | 0.0241995 || Martha Reeves &amp; The Vandellas | Jimmy Ruffin | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | https://open.spotify.com/artist/0hF0PwB04hnXfYMiZWfJzy | 0.0258624 || Jimmy Ruffin | Martha Reeves &amp; The Vandellas | https://open.spotify.com/artist/0hF0PwB04hnXfYMiZWfJzy | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | 0.0258624 || George Harrison | Martha Reeves &amp; The Vandellas | https://open.spotify.com/artist/7FIoB5PHdrMZVC3q2HE5MS | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | 0.0272287 || Martha Reeves &amp; The Vandellas | George Harrison | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | https://open.spotify.com/artist/7FIoB5PHdrMZVC3q2HE5MS | 0.0272287 || Nik Kershaw | Elton John | https://open.spotify.com/artist/7kCL98rPFsNKjAHDmWrMac | https://open.spotify.com/artist/3PhoLpVuITZKcymswpck5b | 0.0272474 || Elton John | Nik Kershaw | https://open.spotify.com/artist/3PhoLpVuITZKcymswpck5b | https://open.spotify.com/artist/7kCL98rPFsNKjAHDmWrMac | 0.0272474 || Tammi Terrell | Kim Carnes | https://open.spotify.com/artist/75jNCko3SnEMI5gwGqrbb8 | https://open.spotify.com/artist/5PN2aHIvLEM98XIorsPMhE | 0.0279891 || Kim Carnes | Tammi Terrell | https://open.spotify.com/artist/5PN2aHIvLEM98XIorsPMhE | https://open.spotify.com/artist/75jNCko3SnEMI5gwGqrbb8 | 0.0279891 || Roger Daltrey | Arcade Fire | https://open.spotify.com/artist/5odf7hjI7hyvAw66tmxhGF | https://open.spotify.com/artist/3kjuyTCjPG1WMFCiyc5IuB | 0.0291541 || Arcade Fire | Roger Daltrey | https://open.spotify.com/artist/3kjuyTCjPG1WMFCiyc5IuB | https://open.spotify.com/artist/5odf7hjI7hyvAw66tmxhGF | 0.0291541 || Billy Fury | Otis Redding | https://open.spotify.com/artist/7rtLZcKWGV4eaZsBwSKimf | https://open.spotify.com/artist/60df5JBRRPcnSpsIMxxwQm | 0.0292248 || Otis Redding | Billy Fury | https://open.spotify.com/artist/60df5JBRRPcnSpsIMxxwQm | https://open.spotify.com/artist/7rtLZcKWGV4eaZsBwSKimf | 0.0292248 || Katy Perry | John Fogerty | https://open.spotify.com/artist/6jJ0s89eD6GaHleKKya26X | https://open.spotify.com/artist/5ujCegv1BRbEPTCwQqFk6t | 0.0302168 || John Fogerty | Katy Perry | https://open.spotify.com/artist/5ujCegv1BRbEPTCwQqFk6t | https://open.spotify.com/artist/6jJ0s89eD6GaHleKKya26X | 0.0302168 || Dierks Bentley | The Cadillac Three | https://open.spotify.com/artist/7x8nK0m0cP2ksQf0mjWdPS | https://open.spotify.com/artist/1nivFfWu6oXBFDNyVfFU5x | 0.0313435 || The Cadillac Three | Dierks Bentley | https://open.spotify.com/artist/1nivFfWu6oXBFDNyVfFU5x | https://open.spotify.com/artist/7x8nK0m0cP2ksQf0mjWdPS | 0.0313435 || Sheryl Crow | Phil Collins | https://open.spotify.com/artist/4TKTii6gnOnUXQHyuo9JaD | https://open.spotify.com/artist/4lxfqrEsLX6N1N4OCSkILp | 0.0317203 || Phil Collins | Sheryl Crow | https://open.spotify.com/artist/4lxfqrEsLX6N1N4OCSkILp | https://open.spotify.com/artist/4TKTii6gnOnUXQHyuo9JaD | 0.0317203 |+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+20 rows in set (1.18 sec) 이제 Facebook messenger API를 통해 챗봇을 서비스를 만들어 볼 것이다. 아래 그림과 같\u001d이 구글에서 Facebook messenger API를 검색하여 페이지로 접속한다. Facebook messenger API 웹페이지를 접속하면 아래 그림처럼 보이며, 그에 대한 작동원리에 대한 설명은 Introduction의 learn more를 클릭하면 두번째 그림과 같이 작동원리를 보여준다. 간단히 말하자면, User가 Facebook messenger API를 통해 질의하면 그 정보를 Business Server에 보내서 해당 질문에 따른 답변을 가져와 보여주는 형식이다. Facebook messenger API를 통해서는 다양한 방식의 답변을 제공할 수 있다. 이미 만들어져있는 UI/UX 템플릿들이 존재하기 때문에 원하는 형식에 맞춰 다양하게 서비스를 제공할 수 있다. Facebook messenger API를 사용하기 위해서는 가장 먼저 페이지가 만들어져 있어야 한다. 아래와 같이 새로운 페이지를 만들거나 이미 만들어져 있는 자신의 페이지를 먼저 등록시킨다. Lambda를 통해서 AWS와 Facebook messenger API를 연결해 볼 것이다. Lambda를 사용하는 이유는 지난번에 언급했던 것과 같이 EC2와 같이 서버를 항상 띄어놓고 정해진 resource를 통해 서비스를 관리하면 늘어나거나 줄어드는 User에 대해서 유연하게 처리하기 힘들기 때문이다. Lambda는 예를 들어 기하급수적으로 User가 늘더라도 그에따라 병렬적으로 작업하기 때문에 Traffic의 크기에 크게 영향을 받지 않는다. 반대로 EC2의 경우에는 해당 Traffic이 증가함에 따라 여러가지 장치를 구현해 놓아야한다. AWS에 로그인한 후 Lambda를 들어가서, 아래 그림과 같이 새로운 Lambda Function을 생성해 준다. 이제 위에서 만든 Lambda Function과 Facebook messenger API를 연결하기 위해서 AWS에서 API 관리 및 설정을 담당하는 API Gateway 페이지로 이동해서 새롭게 API Gate를 만들 것이다. 필자는 REST API 방식으로 생성할 것이기 때문에 아래와 같이 설정하였으며, API이름도 설정해 주었다. 그 다음은 Crawling할 때 한번씩 접해봤을 법한 GET, POST Method를 만들어 주는 과정을 거친다. 먼저 GET은 Integration type을 Lambda Function으로 설정해 주고, Lambda Function은 방금 만들어놓은 것을 사용할 것이다. 이렇게 설정한 뒤에 Integration Request 탭으로 이동하여 Mapping Templates의 Request body passthrough 아래와같이 설정하며, mapping Templates에 application/json을 추가해준다. Generate template을 Method Request passthrough로 설정한 후 최종적으로 save를 하여 GET method의 설정을 마친다. 필자가 사용할 서비스에서는 POST method는 Facebook API에 데이터를 주는 역할 밖에 없기 때문에 크게 설정할 것이 없다. GET, POST method를 다 설정했다면, 사용하기 위해서는 배포를 해야 할 것이다. 아래 그림과 같이 action버튼을 눌러 deploy api를 선택하여 stage를 새롭게 만들어주며, 이름을 설정한다. deploy를 다 완료하게 되면, 아래 그림과 같은 화면이 나타날 것이다. 그 중 아래 빨간색 상자 안에 있는 invoke URL은 우리가 Facebook에 연결해 줄 endpoint 역할을 한다. 추후에 invoke URL 주소를 복사한 후에 아래 그림에서와 같이 Facebook에서 만들어 놓은 app의 콜백 url 추가를 눌러 추가해 줄 것이다. 위의 그림 처럼 webhook url을 추가 해주려면 Lambda Function을 만들어 주어야 하는데 먼저 아래 그림에서와 토큰을 생성해서 복사한 후 Lambda Function을 아래 그림과 같이 작성해 준뒤에 webhook url을 추가해 줄 수 있다. 참고로 페이지 토큰은 Facebook app에서 page token을 생성하여 해당 값을 적어주고, verify token은 임으로 지정해주면 된다. 아래와 같이 Lambda Function을 수정하는 이유는 Facebook의 Webhook 사용법을 살펴보면 알 수 있다. 위에서 지정한 verify token과 아래 그림에서 처럼 API Gateway를 클릭하여 이전에 invoke URL의 주소를 복사하여 아래 그림과 같이 webhook url을 추가해준다. 이렇게 하면 connection은 완료한 상태이다. 이제 본격적으로 챗봇을 구현하기 위해 Lambda Function의 else 밑의 부분을 수정해 볼 것이다. 먼저 이전과 마찬가지로 Lambda Function은 S3에 올려 그 파일을 사용하기 위해 requirements.txt와 shell script를 포함하는 하나의 파일로 만들어 준다. 또한, AWS에서 S3에 새로운 bucket을 생성해준다. 필자는 아래와 같이 spotify-chat-bot이라는 이름으로 새롭게 bucket을 만들어 주었다. 전체적인 구조는 아래와 같다. 1234chatbot├── deploy.sh├── lambda_handler.py└── requirements.txt deploy.sh 12345678910#!/bin/bashrm -rf ./libspip install -r requirements.txt -t ./libsrm *.zipzip spotify.zip -r *aws s3 rm s3://spotify-chat-bot/spotify.zipaws s3 cp ./spotify.zip s3://spotify-chat-bot/spotify.zipaws lambda update-function-code --function-name spotify-lambda --s3-bucket spotify-chat-bot --s3-key spotify.zip 위와 같이 작성했다면 먼저 deploy.sh의 파일 권한을 바꿔준다. 모든 사용자(a)의 실행(x) 권한 추가(+)하여 준다. 1chmod +x deploy.sh requirements.txt 12requestspymysql lambda_hendler.py 12345678910111213141516171819202122232425262728293031# -*- coding: utf-8 -*-import syssys.path.append('./libs')import loggingimport requestsimport pymysqlimport fb_botimport jsonimport base64import boto3logger = logging.getLogger()logger.setLevel(logging.INFO)PAGE_TOKEN = \"your page token\"VERIFY_TOKEN = \"your verify code\"def lambda_handler(event, context): # event['params'] only exists for HTTPS GET if 'params' in event.keys(): if event['params']['querystring']['hub.verify_token'] == VERIFY_TOKEN: return int(event['params']['querystring']['hub.challenge']) else: logging.error('wrong validation token') raise SystemExit else: logger.info(event) 위와 같이 작성한 상태한 후 해당 파일이 존재하는 path에서 아래 shell script를 작동시킨다. 1./deploy.sh AWS S3에서 해당 bucket을 확인해 보면 아래와 같이 spotify.zip 파일이 존재함을 확인할 수 있다. 다시 AWS Lambda Function으로 돌아가 해당 bucket과 연결을 시켜 볼 것이다. Facebook App으로 돌아가서 아래 화면과 같이 필드를 추가해 주어야 한다. 이제 webhook으로 연결해 놓은 페이지로 접속하여 아래와 같이 버튼을 만들어 놓는다. 그 이유는 Lambda Function의 나머지 부분을 작성하기 위해서는 어떻게 event 구조가 구성되어져 있는지 확인해야 하기 때문이다. 이제 AWS Lambda Function을 통해 어떻게 message가 들어오는지 확인하기 위해 아래 그림과 같이 page에서 버튼 테스트를 진행하고, 메세지는 간단하게 hello를 입력해보았다. AWS CloudWatch에서 log를 살펴보면, 아래 그림과 같이 받아오는 것을 확인 할 수 있다. 아래 빨간색 상자안의 key 값 중 recipient는 해당 페이지의 id이며, sender의 id는 Facebook User의 id이다. 고유의 값은 아니고 각 페이지에 각 User에 대한 id이므로 동일한 User가 다른 페이지에서 요청을 했다면, 다른 id를 갖는다. app을 관리할 수 있는 python script 파일을 fb_bot.py라는 이름으로 작성해 주었다. 이 파일 또한 위의 lambda function내에 존재할 수 있도록 path를 잡아주어야 한다. Facebook app은 graph Facebook API를 통해서 control할 수 있다. 아래 패키지 중 Enum은 고유한 이름 집합과 값을 정의하는 데 사용할 수 있는 네 가지 열거형 클래스를 정의하는데 사용되어 진다. 아래함수에서 for문을 통해 NotificationType을 작동시킨다면, NotificationType.REGULAR, NotificationType.SILENT_PUSH, NotificationType.no_push 식으로 값이 프린트 된다. 아래 탬플릿에 맞는 형식은 Facebook messenger API에서 확인할 수 있다. fb_bot.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213#!/usr/bin/env pythonimport syssys.path.append(\"./libs\")import osimport requestsimport base64import jsonimport loggingfrom enum import EnumDEFAULT_API_VERSION = 6.0## messaging types: \"RESPONSE\", \"UPDATE\", \"MESSAGE_TAG\"class NotificationType(Enum): regular = \"REGULAR\" silent_push = \"SILENT_PUSH\" no_push = \"no_push\"class Bot: def __init__(self, access_token, **kwargs): self.access_token = access_token self.api_version = kwargs.get('api_version') or DEFAULT_API_VERSION self.graph_url = 'https://graph.facebook.com/v&#123;0&#125;'.format(self.api_version) @property def auth_args(self): if not hasattr(self, '_auth_args'): auth = &#123; 'access_token': self.access_token &#125; self._auth_args = auth return self._auth_args def send_message(self, recipient_id, payload, notification_type, messaging_type, tag): payload['recipient'] = &#123; 'id': recipient_id &#125; #payload['notification_type'] = notification_type payload['messaging_type'] = messaging_type if tag is not None: payload['tag'] = tag request_endpoint = '&#123;0&#125;/me/messages'.format(self.graph_url) response = requests.post( request_endpoint, params = self.auth_args, json = payload ) logging.info(payload) return response.json() def send_text(self, recipient_id, text, notification_type = NotificationType.regular, messaging_type = 'RESPONSE', tag = None): return self.send_message( recipient_id, &#123; \"message\": &#123; \"text\": text &#125; &#125;, notification_type, messaging_type, tag ) def send_quick_replies(self, recipient_id, text, quick_replies, notification_type = NotificationType.regular, messaging_type = 'RESPONSE', tag = None): return self.send_message( recipient_id, &#123; \"message\":&#123; \"text\": text, \"quick_replies\": quick_replies &#125; &#125;, notification_type, messaging_type, tag ) def send_attachment(self, recipient_id, attachment_type, payload, notification_type = NotificationType.regular, messaging_type = 'RESPONSE', tag = None): return self.send_message( recipient_id, &#123; \"message\": &#123; \"attachment\":&#123; \"type\": attachment_type, \"payload\": payload &#125; &#125; &#125;, notification_type, messaging_type, tag ) def send_action(self, recipient_id, action, notification_type = NotificationType.regular, messaging_type = 'RESPONSE', tag = None): return self.send_message( recipient_id, &#123; \"sender_action\": action &#125;, notification_type, messaging_type, tag ) def whitelist_domain(self, domain_list, domain_action_type): payload = &#123; \"setting_type\": \"domain_whitelisting\", \"whitelisted_domains\": domain_list, \"domain_action_type\": domain_action_type &#125; request_endpoint = '&#123;0&#125;/me/thread_settings'.format(self.graph_url) response = requests.post( request_endpoint, params = self.auth_args, json = payload ) return response.json() def set_greeting(self, template): request_endpoint = '&#123;0&#125;/me/thread_settings'.format(self.graph_url) response = requests.post( request_endpoint, params = self.auth_args, json = &#123; \"setting_type\": \"greeting\", \"greeting\": &#123; \"text\": template &#125; &#125; ) return response def set_get_started(self, text): request_endpoint = '&#123;0&#125;/me/messenger_profile'.format(self.graph_url) response = requests.post( request_endpoint, params = self.auth_args, json = &#123; \"get_started\":&#123; \"payload\": text &#125; &#125; ) return response def get_get_started(self): request_endpoint = '&#123;0&#125;/me/messenger_profile?fields=get_started'.format(self.graph_url) response = requests.get( request_endpoint, params = self.auth_args ) return response def get_messenger_profile(self, field): request_endpoint = '&#123;0&#125;/me/messenger_profile?fields=&#123;1&#125;'.format(self.graph_url, field) response = requests.get( request_endpoint, params = self.auth_args ) return response def upload_attachment(self, url): request_endpoint = '&#123;0&#125;/me/message_attachments'.format(self.graph_url) response = requests.post( request_endpoint, params = self.auth_args, json = &#123; \"message\":&#123; \"attachment\":&#123; \"type\": \"image\", \"payload\": &#123; \"is_reusable\": True, \"url\": url &#125; &#125; &#125; &#125; ) return response 이제 위의 fb_bot.py를 import하여 lambda_hendler.py 파일을 아래와 같이 수정해 주었다. lambda_hendler.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199# -*- coding: utf-8 -*-import syssys.path.append('./libs')import loggingimport requestsimport pymysqlimport fb_botimport jsonimport base64import boto3logger = logging.getLogger()logger.setLevel(logging.INFO)client_id = \"Your Spotify ID\"client_secret = \"Your Spotify PW\"PAGE_TOKEN = \"Your Page Token\"VERIFY_TOKEN = \"Your verify token\"host = \"Your RDS End point\"port = 3306username = \"Your RDS ID\"database = \"Using RDS table name\"password = \"Your RDS PW\"try: conn = pymysql.connect(host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor()except: logging.error(\"could not connect to rds\") sys.exit(1)bot = fb_bot.Bot(PAGE_TOKEN)def lambda_handler(event, context): # event['params'] only exists for HTTPS GET if 'params' in event.keys(): if event['params']['querystring']['hub.verify_token'] == VERIFY_TOKEN: return int(event['params']['querystring']['hub.challenge']) else: logging.error('wrong validation token') raise SystemExit else: messaging = event['entry'][0]['messaging'][0] user_id = messaging['sender']['id'] logger.info(messaging) artist_name = messaging['message']['text'] query = \"SELECT image_url, url FROM artists WHERE name = '&#123;&#125;'\".format(artist_name) cursor.execute(query) raw = cursor.fetchall() # raw가 0인 경우는 DB안에는 존재하지 않으므로 Spotify API에서 직접 search한다. if len(raw) == 0: text = search_artist(cursor, artist_name) bot.send_text(user_id, text) sys.exit(0) image_url, url = raw[0] payload = &#123; 'template_type': 'generic', 'elements': [ &#123; 'title': \"Artist Info: '&#123;&#125;'\".format(artist_name), 'image_url': image_url, 'subtitle': 'information', 'default_action': &#123; 'type': 'web_url', 'url': url, 'webview_height_ratio': 'full' &#125; &#125; ] &#125; bot.send_attachment(user_id, \"template\", payload) query = \"SELECT t2.genre FROM artists t1 JOIN artist_genres t2 ON t2.artist_id = t1.id WHERE t1.name = '&#123;&#125;'\".format(artist_name) cursor.execute(query) genres = [] for (genre, ) in cursor.fetchall(): genres.append(genre) text = \"Here are genres of &#123;&#125;\".format(artist_name) bot.send_text(user_id, text) bot.send_text(user_id, ', '.join(genres)) ## 만약에 아티스트가 없을시에는 아티스트 추가 ## Spotify API hit --&gt; Artist Search ## Database Upload ## One second ## 오타 및 아티스트가 아닐 경우def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersdef insert_row(cursor, data, table): placeholders = ', '.join(['%s'] * len(data)) columns = ', '.join(data.keys()) key_placeholders = ', '.join(['&#123;0&#125;=%s'.format(k) for k in data.keys()]) sql = \"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s\" % (table, columns, placeholders, key_placeholders) cursor.execute(sql, list(data.values())*2)# 추가적으로 S3의 top_tracks에도 업데이트해주기 위해서 top-tracks lambda function을 실행시켜주는 함수이다.# payload 부분은 lambda_handler 함수 안에서 들어오는 event에 관한 부분이다.def invoke_lambda(fxn_name, payload, invocation_type='Event'): lambda_client = boto3.client('lambda') invoke_response = lambda_client.invoke( FunctionName = fxn_name, InvocationType = invocation_type, Payload = json.dumps(payload) ) if invoke_response['StatusCode'] not in [200, 202, 204]: logging.error(\"ERROR: Invoking lmabda function: '&#123;0&#125;' failed\".format(fxn_name)) return invoke_responsedef search_artist(cursor, artist_name): headers = get_headers(client_id, client_secret) ## Spotify Search API params = &#123; \"q\": artist_name, \"type\": \"artist\", \"limit\": \"1\" &#125; r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) raw = json.loads(r.text) if raw['artists']['items'] == []: return \"Could not find artist. Please Try Again!\" artist = &#123;&#125; artist_raw = raw['artists']['items'][0] if artist_raw['name'] == params['q']: artist.update( &#123; 'id': artist_raw['id'], 'name': artist_raw['name'], 'followers': artist_raw['followers']['total'], 'popularity': artist_raw['popularity'], 'url': artist_raw['external_urls']['spotify'], 'image_url': artist_raw['images'][0]['url'] &#125; ) for i in artist_raw['genres']: if len(artist_raw['genres']) != 0: insert_row(cursor, &#123;'artist_id': artist_raw['id'], 'genre': i&#125;, 'artist_genres') insert_row(cursor, artist, 'artists') conn.commit() r = invoke_lambda('top-tracks', payload=&#123;'artist_id': artist_raw['id']&#125;) print(r) return \"We added artist. Please try again in a second!\" return \"Could not find artist. Please Try Again!\" 위에서 다른 trigger lambda function을 참조할 수 있도록 설정해 주었기 때문에 아래와 같이 role을 다른 lambda function을 invoke 할 수 있도록 IAM 페이지에서 permission을 추가해 주어야 정상적으로 작동된다. 이제 해당 페이지의 test를 진행해 보면 아래와 같이 위에서 설정한 것 처럼 해당아티스트의 url과 장르를 보내주는 것을 확인 할 수 있다. 그림은 없지만 필자는 2PM도 검색해보았는데, 잘 검색되어 나왔다. 이제껏 data engineering에 관한 몇가지 기초적인 부분들을 실습해 보며, data engineer는 상황에 맞춰 resource를 사용할 수 있도록 선택과 집중을 해야 한다고 생각했다. 그러한, 상황에 맞는 선택과 집중을 위해 해당 비즈니스가 처해있는 상황과 단계를 잘 진단하고 깊게 알고있어야 할 것 같다는 생각을 하게 되는 프로젝트 였다.","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering (데이터 파이프라인 자동화)","slug":"data_engineering_09","date":"2020-03-01T04:49:06.000Z","updated":"2020-03-02T18:05:24.919Z","comments":true,"path":"2020/03/01/data_engineering_09/","link":"","permalink":"https://heung-bae-lee.github.io/2020/03/01/data_engineering_09/","excerpt":"","text":"데이터 워크 플로우 이전에도 언급했었듯이 데이터 파이프라인은 아래와 같은 서비스들을 S3에 모아 Athena같은 서비스로 분석해준 뒤 그 결과를 저장해놓은 일련의 데이터 작업의 흐름을 일컫는다. 하나의 job이 시작되거나 어떠한 event에 trigger가 됬을때, 또 다른 job으로 연결이 되는 이런 정보들을 DAGs(Directed Acyclic Graphs)라고 부른다. ETL 보통은 Extract -&gt; Transform -&gt; Load순으로 작업을 해 왔지만, 최근에는 Extract -&gt; Load -&gt; Transform 순으로 작업을 하기도 한다. 데이터 파이프 라인의 연장선이다. 하나의 예시를 들자면, 하루의 정해진 시간에 의한 스케쥴링인 Amazon CloudWatch Event와 Automation Script를 통해서 machine이 시작하면, AWS안에 AWS Step Functions는 각 과정에서 그 다음과정으로의 연결에 대한 여러가지 경우의 수에 대한 룰을 정해놓는 서비스로 쉽게 말하면, 임의의 단계에서 fail이 일어나면 어떤 event를 발생시켜야 하고, success를 하면 어떤 event를 발생시켜야 하는지를 관리할 수 있도록 도와주는 서비스이다. 이런 Step Function안의 ETL Flow state machine이 시작하고, 이후에는 다양한 job들이 작동하게 된다. 이러한 ETL job들의 log를 CloudWatch에 저장을 하고, 아래와 같은 Flow를 갖게된다. AWS의 Step function에 관해 조금 더 말하자면, 아래 그림과 같이 사용할 수 있다. start가 되면 job이 submit이 되고, job이 finish될때 까지 기다려 줄 수 있게끔 Wait Seconds를 사용할 수도 있다. 왜냐하면, 예를 들어 Athena는 어느 정도 빅데이터를 처리하는 시스템이기 때문에 MySQL이나 PostgreSQL보다는 느린 부분이 있다. 이런 경우 위와 같이 time sleep을 통해 python script를 잠깐 멈춰두고 그 다음에 해당 시간이 지났을때 그 query에 대한 결과들을 가져올 수 있다. 이후에는 다시 job status를 받고 job이 끝났는지 아닌지에 따라 작업을 진행하는 flow를 볼 수 있다. 이런 service들이 없었을 때는 하나하나 monitoring을 통해서 수동으로 관리를 해야 했다. AWS Glue가 가장 좋은 부분은 이전에는 MySQL 같은 경우에는 만들어 놓은 Schema에 맞춰서 data를 insert하였는데, 이제는 data가 너무나 방대해지고 형식도 다 다른데, 이런 것들을 통합하는는 다 Glue한다는 의미의 서비스라는 점이다. 가장 많이 쓰여지는 부분 중에 하나가 Crawler인데 Crawler를 사용하면 자동으로 해당 data를 Crwaling해서 data가 어떤 형식인\b지에 대해서 지속적으로 Schema 관리가 들어가는 부분이 있다. 그러므로 data 양도 너무나 많고 column도 너무나 많은데 column이 변하는 경우도 있을 경우에 사용하면 좋다. AWS Glue 페이지를 보면 아래 그림과 같이 table과 ETL, Trigger등 다양한 작업을 할 수 있다. 한 가지 예시로 S3에 저장해놓은 python Script를 Jobs 탭에서 바로 수정가능하며, Trigger들도 등록해서 관리 할 수 있다. 해당 job들은 step function이나 Glue를 통해 관리를 하거나, EC2에서 Crontab으로 스케쥴링의 변화를 통해서 관리를 하는 등 다양한 방법으로 관리를 하지만 아래와 같이 서비스들의 지속적인 monitoring을 통해 cost를 효율적으로 사용할 선택과 집중을 해야 할 것이다. 어떤 부분까지 monitoring을 할 것인지에 대해 선택하여 집중하는 것이다. Crontab이란? 여러가지 만들어진 data job들을 우리가 작성한 해당 코드를 특정한 시간이나 하루에 한번 일주일에 한번이 됐건 어떠한 스케쥴링 방법을 통해서 지속적으로 작동시키려고 할때 사용한다. Crontab Quick reference 이러한 스케쥴링을 하려면 규칙에의한 명령이 존재할 것임을 눈치챘을 것이다. 아래 그림과 같이 모두 ‘*‘이면 계속 1분마다 작동하라는 의미이며, 순서대로 분, 시간, 일, 월, 요일 순으로 지정할 수 있다. 한가지 간단한 예로는 아래와 같이 Crontab을 실행하면 매일 오후 6시 30분에 /home/someuser/tmp안의 모든 파일을 제거하라는 job을 스케쥴링할 수 있다. 130 18 * * * rm /home/someuser/tmp/* 이제 Crontab을 실습해 보기 위해 EC2 서버를 하나 AWS에서 생성해 볼 것이다. 먼저 service 탭에서 EC2를 클릭하여 아래 그림과 같이 EC2 service 페이지로 이동한 후 다시 instance 탭으로 이동한다. Launch instance 버튼을 클릭하면 생성할 instance의 환경을 설정하는 페이지로 이동하게된다. Crontab 실습을 위한 서버이기 때문에 사양이 좋은 것을 고르진 않을 것이다. 필자는 Free tier만 사용가능한 2번째 사양을 고를 것이다. 다만, 실무에서 사용할 경우는 각자의 상황에 맞는 서버의 크기와 성능을 골라서 사용해야 할 것이다. 필자는 Free Tier만 가능한 t2.micro type을 선택하였다. Configure instance 탭에서는 개수를 지정하는 등 instance에 대한 설정을 하는 탭인데, 필자는 default값을 사용하기로 생각해서 아무런 설정도 하지않고 넘어갔다. Add storage 탭은 말 그대로 저장 성능을 설정하는 탭으로 필자는 이부분도 별다른 설정없이 기본값으로 하고 넘어갔다. tag를 설정하는 탭이며, 필자는 생략하였다. Configure Security Group 탭은 Security Group의 설정에 관한 탭이며, 새로 만들수도 있고, 이전에 생성되어있는 그룹을 사용해도 무방하다. 허나, 동일한 inbound 규칙을 사용하지 않는다면 새롭게 생성해서 사용하는 것이 좋다. 필자는 새롭게 만들어서 사용할 것이다. 또한, 접근을 ssh로 할 것이므로 아래와 같이 설정한다. 마지막으로 Review 탭은 이제까지 설정한 모든 사항을 점검하고 마지막으로 접속시 사용할 key pair를 어떤것으로 할지 정해주고 나면 모든 과정이 끝이 난다. 이제 다시 EC2의 instance 탭을 살펴보면, 새롭게 EC2 서버가 생성된 것을 확인 할 수 있다. 이제 접속을 해볼 것인데, 아래 그림에서 처럼 자신의 pem 파일이 존재하는 path에서 public DNS 서버 주소를 같이 입력하고 접속하면 된다. 아래와 같이 command를 실행하면 계속진행할 것이냐는 물음이 나올텐데 yes라고 하면 된다. 이는 앞으로 계속해서 접속하기위해 이 DNS를 추가할 것이냐는 물음이다. 1ssh -i pem_file_name.pem ec2-user@Public_DNS 이제 Crontab을 실행할 script 파일을 정해야 하는데, 필자는 이전에 만들어 두었던 script 파일 중에 top_tracks와 audio feature들에 대한 데이터를 S3에 parquet화하여 저장하게끔 코드를 작성한 파일을 사용할 것이다. 아래와 같이 필자의 pem 파일은 data_engineering이라는 파일 안에 존재한다. 123456789101112131415161718192021data_engineering├── Code│ ├── Chatbot\\ Project│ │ ├── deploy.sh│ │ ├── fb_bot.py│ │ ├── lambda_handler.py│ │ └── requirements.txt│ ├── artist_list.csv│ ├── audio_features.parquet│ ├── create_artist_genres_table.py│ ├── create_artist_table.py│ ├── dynamodb_insert.py│ ├── select_dynamodb.py│ ├── spotify_S3.py│ ├── spotify_s3_artist.py│ ├── spotify_s3_make.py│ ├── top-tracks.parquet│ ├── Slide├── foxyproxy-settings.xml└── spotift_chatbot.pem 위와 같은 구조로 되어있으므로 사용할 spotify_s3_make.py을 생성한 EC2 Server로 옮겨 줄 것이다. scp은 서버로 파일을 copy하는 것이라고 생각하면 된다. 단,아래 명령어는 이미 EC2에 접속한 상태가 아닌 로컬에서 진행하여야한다. 아래와 같이 옮겨진것을 볼 수 있고, EC2 서버로 접속하여 확인가능하다. 12345scp -i spotift_chatbot.pem ./Code/spotify_s3_make.py ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.com:~/spotify_s3_make.py 100% 5411 536.2KB/s 00:00ssh -i spotift_chatbot.pem ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.comls 만일 EC2 서버의 어떤 폴더를 만들어 놓았는데 해당 폴더안으로 이동시키고 싶다면 아래와 같이 하면된다. 1scp -i spotift_chatbot.pem ./Code/spotify_s3_make.py ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.com:~/파일이름 이제 script를 실행하기에 앞서서 작동할 수 있게끔 먼저 환경을 만들어주어야 할 것이다. 가장 먼저 python3가 설치되어있는지를 확인해 보자. 123456789101112131415161718sudo yum list | grep python3# 위의 명령문을 실행한 후 python3x에 관한 리스트가 나온다면 python3가 설치되어있는 상태이다.# 허나 잘 모르겠다면 아래 명령어를 통해 설치하도록하자.sudo yum install python36 -y# 그 다음은 pip를 설치해야한다curl -O https://bootstrap.pypa.io/get-pip.pysudo python3 get-pip.py#이제 script파일을 run해본 후에 필요한 패키지들을 추가로 설치해준다.python3 spotify_s3_make.pypip install boto3 --userpip install requests --userpip install pymysql --userpip install pandas --userpip install jsonpath --user 이제 본격적으로 crontab을 실습해 볼 것이다. 보통 crontab은 아래 두 가지를 많이 사용한다. crontab 파일은 vim editor를 사용하는데, 해당 스케쥴을 시작할때 email을 보내주는 기능도 있다. 우선 작동시키고 싶은 파일과 파이썬의 위치를 알고있어야 한다. UTC 시간 변경 123456789pwd# 결과/home/ec2-userwhich python3# 결과/usr/bin/python3 12345678910111213141516sudo service crond startcrontab -l# crontab 파일 쓰는 vim이 켜진 후# 매일 18시 30분에30 18 * * * /usr/bin/python3 /home/ec2-user/spotify_s3_make.py# vim으로 저장한 후 나오면 아래와 같은 메세지가 나와야 정상적으로 crontab을 설정한 것이다.crontab: installing new crontab# 어떠한 사항을 crontab으로 스케쥴링하고 있는지 확인하기crontab -l# 서버는 UTC를 사용하기 때문에 date 명령어를 통해 현재 서버가 몇시인지를 확인하고 우리나라 시간과 맞추도록 crontab을 설정해 주어야 한다.date 마이크로서비스에 대한 이해 전체적인 수집 프로세스에 대한 설명을 다시 한번 짚고가자면, Unknown Artist가 챗봇 메세지로 들어왔을 경우 AWS Serverless Lamda 서비스를 통해 Spotify API에 Access를 하고 그리고 해당 데이터가 Top Tracks, Artist Table에 가야되는지 또는 S3에 가야되는지를 관리를 하게 될 것이다. 또한, Ad Hoc Data Job을 통해 하루에 한번이라던지, 직접 로컬에서 command line을 통해 데이터를 가져올 수도 있게된다. Lambda가 필요한 이유는 우리가 Unknown Artist가 챗봇 메세지로 들어왔을때 내용을 업데이트를 해야되는데, 보통 사람들이 기대하는 챗봇은 업데이트를 바로 해주어서 원하는 정보를 얻을 수 있게끔 해주어야 하기에 이렇게 바로 업데이트를 할 수 있게\u001d끔 Lambda라는 서비스를 통해서 해결 할 수 있다. 이런 Lambda는 마이크로서비스의 개념인데, monolithic 이라는 개념의 반대이다. monolithic은 하나의 서비스를 만들때 크게 프로젝트 단위로 만들어 놓고 관리를 해주는 개념으로써 관리에 있어서 전체 프로세스가 보이기 때문에 컨트롤하기 쉬운 부분이 있다. 이에 반해 마이크로서비스는 세세한 작업하나씩을 단위로 관리를 하는 것이다. 챗봇을 Lambda로 구현하는 이유는 Serverless는 하나의 Function이기 때문에 Stateless라고도 하는데 지금 상태가 어떤지 모르겠다는 의미이다. 예를들어, 어떠한 User\u001c 어떤 메시지를 보냈다고 가정하면, Lambda Function에는 이전의 어떠한 메시지를 갖고 있었는지를 담고\u001c있을 수 없다. 상태가 없는 Function이라고 생각하면 될 것 같다. 그러므로 이런 State를 관리할만한 데이터베이스가 필요할 것이다. 주로 메시지에 특화된 DynamoDB를 사용할 것이다. 또한 Lambda의 경우에는 해당 서비스의 User가 기하급수적으로 늘어났을 때 병렬로 늘어나기 때문에 제한점이 서버로 구현하는것보단 덜하다는 장점도 있다. 서버의 경우에는 메모리나 CPU의 제한된 성능으로 구축된 동일한 서버를 통해 1명에게 서비스하는 것과 백만명에게 서비스하는 것은 완전 다를것이다. 또 한가지 좋은 점은 지속적으로 띄워져 있는 것이 아니라 필요할 때 띄워서 사용한 만큼만 비용을 지불한다는 점이다. Crontab을 통하여 Lambda를 호출할 수도 있고, Lambda가 Lambda를 호출할 수도 있다. 본격적으로 Lambda Function을 만들 것이다. 먼저 AWS Lambda Function 페이지로 이동하여 아래와 같이 Create Function 버튼을 클릭한다. 이번 Lambda Function은 이전에 DynamoDB에 top track정보를 DynamoDB에 저장했었는데, Artist가 추가된다면 DynamoDB에도 저장되어야하므로 이 작업을 작성해 볼 것이다. 아래 그림에서와 같이 Author from scratch는 기본적인 예제를 통해 시작하는 부분이고, Use a blueprint는 흔하게 사용되는 경우들을 코드로 제공하는 부분이다. 마지막은 App repository에서 바로 연결해서 사용하는 것이다. 필자는 Scratch로 진행할 것이다. Lambda는 하나의 Functiond이므로 제한점도 있을 것이다. 아래 그림과 같이 Function의 이름을 정하고 function의 language를 정한다. 또한 가장 중요한 부분인 Permission부분이 남았는데, 이 부분은 현재 필자가 진행할 Function의 목표는 DynamoDB에 새로운 데이터를 추가하는 것이므로 DynamoDB에 대한 permission을 갖고 있어야 오류가 없을 것이다. 그렇기에 2번째 부분인 Use an exsiting role을 클릭하여 사용해야 하는데, 새롭게 규칙을 추가해서 사용하면 error가 어떻게 발생하는 지를 보기위해 우선 첫번째를 선택하였다. 물론 무조건적으로 있던 규칙을 사용하는 것이 아니라 상황에 맞춰 사용해야한다. function을 생성하면 아래 그림과 같이 여러가지 설정 및 작업을 할 수 있는 페이지가 나온다. 아래 부분으로 내려보면 다음과 같이 Function의 코드를 작성할 수 있는 부분이 존재한다. 이 곳에서 Function을 정의할 것이다. 허나, Edit code inline은 거의 사용하지 못하는 설정이라고 볼 수 있다. 해당 Lambda Function은 Linux 기반의 AMI compute system에 있는데, 함수를 동작할 수 있는 패키지들이 아무런 설치나 설정이 되어있지 않은 상태이기 때문이다. 그래서 zip file을 업로드하거나 S3에서 불러오는 방식을 보통 채택한다. 필자는 S3에서 불러오는 방식을 사용할 것이다. 위에서 함수에 사용될 변수들을 정의할 수 있다. 예를 들어, Spotify API에 접속하기 위해서는 ID와 Secret Key가 필요했는데 이 부분을 코드에 적기 보다는 보안의 문제로 따로 변수로 처리해 둔 뒤 함수에는 그 값을 받아 사용하는 형식으로 사용된다. 예를 들면 아래 그림에서 환경 변수의 Edit을 클릭하면 변수의 Key와 Value를 입력하여 추가하게끔 되어있는데 추가했다고 가정하면 함수에서는 os.environ.get(key)로 값을 받으면 된다. 또한 Basic settings부분은 Memory(Max : 3GB)와 timeout(Max : 15분)을 설정할 수 있는 부분인데, 가장 간단하고 명료하게 병렬적으로 분산처리를 할 수 있도록 코드를 작성하는 것이 좋다는 Lambda의 특징을 잘 보여주는 부분이다. 여기서 Memory는 크게 해 놓아도 사용한 만큼만 비용을 지불하는 것이므로 상관없다는 것에 유의하자. 이미 해당 Artist가 존재하는 것은 확인한 상태여서 해당 Artist ID에 대한 top track만 확보하고 싶은 경우라고 가정할 것이다. 그 ID 값을 이 Lambda Function에 보내 줌으로써 다시 한번 Spotify API에 hit을 하여 top track 정보를 가져오고 다시 DynamoDB에 저장하도록 할 것이다. 본격적으로 Lambda Function을 만들어 볼 것이다. 우선 새로운 폴더를 만들어준다. 그 안에는 Lambda Function에 관한 것들만 담기 위해서이다. 위에서 언급 한 것처럼 우선 아래 패키지들을 shell script를 통해 실행하기 위해서 requirements.txt를 작성할 것이다. 헌데 AWS에는 기본적으로 boto3가 설치되어져 있고 나머지들은 기본 내장 패키지들이므로 requests만 설치해 주면 될 것 같다. requirements.txt 1requests 위에서 만들어 놓은 requirements.txt안의 패키지를 -t옵션의 target 파일에 저장한다는 의미이다. 헌데 이렇게 home이 아닌 파일에 저장하게 되면 pointer issue 때문에 error가 발생하는데 이는 새롭게 setup.cfg라는 파일안에 아래와 같이 작성한 후에 저장해주면 해결된다. 다시 아래의 명령문을 실행하면 error 없이 libs안에 설치가 될 것이다. 1pip install -r requirements.txt -t ./libs setup.cfg 12[install]prefix= 매번 우리가 AWS CLI를 통해 명령문을 치고 실행할 수 없으므로 shell script로 작성해 준다. deploy.sh 1234567891011#!/bin/bashrm -rf ./libspip3 install -r requirements.txt -t ./libsrm *.zipzip top_tracks.zip -r *aws s3 rm s3://top-tracks-lambda/top_tracks.zipaws s3 cp ./top_tracks.zip s3://top-tracks-lambda/top_tracks.zipaws lambda update-function-code --function-name top-tracks --s3-bucket top-tracks-lambda --s3-key top_tracks.zip 또한, 위에서 lambda function을 update할 s3 bucket을 지정했으므로 새롭게 위의 이름으로 생성한다. 이 단계는 먼저 bucket을 만든뒤에 앞의 shell script를 만들어도 무관하다. lambda_function.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import syssys.path.append('./libs')import osimport boto3import requestsimport base64import jsonimport loggingclient_id = \"Your Spotify Developer ID\"client_secret = \"Your Spotify Developer PW\"try: dynamodb = boto3.resource('dynamodb', region_name='ap-northeast-2', endpoint_url='http://dynamodb.ap-northeast-2.amazonaws.com')except: logging.error('could not connect to dynamodb') sys.exit(1)def lambda_handler(event, context): headers = get_headers(client_id, client_secret) table = dynamodb.Table('top_tracks') artist_id = event['artist_id'] URL = \"https://api.spotify.com/v1/artists/&#123;&#125;/top-tracks\".format(artist_id) params = &#123; 'country': 'US' &#125; r = requests.get(URL, params=params, headers=headers) raw = json.loads(r.text) for track in raw['tracks']: data = &#123; 'artist_id': artist_id &#125; data.update(track) table.put_item( Item=data ) # AWS CloudWatch에서 Log기록을 살펴볼때 확인하기 위해 return 값을 Success로 주었다. return \"SUCCESS\"def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersif __name__=='__main__': main() 위와 같이 파일을 모두 작성했다면 아래와 같은 구조로 만들어져 있을 것이다. 12345top_tracks├── deploy.sh├── lambda_function.py├── requirements.txt└── setup.cfg 이제 top_tracks의 path에서 아래와 같이 shell script를 실행시켜준다. 1234./deploy.sh# 결과는 아래와 같이 permission denied가 발생할 것이다.-bash: ./deploy.sh: Permission denied Permission 권한을 바꿔주기 위해 다음의 코드를 실행한뒤 다시 shell script를 실행시킨다. 123chmod +x deploy.sh./deploy.sh 위의 코드들을 실행한 뒤 다시 AWS Lambda Function의 페이지로 돌아가 보면 아래와 같이 해당 파일들을 사용할 수 있게 설정이 되어져 있는 것을 확인 할 수 있다. 보통은 아래와 같이 sensitive한 정보들(client_id, client_secret) 같은 정보들은 따로 config.py 파일을 만들어서 모아놓는다. 이런 config.py파일은 해당 관리자만 가지고 있도록 하여 보안에 유지하나, 필자는 혼자 사용하므로 따로 만들지 않았다. 만약 위에서 Function Code내에서는 sensitive한 정보들이 안보이도록 하려면 아래 그림과 같이 environment variable을 추가하고, 코드를 작성하면된다. key값과 value값이 문자라도 기호를 사용하지 않고 넣으면 된다. 이제 Lambda Function을 Test해 볼 차례이다. Test 버튼을 클릭해 준 뒤, event의 artist_id값을 주기 위해 RDS에 접속해서 임의의 ID 하나를 필자는 가져왔다. Test 데이터를 저장하면 아래 그림과 같이 테스트할 파일로 바뀌었다는 것을 확인 할 수 있다. 이제 Test 버튼을 클릭한다. Log를 보니 Error가 발생한 것 같으므로 먼저 확인해 볼 것이다. Log는 AWS CloudWatch 서비스에서 확인할 수 있다. 아래 로그 중 빨간색 박스 부분을 살펴보면 해당 role에 대한 permission이 없기 때문에 발생한 error임을 확인 할 수 있다. 처음에 Lambda Function을 만들때 말했듯이 permission이 없기 때문에 발생하는 error이므로 excution role을 변경해주기 위해 아래 그림의 빨간색 박스 안의 버튼을 클릭해 준다. role을 변경하기 위해 IAM 페이지로 이동되며, Attach를 통해 DynamoDB에 access 할 수 있는 권한을 부여할 것이다. 새롭게 role이 추가되어 기존의 1개에서 2개로 늘어났으며, DynamoDBFullAccess 권한을 부여받음을 확인할 수 있다. 이제 다시 Lambda Function 페이지로 돌아와서 실행시켜 보면 제대로 실행됨을 알 수 있다. Lambda는 Event trigger 뿐만 아니라, Crontab과 같이 스케쥴링에 의한 job을 작동시킬수도 있으며, 적용가능한 여러가지 event들이 존재한다.","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering (Presto란?)","slug":"data_engineering_08","date":"2020-02-24T13:11:09.000Z","updated":"2020-02-28T23:27:23.231Z","comments":true,"path":"2020/02/24/data_engineering_08/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/24/data_engineering_08/","excerpt":"","text":"Presto란? Spark의 단점이라 하면, 물론 Spark SQL도 있지만, 어느 정도 Scripting이 필요한 부분이 있다. MySQL 같이 RDS로 데이터 구축을 했을때에는 SQL을 통해서 쉽게 가져올 수 있었지만, Big data로 넘어 오면서 이전 필자의 글을 보았을 때 S3에서 두 곳에 나누어 저장을 했는데, 이런 경우 그럼 RDS와 다르게 어떻게 합칠 수 있는지를 Presto를 통해 하나의 query로 해결할 수 있다. syntax는 SQL과 비슷하다. 다양한 multiple data source를 single query를 통해서 진행 할 수 있는 것이다. Hadoop의 경우는 performance나 여러가지 data analytics 할때 여러가지 issue들이있으며 이전 방식이기 때문에 최근에는 Spark와 Presto로 넘어오는 추세이다. AWS는 Presto기반인 Athena를 통해서 S3의 데이터를 작업할 수 있다. Serverless란? 말 그대로 server가 없다라고 할 수 있으며, Severless라고 하는 부분은 보통 어떠한 서비스를 만들 때, 우리의 Desktop PC를 계속해서 켜두는 경우가 아니므로 EC2라고 하는 계속 지속적으로 띄어져있는 가상의 서버를 만들게 된다. 이때 서버의 용량을 결정해야하는데, 예를들어 chat bot을 통해 User와 소통을할때 어떤 날은 User가 1명 다른날은 100명 어떤날은 10,000명으로 늘어날수있어서 무작정 큰 서버를 사용하게 되거나 순차적으로 Docker를 통해 병렬적으로 어떤 기준이상이 되면 용량을 늘리는 이러한 작업도 다 비용이 되므로 이러한 문제들을 보완하고자 Serverless라는 개념이 도입된다. 어떠한 요청이 들어올때 server를 띄우는데 지속적으로 요청이 들어온다면 계속적으로 병렬적인 server를 띄운다는 것이다. server안에서 용량을 정하는 것을 알아서 자동적으로 해결해 주므로 비용적인 문제를 보완해준다. AWS에서 EC2같은 경우는 server 하나를 띄우는 것이고, Lambda가 Serverless의 개념을 갖는 서비스이다. 또한 Athena도 Serverless의 개념을 갖는 서비스이다. AWS Athena의 개요 AWS Athena에서도 data lake의 시스템 형태로 데이터를 작업하더라도 query를 통해 작업을 하려면 data warehouse 처럼 table의 형식을 안만들 수는 없다. 먼저, AWS Athena 필자와 같이 처음 Athena를 사용하는 것이라면, create table 버튼을 클릭하면 아래와 같은 형태로 query문을 입력하는 페이지 보일 것이다. 여기서 query문의 결과를 저장하는 곳을 먼저 설정해 주어야 하는데, 이전의 S3의 spotify-chatbot-project bucket에 새로운 폴더를 추가해 주고 아래 그림과 같이 path를 설정해준다. 이제 아래 그림과 같이 이전에 만들어 놓은 parquet형태의 데이터가 존재하는 folder의 path로 query문을 날려준다. 이전에 존재하지 않았던 테이블이 왼쪽의 tab에 생성되며 아래 부분에는 결과를 알려주는 부분이 보여질 것이다. 그 부분에는 partition되어진 부분은 직접적으로 load를 해주어야 한다는 결과를 알려주고있다. 위의 그림에서 맨 아래 부분의 모든 partition을 보려면, MSCK REPAIR TABLE) command 사용하라고한 것 처럼, query문을 추가해서 사용하였다. 일반적인 query문과 같이 작성해서 볼 수 있다. 먼저 top_tracks 테이블의 상위 10개만 불러와 볼 것이다. 그리도 partition을 dt로 했기 때문에 dt도 같이 불러와 지는 것을 확인 할 수 있다. audio_features도 동일한 방식으로 만들어 볼 것이다. 이밖으 사용법은 아래 문서를 통해 사용법을 확인할 수 있으며, 유의할점은 우리가 partition으로 나누어 놓은 것을 전체로 불러 왔기에 최근의 partition만을 살펴보기 위해 date를 어떤 값으로 설정했는지 확인해주어야 한다. prestodb documents Apache Spark 데이터를 처리하는 하나의 시스템이다. 데이터는 항상 늘어나고 그리고 너무나 큰 방대한 양을 처리를 해야하기 데이터가 늘어나면 늘어날수록 속도,시간,비용 여러면에서 효율적으로 처리해야한다. 필자는 제플린을 사용할 것인데, 제플린은 Spark를 기반으로 한 Web UI이다. 그래서 Spark를 통해서 처리된 데이터를 시각화한다던지 어떤식으로 output이 나오는지를 볼 수 있다. Jupyter notebook과 비슷하다고 생각하면 된다. Map Reduce Map Reduce는 데이터가 방대한 양으로 늘어날때 처리하는 방식에 issue가 생길 수 있다. 이런 issue들을 보완하기 위해서 데이터가 여러군데 분산처리 되어있는 형태로 저장되어있는데, S3 bucket에 저장한 방식처럼 partition으로 구분된 데이터를 function이나 어떠한 방식에 의해서 mapping을 해서 필요한 부분만을 줄이는 Reduce 과정을 거치게 된다. 이 방식은 처음 Google File System으로 사용되어지다가 그 뒤에 Map-Reduce방식으로 Hadoop을 사용했으며, 속도에 특화된 Spark를 현재는 주로 사용하고 있다. 예를 들면, 예를들면, 구글같이 다양한 web page를 크롤링해서 각 페이지들의 노출 랭킹을 분석해야 하는 Page Rank라는 알고리즘을 사용할때 html안에 들어가는 tag라던지 이런 문법적인 요소들과 contents들을 한 곳에 몰아서 분석하기 보다는 아래 그림과 같이 Input을 병렬적으로 나누어 진행하고 그 다음 어떠한 Suffling process를 통해서 Reduce하여 결과를 낸다. 필자는 AWS의 EMR(Elastic Map Reduce)서비스를 통해 Spark와 제플린을 설치 해 볼 것이다. EMR은 Spark나 Hadoop 같은 시스템에 cluster를 만드는 곳이라고 생각할 수 있다. cluster는 서버들이라고 말 할 수 있다. EC2를 base로한 EMR을 cluster화해서 하나의 instance ECS server가 아니라 master 아니면 다양한 core Node들이 여러개가 생성이 되어서 방대한 양의 데이터를 처리하기 위해서 필요한 setting을 구성할 수 있는 곳이라고 생각할 수 있다. 서버를 사용하기 위해서는 다양한 권한이라던지 key file들이 필요하므로 security와 같은 부분을 다루어야 하는데 이런 부부은 AWS에서 IAM 서비스에서 작업할 수 있다. 아래 그림과 같이 먼저 AWS에서 EMR 페이지로 이동한다. cluster 이름을 정하고, Application은 Spark를 사용할 것이므로 아래 그림과 같이 설정해 주었다. 또한, hardware 부분은 memory optimization등 여러가지 옵션이 존재하지만 모든 것은 다 비용이므로 우선 간단하게 c4 large를 사용해 보려고 한다. EC2 서버 안에서 다양한 작업들을 하기 위해서 key pair 필요하다. 필자와 같이 한번도 key pair를 생성한 적이 없다면 아래 그림과 같이 Learn how to key pair 버튼을 클릭하여 EC2 페이지로 넘어가 key pair를 생성한다. key pair가 생성되면 동시에 pem 파일이 다운로드 되어질 것이다. 그 파일을 현재 project를 진행하는 폴더로 옮겨 놓는것을 추천한다. 옮겨 놓았다면, 아래 그림과 같이 파일의 모드를 변경해 주어야 한다. 해당 pem 파일이 존재하는 path에서 아래와 같이 변경한다. 1chmod og-rwx spotift_chatbot.pem key pair를 생성하였으므로 다시 cluster를 생성하는 페이지로 돌아가 key pair를 설정한 뒤에 아래 생성 버튼을 클릭한다. cluster가 생성되면 아래 그림과 같이 cluster가 속성들에 대한 페이지가 보일 것이다. 또한, Master 권한으로 접속을 하려면 SSH 방식으로 인증 후 접속해야하기 때문에 security group을 관리 해 주어야 한다. Security groups for Master의 값을 클릭하여 EC2의 security group 페이지로 이동하여 Inbound 규칙에 Master와 slave 둘다 아래 그림과 같이 ssh 규칙을 추가해 주어야 한다. ssh로 접속하는 방법은 아래그림과 같이 enable web connection을 클릭하면 확인 할 수 있다. pem 파일이 존재하는 path로 가서 아래 그림에서 빨간색 줄이 쳐져있는 command를 실행해 놓은 뒤, 동일한 파일 path에 아래 그림에서 처럼 생성하라는 foxyproxy-settings.xml를 회색 네모칸의 내용들을 복사하여 생성한다. 동일한 파일 path에서 하라고 추천하는 것은 이 path에서 계속 접속할 것이기 때문이다. 그 다음은 chrome web store에 가서 foxy proxy를 검색한 후 standard 버젼을 설치해준다. 그러므로 chrome 브라우저를 사용해야 할 것임은 당연히 알 것이라고 생각한다. foxy proxy를 chrome browser에 추가했다면, 오른쪽 상단에 여우모양의 아이콘이 생성되었을 것이다. 클릭한 후 option 버튼을 눌러준다. 그 다음은 왼쪽 tab에서 import/export를 눌러 준 후에 이전에 만들어 준 xml파일을 눌러 replace해 주면 된다. 그런 다음 public dns로 접속해 보면 맨 아래 그림과 같이 test page를 볼 수 있을 것이다. 또한 dns위에 connection들이 활성화 된 것을 볼 수 있다. 아래 그림에서와 같이 connection들 중 제플린을 클릭하면 제플린 페이지로 이동한다. 모든 명령문이 %pyspark로 시작해야 한다. 필자는 기존의 c4.large로 진행시에는 connection error가 생겨 원인을 찾다가 해결치 못하고 우선 r3.xlarge를 선택하여 다시 cluster를 만든 결과 connection error가 발생하지 않았다. 2번째 cell까지는 python에서 진행하던 방식이고 3번째 cell에서 구현하는 방식이 spark의 방식인데 spark는 rdd를 기반으로 방대한 데이터를 분산시켜서 mapping한 후 apply해서 얻은 값을 통합을 해서 받는 구조이다. sc(spark context)를 통해 parallelize하게 3개의 데이터를 쪼개서 rdd로 나누어 준 결과이다. 아래와 같이 rdd로 쪼개서 map함수를 통해 쪼개놓은 데이터에 각각 mapping 시켜주고 선택한다. 또한, sqlContext를 통해 S3에 이전에 저장해 놓았던 데이터를 dataFrame형태로 불러와 작업할 수 있다. printSchema()함수를 통해 각각의 값들이 어떤 형태로 들어있는지에 주의를 갖고 살펴봐야 추후에 작업에 어려움이 없다. 이렇게 불러온 데이터를 내장 함수를 활용해 기본적인 통계량값들을 계산할 수 있으며, 필요에 따라 사용자 정의 함수(UDF : User Definition Function)을 사용하여 전처리를 할 수 있다. UDF를 통해 정의한 함수를 Boolean값으로 처리하여 다음과 같이 filter함수에 적용시켜 condition을 줄 수도 있다. 본격적으로 S3에 저장해 놓은 모든 데이터들을 join한 master table을 만들기 위해 아래와 같은 작업을 한다. 가장 먼저, artists parquet 데이터를 불러와 아래와 같이 DataFrame으로 만들 수 있다. 허나, artists 데이터는 잘 변하지 않기 때문에 zeppelin에서 python을 통해 RDS에서 바로 불러오는 것이 좋을 수도 있다. 아래에서 함수 안의 argument값들은 자신의 값에 맞는 것들로 먼저 정해놓고 실행해야 한다. 최종적으로 artists, top-tracks, 그리고 audio_features 모두 join한 table을 만들 것 이다. 참고로 아래 cell을 실행하기 이전에 master node에 접속해서 먼저 sudo pip install pandas와 sudo pip install pyspark 명령어를 통해 설치해 주어야 한다. zeppelin의 장점 중 하나로 바로 sql table로 지정하여 sql query문으로 작업할 수 있다. 해당 값을 바로 시각화할 수 있는 점도 장점 중의 하나이다. 옵션에서 없는 그래프를 그리는 것은 python이나 다른 library를 활용하여 보완할 수 있다. 데이터의 audio feature의 분포를 통해 예를 들어 가수의 인기도와 트랙의 인기도의 차이가 거의 없는 해당 가수의 대표적인 트랙을 알고 싶다면 아래와 같은 EDA를 먼저 실행하여 audio feature들의 특징을 파악하는 것이 중요하다. 아래 그림에서 처럼 acousticness는 전체적으로 0쪽으로 치우쳐있어 중심을 대표하는 값으로는 median을 사용해야 될 것이라고 판단할 수 있으며, danceability는 정규분포 꼴을 띄고 있어 mean을 사용해도 무방할 것으로 판단 할 수 있다. 참고하면 좋을 문서 - 01 참고하면 좋을 문서 - 02 참고하면 좋을 문서 - 03","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering (데이터 웨어하우스 vs 데이터 레이크)","slug":"data_engineering_07","date":"2020-02-21T16:14:31.000Z","updated":"2020-02-28T22:57:23.162Z","comments":true,"path":"2020/02/22/data_engineering_07/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/22/data_engineering_07/","excerpt":"","text":"데이터 웨어하우스 vs 데이터 레이크 데이터 레이크라는 개념은 비교적 최신의 개념이다. 데이터 웨어하우스라고 하는 MySQL, PostgreSQL 같은 RDBMS 프로그램들을 넘어서 데이터들이 너무나 방대해졌기 때문에 나온 시스템이라고 할 수 있다. 이전의 데이터 웨어하우스는 미리 짜여진 구조를 통해 가공해서 저장했기에 좀 더 접근하기 쉬었다. 반면에 데이터 레이크는 데이터가 너무 방대하기 때문에 어떤 데이터를 어떻게 사용할지 모르므로 Raw 데이터를 저장한다. 그렇기 때문에 데이터에 대한 접근성이 조금 떨어지는 면이 있었지만 Hadoop이나 Spark 같은 다양한 서비스들을 통해 그런 단점을 보완하여 처리할 수 있게 되었다. 관계형 DB를 사용할 경우에는 데이터가 기하급수적으로 늘어날 수록 비용이나 관리비용도 많이 소요될 것이다. ETL, 즉 Extract하고, Transform 한 후에 Load하는 과정을 일컫는 용어이며, 이전의 데이터 웨어하우스에서는 이런 순서를 거쳐 작업했지만 최근에는 ELT, 우선 Extract하고, Load한뒤에 데이터 레이크에 넣은 다음에 Transformer\b하자라고 많이들 이야기 하고 있다. 아래 그림은 하나의 예로서, 먼저 여러 곳에 산재해 있는 정형/비정형 데이터들을 데이터 레이크에 한곳으로 모\b아 그 다\b음 Spark가 됐던 다른 빅데이터 처리 시스템을 통해 재가공을 한 후 다른 애플리케이션이나 BI TooL들에 활용할 수 있다. 데이터 레이크 아키텍쳐 여러 어플리케이션에서 나오는 데이터나 해당 API를 통해서 얻게되는 데이터들을 모아 어떻게 재가공해야할지를 고민하는 것이 가장 큰 문제일 것이다. 또한 \b이런 것들 통해서 redash같은 시각화를 통해 insight를 얻어야 할 것이다. 또한, error가 나온다면, 그 날에 데이터만 어떻\b게 backfill을 통해서 확보를 할 것인지를 고민해야한다. 챗봇을 통해서 구축을 할 때, 아티스트가 늘어났을때, 새로운 artist가 입력을 들어왔을 때, 저장되어있던 정보에는 없던 Unknown artist이면, Trigger base의 Lambda에 의해서 Unknown artist에 대한 데이터를 확보를 하고, 데이터 레이크는 Latency(데이터를 주고받는 속도의 개념)가 느리기 때문에 다양한 DB에 RDBMS를 구축하여 상황에 맞게 저장한다. 그러나 이 모든 데이터들이 존재하는 DB를 한곳으로 묶어야 되는 부분이 데이터 레이크이다. 그 모든걸 필자는 AWS S3에 옮길 것이다. 옮기는 과정에서 스케쥴링은 어떻게 할 것인지 그리고, 어떤 이벤트가 있을때 업데이트를 할 것인지를 정해주어야 할 것이다. S3(Simple Storage System) 데이터 레이크 역할을 할 S3 bucket을 만들 것이다. 일종의 폴더라고 생각하여도 될 것 같다. 아래와 같은 단계로 bucket을 만든다. create bucket 버튼을 클릭한다. 생성할 bucket 이름을 설정한다. bucket에 관련된 설정 중 tag 부분을 설정할 수 있는 부분인데, 생성후에도 설정 가능하므로 다음단계로 넘어간다. configure option을 설정하는 단계이며, 아래와 같이 모두 public access 하게끔 default 설정값으로 선택하였다. 마지막으로 앞의 과정에서 설정한 부분들을 요약해서 보여준다. 맨 아래 Create bucket 버튼을 누르면 bucket 생성이 완료된다. 위의 단계를 다 거치면 처음과 다르게 하나의 bucket이 생성된 것을 확인할 수 있다. 물론 이전에 이미 bucket을 생성하신 분들은 여러개의 bucket 리스트가 보일 것이다. AWS Glue는 어떨때는 데이터가 형식이 없으니까 원래는 키값이 3개였는데, 그 후 점점 키값이 늘어날 수 도 있다. 이런 경우 다양한 Table의 스키마를 관리 할 수 있다. 데이터 레이크 경우에는 지속적으로 변할수 있는 시스템이라는 것을 인지하고 있어야하기 때문에, 위와 같은 경우들을 자동화 할 수 있는 서비스이다. 가장 큰 부분이 Crwaler이다. 어떠한 Table에 변형이 일어났을 때, 이 Crwaler가 감지를 해서, 그것을 반영을 해준다. S3에 올릴 파일작업 Python Script 작성 bucket의 key값이라고 하는 dt(data type)을 정해야한다. top tracks와 같이 지속적으로 변화하는 데이터는 방대한양으로 늘어났을때는 결국엔 쪼개서 scan을 해야하므로 어떠한 형식을 통해서 Spark나 Hadoop이 readable한 형식으로 partition을 만들어놔야 Spark나 Haddep에서 최근의 데이터를 갖고있는 마지막 partition만 확인하면 되기 때문이다. 필자는 날짜를 통해 시점이 언제일지 알 수 있도록 partition을 구분지어 줄 것이다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import sysimport osimport base64import boto3import requestsimport loggingimport jsonimport pymysqlimport sys, os, argparsefrom datetime import datetimeimport pandas as pddef main(host, user, passwd, db, port, client_id, client_secret): try: # use_unicode=True를 써야 한글같은 경우는 깨지지 않는다. conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") # 보통 문제가 없으면 0 # 문제가 있으면 1을 리턴하도록 안에 숫자를 넣어준다. sys.exit(1) headers = get_headers(client_id, client_secret) # RDS - 아티스트 ID를 가져오고 cursor.execute(\"SELECT id FROM artists\") dt = datetime.utcnow().strftime(\"%Y-%m-%d\") print(dt) for (id, ) in cursor.fetchall(): # Spotify API를 통해서 데이터를 불러오고 # .json형태로 저장한뒤에 with open('top_tracks.json', 'w') as f: for top_track in top_tracks: json.dump(top_track, f) f.write(os.linesep) # S3에 import를 시킨다. s3 = boto3.resource('s3') object = s3.Object('spotify-chatbot-project', 'dt=&#123;&#125;/topt-racks.json'.format(dt))def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersif __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) 필자가 사용할 Spark는 Parquet이라는 format을 더 선호하기에, Parquet으로 변형을 한후, compression(압축)을 통해서 데이터 Volume도 줄이면서 더 Performance도 좋게끔 할 것이다. 위에서 만든 top_tracks.json 로컬 파일을 S3에 저장을 할 것이다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import sysimport osimport base64import boto3import requestsimport loggingimport jsonimport pymysqlimport sys, os, argparsefrom datetime import datetimeimport pandas as pddef main(host, user, passwd, db, port, client_id, client_secret): try: # use_unicode=True를 써야 한글같은 경우는 깨지지 않는다. conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") # 보통 문제가 없으면 0 # 문제가 있으면 1을 리턴하도록 안에 숫자를 넣어준다. sys.exit(1) headers = get_headers(client_id, client_secret) # RDS - 아티스트 ID를 가져오고 cursor.execute(\"SELECT id FROM artists LIMIT 10\") # Top tracks를 Spotify에서 가져오고 top_tracks = [] for (id, ) in cursor.fetchall(): URL = 'https://api.spotify.com/v1/artists/&#123;id&#125;/top-tracks'.format(id) params = &#123; 'country' : 'US' &#125; r = requests.get(URL, params=params, headers=headers) raw = json.loads(r.text) top_tracks.extend(raw['tracks']) top_tracks = pd.DataFrame(top_tracks) top_tracks.to_parquet('top-tracks.parquet', engine='pyarrow', compression='snappy') sys.exit(0) dt = datetime.utcnow().strftime(\"%Y-%m-%d\") # S3에 import를 시킨다. s3 = boto3.resource('s3') object = s3.Object('spotify-chatbot-project', 'dt=&#123;&#125;/top-tracks.parquet'.format(dt)) data = open('top-tracks.parquet', 'rb') object.put(Body=data)def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersif __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) 위의 방법처럼 한다면 error가 발생되는데, 그 이유는 nested column이라 해서 아래 그림처럼 어떠한 key값의 안에 list형식으로 존재하는 struct형식이기 때문에 발생된다. parquet화 해서 사용하면 데이터를 통해서 performance가 빨라지지만, 그렇게 performance를 좋게하려면 정확하게 define을 해 주어야 한다. 보통 json 형식으로 가장 raw 형태로 저장한다음, processing job이 한 번 돌은후에, 새로운 data가 가장 raw data가 S3에 들어왔을때, trigger가 되어서 해당 parquet화를 하고 싶은 몇개의 데이터만 뽑은 후 다시 돌아서 다른 S3 bucket안에 저장하는 데이터 파이프라인을 거친다. 앞으로의 작업은 jsonpath라는 package가 필요하므로 아래의 코드처럼 설치를 해주어야 한다. 1pip install jsonpath --user 이제 top_tracks 뿐만 아니라 Audio Feature들도 추가해서 parquet 형태로 S3에 저장해 줄 것이다. 필자의 경우 국가코드는 US에 대해서만 우선 실행했으며, artist_id들 중 Audio feature가 US에서는 존재하지 않는 artist들이 있어 이 부분은 나중에 다른 국가나 artist들에 대해 동일한 현상으로 error가 발생되는 경우를 방지하기 위해 if문으로 Null값이 포함되어있는지 아닌지를 check해본뒤 리스트에 추가해주는 방식으로 코드를 작성했다. spotify_make_s3.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151import sysimport osimport base64import boto3import requestsimport loggingimport jsonimport pymysqlimport sys, os, argparsefrom datetime import datetimeimport pandas as pdimport jsonpathfrom pandas.io.json import json_normalizedef main(host, user, passwd, db, port, client_id, client_secret): try: # use_unicode=True를 써야 한글같은 경우는 깨지지 않는다. conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") # 보통 문제가 없으면 0 # 문제가 있으면 1을 리턴하도록 안에 숫자를 넣어준다. sys.exit(1) headers = get_headers(client_id, client_secret) # RDS - 아티스트 ID를 가져오고 cursor.execute(\"SELECT id FROM artists\") # jsonpath라는 package를 통해서 해당 path안에 어떤 데이터를 insert했을때, # key 값을 자동으로 찾아서 그에 해당하는 value값을 가져오기 때문이다. top_track_keys = &#123; 'id' : 'id', 'name' : 'name', 'popularity' : 'popularity', 'external_url' : 'external_urls.spotify' &#125; # Top tracks를 Spotify에서 가져오고 top_tracks = [] for (id, ) in cursor.fetchall(): URL = 'https://api.spotify.com/v1/artists/&#123;&#125;/top-tracks'.format(id) params = &#123; 'country' : 'US' &#125; r = requests.get(URL, params=params, headers=headers) raw = json.loads(r.text) for i in raw['tracks']: top_track = &#123;&#125; for k, v in top_track_keys.items(): top_track.update(&#123;k: jsonpath.jsonpath(i, v)&#125;) # 데이터를 mapping하기 위해서 artist_id를 추가한다. top_track.update(&#123;'artist_id': id&#125;) top_tracks.append(top_track) # track_id track_ids = [i['id'][0] for i in top_tracks] # parquet화 할 수 있는 방법은 여러가지 package가 있지만 pandas를 사용할 것 이다. # 필자가 사용할 Spark는 Parquet이라는 format을 더 선호하기에, Parquet으로 변형을 한후, # compression(압축)을 통해서 데이터 Volume도 줄이면서 더 Performance도 좋게끔 할 것이다. # 위에서 만든 top_tracks.json local 파일을 S3에 저장을 할 것이다. top_tracks = pd.DataFrame(top_tracks) top_tracks.to_parquet('top-tracks.parquet', engine='pyarrow', compression='snappy') dt = datetime.utcnow().strftime(\"%Y-%m-%d\") # S3에 import를 시킨다. s3 = boto3.resource('s3') # bucket의 key값이라고 하는 data type을 정해야한다. # top tracks와 같이 지속적으로 변화하는 데이터는 방대한양으로 늘어났을때는 # 결국엔 쪼개서 scan을 해야하므로 어떠한 형식을 통해서 Spark나 Hadoop이 readable한 형식으로 # partition을 만들어놔야 Spark나 Haddep에서 최근의 데이터를 갖고있는 마지막 partition만 확인하면 되기 때문이다. # 필자는 날짜를 통해 시점이 언제일지 알 수 있도록 partition을 구분지어 줄 것이다. object = s3.Object('spotify-chatbot-project', 'top-tracks/dt=&#123;&#125;/top-tracks.parquet'.format(dt)) data = open('top-tracks.parquet', 'rb') object.put(Body=data) tracks_batch = [track_ids[i: i+100] for i in range(0, len(track_ids), 100)] audio_features = [] null_features = [] for batch in tracks_batch: ids = ','.join(batch) URL = 'https://api.spotify.com/v1/audio-features/?ids=&#123;&#125;'.format(ids) r = requests.get(URL, headers=headers) if 'null' in r.text: raw = json.loads(r.text) for i in raw['audio_features']: if pd.isnull(i) == False: # audio_features는 dictionary key값 안에 또 다른 list형식으로 되어있지 않으므로 # 그냥 사용해도 된다. null_features.append(i) else: raw = json.loads(r.text) audio_features.extend(raw['audio_features']) audio_features.extend(null_features) audio_features = json_normalize(audio_features) audio_features.to_parquet('audio_features.parquet', engine='pyarrow', compression='snappy') s3 = boto3.resource('s3') object = s3.Object('spotify-chatbot-project', 'audio_features/dt=&#123;&#125;/top-tracks.parquet'.format(dt)) data = open('audio_features.parquet', 'rb') object.put(Body=data)def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersif __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) 위의 코드 실행 결과 아래 그림과 같이 S3에 각각의 파일 형식으로 저장되어 업데이트시에 해당 시간과 날짜에 의해 partition되어지는 데이터 저장 결과를 볼 수 있다. 여기에 top-tracks 데이터와 audio feature 데이터 뿐만아니라 artist 데이터도 S3에 parquet형식으로 저장해 줄 것이다. spotify_s3_artist.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import sysimport osimport base64import boto3import requestsimport loggingimport jsonimport pymysqlimport sys, os, argparsefrom datetime import datetimeimport pandas as pdimport jsonpathfrom pandas.io.json import json_normalizedef main(host, user, passwd, db, port, client_id, client_secret): try: # use_unicode=True를 써야 한글같은 경우는 깨지지 않는다. conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") # 보통 문제가 없으면 0 # 문제가 있으면 1을 리턴하도록 안에 숫자를 넣어준다. sys.exit(1) # RDS - 아티스트 ID를 가져오고 cursor.execute(\"SELECT * FROM artists\") colnames = [d[0] for d in cursor.description] artists = [dict(zip(colnames, row)) for row in cursor.fetchall()] artists = pd.DataFrame(artists) artists.to_parquet('artists.parquet', engine='pyarrow', compression='snappy') dt = datetime.utcnow().strftime(\"%Y-%m-%d\") s3 = boto3.resource('s3') object = s3.Object('spotify-chatbot-project', 'artists/dt=&#123;&#125;/artists.parquet'.format(dt)) data = open('artists.parquet', 'rb') object.put(Body=data)if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret)","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering (AWS DynamoDB 사용해서 오디오 feature 활용하기)","slug":"data_engineering_06","date":"2020-02-20T09:25:18.000Z","updated":"2020-02-24T13:13:09.501Z","comments":true,"path":"2020/02/20/data_engineering_06/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/20/data_engineering_06/","excerpt":"","text":"NoSQL RDBMS가 제한되는 점들을 보완하기 위해 활용하기 시작했다. NoSQL 같은 경우는 RDB 처럼 특정 Structure가 정해져 있지 않기 때문에 데이터의 추가가 용이하다. Scalability SQL Databases are vertically scalable - CPU, RAM or SSD 여러가지 사양들을 정해놓아서 정해진 resource를 초과할 경우 또다른 resource를 추가 시켜주어야 한다. NoSQL Databases are horizontally scalable - Sharding / Partitioning NoSQL은 Partitioning을 통해 다른 resource에서 남는 부분을 부족한 부분에 넘겨줄 수 있다. 그렇다면 partition이 무엇이냐 조금 더 자세하게 알아보자면, 우선 해석 그대로 나눈다는 의미가 있다. partition을 해야하는 이유 중 가장 중요한 이유는 데이터가 늘어나면 늘어날수록 SQL을 통하\u001d여 query문의 performance가 느려질수 밖에 없는데, 이런 부분을 방지하고자 나누어 주는 것이다. 데이터의 양이 늘어나는데 매번 모든 데이터를 다 읽어와서 작업을 하는 것은 너무 비효율적이기 때문이다. 물론 무조건적으로 NoSQL이 좋다는 의미는 아니다. NoSQL의 경우에는 아래의 그림에서 오른쪽 그림처럼 애초에 우리가 lookup(참조)해야 하는 데이터의 양을 줄여서 나누어 놓는 것이다. 이는 Spark의 경우에도 동일하다. Vertical partitioning을 하는 이유는 중복적인 데이터는 ERD를 Table을 분리해서 관리를 하게끔 해서 Normalization을 하였는데, Normalization을 진행하고도 column 수가 늘어나면, 해당 Table을 읽는데 속도가 느려질수가 있다. 또한, 어떤 column들은 지속적으로 업데이트가 될 수도 있지만, 어떤 column들은 지속적으로 업데이트가 되지 않을 수 있기 때문에 사용한다. RDBM에서는 나눌수는 있지만, 데이터가 엄청나게 많은 양이어서 데이터를 처리하는데 속도의 향상을 기대한다면 사용하지만, 그렇지 않은 경우는 많이 사용하지 않는다. Horizontal Partition은 NoSQL에서 무조건 사용된다. 어떤 데이터를 검색할 때 Sharded Key로 빠르게 Access하기 위해서 사용되는 것이다. 마치 RDB에서 primary key를 통한 빠른 search와 유사한 느낌이다. 동일한 컬럼이지만 데이터의 양을 나누어서 각각 저장하는 방식이다. Partition key를 통해 데이터를 나누어 주는\u001d데, AWS안의 DynamoDB를 만들면서 확인해 볼 것이다. spotify Developer artist’s top tracks를 들어가보면, 아래 그림과 같이 path patameter로 해당 artist id를 받아서 결과를 출력 해준다. NoSQL이므로 track 전체를 가져와도 되지만, 그 안에서 필요한 데이터를 또 작업을 해야하는데, artist 한명이 여러개의 track을 가지고 있는 경우도 있으므로 artist id만을 partition key로 사용할 수 없다. 왜냐하면 partition key로 사용되는 경우 해당 column에 동일한 값을 사용할 수 없기 때문이다. 그러므로 artist_id를 partition key로 해놓고, track id를 sort key로 사용할 것이다. AWS DynamoDB 사용하기 먼저 AWS에 로그인을 마친 후, 아래 그림과 같이 service 탭을 눌러 내려보면 이전의 RDS를 만들었을 때 보았던, Database란에서 DynamoDB를 볼 수 있다. 클릭하면 하게 되면 아래 그림과 같은 페이지가 나올 것이다. 그 다음은, 위의 페이지에서 보이는 것처럼 create table을 클릭하면 다음과 같은 페이지로 이동 될 것이다. 여기서 Table name은 말 그대로 Table의 이름을 지정해주는 부분이고, Partition key 부분은 RDS에서 Primary Key와 동일한 역할(빠르게 참조할 수 있게 해주는)을 하는 부분이라고 생각하면된다. 그러므로 해당 artist_id에 해당하는 row는 유일해야 한다. 하지만 sort key를 사용할 수 있는데, 이 부분은 해당 세션이 언제 생성되었는지를 알려주기 위한 역할이라고 할 수 있다. 위에서 말했던 것 처럼 partition key는 artist_id로 sort key는 track id를 사용할 것이다. Provisioned는 server를 띄우기 때문에 그 상황 안에서 free tier를 사용가능한 사람만 사용할 수 있는데, Auto-Scale이 가능한 부분이 있다. 하지만 Auto-Scale이 실질적으로 적용이 되는 시간차가 있어 해당 Scaling이 진행되는 동안은 traffic이 많아진다면 속도가 느려질 수 있다는 단점이 있을 수 있다. On-demand는 어느 정도 필요한지를 모를 경우에 aws에서 알아서 scaling을 해주고 쓰이는 만큼만 돈을 지불할 상황일 때 사용한다. 필자는 아래와 같이 설정한 후에, DynamoDB를 생성하였다. 생성한 후에는 생성된 DynamoDB의 페이지가 아래와 같이 나온다. Read capacity units(읽기 요청 단위 1)은 강력히 일관된 읽기 요청 1 또는 최종적 일관된 읽기 요청 2(최대 4 KB 크기 항목의 경우)를 나타냅니다. 트랜잭션 읽기 요청은 최대 4 KB 크기 항목의 1회 읽기를 수행하는 데 2개의 읽기 요청 단위가 필요합니다. Write capacity units(쓰기 요청 단위 1)은 최대 1 KB 크기의 항목에 대해 1회 쓰기를 나타냅니다. 참고 DB를 사용하다가 primary key가 Table내에서 유일한 RDBMS와는 다르게 partition key를 추가할 수도 있다. 필자의 경우 추가를 할 필요가 없었기에 하진 않았지만, 추가하여 사용할 경우는 아래와 같이 생성해주면된다. Audio Feature DynamoDB에 insert 하기 AWS service를 사용하기 위한 패키지인 boto3를 설치한다. boto3는 AWS가 제공하는 Python SDK의 이름이다. DynamoDB에 insert를 하거나 select해오거나 할 때 필요하다고 간단하게 이해해도 좋을 것 같다. console에서는 로그인을 통해 인증절차를 거치지만 boto3를 통한 인증방식은 이전의 우리가 설정해 놓았던 aws configure를 통해 설정되있는 상태를 통해 인증하므로 AWS를 console에 로그인하지 않고도 컨트롤할 수 있는 것이다. boto3 documents 12#pip3 install boto3 --userpip install boto3 --user 오디오 피처 살펴보기 전체적인 flow는 이미 RDS받아놓은 artists Table에서 artist_id들을 받아온 후, Spotify API를 통해 해당 artist_id의 top track 정보를 받아서 AWS의 DynamoDB에 저장할 것이다. Spotify API에서는 해당 artist_id 뿐만아니라 국가에대한 parameter도 받는데, 이는 국가마다 해당 artist의 top track이 다를 수도 있기 때문이다. 필자는 우선 US(미국)과 CA(캐나다)에대해서만 살펴 볼 것이다. boto3에서 DynamoDB 사용법 documents 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798import sysimport osimport boto3import requestsimport base64import jsonimport loggingimport pymysqldef main(host, user, passwd, db, port, client_id, client_secret): try: dynamodb = boto3.resource('dynamodb', region_name='ap-northeast-2', endpoint_url='http://dynamodb.ap-northeast-2.amazonaws.com') except: logging.error('could not connect to dynamodb') sys.exit(1) try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") sys.exit(1) headers = get_headers(client_id, client_secret) # 사용할 DynamoDB로 만든 Table 이름 table = dynamodb.Table('top_tracks') cursor.execute('SELECT id FROM artists') # 국가는 미국과 캐나다만을 선택할 것이다. countries = ['US', 'CA'] for country in countries: for (artist_id, ) in cursor.fetchall(): URL = \"https://api.spotify.com/v1/artists/&#123;&#125;/top-tracks\".format(artist_id) params = &#123; 'country': country &#125; r = requests.get(URL, params=params, headers=headers) raw = json.loads(r.text) for track in raw['tracks']: # 위에서 가져온 track에서는 artist_id가 없기 때문에 아래와 같이 먼저 dictionary로 만들어줌. data = &#123; 'artist_id': artist_id, 'country': country &#125; data.update(track) table.put_item( Item=data )def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersif __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) 아래와 같이 AWS console 창에서도 데이터가 insert된 것을 확인 할 수 있다. DynamoDB 저장된 데이터 불러오기 아래 그림에서와 같이 boto3를 이용하여 DynamoDB에서 데이터를 select하여 가져오는 방법은 get_item 함수를 사용하는 방법이 있다. 단, 해당 Table의 Partition key 나 sort key로 지정한 column의 값을 전부 입력해 주어야 error 없이 작동된다. 123456789101112131415161718192021222324import sysimport osimport boto3def main(): try: dynamodb = boto3.resource('dynamodb', region_name='ap-northeast-2', endpoint_url='http://dynamodb.ap-northeast-2.amazonaws.com') except: logging.error('could not connect to dynamodb') sys.exit(1) table = dynamodb.Table('top_tracks') response = table.get_item( Key=&#123; 'artist_id' : '0L8ExT028jH3ddEcZwqJJ5', 'id' : '0uppYCG86ajpV2hSR3dJJ0' &#125; ) print(response)if __name__=='__main__': main() 결과1&#123;'Item': &#123;'is_playable': True, 'duration_ms': Decimal('282906'), 'external_ids': &#123;'isrc': 'USWB19901574'&#125;, 'uri': 'spotify:track:0uppYCG86ajpV2hSR3dJJ0', 'country': 'US', 'name': 'Give It Away', 'album': &#123;'total_tracks': Decimal('19'), 'images': [&#123;'width': Decimal('640'), 'url': 'https://i.scdn.co/image/ab67616d0000b273153d79816d853f2694b2cc70', 'height': Decimal('640')&#125;, &#123;'width': Decimal('300'), 'url': 'https://i.scdn.co/image/ab67616d00001e02153d79816d853f2694b2cc70', 'height': Decimal('300')&#125;, &#123;'width': Decimal('64'), 'url': 'https://i.scdn.co/image/ab67616d00004851153d79816d853f2694b2cc70', 'height': Decimal('64')&#125;], 'artists': [&#123;'name': 'Red Hot Chili Peppers', 'href': 'https://api.spotify.com/v1/artists/0L8ExT028jH3ddEcZwqJJ5', 'id': '0L8ExT028jH3ddEcZwqJJ5', 'type': 'artist', 'external_urls': &#123;'spotify': 'https://open.spotify.com/artist/0L8ExT028jH3ddEcZwqJJ5'&#125;, 'uri': 'spotify:artist:0L8ExT028jH3ddEcZwqJJ5'&#125;], 'release_date': '1991-09-24', 'name': 'Blood Sugar Sex Magik (Deluxe Edition)', 'album_type': 'album', 'release_date_precision': 'day', 'href': 'https://api.spotify.com/v1/albums/30Perjew8HyGkdSmqguYyg', 'id': '30Perjew8HyGkdSmqguYyg', 'type': 'album', 'external_urls': &#123;'spotify': 'https://open.spotify.com/album/30Perjew8HyGkdSmqguYyg'&#125;, 'uri': 'spotify:album:30Perjew8HyGkdSmqguYyg'&#125;, 'popularity': Decimal('72'), 'artists': [&#123;'name': 'Red Hot Chili Peppers', 'href': 'https://api.spotify.com/v1/artists/0L8ExT028jH3ddEcZwqJJ5', 'id': '0L8ExT028jH3ddEcZwqJJ5', 'type': 'artist', 'external_urls': &#123;'spotify': 'https://open.spotify.com/artist/0L8ExT028jH3ddEcZwqJJ5'&#125;, 'uri': 'spotify:artist:0L8ExT028jH3ddEcZwqJJ5'&#125;], 'disc_number': Decimal('1'), 'href': 'https://api.spotify.com/v1/tracks/0uppYCG86ajpV2hSR3dJJ0', 'track_number': Decimal('9'), 'external_urls': &#123;'spotify': 'https://open.spotify.com/track/0uppYCG86ajpV2hSR3dJJ0'&#125;, 'artist_id': '0L8ExT028jH3ddEcZwqJJ5', 'preview_url': 'https://p.scdn.co/mp3-preview/fcdf3224d230b26b637418c2d1028bc482db7fce?cid=93b8cdc701294ab7992eaf370c7ba1cd', 'is_local': False, 'id': '0uppYCG86ajpV2hSR3dJJ0', 'explicit': False, 'type': 'track'&#125;, 'ResponseMetadata': &#123;'RequestId': 'KKGRB8AQD7G3H83UAA4J40TPE7VV4KQNSO5AEMVJF66Q9ASUAAJG', 'HTTPStatusCode': 200, 'HTTPHeaders': &#123;'x-amzn-requestid': 'KKGRB8AQD7G3H83UAA4J40TPE7VV4KQNSO5AEMVJF66Q9ASUAAJG', 'x-amz-crc32': '1465669044', 'content-type': 'application/x-amz-json-1.0', 'content-length': '2296', 'date': 'Fri, 21 Feb 2020 12:56:26 GMT'&#125;, 'RetryAttempts': 0&#125;&#125; 그런데, 여기서 어떠한 형식으로든 DB를 사용하려면 query도 사용할 필요가 있을 것이다. 이런 경우 사용하는 것이 Scanning과 Querying이다. Querying은 Partition key(Primary key)를 알고있을 경우에 사용하고, Scanning은 그 이외의 부수적인 다른 attribute(column)의 value를 알고 있을 때 사용한다. partition key와 다른 attribute를 사용한 query문123456789101112131415161718192021222324import sysimport osimport boto3from boto3.dynamodb.conditions import Key, Attrdef main(): try: dynamodb = boto3.resource('dynamodb', region_name='ap-northeast-2', endpoint_url='http://dynamodb.ap-northeast-2.amazonaws.com') except: logging.error('could not connect to dynamodb') sys.exit(1) table = dynamodb.Table('top_tracks') response = table.query( KeyConditionExpression=Key('artist_id').eq('0L8ExT028jH3ddEcZwqJJ5') FilterExpression=Attr('popularity').gt(80) ) print(response['Items'])if __name__=='__main__': main() Attribute만 사용하는 scanning 허나, 되도록이면 querying을 사용하는 것을 추천한다. 왜냐하면, scan을 key값이 없기 때문에 전체 데이터를 한번 다 돌아서 조건에 맞는 데이터들을 가져오는 것이므로, 데이터의 양이 많아 row의 수가 많다면, 속도가 느려지기 때문이다. 1234567891011121314151617181920212223import sysimport osimport boto3from boto3.dynamodb.conditions import Key, Attrdef main(): try: dynamodb = boto3.resource('dynamodb', region_name='ap-northeast-2', endpoint_url='http://dynamodb.ap-northeast-2.amazonaws.com') except: logging.error('could not connect to dynamodb') sys.exit(1) table = dynamodb.Table('top_tracks') response = table.scan( FilterExpression=Attr('popularity').gt(80) ) print(response['Items'])if __name__=='__main__': main()","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"NLP 실습 Chat bot 만들기","slug":"NLP_13","date":"2020-02-20T07:01:42.000Z","updated":"2020-02-20T07:19:38.147Z","comments":true,"path":"2020/02/20/NLP_13/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/20/NLP_13/","excerpt":"","text":"지금까지 두 가지 문제에 대해 실습을 진행하였다. 1) 텍스트를 분석해서 각 텍스트를 분류하는 문제를 실습했고, 2) 두 개의 텍스트가 있을 때 각 텍스트끼리의 유사도를 판단하는 문제를 실습했다. 마지막으로 이번에는 텍스트를 단순히 분석해서 분류나 유사도를 측정하는 것이 아닌 직업 문장을 생성할 수 있는 text generation 문제를 실습해 볼 것이다. text generation에도 많은 문제가 있지만 ‘자연어의 꽃’이라고 불리는 ‘Chat bot’을 제작해 볼 것이다. Chat bot 만들기 일반적으로 chat bot을 제작하는 방법은 매우 다양하다. 단순하게 rule based 기반으로 제작할 수도 있고, machine learning을 섞은 hybrid 기반, 특정 시나리오에서 동작 가능해지는 시나리오 기반까지 정의하는 사람에 따라 제작 방법이 매우 다양하다. 물론, 정의하는 것은 어디까지나 가용할 데이터의 성격에 매우 의존적일 것이다. 필자는 우선 제작방법 중에서 딥러닝 모델을 통한 chat bot을 만들어 볼 것이다. 또한 chat bot을 만들기 위한 딥러닝 모델에도 여러 가지가 있지만 그 중에서 번역 문제에서 이미 성능이 입증 된 Seq2seq를 활용하여 제작할 것이다.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP 실습 유사도를 반영한 검색 키워드 최적화","slug":"NLP_12","date":"2020-02-11T08:16:56.000Z","updated":"2020-02-20T07:19:46.701Z","comments":true,"path":"2020/02/11/NLP_12/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/11/NLP_12/","excerpt":"","text":"이번 실습의 소개는 프로젝트성으로 진행 할 것이다. 프로젝트 소개더존 ICT 온라인 고객센터 키워드 검색 최적화 및 챗봇 구현 프로젝트를 하게 된 계기 먼저, 더존 온라인 고객센터 페이지 중 smart A에 관한 페이지에서 전체 탭을 클릭한 후, 살펴본 QnA 페이지를 살펴보았다. 필자는 고객들의 입장에서 생각해보았을때, 자신이 작성하는 질문(물론, 그림으로 첨부해야할 만큼 그 환경이 중요한 질문들은 제외하고)과 비슷한 질문들이 존재할 거라는 생각을 갖고 키워드를 통해 검색해볼 것이다. 아래 그림은 재입사자라는 키워드를 smart A페이지에서 검색했을 때 출력되는 결과이다. 11건의 총 검색 결과 중 재입사자에 대한 연말정산과 관련된 문건이 8건이 존재한다. 그래서 필자는 재입사자 연말정산이라는 키워드를 통해 검색을 해보았다. 위에서 재입사자라는 키워드를 통해 검색 했을 때, 재입사자의 연말정산에 대한 질문이 8건이 존재한 반면에 아래 그림에서와 같이 8건 중 5건 만을 보여준다. 필자는 질문의 내용이 아닌 질문의 제목에 재입사자 연말정산이라는 키워드가 8건이 존재할 뿐 내용은 그와는 다를 수도 있다는 생각이 들어, 검색결과에 포함되어 있지 않는 질문들을 살펴보았다. 또한, 질문 내용 자체가 본질적으로 물어보는 의미가 검색결과에 포함되지 않은 질문들은 다를 수도 있기에 특정 알고리즘을 통해 결과를 보여줄 수 있다는 생각이 들어 검색 결과에 포함된 질문과도 비교해 보기로 했다. 검색결과에 포함되지 않은 질문과 답변이 왼쪽 그림이고, 검색결과에 포함된 질문과 답변이 오른쪽의 빨강색 네모로 되어있는 그림이다. 두 질문은 비슷한 질문이라고 보인다. 그런데도 불구하고 재입사자 연말정산이라는 키워드 검색 결과에 포함되어 있지 않는 점을 통해 필자는 각각의 질문들과 검색 키워드 간의 유사성을 점수화해 유사성이 높은 질문들을 보여주는 시스템도입이 필요할 것 같다는 생각이 들었다. 또한, 챗봇을 만드는 부분에 있어서 입력과 출력의 문장의 sequence 길이를 맞춰주어야 하는데, 그에 따라서 답변이 특정 분야(예를들면, 연말정산이나 원천징수등)에서는 긴 문장으로 이루어질 수도 있으므로, 챗봇을 구현한다면, 각 분야에 따른 문장길이를 분석해 보기도 해야 할 것 같다는 생각이들었다. 이런 제한 상황으로 인해 챗봇 구현이 힘들다면, 질문과 검색 키워드 간의 유사도를 반영한 검색 결과를 통해서라도 더존 온라인 고객센터의 질문을 하시는 고객 분들에게 조금이나마 더 편의성을 드릴수 있게끔 하면 좋을 것 같다는 생각이 들었다. 더존 사이트내에서 영업 문의 전화나 구매자에 대한 상담은 따로 서비스를 제공하고 있지만, 온라인 고객센터 tap부분에서만 Q&amp;A에 관한 사항을 다루는데 답변을 해주는 시간은 업무 시간내로만 제한 되어있다. 이에 따라 24시간 또는 업무 이외의 시간에는 챗봇 서비스를 시행한다면 고객들의 입장에서 보았을 때 조금 더 편리하게 더존의 서비스나 솔루션을 이용할 수 있을 것이라는 취지에 의해서 챗봇 구현에 관심을 갖게 되었다. 데이터 이름 : qna_smart_a.csv 더존에서는 WEHAGO 플랫폼상에서 여러가지 서비스를 제공하고 있다. 그 중 더존 Smart A는 재무회계, 세무신고, 인사·급여관리, 물류관리까지 중소기업의 업무를 통합적으로 관리할 수 있는 회계프로그램로서, 이 프로그램의 질문과 답변에 의해서만 먼저 학습을 해 볼 것이다. 그 이유는 다른 프로그램들(ERP와 WEHAGO)은 사용자들의 성격에 따라 다양한 용도로 개발 되어있지만, 회계프로그램인 Smart A는 모든 기업이 공용으로 사용하기 때문에 우선적으로 학습해 볼 것이다. 또한, 가장 주요한 선택 이유는 Q&amp;A 게시판의 데이터 중 가장 많은 데이터를 포함하고 있었기 때문이다. 데이터 용도 : 데이터 출처 : 더존 온라인 고객센터 Smart A 전체 tap의 전체 질문과 답변들을 크롤링 해서 사용하였다. 크롤링 방식은 Scrapy를 통해 페이지를 순회하게끔 코드를 작성하여 크롤링해서 얻었다. 먼저, 더존 온라인 고객 센터페이지에서 질문과 답변을 크롤링해와서 데이터 셋을 구성할 것이다. Spider bot 만들기 전체 scrapy bot의 구성은 다음과 같다. items.py와 settings.py를 활용했으며, 마지막 결과 파일은 csv로 저장했다. 혹시 db파일로 저장하고 싶다면 추가적으로 pipelines에서 작업을 하면된다. 더존 온라인 고객센터의 게시판에서 최근 게시판에서는 답변 완료상태인 데이터가 주로 많지만 예전 데이터 중에는 간간히 답변 대기 상태인 데이터가 존재한다. 그러므로 pipeline.py에서 이를 통해 답변 완료인 상태인 데이터만을 크롤링하여도 되지만, 필자는 어떤 데이터가 답변 대기 상태인 데이터인지 눈으로 살펴보기 위해 그냥 모두 크롤링하는 것으로 처리하였다. 123456789101112131415161718192021thezone├── scrapy.cfg└── thezone ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-37.pyc │ ├── items.cpython-37.pyc │ ├── pipelines.cpython-37.pyc │ └── settings.cpython-37.pyc ├── items.py ├── middlewares.py ├── pipelines.py ├── settings.py └── spiders ├── __init__.py ├── __pycache__ │ ├── __init__.cpython-37.pyc │ └── qnacrawler.cpython-37.pyc ├── last_qna_smart_a.csv ├── qna_smart_a.csv └── qnacrawler.py qnacrawler.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Ruleimport syssys.path.insert(0, '/Users/heungbaelee/workspace/project/chat_bot_project/thezone/thezone')from items import ThezoneItemclass QnacrawlerSpider(CrawlSpider): name = 'qnacrawler' allowed_domains = ['help.douzone.com'] start_urls = ['http://help.douzone.com/pboard/index.jsp?code=qna10&amp;pid=10&amp;s_category_id=all&amp;type=all&amp;s_listnum=50&amp;s_field=&amp;s_keyword='] # rules = [ # Rule(LinkExtractor(allow=r'/pboard/index.jsp?code=qna10&amp;pid=10&amp;s_category_id=all&amp;type=all&amp;s_listnum=50&amp;s_field=&amp;s_keyword=&amp;page=\\d+', ), callback='parse_parent', follow=True), # ] rules = [ Rule(LinkExtractor(restrict_css='div.page_box &gt; ul &gt; li:nth-child(n+4)',attrs='href'), callback='parse_parent', follow=True), ] def parse_parent(self, response): # link = LinkExtractor(allow=r'/pboard/index.jsp?code=qna10&amp;pid=10&amp;s_category_id=all&amp;type=all&amp;s_listnum=50&amp;s_field=&amp;s_keyword=&amp;page=\\d+') # links = link.extract_links(response) # print(links) # print(response.status) for url in response.css('div.tab_cnt.mt30 &gt; table &gt; tbody &gt; tr'): article_num = url.css('td:nth-child(1)::text').extract_first().strip() self.logger.info('Article number : %s' % article_num) article_link = url.css('td:nth-child(3) &gt; a::attr(href)').extract_first().strip() self.logger.info('Article link : %s' % article_link) # print(article_num, response.urljoin(article_link)) yield scrapy.Request(response.urljoin(article_link), self.parse_child, meta=&#123;'article_num': article_num&#125;) def parse_child(self, response): # 부모, 자식 수신 정보 로깅 self.logger.info('----------------------------------------') self.logger.info('Child Response URL : %s' % response.url) self.logger.info('Child Response Status ; %s' % response.status) self.logger.info('----------------------------------------') # 질문 번호 article_num = response.meta['article_num'] # 유형 category = response.css(\"div.qna_read.mt30 &gt; table:nth-child(1) &gt; tbody &gt; tr:nth-child(2) &gt; td &gt; dl &gt; dd:nth-child(2)::text\").extract_first().strip() # 질문 question = \"\".join(response.css(\"div.qna_read.mt30 &gt; table:nth-child(1) &gt; tbody &gt; tr:nth-child(2) &gt; td &gt; div.q &gt; div.q_cnt &gt; p::text\").extract()).strip() # 등록일 enrolled_date_time = response.css(\"div.qna_read.mt30 &gt; table:nth-child(1) &gt; tbody &gt; tr:nth-child(1) &gt; td:nth-child(4)::text\").extract_first().strip() # 작성일 answer_date_time = response.css(\"table.mt10 &gt; tbody &gt; tr:nth-child(1) &gt; td:nth-child(4)::text\").extract_first().strip() # 답변여부 answer_yes = response.css(\"table.mt10 &gt; tbody &gt; tr:nth-child(1) &gt; td.ta_l &gt; span::text\").extract_first().strip() # 답변 answering = \"\".join(response.css(\"table.mt10 &gt; tbody &gt; tr:nth-child(2) &gt; td.ta_l.pd20 &gt; div.a &gt; div &gt; p::text\").extract()).strip() yield ThezoneItem(article_num=article_num, category=category, enrolled_date_time=enrolled_date_time, question=question, answer_date_time=answer_date_time, answer_yes=answer_yes, answering=answering) items.py 123456789101112131415161718192021222324252627282930313233# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# https://docs.scrapy.org/en/latest/topics/items.htmlimport scrapyclass ThezoneItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 문서번호 article_num = scrapy.Field() # 유형 category = scrapy.Field() # 질문 question = scrapy.Field() # 답변 answering = scrapy.Field() # 작성일 answer_date_time = scrapy.Field() # 등록일 enrolled_date_time = scrapy.Field() # 답변여부 answer_yes = scrapy.Field() settings.py 1234567891011121314151617181920212223242526272829303132333435# -*- coding: utf-8 -*-BOT_NAME = 'thezone'SPIDER_MODULES = ['thezone.spiders']NEWSPIDER_MODULE = 'thezone.spiders'DEFAULT_REQUEST_HEADERS = &#123;'Referer' : 'http://help.douzone.com'&#125;# Obey robots.txt rulesROBOTSTXT_OBEY = False# 쿠키사용COOKIES_ENABLED = TrueDOWNLOAD_DELAY = 3# User-Agent 미들웨어 사용DOWNLOADER_MIDDLEWARES = &#123; 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,&#125;# 파이프 라인 활성화# 숫자가 작을 수록 우선순위 상위ITEM_PIPELINES = &#123; 'thezone.pipelines.ThezonePipeline': 300,&#125;# 재시도 횟수RETRY_ENABLED = TrueRETRY_TIMES = 2# 한글 쓰기(출력 인코딩)FEED_EXPORT_ENCODING = 'utf-8' 위의 scrapy 파일들을 통해서 데이터를 먼저 확보 했다. 필자의 로컬환경을 통해서는 10시간 정도 걸렸다. 1scrapy runspider qnacrawler.py -o qna_smart_a.csv - t csv 데이터 소개 위의 scrapy spider bot을 통해서 얻은 데이터를 통해 다음과 같은 feature들을 얻었다. 회계프로그램인 smart_a에 대한 전체 Q&amp;A를 크롤링하여 챗봇을 만드는 것이 프로젝트의 목표이다. raw 데이터 구성 answer_date_time : 답변완료일자 answer_yes : 답변 여부 answering : 답변 내용 category : 질문의 유형 enrolled_date_time : 질문등록일자 question : 질문 내용 1raw_data.info() 1234567891011&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 12475 entries, 0 to 12474Data columns (total 6 columns):answer_date_time 12436 non-null objectanswer_yes 12436 non-null objectanswering 12420 non-null objectcategory 12475 non-null objectenrolled_date_time 12475 non-null objectquestion 12409 non-null objectdtypes: object(6)memory usage: 584.9+ KB 데이터에 null 값이 포함되어 있기 때문에 null값들을 제거해주고, 답변 대기 상태인 데이터들은 총 39건이 있었는데 답변이 작성되지 않은 데이터 이므로 답변 대기 상태인 데이터들도 같이 제거해준다. 12345678raw_data = raw_data[raw_data[\"answer_yes\"]==\"답변완료\"]raw_data.reset_index(drop=True, inplace=True)raw_data = raw_data[pd.isnull(raw_data[\"question\"])!=True].reset_index(drop=True)print(sum(raw_data[\"question\"].apply(lambda x: pd.isnull(x))))raw_data = raw_data[pd.isnull(raw_data[\"answering\"])!=True].reset_index(drop=True)print(sum(raw_data[\"question\"].apply(lambda x: pd.isnull(x)))) 답변대기 상태인 데이터들을 제거하고 총 사용가능한 데이터는 12,354건의 질문과 답변 쌍이다. 1234567891011&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 12354 entries, 0 to 12353Data columns (total 6 columns):answer_date_time 12354 non-null objectanswer_yes 12354 non-null objectanswering 12354 non-null objectcategory 12354 non-null objectenrolled_date_time 12354 non-null objectquestion 12354 non-null objectdtypes: object(6)memory usage: 579.2+ KB 먼저, 간단하게 데이터들의 분류 카테고리에 따라서 어떤 분포를 띄고 있는지 간략하게 살펴볼 것이다. 질문 데이터 전처리 세무/회계관련 질문들이라서 금액에 관한 질문과 답변들이 많이 있기에 숫자에 대한 내용을 제거할지 하지 말하야 할지를 두고 필자는 생각이 많았는데, 우선 프로젝트의 첫번째 목표인 검색 키워드와 질문의 내용간의 유사도를 측정하는 면에 있어서는 숫자들이 크게 중요하지 않을 것이라는 판단하에 숫자부분들과 마침표같은 부호들을 제거하기로 결정했다. 다만, []안의 내용은 대부분 smart A의 메뉴명을 의미하기 때문에 살려두었다. [메뉴명]을 하나의 명사로 인식하기 위해 형태소 분석을 할 경우에도 비지도 학습을 통한 방식을 채택하기 위해 soynlp를 사용할 것이다. 123456789def pattern_match(x): pattern = \"\\d+\" reg = re.compile(pattern) sentence = re.sub(reg, \" \", x) pattern = \"[!|,|.|?|~|※|)|(|■|+|=|-|/|*|-|&gt;|-|;|^|]|-|%|'|'|ㅠ+|ㅎ+]\" reg = re.compile(pattern) sentence = re.sub(reg, \" \", sentence) 1raw_data['question_after'] = raw_data['question_after'].apply(lambda x : pattern_match(str(x))) 기본적인 부호들과 숫자들을 제거해 주었으므로 이제 기본적인 띄어쓰기 단위 어절과 음절(문자 하나하나를 의미)단위로 질문의 평균적인 길이와 한 질문당 단어의 평균적인 사용량을 대략적으로 살펴볼 것이다. 12# 띄어쓰기 단위로 나눈 어절 기초통계량raw_data['question_after'].apply(lambda x: len(str(x).split(\" \"))).describe() 위의 띄어쓰기 단위로 나눈 질문의 Token의 개수는 평균적으로 33개의 어절과 중앙값은 27개의 어절을 갖는다는 것을 확인 할 수 있다. 평균이 올라간것은 3사분위수가 42개인 것과 최대 어절이 791개인 것으로 미루어 보아 이상치에 의한 영향을 받아 평균이 데이터의 중심을 잘 반영하고 있지 않다고 판단해 볼 수 있다. 그러므로 이상치들의 데이터 형태를 살펴보고 문제점이 무엇인지 파악해 볼 것이다. 123456789count 12290.000000mean 35.055411std 33.537140min 1.00000025% 17.00000050% 27.00000075% 42.000000max 791.000000Name: question_after, dtype: float64 가장 높은 최댓값을 갖는 데이터를 살펴보면, 아래의 그림과 같이 공백으로 일정한 형식을 맞춰보려고 한 것 같이 되어있다. 그러나 우리는 이 질문의 내용적인 면이나 키워드가 중요한 것이므로 형식이 우리가 푸는 문제에는 큰 영향을 주지 못하므로 공백을 제거해 줄 것이다. 12345def pattern_match(x): pattern = \" +\" reg = re.compile(pattern) sentence = re.sub(reg, \" \", x) return sentence 123raw_data['question_after'] = raw_data['question_after'].apply(lambda x : pattern_match(str(x)))sent_len_by_token = raw_data['question_after'].apply(lambda x: len(str(x).split(\" \"))) 공백이 많은 데이터들을 공백을 줄여주는 함수를 통해 처리를 해준 후에 다시 질문 당 띄어쓰기 단위 어절의 길이에 관한 기초 통계량을 살펴보면 다음과 같다. 역시 함수를 통해 공백을 줄여준 후에 다시 측정해보니 평균과 중앙값의 차이가 이전과 다르게 확연히 줄어든 것을 볼 수 있으며, 평균적으로 22~23개의 어절을 사용함을 확인해 볼 수 있다. 123456789count 12290.000000mean 28.416273std 22.040055min 1.00000025% 15.00000050% 23.00000075% 35.000000max 355.000000Name: question_after, dtype: float64 90%의 위치에 위치하고 있는 어절의 길이는 52개 였다. 1np.quantile(sent_len_by_token, 0.90) 152.0 또한, 위에서 355개의 어절을 갖는 데이터에 관해서도 이상치이므로 살펴보았다. 아래와 같이 오류 코드에 관한 질문이었기 때문에 공백이 많이 포함되어있을 수 밖에 없다는 것을 확인 할 수 있었다. 그러므로 이 데이터의 공백은 질문의 내용을 표현하는데 불필요한 요소가 아니므로 그대로 상태를 유지 할 것이다. 그 다음은 음절 단위 길이를 분석해 볼 것이다. 음절 단위의 기초 통계량은 아래와 같다. 평균적으로 136자를 사용하였으며, 중앙값은 112자이다. 123456789count 12279.000000mean 136.459647std 109.823703min 3.00000025% 74.00000050% 112.00000075% 167.000000max 2637.000000Name: question_after, dtype: float64 위에서의 기초 통계량 값을 시각화해서 간단히 살펴 보기위해서 아래와 같이 히스토그램을 활용하였다. 상식적으로도 알 수 있듯이, 음절이 어절보다 훨씬 단위가 클수밖에 없을 것이다. 여기서 볼 것은 꼬리 분포이다. 음절과 어절 단위로 살펴본 질문의 길이는 둘다 일정 수준이하에 주로 분포돼있고, 일정 수준 이상은 이상치가 존재하고 있다. 모델 설정 제일 먼저, TF-IDF 행렬을 사용해 LSA 분석의 일종인 TruncatedSVD 행렬을 이용해 문장 임베딩을 실행한 후, 키워드 검색어와의 유사한 문서들을 살펴 볼 것이다. 12345678910111213141516171819202122232425262728293031323334353637383940import mathfrom sklearn.feature_extraction.text import TfidfVectorizerfrom soynlp.word import WordExtractorfrom soynlp.tokenizer import LTokenizerfrom sklearn.decomposition import TruncatedSVDfrom sklearn.preprocessing import normalizefrom sklearn.metrics.pairwise import cosine_similarityq_sentence = list(raw_data['question_after'])word_extractor = WordExtractor(min_frequency=1, min_cohesion_forward=0.05, min_right_branching_entropy=0.0)word_extractor.train(q_sentence)scores = word_extractor.word_scores()cohesion_scores = &#123;key:(scores[key].cohesion_forward * math.exp(scores[key].right_branching_entropy)) for key in scores.keys()&#125;tokenizer = LTokenizer(scores=cohesion_scores)tokens = []for q_s in q_sentence: tokens.append(tokenizer.tokenize(q_s))sentence_by_tokens = [' '.join(word) for word in tokens]## TfidfVectorizervectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,1), lowercase=True, tokenizer=lambda x : x.split(\" \"))input_matrix = vectorizer.fit_transform(sentence_by_tokens)vocab2id = &#123;token : vectorizer.vocabulary_[token] for token in vectorizer.vocabulary_.keys()&#125;id2vocab = &#123;vectorizer.vocabulary_[token]: token for token in vectorizer.vocabulary_.keys()&#125;## TruncatedSVDsvd = TruncatedSVD(n_components=100)vecs = svd.fit_transform(input_matrix)criterion_sentence = \"재입사자 연말정산\"criterion_tokens = tokenizer.tokenize(criterion_sentence)criterion_tokens 재입사자 연말정산이라는 키워드를 tokenizing한 결과는 아래와 같다. 재입사, 자, 연말정산 이렇게 3가지 형태로 형태소를 분리했다. 1['재입사', '자', '연말정산'] 1234567891011121314151617181920212223242526criterion_sentence_by_token = [\" \".join(criterion_tokens)]criterion_vec = vectorizer.transform(criterion_sentence_by_token)criterion_vec = svd.transform(criterion_vec)svd_l2norm_vectors = normalize(vecs, axis=1, norm='l2')svd_l2norm_criterion_vectors = normalize(criterion_vec, axis=1, norm='l2').reshape(100,1)cosine_similarity = np.dot(svd_l2norm_vectors, svd_l2norm_criterion_vectors)ls=[]for idx, cosine_similarity in enumerate(cosine_similarity.tolist()): ls.append((idx, cosine_similarity))sorted_list = sorted(ls, key= lambda x: x[1], reverse=True)criterion_tokens_list = []for i in np.arange(len(sorted_list)): criterion_tokens_list.append(criterion_tokens)show_list = []for sorted_lists, criterion_tokens in zip(sorted_list, criterion_tokens_list): idx=sorted_lists[0] similarity=sorted_lists[1] tf_list=[] for token in criterion_tokens: tf_list.append(token in raw_data['question'].loc[idx]) if (np.array(tf_list) == True).all(): show_list.append((idx, similarity))show_list 위의 show_list결과 중 몇가지 질문들을 살펴보자면, 아래와 같다. 위에서 형태소가 재입사, 자, 연말정산 이렇게 3가지로 분리했던 것을 우리가 알 고 있듯이 재입사자, 연말정산 2가지로 잘 분리하도록 명사 추출기 점수를 더한 점수를 통해서 다시 tokenize할 것이다. 명사 추출기를 통한 명사 점수를 합산한 score를 통한 tokenizer 활용1234567891011121314151617181920212223242526272829303132333435363738394041import mathfrom soynlp.word import WordExtractorfrom soynlp.noun import LRNounExtractor_v2from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.decomposition import TruncatedSVDfrom sklearn.preprocessing import normalizefrom sklearn.metrics.pairwise import cosine_similaritynoun_extractor = LRNounExtractor_v2(verbose=True)nouns = noun_extractor.train_extract(q_sentence)noun_scores = &#123;noun:score.score for noun, score in nouns.items()&#125;combined_scores = &#123;noun:score + cohesion_scores.get(noun, 0) for noun, score in noun_scores.items()&#125;combined_scores = combined_scores.update( &#123;subword:cohesion for subword, cohesion in cohesion_scores.items() if not (subword in combined_scores)&#125;)tokenizer = LTokenizer(scores=combined_scores)tokens = []for q_s in q_sentence: tokens.append(tokenizer.tokenize(q_s))sentence_by_tokens = [' '.join(word) for word in tokens]vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,1), lowercase=True, tokenizer=lambda x : x.split(\" \"))input_matrix = vectorizer.fit_transform(sentence_by_tokens)vocab2id = &#123;token : vectorizer.vocabulary_[token] for token in vectorizer.vocabulary_.keys()&#125;id2vocab = &#123;vectorizer.vocabulary_[token]: token for token in vectorizer.vocabulary_.keys()&#125;svd = TruncatedSVD(n_components=100)vecs = svd.fit_transform(input_matrix)criterion_sentence = \"재입사자 연말정산\"criterion_tokens = tokenizer.tokenize(criterion_sentence)criterion_tokens 재입사자 연말정산이라는 검색 키워드를 tokenizing한 결과 아래와 같이 재입사자, 연말정산이라고 분류해냈다. 허나, 위에서와 같이 1['재입사자', '연말정산'] sorted_list에 포함된 질문들을 보면 대부분 연말정산이 들어가있는 질문들이 유사도가 높다는 것을 확인할 수 있었다. 이런 문제점은 필자의 개인적인 생각으로 input matrix로 TF-IDF matrix를 사용했기 때문에 전체 질문 건수에서 연말정산이 차지하는 비율이 높다보니 나타나는 현상이라고 생각했다. 이를 해결하기 위해 먼저 필자는 각 분야의 질문의 수를 맞추거나 다른 방법의 input matrix를 사용해서 문제를 해결해야 할 것이라고 생각했다. 123456789101112131415161718192021222324252627criterion_sentence_by_token = [\" \".join(criterion_tokens)]criterion_vec = vectorizer.transform(criterion_sentence_by_token)criterion_vec=svd.transform(criterion_vec)svd_l2norm_vectors = normalize(vecs, axis=1, norm='l2')svd_l2norm_criterion_vectors = normalize(criterion_vec, axis=1, norm='l2').reshape(100,1)cosine_similarity = np.dot(svd_l2norm_vectors, svd_l2norm_criterion_vectors)ls=[]for idx, cosine_similarity in enumerate(cosine_similarity.tolist()): ls.append((idx, cosine_similarity))sorted_list = sorted(ls, key= lambda x: x[1], reverse=True)criterion_tokens_list = []for i in np.arange(len(sorted_list)): criterion_tokens_list.append(criterion_tokens)show_list_noun = []for sorted_lists, criterion_tokens in zip(sorted_list, criterion_tokens_list): idx=sorted_lists[0] similarity=sorted_lists[1] tf_list=[] for token in criterion_tokens: tf_list.append(token in raw_data['question'].loc[idx]) if (np.array(tf_list) == True).all(): show_list_noun.append((idx, similarity))show_list_noun 123456789show_list_index = []for idx, score in show_list: show_list_index.append(idx)show_list_noun_index = []for idx, score in show_list_noun: show_list_noun_index.append(idx)set(show_list_noun_index) == set(show_list_index) 결과는 False로 처음 명사추출기 점수를 더해서 tokenizing한 결과가 더 많고 좋은 질문들을 검색해 내었다. 1False 결론 연말정산 카테고리의 문건이 전체 문건 중 다수를 포함하고 있기 때문에, 그에 따른 영향으로 검색 키워드에 연말정산이 포함되면 유사도가 큰 문건들은 대부분 연말정산의 내용만을 담고 있었기 때문에 추후에 tokenizing한 검색 키워드를 전부 포함하고 있는 문건들을 출력해주는 방식으로 바꾸었다. 처음의 model을 최종적으로 선택할 것이며, 현재의 검색어 시스템에서 재입사자와 재입사자 연말정산이라는 두 가지 키워드에 대한 검색이 아래 그림과 같이 재입사자는 11건 재입사자 연말정산는 5건으로 재입사자 키워드에서 대부분이 연말정산에 관한 내용임에도 불구하고 검색이 되지 않는 문제점은 해결할 수 있다는 것에 만족할 것이다. 게다가 기존의 띄어쓰기에 취약한 문제점도 보완할 수 있기에 이전보다는 더 나은 검색 시스템이라고 주장한다. 아래 그림은 더존 온라인 고객센터의 smart A 게시판에서 동일한 내용이지만 띄어쓰기만 다른 재입사자 연말정산(위)과 재입사자 연말 정산(아래)이라는 두 키워드를 검색한 결과이다. 위 같이 띄어쓰기가 달라도 검색어를 입력했을 때 동일한 결과를 얻을 수 있었다. 보완점 TF-IDF를 사용하였기 때문에 단어와 단어가 사용된 문건의 수에 의한 가중치에 의해 영향을 받는다는 점을 고려했었야 한다는 판단을 내렸다. 또한, 검색 키워드를 나중에 유사도를 계산한 리스트 중에 필터링 역할로 사용하기에 검색 키워드를 기반으로 하되 불필요한 부분을 제거하여 사용할 수 있는 알고리즘을 만들면 더 좋은 검색 시스템을 구성할 수 있을 것으로 기대 된다. 또한, 검색 시스템 뿐만 아니라 자신이 질문을 작성한 후에 자신의 질문과 유사도가 높은 질문들의 리스트를 보여주는 페이지로 전환시켜 주는 서비스도 좋을 것 같다. 이러한 생각이 들었던 이유는 더존 온라인 고객센터의 답변들 중 연말정산 같은 회계분야의 특정 시즌 때 질문들에 대한 답변이 조금 늦는 경우(물론 하루이상을 넘기지 않고 답변을 다 달아주신다)를 보았는데, 위와 같은 시스템을 도입하면 온라인 고객센터의 직원 분들도 덜 고생하시고, 고객님들께서도 조금 더 해결방안을 빨리 찾으실 수 있을 것 같다고 생각했기 때문이다. 토이 프로젝트를 하면서 느낀점 제일 많은 것을 느낀것은 전처리부분이었다. 데이터 분석에 있어서 전처리가 80%이상이라는 말은 매번 되새기게되지만, 이번에는 특히나 더 와 닿았던 토이 프로젝트 였던 것 같다. 다음 토이 프로젝트로 질문에 대한 답변을 작성해 주는 챗봇을 구현해 보려고하는데, 질문의 카테고리별로 모델을 따로 만들어야 될 것 같다는 생각이 들었다. 챗봇을 구현할 때 먼저 입,출력 벡터의 크기를 일정하게 정해서 부족하면 패딩처리하는 방식으로 사용하여야 하는데, 각 카테고리별로 답변을 주는 평균적인 답변과 질문의 sequence의 길이가 다를 것이라고 생각 했기 때문이다.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP 실습 텍스트 유사도 - 02 (XGBoost, 1D-CNN, MaLSTM)","slug":"NLP_11","date":"2020-02-10T16:36:58.000Z","updated":"2020-02-11T08:10:01.052Z","comments":true,"path":"2020/02/11/NLP_11/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/11/NLP_11/","excerpt":"","text":"모델은 총 3가지를 종류를 만들어 볼 것이다. XGBoost CNN MaLSTM XGBoost 앙상블 모델 중 하나인 XGBoost 모델은 ‘eXtream Gradient Boosting’의 약자로 캐글 사용자에 큰 인기를 얻은 모델 중 하나이다. 앙상블 기법이란 여러 개의 학습 알고즘을 사용해 더 좋은 성능을 얻는 방법을 뜻한다. 앙상블 기법에는 Bagging과 Boosting이라는 방법이 있다. Bagging은 여러 개의 학습 알고리즘, 모델을 통해 각각 결과를 예측하고 모든 결과를 동등하게 보고 취합해서 결과를 얻는 방식이다. Random Forest도 여러개의 decision tree 결과값의 평균을 통해 결과를 얻는 Bagging의 일종이다. Boosting은 여러 알고리즘, 모델의 결과를 순차적으로 취합하는데, 단순히 하나씩 취하는 방법이 아니라 이전 알고리즘, 모델이 학습 후 잘못 예측한 부분에 가중치를 줘서 다시 모델로 가서 학습하는 방식이다. XGBoost는 Boosting 기법 중 Tree Bossting 기법을 활용한 모델이다. 쉽게 말해 Random Forest와 비슷한 원리에 Boosting 기법을 적용했다고 생각하면된다. 여러개의 Decision Tree를 사용하지만 단순히 결과를 평균내는 것이 아니라 결과를 보고 오답에 대해 가중치를 부여한다. 그리고 가중치가 적용된 오답에 대해서는 관심을 가지고 정답이 될 수 있도록 결과를 만들고 해당 결과에 대한 다른 오답을 찾아 다시 똑같은 작업을 반복적으로 진행하는 것이다. 최종적으로는 XGBoost란 이러한 Tree Boosting 방식에 경사하강법을 통해 optimization을 하는 방법이다. 그리고 연산량을 줄이기 위해 Decision Tree를 구성할 때 병렬 처리를 사용해 빠른 시간에 학습이 가능하다. 1234567TRAIN_Q1_DATA_FILE = 'q1_train.npy'TRAIN_Q2_DATA_FILE = 'q2_train.npy'TRAIN_LABEL_DATA_FILE = 'label_train.npy'train_q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, 'rb'))train_q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, 'rb'))train_labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, 'rb')) numpy의 stack 함수를 사용해 두 질문을 하나의 쌍으로 만들었다. 예를 들어, 질문 [A]와 질문 [B]가 있을 때 이 질문을 하나로 묶어 [[A], [B]] 형태로 만들었다. 이와 같은 형태는 다음과 같이 여러가지 방법으로 구현할 수 있다. 12train_input_expand = np.concatenate((np.expand_dims(train_q1_data, 1), np.expand_dims(train_q2_data, 1)), axis=1)train_input_expand.shape 결과1(298526, 2, 31) 12train_input_concate = np.concatenate((train_q1_data[:,np.newaxis,:], train_q2_data[:,np.newaxis,:]), axis=1)train_input_concate.shape 결과1(298526, 2, 31) 12train_input_stack = np.stack((train_q1_data, train_q2_data), axis=1)train_input_stack.shape 결과1(298526, 2, 31) 1(train_input_concate == train_input_stack).all() and (train_input_stack == train_input_expand).all() and (train_input_concate == train_input_expand).all() 결과1True 전체 29만개 정도의 데이터에 대해 두 질문이 각각 31개의 질문 길이를 가지고 있음을 확인 할 수 있다. 두 질문 쌍이 하나로 묶여 있는 것도 확인할 수 있다. 이제 학습 데이터의 20%를 모델 검증을 위한 validation set으로 만들어 둘 것이다. 123from sklearn.model_selection import train_test_splittrain_input, eval_input, train_label, eval_label = train_test_split(train_input_stack, train_labels, test_size=0.2, random_state=4242) XGBoost를 사용하려면 입력값을 xgb 라이브러리의 데이터 형식인 DMatrix 형태로 만들어야 한다. 학습 데이터와 검증 데이터 모두 적용해서 해당 데이터 형식으로 만든다. 적용 과정에서 각 데이터에 대해 sum 함수를 사용하는데 이는 각 데이터의 두 질문을 하나의 값으로 만들어 주기 위해서이다. 그리고 두 개의 데이터를 묶어서 하나의 리스트로 만든다. 이때 학습 데이터와 검증 데이터는 각 상태의 문자열과 함께 tuple형태로 구성한다. 참고로 XGBoost와 sklearn의 ensemble.GradientBoostingClassifier은 동일하게 Tree Boosting 모델을 가지고 있지만 속도면에서 XGBoost가 훨씬 빠르다. (사용법도 조금 다름) 123456import xgboost as xgbtrain_data = xgb.DMatrix(train_input.sum(axis=1), label=train_label)eval_data = xgb.DMatrix(eval_input.sum(axis=1), label=eval_label)data_list = [(train_data, 'train'), (eval_data, 'valid')] 우선 모델을 만들고 학습하기 위해 몇 가지 선택해야 하는 옵션은 dictionary를 만들어 넣으면 된다. 이때 dictionary에는 모델의 objective(loss) function와 평가 지표를 정해서 넣어야 하는데 여기서는 우선 objective(loss) function의 경우 이진 로지스틱 함수를 사용한다. 평가 지표의 경우 RMSE를 사용한다. 이렇게 만든 인자와 학습 데이터, 데이터를 반복하는 횟수인 num_boost_round, 모델 검증 시 사용할 전체 데이터 쌍, 그리고 early stopping을 위한 횟수를 정한다. 데이터를 반복하는 횟수, 즉 Epoch을 의미하는 값으로는 1000을 설정했다. 전체 데이터를 1000번 반복해야 끝나도록 설정한 것이다. 그리고 early stopping을 위한 횟수값으로는 10을 설정해서 만약 10 epoch 동안 error값이 크게 줄지 않는다면 학습을 종료시키도록 하였다. 12345params = &#123;&#125;params['objective'] = 'binary:logistic'params['eval_metric'] = 'rmse'bst = xgb.train(params, train_data, num_boost_round = 1000, evals = data_list, early_stopping_rounds=10) 예측하기1234567TEST_Q1_DATA_FILE = 'test_q1.npy'TEST_Q2_DATA_FILE = 'test_q2.npy'TEST_ID_DATA_FILE = 'test_id.npy'test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, 'rb'))test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, 'rb'))test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb')) 123test_input = np.stack((test_q1_data, test_q2_data), axis=1)test_data = xgb.DMatrix(test_input.sum(axis=1))test_predict = bst.predict(test_data) 1234567DATA_OUT_PATH = '/content/'if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_OUT_PATH)output = pd.DataFrame(&#123;'test_id': test_id_data, 'is_duplicate': test_predict&#125;)output.to_csv(DATA_OUT_PATH + 'sample_xgb.csv', index=False) kaggle API를 통해서 바로 파일 올려주었다. 1!kaggle competitions submit quora-question-pairs -f \"sample_xgb.csv\" -m \"XGBoost Model\" 3294팀 중 2714\u001d등이다. 물론, 임베딩 벡터라든지 아무런 조치를 취하지 않았기 때문에 score가 안좋을 수 밖에 없다. 추후에 TF-IDF 행렬을 사용하거나 더 좋은 임베딩 기법을 사용해서 다시 올려볼 것이다. 지금은 유사도를 구하는 방법에 대한 기본 튜토리얼이므로 이 정도에서 그치겠다. CNN 텍스트 유사도 분석 모델 합성곱 신경망 구조를 활용해 텍스트 유사도를 측정하는 모델을 만들어 보겠다. 기본적인 구조는 이전 장의 합성곱 모델과 유사하지만 이번 경우에는 각 데이터가 두 개의 텍스트 문장으로 돼 있기 때문에 병렬적인 구조를 가진 모델을 만들어야 한다. 모델에 입력하고자 하는 데이터는 문장 2개다. 문장에 대한 유사도를 보기 위해서는 기준이 되는 문장이 필요하다. 이를 ‘기준 문장’이라 정의한다. 그리고 ‘기준 문장’에 대해 비교해야 하는 문장이 있는데 이를 ‘대상문장’이라 한다. 만약 모델에 입력하고자 하는 기준 문장이 ‘I love deep NLP’이고 이를 비교할 대상 문장이 ‘Deep NLP is awesome’이라 하자. 이 두 문장은 의미가 상당히 유사하다. 만약 학습이 진행된 후에 두 문장에 대한 유사도를 측정하고하 한다마녀 아마도 높은 유사도 점수를 보일 것이다. 이처럼 문장이 의미적으로 가까우면 유사도 점수는 높게 표현 될 것이고 그렇지 않을 경우에는 낮게 표현될 것이다. 전반적인 유사도 분석 모델 구조에 대한 흐름을 보자. 모델에 데이터를 입력하기 전에 기준 문장과 대상 문장에 대해서 인덱싱을 거쳐 문자열 형태의 문장을 인덱스 벡터 형태로 구성한다. 인덱스 벡터로 구성된 문장 정보는 임베딩 과정을 통해 각 단어들이 임베딩 벡터로 바뀐 행렬로 구성 될 것이다. 임베딩 과정을 통해 나온 문장 행렬은 기준 문장과 대상 문장 각각에 해당하는 CNN 블록을 거치게 한다. CNN 블록은 Convolution 층과 Max Pooling층을 합친 하나의 신경망을 의미한다. 두 블록을 거쳐 나온 벡터는 문장에 대한 의미 벡터가 된다. 두 문장에 대한 의미 벡터를 가지고 여러 방식으로 유사도를 구할 수 있다. 여기서는 FC layer를 거친 후 최종적으로 logistic regression 방법을 통해 문자 유사도 점수를 측정할 것이다. 이렇게 측정한 점수에 따라 두 문장의 유사 여부를 판단할 것이다. 모델 구현 준비1234567import tensorflow as tfimport numpy as npimport osfrom sklearn.model_selection import train_test_splitimport json 12345678910DATA_IN_PATH = '/content/'DATA_OUT_PATH = '/content/'TRAIN_Q1_DATA_FILE = 'q1_train.npy'TRAIN_Q2_DATA_FILE = 'q2_train.npy'TRAIN_LABEL_DATA_FILE = 'label_train.npy'DATA_CONFIGS = 'data_configs.json'TEST_SPLIT = 0.1RNG_SEED = 13371447 모델 파라미터 설정 1234567891011121314151617EPOCH=10BATCH_SIZE=1024MAX_SEQUENCE_LENGTH = 26 # 31WORD_EMBEDDING_DIM = 100CONV_FEATURE_DIM = 300CONV_OUTPUT_DIM = 128CONV_WINDOW_SIZE = 3SIMILARITY_DENSE_FEATURE_DIM = 200prepro_configs = Nonewith open(DATA_IN_PATH + DATA_CONFIGS, 'r') as f: prepro_configs = json.load(f)VOCAB_SIZE = prepro_configs['vocab_size'] #76464개 123q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, 'rb'))q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, 'rb'))labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, 'rb')) 123456789X = np.stack((q1_data, q2_data), axis=1)y = labelstrain_X, eval_X, train_y, eval_y = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)train_Q1 = train_X[:, 0]train_Q2 = train_X[:, 1]eval_Q1 = eval_X[:,0]eval_Q2 = eval_X[:,1] estimator에 활용할 데이터 입력 함수를 만들 것이다. map 함수 학습 입력 함수 검증 입력 함수 우선 map 함수로 정의한 rearrange 함수부터 설명하면 3개의 값이 인자로 들어오는데, 각각 기준 질문, 대상 질문, 라벨값이다. 이렇게 들어온 인자 값을 통해 2개의 질문을 하나의 dictionary 형태의 입력값으로 만든다. 그리고 이렇게 만든 dictionary와 label을 return하는 구조로 돼 있다. 이 함수를 학습 입력함수와 검증 입력 함수에 적용할 것이다. 12345678910111213141516171819202122def rearrange(base, hypothesis, label): features = &#123;'x1' : base, 'x2' : hypothesis&#125; return features, labeldef train_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y)) dataset = dataset.shuffle(buffer_size=100) dataset = dataset.batch(16) dataset = dataset.map(rearrange) dataset = dataset.repeat(EPOCH) iterator = dataset.make_one_shot_iterator() return iterator.get_next()def eval_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((eval_Q1, eval_Q2, eval_y)) dataset = dataset.shuffle(buffer_size=100) dataset = dataset.batch(16) dataset = dataset.map(rearrange) iterator = dataset.make_one_shot_iterator() return iterator.get_next() 모델 구현 CNN 블록 함수를 먼저 정의할 것이다. CNN 블록 함수는 convolution layer와 Pooling, Dense를 하나로 합친 형태로 정의할 것이다. 이 함수는 2개의 인자값을 받는데, 각각 입력값과 이름을 의미한다. 이 함수에서 합성곱의 경우 이전 장의 CNN 모델을 구성할 때와 동일하게 Conv1D를 사용할 것이다. Max Pooling도 마찬사지로 MaxPooling1D 객체를 활용한다. 그리고 이렇게 합성곱과 Max Pooling을 적용한 값에 대해 차원을 바꾸기 위해 Dense 층을 통과 시킨다. 1234567891011121314def basic_conv_sementic_network(inputs, name): conv_layer = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, CONV_WINDOW_SIZE, activation=tf.nn.relu, name=name + 'conv_1d', padding='same')(inputs) #1024 X 26 X 300 max_pool_layer = tf.keras.layers.MaxPool1D(MAX_SEQUENCE_LENGTH, 1)(conv_layer) # 1024 X 1 X 300 output_layer = tf.keras.layers.Dense(CONV_OUTPUT_DIM, activation=tf.nn.relu, name=name + 'dense')(max_pool_layer) #1024 X 1 X 128 output_layer = tf.squeeze(output_layer, 1) # 1024 X 128 return output_layer 이제 모델 함수를 설명 할 것이다. 먼저 현재 튜토리얼은 임베딩 벡터에 크게 신경쓰지 않고 모델의 구조에 대해 집중하는 튜토리얼이므로 특별한 기법 없이 tf.keras.layers.Embedding으로 임베딩 벡터를 만들어준 뒤 Conv1D 구조를 3번 거쳐 최종적으로 dense layer를 통해 1개의 노드로 맞춰준 logit 값을 sigmoid함수를 통해 마치 로지스틱 회귀와 같은 구조를 만들어 줄 것이다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263def model_fn(features, labels, mode): TRAIN = mode == tf.estimator.ModeKeys.TRAIN EVAL = mode == tf.estimator.ModeKeys.EVAL PREDICT = mode == tf.estimator.ModeKeys.PREDICT embedding = tf.keras.layers.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM) base_embedded_matrix = embedding(features['x1']) # 1024 X 31 X 100 hypothesis_embedded_matrix = embedding(features['x2']) # 1024 X 31 X 100 base_embedded_matrix = tf.keras.layers.Dropout(0.2)(base_embedded_matrix) hypothesis_embedded_matrix = tf.keras.layers.Dropout(0.2)(hypothesis_embedded_matrix) conv_layer_base_first = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, CONV_WINDOW_SIZE, activation=tf.nn.relu, padding='same')(base_embedded_matrix) #1024 X 31 X 300 max_pool_layer_base_first = tf.keras.layers.MaxPool1D(2, 1)(conv_layer_base_first) # 1024 X 30 X 300 conv_layer_hypothesis_first = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, CONV_WINDOW_SIZE, activation=tf.nn.relu, padding='same')(hypothesis_embedded_matrix) #1024 X 31 X 300 max_pool_layer_hypothesis_first = tf.keras.layers.MaxPool1D(2, 1)(conv_layer_hypothesis_first) # 1024 X 30 X 300 conv_layer_base_second = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, 5, activation=tf.nn.relu, padding='same')(max_pool_layer_base_first) #1024 X 30 X 300 max_pool_layer_base_second = tf.keras.layers.MaxPool1D(5, 1)(conv_layer_base_second) # 1024 X 26 X 300 conv_layer_hypothesis_second = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, 5, activation=tf.nn.relu, padding='same')(max_pool_layer_hypothesis_first) #1024 X 30 X 300 max_pool_layer_hypothesis_second = tf.keras.layers.MaxPool1D(5, 1)(conv_layer_hypothesis_second) # 1024 X 26 X 300 base_sementic_matrix = basic_conv_sementic_network(max_pool_layer_base_second, 'base') # 1024 X 128 hypothesis_sementic_matrix = basic_conv_sementic_network(max_pool_layer_hypothesis_second, 'hypothesis') # 1024 X 128 merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1) # 1024 X 256 similarity_dense_layer = tf.keras.layers.Dense(SIMILARITY_DENSE_FEATURE_DIM, activation=tf.nn.relu)(merged_matrix) # 1024 X 200 similarity_dense_layer = tf.keras.layers.Dropout(0.2)(similarity_dense_layer) logit_layer = tf.keras.layers.Dense(1)(similarity_dense_layer) # 1024 X 1 logit_layer = tf.squeeze(logit_layer, 1) # (1024, ) similarity = tf.nn.sigmoid(logit_layer) if PREDICT: return tf.estimator.EstimatorSpec( mode=mode, predictions=&#123; 'is_duplicate':similarity &#125;) loss = tf.losses.sigmoid_cross_entropy(labels, logit_layer) if EVAL: accuracy = tf.metrics.accuracy(labels, tf.round(similarity)) return tf.estimator.EstimatorSpec( mode=mode, eval_metric_ops= &#123;'acc': accuracy&#125;, loss=loss) if TRAIN: global_step = tf.train.get_global_step() train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step) return tf.estimator.EstimatorSpec( mode=mode, train_op=train_op, loss=loss) 먼저, 변수값 등 모델과 관련된 내용을 담은 체크포인트 파일을 저장할 경로를 설정해야한다. 경로를 지정한 후 해당 경로가 없다면 생성하고 Estimator 객체를 생성할 때 해당 경로를 설정한다. 12345DATA_OUT_PATH = '/content/'if not os.path.exists(DATA_OUT_PATH): os.akedirs(DATA_OUT_PATH)est = tf.estimator.Estimator(model_fn, model_dir=DATA_OUT_PATH + 'checkpoint') 1est.train(train_input_fn) 1est.evaluate(eval_input_fn) 데이터 제출1234567TEST_Q1_DATA_FILE = 'test_q1.npy'TEST_Q2_DATA_FILE = 'test_q2.npy'TEST_ID_DATA_FILE = 'test_id.npy'test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, 'rb'), allow_pickle=True)test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, 'rb'), allow_pickle=True)test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA_FILE, 'rb'), allow_pickle=True) 입력 함수나 검증 함수처럼 별도의 함수로 정의하지 않고 Estimator의 기본 numpy_input_fn 함수를 사용한다. 입력 형태는 앞서 다른 입력 함수와 마찬가지로 두 질문을 dictionary 형태로 만들었다. 그리고 shffle=False로 설정하는데 이는 두 개의 질문쌍이 같은 순서로 입력돼야 하기 때문이다. 이제 이 함수를 활용해 Estimator의 predict 함수를 실행할 것이다. predict 함수를 활용하고 반복문을 통해 데이터에 대한 예측값을 받을 수 있게 한다. 이때 받고자 하는 예측값에 대해서는 is_duplicate라는 key 값으로 정의했기 때문에 아래와 같이 유사도 예측값만을 받을 것이다. 12345678predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;\"x1\":test_q1_data, \"x2\":test_q2_data&#125;, shuffle=False)predictions = np.array([p['is_duplicate'] for p in est.predict(input_fn=predict_input_fn)])output = pd.DataFrame( data=&#123;\"test_id\":test_id_data, \"is_duplicate\": list(predictions)&#125; )output.to_csv(\"cnn_predict.csv\", index=False, quoting=3) kaggle 제출1!kaggle competitions submit quora-question-pairs -f \"cnn_predict.csv\" -m \"cnn conv1d 3layer 10 Epoches\" MaLSTM 마지막으로 텍스트 유사도 측정을 위해 사용할 모델은 MaLSTM 모델이다. 순서가 있는 입력 데이터에 적합하다는 평을 받는 RNN 모델을 통해 유사도를 측정한다. 유사도를 구하기 위해 활용하는 대표적인 모델인 MaLSTM 모델은 2016년 MIT에서 Jonas Mueller가 쓴 “Siamese Recurrent Architectures for Learning Sentence Similarity”라는 논문에서 처음 소개 되었다. MaLSTM이란 Manhattan Distance + LSTM 의 줄임말로써, 일반적으로 문장의 유사도를 계산할 때 코사인 유사도를 사용하는 대신 맨하탄 거리를 사용하는 모델이다. 이전의 합성곱 신경망 모델에서도 두 개의 문장 입력값에 대해 각각 합성곱 층을 적용한 후 최종적으로 각 문장에 대해 의미 벡터를 각각 뽑아내서 concatenate한 후 dese layer를 통해 선형 변환 해준 뒤 로지스틱 모형과 같이 값을 구해 두 문장의 유사도를 구했다. 이번에는 맨하탄 거리로 비교하는 형태의 모델로서, LSTM의 마지막 스텝의 LSTM hidden state는 문장의 모든 단어에 대한 정보가 반영된 값으로 전체 문장을 대표하는 벡터가 된다. 이렇게 뽑은 두 벡터에 대해 맨하탄 거리를 계산해서 두 문장 사이의 유사도를 측정 할 것이다. 그리고 이렇게 계산한 유사도를 실제 라벨과 비교해서 학습하는 방식으로 모델을 설계할 것이다. 123456789import sysimport tensorflow as tfimport numpy as npimport osimport pandas as pdfrom sklearn.model_selection import train_test_splitimport json 모델 구현 미리 Global 변수를 지정하자. 파일 명, 파일 위치, 디렉토리 등이 있다. 123456789101112131415161718192021DATA_IN_PATH = '/content/'DATA_OUT_PATH = '/content/'TRAIN_Q1_DATA_FILE = 'q1_train.npy'TRAIN_Q2_DATA_FILE = 'q2_train.npy'TRAIN_LABEL_DATA_FILE = 'label_train.npy'NB_WORDS_DATA_FILE = 'data_configs.json'## 학습에 필요한 파라메터들에 대해서 지정하는 부분이다.## CPU에서는 Epoch 크기를 줄이는 걸 권장한다.BATCH_SIZE = 4096EPOCH = 50HIDDEN = 64NUM_LAYERS = 3DROPOUT_RATIO = 0.2TEST_SPLIT = 0.1RNG_SEED = 13371447EMBEDDING_DIM = 128MAX_SEQ_LEN = 31 데이터 불러오기 데이터를 불러오는 부분이다. 효과적인 데이터 불러오기를 위해, 미리 넘파이 형태로 저장시킨 데이터를 로드한다. 1234567q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, 'rb'))q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, 'rb'))labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, 'rb'))prepro_configs = Nonewith open(DATA_IN_PATH + NB_WORDS_DATA_FILE, 'r') as f: prepro_configs = json.load(f) 12VOCAB_SIZE = prepro_configs['vocab_size']BUFFER_SIZE = len(labels) 테스트 및 검증 데이터 나누기 데이터를 나누어 저장하자. sklearn의 train_test_split을 사용하면 유용하다. 하지만, 쿼라 데이터의 경우는 입력이 1개가 아니라 2개이다. 따라서, np.stack을 사용하여 두개를 하나로 쌓은다음 활용하여 분류한다. 12345678X = np.stack((q1_data, q2_data), axis=1)y = labelstrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)train_Q1 = train_X[:,0]train_Q2 = train_X[:,1]test_Q1 = test_X[:,0]test_Q2 = test_X[:,1] 123456789101112131415161718192021def rearrange(base, hypothesis, labels): features = &#123;\"base\": base, \"hypothesis\": hypothesis&#125; return features, labelsdef train_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y)) dataset = dataset.shuffle(buffer_size=len(train_Q1)) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.map(rearrange) dataset = dataset.repeat(EPOCH) iterator = dataset.make_one_shot_iterator() return iterator.get_next()def eval_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((test_Q1, test_Q2, test_y)) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.map(rearrange) iterator = dataset.make_one_shot_iterator() return iterator.get_next() 모델 설계 양방향 LSTM을 사용할 것이다. 즉, 2개의 LSTM을 먼저 정의해야 한다. 정방향 LSTM층과 역방향 LSTM 층을 먼저 정의할 것이다. 그리고 나선 이 2개의 LSTM 층에 데이터를 적용한 후 결과값을 하나로 concatenate한다. 양방향 순환 신경망 함수의 경우 2개의 return 값이 있는데, 하나는 순환 신경망의 출력 값이고, 나머지 하나는 순환 신경망 마지막 스텝의 hidden state 벡터 값이다. 사용해야 할 것은 마지막 hidden state 벡터이므로 각각 q_output_states와 sim_output_states로 할당한다. 이렇게 뽑은 hidden state 벡터의 경우 해당 모델이 양방향 순환 신경망을 활용해 2개의 hidden state 값을 concatenate하여 하나의 벡터로 만든다. 이는 순환 신경망이 문장의 순방향과 역방향 모두 학습함으로써 성능 개선에 도움을 준다. 맨하탄 거리의 경우 두 벡터를 뺀 후 절대값을 취하면 된다. 이렇게 뺀 값의 경우 벡터 형태이기 때문에 하나의 상수, 즉 scalar값으로 만들기 위해 reduce_sum 함수를 이용한다. 이렇게 되면 구한 값이 0~1사이의 값을 갖게 될 것이다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879def Malstm(features, labels, mode): TRAIN = mode == tf.estimator.ModeKeys.TRAIN EVAL = mode == tf.estimator.ModeKeys.EVAL PREDICT = mode == tf.estimator.ModeKeys.PREDICT def basic_bilstm_network(inputs, name): with tf.variable_scope(name, reuse=tf.AUTO_REUSE): lstm_fw = [ tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(HIDDEN), output_keep_prob=DROPOUT_RATIO) for layer in range(NUM_LAYERS) ] lstm_bw = [ tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(HIDDEN), output_keep_prob=DROPOUT_RATIO) for layer in range(NUM_LAYERS) ] multi_lstm_fw = tf.nn.rnn_cell.MultiRNNCell(lstm_fw) multi_lstm_bw = tf.nn.rnn_cell.MultiRNNCell(lstm_bw) (fw_outputs, bw_outputs), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=multi_lstm_fw, cell_bw=multi_lstm_bw, inputs=inputs, dtype=tf.float32) outputs = tf.concat([fw_outputs, bw_outputs], 2) return outputs[:,-1,:] embedding = tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM) base_embedded_matrix = embedding(features['base']) hypothesis_embedded_matrix = embedding(features['hypothesis']) base_sementic_matrix = basic_bilstm_network(base_embedded_matrix, 'base') hypothesis_sementic_matrix = basic_bilstm_network(hypothesis_embedded_matrix, 'hypothesis') base_sementic_matrix = tf.keras.layers.Dropout(DROPOUT_RATIO)(base_sementic_matrix) hypothesis_sementic_matrix = tf.keras.layers.Dropout(DROPOUT_RATIO)(hypothesis_sementic_matrix)# merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1)# logit_layer = tf.keras.layers.dot([base_sementic_matrix, hypothesis_sementic_matrix], axes=1, normalize=True) # logit_layer = K.exp(-K.sum(K.abs(base_sementic_matrix - hypothesis_sementic_matrix), axis=1, keepdims=True)) logit_layer = tf.exp(-tf.reduce_sum(tf.abs(base_sementic_matrix - hypothesis_sementic_matrix), axis=1, keepdims=True)) logit_layer = tf.squeeze(logit_layer, axis=-1) if PREDICT: return tf.estimator.EstimatorSpec( mode=mode, predictions=&#123; 'is_duplicate':logit_layer &#125;) #prediction 진행 시, None if labels is not None: labels = tf.to_float(labels)# loss = tf.reduce_mean(tf.keras.metrics.binary_crossentropy(y_true=labels, y_pred=logit_layer)) loss = tf.losses.mean_squared_error(labels=labels, predictions=logit_layer)# loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(labels, logit_layer)) if EVAL: accuracy = tf.metrics.accuracy(labels, tf.round(logit_layer)) eval_metric_ops = &#123;'acc': accuracy&#125; return tf.estimator.EstimatorSpec( mode=mode, eval_metric_ops= eval_metric_ops, loss=loss) elif TRAIN: global_step = tf.train.get_global_step() train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step) return tf.estimator.EstimatorSpec( mode=mode, train_op=train_op, loss=loss) 학습 및 평가12345678# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #For GPUmodel_dir = os.path.join(os.getcwd(), DATA_OUT_PATH + \"checkpoint/rnn2/\")os.makedirs(model_dir, exist_ok=True)config_tf = tf.estimator.RunConfig()lstm_est = tf.estimator.Estimator(Malstm, model_dir=model_dir) 1lstm_est.train(train_input_fn) 1lstm_est.evaluate(eval_input_fn) 테스트 데이터 예측 및 캐글 제출하기1234567TEST_Q1_DATA_FILE = 'test_q1.npy'TEST_Q2_DATA_FILE = 'test_q2.npy'TEST_ID_DATA_FILE = 'test_id.npy'test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, 'rb'), allow_pickle=True)test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, 'rb'), allow_pickle=True)test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA_FILE, 'rb'), allow_pickle=True) 12345predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;\"base\":test_q1_data, \"hypothesis\":test_q2_data&#125;, shuffle=False)predictions = np.array([p['is_duplicate'] for p in lstm_est.predict(input_fn=predict_input_fn)]) 1234print(len(predictions)) #2345796output = pd.DataFrame( data=&#123;\"test_id\":test_id_data, \"is_duplicate\": list(predictions)&#125; )output.to_csv( \"rnn_predict.csv\", index=False, quoting=3 ) 1!kaggle competitions submit quora-question-pairs -f \"rnn_predict.csv\" -m \"MaLSTM Model with 5layers BiLSTM 50 Epoches\" 구조에 대한 튜토리얼형식으로 만들다보니 임베딩의 질이 떨어진다면 머신러닝 기법이 딥러닝 방식보다 결과가 더 좋을 수 있다는 사실을 다시 한번 체감할 수 있는 작업이었다. 추후에 TF-IDF행렬, Word2Vec과 문장 단위 LSA를 시행해 얻은 임베딩을 사용하여 다시 한번 결과를 비교할 것이다.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP 실습 텍스트 유사도 - 01 (데이터 EDA 및 전처리)","slug":"NLP_10","date":"2020-02-09T17:34:30.000Z","updated":"2020-02-10T18:28:10.695Z","comments":true,"path":"2020/02/10/NLP_10/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/10/NLP_10/","excerpt":"","text":"텍스트 유사도 텍스트 유사도 문제한 두 문장(글)이 있을 때 두 문장 간의 유사도를 측정할 수 있는 모델을 만드는 것이다. 문제소개 데이터 이름 : Quora Question Pairs 텍스트 용도 : 텍스트 유사도 학습을 목적으로 사용 데이터 권한 : Quora 권한을 가지고 있으며 Kaggle 가입 후 데이터를 내려받으면 문제없다. 데이터 출처 : https://www.kaggle.com/c/quora-question-pairs/data 이번에도 Kaggle의 대회 중 하나를 해결해 보려고 한다. “Quora Questions Pairs”라는 문제를 해결해보도록 할 것이다. Quora는 질문을 하고 다른 사용자들로부터 답변을 받을 수 있는 서비스이다. 실제로 딥러닝 공뷰할 때도 Quora의 질문들은 참고하면서 많은 공부를 할 수 있다. Quora의 월 사용자는 대략 1억명 정도 된다. 매일 수 많은 질문들이 사이트에 올라올 텐데 이 많은 질문 중에는 분명히 중복된 것들이 포함될 것이다. 따라서 Quora 입장에서는 중복된 질문들을 잘 찾기만 한다면 이미 잘 작성된 답변들을 사용자들이 참고하게 할 수 있고, 더 좋은 서비스를 제공할 수 있게 된다. 목표 : 여러 질문들 중에서 어떤 질문이 서로 유사한지 판단하는 모델을 만드는 것 캐글 API를 colab에서 사용하기 위한 인증 및 google storage에 업로드 되어있는 인증키 파일 현재 colab pwd로 복사해온 후 설정완료하기 12345678from google.colab import authimport warnings%matplotlib inline%config InlineBackend.figure_format = 'retina'warnings.filterwarnings(\"ignore\")auth.authenticate_user()!gsutil cp gs://kaggle_key/kaggle.json kaggle.json 1234!mkdir -p ~/.kaggle!mv ./kaggle.json ~/.kaggle/!chmod 600 ~/.kaggle/kaggle.json!pip install kaggle 데이터 불러오기와 분석하기 데이터를 내려받는 것부터 시작할 것이다. 필자는 Google colab에서 kaggle API를 통해 다운로드 받을 것이다. 아래 그림과 같은 error가 발생한다면 kaggle API Token파일을 다시 받지 말고 그 전에 먼저 해당 competition의 rule을 수락을 했는지를 확이해보아야 한다. https://www.kaggle.com/c/quora-question-pairs/rules 1!kaggle competitions download -c quora-question-pairs 해당 데이터가 잘 다운로드 됐는지 확인한다. 확인해 보면 다음과 같이 3가지 파일이 있을 것이다. sample_submission.csv.zip test.csv.zip train.csv.zip 총 3개의 파일이 zip 형식으로 압축된 형태다. 이 파일들의 압축을 풀어주는 과정까지 할 것이다. 123456789import zipfileDATA_IN_PATH = '/content/'zip_list=['sample_submission.csv.zip', 'test.csv.zip', 'train.csv.zip']for file in zip_list: zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r') zipRef.extractall(DATA_IN_PATH) zipRef.close() 본격적으로 데이터를 불러온 후 데이터 분석을 해보기 위해 필요한 라이브러리들을 모두 Import 할 것이다. 1234567import numpy as npimport pandas as pdimport osimport matplotlib.pyplot as pltimport seaborn as snsimport pathlib as Path%matplotlib inline 가장 먼저 학습 데이터를 불러와서 어떤 형태로 데이터가 구성돼 있는지 확인해 볼 것이다. 데이터는 ‘id’, ‘qid1’, ‘qid2’, ‘question1’, ‘question2’, ‘is_duplicate’열로 구성돼 있다. 각각의 description은 아래와 같다. id : 각 행 데이터의 고유한 index 값 qid1 : 질문들의 고유한 index 값 qid2 : 질문들의 고유한 index 값 question1 : 질문의 내용 question2 : 질문의 내용 is_duplicate : 0 또는 1(0이면 두 개의 질문이 중복이 아님을 의미, 1이면 두 개의 질문이 중복을 의미) 12train_data = pd.read_csv(DATA_IN_PATH + 'train.csv')train_data.head() 사용할 데이터가 어떤 데이터이고, 크기는 어느 정도 되는지 알아보기 위해 데이터 파일의 이름과 크기를 각각 출력해서 확인해 볼 것이다. 대부분 train data가 test data 보다 크기가 큰데, 이 데이터는 test data가 train data 보다 5배 정도 더 큰 것을 알 수 있다. test data가 큰 이유는 Quora의 경우 질문에 대해 데이터의 수가 적다면 각각을 검색을 통해 중복을 찾아내는 편볍을 사용할 수 있으므로 이러한 편법을 방지하기 위해 Quora에서 직접 컴퓨터가 만든 질문 싸을 test data에 임의적으로 추가했기 때문이다. 따라서 test data가 크지만 실제 question data는 얼마 되지 않는다. 그리고 Kaggle의 경우 예측 결과를 제출하면 점수를 받을 수 있는데, 컴퓨터가 만든 질문 쌍에 대한 예측은 점수에 포함도지 않는다. 1234print('파일 크기: ')for file in os.listdir(DATA_IN_PATH): if ('csv' in file) and ('zip' not in file): print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + 'MB') 결과1234파일 크기:test.csv 314.02MBsample_submission.csv 22.35MBtrain.csv 63.4MB 전체 데이터의 개수와 학습 데이터안의 NULL값이 존재하는지 먼저 확인 할 것이다. 결과를 보면 전체 질문 쌍의 개수는 대략 40만개이며 3개의 데이터에 NULL값이 존재한다. 1train_data.info() 결과1234567891011&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 404290 entries, 0 to 404289Data columns (total 6 columns):id 404290 non-null int64qid1 404290 non-null int64qid2 404290 non-null int64question1 404289 non-null objectquestion2 404288 non-null objectis_duplicate 404290 non-null int64dtypes: int64(4), object(2)memory usage: 18.5+ MB 전체 질문(두 개의 질문)을 한번에 분석하기 위해 Pandas의 Series를 통해 두 개의 질문을 하나로 합친다. 각 질문을 list로 만든 뒤 하나의 Series 데이터 타입으로 만든다. 결과를 보면 아래와 같은 구조로 합쳐졌다. 기존 데이터에서 질문 쌍의 수가 40만개 정도이고 각각 질문이 2개 이므로 대략 80만개 정도의 질문이 있다. 12train_set = pd.Series(train_data['question1'].to_list() + train_data['question2'].to_list()).astype(str)train_set.tail() 이제 질문들의 중복 여부를 확인해 볼 것이다. Numpy의 unique함수를 이용해 중복을 제거한 총 질문의 수와 반복해서 나오는 질문의 수를 확인한다. 결과를 보면 80만 개의 데이터에서 537,361건이 Unique한 데이터이므로 262,639건이 중복돼 있음을 알 수 있다. 그러므로 262,639개 데이터는 131,318개의 동일한 질문 쌍으로 이루어져 있음을 알 수 있다.(1쌍은 NULL값이고, 1쌍은 값이 하나만 존재하므로) 12print('교육 데이터의 총 질문 수 : &#123;&#125; 건'.format(len(np.unique(train_set))))print('반복해서 나타나는 질문의 수 : &#123;&#125; 건'.format(np.sum(train_set.value_counts() &gt; 1))) 결과12교육 데이터의 총 질문 수 : 537361 건반복해서 나타나는 질문의 수 : 111873 건 위의 결과를 시각화 해 볼 것이다. y축의 범위를 줄이기 위해 log을 사용했다. x값은 중복의 개수이며, y값은 동일한 중복 횟수를 가진 질문의 개수를 의미한다. histogram을 살펴보면 우선 중복 횟수가 1인 질문들, 즉 유일한 질문들이 가장 많고 대부분의 질문이 중복 횟수가 50번 이하이다. 그리고 매우 큰 빈도를 가진 질문은 이상치가 될 것이다. 123456plt.figure(figsize=(12,5))plt.hist(train_set.value_counts(), bins=50, alpha=0.5, color='r', label='word')plt.yscale('log', nonposy='clip')plt.title('Log-Histogram of question appearance counts')plt.ylabel('Number of questions')plt.show() 1print('교육 데이터의 총 질문 수 : &#123;&#125; 건'.format(len(np.unique(train_set)))) 결과1교육 데이터의 총 질문 수 : 537361 건 중복이 최대로 발생한 개수는 161번이고, 평균적으로 보면 문장당 1.5개의 중복을 가지며, 표준편차는 1.9다. 중복이 발생하는 횟수의 평균이 1.5라는 것은 많은 데이터가 최소 1개 이상 중복돼 있음을 의미한다. 즉 중복이 많다는 의미이다. 1train_set.value_counts().describe() 결과123456789count 537361.000000mean 1.504724std 1.911439min 1.00000025% 1.00000050% 1.00000075% 1.000000max 161.000000dtype: float64 이제 box plot을 통해 중복횟수와 관련해서 데이터를 직관적으로 이해해 보자. 아래의 분포는 중복 횟수의 이상치가 너무 넓고 많이 분포해서 box plot의 다른 값을 확인하기조차 어려운 데이터이다. 12345plt.figure(figsize=(12, 5))plt.boxplot([train_set.value_counts()], labels=['counts'], showmeans=True)plt.show() 데이터에 어떤 단어가 포함됐는지 간단히 알아보기 위해 워드클라우드를 사용할 것이다. 워드 클라우드로 그려진 결과를 확인해 보면 best, way, good, difference 등의 단어들이 질문을 할 때 일반적으로 가장 많이 사용된다는 것을 알 수 있다. 특이한 점은 해당 결과에서 ‘Donald Trump’가 존재하는 것이다. ‘Donald Trump’가 존재하는 이유는 선거 기간 중 학습 데이터를 만들었기 때문이라고 많은 캐글러들이 말하고 있다. 123456from wordcloud import WordCloudcloud = WordCloud(width=700, height=400).generate(' '.join(train_set.astype(str)))plt.figure(figsize=(15,13))plt.imshow(cloud)plt.axis('off')plt.show() 질문 텍스트가 아닌 데이터의 라벨인 ‘is_duplicate’에 대해 count plot을 통해 살펴볼 것이다. 라벨값의 개수를 확인해 보면 총 40만 개의 데이터에서 중복이 아닌 데이터가 25만개 정도이고 중복된 데이터가 약 15만개 정도로 보인다. 이 상태로 학습한다면 중복이 아닌 데이터 25만개에 의존도가 높아지면서 데이터가 한쪽 라벨로 편향된다. 이러한 경우 학습이 원활하게 되지 않을 수도 있으므로 최대한 라벨의 개수를 균형 있게 맞춰준 후 진행하는 것이 좋다. 많은 수의 데이터를 줄인 후 학습할 수도 있고, 적은 수의 데이터를 늘린 후 학습할 수도 있다. 1234fig, axe = plt.subplots(ncols=1)fig.set_size_inches(10, 5)sns.countplot(train_data['is_duplicate'])plt.show() 학습 데이터의 길이를 분석해 볼 것이다. 문자단위로 먼저 길이를 분석한 후 단어 단위로 분석 할 것이다. 데이터의 각 질문의 길이 분포는 15 ~ 150에 대부분 모여 있으며 길이가 150에서 급격하게 주어드는 것을 볼 때 Quora의 질문 길이 제한이 150 정도라는 것을 추정해 볼 수 있다. 길이가 150 이상인 데이터는 거의 없기 때문에 해당 데이터 때문에 문제가 되지는 않을 것이다. 123456789train_length = train_set.apply(len)plt.figure(figsize=(15, 10))plt.hist(train_length, bins=200, range=[0, 200], facecolor='r', normed=True, label='train')plt.title('Normalized histogram of chracter count in questions', fontsize=15)plt.legend()plt.xlabel('Number of characters', fontsize=15)plt.ylabel('Probability', fontsize=15)plt.show() 그에 따른 기초 통계량은 다음과 같으며, 평균적으로 길이가 60 정도라는 것을 확인할 수 있다. 그리고 중앙값의 경우 51 정도이다. 하지만 최댓값을 확인해 보면 1169로서 평균, 중앙값에 비해 매우 큰 차이를 보인다. 이런 데이터는 제외하고 학습하는 것이 좋을 것이다. 1train_length.describe() 결과123456789count 808580.000000mean 59.822548std 31.963751min 1.00000025% 39.00000050% 51.00000075% 72.000000max 1169.000000dtype: float64 데이터의 질문 길이값에 대해서도 box plot을 그려서 확인해 볼 것이다. 분포를 보면 문자 수의 이상치 데이터가 너무 많이 분포해서 box plot의 다른 값을 확인하기 조차 어려운 상태다. 123plt.figure(figsize=(12, 5))plt.boxplot(train_length, labels=['char counts'], showmeans=True)plt.show() 이제 문자가 아닌 단어를 한 단위로 사용해 길이값을 분석해 볼 것이다. 하나의 단어로 나누는 기준은 단순히 띄어쓰기로 정의할 것이다. histogram을 보면 대부분 10개 정도의 단어로 구성된 데이터가 가장 많다는 것을 볼 수 있다. 20개 이상의 단어로 구성되 데이터는 매우 적다는 것을 확인할 수 있다. 123456789train_word_counts = train_set.apply(lambda x: len(x.split(' ')))plt.figure(figsize=(15, 10))plt.hist(train_word_counts, bins=50, range=[0, 50], color='r', label='train', normed=True)plt.title('Normalized histogram of word count in one question', fontsize=15)plt.legend()plt.xlabel('Number of Words', fontsize=15)plt.ylabel('Probability')plt.show() 그에 따른 기초통계량을 살펴볼 것이다. 데이터의 문자 단위 길이를 확인했을 때와 비슷한 분포를 갖는다. 평균 개수의 경우 11개이며, 중앙값의 경우 평균 보다 1개 적은 10개를 갖는다. 문자 길이의 최댓값인 경우 1100 정도의 값을 보인다. 단어 길이는 최대 237개이다. 해당 데이터의 경우 지나치게 긴 문자 길이와 단어 개수를 보여준다. 1train_word_counts.describe() 결과123456789count 808580.000000mean 11.064856std 5.889168min 1.00000025% 7.00000050% 10.00000075% 13.000000max 237.000000dtype: float64 1np.quantile(train_word_counts, 0.99) 결과131.0 box plot을 통해 데이터 분포를 다시 한번 확인해보자. 문자 길이에 대한 box plot과 비슷한 모양의 그래프를 보여준다. Quora 데이터의 경우 이상치가 넓고 많이 분포 돼 있음을 알 수 있다. 123plt.figure(figsize=(12, 5))plt.boxplot(train_word_counts, labels=['word counts'], showmeans=True)plt.show() 몇 가지 특정 경우에 대한 비율을 확인해 볼 것이다. 특수 문자 중 구두점, 물음표, 마침표가 사용된 비율과 수학 기호가 사용된 비율, 대/소문자의 비율을 확인해 본다. 대문자가 첫 글자인 질문과 물음표를 동반하는 질문이 99% 이상을 차지한다. 전체적으로 질문들이 물음표와 대문자로 된 첫 문자를 가지고 있음을 알 수 있다. 그럼 여기서 생각해 볼 부분이 있다. 즉, 모든 질문이 보편적으로 가지고 있는 이 특징의 유지 여부에 대해서인데, 모두가 가지고 있는 보편적인 특징은 여기서 제거한다. 12345678910111213qmarks = np.mean(train_set.apply(lambda x : '?' in x))math = np.mean(train_set.apply(lambda x : '[math]' in x))fullstop = np.mean(train_set.apply(lambda x : '.' in x))capital_first = np.mean(train_set.apply(lambda x : x[0].isupper()))capitals = np.mean(train_set.apply(lambda x : max([y.isupper() for y in x]))) # 대문자가 사용된 질문이 몇 개인지numbers = np.mean(train_set.apply(lambda x : max([y.isdigit() for y in x]))) # 숫자가 사용된 질문이 몇 개인지print('물음표가 있는 질문: &#123;:.2f&#125;%'.format(qmarks * 100))print('수학 태그가 있는 질문: &#123;:.2f&#125;%'.format(math * 100))print('마침표가 있는 질문: &#123;:.2f&#125;%'.format(fullstop * 100))print('첫 글자가 대문자인 질문: &#123;:.2f&#125;%'.format(capital_first * 100))print('대문자가 있는 질문: &#123;:.2f&#125;%'.format(capitals * 100))print('숫자가 있는 질문: &#123;:.2f&#125;%'.format(numbers * 100)) 결과123456물음표가 있는 질문: 99.87%수학 태그가 있는 질문: 0.12%마침표가 있는 질문: 6.31%첫 글자가 대문자인 질문: 99.81%대문자가 있는 질문: 99.95%숫자가 있는 질문: 11.83% 데이터 전처리 지금까지 데이터 EDA(탐색적 데이터 분석)를 통해 데이터의 구조와 분포를 확인했다. 질문 데이터의 중복 여부 분포, 즉 라벨의 분포가 크게 차이나서 학습에 편향을 주므로 좋지 않은 영향을 줄 수 있다. 따라서 전처리 과정에서 분포를 맞춰줄 것이다. 그리고 대부분의 질문에 포함된 첫 번째 대문자는 소문자로 통일한다. 물음표 같은 구두점은 삭제하는 식으로 보편적인 특성은 제거함으로써 필요한 부분만 학습하게 하는 이점을 얻을 수 있다. 12345import reimport jsonfrom tensorflow.python.keras.preprocessing.text import Tokenizerfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences 123DATA_IN_PATH = '/content/'train_data = pd.read_csv(DATA_IN_PATH + 'train.csv', encoding='utf-8') 맨 먼저 진행할 전처리 과정은 앞서 분석 과정에서 확인했던 내용 중 하나인 라벨 개수의 균형을 맞추는 것이다. 앞서 분석 과정에서 확인했듯이 중복이 아닌 데이터의 개수가 더욱 많기 때문에 이 경우에 해당하는 데이터의 개수를 줄인 후 분석을 진행하겠다. 먼저 중복인 경우와 아닌 경우로 데이터를 나눈 후 중복이 아닌 개수가 비슷하도록 데이터의 일부를 다시 뽑는다. 1234567train_duplicate_data = train_data.loc[train_data['is_duplicate']==1]train_non_duplicate_data = train_data.loc[train_data['is_duplicate']==0]class_difference = len(train_non_duplicate_data) - len(train_duplicate_data)sample_frac = 1 - (class_difference / len(train_non_duplicate_data))train_non_duplicate_data = train_non_duplicate_data.sample(frac = sample_frac) 샘플링한 후 데이터의 개수가 동일해졌다. 이제 해당 데이터를 사용하먄 균형 있게 학습할 수 있을 것이다. 12print(\"중복 질문 개수 : &#123;&#125; 건\".format(len(train_duplicate_data)))print(\"중복이 아닌 질문 개수 : &#123;&#125; 건\".format(len(train_non_duplicate_data))) 결과12중복 질문 개수 : 149263 건중복이 아닌 질문 개수 : 149263 건 우선 라벨에 따라 나눠진 데이터를 다시 하나로 합치자. 1train_data = pd.concat([train_non_duplicate_data, train_duplicate_data]) 앞서 전처리에서 분석한 대로 문장 문자열에 대한 전처리를 먼저 진행한다. 우선 학습 데이터의 질문 쌍을 하나의 질문 리스트로 만들고, 정규 표현식을 사용해 물음표와 마침표 같은 구두점 및 기호를 제거하고 모든 문자를 소문자로 바꾸는 처리를 한다. 1train_data.head() 물음표와 마침표 같은 기호에 대해 정규 표현식을 사용하여 전처리하기 위해 re 라이브러리를 활용한다. 123456789101112131415FILTERS = \"([~.,!?\\\"':;)(])\"change_filter = re.compile(FILTERS)questions1 = [str(s) for s in train_data['question1']]questions2 = [str(s) for s in train_data['question2']]filtered_questions1 = []filtered_questions2 = []for q in questions1: filtered_questions1.append(re.sub(change_filter, \"\", q).lower())for q in questions2: filtered_questions2.append(re.sub(change_filter, \"\", q).lower()) 이제 남은 과정은 정제된 위의 텍스트 테이터를 토크나이징하고 각 단어를 인덱스로 바꾼 후, 전체 데이터의 길이를 맞추기 위해 정의한 최대 길이보다 긴 문장은 자르고 짧은 문장은 패딩 처리를 하는 것이다. 문자열 토크나이징은 tensorflow keras에서 제공하는 NLP Processing 모듈을 활용한다.객체를 만들 때는 두 질문 텍스트를 합친 리스트에 적용하고, 토크나이징은 해당 객체를 활용해 각 질문에 대해 따로 진행할 것이다. 이러한 방법은 두 질문에 대해 동일한 토크나이징 방식을 사용해야하며, 두 질문을 합친 전체 vocabulary를 만들어야 하기 때문이다. 토크나이징 이후에는 패딩 처리를 한 벡터화를 진행할 것이다. 12345tokenizer = Tokenizer()tokenizer.fit_on_texts(filtered_questions1 + filtered_questions2)questions1_sequence = tokenizer.texts_to_sequences(filtered_questions1)questions2_sequence = tokenizer.texts_to_sequences(filtered_questions2) 이제 모델에 적용하기 위해 특정 길이로 동일하게 맞춰야 한다. 따라서 최대 길이를 정한 후 그 길이보다 긴 질문은 자르고, 짧은 질문은 부족한 부분을 0으로 채우는 패딩 과정을 진행 할 것이다. 최대 길이는 앞서 EDA에서 확인했던 단어 개수의 99%인 31로 설정했다. 이렇게 설정한 이유는 이상치를 뺀 나머지를 포함하기 위해서이다.(다양한 값으로 실험했을 때 이 값이 가장 좋은 값이었다.) 전처리 모듈의 패딩 함수를 사용해 최대 길이로 자르고 짧은 데이터에 대해서는 데이터 뒤에 패딩값을 채워넣었다. 1234MAX_SEQUENCE_LENGTH = 31q1_data = pad_sequences(questions1_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')q2_data = pad_sequences(questions2_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post') 전처리가 끝난 데이터를 저장한다. 저장하기 전에 라벨값과 단어 사전을 저장하기 위해 값을 저장한 후 각 데이터의 크기를 확인해 보자. 두 개의 질문 문장의 경우 각각 길이를 31로 설정했고, vocabulary의 길이인 전체 단어 개수는 76,594개로 돼 있다. 123456789word_vocab = &#123;&#125;word_vocab = tokenizer.word_indexlabels = np.array(train_data['is_duplicate'], dtype=int)print('Shape of question1 data : &#123;&#125;'.format(q1_data.shape))print('Shape of question2 data : &#123;&#125;'.format(q2_data.shape))print('Shape of question1 data : &#123;&#125;'.format(labels.shape))print('Words in index : &#123;&#125;'.format(len(word_vocab))) 결과1234Shape of question1 data : (298526, 31)Shape of question2 data : (298526, 31)Shape of question1 data : (298526,)Words in index : 76594 단어 사전과 전체 단어의 개수는 dictionary 형태로 저장해 둘 것이다. 123data_configs = &#123;&#125;data_configs['vocab'] = word_vocabdata_configs['vocab_size'] = len(word_vocab)+1 이제 각 데이터를 모델링 과정에서 사용할 수 있게 저장하면 된다. 12345678910TRAIN_Q1_DATA = 'q1_train.npy'TRAIN_Q2_DATA = 'q2_train.npy'TRAIN_LABEL_DATA = 'label_train.npy'DATA_CONFIGS = 'data_configs.npy'np.save(open(DATA_IN_PATH + TRAIN_Q1_DATA, 'wb'), q1_data)np.save(open(DATA_IN_PATH + TRAIN_Q2_DATA, 'wb'), q2_data)np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), labels)json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w')) 이제 평가 데이터에 대해서도 동일한 전처리를 실행해줄 것이다. 123test_data = pd.read_csv(DATA_IN_PATH + 'test.csv', encoding='utf-8')valid_ids = [type(x) == int for x in test_data.test_id]test_data = test_data[valid_ids].drop_duplicates() 1234567891011test_questions1 = [str(s) for s in test_data['question1']]test_questions2 = [str(s) for s in test_data['question2']]filtered_test_questions1 = list()filtered_test_questions2 = list()for q in test_questions1: filtered_test_questions1.append(re.sub(change_filter, \"\", q).lower())for q in test_questions2: filtered_test_questions2.append(re.sub(change_filter, \"\", q).lower()) 12345test_questions1_sequence = tokenizer.texts_to_sequences(filtered_test_questions1)test_questions2_sequence = tokenizer.texts_to_sequences(filtered_test_questions2)test_q1_data = pad_sequences(test_questions1_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post')test_q2_data = pad_sequences(test_questions2_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding='post') 12345test_id = np.array(test_data['test_id'])print('Shape of question1 data: &#123;&#125;'.format(test_q1_data.shape))print('Shape of question2 data:&#123;&#125;'.format(test_q2_data.shape))print('Shape of ids: &#123;&#125;'.format(test_id.shape)) 1234567TEST_Q1_DATA = 'test_q1.npy'TEST_Q2_DATA = 'test_q2.npy'TEST_ID_DATA = 'test_id.npy'np.save(open(DATA_IN_PATH + TEST_Q1_DATA, 'wb'), test_q1_data)np.save(open(DATA_IN_PATH + TEST_Q2_DATA , 'wb'), test_q2_data)np.save(open(DATA_IN_PATH + TEST_ID_DATA , 'wb'), test_id)","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP 문장 수준 임베딩 - 02","slug":"NLP_09","date":"2020-02-07T16:02:58.000Z","updated":"2020-02-09T16:16:52.359Z","comments":true,"path":"2020/02/08/NLP_09/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/08/NLP_09/","excerpt":"","text":"ELMo(Embedding from Language Models) 미국 연구기관 Allen Institute for Artificial Intelligence와 미국 워싱턴대학교 공동연구팀이 발표한 문장 임베딩 기법이다. Computer vision 분야에서 널리 쓰이고 있었던 Transfer leaning을 자연어 처리에 접목하여 주목받았다. Transfer learning이란 이미 학습된 모델을 다른 Deep learning 모델의 입력값 또는 부분으로 재사용하는 기법을 일컫는다. ELMo가 제안된 이후 자연어 처리 분야에서는 모델을 Pretrain한 후 이를 각종 DownStream Task에 적용하는 양상이 일반화됐다. BERT(Bidirectional Encoder Representations from Transfomer), GPT(Generative Pre-Training)등이 이 방식을 따른다. Pretrain한 모델을 downstream task에 맞게 업데이트하는 과정을 Fine-tuning이라고 한다. ELMo는 Language Model이다. 단어 sequence가 얼마나 자연스러운지 확률값을 부여한다. 예를들어, ‘발 없는 말이 천리’라는 단어 sequence 다음에 ‘간다’라는 단어가 자주 등장했다면, 모델은 ‘발 없는 말이 천리’를 일력박아 ‘간다’를 출력해야 한다. ELMo는 크게 3가지 요소로 구성돼 있다. 1) 문자 단위 Convolution Layer 각 단어 내 문자들 사이의 의미적, 문법적 관계를 도출한다. 2) 양방향 LSTM Layer 단어들 사이의 의미적, 문법적 관계를 추출해내는 역할을 한다. 3) ELMo Layer 문자 단위 convolution Layer와 양방야 LSTM Layer는 ELMo를 Pretrain하는 과정에서 학습된다. 하지만 ELMo Layer는 Pretrain이 끝난 후 구체적인 DownStream task를 수행하는 과정에 학습된다. 문자단위 conv layer와 양방향 LSTM layer의 출력벡터등을 가중합하는 방식으로 계산된다. 이들 가중치들은 downstream task의 학습 손실을 최소화하는 방향으로 업데이트되면서 학습된다. 문자 단위 Convolution Layer ELMo 입력은 문자다 구체적으로는 유니코드 ID이다. 그러므로 corpus를 해당 단어를 유니코드로 변환해야한다. 한글 유니코드 블록은 UTF-8에서 3byte로 표현되기 때문에 예를 들어 ‘밥’이라는 단어의 유니코드를 10진수로 바꾸면 3가지 숫자가 된다. 여기서 단어(문자가 아님)의 시작과 끝을 알게하기 위해 BOW와 EOW에 해당하는 값을 유니코드 앞 뒤로 붙인다. 이후에 문자 임베딩 행렬에서 각각의 ID에 해당하는 행 벡터를 참조해 붙인다. 문자의 길이가 각각 다르므로 처음에 문자의 최대 길이를 정해주면 그에따른 padding처리를 해준다. Convolution filter의 크기는 (같이 보고 싶은 문자길이) $\\times$ (문자 임베딩의 차원 수)가 된다. 이를통해 피처맵을 얻고 여기서 max pooling을 해주어 결과를 낸다. 위에서 같이 보고 싶은 문자길이를 조정해 가면서 여러개의 filter를 사용해 얻은 풀링 벡터들을 concatenate한 뒤 highway network와 projection(차원 조정)을 한다. 양방향 LSTM, 스코어 레이어 문자 단위 convolution layer가 반환 한 단어벡터 sequence(맨 하단의 보라색 벡터들)에서 시작과 끝을 알이는 , 토큰을 앞 뒤로 붙인 뒤 학습 시킨다. 순방향 LSTM layer와 역방향 LSTM layer에 모두 위의 벡터 sequence들을 입력하는데 각각 n개의 LSTM layer를 구성한다. ELMo 기본 모델은 n=2로 설정하고 있다. ELMo에는 LSTM layer에 residual connection 구조를 적용시켜 일부 계산 노드를 생략하게하여 효율적인 Gradient 전파를 돕는다. 프리트레인 단계에서 Word2vec에서 사용되었던 negative sampling 기법이 사용된다. ELMo 레이어 ELMo 입베딩은 Pretrain이 끝나고 구체적인 DownStream task를 학습하는 과정에서 도출되며, 각 layer별 hidden state를 가중합한 결과이다. 임의의 task를 수행하기 위한 문장 k번째 Token의 ELMo 임베딩의 구체적인 수식은 다음과 같다. $h_{k,j}^{LM}$ : k번째 Token의 j번째 layer의 순방향, 역방향 LSTM hidden state를 concatenate한 벡터를 의미한다. $s_{j}^{task}$ : j번째 layer가 해당 task 수행에 얼마나 중요한지를 의미하는 scalar값이다. downstream task를 학습하는 과정에서 loss를 최소화하는 방향을 업데이트한다. $\\gamma^{task}$ : ELMo 벡터의 크기를 scaling하여 해당 task 수행을 돕는 역할을 한다. L : 양방향 LSTM layer 개수(보통 2로 설정함) j=0 -&gt; 문자 단위 convolution Layer j=1 -&gt; 양방향 LSTM layer의 첫번째 출력 j=2 -&gt; 양방향 LSTM layer의 두번째 출력 ELMo_{k}^{task} = \\gamma^{task} \\sum^{L}_{j=0} s_{j}^{task} h_{k,j}^{LM}트랜스포메 네트워크 트랜스포머 네트워크는 구글 연구 팀이 NIPS에 공개한 딥러닝 아키텍처다. 뛰어난 성능으로 주목받았다. 이후 발표된 GPT, BERT 등 기법은 트랜스포머 블록을 기본 모델로 쓰고 있다. 크게 두가지 작동원리로 나눌수 있다. Multi-head Attention과 feedforward Network이다. Scaled Dot-Product Attention Scaled Dot-Product Attention의 입력(x)는 기본적으로 행렬 형태를 가지며 그 크기는 입력 문장의 단어수 $\\times$ 입력 임베딩의 차원 수이다. 트랜스 포어믜 Scaled Dot-Product Attention 매커니즘은 query, key, value 3가지 사이의 관계가 핵심이다. 입력행렬 X와 Query, Key, Value에 따르는 가중치 행렬($W^{q}, W^{k}, W^{v}$)을 각각 곱해 계산한다. 이후 query와 key가 얼마나 유사한지를 구하기 위해 두 벡터간 내적을 구해 코사인 유사도를 구한다. 이를 통해 어떤 query와 key가 특정 task 수행에 중요한 역할을 하고 있다면 트랜스포머 블록은 이들 사이의 내적값을 키우는 방식으로 학습한다. 아래 식에서 제곱근 스케일을 하는 이유는 query-key 내적 행렬의 분산을 줄이게 돼 softmax 함수의 gradient가 지나치게 작아지는 것을 방지할 수 있기 때문이다. softmax 노드의 gradient는 softmax 확률 벡터 y의 개별 요소 값에 아주 민감하기 때문에 softmax 확률 벡터의 일부 값이 지나치게 작다면 gradient vanishing 문제가 나타날 수 있다. Scaled Dot-Product AttentionAttention(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt(d_{k})}) \\cdot V소프트맥스 노드의 gradient\\frac{\\delta y_{i}}{\\delta x_{i}} = y_{i}(1-y_{i})\\frac{\\delta y_{i}}{\\delta x_{j}} = -y_{i}y_{j} 아래 그림은 Scaled Dot-Product Attention 기법으로 Query, Key, Value 사이의 관계들이 농축된 새로운 Z를 만드는 예시이다. 파란색 선으로 둘러싸인 행렬은 Query, Key 내적을 $\\sqrt(d_{k})$로 나눈 뒤 softmax 함수를 취한 결과이다. 이 행렬의 행은 Query 단어들에 대응하며, 열은 Key 단어들에 대응한다. 아래 그림처럼 Query와 Key값이 동일한 attention을 self-attention이라고 한다. 이는 같은 문장 내 모든 단어 쌍 사이의 의미적, 문법적 관계를 포착해낸다는 의미이다. softmax를 취한 결과는 확률이 된다. 따라서 각 행의 합은 1이다. &#39;드디어-금요일&#39;값이 가장 높아 벡터 공간상에서도 가까이 있을 가능성이 높고 두 단어사이의 관계가 task 수행(번역, 분류등)에 중요하다는 이야기이다. 마지막으로는 softmax 확률을 가중치 삼아 각 값 벡터들을 가중합하는 것과 같다. 새롭게 만들어진 &#39;드디어&#39;에 해당하는 벡터는 해당 문장 내 단어 쌍간 관계가 모두 농축된 결과이다. self-attention은 RNN, CNN보다 장점이 많다. CNN의 경우 사용자가 지정한 window내의 context만 살피기 때문에 문장이 길고 처음 단어와 마지막 단어 사이의 연관성 파악이 task 수행에 중요한 데이터라면 해결하기 어렵다. RNN은 sequence의 길이가 길어질수록 gradient vanishing이 일어나기 쉽기 때문에 처음 입력받았던 단어를 기억하기 쉽지 않다. 하지만 self-attention은 문장 내 모든 단어쌍 사이의 관계를 늘 전체적으로 파알할 수 있다. Multi-Head Attention Scaled Dot-Product Attention을 여러 번 시행하는 것을 가리킨다. 동일한 문장을 여러 명의 독자가 동시에 분석해 최선의 결과를 내려고 하는 것에 비유할 수 있다. Multi-Head attention의 계산 과정은 아래의 수식과 그림과 같이 이루어진다. Query, Key, Value를 Scaled Dot-Product를 통해 얻은 Attention Value를 concatenate한다. 그 후 여기에 $W^{0}$를 내적해 Multi-Head Attention 수행 결과 행렬의 크기를 Scaled Dot-Product Attention의 입력 행렬과 동일하게 맞춘다. Multi-Head Attention 수식MultiHead(Q, K, V) = Concat(head_{1}, \\cdots, head_{h})W^{0}head_{i} = Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i}) Position-wise Feedforward Networks Multi-Head Attention Layer의 입력 행렬과 출력 행렬의 크기는 입력 단어 수 $\\times$ 히든 벡터 차원 수로 동일하다. Position-wise Feedforward Networks Layer에서는 Multi-Head Attention Layer의 출력 행렬을 행 벡터 단위로, 다시 말해 단어 벡터 각각에 관해 아래의 수식을 적용한다. Multi-Head Layer의 출력 행렬 가운데 하나의 단어 벡터를 x라고 하자. 이 x에 관해 두 번의 선형변환을 하는데 그 사이에 activation을 해서 적용한다. Position-wise Feedforward Networks 수식FFN(x) = max(0, x \\cdot W_{1} + b_{1})W_{2} + b_{2}트랜스포머의 학습 전략 트랜스포머의 학습 전략은 warm up이다. 아래 그림과 같이 사용자가 정한 step수에 이르기 까지 learning rate를 높였다가 step 수를 만족하면 조금씩 떨어끄리는 방식이다. 대규모 데이터, 큰 모델 학습에 적합하다. 이 전략은 BERT 등 이후 제안된 모델에도 널리 쓰이고 있다. 이밖에 Layer Normalization 등도 트랜스포머의 안정적인 학습에 기여하고 있는 것으로 보인다. 좀더 자세한 사항을 알고 싶다면 여기를 눌러 공부해보자. BERT(Bidirectional Encoder Representations from Transformer) BERT는 구글에서 공개한 모델이다. 성능이 뛰어나 널리 쓰이고 있다. BERT, ELMo, GPT BERT의 성공 비결은 그 performance가 검증된 트랜스포머 블록을 썼을뿐더러 모델의 속성이 양방향을 지향한다는 점에 있다. 아래 그림은 BERT 이전의 모델인 GPT(Generative Pre-Training)와 ELMo 모델과의 차이점을 시각화한 것이다. GPT는 단어 sequence를 왼쪽에서 오른쪽으로 한 방향으로만 보는 아키텍쳐이다. ELMo는 Bi-LSTM Layer의 상단은 양방향이지만 중간 Layer는 역시 한 방향인 모델이다. 반면 BERT의 경우 모든 Layer에서 양방향 성질을 잃지 않고 있다. BERT와 GPT 모델은 모두 트랜스포머 블록을 사용하고 있다. GPT는 주어진 단어 sequence를 가지고 그 다음 단어를 예측하는 과정에서 학습하는 Language Model이기 때문에 입력 단어 이후의 단어를 모델에게 알려주는 것을 하지 못한다. 따라서 아래 그림 중 1번에 속한다. 이 문제를 극복하기 위해 Masked Language Model이 제안되었다. 주어진 sequence 다음 다음를 맞추는 것에서 벗어나, 일단 문장 전체를 모델에 알려주고, Masking에 해당하는 단어가 어떤 단어일지 예측하는 과정에서 학습해보자는 아이디어이다. 이는 아래 그림 중 2번에 속한다. Masked Language Model Task에서는 모델에 문장 전체를 다 주어도 반칙이 될 수 없다. BERT 모델은 빈칸을 채워야 하기 때문이다. 아래 그림은 GPT가 Scaled Dot-Product Attention을 하는 과정을 도식화한 것이다. 예측해야 할 단어를 보지 않기 위해 softmax score 행렬의 일부 값을 0으로 만든다. 예를 들어, 입력 문장이 ‘뜨끈한, 국밥, 한 그릇’이고 이번에 예측해야 할 단어가 ‘국밥’이라면 GPT는 이전 단어인 ‘뜨끈한’만 참고해야 한다. 반면 BERT는 빈칸만 맞추면 되기 때문에 문장 내 단어 쌍 사이의 관계를 모두 볼 수 있다. BERT 임베딩을 각종 Downstream Task에 적용해 실험한 결과 BERT의 임베딩 품질이 GPT보다 좋음을 입증했다. 또한 같은 BERT 모델이더라도 Pre-Train을 할 때 한 방향(Left-to-Right)만 보게 할 경우 그 성능이 기본 모델 대비 크게 감소하는 것을 확인할 수 있다. 그만큼 모델이 양방향 전후 context를 모두 보게 하는 것이 중요하다는 이야기이다. Pre-Train Task와 학습 데이터 구축 BERT의 Pre-Train Task에는 크게 Masked Language Model과 다음 문장인지 여부 맞추기(NSP, Next Sentence Prediction)로 되어있는데, 이 두가지로 인해 BERT가 양방향 모델이 될 수 있었다. Masked Language Model Task 수행을 위한 학습 데이터는 다음과 같이 만든다. 1) 학습 데이터 한 문장 Token의 15%를 Masking한다. 2) Masking 대상 Token 가운데 80%는 실제 빈칸으로 만들고, 모델은 그 빈칸을 채운다. ex) 뜨끈한 국밥 [Mask] 하는게 낫지 -&gt; 한 그릇 3) Masking 대상 Token 가운데 10%는 랜덤으로 다른 Token으로 대체하고, 모델은 해당 위치의 정답 단어가 무엇일지 맞추도록 한다. ex) 뜨끈한 국밥 [한 개] 하는게 낫지 -&gt; 한 그릇 4) Masking 대상 Token 가운데 10%는 Token을 그대로 두고, 모델은 해당 위치의 정답 단어가 무엇일지 맞추도록 한다. ex) 뜨끈한 국밥 [한 그릇] 하는게 낫지 -&gt; 한 그릇 위와 같이 학습 데이터를 만들게 되면 우리는 다음과 같은 점들을 기대하게 된다. ‘뜨끈한 국밥 [Mask] 하는게 낫지’의 빈칸을 채워야 하기 때문에 문장 내 어느 자리에 어떤 단어를 사용하는게 자연스러운지 앞뒤 문맥을 읽어낼 수 있게 된다. ‘뜨끈한 국밥 한 그릇 하는게 낫지’, ‘뜨끈한 국밥 한 개 하는게 낫지’를 비교해 보면서 주어진 문장이 의미/문법상 비문인지 아닌지 분별할 수 있게 된다. 모델은 어떤 단어가 Masking 될지 전혀 모르기 때문에 문장 내 모든 단어 사이의 의미적, 문법적 관계를 세밀히 살피게 된다. 다음 문자인지 여부(NSP)를 맞추기 위한 학습 데이터는 다음과 같이 만든다. 1) 모든 학습 데이터는 1건당 문장 2 개로 구성된다. 2) 이 가운데 절반은 동일한 문서에서 실제 이어지는 문장을 2 개 뽑고, 그 정답으로 True를 부여한다. 3) 나머지 절반은 서로 다른 문서에서 문장 1개씩 뽑고, 그 정답으로 False를 부여한다. 4) max_num_tokens를 정의한다. 학습 데이터의 90%는 max_num_tokens가 사용자가 정한 max_sequence_length가 되도록 한다. 나머지 10%는 max_num_tokens가 max_sequence_length보다 짧게 되도록 랜덤으로 정한다. 5) 이전에 뽑은 문장 2 개의 단어 총 수가 max_num_token을 넘지 못할 때까지 두 문장 중 단어 수가 많은 쪽을 50%의 확률로 문장 맨 앞 또는 맨 뒤 단어 하나씩 제거한다. 이같이 학습 데이터를 만들면 우리는 다음과 같은 점들을 기대하게 된다. 모델은 ‘내일은 비가 올 것이다’, ‘우산을 챙겨야 할 것 같다’가 이어진 문장인지 아닌지 반복 학습한다. 따라서 문장 간 의미 관계를 이해할 수 있다. NSP Task가 너무 쉬워지는 것을 방지하기 위해 문장 맨 앞 또는 맨 뒤쪽 단어 일부를 삭제했기 때문에 일부 문장 성분이 없어도 전체 의미를 이해하는 데 큰 무리가 없다. 학습 데이터의 10%는 사용자가 정한 최대 길이(max_sequence_length)보다 짧은 데이터로 구성돼 있기 때문에 학습 데이터에 짧은 문장이 포함돼 있어도 성능이 크게 떨어지지 않는다. BERT 모델의 문장 구조 BERT 모델은 트랜스포머 Encoder를 일부 변형한 아키텍쳐이다. 참고로 GPT는 트랜스포머의 Decoder 구조를 일부 변형한 아키텍쳐이다. Original 트랜스포머와 차이점을 위주로 설명할 것이다. BERT 모델은 문장의 시작을 알리는 [CLS], 문장의 종결을 의미하는 [SEP], 마스크 Token [MASK], 배치 데이터의 길이를 맞춰주기 위한 [PAD] 등의 4가지 스페셜 Token을 사용한다. BERT 모델의 입력 Layer을 시각화하면 다음과 같다. 우선 입력 Token에 해당하는 Token 벡터를 참조해 Token 임베딩을 만든다. 여기에 첫번째 문장인지, 두 번째 문장인지에 해당하는 segment 임베딩을 참조해 더해준다. 마지막으로 입력 Token의 문장 내 절대적인 위치에 해당하는 Position Embedding을 더 한다. 이렇게 3개 임베딩을 더한 각각의 벡터에 Layer Normalization을 하고 Dropout을 시행해 첫 번째 트랜스포머 블록의 입력 행렬을 구성한다. 아래 그림처럼 Token 수가 11개인 문장이라면 트랜스포머 블록의 입력행렬의 크기는 11$\\times$ Hidden 차원수가 된다. Token, Segment, Position 벡터를 만들 때 참조하는 행렬은 Pre-Train Task 수행을 잘하는 방향으로 다른 학습 parameter와 함께 업데이트된다. BERT가 사용하는 트랜스포머 블록에서 Original 트랜스포머와 가장 큰 차이점을 보이는 대목은 Position-wise Feedforward Networks 부분이다. Activation function을 기존의 ReLU 대신 GELU(Gaussian Error Linear Units)를 사용한다. 정규분포의 누적분포함수(cumulative distribution functions)인 GELU는 ReLU보다 0 주위에서 부드럽게 변화해 학습 성능을 높인다. Original 트랜스포머와 BERT가 가장 크게 차이를 보이는 또 하나의 부분은 마지막 Prediction Layer이다. Mask Language Model, NSP를 수행하기 위해서이다. Mask Language Model과 관련된 Layer는 실제 모델에서 Pre-Train이 끝나면 그 역할을 다하고 제거되어 Transfer learning의 Pine Tuning할 경우에는 사용되지 않는다. Mask Language Model Layer의 입력은 마지막 트랜스포머 블록의 Mask 위치에 해당하는 Token 벡터이다. 예를 들면, BERT 모델의 입력 문장이 ‘너 오늘 [MASK] 몇시에 할 꺼야’이고, 띄어쓰기 기준으로 Token을 나눈다면 3번째 벡터가 Input_tensor가 된다. 이 벡터를 입력 당시와 동일한 차원 수로 선형변환을 한 뒤 Layer Normalization을 시행한다. 이후 Vocabulary 수 만큼으로 Projection하기 위해 가중치 벡터인 output_weights를 곱하고 output_bias를 더해 logit 벡터를 만든다. 여기서 주목할 점은 입력 Layer에서 Token 벡터를 만들 때 참조하는 행렬을 output_weights로 재사용한다는 점이다. BERT-base 다국어 모델의 단어 수가 10만 개 안팎인 점을 고려하면 계산, 메모리 효율성을 모두 달성하기 위한 전략이라고 생각할 수 있다. NSP Layer의 입력은 마지막 트랜스포머 블록의 첫 번째 Token([[CLS]])에 해당하는 벡터이다. 이 벡터를 2차원으로 projection하는 가중치 행렬 output_weights를 곱하고, 여기에 2차원 크기의 바이어스 벡터를 더한 뒤 softmax함수를 취한다. 이 확률 벡터와 정답(True or False)과 비교해 CrossEntropy를 구하고 이를 최소화하는 방향으로 Parameter들을 업데이트한다. Pre-Train Tutorial BERT 모델을 Pre-Train하려면 GPU가 여러 개 있어야 한다. GPU 8개를 썼을 때 12개 Layer 크기의 기본 모델(BERT-base)를 Pre-Train 하는데 10~15일 정도 소요된다. 리소스가 많지 않은 분들은 이미 공개돼 있는 BERT Pre-Train 모델을 사용하는 것을 추천한다. 자연어 처리 연구자 오연택 님께서 한국어 BERT Pre-Train 모델을 공개했다. Pre-Train 과정 및 hyper parameter 세팅 등 자세한 내용은 여기에서 확인 해볼 수 있다.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP 문장 수준 임베딩 - 01","slug":"NLP_08","date":"2020-02-05T15:32:51.000Z","updated":"2020-02-07T19:58:36.698Z","comments":true,"path":"2020/02/06/NLP_08/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/06/NLP_08/","excerpt":"","text":"참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.문장 수준 임베딩 크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다. 행렬 분해 LSA(잠재 의미 분석) 확률 모형 LDA(잠재 디리클레 할당) Neural Network Doc2Vec ELMo GPT (transformer 구조 - self-attention) BERT (transformer 구조 - self-attention) 잠재 의미 분석(LSA, Latent Semantic Analysis) 단어 수준 임베딩에서의 LSA 방법론들은 word-documents 행렬이나 TF-IDF 행렬, word-context 행렬 또는 PMI 행렬에 SVD로 차원 축소를 시행하고, 여기에서 단어에 해당하는 벡터를 취해 임베딩을 만드는 방법이었다. 문장 수준 입베딩에서의 LSA 방법은 단어 수준 임베딩에서의 LSA 방법론을 통해 얻게된 정확히 말하자면 SVD를 통해 축소된 행렬에서 문서에 대응하는 벡터를 취해 문서 임베딩을 만드는 방식이다. 실습 대상 데이터는 ratsgo.github.uo의 아티클 하나로 markdwon 문서의 제목과 본문을 그대로 텍스트로 저장한 형태이다. 1개 라인이 1개 문서에 해당한다. 불필요한 기호나 LaTex math 패기지의 문법으로 작성되어있는 부분들이 다수 존재한다. 우선 이 실습의 가정을 수식이나 기호는 분석에 있어서 큰 의미를 갖지 않는다라고 가정하고 시작하겠다. 우선, 형태소분석기를 어떤것을 사용하던 가능하게 함수를 하나 만들어준다. 1234567891011121314151617181920from khaiii import KhaiiiApifrom konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkmadef get_tokenizer(tokenizer_name): if tokenizer_name == \"komoran\": tokenizer = Komoran() elif tokenizer_name == \"okt\": tokenizer = Okt() elif tokenizer_name == \"mecab\": tokenizer = Mecab() elif tokenizer_name == \"hannanum\": tokenizer = Hannanum() elif tokenizer_name == \"kkma\": tokenizer = Kkma() elif tokenizer_name == \"khaiii\": tokenizer = KhaiiiApi() else: tokenizer = Mecab() return tokenizer 한 문단 별로 구분자를 어떤것으로 했는지 확인하기 하나씩 프린트해보았다. 12345678910corpus_fname = \"./data/processed/processed_blog.txt\"with open(corpus_fname, 'r', encoding='utf-8') as f: print(f.readline()) print(\"---------------------------------------------------------------------------------------------------------------------\") print(f.readline()) print(\"---------------------------------------------------------------------------------------------------------------------\") print(f.readline()) print(\"---------------------------------------------------------------------------------------------------------------------\") print(f.readline()) 아래 코드를 실행하면 제일 처음 문서의 임베딩과 코사인 유사도가 가장 높은 문서 임베딩의 제목을 return해준다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.decomposition import TruncatedSVDfrom sklearn.preprocessing import normalizefrom sklearn.manifold import TSNEfrom sklearn.metrics.pairwise import cosine_similarityfrom bokeh.io import export_png, output_notebook, showfrom bokeh.plotting import figurefrom bokeh.models import Plot, Range1d, MultiLine, Circle, HoverTool, TapTool, BoxSelectTool, LinearColorMapper, ColumnDataSource, LabelSet, SaveTool, ColorBar, BasicTickerfrom bokeh.models.graphs import from_networkx, NodesAndLinkedEdges, EdgesAndLinkedNodesfrom bokeh.palettes import Spectral8def LSAeval(corpus_file, doc_idx, nth_top): tokenizer = get_tokenizer(\"mecab\") titles, raw_corpus, noun_corpus = [], [], [] with open(corpus_fname, 'r', encoding='utf-8') as f: for line in f: try: title, document = line.strip().split('\\u241E') titles.append(title) raw_corpus.append(document) nouns = tokenizer.nouns(document) noun_corpus.append(' '.join(nouns)) except: continue # 문서(단락)에서 기호들과 조사를 제외하고 명사들만 추출한 데이터 중 Unigram(ngram_range(1,1)), # DF가 1이상(min_df=1)인 데이터를 추려 TF-IDF 행렬을 만들 것이다. vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,1), # tokenizing전에 모든 문자를 소문자로 바꿔준다. lowercase=True, # analyzer == 'word'인 경우만 사용가능. tokenizer=lambda x: x.split()) # 행은 문서, 열은 단어에 각각 대응한다. (204 x 37153) input_matrix = vectorizer.fit_transform(noun_corpus) id2vocab = &#123;vectorizer.vocabulary_[token]:token for token in vectorizer.vocabulary_.keys()&#125; # curr_doc : Corpus 첫번 째 문서의 TF-IDF 벡터 curr_doc, result = input_matrix[doc_idx], [] # curr_doc에서 TF-IDF 값이 0이 아닌 요소들은 내림차순 정렬 # curr_doc은 105개의 원소(단어)만이 저장되어 있는 Compressed Sparse Row format이다. # 그러므로 indices(CSR format index array of the matrix)로 해당 index에 위치하는 단어와 그에대한 tf-idf값을 쌍으로 tuple형태로 넣어준다. for idx, el in zip(curr_doc.indices, curr_doc.data): result.append((id2vocab[idx], el)) sorted(result, key=lambda x : x[1], reverse=True)[:5] # 이번에는 이 TF-IDF 행렬에 100차원 SVD를 수행할 것이다. 204 x 37153의 희소 행렬을 # 204 x 100 크기의 Dense Matrix로 linear Transforamtion하는 것이다. svd = TruncatedSVD(n_components=100) vecs = svd.fit_transform(input_matrix) svd_l2norm_vectors = normalize(vecs, axis=1, norm='l2') cosine_similarity = np.dot(svd_l2norm_vectors, svd_l2norm_vectors[doc_idx]) query_sentence = titles[doc_idx] return titles, svd_l2norm_vectors, [query_sentence, sorted(zip(titles, cosine_similarity), key=lambda x: x[1], reverse=True)[1:nth_top + 1]] 상위 5개의 벡터의 내적이 높은 순으로 내림차순 정력했을때의 결과물 출력 12titles, svd_l2norm_vectors, top_five = LSAeval(corpus_file=\"./data/processed/processed_blog.txt\", doc_idx=0, nth_top=5)top_five 임베딩 시각화 t-SNE 기법을 사용해서 벡터공간을 2차원으로 줄여준 뒤 시각화 할 것이다. 또한 벡터들간의 전체적인 유사도는 시각적으로 그리기보다는 상관행렬 방식으로 나타내 줄 것이다. 시각화에 필요한 함수들 정의 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172def visualize(titles, vectors, mode=\"between\", num_sents=30, palette=\"Viridis256\", use_notebook=False): doc_idxes = random.sample(range(len(titles)), num_sents) sentences = [titles[idx] for idx in doc_idxes] vecs = [vectors[idx] for idx in doc_idxes] if mode == \"between\": visualize_between_sentences(sentences, vecs, palette, use_notebook=use_notebook) else: visualize_sentences(vecs, sentences, palette, use_notebook=use_notebook)def visualize_between_sentences(sentences, vec_list, palette=\"Viridis256\", filename=\"between-sentences.png\", use_notebook=False): df_list, score_list = [], [] for sent1_idx, sentence1 in enumerate(sentences): for sent2_idx, sentence2 in enumerate(sentences): vec1, vec2 = vec_list[sent1_idx], vec_list[sent2_idx] if np.any(vec1) and np.any(vec2): score = cosine_similarity(X=[vec1], Y=[vec2]) # [0][0]인 이유는 값만 뽑아 내기 위해서이다. df_list.append(&#123;'x': sentence1, 'y': sentence2, 'similarity': score[0][0]&#125;) score_list.append(score[0][0]) df = pd.DataFrame(df_list) color_mapper = LinearColorMapper(palette=palette, low=np.max(score_list), high=np.min(score_list)) TOOLS = \"hover,save,pan,box_zoom,reset,wheel_zoom\" p = figure(x_range=sentences, y_range=list(reversed(sentences)), x_axis_location=\"above\", plot_width=900, plot_height=900, toolbar_location='below', tools=TOOLS, tooltips=[('sentences', '@x @y'), ('similarity', '@similarity')]) p.grid.grid_line_color = None p.axis.axis_line_color = None p.axis.major_tick_line_color = None p.axis.major_label_standoff = 0 p.xaxis.major_label_orientation = 3.14 / 3 p.rect(x=\"x\", y=\"y\", width=1, height=1, source=df, fill_color=&#123;'field': 'similarity', 'transform': color_mapper&#125;, line_color=None) color_bar = ColorBar(ticker=BasicTicker(desired_num_ticks=5), color_mapper=color_mapper, major_label_text_font_size=\"7pt\", label_standoff=6, border_line_color=None, location=(0, 0)) p.add_layout(color_bar, 'right') if use_notebook: output_notebook() show(p) else: export_png(p, filename) print(\"save @ \" + filename)def visualize_sentences(vecs, sentences, palette=\"Viridis256\", filename=\"/notebooks/embedding/sentences.png\", use_notebook=False): tsne = TSNE(n_components=2) tsne_results = tsne.fit_transform(vecs) df = pd.DataFrame(columns=['x', 'y', 'sentence']) df['x'], df['y'], df['sentence'] = tsne_results[:, 0], tsne_results[:, 1], sentences source = ColumnDataSource(ColumnDataSource.from_df(df)) labels = LabelSet(x=\"x\", y=\"y\", text=\"sentence\", y_offset=8, text_font_size=\"12pt\", text_color=\"#555555\", source=source, text_align='center') color_mapper = LinearColorMapper(palette=palette, low=min(tsne_results[:, 1]), high=max(tsne_results[:, 1])) plot = figure(plot_width=900, plot_height=900) plot.scatter(\"x\", \"y\", size=12, source=source, color=&#123;'field': 'y', 'transform': color_mapper&#125;, line_color=None, fill_alpha=0.8) plot.add_layout(labels) if use_notebook: output_notebook() show(plot) else: export_png(plot, filename) print(\"save @ \" + filename) 혹시 이러한 error가 난다면, 다음과 같이 PhantomJS를 설치한다. 간단히 말하자면 PhantomJS도 Selenium같이 웹브라우져 개발용으로 만들어진 프로그램이다. bokeh는 javascript기반으로 짜여져있어서 필요한 것 같다. 1conda install -c conda-forge phantomjs 1visualize(titles, svd_l2norm_vectors, mode=\"between\", num_sents=30, palette=\"Viridis256\", use_notebook=True) 1visualize(titles, svd_l2norm_vectors, mode=\"tsne\", num_sents=30, palette=\"Viridis256\", use_notebook=True) Doc2Vec모델 개요 Word2Vec에 이어 구글 연구 팀이 개발한 문서 임베딩 기법이다. 이전 단어 sequence k개가 주어졌을 때 그 다음 단어를 맞추는 언어 모델을 만들었다. 이 모델은 문장 전체를 처음부터 끝까지 한 단어씩 슬라이딩해 가면서 다음 단어가 무엇일지 예측한다. 로그 확률 평균의 값이 커진다는 의미는 이전 k개 단어들을 입력하면 모델이 다음 단어를 잘 예측하므로 로그 확률 평균을 최대화는 과정에서 학습된다. NPLM에서 설명했던 방식처럼 문장 전체를 한 단어씩 슬라이딩해가면서 다음 target word를 맞추는 과정에서 context word에 해당하는 $(w_{t-k}, \\cdots, w_{t-1})$에 해당하는 W 행렬의 벡터들이 업데이트 한다. 따라서 주변 이웃 단어 집합 즉 context가 유사한 단어벡터는 벡터 공간에 가깝게 임베딩 된다.학습이 종료되면 W를 각 단어의 임베딩으로 사용한다. Doc2vec 언어 모델 $T$ : 학습 데이터 문장 하나의 단어 개수 $w_{t}$ : 문장의 t번째 단어 $y_{i}$ : corpus 전체 어휘 집합 중 i번째 단어에 해당하는 점수 1) 이전 k개 단어들을 W라는 단어 행렬에서 참조한 뒤 평균을 취하거나 이어 붙인다. 여기에 U라는 행렬을 내적하고 bias 벡터인 b를 더해준 뒤 softmax를 취한다. U의 크기는 어휘집합 크기 $\\times$ 임베딩 차원 수 이다. $h$ : 벡터 sequence가 주어졌을 때 평균을 취하거나 concatenate하여 고정된 길이의 벡터 하나를 반환하는 역할을 하는 함수이다. L = frac{1}{T} \\sum^{T-1}-{t=k} log(w_{t}|w_{t-k}, \\cdots, w_{t-1})Doc2Vec 언어 모델 Score 계산P(w_{t}| w_{t-k}, \\cdots, w_{t-1}) = \\frac{exp(y_{w_{t}})}{\\sum_{i} exp(y_{i})}y = b + U \\cdot h(w_{t-k}, \\cdots, w_{t-1}; W) 위의 초기 구조에서 문서 id를 추가해 이전 k개 단어들과 문서 id를 넣어서 다음 단어를 예측하게 했다. y를 계산할 때 D라는 문서 행렬(Paragraph matrix)에서 해당 문서 ID에 해당하는 벡터를 참조해 h 함수에 다른 단어 벡터들과 함께 입력하는 것 외에 나머지 과정은 동일하다. 이런 구조를 PV-DM(the Distributed Memory Model of Paragraph Vectors)이라고 부른다. 학습이 종료되면 문서 수 $\\times$ 임베딩 차원 수 크기를 가지는 문서 행렬 D를 각 문서의 임베딩으로 사용한다. 이렇게 만든 문서 임베딩이 해당 문서의 주제 정보를 함축한다고 설명한다. PV-DM은 단어 등장 순서를 고려하는 방식으로 학습하기 때문에 순서 정보를 무시하는 Bag of Words 기법 대비 강점이 있다고 할 수 있을 것이다. 또한, Word2Vec의 Skip-gram을 본뜬 PV-DBOW(the Distributed Bag of Words version of Paragraph Vectors)도 제안했다. Skip-gram은 target word를 가지고 context word들을 예측하는 과정에서 학습되었다. PV-DBOW도 문서 id를 가지고 context word들을 맞춘다. 따라서 문서 id에 해당하는 문서 임베딩엔 문서에 등장하는 모든 단어의 의미 정보가 반영된다. Doc2Vec 실습실습 데이터 영화 댓글과 해당 영화의 ID가 라인 하나를 구성하고 있다. 영화하나를 문서로 보고 Doc2Vec 모델을 학습할 예정이다. 따라서 영화 ID가 동일한 문장들을 하나의 문서로 처리해 줄 것이다. 12345678910111213with open(\"./data/processed/processed_review_movieid.txt\") as f: print(f.readline()) print(\"--------------------------------------------------------------------------------------------\") print(f.readline()) print(\"--------------------------------------------------------------------------------------------\") print(f.readline()) print(\"--------------------------------------------------------------------------------------------\") print(f.readline()) print(\"--------------------------------------------------------------------------------------------\") count = 4 for sentence in f: count+=1 print(\"총 sentence의 개수 : &#123;&#125;개\".format(count)) 결과12345678910111213종합 평점은 4점 드립니다.␞92575--------------------------------------------------------------------------------------------원작이 칭송받는 이유는 웹툰 계 자체의 질적 저하가 심각하기 때문. 원작이나 영화나 별로인건 마찬가지.␞92575--------------------------------------------------------------------------------------------나름의 감동도 있고 안타까운 마음에 가슴도 먹먹 배우들의 연기가 good 김수현 최고~␞92575--------------------------------------------------------------------------------------------이런걸 돈주고 본 내자신이 후회스럽다 최악의 쓰레기 영화 김수현 밖에없는 저질 삼류영화␞92575--------------------------------------------------------------------------------------------총 sentence의 개수 : 712532개 Doc2Vec 모델 학습을 위해 Python gensim 라이브러리의 Doc2Vec 클래스를 사용하는데 Doc2VecInput은 이 클래스가 요구하는 입력 형태를 맞춰주는 역할을 한다. 12345678910111213141516171819202122232425262728293031323334353637from khaiii import KhaiiiApifrom konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkmafrom gensim.models.doc2vec import TaggedDocumentdef get_tokenizer(tokenizer_name): if tokenizer_name == \"komoran\": tokenizer = Komoran() elif tokenizer_name == \"okt\": tokenizer = Okt() elif tokenizer_name == \"mecab\": tokenizer = Mecab() elif tokenizer_name == \"hannanum\": tokenizer = Hannanum() elif tokenizer_name == \"kkma\": tokenizer = Kkma() elif tokenizer_name == \"khaiii\": tokenizer = KhaiiiApi() else: tokenizer = Mecab() return tokenizerclass Doc2VecInput: def __init__(self, fname, tokenizer_name='mecab'): self.fname = fname self.tokenizer = get_tokenizer(tokenizer_name) def __iter__(self): with open(self.fname, encoding='utf-8') as f: for line in f: try: sentence, movie_id = line.strip().split(\"\\u241E\") tokens = self.tokenizer.morphs(sentence) tagged_doc = TaggedDocument(words=tokens, tags=['movie_%s' % movie_id]) yield tagged_doc except: continue dm : 1 (default) -&gt; PV-DM, 0- &gt; PV-DBOW 12345678from gensim.models import Doc2Veccorpus_fname = './data/processed/processed_review_movieid.txt'output_fname = './doc2vec.model'corpus = Doc2VecInput(corpus_fname)model = Doc2Vec(corpus, dm=1, vector_size=100)model.save(output_fname) 학습이 잘 되었는지를 평가하기 위해서 아래와 같이 평가 클래스를 이용할 것이며, 평가를 하면 tag된 영화 id가 나올텐데 직관적으로 그 영화가 어떤 영화인지 모를 것이다. 그러므로 직관적으로 결과를 이해하기 위해 학습 데이터에는 없는 영화 제목을 네이버 영화 사이트에 접속해 id에 맞는 영화 제목을 스크래핑해 올 것이다. 1234567891011121314151617181920212223242526272829import requestsfrom lxml import htmlclass Doc2VecEvaluator: def __init__(self, model_fname=\"data/doc2vec.vecs\", use_notebook=False): self.model = Doc2Vec.load(model_fname) self.doc2idx = &#123;el:idx for idx, el in enumerate(self.model.docvecs.doctags.keys())&#125; self.use_notebook = use_notebook def most_similar(self, movie_id, topn=10): similar_movies = self.model.docvecs.most_similar('movie_' + str(movie_id), topn=topn) for movie_id, score in similar_movies: print(self.get_movie_title(movie_id), score) def get_titles_in_corpus(self, n_sample=5): movie_ids = random.sample(self.model.docvecs.doctags.keys(), n_sample) return &#123;movie_id: self.get_movie_title(movie_id) for movie_id in movie_ids&#125; def get_movie_title(self, movie_id): url = 'http://movie.naver.com/movie/point/af/list.nhn?st=mcode&amp;target=after&amp;sword=%s' % movie_id.split(\"_\")[1] resp = requests.get(url) root = html.fromstring(resp.text) try: title = root.xpath('//div[@class=\"choice_movie_info\"]//h5//a/text()')[0] except: title = \"\" return title 123model_fname='./doc2vec.model'model = Doc2VecEvaluator(model_fname)print(\"영화의 종류 : &#123;&#125; 개\".format(len(model.doc2idx.keys()))) 결과1영화의 종류 : 14730 개 학습 데이터에 포함된 아무 영화 10개의 제목을 보여준다. 1model.get_titles_in_corpus(n_sample=14730) 결과12345678910&#123;'movie_89743': '여자가 두 번 화장할 때', 'movie_16490': '투 문 정션 2', 'movie_100953': '더 퍼지', 'movie_84375': '퍼펙트 센스', 'movie_24203': '섀터드 이미지', 'movie_12054': '나폴레옹', 'movie_10721': '더티 해리', 'movie_11440': '킹 뉴욕', 'movie_20896': '미망인', 'movie_123068': '캠걸'&#125; 해당 id와 유사한 영화 상위 5개를 보여준다. 1model.most_similar(37758, topn=5) 결과12345돈비 어프레이드-어둠 속의 속삭임 0.746237576007843더 퍼지:거리의 반란 0.7402248382568359고양이: 죽음을 보는 두 개의 눈 0.6967850923538208ATM 0.690518856048584힛쳐 0.6751468777656555 잠재 디리클레 할당(LDA, Latent Dirichlet Allocation) 주어진 문서에 대하여 각 문서에 어떤 topic들이 존재하는지에 대한 확률 모형이다. corpus의 이면에 잠재된 topic을 추출한다는 의미에서 topic modeling이라고 부르기도 한다. 문서를 topic 확률 분포로 나타내 각각을 벡터화한다는 점에서 LDA를 임베딩 기법의 일종으로 이해할 수 있다. 모델 개요 LDA는 topic별 단어의 분포, 문서별 topic의 분포를 모두 추정해 낸다. LDA는 topic에 특정 단어가 나타날 확률을 내어 준다. 위의 그림에서 각 색깔별로 topic에 따른 단어가 등장할 확률을 보여주고있다. 문서를 보면 노란색 topic에 해당하는 단어가 많기 때문에 위 문서의 메인 주제는 노란색 topic인 유전자 관련 topic일 가능성이 클 것이다. 이렇듯 문서의 topic 비중 또한 LDA의 산출 결과가 된다. LDA가 가정하는 문서 생성 과정은 우리가 글을 쓸때와 같다. 실제 글을 작성할 때는 글감 내지 주제를 먼저 결정한 후 어떤 단어를 써야 할지 결정한다. 이와 마찬가지로 LDA는 우선 corpus로 부터 얻은 topic 분포로부터 topic을 뽑는다. 이후 해당 topic에 해당하는 단어들을 뽑는다. 그런데 corpus에 등장하는 단어들 각각에 꼬리표가 달려 있는 것은 아니기 때문에 현재 문서에 등장한 단어들은 어떤 topic에서 뽑힌 단어들인지 우리가 명시적으로 알기는 어려울 것이다. 하지만, LDA는 이런 corpus 이면에 존재하는 정보를 추론해낼 수 있다. 아키텍처 LDA가 가정하는 문서 생성 과정은 아래의 그림과 같다. D : corpus 전체 문서 개수 K : 전체 topic 수(hyper parameter) N : d번째 문서의 단어 수 네모칸 : 해당 횟수만큼 반복하라는 의미 $\\phi_{k}$ : k번째 topic에 해당하는 벡터 $\\phi_{k} \\in R^{|V|}$, $\\phi_{k}$는 word-topic 행렬의 k번째 열을 의미한다. $\\phi_{k}$의 각 요소 값은 해당 단어가 k번째 토픽에서 차지하는 비중을 의미하며 확률값이므로 이 벡터의 요소값의 합은 1이 된다. 이런 topic의 단어비중을 의미하는 $\\phi_{k}$는 디리클레 분포를 따른 다는 가정을 취하므로 $\\beta$의 영향을 받는다. $\\theta_{d}$ : d번째 문서가 가진 topic 비중을 나타내는 벡터이다. 그러므로 전체 topic의 개수만큼의 길이 갖으며, 각 벡터는 확률을 의미하므로 합은 1이된다. 문서의 topic 비중 $\\theta_{d}$는 디리클레 분포를 따른다는 가정을 취하므로 $\\alpha$의 영향을 받는다. $ z_{d,n}$ : d번째 문서에서 n번째 단어가 어떤 topic인지를 나타내는 변수이다. 그러므로 이 변수는 d번째 문서의 topic 확률 분포인 $\\theta_{d}$에 영향을 받는다. 동그라미 : 변수를 의미 화살표가 시작되는 변수는 조건, 화살표가 향하는 변수는 결과에 해당하는 변수 관찰 가능한 변수는 d번째 문서에 등장한 n번째 단어 $w_{d,n}$이 유일하다. 우리는 이 정보만을 가지고 하이퍼파라메터(사용자 지정) α,β를 제외한 모든 잠재 변수를 추정해야 한다. 예시를 들자면, 아래 Document-topic 행렬을 살펴보자. 3번째 문서에 속한 단어들은 가장 높은 확률값 0.625를 갖는 topic-2일 가능성이 높다. $w_{d,n}$은 d번째 문서 내에 n번째로 등장하는 단어를 의미하며, 동시에 우리가 유일하게 corpus에서 관찰할 수 있는 데이터이다. 이는 $\\phi_{k}$와 $\\z_{d,n}$에 동시에 영향을 받는다. 예를 들면, $z_{3,1}$가 topic-2이라고 가정했을 경우, $w_{3,1}$은 word-topic 행렬에서 보게되면 제일 높은 확률값 0.393을 갖는 ‘코로나 바이러스’일 가능성이 높다. 이처럼 LDA는 topic의 word 분포($\\phi$)와 문서의 topic 분포($\\theta$)의 결합으로 문서 내 단어들이 생성된다고 가정한다.Document-topic 행렬 문서 topic-1 topic-2 topic-3 문서1 0.400 0.000 0.600 문서2 0.000 0.600 0.400 문서3 0.375 0.625 0.000 word-topic 행렬 단어 topic-1 topic-2 topic-3 코로나 바이러스 0.000 0.393 0.000 우한폐렴 0.000 0.313 0.000 AWS 0.119 0.000 0.000 데이터 엔지니어링 0.181 0.000 0.000 Hadoop 0.276 0.000 0.000 Spark 0.142 0.000 0.000 낭만 닥터 김사부2 0.000 0.012 0.468 tensorflow 0.282 0.000 0.000 마스크 0.000 0.282 0.000 사랑의 불시착 0.000 0.000 0.532 합 1.0 1.0 1.0 실제 관찰 가능한 corpus를 가지고 알고 싶은 topic의 word 분포, 문서의 topic 분포를 추정하는 과정을 통해 LDA는 학습한다. 즉, topic의 word 분포와 문서의 topic 분포의 결합 확률이 커지는 방향으로 학습을 한다는 의미이다. LDA의 단어 생성 과정p(\\phi_{1:K}, \\theta_{1:D}, z_{1:D}, w_{1:D}) =\\prod^{K}_{i=1} p(\\phi_{i}|\\beta) \\prod^{D}_{d=1} p(\\theta_{d}|\\alpha) \\left(\\prod^{N}_{n=1} p(z_{d,n}|\\theta_{d}) p(w_{d,n}|\\phi_{1:K}, z_{d, n}) \\right) 우리가 구해야할 사후확률 분포는 $ p(z, \\phi, \\theta|w) = p(z, \\phi, \\theta, w)/p(w)$를 최대로 만드는 $ z, \\phi, \\theta$를 찾아야 한다. 이 사후확률을 직접 계산하려면 분자도 계산하기 어렵겠지만 분모가 되는 $ p(w)$도 반드시 구해야 확률값으로 만들어 줄 수 있다. $ p(w)$는 잠재변수 $ z, \\phi, \\theta$의 모든 경우의 수를 고려한 각 단어(w)의 등장 확률을 의미하는데, $ z, \\phi, \\theta$를 직접 관찰하는 것은 불가능하다. 이러한 이유로 깁스 샘플링(gibbs sampling)같은 표본 추출 기법을 사용해 사후확률을 근사시키게 된다. 깁스 샘플링이란 나머지 변수는 고정시킨 채 하나의 랜덤변수만을 대상으로 표본을 뽑는 기법이다. LDA에서는 사후확률 분포 $ p(z, \\phi, \\theta|w)$를 구할 때 topic의 단어 분포($\\phi$)와 문서의 topic 분포($\\theta$)를 계산에서 생략하고 topic(z)만을 추론한다. z만 알 수 있으면 나미저 변수를 이를 통해 계산 할 수 있도록 설계 했기 때문이다. 깁스 샘플링 참조 깁스 샘플링을 활용한 LDAp(z_{d, i} = j|z_{-i}, w) = \\frac{ n_{d,k} + \\alpha_{j} }{ \\sum^{K}_{i=1} (n_{d,i}) + \\alpha_{i} } \\times \\frac{ v_{k, w_{d,n} } + \\beta_{w_{d,n}} }{ \\sum^{V}_{ㅓ=1} ( v_{k,j} + \\beta_{j} ) } = ABLDA 변수 표기법 표기 내용 $n_{d,k}$ k번째 topic에 할당된 d번째 문서의 빈도 $v_{k,w_{d,n}}$ 전체 corpus에서 k번째 topic에 할당된 단어 $w_{d,n}$의 빈도 $w_{d,n}$ d번째 문서에 n번째로 등장한 단어 $\\alpha$ 문서의 topic 분포 생성을 위한 디리클레 분포 파라미터 $\\beta$ topic의 word 분포 생성을 위한 디리클레 분포 파라미터 K 사용자가 지정하는 topic 개수 V corpus에 등장하는 전체 word의 수 A d번째 문서가 k번째 topic과 맺고 있는 연관성 정도 B d번째 문서의 n번째 단어($ w_{d,n}$)가 k번째 topic과 맺고 있는 연관성 정도 LDA와 깁스 샘플링 LDA가 각 단어에 잠재된 주제를 추론하는 방식을 살펴본다. 아래표와 같이 단어 5개로 구성된 문서1의 모든 단어에 주제(z)가 이미 할당돼 있다고 가정해보자. LDA는 이렇게 문서 전체의 모든 단어의 주제를 랜덤하게 할당을 하고 학습을 시작하기 때문에 이렇게 가정하는 게 크게 무리가 없다. 또한, topic 수는 사용자가 3개로 이미 지정해 놓은 상태라고 하자. 문서1의 첫 번째 단어($ w_{11} = 천주교)$ 의 주제($z_{11}$)는 3번 topic이다. 마찬가지로 문서1의 3 번째 단어($ w_{13} = 가격$)의 주제($z_{13}$)는 1번 topic이다. 이런 방식으로 Corpus 전체 문서 모든 단어에 topic이 이미 할당됐다고 가정한다. 이로부터 word-topic 행렬을 만들수 있다. 전체 문서 모든 단어에 달린 주제들을 일일이 세어서 만든다. 같은 단어라도 topic이 다른 배(동음다의어)같은 경우가 있으므로 각 단어별로 topic 분포가 생겨난다. 문서 1의 단어별 topic 분포($\\theta$) $z_{1i}$ 3 2 1 3 1 $w_{1,n}$ 천주교 무역 가격 불교 시장 word-topic 행렬 단어 topic-1 topic-2 topic-3 천주교 1 0 35 시장 50 0 1 가격 42 1 0 불교 0 0 20 무역 10 8 1 $\\cdots$ $\\cdots$ $\\cdots$ $\\cdots$ 깁스 샘플링으로 문서1 두 번째 단어의 잠재된 topic이 무엇인지 추론해보자면, 깁스 샘플링을 적용하기 위해 문서1의 두 번째 topic정보를 지울것이다. 그렇다면 아래와 같은 표로 변화될 것이다. 문서1의 단어별 topic 분포 $z_{1i}$ 3 ? 1 3 1 $w_{1,n}$ 천주교 무역 가격 불교 시장 word-topic 행렬 단어 topic-1 topic-2 topic-3 천주교 1 0 35 시장 50 0 1 가격 42 1 0 불교 0 0 20 무역 10 7 = (8 - 1) 1 $\\cdots$ $\\cdots$ $\\cdots$ $\\cdots$ p($ z_{1,2} $)는 A와 B의 곱으로 도출된다. A값은 파란색 영역을 의미하며, 문서 내 단어들의 topic 분포에($\\theta$)에 영향을 받는다. 또한, B값은 topic의 단어 분포($\\phi$)에 영향을 받는다. A와 B를 각각 직사각형의 높이와 너비로 둔다면, p($ z_{1,2} $)는 아래와 같이 직사각형의 넓이로 이해할 수 있다. 이와 같은 방식으로 모든 문서, 모든 단어에 관해 깁스 샘플링을 수행하면 모든 단어마다 topic을 할당해줄 수가 있게 된다. 즉, word-topic 행렬을 완성할 수 있다는 것이다. 보통 1,000회 ~ 10,000회 반복 수행하면 그 결과가 수렴한다고 하며, 이를 토대로 문서의 topic 분포, topic 단어 분포 또한 구할 수 있게 된다. $\\theta$의 경우 각 문서에 어떤 단어사 쓰였는지 조사해 그 단어의 topic 분포를 더해주는 방식으로 계산한다. 사용자가 지정하는 하이퍼파라메터 α 존재 덕분에 A가 아예 0으로 되는 일을 막을 수 있게 된다. 일종의 smoothing 역할을 한다. 따라서 α가 클수록 토픽들의 분포가 비슷해지고, 작을 수록 특정 토픽이 크게 나타나게 된다. 이는 β가 B에서 차지하는 역할도 동일하다. 최적 토픽 수 찾기 LDA의 토픽수 K는 여러 실험을 통해 사용자가 지정하는 미지수인 hyper parameter이다. 최적 토픽수를 구하는 데 쓰는 Perplexity 지표있다. p(w)는 클수록 좋은 inference이므로 exp(−log(p(w)))는 작을수록 좋다. 따라서 토픽 수 K를 바꿔가면서 Perplexity를 구한 뒤 가장 작은 값을 내는 K를 최적의 토픽수로 삼으면 된다. Perplexity(w)=exp\\left[ -\\frac { log\\left\\{ p(w) \\right\\} }{ \\sum _{ d=1 }^{ D }{ \\sum _{ j=1 }^{ V }{ { n }^{ jd } } } } \\right]LDA 실습데이터 소개| 네이버 영화 corpus를 soynlp로 띄어쓰기 교정한 결과를 LDA의 학습 데이터로 사용할 것이다. 아래의 코드는 LDA 모델 피처를 생성하는 역할을 한다. LDA의 입력값은 문서 내 단어의 등장 순서를 고려하지 않고 해당 단어가 몇 번 쓰였는지 그 빈도만을 따진다. 그런데 ‘노잼! 노잼! 노잼!’ 같이 특정 단어가 중복으로 사용된 문서가 있다면 해당 문서의 topic 분포가 한쪽으로 너무 쏠릴 염려가 있다. 이 때문에 token의 순서를 고려하지 않고 중복을 제거한 형태로 LDA 피처를 만들 것이다. 123456789101112131415161718192021from gensim import corporafrom khaiii import KhaiiiApifrom konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkmadef get_tokenizer(tokenizer_name): if tokenizer_name == \"komoran\": tokenizer = Komoran() elif tokenizer_name == \"okt\": tokenizer = Okt() elif tokenizer_name == \"mecab\": tokenizer = Mecab() elif tokenizer_name == \"hannanum\": tokenizer = Hannanum() elif tokenizer_name == \"kkma\": tokenizer = Kkma() elif tokenizer_name == \"khaiii\": tokenizer = KhaiiiApi() else: tokenizer = Mecab() return tokenizer 12345678910111213corpus_fname = './data/processed/corrected_ratings_corpus.txt'documents, tokenized_corpus = [], []tokenizer = get_tokenizer('mecab')with open(corpus_fname, 'r', encoding='utf-8') as f: for document in f: tokens = list(set(tokenizer.morphs(document.strip()))) documents.append(document) tokenized_corpus.append(tokens)dictionary = corpora.Dictionary(tokenized_corpus)corpus = [dictionary.doc2bow(text) for text in tokenized_corpus] corpora.Dictionary 참조 dictionary형태로 vocabulary dictionary를 만들어주는 것이다. add_documents로 문서를 추가할 수도 있다. doc2bow는 bag-of-words (BoW) 형태로 문서를 변환시켜준다. [(token_id, token_count)]형태이다. doc2bow의 옵션으로 return_missing=True를 주면 해당 sentence의 단어중 미등록 단어와 카운트를 같이 출력해준다. 아래 코드를 실행하면 LDA를 학습하고 그 결과를 확인 할 수 있다. LdaMulticore에서 num_topicss는 토픽 수(K)에 해당되는 parameter이다. get_document_topics라는 함수는 학습이 끝난 LDA 모델로부터 각 문서별 topic 분포를 리턴한다. minimum_probability 인자를 0.5를 줬는데, 이는 0.5미만의 topic 분포는 무시한다는 뜻이다. 특정 토픽의 확률이 0.5보다 클 경우에만 데이터를 리턴한다. 확률의 합은 1이기 때문에 해당 토픽이 해당 문서에서 확률값이 가장 큰 토픽이 된다. 12345from gensim.models import ldamulticoreLDA = ldamulticore.LdaMulticore(corpus, id2word=dictionary, num_topics=30, workers=4)all_topics = LDA.get_document_topics(corpus, minimum_probability=0.5, per_word_topics=False) 아래 결과를 해석하자면, 0번 문서는 전체 topic 30개 중 19번에 해당하는 topic의 확률 값이 제일 높으며 그 값은 0.7227057이다. 3번 문서 같은 경우 전체 topic 중 0.5를 넘는 topic이 없음을 확인 할 수 있다. 12for doc_idx, topic in enumerate(all_topics[:5]): print(doc_idx, topic) 결과123450 [(19, 0.7227057)]1 [(9, 0.5350808)]2 []3 [(19, 0.7778823)]4 [(14, 0.80464107)] 모델 저장123456789output_fname = './lda'all_topics = LDA.get_document_topics(corpus, minimum_probability=0.5, per_word_topics=False)with open(output_fname + \".results\", 'w') as f: for doc_idx, topic in enumerate(all_topics): if len(topic) == 1: # tuple 형태로 되어있는 데이터로 가져와서 나눠줌 topic_id, prob = topic[0] f.writelines(documents[doc_idx].strip() + \"\\u241E\" + ' '.join(tokenized_corpus[doc_idx]) + \"\\u241E\" + str(topic_id) + \"\\u241E\" + str(prob) + \"\\n\")LDA.save(output_fname + \".model\") LDA 평가123456789101112131415161718192021222324252627282930313233343536from gensim.models import LdaModelfrom collections import defaultdictclass LDAEvaluator: def __init__(self, model_path=\"./lda\", tokenizer_name=\"mecab\"): self.tokenizer = get_tokenizer(tokenizer_name) self.all_topics = self.load_results(model_path + \".results\") self.model = LdaModel.load(model_path + \".model\") def load_results(self, results_fname): topic_dict = defaultdict(list) with open(results_fname, 'r', encoding='utf-8') as f: for line in f: sentence, _, topic_id, prob = line.strip().split(\"\\u241E\") topic_dict[int(topic_id)].append((sentence, float(prob))) for key in topic_dict.keys(): topic_dict[key] = sorted(topic_dict[key], key=lambda x: x[1], reverse=True) return topic_dict def show_topic_docs(self, topic_id, topn=10): return self.all_topics[topic_id][:topn] def show_topic_words(self, topic_id, topn=10): return self.model.show_topic(topic_id, topn=topn) def show_new_document_topic(self, documents): tokenized_documents = [self.tokenizer.morphs(document) for document in documents] curr_corpus = [self.model.id2word.doc2bow(tokenized_document) for tokenized_document in tokenized_documents] topics = self.model.get_document_topics(curr_corpus, minimum_probability=0.5, per_word_topics=False) for doc_idx, topic in enumerate(topics): if len(topic) == 1: topic_id, prob = topic[0] print(documents[doc_idx], \", topic id:\", str(topic_id), \", prob:\", str(prob)) else: print(documents[doc_idx], \", there is no dominant topic\") 모델 초기화1model = LDAEvaluator('./lda') topic 문서 확인 show_topic_docs 함수에 topic ID를 인자로 주어 실행하면 해당 topic 확률 값이 가장 높은 문서 상위 10개를 출력한다. 1model.show_topic_docs(topic_id=0) 결과123456789101112[('내가 가장 좋아하는 영화! 색감과 영상 인물들의 감정이입 대사 한마디 한마디가 너무나 완벽한. 몇번을 봐도 또 보고싶은 영화.', 0.9707017), ('영화보다가 운적은 정우 주연 영화 바람말고는 없는데 이건 감정이입이 되서 그런지 몰라도 진짜 눈 충혈되도록 펑펑울었음', 0.9677608), ('세 명품배우, 몰입도 최고의 현출,간결하고 시같은 대사,상처받은 사람들의 아름다운 치유!', 0.95923144), ('\"아.. 따듯하다.. \"\"천국의 아이들\"\" 못보신 분 꼭 보세요.. 같은 감독임..\"', 0.957968), ('몸이 마음처럼 움직여 주지 않는 지체장애우들의 혼신의 노력과 열연이 돋보이는 영화였다', 0.95778286), ('영상미 아름답고 주인공의 사랑이 순수하고 풋풋하다. 한 번 더 보고싶은 영화!', 0.9539619), ('음악이 아름답고 가슴이 뭉클하니 감동적이었어. 마음이 따뜻해지는 영화예요.', 0.9539556), ('피아니스트와 같은 동급 영화라 생각합니다 보시면 후회없을거예요 실화영화이니요', 0.953952), ('그냥 고민말고 보세요.진짜 이건 명작이라는 말로는 부족합니다..꼭 보세요!', 0.9491169), ('영화를 보는내내 감정이 이입되고 첫사랑이 보고싶어지는 그런 영화입니다.', 0.9491167)] topic 별 단어 확인 해당 topic ID에서 가장 높은 확률 값을 지니는 단어들 중 상위 n개의 목록을 확인할 수 있다. 어미나 조사가 많이 끼어 있음을 확인할 수 있다. LDA의 품질을 끌어 올리기 위해 피처를 만드는 과정에서 명사만 쓰기도 한다. 1model.show_topic_words(topic_id=0) 결과12345678910[('보', 0.03599964), ('는', 0.03479155), ('.', 0.030642439), ('고', 0.030473521), ('영화', 0.029688885), ('이', 0.018665483), ('로', 0.017465018), ('내내', 0.016784767), ('다', 0.016693212), ('한', 0.013309637)] 새로운 문서의 topic 확인 show_new_document_topic 함수는 새로운 문서의 topic을 확인하는 역할을 한다. 문서를 형태소 분석한 뒤 이를 LDA 모델에 넣어 topic을 추론해 가장 높은 확률 값을 지니는 topic id와 그 확률을 리턴해준다. 해당 문서의 topic 분포 중 0.5를 넘는 지배적인 topic이 존재하지 않을 경우 ‘there is no dominant topic’메시지를 리턴한다. 1model.show_new_document_topic([\"너무 사랑스러운 영화\", \"인생을 말하는 영화\"]) 결과12너무 사랑스러운 영화 , topic id: 28 , prob: 0.8066608인생을 말하는 영화 , topic id: 9 , prob: 0.7323683 12 12 12","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP - 단어 수준 임베딩","slug":"NLP_06","date":"2020-02-01T11:47:03.000Z","updated":"2020-02-07T19:26:13.782Z","comments":true,"path":"2020/02/01/NLP_06/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/01/NLP_06/","excerpt":"","text":"단어 수준 임베딩 예측 기반 모델 NPLM Word2Vec FastText 행렬 분해 기반 모델 LSA GloVe Swivel 단어 임베딩을 문장 수준 임베딩으로 확장하는 방법 가중 임베딩(Weighted Embedding) NPLM(Neural Probabilistic Language Model) NLP 분야에서 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델로서 임베딩 역사에서 차지하는 역할이 작지 않다. ‘단어가 어떤 순서로 쓰였는가’라는 가정을 통해 만들어진 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 만들어졌다는데 의의가 있으며 NPLM 자체가 단어 임베딩 역할을 수행할 수 있다. 다음과 같은 한계점들이 기존에는 존재했다. 1) 학습 데이터에 존재하지 않는 데이터에 대해 나타날 확률을 0으로 부여 한다. 물론, 백오프(back-off)나 스무딩(smoothing)으로 완화시켜 줄 순 있지만 근본적인 해결방안은 아니였다. 2) 문장의 장기 의존성(long-term dependency)을 포착해내기 어렵다. 다시 말해서 n-gram모델의 n을 5 이상으로 길게 설정할 수 없다. n이 커질수록 확률이 0이될 가능성이 높기 때문이다. 3) 단어/문장 간 유사도를 계산할 수 없다. NLPM의 학습 NLPM은 직전까지 등장한 n-1개(n: gram으로 묶을 수) 단어들로 다음 단어를 맞추는 n-gram 언어 모델이다. NLPM 구조의 말단 출력 $|V|$(corpus의 token vector, 어휘집합의 크기) 차원의 score 벡터 $y_{w_{t}}$에 softmax 함수를 적용한 $|V|$차원의 확률 벡터이다. NPLM은 확률 벡터에서 가장 높은 요소의 인덱스에 해당하는 단어가 실제 정답 단어와 일치하도록 학습 한다. P(w_{t})|w_{t-1}, \\cdots ,w_{t-n+1}) = \\frac{exp(y_{w_{t}})}{\\sum_{i} exp(y_{i}) }y_{w_{t}} \\in R^{|V|} : w_{t}라는 단어에 해당하는 점수 벡터 NLPM 구조의 입력 문장 내 t번째 단어($w_{t}$)에 대응하는 단어 벡터 $x_{t}$를 만드는 과정은 다음과 같다. 먼저 $|V| \\times m (m: x_{t}의 차원 수)$크기를 갖는 행렬 C에서 $w_{t}$에 해당하는 벡터를 참조하는 형태이다. C 행렬의 원소값은 초기에 랜덤 설정한다. 참조한다는 의미는 예를 들어 아래 그림에서 처럼 어휘 집합에 속한 단어가 5개이고 $w_{t}$가 4번째 인덱스를 의미한다고 하면 행렬 $C$와 $w_{t}$에 해당하는 one-hot 벡터를 내적한 것과 같다. 즉 $C$라는 행렬에서 $w_{t}$에 해당하는 행만 참조하는 것과 동일한 결과를 얻을 수 있다. 따라서 이 과정을 Look-up table이라는 용어로 많이 사용한다 문장 내 모든 단어들을 한 단어씩 훑으면서 Corpus 전체를 학습하게 된다면 NPLM 모델의 C 행렬에 각 단어의 문맥 정보를 내재할 수 있게 된다. x_{t} = w_{t} \\cdot C = C(w_{t}), C \\in R^{|v| \\times m} 모델 구조 및 의미정보이 때, walking을 맞추는 과정에서 발생한 손실(train loss)를 최소화하는 그래디언트(gradient)를 받아 각 단어에 해당하는 행들이 동일하게 업데이트 된다. 따라서 이 단어들의 벡터는 벡터 공간에서 같은 방향으로 조금씩 움직인다고 볼 수 있다. 결과적으로 임베딩 벡터 공간에서 해당 단어들 사이의 거리가 가깝다는 것은 의미가 유사하다라는 의미로 해석할 수 있다. NPLM의 특징 NPLM은 그 자체로 언어 모델 역할을 수행할 수 있다. 기존의 통계 기반 n-gram 모델은 학습 데이터에 한 번도 등장하지 않은 패턴에 대해서는 그 등장 확률을 0으로 부여하는 문제점을 NPLM은 문장이 Corpus에 없어도 문맥이 비슷한 다른 문장을 참고해 확률을 부여하는 방식으로 극복하는데 큰 의의가 있다. 하지만, 학습 파라미터가 너무 많아 계산이 복잡해지고, 자연스럽게 모델 과적합(overfitting)이 발생할 가능성이 높다는 한계를 가진다. 이 한계를 극복하기 위해 다음 임베딩 방법론인 Word2Vec이 등장하였다. Word2VecWord2vec은 가장 널리 쓰이고 있는 단어 임베딩 모델이다. Skip-gram과 CBOW라는 모델이 제안되었고, 이 두 모델을 근간으로 하되 negative sampling등 학습 최적화 기법을 제안한 내용이 핵심이다. CBOW 주변에 있는 context word들을 가지고 target word 하나를 맟추는 과정에서 학습된다. 입,출력 데이터 쌍 {context words, target word} Skip-gram 처음 제안된 방식은 target word 하나를 가지고 context word들이 무엇일지 예측하는 과정에서 학습된다. 하지만, 이 방식은 정답 문맥 단어가 나타날 확률은 높이고 나머지 단어들 확률은 그에 맞게 낮춰야 한다. 그런데 어휘 집합에 속한 단어 수는 보통 수십만 개나되므로 이를 모두 계산하려면 비효율 적이다. 이런 점을 극복하기 위해 negative sampling이라는 target word와 context word 쌍이 주어졌을 때 해당 쌍이 positive sample인지 negative sample인지 이진 분류하는 과정에서 학습하는 방식을 제안했다. 이런다면 학습 step마다 1개의 positive sample과 나머지 k개(임의의 k:target 단어의 negative sampling 개수)만 계산하면 되므로 차원수가 2인 시그모이드를 k+1회만 계산하면된다. 이전의 매 step마다 어휘 집합 크기만큼의 차원을 갖는 softmax를 1회 계산하는 방법보다 계산량이 훨씬 적다. 또한 Corpus에서 자주 등장하지 않는 희귀한 단어가 negative sample로 조금 더 잘 뽑힐 수 있도록 하고 자주 등장하는 단어는 학습에서 제외하는 subsampling이라는 기법을 적용하였다. Skip-gram은 Corpus로 부터 엄청나게 많은 학습 데이터 쌍을 만들어 낼 수 있기 때문에 고빈도 단어의 경우 등장 횟수만큼 모두 학습시키는 것이 비효울적이라고 보았다. 이 또한, 학습량을 효과적으로 줄여 계산량을 감소시키는 전략이다. 작은 Corpus는 k=5~20, 큰 Corpus는 k=2~5로 하는 것이 성능이 좋다고 알려져 있다. skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 CBOW보다 좋은 경향이 있다. 입,출력 데이터 쌍 {target word, target word 전전 단어}, {target word, target word 직전 단어}, {target word, target word 다음 단어}, {target word, target word 다다음 단어} negative sample Prob P_{negative}(w_{i}) = \\frac{U(w_{i})^{3/4}}{\\sum^{n}_{j=0}} U(w_{i]}^{3/4})U(w_{i]}) = \\frac{해당 단어 빈도}{전체 단어 수} = 해당 단어의 Unigram Prob subsampling Prob P_{subsampling}(w_{i}) = 1 - \\sqrt(\\frac{t}{f(w_{i})}) = w_{i}가 학습에서 제외될 확률f(w_{i]}) = w_{i]}'s frequency, t = 0.00001 t, c가 positive sample(=target word 주변에 context word가 존재)일 확률 target word와 context가 실제 positive sample이라면 아래의 조건부 확률을 최대화해야 한다. 모델의 학습 parameter는 U와 V 행렬 두개 인데, 둘의 크기는 어휘 집합 크기$(|V|) \\times 임베딩 차원 수(d)$로 동일하다. U와 V는 각각 target word와 context word에 대응한다. P(+|t, c) = \\frac{1}{1 + exp(-u_{t}v_{c})} \u001c위의 식을 최대화 하려면 분모를 줄여야한다. 분모를 줄이려면 $exp(-u_{t}v_{c})$를 줄여야 한다. 그러려면 두 벡터의 내적값이 커지게 해야한다. 이는 코사인유사도와 비례함을 알 수 있다. 결론적으로 두 벡터간의 유사도를 높인다는 의미이다. 잘 이해가 가지 않는다면 아래과 그림을 보자. A 가 B에 정확히 포개어져 있을 때(θ=0도) cos(θ)는 1이다. 녹색선의 길이가 단위원 반지름과 일치하기 때문이다. B는 고정한 채 A가 y축 상단으로 옮겨간다(θ가 0도에서 90도로 증가)고 할때 cos(θ)는 점점 감소하여 0이 되게 됩니다. 아래 그림의 경우 빨간색 직선이 x축과 만나는 점이 바로 cos(θ)를 의미한다. t, c가 negative sample(target word와 context word가 무관할때)일 확률 만약 학습데이터가 negative sample에 해당한다면 아래의 조건부 확률을 최대화하여야 한다. 이 때는 분자를 최대화 해주어야 하므로, 두 벡터의 내적값을 줄여야 한다. P(-|t, c) = 1 - P(+|t,c) = \\frac{exp(-u_{t}v_{c})}{1 + exp(-u_{t}v_{c})} 모델의 손실함수를 이제 알았으니 최대화를 하는 파라미터를 찾으려면 MLE를 구해야 할 것이다. 그렇다면 log likelihood function은 아래와 같을 것이다. 임의의 모수인 모델 파라미터인 $\\theta$라고 가정 했을때, $\\theta$를 한 번 업데이트할 때 1개 쌍의 positive sample과 k개의 negative sample이 학습된다는 의미이다. Word2vec은 결국 두 단어벡터의 유사도 뿐만아니라 전체 Corpus 분포 정보를 단어 Embedding에 함축시키게 된다. 분포가 유사한 단어 쌍은 그 속성 또한 공유할 가능성이 높다. 유사도 검사를 통해 비슷한 단어들을 출력 했을때, 그 단어들이 반드시 유의 관계를 보여준다기 보다는 동일한 속성을 갖는 관련성이 높은 단어를 출력한다는 의미로 이해해야한다. 모델 학습이 완료되면 U(target_word에 관한 행렬)만 d차원의 단어 임베딩으로 사용할 수도 있고, U+V.t 행렬을 임베딩으로 쓸 수도 있다. 혹은 concatenate([U, V.t])를 사용할 수도 있다. L(\\theta) = log P (+|t_{p},c_{p}) + \\sum^{k}_{i=1} log P (-|t_{n_{i}},c_{n_{i}})참고12345678910111213141516171819202122232425262728293031323334353637# from gensim.models import word2vecfrom gensim.models import Word2Veccorpus = '원하는 텍스트를 단어를 기준으로 tokenize한 상태의 파일(가장 쉽게는 공백을 기준으로)'# model = word2vec.Word2Vec()model = Word2Vec(corpus, size=임베딩 특징 벡터 차원수, # 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다. min_count=단어에 대한 최소 빈도 수, # default negative=5, 보\u001c통 5~20을 많이 사용 negative=negative sample을 뽑는 k, workers=학습시 사용하는 프로세스 개수, window=context window 크기, # 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정한다. # 유용한 범위는 (0, 1e-5)이며, 보통 0.001이 좋은 성능을 낸다고 한다. sample=다운 샘플링 비율, # default sg=0 =&gt; CBOW, if sg=1 =&gt; skip-gram sg=1 )model.save(\"모델을 저장할 directory path\")# 저장했던 모델을 불러와서 추가적으로 훈련시킬 수 있다.model = Word2Vec.load(\"이미 존재하는 모델의 directory path\")model.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)# 훈련된 벡터를 KeyedVector로 분리하는 이유는 전체 모델 상태가 더 이상 필요하지 않을 경우(훈련을 계속할 필요가 없음)# 모델이 폐기될 수 있기 때문에 프로세스 간에 RAM의 벡터를 빠르게 로드하고 공유할 수 있는 훨씬 작고 빠른 상태로 만드는 것이다.vector = model.wv['computer']from gensim.models import KeyedVectorspath = get_tmpfile(\"wordvectors 파일명\")model.wv.save(path)wv = KeyedVectors.load(\"model.wv\", mmap='r')vector = wv['computer'] 학습이 완료된 임베딩 결과물을 활요하여 코사인 유사도가 가장 높은 단어들을 뽑아 임베딩을 평가해 볼 수도 있다. 이는 추후에 한번에 소개할 것이다. FastText Facebook에서 개발해 공개한 단어 임베딩 기법이다. 각 단어를 문자단위 n-gram으로 표현한다. 이외의 점은 모두 Word2Vec과 같다. 동일하게 negative sampling을 사용하며, 조금 다른 점은 Fasttext는 target word(t), context word(c) 쌍을 학습할 때 target word(t)에 속한 문자 단위 n-gram 벡터(z)들을 모두 업데이트 한다는 점이다. 설치 방법은 gensim에서 FastText를 제공하고 있기에 pip를 통해 설치해주거나 이 방법이 안된다면, 참조페이지를 클릭해서 직접 C++방식으로 받아도 상관없다. 모델 기본 구조 예를 들어 시나브로라는 단어의 문자 단위 3-gram은 다음과 같이 n-gram 벡터의 합으로 표현한다. 아래 식에서 $G_{t}$는 target word t에 속한 문자 단위 n-gram집합을 의미한다. Fasttext의 단어 벡터 표현(&lt;,&gt;는 단어의 경계를 나타내 주기 위해 모델이 사용하는 기호) u_{시나브로} = z_{} + z_{시나브로}, u_{t}=\\sum_{g \\in G_{t}} z_{g} n-gram 참조 및 NLP에 도움이 되는 사이트 n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어진다. 그렇기 때문에 적절한 n을 선택해야 한다. trade-off 문제로 인해 정확도를 높이려면 n은 최대 5를 넘게 잡아서는 안 된다고 권장되고 있다. 손실함수 자체는 위의 식을 word2vec 손실함수 $u_{t}$에 대입해 주기만 하면된다. FastText 모델의 강점은 조사나 어미가 발달한 한국어에 좋은 성능을 낼 수 있다는 점이다. 용언(동사, 형용사)의 활용이나 그와 관계된 어미들이 벡터 공간상 가깝게 임베딩 되기 때문이다.(예를들면, ‘하였다’가 t이고, ‘공부’가 c라면 ‘공부’와 ‘했(다), 하(다), 하(였으며)’등에 해당하는 벡터도 비슷한 공간상에 있다는 의미이다.) 한글은 자소 단위(초성, 중성, 종성)로 분해할 수 있고, 이 자소 각각을 하나의 문자로 보고 FastText를 실행할 수 있다는 점도 강점이다. 또한, 각 단어의 임베딩을 문자 단위 n-gram 벡터의 합으로 표현하기 때문에 오타나 미등록단어(unknown word)에도 robust하다. 그래서 미등록된 단어도 벡터를 뽑아낼수 있다. 동일한 음절이나 단어를 가진 공간상의 벡터를 추출할 수 있기 때문이다. 다른 단어 임베딩 기법이 미등록 단어 벡터를 아예 추출할 수 없다는 사실을 감안하면 FastText는 경쟁력이 있다. Fasttext 참조 123456789101112131415161718192021222324from gensim.models import FastTextcorpus = '원하는 텍스트를 단어를 기준으로 tokenize한 상태의 파일(가장 쉽게는 공백을 기준으로)'model = Word2Vec(corpus, size=임베딩 특징 벡터 차원수, # 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다. min_count=단어에 대한 최소 빈도 수, # default negative=5, 보\u001c통 5~20을 많이 사용 negative=negative sample을 뽑는 k, workers=학습시 사용하는 프로세스 개수, window=context window 크기, # 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정한다. # 유용한 범위는 (0, 1e-5)이며, 보통 0.001이 좋은 성능을 낸다고 한다. sample=다운 샘플링 비율, # default sg=0 =&gt; CBOW, if sg=1 =&gt; skip-gram sg=1, # default word_ngrams=1 =&gt; n-gram 사용, 0 =&gt; 미사용(word2vec과 동일) word_ngrams=1, # n-gram 최소 단위 min_n=3, # n-gram 최대 단위 (최소단위보단 커야한다.) max_n=6, ) 잠재 의미 분석(LSA, Latent Semantic Analysis) word-document 행렬이나 TF-IDF 행렬, word-context 행렬 같은 커다란 행렬에 차원 축소 방법의 일종인 특이값 분해를 수행해 데이터의 차원 수를 줄여 계산 효율성을 키우는 한편 행간에 숨어있는 잠재 의미를 추출해내는 방법론이다. 예를 들면, word-documents 행렬이나 word-context 행렬 등에 SVD를 한 다음 그 결과로 도출되는 행벡터들을 단어 임베딩으로 사용할 수 있다. 잠재 의미 분석은 GloVe나 Swivel과 더불어 Matrix Factorization 기반의 기법으로 분류된다. PPMI(점별 상호 정보량) 행렬 word-document 행렬, TF-IDF 행렬, word-context 행렬, PMI 행렬에 모두 LSA를 수행할 수 있다. 이 중 PMI 행렬을 보완하는 PPMI 행렬에 대해 소개하고자한다. PMI 행렬과 위의 행렬들을 모른다면 클릭! PPMI란 간단히 말해 우리가 가진 말뭉치의 크기가 충분히 크지 않다면, PMI식의 로그 안 분자가 분모보다 작을 때 음수가 되거나, 극단적으로 단어 A,B가 단 한번도 같이 등장하지 않는다면 $-inf$값을 갖게 된다. 이러한 이유로 NLP 분야에서는 PMI 대신 PPMI(Positive Pointwise Mutual Information)를 지표로 사용한다. PMI가 양수가 아닌 경우 그 값을 신뢰하기 힘들어 0으로 치환해 무시한다는 뜻이다. PPMI(A, B) = max(PMI(A,B), 0) Shifted PMI(SPMI)는 Word2Vec과 깊은 연관이 있다는 논문이 발표되기도 했다. SPMI(A, B) = PMI(A, B) - log k, k > 0행렬 분해로 이해하는 잠재 의미 분석 Eigenvalue Decomposition(고유값 분해)를 우선 알고 있다는 전제조건으로 SVD를 모르실수도 있는 분들을 위해 간략히 설명하자면, 고유값 분해는 행렬 A가 정방행렬일 경우만 가능한데, 만약 정방행렬이 아닌 행렬은 고유값 분해를 어떻게 해야 하는지에 대한 개념이라고 말할 수 있겠다. 혹시 고유값 분해도 잘 모르시겠다면 이곳을 클릭해서 필자가 추천하는 강의들을 꼭 공부해 보시길 추천한다. 필자는 개인적으로 선형대수는 Computer Science(or 데이터 분석)를 하는데 기본적으로 어느 정도 알고 있어야 한다고 생각한다. 참조 예를 들어, 행렬 A의 m개의 word, n개 documents로 이루어져 shape이 $ m \\times n $인 word-documents 행렬에 truncated SVD를 하여 LSA를 수행한다고 가정해본다. 그렇다면 U는 단어 임베딩, V.t는 문서 임베딩에 대응한다. 마찬가지로 m개 단어, m개 단어로 이루어진 PMI 행렬에 LSA를 하면 d차원 크기의 단어 임베딩을 얻을 수 있다. 각종 연구들에 따르면 LSA를 적용하면 단어와 문맥 간의 내재적인 의미를 효과적으로 보존할 수 있게 돼 결과적으로 문서 간 유사도 측정 등 모델의 성능 향상에 도움을 줄 수 있다고 한다. 또한 입력 데이터의 노이즈, sparsity(희소)를 줄일 수 있다. 행렬 분해로 이해하는 Word2Vec negative sampling 기법으로 학습된 Word2Vec의 Skip-gram 모델(SGNS, Skip-Gram with Negative Sampling)은 Shifted PMI 행렬을 분해한 것과 같다는 것을 볼 수 있다. $A_{ij}$는 SPMI행렬의 i,j번째 원소이다. k는 Skip-gram 모델의 negative sample 수를 의미한다. 그러므로 k=1인 negative sample 수가 1개인 Skip-gram 모델은 PMI 행렬을 분해하는 것과 같다. A^{SGNS}_{ij} = U_{i} \\cdot V_{j} = PMI(i,j) - log k soynlp에서 제공하는 sent_to_word_contexts_matrix 함수를 활용하면 word-context 행렬을 구축할 수 있다. dynamic_weight=True는 target word에서 멀어질수록 카운트하는 동시 등장 점수(co-occurrence score)를 조금씩 깎는다는 의미이다. dynamic_weight=False라면 window 내에 포함된 context word들의 동시 등장 점수는 target word와의 거리와 관계 없이 모두 1로 계산한다. 예를 들어서 window=3이고 ‘도대체 언제쯤이면 데이터 사이언스 분야를 조금은 공부했다고 말할 수 있을까…’라는 문장의 target word가 ‘분야’라면, ‘를’과 ‘사이언스’의 동시 등장 점수는 1, ‘데이터’, ‘조금’은 0.66, ‘은’, ‘이면’은 0.33이 된다. word-context 행렬을 활용한 LSA 12345678910111213141516from sklearn.decomposition import TruncatedSVDfrom soynlp.vectorizer import sent_to_word_contexts_matrixcorpus_filecorpus = [sent.replace('\\n', '').strip() for sent in open(corpus_file, 'r').readlines()]input_matrix, idx2vocab = sent_to_word_contexts_matrix(corpus, window=3, # 최소 단어 빈도 수 min_tf=10, dynamic_weight=True, verbose=True)cooc_svd = TruncatedSVD(n_components=100)cooc_vecs = cooc_svd.fit_transform(input_matrix) 구축한 word-context 행렬에 soynlp에서 제공하는 pmi 함수를 적용한다. min_pmi 보다 낮은 PMI 값은 0으로 치환한다. 따라서 min_pmi=0으로 설정하면 정확히 PPMI와 같다. 또한, pmi matrix의 차원수는 어휘 수 x 어휘 수의 정방 행렬이다. 1234from soynlp import pmippmi_matrix, _, _ = pmi(input_matrix, min_pmi=0)ppmi_svd = TruncatedSVD(n_components=100)ppmi_vecs = ppmi_svd.fit_transform(input_matrix) GloVe(Global Word Vectors) 미국 스탠포트대학교연구팀에서 개발한 단어 임베딩 기법이다. 임베딩된 단어 벡터 간 유사도 측정을 수월하게 하면서도 Corpus 전체의 통계 정보를 좀 더 잘 반영하는 것을 지향하여 Vanilla Word2Vec과 LSA 두 기법의 단점을 극복하고자 했다. LSA(잠재 의미 분석)은 Corpus 전체의 통계량을 모두 활용할 수 있지만, 그 결과물로 단어 간 유사도를 측정하기는 어렵다. 반대로 Vanilla Word2Vec은 단어 벡터 사이의 유사도를 측정하는 데는 LSA보다 유리하지만 사용자가 지정한 window 내의 local context만 학습하기 때문에 Corpus 전체의 통계 정보는 반영되기 어렵다는 단점을 지닌다. 물론 GloVe 이후 발표된 Skip-gram 모델이 Corpus 전체의 Global한 통계량인 SPMI 행렬을 분해하는 것과 동치라는 점을 증명하기는 했다. 손실 함수 임베딩된 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 증장 빈도의 로그 값이 되도록 정의했다. 단어 i,j 각각에 해당하는 벡터 $U_{i}$, $V_{j}$ 사이의 내적값과 두 단어 동시 등장 빈도 $A_{ij}$의 로그값 사이의 차이가 최소화될수록 학습 손실이 작아진다. bias항 2개와 f(A_{ij})는 임베딩 품질을 높이기 위해 고안된 장치이다. Glove는 word-context 행렬 A를 만든 후에 학습이 끝나면 U를 단어 임베딩으로 사용하거나 U+V.t, concatenate([U, V.t])를 임베딩으로 사용할 수 있다. J = \\sum^{|V|}_{i,j=1} f(A_{ij}) (U_{i} \\cdot V_{j} + b_{i} + b+{j} - log A_{ij})^{2}Swivel Google 연구팀이 발표한 행렬 분해 기반의 단어 임베딩 기법이다. PMI 행렬을 U와 V로 분해하고, 학습이 종료되면 U를 단어 임베딩으로 쓸 수 있으며 U+V.t, concatenate([U, V.t])도 임베딩으로 사용할 수 있다. PMI 행렬을 분해한다는 점에서 word-context 행렬을 분해하는 GloVe와 다르며, Swivel은 목적함수를 PMI의 단점을 보완할 수 있도록 설계했다. 두 단어가 한번도 동시에 등장하지 않았을 경우 PMI가 -inf로 가능 현상을 보완하기 위해 이런경우의 손실함수를 따로 정의했다. 그 결과, i,j가 각각 고빈도 단어인데 두 단어의 동시 등장빈도가 0이라면 두 단어는 정말로 등장하지 않는 의미상 무관계한 단어라고 가정하고, 단어 i,j가 저빈도 단어인데 두 단어의 동시 등장빈도가 0인 경우에는 두 단어는 의미상 관계가 일부 있을 수 있다고 가정한다. 단어 임베딩 평가 방법 참고로 카카오브레인 박규병 님께서는 한국어, 일본어, 중국어 등 30개 언어의 단어 임베딩을 학습해 공개했다. 모델은 주로 해당 언어의 위키백과 등으로 학습됐으며 벡터 차원 수는 100, 3000차원 두 종류가 있다. https://github.com/Kyubyong/wordvectors 단어 유사도 평가(word similarity test) 일련의 단어 쌍을 미리 구성한 후에 사람이 평가한 점수와 단어 벡터 간 코사인 유사도 사이의 상관관계를 계산해 단어 임베딩의 품질을 평가하는 방법이다. Word2Vec과 FastText 같은 예측 기반 임베딩 기법들이 GloVe, Swivel 등 행렬 분해 방법들에 비해 상관관계가 상대적으로 강한 것을 알 수 있다. 물론 무조건 예측기반이 좋다는 의미는 아니다. 데이터에 다르겠지만 보통은 저런 결과를 얻을 것이다. 단어 유추 평가(word analogy test) 의미론적 유추에서 단어 벡터 간 계산을 통해 갑 - 을 + 병 = 정을 통해 평가하는 방법이다. 갑 - 을 + 병에 해당하는 벡터에 대해 코사인 유사도가 가장 높은 벡터에 해당하는 단어가 실제 정인지를 확인한다. 단어 임베딩 시각화 시각화 또한 단어 임베딩을 평가하는 방법이다. 다만 단어 임베딩은 보통 고차원 벡터이기 때문에 사람이 인식하는 2, 3차원으로 축소해 시각화를 하게 된다. t-SNE(t-Stochastic Neighbor Embedding)은 고차원의 원공간에 존재하는 벡터 x의 이웃 간의 거리를 최대한 보존하는 저차원 벡터 y를 학습하는 방법론이다. 원 공간의 데이터 확률 분포와 축소된 공간의 분포 사이의 차이를 최소화하는 방향으로 벡터 공간을 업데이트한다. 가중 임베딩 단어 임베딩을 문장 수준 임베딩으로 확장하는 방법을 설명하겠다. 아주 간단한 방법이지만 성능 효과가 좋아서 사용해볼만한 방법이다. 미국 프린스턴 대학교 연구팀이 ICLR에 발표한 방법론이다. 모델 개요 Arora et al.(2016)은 문서 내 단어의 등장은 저자가 생각한 주제에 의존한다고 가정했다. 이를 위해 주제 벡터(discourse vector)라는 개념을 도입했다. 주제 벡터 $c_{s}$가 주어졌을 때 어떤 단어 w가 나타날 확률을 아래와 같이 정의했다. $\\tilde{c_{s}}$는 $c_{s}$로 부터 도출되는데 그 과정은 생략하고, 간단히 말하면 주제 벡터 c_{s}와 거의 비슷한 역할을 하는 임의의 어떤 벡터라고 보겠다. Z는 우변 두번째 항이 확률 값이 되도록 해주는 Normalize Factor이다. 우변의 첫째항은 단어 w가 주제와 상관없이 등장할 확률이며, 한국어에서는 조사(은,는,이,가 등)가 P(w)가 높은 축에 속한다. 두 번째 항은 단어 w가 주제와 관련을 가질 확률을 의미한다. 주제 벡터 $\\tilde{c_{s}}$와 w에 해당하는 단어 벡터 $v_{w}$가 내적값이 클수록 그 값이 커진다. $\\alpha$는 사용자가 지정하는 hyper parameter이다. 단어 등장 확률 P(w|c_{s}) = \\alpha P(w) + (1-\\alpha) frac{ exp( \\tilde{c_{s}} \\cdot v_{w}) }{Z} 단어 sequence는 문장이다. 문장 등장 확률(단어들이 동시에 등장할 확률)은 문장에 속한 모든 단어들이 등장할 확률의 누적 곱으로 나타낼 수 있다. 그런데 확률을 누적해서 곱하면 너무 작아지는 underflow 문제가 발생하므로 로그를 취해 덧셈을 하는 것으로 대체한다. 문장 등장확률 P(s|c_{s}) \\propto \\sum_{w \\in s} log P(w|c_{s}) = \\sum_{w \\in s} f_{w}(\\tilde{c_{s}}) 단어 등장 확률의 Taylor Series approximation f_{w}(\\tilde{c_{s}}) \\approx f_{w}(0) + \\triangledown f_{w}(0)^{T} \\tilde{c_{s}} = constant + frac{ (1-\\alpha) / \\alpha Z }{ P(w) + (1-\\alpha) / \\alpha Z } \\tilde{c_{s}} \\cdot v_{w} 우리가 관찰하고 있는 단어 w가 등장할 확률을 최대화하는 주제벡터 $ c_{s} / \\tilde{c_{s}} $를 찾는 것이 목표이다. w가 등장할 확률을 최대화하는 $ c_{s} / \\tilde{c_{s}} $를 찾게 된다면 이 $ c_{s} / \\tilde{c_{s}} $ 는 해당 단어의 사용을 제일 잘 설명하는 주제 벡터가 될 것이다. 직관적으로 말하자면, 우리가 관찰하고 있는 문장이 등장할 확률을 최대화하는 주제 벡터 $ c_{s} / \\tilde{c_{s}} $는 문장에 속한 단어들에 해당하는 단어 벡터에 가중치를 곱해 만든 새로운 벡터들의 합에 비례한다. 희귀한 단어라면 높은 가중치를 곱해 해당 단어 벡터의 크기를 키우고, 고빈도 단어라면 해당 벡터의 크기를 줄니다. 이는 정보성이 높은, 희귀한 단어에 가중치를 높게 주는 TF-IDF의 철학과도 맞닿아 있는 부분이다. 또한 문장 내 단어의 등장 순서를 고려하지 않는다는 점에서 Bag of Words 가정과도 연결된다. 모델 구현 문장을 Token으로 나눈 뒤 해당 Token들에 대응하는 벡터들의 합으로 문장의 임베딩을 구한다. 예측은 테스트 문장이 들어오면 Token 벡터의 합으로 만들고, 이 벡터와 코사인 유사도가 가장 높은 학습 데이터 문장의 임베딩을 찾는다. 이후 해당 학습 데이터 문장에 달려 있는 레이블을 리턴하는 방식이다. 예를들어, ‘영화 정말 재밌다.’가 테스트 문장이고, 이 문장과 유사한 학습 데이터 임베딩이 ‘영화가 진짜 재미지네요.+긍정’이라면, 테스트 문장을 긍정이라고 예측한다는 것이다. 또한, 과연 어느정도의 효과가 있는지 비교하기위해 대조군으로 일반적인 합을 통한 임베딩 방식도 수행해볼것이다. Weighted Sum을 이용한 Documents Classification Model 참고로 해당 모델을 수행하려면 먼저 형태소 분석이 완료된 Corpus file과 Corpus를 통해 만들어진 Embedding File이 존재해야한다. 먼저 tokenizer를 선택해서 사용할 수 있도록 각 Tokenizer에 따른 객체를 생성해주는 함수를 만들어준다. 1234567891011121314151617181920from khaiii import KhaiiiApifrom konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkmadef get_tokenizer(tokenizer_name): if tokenizer_name == \"komoran\": tokenizer = Komoran() elif tokenizer_name == \"okt\": tokenizer = Okt() elif tokenizer_name == \"mecab\": tokenizer = Mecab() elif tokenizer_name == \"hannanum\": tokenizer = Hannanum() elif tokenizer_name == \"kkma\": tokenizer = Kkma() elif tokenizer_name == \"khaiii\": tokenizer = KhaiiiApi() else: tokenizer = Mecab() return tokenizer 모델을 저장할 path가 존재하지 않는다면 directory를 만들어주는 함수를 만들어준다. 12345678import osdef make_save_path(full_path): if full_path[:4] == \"data\": full_path = os.path.join(os.path.abspath(\".\"), full_path) model_path = '/'.join(full_path.split(\"/\")[:-1]) if not os.path.exists(model_path): os.makedirs(model_path) 아래 embedding_method의 default값은 fasttext이지만 실제로 필자가 실행시에는 word2vec을 사용할 것이다. defaultdict은 말 그대로 처음에 값을 지정해주지 않으면 default값을 넣어준다는 의미이다. 비교 할 사항 embedding method : fasttext vs word2vec sum method : weighted sum vs sum average or nor : average vs not average 이 글에서는 2번째 항목의 비교한 결과만을 보여 줄 것이다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205from collections import defaultdictfrom gensim.models import Word2Vecclass CBoWModel(object): def __init__(self, train_fname, embedding_fname, model_fname, embedding_corpus_fname, embedding_method=\"fasttext\", is_weighted=None, average=False, dim=100, tokenizer_name=\"mecab\"): # configurations make_save_path(model_fname) self.dim = dim # 평균을 내줄것인지 아니면 합만을 사용할 것인지에 대한 옵션이다. self.average = average if is_weighted: model_full_fname = model_fname + \"-weighted\" else: model_full_fname = model_fname + \"-original\" self.tokenizer = get_tokenizer(tokenizer_name) if is_weighted: # ready for weighted embeddings # dictionary 형태로 이루어져있다. (embedding[\"word\"]=embedding_vaector) self.embeddings = self.load_or_construct_weighted_embedding(embedding_fname, embedding_method, embedding_corpus_fname) print(\"loading weighted embeddings, complete!\") else: # ready for original embeddings words, vectors = self.load_word_embeddings(embedding_fname, embedding_method) self.embeddings = defaultdict(list) for word, vector in zip(words, vectors): self.embeddings[word] = vector print(\"loading original embeddings, complete!\") # 모델이 존재하지 않는다면 새롭게 훈련시키고 존재한다면 load해 온다. if not os.path.exists(model_full_fname): print(\"train Continuous Bag of Words model\") self.model = self.train_model(train_fname, model_full_fname) else: print(\"load Continuous Bag of Words model\") self.model = self.load_model(model_full_fname) def evaluate(self, test_data_fname, batch_size=3000, verbose=False): print(\"evaluation start!\") test_data = self.load_or_tokenize_corpus(test_data_fname) data_size = len(test_data) num_batches = int((data_size - 1) / batch_size) + 1 eval_score = 0 for batch_num in range(num_batches): batch_sentences = [] batch_tokenized_sentences = [] batch_labels = [] start_index = batch_num * batch_size end_index = min((batch_num + 1) * batch_size, data_size) features = test_data[start_index:end_index] for feature in features: sentence, tokens, label = feature batch_sentences.append(sentence) batch_tokenized_sentences.append(tokens) batch_labels.append(label) preds, curr_eval_score = self.predict_by_batch(batch_tokenized_sentences, batch_labels) eval_score += curr_eval_score if verbose: for sentence, pred, label in zip(batch_sentences, preds, batch_labels): print(sentence, \", pred:\", pred, \", label:\", label) print(\"number of correct:\", str(eval_score), \", total:\", str(len(test_data)), \", score:\", str(eval_score / len(test_data))) def predict(self, sentence): # 문장을 예측을 하기 위해서는 우선 형태소를 분석을 해야한다. tokens = self.tokenizer.morphs(sentence) # 문장의 형태소들을 임베딩 벡터와 같은 크기의 영벡터를 만든후 계속해서 더해주는 방식으로 문장 임베딩 벡터를 생성한다. # 만약 average=True했다면, sentence_vector = self.get_sentence_vector(tokens) # 모델의 문장 임베딩 벡터와 sentence 문장 벡터와의 내적으로 유사도를 측정한다. scores = np.dot(self.model[\"vectors\"], sentence_vector) # 제일높은 유사도를 지닌 라벨을 출력해준다. pred = self.model[\"labels\"][np.argmax(scores)] return pred def predict_by_batch(self, tokenized_sentences, labels): sentence_vectors, eval_score = [], 0 for tokens in tokenized_sentences: sentence_vectors.append(self.get_sentence_vector(tokens)) scores = np.dot(self.model[\"vectors\"], np.array(sentence_vectors).T) preds = np.argmax(scores, axis=0) for pred, label in zip(preds, labels): if self.model[\"labels\"][pred] == label: eval_score += 1 return preds, eval_score def get_sentence_vector(self, tokens): vector = np.zeros(self.dim) for token in tokens: if token in self.embeddings.keys(): vector += self.embeddings[token] if self.average: vector /= len(tokens) vector_norm = np.linalg.norm(vector) if vector_norm != 0: unit_vector = vector / vector_norm else: unit_vector = np.zeros(self.dim) return unit_vector def load_or_tokenize_corpus(self, fname): data = [] if os.path.exists(fname + \"-tokenized\"): with open(fname + \"-tokenized\", \"r\") as f1: for line in f1: sentence, tokens, label = line.strip().split(\"\\u241E\") data.append([sentence, tokens.split(), label]) else: with open(fname, \"r\") as f2, open(fname + \"-tokenized\", \"w\") as f3: for line in f2: sentence, label = line.strip().split(\"\\u241E\") tokens = self.tokenizer.morphs(sentence) data.append([sentence, tokens, label]) f3.writelines(sentence + \"\\u241E\" + ' '.join(tokens) + \"\\u241E\" + label + \"\\n\") return data def compute_word_frequency(self, embedding_corpus_fname): total_count = 0 # &#123;단어 : 해당 단어 개수&#125;로 표현해주기 위해 다음과 같이 defaultdict을 사용했다. # defaultdict 을 사용한 이유는 값을 따로 지정해 주지 않는다면 default 값을 사용하기 위해서이다. words_count = defaultdict(int) with open(embedding_corpus_fname, \"r\") as f: for line in f: tokens = line.strip().split() for token in tokens: words_count[token] += 1 total_count += 1 return words_count, total_count def load_word_embeddings(self, vecs_fname, method): if method == \"word2vec\": model = Word2Vec.load(vecs_fname) words = model.wv.index2word vecs = model.wv.vectors else: words, vecs = [], [] with open(vecs_fname, 'r', encoding='utf-8') as f1: if \"fasttext\" in method: next(f1) # skip head line for line in f1: if method == \"swivel\": splited_line = line.replace(\"\\n\", \"\").strip().split(\"\\t\") else: splited_line = line.replace(\"\\n\", \"\").strip().split(\" \") words.append(splited_line[0]) vec = [float(el) for el in splited_line[1:]] vecs.append(vec) return words, vecs def load_or_construct_weighted_embedding(self, embedding_fname, embedding_method, embedding_corpus_fname, a=0.0001): dictionary = &#123;&#125; # 이미 만들어진 가중합 embedding이 존재할 경우 if os.path.exists(embedding_fname + \"-weighted\"): with open(embedding_fname + \"-weighted\", \"r\") as f2: for line in f2: # \\u241E : Symbol for Record Seperator word, weighted_vector = line.strip().split(\"\\u241E\") weighted_vector = [float(el) for el in weighted_vector.split()] dictionary[word] = weighted_vector else: # 위에서 embedding-weighted 파일이 존재하지 않는다면 훈련을 해야하므로 # 우선 이미 embedding된 파일의 단어와 해당단어의 임베딩 벡터를 불러온다. # load pretrained word embeddings # 해당 임베딩 파일에 있는 단어와 그에 해당하는 임베딩벡터를 순서대로 불러온다. words, vecs = self.load_word_embeddings(embedding_fname, embedding_method) # compute word frequency words_count, total_word_count = self.compute_word_frequency(embedding_corpus_fname) # construct weighted word embeddings # embedding_fname - weighted로 가중합을 계산한 임베딩벡터 파일을 생성한다. with open(embedding_fname + \"-weighted\", \"w\") as f3: for word, vec in zip(words, vecs): if word in words_count.keys(): word_prob = words_count[word] / total_word_count else: word_prob = 0.0 weighted_vector = (a / (word_prob + a)) * np.asarray(vec) dictionary[word] = weighted_vector f3.writelines(word + \"\\u241E\" + \" \".join([str(el) for el in weighted_vector]) + \"\\n\") return dictionary def train_model(self, train_data_fname, model_fname): model = &#123;\"vectors\": [], \"labels\": [], \"sentences\": []&#125; # [sentence, tokens, label]형태로 출력 train_data = self.load_or_tokenize_corpus(train_data_fname) with open(model_fname, \"w\") as f: for sentence, tokens, label in train_data: tokens = self.tokenizer.morphs(sentence) sentence_vector = self.get_sentence_vector(tokens) model[\"sentences\"].append(sentence) model[\"vectors\"].append(sentence_vector) model[\"labels\"].append(label) str_vector = \" \".join([str(el) for el in sentence_vector]) f.writelines(sentence + \"\\u241E\" + \" \".join(tokens) + \"\\u241E\" + str_vector + \"\\u241E\" + label + \"\\n\") return model def load_model(self, model_fname): model = &#123;\"vectors\": [], \"labels\": [], \"sentences\": []&#125; with open(model_fname, \"r\") as f: for line in f: sentence, _, vector, label = line.strip().split(\"\\u241E\") vector = np.array([float(el) for el in vector.split()]) model[\"sentences\"].append(sentence) model[\"vectors\"].append(vector) model[\"labels\"].append(label) return model 모델의 파라미터 값 설정123456train_fname = \"./data/processed/processed_ratings_train.txt\"embedding_fname = \"./data/word-embeddings/word2vec/word2vec\"model_fname = \"./data/word-embeddings/cbow/word2vec\"embedding_corpus_fname = \"./data/tokenized/ratings_mecab.txt\"embedding_method = \"word2vec\"test_data_fname = \"./data/processed/processed_ratings_test.txt\" 모델 학습 및 평가해당 문장에 대한 단어벡터들의 합만을 가지고 예측1234original_Model=CBoWModel(train_fname=train_fname, embedding_fname=embedding_fname, model_fname=model_fname, embedding_corpus_fname=None, embedding_method=embedding_method, is_weighted=False, average=False, dim=100, tokenizer_name=\"mecab\")original_Model.evaluate(test_data_fname) 결과12345loading original embeddings, complete!train Continuous Bag of Words modelevaluation start!number of correct: 36498 , total: 49997 , score: 0.7300038002280137 해당 문장에 대한 단어벡터들의 가중합을 가중합을 가지고 예측12345weighted_Model=CBoWModel(train_fname=train_fname, embedding_fname=embedding_fname, model_fname=model_fname, embedding_corpus_fname=embedding_corpus_fname,embedding_method=embedding_method, is_weighted=True, average=False, dim=100, tokenizer_name=\"mecab\")weighted_Model.evaluate(test_data_fname) 결과12345loading weighted embeddings, complete!train Continuous Bag of Words modelevaluation start!number of correct: 34208 , total: 49997 , score: 0.6842010520631238","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03","slug":"NLP_05","date":"2020-02-01T07:57:48.000Z","updated":"2020-02-03T14:30:51.344Z","comments":true,"path":"2020/02/01/NLP_05/","link":"","permalink":"https://heung-bae-lee.github.io/2020/02/01/NLP_05/","excerpt":"","text":"순환신경망 분류 모델 앞선 모델들과 달리 이미 주어진 단어 특징 벡터를 활용해 모델을 학습하지 않고 텍스트 정보를 입력해서 문장에 대한 특징 정보를 추출한다. RNN은 현재 정보는 이전 정보가 점층적으로 쌓이면서 정보를 표현할 수 있는 모델이다. 따라서 시간에 의존적인 또는 순차적인 데이터에 대한 문제에 활용된다. 이 모델은 한단에 대한 정보를 입력하면 이 단어 다음에 나올 단어를 맞추는 모델이라 순차적인 데이터에 대한 모델링이 가능한 것이다. 1234567891011121314DATA_IN_PATH = '/content/'DATA_OUT_PATH = '/content/'INPUT_TRAIN_DATA_FILE_NAME = 'train_input.npy'LABEL_TRAIN_DATA_FILE_NAME = 'train_label.npy'DATA_CONFIGS_FILE_NAME = 'data_configs.json'train_input = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'rb'))train_label = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'rb'))prepro_configs = Nonewith open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, 'r') as f: prepro_configs = json.load(f) 학습과 검증 데이터셋 분리12345from sklearn.model_selection import train_test_splitTEST_SPLIT=0.1RANDOM_SEED=13371447input_train, input_eval, label_train, label_eval = train_test_split(train_input, train_label, test_size=TEST_SPLIT, random_state=RANDOM_SEED) 데이터 입력 함수1234567891011121314151617181920212223242526import tensorflow as tfBATCH_SIZE = 16NUM_EPOCHS = 20def mapping_fn(X, Y): inputs, labels = &#123;'x' : X&#125;, Y return inputs, labelsdef train_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train)) dataset = dataset.shuffle(buffer_size=50000) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.repeat(count=NUM_EPOCHS) dataset = dataset.map(mapping_fn) iterator = dataset.make_one_shot_iterator() return iterator.get_next()def eval_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((input_eval, label_eval)) dataset = dataset.map(mapping_fn) dataset = dataset.batch(BATCH_SIZE) iterator = dataset.make_one_shot_iterator() return iterator.get_next() 모델 함수모델 하이퍼파라미터 정의123456789VOCAB_SIZE = prepro_configs['vocab_size']WORD_EMBEDDING_DIM = 100HIDDEN_STATE_DIM = 150DENSE_FEATURE_DIM = 150learning_rate = 0.001 모델 구현 먼저 모델에서 배치 데이터를 받게 된다면 단어 인덱스로 구성된 Sequence 형태로 입력이 들어온다. 데이터 입력 함수에서 정의했듯이 모델 함수의 입력 인자인 features는 Python dictionary 형태로 구성돼 있다. 모델에 들어온 입력 데이터는 보통 Embedding Layer를 거친다. 구현하고자 하는 모델에서는 tf.keras.Embedding함수가 이 같은 역할을 수행한다. Embedding Layer를 거쳐 나온 데이터는 순환 신경망 층을 거쳐 문자의 벡터를 출력한다. 여기서는 간단한 심층 순환 신경망 모델로 LSTM 모델을 통해 구현한다. 순환 신경망을 구현하기 위해서는 RNNCell이란 객체를 활용함ㄴ다. RNNCell은 순환 신경망 객체라 보면된다. LSTM으로 순환 신경망을 구현하기 위해 tf.nn.rnn_cell.LSTMCell객체를 생성하며, 이 객체는 하나의 LSTM Cell을 의미한다. 따라서 해당 Cell 객체를 여러개 생성해서 하나의 리스트로 만들어 준다. LSTMCell을 생성할 때는 은닉 상태 벡터(Hidden state vector)에 대한 차원만 정의하면 된다. 여러 LSTMCell을 쌀게 되면 이를 하나의 MultiRNN으로 묶어야, 즉 wrapping해야한다. tf.nn.rnn_cell.MultiRNNCell을 생성함으로써 Stack 구조의 LSTM 신경망을 구현할 수 있다. 단순히 RNNCell 만으로 구성해 모델 연산 그래프를 만들 수 있다. RNNCell 객체는 Sequence 한 스텝에 대한 연산만 가능하다. 따라서 여러 스텝에 대한 연산을 하기 위해서는 for 문을 활용해 연산을 할 수 있게 구현해야한다. 하지만 이보다 더 간단하게 구현할 수 있는 방법은 tf.nn.dynamic_rnn 함수를 사용하는 것이다. 이 함수는 for 문 없이 자동으로 순환 신경망을 만들어 주는 역할을 한다. dynamic_rnn 함수에 필요한 입력 인자는 2개다. 첫 번째 순환 신경망 객체인 MultiRNNCell 객체이고, 나머지 하나는 입력값을 넣어주면된다. Dense에 적용시키는 입력값은 LSTM 신경망의 마지막 출력값을 넣어준다. 출력값에 [:, -1, :]로 마지막 값만 뽑아낸 후 Dense에 적용시킨다. 마지막으로 감정이 긍정인지 부정인지 판단할 수 있도록 출력값을 하나로 만들어야 한다. 보통 선형변환을 통해 입력 벡터에 대한 차원수를 바꾼다. 모델 학습, 검정 및 테스트를 위한 구현 앞서 모델에서 구현한 값과 정답 label을 가지고 loss 값을 구해 Adam optimizer를 활용해 모델 parameter를 최적화 해 볼 것이다. 모델 예측 loss값은 모델에서 구한 logits 변수의 경우 아직 Logistic 함수를 통해 0~1 사이의 값으로 스케일을 맞춰두지 않았다. 물론 앞서 dense 층에서 activation 인자를 tf.nn.sigmoid로 설정해둘 수 있다. 하지만 여기서는 tf.losses.sigmoid_cross_entropy 함수를 활용해 손실값을 구할 수 있기 때문에 dense 층에서 설정하지 않았다. 예측 loss값을 구하고 나면 이제 parameter optimization을 하고자 SGD를 진행한다. 여기서는 tf.train.AdamOptimizer클래스를 활용할 것이다. tf.train.AdamOptimizer.minimize 함수를 선언 할 때 전체 학습에 대한 global step값을 넣어야 한다. tf.train.get_global_step을 선언하면 현재 학습 global step을 얻을 수 있다. 보통 직접 모델 함수를 구현하게 되면 tf.estimator.EstimatorSpec 객체를 생성해서 반환하게 한다. 이 객체는 현재 함수가 어느 모드에서 실행되고 있는지 확인한다. 그리고 각 모드에 따라 필요한 입력 인자가 다르다. 12345678910111213141516171819202122232425262728293031323334353637383940414243def model_fn(features, labels, mode): TRAIN = mode == tf.estimator.ModeKeys.TRAIN EVAL = mode == tf.estimator.ModeKeys.EVAL PREDICT = mode == tf.estimator.ModeKeys.PREDICT embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM)(features['x']) embedding_layer = tf.keras.layers.Dropout(0.2)(embedding_layer) rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) for size in [HIDDEN_STATE_DIM, HIDDEN_STATE_DIM]] multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers) outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell, inputs=embedding_layer, dtype=tf.float32) outputs = tf.keras.layers.Dropout(0.2)(outputs) hidden_layer = tf.keras.layers.Dense(DENSE_FEATURE_DIM, activation=tf.nn.tanh)(outputs[:, -1, :]) hidden_layer = tf.keras.layers.Dropout(0.2)(hidden_layer) logits = tf.keras.layers.Dense(1)(hidden_layer) logits = tf.squeeze(logits, axis=-1) sigmoid_logits = tf.nn.sigmoid(logits) if PREDICT: predictions = &#123;'sentiment': sigmoid_logits&#125; return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions) loss = tf.losses.sigmoid_cross_entropy(labels, logits) if EVAL: accuracy = tf.metrics.accuracy(labels, tf.round(sigmoid_logits)) eval_metric_ops = &#123;'acc':accuracy&#125; return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops) if TRAIN: global_step = tf.train.get_global_step() train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step) return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss) TF Estimator 활용한 모델 학습 및 성능 검증12345678910DATA_OUT_PATH = '/content/'if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_OUT_PATH)est = tf.estimator.Estimator(model_fn, model_dir=DATA_OUT_PATH + 'checkpoint')os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"est.train(train_input_fn) validation data에 대한 성능이 약 85%정도였다. 오히려 앞의 머신러닝 기법들 중 어떤 기법보다는 성능이 떨어진다는 것을 볼 수 있었지만, test data에 대한 성능을 한번 체크해 보아야 할 것 같다. 1234est.evaluate(eval_input_fn)# 결과# &#123;'acc': 0.8472, 'global_step': 18291, 'loss': 0.6007853&#125; 데이터 제출1234DATA_OUT_PATH = '/content/'TEST_INPUT_DATA = 'test_input.npy'test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb')) estimator를 통해 예측하기 위해서는 데이터 입력 함수를 정의해야 했다. 이 경우는 tf.estimator.inputs.numpy_input_fn 함수를 활용해 데이터 입력 함수를 생성한다. 123predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;\"x\": test_input_data&#125;, shuffle=False)predictions = np.array([p['sentiment'] for p in est.predict(input_fn=predict_input_fn)]) 1234TEST_ID_DATA = 'test_id.npy'test_id = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb'), allow_pickle='True')output = pd.DataFrame(&#123;'id': test_id, 'sentiment': list(predictions)&#125;)output.to_csv(DATA_OUT_PATH + \"rnn_predic.csv\", index=False, quoting=3) 1!kaggle competitions submit word2vec-nlp-tutorial -f \"rnn_predic.csv\" -m \"LSTM Model with Epoch 10\" CNN을 이용한 문장 분류 CNN은 보통 image에서 많이 사용된다고 생각들지만, 텍스트에서도 좋은 효과를 낼 수 있다는 점을 Yoon Kimm(2014) 박사가 쓴 “Convolutional Neural Network for Sentence Classification”을 통해 입증되었다. RNN이 단어의 입력 순서를 중요하게 반영한다면 CNN은 문장의 지역정보를 보존하면서 각 문장 성분의 등장 정보를 학습에 반영하는 구조로 풀어가고 있다. 학습할 때 각 필터 크기를 조절하면서 언어의 특징 값을 추출하게 되는데, 기존의 n-gram(2그램, 3그램) 방식과 유사하다고 볼 수 있다. 모델 구현123456789# 기본적인 라이브러리들을 불러온다import sysimport osimport numpy as npimport jsonfrom sklearn.model_selection import train_test_splitimport tensorflow as tffrom tensorflow import keras 1234567891011121314151617# 이전에 저장했던 학습에 필요한 디렉터리 설정 및 학습/평가 데이터를 불러온다.DATA_IN_PATH = '/content/'DATA_OUT_PATH = '/content/'INPUT_TRAIN_DATA_FILE_NAME = 'train_input.npy'LABEL_TRAIN_DATA_FILE_NAME = 'train_label.npy'INPUT_TEST_DATA_FILE_NAME = 'test_input.npy'DATA_CONFIGS_FILE_NAME = 'data_configs.json'train_input_data = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, 'rb'))train_label_data = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, 'rb'))test_input_data = np.load(open(DATA_IN_PATH + INPUT_TEST_DATA_FILE_NAME, 'rb'))with open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, 'r') as f: prepro_configs = json.load(f) print(prepro_configs.keys()) 학습과 검증 데이터셋 분리12345678910# 파라미터 변수RNG_SEED = 1234BATCH_SIZE = 16NUM_EPOCHS = 10VOCAB_SIZE = prepro_configs['vocab_size']EMB_SIZE = 128VALID_SPLIT = 0.2# 학습 데이터와 검증 데이터를 train_test_split 함수를 활용해 나눈다.train_input, eval_input, train_label, eval_label = train_test_split(train_input_data, train_label_data, test_size=VALID_SPLIT, random_state=RNG_SEED) 데이터 입력 함수1234567891011121314151617181920212223242526# 전처리 학습을 위해 tf.data를 설정한다.def mapping_fn(X, Y=None): input, label = &#123;'x': X&#125;, Y return input, labeldef train_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label)) dataset = dataset.shuffle(buffer_size=len(train_input)) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.map(mapping_fn) dataset = dataset.repeat(count=NUM_EPOCHS) iterator = dataset.make_one_shot_iterator() return iterator.get_next()def eval_input_fn(): dataset = tf.data.Dataset.from_tensor_slices((eval_input, eval_label)) dataset = dataset.shuffle(buffer_size=len(eval_input)) dataset = dataset.batch(BATCH_SIZE) dataset = dataset.map(mapping_fn) iterator = dataset.make_one_shot_iterator() return iterator.get_next() 모델 구현 합성곱 연산의 경우 케라스 모듈 중 Conv1D를 활용해 진행한다. 총 3개의 합성곱 층을 사용하는데, 각각 필터의 크기를 다르게 해서 적용한다. 즉, kernel_size를 3,4,5로 설정할 것이다. 그리고 이렇게 각각 다른 필터의 크기로 적용한 합성곱 층 출력값을 하나로 합칠 것이다. 그리고 추가로 각 합성곱 신경망 이후에 max pooling 층을 적용한다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def model_fn(features, labels, mode): TRAIN = mode == tf.estimator.ModeKeys.TRAIN EVAL = mode == tf.estimator.ModeKeys.EVAL PREDICT = mode == tf.estimator.ModeKeys.PREDICT # embedding layer를 선언 embedding_layer = keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features['x']) # embedding layer에 대한 output에 대해 dropout을 취한다. dropout_emb = keras.layers.Dropout(0.5)(embedding_layer) ## filters = 128이고 kernel_size = 3,4,5이다. ## 길이기ㅏ 3, 4, 5인 128개의 다른 필터를 생성한다. 3, 4, 5 gram의 효과처럼 다양한 각도에서 문장을 보는 효과가 있다. ## conv1d는 (배치 크기, 길이, 채널)로 입력값을 받는데, 배치 사이즈 : 문장 숫자 | 길이 : 각 문장의 단어의 개수 | 채널 : 임베딩 출력 차원수 conv1 = keras.layers.Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb) pool1 = keras.layers.GlobalMaxPool1D()(conv1) conv2 = keras.layers.Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb) pool2 = keras.layers.GlobalMaxPool1D()(conv2) conv3 = keras.layers.Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb) pool3 = keras.layers.GlobalMaxPool1D()(conv3) # 3,4,5 gram이후 모아주기 concat = keras.layers.concatenate([pool1, pool2, pool3]) hidden = keras.layers.Dense(250, activation=tf.nn.relu)(concat) dropout_hidden = keras.layers.Dropout(0.5)(hidden) logits = keras.layers.Dense(1, name='logits')(dropout_hidden) logits = tf.squeeze(logits, axis=-1) # 최종적으로 학습, 검증, 평가의 단계로 나누어 활용 if PREDICT: return tf.estimator.EstimatorSpec(mode=mode, predictions=&#123;'prob': tf.nn.sigmoid(logits)&#125;) loss = tf.losses.sigmoid_cross_entropy(labels, logits) if EVAL: pred = tf.nn.sigmoid(logits) accuracy = tf.metrics.accuracy(labels, tf.round(pred)) return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=&#123;'acc':accuracy&#125;) if TRAIN: global_step = tf.train.get_global_step() train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step) return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss) 모델 학습123456789model_dir = os.path.join(os.getcwd(), \"data_out/checkpoint/cnn\")os.makedirs(model_dir, exist_ok=True)config_tf = tf.estimator.RunConfig(save_checkpoints_steps=200, keep_checkpoint_max=2, log_step_count_steps=400)# Estimator 객체 생성cnn_est = tf.estimator.Estimator(model_fn, model_dir=model_dir)cnn_est.train(train_input_fn) 검증 데이터 평가 검증 데이터에 대한 정확도가 약 88%정도로 측정되었다. 지금껏 간단한 모델들 중 제일 높은 성능을 보이고 있어 필자는 약간 기대하고 있었다. 이에따른 test data의 성능을 알아보기 위해 캐글에 test data의 예측값을 제출해 볼 것이다. 1234cnn_est.evaluate(eval_input_fn)# 결과# &#123;'acc': 0.8774, 'global_step': 94200, 'loss': 1.3248637&#125; 1234567DATA_IN_PATH = '/content/'DATA_OUT_PATH = '/content/'TEST_INPUT_DATA = 'test_input.npy'TEST_ID_DATA = 'test_id.npy'test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, 'rb'))ids = np.load(open(DATA_IN_PATH + TEST_ID_DATA, 'rb'), allow_pickle=True) 1234567predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;'x':test_input_data&#125;, shuffle=False)predictions = np.array([p['prob'] for p in cnn_est.predict(input_fn=predict_input_fn)])output = pd.DataFrame(&#123;\"id\": list(ids), \"sentiment\":list(predictions)&#125;)output.to_csv(DATA_OUT_PATH + \"Bag_of_Words_model_test.csv\", index=False, quoting=3) 1!kaggle competitions submit word2vec-nlp-tutorial -f \"Bag_of_Words_model_test.csv\" -m \"CNN 1d Model with EPOCHS 10\"","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP 실습 텍스트 분류(TF-IDF, CountVectorizer, Word2Vec) -02","slug":"NLP_04","date":"2020-01-29T15:13:48.000Z","updated":"2020-02-04T13:24:23.984Z","comments":true,"path":"2020/01/30/NLP_04/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/30/NLP_04/","excerpt":"","text":"모델링 소개 선형모델 로지스틱회귀 모델 입력 벡터를 word2vec과 tf-idf를 사용해본다. 랜던포레스트 TF-IDF를 활용한 모델 구현 모델의 입력값으로 TF-IDF 값을 갖는 벡터를 사용할 것이기 때문에 scikit-learn의 TfidfVectorizer를 사용할 것이다. 이를 위해서는 입력값이 텍스트로 이뤄진 데이터 형태이어야 한다. 123train_data = pd.read_csv('train_clean.csv')reviews = list(train_data['clean_review'])sentiments = list(train_data['sentiment']) TF-IDF Vectorizing 데이터에 대해 TF-IDF 값으로 벡터화를 진행한다. min_df : 설정한 값보다 특정 Token의 df 값이 더 적게 나오면 벡터화 과정에서 제거 anlayzer : 분석 단위를 의미, ‘word’의 경우 간어 하나를 단위로, ‘char’는 문자 하나를 단위로 sublinear_tf : 문서의 단어 빈도수(tf:term frequency)에 대한 smoothing 여부를 설정 ngram_range : 빈도의 기본 단위를 어떤 범위의 n-gram으로 설정할 것인지를 보는 인자 max_features : 각 벡터의 최대 길이(특징의 길이)를 설정 123456from sklearn.feature_extraction.text import TfidfVectorizervectorizer = TfidfVectorizer(min_df=0.0, analyzer='char', sublinear_tf=True, ngram_range=(1,3), max_features=5000)X = vectorizer.fit_transform(reviews)X 1train_data.shape 학습과 검증 데이터셋 분리123456789from sklearn.model_selection import train_test_splitimport numpy as npRANDOM_SEED = 42TEST_SPLIT = 0.2y = np.array(sentiments)X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT) class_wight=’balanced’로 설정해서 각 label에 대해 균형 있게 학습할 수 있게 한 것이다. 1234from sklearn.linear_model import LogisticRegressionlgs = LogisticRegression(class_weight = 'balanced')lgs.fit(X_train, y_train) 1print(\"Accuracy: &#123;&#125;\".format(lgs.score(X_eval, y_eval))) 필자는 Accuracy: 0.8676을 출력으로 받았다. validation data에 대한 성능이 약 87%의 정확도를 갖으므로 test data에 대해서도 비슷한 수준일 것이라고 기대하며 kaggle에 test data의 예측값을 제출해 볼 것이다. 데이터 제출하기 만든 모델을 활용해 평가 데이터 결과를 예측하고 캐글에 제출할 수 있도록 파일로 저장할 것이다. 123456test_data = pd.read_csv('test_clean.csv')testDataVecs = vectorizer.transform(test_data[\"review\"])test_predicted = lgs.predict(testDataVecs)print(test_predicted) 123456if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_OUT_PATH)ids = list(test_data['id'])answer_dataset = pd.DataFrame(&#123;'id' : ids, \"sentiment\" : test_predicted&#125;)answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_tfidf_answer.csv', index=False, quoting=3) 1!kaggle competitions submit word2vec-nlp-tutorial -f \"lgs_tfidf_answer.csv\" -m \"LogisticRegression Model with tf-idf\" Woed2vec(CBOW)을 활용한 모델 구현 이번에는 word2vec을 활용해 모델을 구현할 것이다. 우선 각 단어에 대해 word2vec으로 벡터화해야 한다. word2vec의 경우 단어로 표현된 리스트를 입력값으로 넣어야 하기 때문에 전처리한 넘파이 배열을 바로 사용하지 않는다. 따라서 전처리된 텍스트 데이터를 불러온 후 각 단어들의 리스트로 나눠야 한다. 123456789101112DATA_IN_PATH = \"/content/\"TRAIN_CLEAN_DATA = 'train_clean.csv'train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)reviews = list(train_data['review'])sentiments = list(train_data['sentiment'])sentences = []for review in reviews: sentences.append(review.split()) word2ve 벡터화 num_features : 각 단어에 대해 임베딩된 벡터의 차원을 정한다. min_word_count : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습 하지 않기 위해 최소 빈도수를 설정한다. num_workers : 모델 학습 시 학습을 위한 프로세스 개수를 지정한다. context : word2vec을 수행하기 위한 context 윈도우 크기를 지정한다. downsampling : word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 label에 대한 downsampling 비율을 지정한다. 보통 0.001이 좋은 성능을 낸다고 한다. 참고로 parameter 중에 sg의 default값인 0을 사용했으므로 이 모델은 Word2vec의 CBOW모델이다.12345num_features = 300min_word_count = 40num_workers = 4context = 10downsampling = 1e-3 1!pip install gensim word2vec을 학습하는 과정에서 진행 상황을 확인해 보기 위해 다음과 같이 logging을 통해 확인해 볼 수 있다. 로깅을 할 때 format을 위와 같이 지정하고, 로그 수준은 INFO에 맞추면 word2vec의 학습과정에서 로그 메시지를 양식에 맞게 INFO 수준으로 보여준다. 12import logginglogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) 1234from gensim.models import word2vecprint(\"Training model ....\")model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling) word2vec으로 학습시킨 모델의 경우 모델을 따로 저장해두면 이후에 다시 사용할 수 있기 때문에 저장해 두고 이후에 학습한 값이 추가로 필요할 경우 사용하면 된다. 12345# 모델의 하이퍼파라미터를 설정한 내용을 모델 이름에 담는다면 나중에 참고하기에 좋다.# 모델을 저장하면 Word2Vec.load()를 통해 모델을 다시 사용할 수 있다.model_name = \"300features_40minwords_10context\"model.save(model_name) word2vec 모델을 활용해서 선형 회귀 모델을 학습할 것이다. 우선 학습을 하기 위해서는 하나의 review를 같은 형태의 입력값으로 만들어야 한다. 지금은 word2vec 모델에서 각 단어가 벡터로 표현되어 있다. 그리고 review 마다 단어의 개수가 모두 다르기 때문에 입력값을 하나의 형태로 만들어야 한다. 아래 model을 통해 얻은 단어 하나의 feature는 (300,)의 shape를 갖게 될 것이다. 가장 단순한 방법은 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법이 있다. words : 단어의 모음인 하나의 review model : 학습한 word2vec 모델 num_features : word2vec으로 임베딩할 때 정했던 벡터의 차원 수 1234567891011121314151617def get_features(words, model, num_features): # 출력 벡터 초기화 feature_vector = np.zeros((num_features), dtype=np.float32) num_words = 0 # 어휘사전 준비 index2word_set = set(model.wv.index2word) for w in words: if w in index2word_set: num_words +=1 # 사전에 해당하는 단어에 대해 단어 벡터를 더함 feature_vector = np.add(feature_vector, model[w]) # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함 feature_vector = np.divide(feature_vector, num_words) return feature_vector 123456789def get_dataset(reviews, model, num_features): dataset = list() for s in reviews: dataset.append(get_features(s, model, num_features)) reviewFeatureVecs = np.stack(dataset) return reviewFeatureVecs 1train_data_vecs = get_dataset(sentences, model, num_features) 학습과 검증 데이터셋 분리12345678910from sklearn.model_selection import train_test_splitimport numpy as npX = train_data_vecsy = np.array(sentiments)RANDOM_SEED = 42TEST_SPLIT = 0.2X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED) 모델 선언 및 학습1234from sklearn.linear_model import LogisticRegressionlgs = LogisticRegression(class_weight='balanced')lgs.fit(X_train, y_train) 검증 데이터셋을 이용한 성능 평가 이전의 TF-IDF를 사용해서 학습한 것보단 상대적으로 성능이 떨어진다. word2vec이 단어 간의 유사도를 보는 관점에서는 분명히 효과적일 수는 있지만 word2vec을 사용하는 것이 항상 가장 좋은 성능을 보장하지는 않는다는 것을 다시 한번 알 수 있다!!! 1print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval)) validation data에 대한 정확도는 83%정도로 TF-IDF로 했던 것보단 조금 떨어지지만 캐글에 제출해보고 overfitting이 발생했는지 점검해 본다. 12345TEST_CLEAN_DATA = 'test_clean.csv'test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)test_review = list(test_data['review']) 123test_sentences = []for review in test_review: test_sentences.append(review.split()) 1test_data_vecs = get_dataset(test_sentences, model, num_features) 123456789DATA_OUT_PATH = '/content/'test_predicted = lgs.predict(test_data_vecs)if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_OUT_PATH)test_data['id']=test_data['id'].apply(lambda x : x[1:-1])ids = list(test_data['id']) 12answer_dataset = pd.DataFrame(&#123;'id': ids, 'sentiment': test_predicted&#125;)answer_dataset.to_csv(DATA_OUT_PATH + 'lgs_answer.csv', index=False) 1!kaggle competitions submit word2vec-nlp-tutorial -f \"lgs_answer.csv\" -m \"LogisticRegression Model with Word2vec\" 랜덤포레스트 분류 모델CountVectorizer를 활용한 벡터화 CountVectorizer는 TF-IDF vectorizing과 동일하게 문장을 input으로 받기 때문에 Word2vec처럼 공백단위로 쪼개 단어로 사용하지 않을 것이다. 12345678import pandas as pdDATA_IN_PATH = '/content/'TRAIN_CLEAN_DATA = 'train_clean.csv'train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)reviews = list(train_data['clean_review'])y = np.array(train_data['sentiment']) 12345from sklearn.feature_extraction.text import CountVectorizervectorizer = CountVectorizer(analyzer = 'word', max_features = 5000)train_data_features = vectorizer.fit_transform(reviews) 학습과 검증 데이터 분리1234TEST_SIZE = 0.2RANDOM_SEED = 42train_input, eval_input, train_label, eval_label = train_test_split(train_data_features, y, test_size=TEST_SIZE, random_state=RANDOM_SEED) 모델 구현 및 학습1234567from sklearn.ensemble import RandomForestClassifier# 랜덤 포레스트 분류기에 100개의 의사결정 트리를 사용한다.forest = RandomForestClassifier(n_estimators=100)# 단어 묶음을 벡터화한 데이터와 정답 데이터를 가지고 학습을 시작한다.forest.fit(train_input, train_label) 검증 데이터셋으로 성능 평가 결과를 보면 대략 85%의 정확도를 보여준다. 앙상블 모델인데도 앞서 사용한 간단한 모델(TF_IDF보단 상대적으로)보다 좋지 않은 성능을 보여준다. 이는 모델의 문제일 수도 있고 데이터에서 특징을 추출하는 방법의 문제일 수도 있다. 즉, 모델을 바꾸지 않더라도 특징 추출 방법을 앞서 사용한 TF-IDF나 word2vec을 사용해서 입력값을 만든다면 성능이 높아질 수 있다. 1print(\"Accuracy: %f\" % forest.score(eval_input, eval_label)) 데이터 제출1234567TEST_CLEAN_DATA = 'test_clean.csv'DATA_OUT_PATH = '/content/'test_data = pd.read_csv(DATA_OUT_PATH + TEST_CLEAN_DATA)test_reviews = list(test_data['review'])ids = list(test_data['id']) 1test_data_features = vectorizer.transform(test_reviews) 12345678if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_OUT_PATH)result = forest.predict(test_data_features)output = pd.DataFrame(&#123;'id': ids, \"sentiment\": result&#125;)output.to_csv(DATA_OUT_PATH + 'Randomforest_model_with_Countvectorizer.csv', index=False, quoting=3) 1!kaggle competitions submit word2vec-nlp-tutorial -f \"Randomforest_model_with_Countvectorizer.csv\" -m \"Randomforest Model with Countvectorizer\"","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"NLP 실습 텍스트 분류 -01","slug":"NLP_03","date":"2020-01-29T14:40:09.000Z","updated":"2020-02-01T09:00:50.736Z","comments":true,"path":"2020/01/29/NLP_03/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/29/NLP_03/","excerpt":"","text":"영어 텍스트 분류 한국어는 띄어쓰기를 기준으로 모든 단어를 처리할 수 없으므로 상대적으로 전처리하기 쉬운 영어 텍스트를 가지고 먼저 감각을 키워보겠다. 데이터 이름 : Bag of Words Meets Bags of Popcorn 데이터 용도 : 텍스트 분류 학습을 목적으로 사용 데이터 권한 : MIT 데이터 출처 : https://www.kaggle.com/c/word2vec-nlp-tutorial/data 문제 소개영어 텍스트 분류 문제 중 캐글의 대회인 워드팝콘 문제를 활용할 것이다. 이 문제를 해결하면서 텍스트 분류 기술을 알아볼것이다. 워드 팝콘 워드 팝콘은 인터넷 영화 데이터베이스(IMDB)에서 나온 영화 평점 데이터를 활용한 캐글 문제다. 영화 평점 데이터이므로 각 데이터는 영화 리뷰 텍스트와 평점에 따른 감정 값(긍정 혹은 부정)으로 구성돼 있다. 이 데이터는 보통 감성 분석(sentiment analysis) 문제에서 자주 활용된다. 목표 1) 데이터를 불러오고 정제되지 않은 데이터를 활용하기 쉽게 전처리하는 과정 2) 데이터 분석 과정 데이터를 분석하여 어떻게 문제를 풀어가야 할지 접근하는 과정 3) 실제 문제를 해결하기 위해 알고리즘을 모델링하는 과정 캐글 API를 colab에서 사용하기 위한 인증 및 google storage에 업로드 되어있는 인증키 파일 현재 colab pwd로 복사해온 후 설정완료하기 12345678from google.colab import authimport warnings%matplotlib inline%config InlineBackend.figure_format = 'retina'warnings.filterwarnings(\"ignore\")auth.authenticate_user()!gsutil cp gs://kaggle_key/kaggle.json kaggle.json 1234567!mkdir -p ~/.kaggle!mv ./kaggle.json ~/.kaggle/!chmod 600 ~/.kaggle/kaggle.json!pip install kaggle# 캐글 competition 목록확인#!kaggle competitions list 목표 competition의 데이터 다운로드 12345# 파일 확인!kaggle competitions files -c word2vec-nlp-tutorial# 파일 다운로드!kaggle competitions download -c word2vec-nlp-tutorial 데이터 분석 및 전처리 모델을 학습시키기 전에 데이터를 전처리하는 과정을 거쳐야 한다. 전처리는 데이터를 모델에 적용하기에 적합하도록 데이터를 정제하는 과정이다. 그전에 데이터를 불러오고 분석하는 과정을 선행할 것이다. EDA과정을 거친 후 분석 결과를 바탕으로 전처리 작업을 할 것이다. 참고로 데이터를 불러오는데 403 error가 출력된다면, 우선적으로 대회의 rule을 check했는지 확인해 보아야 한다. sampleSubmission.csv 파일을 제외한 나머지 파일이 zip으로 압축돼 있기 때문에 압축을 푸는 과정부터 시작한다. 압축을 풀기 위해 zipfile이라는 내장 라이브러리를 사용할 것이다. - 압축을 풀기 위해 경로와 압축을 풀 파일명을 리스트로 선언한 후 반복문을 사용해 압축을 풀 것이다. 123456789101112import zipfileDATA_IN_PATH = '/content/'file_list = ['labeledTrainData.tsv.zip', 'testData.tsv.zip', 'unlabeledTrainData.tsv.zip']for file in file_list: # 압축풀기 대상 설정 및 모드 설정 zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r') # 압축 풀기 및 저장 경로 설정 zipRef.extractall(DATA_IN_PATH) # 호출 종료 zipRef.close() 12345678import numpy as npimport pandas as pdimport osimport matplotlib.pyplot as pltimport seaborn as sns# 그래프를 바로 그리도록 함%matplotlib inline 현재 사용할 데이터는 tap(\\t)으로 구분돼 있으므로 delimeter=’\\t’로 설정해주었고, 각 데이터에 각 항목명(Header)이 포함돼 있기 때문에 header인자에 0을 설정한다. R에서는 header=Ture로 하는 역할과 같다고 보면된다. 그리고 쌍따옴표를 무시하기 위해 quoting=3을 설정해 주었다. 123train_data = pd.read_csv(DATA_IN_PATH+\"labeledTrainData.tsv\", header=0, delimiter='\\t', quoting=3)train_data.head() 데이터 분석 진행 순서 1) 데이터 크기 2) 데이터의 개수 3) 각 리뷰의 문자 길이 분포 4) 많이 사용된 단어 5) 긍정, 부정 데이터의 분포 6) 각 리뷰의 단어 개수 분포 7) 특수문자 및 대문자, 소문자 비율 1234567891011# 데이터 크기print(\"파일 크기 : \")for file in os.listdir(DATA_IN_PATH): if 'tsv' in file and 'zip' not in file: print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + 'MB')# 학습 데이터의 개수print('전체 학습 데이터의 개수: &#123;&#125;'.format(len(train_data)))# 결과# 전체 학습 데이터의 개수: 25000 각 review의 길이를 분석 12train_length = train_data['review'].apply(len)train_length.head() 각 리뷰의 문자 길이가 대부분 6,000 이하이고 대부분 2,000이하에 분포돼 있음을 알 수 있다. 그리고 일부 데이터의 경우 이상치로 10,000 이상의 값을 가지고 있다. 123456789101112plt.figure(figsize=(12, 5))plt.hist(train_length, bins=200, alpha=0.5, color='r', label='word')# y축의 범위를 log단위로 바꿔주고 non-positive에 대해서는 아주작은 양수로 클리핑한다.plt.yscale('log', nonposy='clip')plt.title('Log-Histogram of length of review')plt.xlabel('Length of review')plt.ylabel('Number of review') 12# 기초 통계량 확인train_length.describe() 123plt.figure(figsize=(12, 5))plt.boxplot(train_length, labels=['train data review length'], showmeans=True) wordcloud를 통해 시각적으로 빈도수를 확인하기 위해 설치한다. 워드 클라우드를 보면 가장 많이 사용된 단어는 br이라는 것을 확인할 수 있다. HTML 태그인 br 해당 데이터가 높은 빈도수를 보이는 것으로 미루어보아 정제되지 않은 인터넷 상의 리뷰 형태로 작성돼 있음을 알 수 있다. 이후 전처리 작업에서 이 태그들을 모두 제거하겠다. 1!pip install wordcloud 12345from wordcloud import WordCloudcloud = WordCloud(width=800, height=600).generate(' '.join(train_data['review']))plt.figure(figsize=(20, 15))plt.imshow(cloud)plt.axis('off') 이제 각 라벨의 분포를 확인해 본다. 해당 데이터의 경우 긍정과 부정이라는 두 가지 라벨만 가지고 있다. 분포의 경우 또 다른 시각화 도구인 seaborn을 사용해 시각화하겠다. label의 분포 그래프를 보면 거의 동일한 개수로 분포돼 있음을 확인 할 수 있다. 123fig, axe = plt.subplots(ncols=1)fig.set_size_inches(6, 3)sns.countplot(train_data['sentiment']) 123456print(\"긍정 리뷰 개수: &#123;&#125;\".format(train_data['sentiment'].value_counts()[1]))print(\"부정 리뷰 개수: &#123;&#125;\".format(train_data['sentiment'].value_counts()[0]))# 결과# 긍정 리뷰 개수: 12500# 부정 리뷰 개수: 12500 각 리뷰를 단어 기준으로 나눠서 각 리뷰당 단어의 개수를 확인해 본다. 단어는 띄어쓰기 기준으로 하나의 단어라 생각하고 개수를 계산한다. 우선 각 단어의 길이를 가지는 변수를 하나 설정하자. 1train_word_counts = train_data['review'].apply(lambda x: len(x.split(' '))) 대부분의 단어가 1000개 미만의 단어를 가지고 있고, 대부분 200개 정도의 단어를 가지고 있음을 확인할 수 있다.1234567plt.figure(figsize=(15,10))plt.hist(train_word_counts, bins=50, facecolor='r', label='train')plt.title('Log-Histogram of word count in review', fontsize=15)plt.yscale('log', nonposy='clip')plt.legend()plt.xlabel('Number of words', fontsize=15)plt.ylabel('Number of reviews', fontsize=15) review의 75%가 300개 이하의 단어를 가지고 있음을 확인 할 수 있다.1train_word_counts.describe() 마지막으로 각 review에 대해 구두점과 대소문자 비율 값을 확인한다. 대부분 마침표를 포함하고 있고, 대문자도 대부분 사용하고 있다. 따라서 전처리 과정에서 대문자의 경우 모두 소문자로 바꾸고 특수 문자의 경우 제거한다. 이 과정은 학습에 방해가 되는 요소들을 제거하기 위함이다. 1234567891011121314151617181920212223# 물음표가 구두점으로 사용되는 비율qmarks = np.mean(train_data['review'].apply(lambda x : '?' in x))# 마침표가 구두점으로 사용되는 비율fullstop = np.mean(train_data['review'].apply(lambda x : '.' in x))# 첫 번째 대문자의 비율capital_first = np.mean(train_data['review'].apply(lambda x : x[0].isupper()))# 대문자 비율capitals = np.mean(train_data['review'].apply(lambda x : max([y.isupper() for y in x])))# 숫자 비율numbers = np.mean(train_data['review'].apply(lambda x : max([y.isdigit() for y in x])))print('물음표가 있는 질문: &#123;:.2f&#125;%'.format(qmarks * 100))print('마침표가 있는 질문: &#123;:.2f&#125;%'.format(fullstop * 100))print('첫 글자가 대문자인 질문: &#123;:.2f&#125;%'.format(capital_first * 100))print('대문자가 있는 질문: &#123;:.2f&#125;%'.format(capitals * 100))print('숫자가 있는 질문: &#123;:.2f&#125;%'.format(numbers * 100))# 결과# 물음표가 있는 질문: 29.55%# 마침표가 있는 질문: 99.69%# 첫 글자가 대문자인 질문: 0.00%# 대문자가 있는 질문: 99.59%# 숫자가 있는 질문: 56.66% 데이터 전처리 데이터를 모델에 적용할 수 있도록 데이터 전처리를 진행한다. 먼저 데이터 전처리 과정에서 사용할 라이브러리들을 불러올 것이다. 우선 전처리를 위해 nltk의 stopword를 이용하기 위해 nltk에서 다운로드를 받아야한다. 필자는 all를 선택하여서 모든 파일을 download를 받았지만, stopword만 받아도 상관없다. re, BeautifulSoup : 데이터 정제 stopwords : 불용어 제거 pad_sequence, Tokenizer : 데이터 전처리 12345678910import reimport pandas as pdimport numpy as npimport jsonfrom bs4 import BeautifulSoupfrom nltk.corpus import stopwordsfrom tensorflow.python.keras.preprocessing.sequence import pad_sequencesfrom tensorflow.python.keras.preprocessing.text import Tokenizerimport nltknltk.download() 리뷰 데이터를 보면 문장 사이에 과 같은 HTML 태그와 ‘\\’, ‘…’ 같은 특수문자가 포함된 것을 확인할 수 있다. 문장부호 및 특수문자는 일반적으로 문자의 의미에 크게 영향을 미치지 않기 때문에 최적화된 학습을 위해 제거하자 BeautifulSoup의 get_text함수를 이용하면 HTML 태그를 제거한 나머지 텍스트를 얻을 수 있고 re.sub을 이용해 특수문자를 제거한다. stopwords(불용어)를 삭제할 것이다. 불용어란 문장에서 자주 출현하나 전체적인 의미에 큰 영향을 주지 않는 단어를 말한다. 예를들어, 영어에서는 조사, 관사 등과 같은 어휘가 있다. 데이터에 따라 불용어를 제거하는 것은 장단점이 있다. 경우에 따라 불용어가 포함된 데이터를 모델링하는 데 있어 노이즈를 줄 수 있는 요인이 될 수 있어 불용어를 제거하는 것이 좋을 수 있다. 그렇지만 데이터가 많고 문장 구문에 대한 전체적인 패턴을 모델링하고자 한다면 이는 역효과를 줄 수도 있다. 지금 시행하고자 하는 분석은 감성 분석을 하고 있으므로 불용어가 감정 판단에 영향을 주지 않는다고 가정하고 불용어를 제거한다. 불용어를 제거하려면 따로 정의한 불용어 사전을 이용해야 한다. 사용자가 직접 정의할 수도 있지만 고려해야 하는 경우가 너무 많아서 보통 라이브러리에서 일반적으로 정의해놓은 불용어 사전을 이용한다. NLTK의 불용어 사전을 이용할 것이며, NLTK에서 제공하는 불용어 사전은 전부 소문자 단어로 구성돼 있기 때문에 불용어를 제거하기 위해서는 모든 단어를 소문자로 바꿔야한다. review_text를 lower함수를 사용해 모두 소문자로 바꿔주었고, 이후 split 함수를 사용해 공백을 기준으로 reivew_text를 단어 리스트로 바꾼 후 불용어에 해당하지 않는 단어만 다시 모아서 리스트로 만들었다. 결과를 보면 단어 리스트가 하나의 문자열로 바뀐 것을 확인할 수 있다. 데이터를 한번에 처리하기 위해 위의 과정을 하나의 함수로 작성한다음에 apply로 적용시킨다. 함수의 경우 불용어 제거는 인자값으로 받아서 선택할 수 있게 하였다. 1234567891011121314151617181920212223242526272829def preprocessing(review, remove_stopwords=False): # 불용어 제거는 옵션으로 선택 # 1. HTML 태그 제거 review_text = BeautifulSoup(review, 'html5lib').get_text() # 2. 영어가 아닌 특수문자를 공백(\" \")으로 대체 review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text) # 3. 대문자를 소문자로 바꾸고 공백 단위로 텍스트를 나눠서 리스트로 만든다. words = review_text.lower().split() if remove_stopwords: # 4. 불용어 제거 # 영어 불용어 불러오기 stops = set(stopwords.words('english')) # 불용어가 아닌 단어로 이뤄진 새로운 리스트 생성 words = [w for w in words if not w in stops] # 5. 단어 리스트를 공백을 넣어서 하나의 글로 합친다. clean_review = ' '.join(words) else: # 불용어를 제거하지 않을 때 clean_review = ' '.join(words) return clean_review 123train_data['clean_review']=train_data['review'].apply(lambda x : preprocessing(review=x, remove_stopwords=True))train_data['clean_review'][0] 우선 전처리한 데이터에서 각 단어를 인덱스로 벡터화해야 한다. 그리고 모델에 따라 입력값의 길이가 동일해야 하기 때문에 일정 길이로 자르고 부족한 부분은 특정값으로 채우는 패딩 과정을 진행해야 한다. 하지만 모델에 따라 각 review가 단어들의 인덱스로 구성된 벡터가 아닌 텍스트로 구성돼야 하는 경우도 있다. 따라서 지금까지 전처리한 데이터를 pandas의 DataFrame으로 만들어 두고 이후에 전처리 과정이 모두 끝난 후 전처리한 데이터를 저장할 때 함께 저장하게 한다. 123456# from tensorflow.python.keras.preprocessing.sequence import pad_sequences# from tensorflow.python.keras.preprocessing.text import Tokenizertokenizer = Tokenizer()tokenizer.fit_on_texts(train_data['clean_review'])text_sequences = tokenizer.texts_to_sequences(train_data['clean_review']) 위와 같이 사전에 등록되어진 인덱스로 각 review의 값들이 변경되었음을 확인할 수 있었다. 단어 사전은 앞서 정의한 tokenizer 객체에 word_index 값을 뽑으면 dictionary 형태로 구성되어 있음을 확인 할 수 있다. 또한 단어 사전 뿐만 아니라 전체 단어 개수도 이후 모델에서 사용되기 때문에 저장해 둔다. 123word_vocab = tokenizer.word_indexprint(word_vocab)print(\"전체 단어 개수:\", len(word_vocab)) 12345data_configs = &#123;&#125;data_configs['vocab'] = word_vocabdata_configs['vocab_size'] = len(word_vocab) + 1 현재 각 데이터는 서로 길이가 다른데 이 길이를 하나로 통일해야 이후 모델에 바로 적용할 수 있기 때문에 특정 길이를 최대 길이로 정하고 더 긴 데이터의 경우 뒷부분을 자르고 짧은 데이터의 경우에는 0 값으로 패딩하는 작업을 진행한다. 패딩 처리에는 앞서 불러온 pad_sequences 함수를 사용한다. 이 함수를 사용할 때는 인자로 패딩을 적용할 데이터, 최대 길이값, 0 값을 데이터 앞에 넣을지 뒤에 넣을 지 여부를 설정한다. 또한, 제일 마지막 단어부터 단어를 카운트한다는 것에 유의하자. 여기서 최대 길이를 174로 설정했는데, 이는 앞서 데이터 분석 과정에서 단어 개수의 통계를 계산했을 때 나왔던 중앙값(median)이다. 보통 평균이 아닌 중앙값(median)을 사용하는 경우가 많은데, 평균은 이상치에 민감하기 때문이다. 1234567# 문장 최대 길이MAX_SEQUENCE_LENGTH = 174# padding을 뒷부분에 한다.train_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post')print('Shape of train data: ', train_inputs.shape) 마지막으로 학습 시 label 값을 넘파이 배열로 저장한다. 그 이유는 이후 전처리한 데이터를 저장할 때 넘파이 형태로 저장하기 때문이다. 12train_labels = np.array(train_data['sentiment'])print('Shape of label tensor: ', train_labels.shape) 이제 전처리한 데이터를 이후 모델링 과정에서 사용하기 위해 저장할 것이다. 여기서는 다음과 같은 총 4개의 데이터를 저장할 것이다. 텍스트 데이터의 경우 CSV 파일로 저장하고, 벡터화한 데이터와 정답 라벨의 경우 넘파이 파일로 저장한다. 마지막 데이터 정보의 경우 dictionary 형태이기 때문에 Json 파일로 저장한다. 정제된 텍스트 데이터 벡터화한 데이터 정답 라벨 데이터 정보 우선 경로와 파일명을 설정하고 os 라이브러리를 통해 폴더가 없는 경우 폴더를 생성한다. 1234567891011121314151617181920DATA_OUT_PATH = '/content/'TRAIN_INPUT_DATA = 'train_input.npy'TRAIN_LABEL_DATA = 'train_label.npy'TRAIN_CLEAN_DATA = 'train_clean.csv'DATA_CONFIGS = 'data_configs.json'import os# 저장하는 디렉터리가 존재하지 않으면 생성if not os.path.exists(DATA_OUT_PATH): os.makedirs(DATA_IN_PATH)# 전처리된 데이터를 numpy 형태로 저장np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)# 정제된 텍스트를 CSV 형태로 저장train_data.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index=False)# 데이터 사전을 JSON 형태로 저장json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, 'w'), ensure_ascii=False) 지금까지 학습 데이터에 대해서만 전처리를 했으므로 테스트 데이터에 대해서도 위와 동일한 과정을 진행하면 된다. 다른 점은 평가 데이터의 경우 라벨 값이 없기 때문에 라벨은 따로 저장하지 않아도 되며, 데이터 저옵인 단어 사전과 단어 개수에 대한 정보도 학습 데이터의 것을 사용하므로 저장하지 않아도 된다. 추가로 테스트 데이터에 대해 저장해야 하는 값이 있는데 각 review 데이터에 대해 review에 대한 &#39;id&#39;값을 저장해야 한다. 평가 데이터를 전처리 할 때 한 가지 중요한 점은 Tokenizer를 통해 인덱스 벡터로 만들 때 Tokenizing 객체로 새롭게 만드는 것이 아니라, 기존에 학습 데이터에 적용한 Tokenizer 객체를 사용해야 한다는 것이다. 만약 새롭게 만들 경우 학습 데이터와 평가 데이터에 대한 각 단어의들의 인덱스가 달라져서 모델에 정상적으로 적용할 수 없기 때문이다. fit_on_texts, fit_on_sequences는 사전을 업데이트하는 행위인데 아래의 코드에서 fit_on_texts를 실행하지 않는 이유는 Tokenizer 객체를 새로 생성하지 않았기에 Train data의 사전을 갖고 만약에 Train data에 포함되어 있지 않은 단어가 Test data에 존재한다면 확률을 0으로 주어야 하기 때문이다.1234567test_data = pd.read_csv(DATA_IN_PATH + 'testData.tsv', header=0, delimiter='\\t', quoting=3)test_data['review'] = test_data['review'].apply(lambda x: preprocessing(x, True))test_id = np.array(test_data['id'])text_sequences = tokenizer.texts_to_sequences(test_data['review'])test_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post') 1234567TEST_INPUT_DATA = 'test_input.npy'TEST_CLEAN_DATA = 'test_clean.csv'TEST_ID_DATA = 'test_id.npy'np.save(open(DATA_IN_PATH + TEST_INPUT_DATA, 'wb'), test_inputs)np.save(open(DATA_IN_PATH + TEST_ID_DATA, 'wb'), test_id)test_data.to_csv(DATA_IN_PATH + TEST_CLEAN_DATA, index=False)","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"Scrapy 웹 크롤링 04 - 실습","slug":"Crawling_03","date":"2020-01-28T05:55:17.000Z","updated":"2020-01-28T15:50:27.205Z","comments":true,"path":"2020/01/28/Crawling_03/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/28/Crawling_03/","excerpt":"","text":"Scrapy PracticeDaum 크롤링하기 다음 디지털 뉴스 페이지에서 현재 URL, 기사 타이틀에 걸려있는 href URL, 기사 페이지로 이동한 후 기사 제목, 기사 내용을 크롤링하는 것을 목표로 크롤러를 만들것 items.py 먼저, 크롤링 대상을 items를 활용하기 위해 items.py에 Field를 생성한다. 위에서 언급했던 사항뿐만 아니라 SQLite에 저장할때, 수집된 시간을 로그로 남겨 놓기 위한 Field도 생성시켜 준다. 12345678910111213141516171819import scrapyclass PracticeItem(scrapy.Item): # name = scrapy.Field() # 기사 제목 headline = scrapy.Field() # 기사 본문 contents = scrapy.Field() # 요청 리스트 페이지 parent_link = scrapy.Field() # 기사 페이지 article_link = scrapy.Field() # 수집된 시간 crawled_time = scrapy.Field() setting.pyMiddlware 개인이 만든 기능을 추가해서 사용가능하게 하는것, 즉, Pipeline은 Item이 Export되어 파일에 저장되기 직전에 작업을 수행한다면, Middleware는 요청하기 직전, 응답 후에, 어떤함수가 실행전에, 이런 중간에서 작업을 수행하는 것이라고 비교해볼 수 있다. 다른 크롤링할때와 마찬가지로 동일한 User-agent를 가지고 한다면, 서버에 지속적인 부하를 주게되어서 벤을 당한다거나 할 수 있으므로 Fake User-agent를 활용하여 크롤링을 할 것이다. Middlware의 장점은 커스터마이징할 수 있으므로 다른 사람들이 이미 개발해 놓은 것들이 많다. 특히 Github에서 검색한다면 star가 많은 것을 사용하는 것이 좋다는 것은 누구나 알고 있는 사실!! 필자가 사용한 Middleware의 사이트를 보면 scrapy 1.0이상과 1.0미만 버전에 따라 사용방법이 다른 것을 확인할 수 있다. 필자의 경우 1.8이므로 1.0이상의 방법을 사용할 것이다. 1pip install scrapy-fake-useragent 아래 주석 처리된 USER_AGENT 변수는 실제 자신의 user-agent를 사용해야하며, 주석처리한 이유는 추후에는 서버에 과부하주어 벤당하는 것을 방지하기 위해 fake-agent를 사용할 것이기 때문이다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051BOT_NAME = 'practice'SPIDER_MODULES = ['practice.spiders']NEWSPIDER_MODULE = 'practice.spiders'# User-agent 설정(개발자도구에서 Network창에서 찾아서 자신의 정보를 복사#USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'# Obey robots.txt rulesROBOTSTXT_OBEY = False# 다운로드 간격DOWNLOAD_DELAY = 2# 쿠키사용COOKIES_ENABLED = True# Referer 삽입# daum은 보안이 엄격하기에 referer속성을 주어야 한다.DEFAULT_REQUEST_HEADERS = &#123;'Referer' : 'https://news.daum.net/breakingnews/digital?page=2'&#125;# 재시도 횟수RETRY_ENABLED = TrueRETRY_TIME = 2# 한글 쓰기(출력 인코딩)FEED_EXPORT_ENCODING = 'utf-8'# User-agent 미들웨어 사용DOWNLOADER_MIDDLEWARES = &#123; 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None, 'scrapy.downloadermiddlewares.retry.RetryMiddleware': None, 'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400, 'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,&#125;# 파이프 라인 활성화# 숫자가 작을수록 우선순위 상위ITEM_PIPELINES = &#123; 'practice.pipelines.PracticePipeline': 300,&#125;# 캐시 사용#HTTPCACHE_ENABLED = True#HTTPCACHE_EXPIRATION_SECS = 0#HTTPCACHE_DIR = 'httpcache'#HTTPCACHE_IGNORE_HTTP_CODES = []#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage' pipelines.py 크롤링으로 수집된 시간의 로그를 남기기 위해 datetime 라이브러리를, DB에 저장하기 위해 sqlite3, 마지막으로 예외적인 처리를 위해 DropItem 라이브러리를 사용할 것이다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import datetimeimport sqlite3from scrapy.exceptions import DropItemclass PracticePipeline(object): # 초기화 메소드 def __init__(self): # DB 설정(자동 커밋) # isolation_level=None =&gt; Auto Commit self.conn = sqlite3.connect('저장할 위치에 대한 path/저장할 파일명.db', isolation_level=None) # DB 연결 self.c = self.conn.cursor() # 최초 1회 실행 def open_spider(self, spider): spider.logger.info('NewsSpider Pipeline Started.') self.c.execute(\"CREATE TABLE IF NOT EXISTS NEWS_DATA(id INTEGER PRIMARY KEY AUTOINCREMENT, headline text, contents text, parent_link text, article_link text, crawled_time text)\" # Item 건수 별 실행 def process_item(self, item, spider): if not item.get('contents') is None: # 삽입 시간 crawled_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') # 크롤링 시간 필드 추가 item['crawled_time'] = crawled_time # 데이터 -&gt; DB 삽입 # tuple(item[k] for k in item.keys()) 로 대신해도 된다. self.c.execute('INSERT INTO NEWS_DATA(headline, contents, parent_link, article_link, crawled_time) VALUES(?, ?, ?, ?, ?);', (item.get('headline'), item.get('contents'), item.get('parent_link'), item.get('article_link'), item.get('crawled_time'))) # tuple(item[k] for k in item.keys()) # 로그 spider.logger.info('Item to DB inserted.') # 결과 리턴 return item else: raise DropItem('Dropped Item. Because This Contents is Empty.') # 마지막 1회 실행 def close_spider(self, spider): spider.logger.info('NewsSpider Pipeline Stopped.') # commit(auto commit으로 설정했지만 혹시 모르니) self.conn.commit() # 연결 해제 self.conn.close() Spider.py 먼저 도메인과 시작하는 URL을 정하고나서 페이지의 규칙을 찾아본다. 규칙을 정해주면 LinkExtractor로 반복되는 URL을 보내줄 수 있다. 단, 1자리수 이외의 2자리수부터의 반복은 Rule함수에 follow=True로 주어야한다. 또한 변수명 rules로 Rule객체를 받아야 사용가능함을 기억하자. parent page에서 해당 기사들에 대한 url을 parse_child 함수로 넘겨주는데, 이때 그냥 넘겨주지 않고 parent page에서 얻은 정보 또한 meta parameter를 통해 같이 넘겨 줄수 있다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Ruleimport syssys.path.insert(0, 'items.py가 있는 절대 path')from items import PracticeItemclass NewcralSpider(CrawlSpider): name = 'newcral' allowed_domains = ['news.daum.net'] start_urls = ['https://news.daum.net/breakingnews/digital'] # 링크 크롤링 규칙(정규표현식 사용 추천) # page=\\d$ : 1자리 수 # page=\\d+ : 연속, follow=True rules = [ Rule(LinkExtractor(allow=r'/breakingnews/digital\\?page=\\d+'), callback='parse_parent', follow=True), ] def parse_parent(self, response): # 부모 URL 로깅 self.logger.info('Parent Response URL : %s' % response.url) for url in response.css('ul.list_news2.list_allnews &gt; li &gt; div'): # URL 신문 기사 URL article_link = url.css('strong &gt; a::attr(href)').extract_first().strip() yield scrapy.Request(article_link, self.parse_child, meta=&#123;'parent_url': response.url&#125;) def parse_child(self, response): # 부모, 자식 수신 정보 로깅 self.logger.info('----------------------------------------') self.logger.info('Response From Parent URL : %s' % response.meta['parent_url']) self.logger.info('Child Response URL : %s' % response.url) self.logger.info('Child Response Status ; %s' % reponse.status) self.logger.info('----------------------------------------') # 요청 리스트 페이지 parent_link = response.meta['parent_url'] # 기사 페이지 article_link = response.url # 헤드라인 headline = response.css('h3.tit_view::text').extract_first().strip() # 본문 c_list = response.css('div.article_view &gt; section &gt; p::text').extract() contents = ''.join(c_list).strip() yield PracticeItem(headline=headline, contents=contents, article_link=article_link, parent_link=parent_link) 도움이 되는 학습 비동기(asyncio), 병렬프로그래밍, 스레드, 멀티 프로세싱등 routine 개념을 학습해야 네트워크상의 블록 또는 논블럭 io로 인해 지연시간이 발생되는데, 제어권을 넘겨주면서 좀 더 성능을 올릴 수 있다. scrapy twisted는 예를 들어, 위의 실습에서와 같이 데이터를 크롤링한 후 pipeline을 통해 DB에 insert 할 때 이 작업이 다 끝나지 않으면, 다음 데이터에 대한 작업으로 넘어 갈 수 없어 시간적으로 성능이 떨어질수 있는데, 이런 경우 비동기식으로 처리를 해줌으로써 성능을 올릴수 있게끔 하는 framework이다.","categories":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/categories/crawling/"}],"tags":[]},{"title":"Scrapy 웹 크롤링 03 - Exports, Settings, pipeline","slug":"Crawling_02","date":"2020-01-23T16:56:11.000Z","updated":"2020-01-27T16:05:56.552Z","comments":true,"path":"2020/01/24/Crawling_02/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/24/Crawling_02/","excerpt":"","text":"Exports 우리가 실행후 크롤링한 데이터를 저장하는 path를 실행할때마다 지정하거나 실행했는데, 일종의 template같이 미리 만들어 놓을 수 있는 기능이 Exports이다. Exports 참조 사이트 : https://docs.scrapy.org/en/latest/topics/feed-exports.html 1234# 아래 2가지 방법은 동일한 방법이다.scrapy runspider using_items.py -o test.json -t jsonscrapy runspider using_items.py --output test.json --output-format json 위에서와 같이 크롤링을 할 경우에 명령어를 통해 결과의 형식과 파일 이름을 지정해주는 것과 다르게 Settings.py에서 미리 지정하여 사용할 수 있다. 커맨드라인에서도 가능하지만, 모든 테스트를 다 거친 후 확정적으로 사용할 것이라면, settings.py에서 변수설정을 하는 것이 더 좋다. 우리가 크롤링할 사이트 https://globalvoices.org/ 사이트의 기사들의 제목만을 크롤링 할 것이다. 1234567891011121314151617181920212223242526# -*- coding: utf-8 -*-import scrapy# Scrapy 환경설정# 중요# 실행방법# 1.커맨드 라인 실행 -&gt; scrapy crawl 크롤러명 -s(--set) &lt;NAME&gt;=&lt;VALUE&gt;class ScrapyWithSettingsSpider(scrapy.Spider): name = 'scrapy_with_settings' allowed_domains = ['globalvoices.org'] start_urls = ['https://globalvoices.org/'] def parse(self, response): # 아래 3가지는 동일한 결과를 보여주는 코드 # response.css('#main &gt; div.post-archive-container &gt; div#post-archive div.dategroup div.post-excerpt-container &gt; h3 &gt; a::text').getall() # response.xpath('//*[@id=\"post-archive\"]//div[@class=\"dategroup\"]//div[@class=\"post-summary-content\"]//div[@class=\"post-excerpt-container\"]/h3/a/text()').getall() # xpath + css 혼합 for i, v in enumerate(response.xpath('//div[@class=\"post-summary-content\"]').css('div.post-excerpt-container &gt; h3 &gt; a::text').extract(),1): # 인덱스 번호, 헤드라인 yield dict(num=i, headline=v) settings.py에서 export하는 변수 설정 settings.py에 아래와 같이 필요한 변수를 추가로 설정하면된다. 저장소, 저장 형식 관련 레퍼런스 12345678910111213# 출력(Exports)설정# 파일이름 및 경로# 만약 다른 특정 위치를 지정하고 싶다면 가능하다.FEED_URI = 'result.json'# 파일 형식FEED_FORMAT = 'json'# 출력 인코딩FEED_EXPORT_ENCODING = 'utf-8'# 기본 들여쓰기FEED_EXPORT_INDENT = 2 또한, 동일한 자원을 반복해서 크롤링할 경우 서버에 과부하를 주는 것을 막기 위해 cache를 사용할 수도 있다. 이 또한, setting.py에서 변수를 설정할 수 있다. pipeline 참고 pipeline은 item들이 최종적으로 나오는 파\u001c일을 만들기 전\u001c에 약간의 처리를 해주는 작업이라고 생각하면된다. 물론 spider에서도 가능하지만 장기적으로 코드 관리적인 면을 봤을 때 너무 좋지 않은 방식이다. 예를 들어, 크롤러를 만들었던 사이트의 구조가 바뀌었다면 한 python script에 모든 코드를 작성한다면 변경된 사이트의 구조에 맞춰 코드를 변경하려면 코드를 해석하는데 오랜시간을 투자해야 할 것이다. Item pipeline의 전형적인 예시 HTML data 제거 정확하지 않은 데이터(또는 동일 데이터)가 수집되었다면 출력 전 pipeline단계에서 validation을 할 수 있다. 중복 체크 데이터베이스에 저장 pipeline 사용을 위한 새로운 크롤링 사이트 : https://www.alexa.com/topsites 자신의 사이트 방문 50위 사이트를 알 수 있는 웹사이트이다. 사이트 순위, 사이트 명, 하루에 방문하는 평균 시간, 하루에 방문하는 평균 페이지뷰수를 크롤링할 것이다. items를 사용할 것이며, 모든 정보를 크롤링한후 pipeline을 통해 40위권안의 순위에 해당하는 데이터만 저장하는 방식으로 코드를 작성할 것이다. setting.py에서 아래 그림과 같이 pipeline을 사용하기 위해 주석을 풀어주어서 사용할 것이다. 만약 아래에서와 다르게 여러개의 pipeline을 사용한다면 숫자가 낮을 수록 우선 순위를 갖는다는 점에 유의하자. spider를 다음과 같이 구성하였다. 12345678910111213141516171819202122232425262728293031323334353637# -*- coding: utf-8 -*-import scrapyimport sys# items.py에 대한 path 추가sys.path.insert(0, '../project/chat_bot_project/section01_2/section01_2')from items import SiteRankItemsclass Pipeline01Spider(scrapy.Spider): name = 'pipeline_01' allowed_domains = ['alexa.com/topsites'] start_urls = ['https://www.alexa.com/topsites'] def parse(self, response): \"\"\" :param :response : return : SiteRankItems \"\"\" for p in response.css('div.listings.table &gt; div.tr.site-listing'): # 아이템 객체 생성 item = SiteRankItems() # 순위 item['rank_num'] = p.xpath('./div[1]/text()').get() # 사이트명 item['site_name'] = p.xpath('./div[2]/p/a/text()').get() # 평균 접속 시간 item['daily_time_site'] = p.xpath('./div[3]/p/text()').get() # 평균 본 횟수 item['daily_page_view'] = p.xpath('./div[4]/p/text()').get() yield item~ 위의 코드를 통해 크롤링한 데이터를 이제 pipeline을 통해 처리해보자. 간단히 csv파일과 엑셀파일을 만들어 볼 것이다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Section012Pipeline(object): # 초기화 method # init method도 class가 초기화될 때 최초로 실행되므로 open_spider와 동일하게 사용가능 def __init__(self): # 엑셀 처리 선언 self.workbook = xlsxwriter.Workbook(\"../chat_bot_project/section01_2/section01_2/spiders/result_excel.xlsx\") # CSV 처리 선언 (a, w 옵션 변경) self.file_opener = open(\"../chat_bot_project/section01_2/section01_2/spiders/result_csv.csv\", \"w\") self.csv_writer = csv.DictWriter(self.file_opener, fieldnames=['rank_num','site_name','daily_time_site','daily_page_view','is_pass']) # 워크시트 self.worksheet = self.workbook.add_worksheet() # 삽입 수 self.rowcount = 1 # 최초 1회 실행 def open_spider(self, spider): spider.logger.info(\"TestSpider Pipelines Started.\") # 데이터를 크롤링할때 매번실행 def process_item(self, item, spider): # 현재 item은 spider에서 item을 활용해서 작성했으므로 dictionary로 되어있다. # rank_num이 40위 안에 있는 사이트들만 저장하기 위한 코드 if int(item.get('rank_num')) &lt; 41 : item['is_pass'] = True # 엑셀 저장 # item['rank_num']처럼 접근가능하지만 데이터가 없다면 에러가 발생하므로 아래에서 처럼 get method를 사용하는 것이 좋다. self.worksheet.write('A%s' % self.rowcount, item.get('rank_num' )) self.worksheet.write('B%s' % self.rowcount, item.get('site_name' )) self.worksheet.write('C%s' % self.rowcount, item.get('daily_tiem_site' )) self.worksheet.write('D%s' % self.rowcount, item.get('daily_page_view' )) self.worksheet.write('E%s' % self.rowcount, item.get('is_pass' )) self.rowcount += 1 # csv 저장 self.csv_writer.writerow(item) return item else: # raise DropItem(f'Dropped Item. Because This Site Rank is &#123;item.get(\"rank_num\")&#125;') # 마지막 1회 실행 def close_spider(self, spider): # 엑셀 파일 닫기 self.workbook.close() # CSV 파일 닫기 self.file_opener.close() spider.logger.info(\"TestSpider Pipelines Finished\")","categories":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/categories/crawling/"}],"tags":[]},{"title":"Attention mechanism을 사용한 Seq2seq 구현","slug":"deep_learning_11","date":"2020-01-21T20:15:09.000Z","updated":"2020-01-23T17:01:03.099Z","comments":true,"path":"2020/01/22/deep_learning_11/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/22/deep_learning_11/","excerpt":"","text":"Vallina Seq2seq tf.function을 사용하기 위해 tensorflow 2.0.0-beta1버전을 설치한다. 한글 텍스트의 형태소분석을 위해 konlpy에서 Okt(Original Korean tag, Twitter에서 공개한 오픈소스 라이브러리)를 사용하기 위해 설치해준다. 12!pip install tensorflow==2.0.0-beta1!pip install konlpy 필요한 라이브러리 import 123import randomimport tensorflow as tffrom konlpy.tag import Okt tensorflow 버전이 맞는지 확인 1print(tf.__version__) 하이퍼 파라미터 설정123EPOCHS = 200# 가장 많이 사용된 2000개를 사용하기 위해NUM_WORDS = 2000 Encoder1234567891011121314class Encoder(tf.keras.Model): def __init__(self): super(Encoder, self).__init__() # 2000개의 단어들을 64크기의 vector로 Embedding해줌. self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64) # return_state는 return하는 Output에 최근의 state를 더해주느냐에 대한 옵션 # 즉, Hidden state와 Cell state를 출력해주기 위한 옵션이라고 볼 수 있다. # default는 False이므로 주의하자! self.lstm = tf.keras.layers.LSTM(512, return_state=True) def call(self, x, training=False, mask=None): x = self.emb(x) _, h, c = self.lstm(x) return h, c Decoder1234567891011121314151617class Decoder(tf.keras.Model): def __init__(self): super(Decoder, self).__init__() self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64) # return_sequence는 return 할 Output을 full sequence 또는 Sequence의 마지막에서 출력할지를 결정하는 옵션 # False는 마지막에만 출력, True는 모든 곳에서의 출력 self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True) self.dense = tf.keras.layers.Dense(NUM_WORDS, activation='softmax') def call(self, inputs, training=False, mask=None): x, h, c = inputs x = self.emb(x) # initial_state는 셀의 첫 번째 호출로 전달 될 초기 상태 텐서 목록을 의미 # 이전의 Encoder에서 만들어진 Hidden state와 Cell state를 입력으로 받아야 하므로 x, h, c = self.lstm(x, initial_state=[h, c]) return self.dense(x), h, c Seq2seq1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Seq2seq(tf.keras.Model): def __init__(self, sos, eos): super(Seq2seq, self).__init__() self.enc = Encoder() self.dec = Decoder() self.sos = sos self.eos = eos def call(self, inputs, training=False, mask=None): if training is True: # 학습을 하기 위해서는 우리가 입력과 출력 두가지를 다 알고 있어야 한다. # 출력이 필요한 이유는 Decoder단의 입력으로 shited_ouput을 넣어주게 되어있기 때문이다. x, y = inputs # LSTM으로 구현되었기 때문에 Hidden State와 Cell State를 출력으로 내준다. h, c = self.enc(x) # Hidden state와 cell state, shifted output을 초기값으로 입력 받고 # 출력으로 나오는 y는 Decoder의 결과이기 때문에 전체 문장이 될 것이다. y, _, _ = self.dec((y, h, c)) return y else: x = inputs h, c = self.enc(x) # Decoder 단에 제일 먼저 sos를 넣어주게끔 tensor화시키고 y = tf.convert_to_tensor(self.sos) # shape을 맞춰주기 위한 작업이다. y = tf.reshape(y, (1, 1)) # 최대 64길이 까지 출력으로 받을 것이다. seq = tf.TensorArray(tf.int32, 64) # tf.keras.Model에 의해서 call 함수는 auto graph모델로 변환이 되게 되는데, # 이때, tf.range를 사용해 for문이나 while문을 작성시 내부적으로 tf 함수로 되어있다면 # 그 for문과 while문이 굉장히 효율적으로 된다. for idx in tf.range(64): y, h, c = self.dec([y, h, c]) # 아래 두가지 작업은 test data를 예측하므로 처음 예측한값을 다시 다음 step의 입력으로 넣어주어야하기에 해야하는 작업이다. # 위의 출력으로 나온 y는 softmax를 지나서 나온 값이므로 # 가장 높은 값의 index값을 tf.int32로 형변환해주고 # 위에서 만들어 놓았던 TensorArray에 idx에 y를 추가해준다. y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32) # 위의 값을 그대로 넣어주게 되면 Dimension이 하나밖에 없어서 # 실제로 네트워크를 사용할 때 Batch를 고려해서 사용해야 하기 때문에 (1,1)으로 설정해 준다. y = tf.reshape(y, (1, 1)) seq = seq.write(idx, y) if y == self.eos: break # stack은 그동안 TensorArray로 받은 값을 쌓아주는 작업을 한다. return tf.reshape(seq.stack(), (1, 64)) 학습, 테스트 루프 정의1234567891011121314151617181920# Implement training loop@tf.functiondef train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy): # output_labels는 실제 output과 비교하기 위함 # shifted_labels는 Decoder부분에 입력을 넣기 위함 output_labels = labels[:, 1:] shifted_labels = labels[:, :-1] with tf.GradientTape() as tape: predictions = model([inputs, shifted_labels], training=True) loss = loss_object(output_labels, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train_loss(loss) train_accuracy(output_labels, predictions)# Implement algorithm test@tf.functiondef test_step(model, inputs): return model(inputs, training=False) 데이터셋 준비 http://www.aihub.or.kr에서 text데이터 중 AI chatbot 데이터를 사용할 것이다. 이 데이터를 다운받아 필자는 google storage 서비스를 이용해서 기존의 생성해놓았던 버킷을 통해 데이터를 업로드 한 후, 받아와서 사용할 것이다. 이 방법은 google storage에서 파일을 받아 사용하는 gsutil 방식이며 빠르다는 점이 장점이지만 현재 세션이 종료되거나 새로시작할 경우 다시 실행 시켜주어야 하는 방식이다. 또한 필자처럼 google colab이 아닌 자신의 로컬PC로 실행할 경우 아래 단계는 건너 뛰어도 상관없다. 12345from google.colab import authauth.authenticate_user()import pandas as pd!gsutil cp gs://kaggle_key/chatbot_data.csv chatbot_data.csv chatbot_data.csv 파일이 현재 path에 존재하는지 확인 1%ls chatbot_data.csv파일을 pandas DataFrame으로 읽어 어떤 데이터들이 존재하고 추후에 x(Question)와 y(Answer)로 나눠주려면 패턴을 찾아야 하기 때문에 모든 데이터를 볼 것이다. 전체 데이터는 999개이기 떄문에 출력되어지는 row의 수를 1000개로 맞춰준다. 1pd.options.display.max_rows = 1000 12chatbot_data = pd.read_csv('chatbot_data.csv',header=0)chatbot_data 위에서 pandas로 불러들인 QA(Question &amp; Answer) data를 보면 Question과 Answer로 이루어져있다. 즉, 순차적인 데이터인 것이다. 또한 대화의 끝이 나누어져 있지 않아 입력으로 넣어주려면 Data를 Question과 Answer 쌍으로 가공해주어야 할 것이다. 맨처음 줄부터 Question 그다음은 Answer 이순으로 되어있다는 것을 확인할 수 있다. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778dataset_file = 'chatbot_data.csv'okt = Okt()with open(dataset_file, 'r') as file: lines = file.readlines() # okt 라이브러리를 통해 형태소 분석을 한줄씩 진행하였고 # 나누어진 형태소들을 하나의 sequence로 묶어주기위해 # 구분자는 공백을 사용해서 join해주었다. # 구분자를 space로 한 이유는 나중에 사용할 tokenizer에서 space를 기준으로 단어를 구분하기 때문이다. seq = [\" \".join(okt.morphs(line)) for line in lines]questions = seq[::2]# tap은 Decoder단에서 Shifted Output을 입력으로 받을때 시작점을 알려주기 위한 SOS로 tap(\\t)을 사용answers = ['\\t' + lines for lines in seq[1::2]]num_sample = len(questions)perm = list(range(num_sample))random.seed(0)random.shuffle(perm)train_q = list()train_a = list()test_q = list()test_a = list()for idx, qna in enumerate(zip(questions, answers)): q, a = qna if perm[idx] &gt; num_sample//5: train_q.append(q) train_a.append(a) else: test_q.append(q) test_a.append(a)# filters의 default에는 \\t,\\n도 제거하기 때문에 이 둘을 제외하고 나머지 문장기호들만 제거하게끔 변경해주었다.tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS, filters='!\"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`&#123;|&#125;~')# 시퀀스 목록을 기반으로 내부 어휘를 업데이트한다.tokenizer.fit_on_texts(train_q + train_a)# 위에서 업데이트한 어휘를 기반으로 실수형태의 벡터 형태로 나타내 준다.# 출력을 통해 나타나는 실수는 count의 수를 나타내는 것은 아니다!train_q_seq = tokenizer.texts_to_sequences(train_q)train_a_seq = tokenizer.texts_to_sequences(train_a)test_q_seq = tokenizer.texts_to_sequences(test_q)test_a_seq = tokenizer.texts_to_sequences(test_a)# y값에는 maxlen=65인 이유는 앞에 SOS와 뒤에 EOS가 붙어 있는 상황이므로 학습시에는 앞에 하나를 떼고# 학습하므로 실제로는 64길이만 사용하는 것과 동일하게 된다.x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq, value=0, padding='pre', maxlen=64)y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq, value=0, padding='post', maxlen=65)x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq, value=0, padding='pre', maxlen=64)y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq, value=0, padding='post', maxlen=65)# prefetch(1024)는 GPU에 미리 1024개의 데이터를 미리 fetch하는 기능!# 근데 batch size도 아니고 왜 1024개??train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024) 학습 환경 정의모델 생성, 손실 함수, 최적화 알고리즘, 평가지표 정의1234567891011# 모델 생성model = Seq2seq(sos=tokenizer.word_index['\\t'], eos=tokenizer.word_index['\\n'])# 손실함수 및 최적화 기법 정의loss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()# 성능 지표 정의train_loss = tf.keras.metrics.Mean(name='train_loss')train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy') 학습 루프 동작12345678for epoch in range(EPOCHS): for seqs, labels in train_ds: train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy) template='Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy:&#123;&#125;' print(template.format(epoch + 1, train_loss.result(), train_accuracy.result() * 100)) 테스트 루프1234567891011for test_seq, test_labels in test_ds: prediction = test_step(model, test_seq) test_text = tokenizer.sequences_to_texts(test_seq.numpy()) # ground_truth gt_text = tokenizer.sequences_to_texts(test_labels.numpy()) # prediction texts = tokenizer.sequences_to_texts(prediction.numpy()) print('_') print('q: ', test_text) print('a: ', gt_text) print('p: ', texts) 예측된 값들을 보면 train data에 과적합된 것을 충분히 알 수 있을 것이다. 이제 여기서 Attention mechanism을 적용시켜보자.Encoder, Decoder, Seq2seq 부분을 수정하면된다.Encoder 이전과 다르게 LSTM 구조에서 return_sequences=True를 넣어 전체 Hidden State를 출력하게 해주었다. 이를 Key-Value로 사용할 것이다. Embedding 결과의 Dimension : (32(batch_szie), 64(sequence의 길이), 64(Embedding Feature의 수)) LSTM의 결과의 Dimension : (32(batch_szie), 64(sequence의 길이), 512(LSTM unit의 갯수)) H : 32(batch size) 64(sequence의 길이) 512(LSTM unit수) h(s0) : 32(batch size) * 512 (LSTM unit수) c(c0) : 32(batch size) * 512 (LSTM unit수)12345678910111213141516class Encoder(tf.keras.Model): def __init__(self): super(Encoder, self).__init__() # 2000개의 단어들을 64크기의 vector로 Embedding해줌. self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64) # return_state는 return하는 Output에 최근의 state를 더해주느냐에 대한 옵션 # 즉, Hidden state와 Cell state를 출력해주기 위한 옵션이라고 볼 수 있다. # default는 False이므로 주의하자! # return_sequence=True로하는 이유는 Attention mechanism을 사용할 때 우리가 key와 value는 # Encoder에서 나오는 Hidden state 부분을 사용했어야 했다. 그러므로 모든 Hidden State를 사용하기 위해 바꿔준다. self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True) def call(self, x, training=False, mask=None): x = self.emb(x) H, h, c = self.lstm(x) return H, h, c Decoder LSTM 다음에 Attention 구조를 넣어주고, Encoder의 출력 중 모든 sequence의 Hidden State를 모아놓은 H와 s0, c0, shifted Output을 받아서 Attention value를 구하기 위한 코드를 수정시킨다. Dimension : x : shifted_labels로 맨마지막을 제외한 나머지데이터들 =&gt; 32(batch szie) * 64(sequence의 길이) s0 : 이전 step의 hidden state =&gt; 32(batch size) * 512(LSTM의 Unit 갯수) c0 : 이전 step의 cell state =&gt; 32(batch size) * 512(LSTM의 Unit 갯수) H : Encoder단의 모든 Hidden state를 모은 것 =&gt; 32(batch size) 64(sequence의 길이) 512(LSTM의 Feature의 갯수) embedding 결과 =&gt; 32(batch size) 64(sequence의 길이) 64(Embedding Feature의 수) LSTM의 결과의 Dimension : (32(batch_szie), 64(sequence의 길이), 512(LSTM unit의 갯수)) S : 32(batch size) 64(sequence의 길이) 512(LSTM unit수) h : 32(batch size) * 512 (LSTM unit수) c : 32(batch size) * 512 (LSTM unit수) S_의 Dimension: 32(batch size) 64(sequence의 길이) 512(LSTM unit 수) A의 Dimension: 32(batch size) 64(sequence의 길이) 512(LSTM unit 수) y의 Dimension: 32(batch size) 64(sequence의 길이) 1024 1234567891011121314151617181920212223242526272829303132333435class Decoder(tf.keras.Model): def __init__(self): super(Decoder, self).__init__() self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64) # return_sequence는 return 할 Output을 full sequence 또는 Sequence의 마지막에서 출력할지를 결정하는 옵션 # False는 마지막에만 출력, True는 모든 곳에서의 출력 self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True) # LSTM 출력에다가 Attention value를 dense에 넘겨주는 것이 Attention mechanism이므로 self.att = tf.keras.layers.Attention() self.dense = tf.keras.layers.Dense(NUM_WORDS, activation='softmax') def call(self, inputs, training=False, mask=None): # x : shifted output, s0 : Decoder단의 처음들어오는 Hidden state # c0 : Decoder단의 처음들어오는 cell state H: Encoder단의 Hidden state(Key와 value로 사용) x, s0, c0, H = inputs x = self.emb(x) # initial_state는 셀의 첫 번째 호출로 전달 될 초기 상태 텐서 목록을 의미 # 이전의 Encoder에서 만들어진 Hidden state와 Cell state를 입력으로 받아야 하므로 # S : Hidden state를 전부다 모아놓은 것이 될 것이다.(Query로 사용) S, h, c = self.lstm(x, initial_state=[s0, c0]) # Query로 사용할 때는 하나 앞선 시점을 사용해줘야 하므로 # s0가 제일 앞에 입력으로 들어가는데 현재 Encoder 부분에서의 출력이 batch 크기에 따라서 length가 현재 1이기 때문에 2차원형태로 들어오게 된다. # 그러므로 이제 3차원 형태로 확장해 주기 위해서 newaxis를 넣어준다. # 또한 decoder의 S(Hidden state) 중에 마지막은 예측할 다음이 없으므로 배제해준다. S_ = tf.concat([s0[:, tf.newaxis, :], S[:, :-1, :]], axis=1) # Attention 적용 # 아래 []안에는 원래 Query, Key와 value 순으로 입력해야하는데 아래처럼 두가지만 입력한다면 # 마지막 것을 Key와 value로 사용한다. A = self.att([S_, H]) y = tf.concat([S, A], axis=-1) return self.dense(y), h, c Seq2seq 이전의 코드에서 encoder의 출력에 전체 Hidden State를 모아놓은 것과 decoder의 입력으로 이값을 받는 코드를 추가해주었다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253class Seq2seq(tf.keras.Model): def __init__(self, sos, eos): super(Seq2seq, self).__init__() self.enc = Encoder() self.dec = Decoder() self.sos = sos self.eos = eos def call(self, inputs, training=False, mask=None): if training is True: # 학습을 하기 위해서는 우리가 입력과 출력 두가지를 다 알고 있어야 한다. # 출력이 필요한 이유는 Decoder단의 입력으로 shited_ouput을 넣어주게 되어있기 때문이다. x, y = inputs # LSTM으로 구현되었기 때문에 Hidden State와 Cell State를 출력으로 내준다. H, h, c = self.enc(x) # Hidden state와 cell state, shifted output을 초기값으로 입력 받고 # 출력으로 나오는 y는 Decoder의 결과이기 때문에 전체 문장이 될 것이다. y, _, _ = self.dec((y, h, c, H)) return y else: x = inputs H, h, c = self.enc(x) # Decoder 단에 제일 먼저 sos를 넣어주게끔 tensor화시키고 y = tf.convert_to_tensor(self.sos) # shape을 맞춰주기 위한 작업이다. y = tf.reshape(y, (1, 1)) # 최대 64길이 까지 출력으로 받을 것이다. seq = tf.TensorArray(tf.int32, 64) # tf.keras.Model에 의해서 call 함수는 auto graph모델로 변환이 되게 되는데, # 이때, tf.range를 사용해 for문이나 while문을 작성시 내부적으로 tf 함수로 되어있다면 # 그 for문과 while문이 굉장히 효율적으로 된다. for idx in tf.range(64): y, h, c = self.dec([y, h, c, H]) # 아래 두가지 작업은 test data를 예측하므로 처음 예측한값을 다시 다음 step의 입력으로 넣어주어야하기에 해야하는 작업이다. # 위의 출력으로 나온 y는 softmax를 지나서 나온 값이므로 # 가장 높은 값의 index값을 tf.int32로 형변환해주고 # 위에서 만들어 놓았던 TensorArray에 idx에 y를 추가해준다. y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32) # 위의 값을 그대로 넣어주게 되면 Dimension이 하나밖에 없어서 # 실제로 네트워크를 사용할 때 Batch를 고려해서 사용해야 하기 때문에 (1,1)으로 설정해 준다. y = tf.reshape(y, (1, 1)) seq = seq.write(idx, y) if y == self.eos: break # stack은 그동안 TensorArray로 받은 값을 쌓아주는 작업을 한다. return tf.reshape(seq.stack(), (1, 64))","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"Attention 기법","slug":"deep_learning_10","date":"2020-01-21T08:10:39.000Z","updated":"2020-02-03T15:46:55.627Z","comments":true,"path":"2020/01/21/deep_learning_10/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/21/deep_learning_10/","excerpt":"","text":"Attention 기법Sequence-to-sequence 우선, Attention 기법이 가장 먼저 적용되었던 모델인 Sequence-to-sequence 모델을 살펴보면서 간략하게 conception적인 것을 살펴보겠다. 아래 그림의 왼쪽 부분은 Encoder 구조로 되어 있어, Input에 번역하고자 하는 문장을 단어 하나씩 받는 형태로 되어있다. 마지막 입력 단어에는 EOS(End-Of-Sequence)라는 특별한 단어(Token)를 받도록 되어있다. 마지막 EOS까지 받은 뒤의 Output을 Context라고 한다. 그렇게 나온 Context를 Decoder 구조에서 넘겨 받으며, 동시에 SOS(Start-Of-Sequence)라는 Token을 처음 Input에서 입력해주면 넘겨 받은 Context로 부터 첫번째 단어를 생성한다. 생성된 단어를 다음 입력으로 넣어주고 또 다음 단어를 생성하고 이런 작업을 EOS Token이 Output으\u001d로 나올때까지 반복해준다. Encoder는 결국 Context를 만들기 위해서 RNN으로 동작하는 구조이고, Decoder는 한단어씩 출력하는\u001c데, 그 출력된 Output(단어)을 오른쪽으로 Shift해서 입력으로 받은 뒤 새로운 단어를 만들어주는 구조이다. 영어 문장의 데이터화 한글 문장의 데이터화 Seq2seq 모델같은 경우는 입출력간의 step이 너무 멀리 떨어져 있으면 Gradient Vanishing이 일어나 잘 학습되지 않는다. 예를 들어 아래와 같은 그림에서 처음 입력인 $x_{0}$가 $y_{1}$에 영향을 준다면(영어에서 한글로 번역할시 어순이 반대로 되어있는 경우를 예시로 생각하면 이해하기 쉽다.), BPTT로 펼쳐놓고 봤을때, 기본적으로 Encoder단과 Decoder단이 분리가 되어 있기 때문에, Ecoder단의 앞쪽과 Decoder 뒷쪽과은 거리가 멀다. 그렇다면 아무리 LSTM을 쓰고 GRU를 사용한다\b고 하더라도 한계가 있다. 위에서 언급했던 것과 같이 그렇다면 Gradient Vanishing 문제를 해결할 수 있는 방법이 없을까? 그에 대한 답을 아래의 그림을 통해 설명하겠다. 이전에는 Encoder 부분에서 Context 하나(Feature vector 하나)만 넘겨줘서 그 Context를 통해 Decoder가 출력을 내주었어야 했는데, 직관적으로 볼때 Input들을 통해 만든 마지막 Hidden state인 Feature vector가 Encoder단의 모든 Input들을 많은 부분 커버할 수 없으며, Decoder 부분의 RNN을 통해 지나면서 우리가 원하는 방향대로 넘어갈것이라는 보장 또한 없다. 그래서 이제는 Encoder Hidden state를 모아서 Decoder로 각각 전달시켜 줌으로써 각 출력에 필요한 Context를 새로이 뽑을 수 있는 구조로 변경해주어 기울기 소실 문제를 해결할 수 있을 것이다. 그렇다해서 모든 Encoder단의 모든 Hidden state를 다 concate해서 하나의 긴 Feature vector로 넘겨준다면 좋은 성능을 기대하긴 어렵울 것이다. 왜냐하면 무조건 Feature vector의 데이터량을 늘려 준다면, 그만큼 많은 데이터셋을 통해 데이터셋이 cover를 해주어야 하기 때문이다. (자세히 말하면 학습데이터량은 고정적으로 동일한데, Input vector인 Feature vector의 차원만 늘려주게 된다면 Underfitting 문제가 발생하면서 Sparsity 문제가 발생해서 학습이 제대로 동작하지 않기 때문이다.) 이런 구조를 어떻게 효율적으로 구성할 수 있을까에 대한 답이 Attention 메커니즘이다. Attention 메커니즘에서는 예를 들어 Encoder 부분과 Decoder 부분의 Layer들이 아래 그림에서 처럼 색깔별로 연관되어 있다고 했을때, 각 연관되어 있는 부분을 알게 해주는 것이 Attention Mechanism의 기본 아이디어이다. Attention 신경망 Dictionary형태로 이루어져 있다는 점은 순서는 상관 없다는 의미임을 명심하자! 질의-응답이 이루어지는 메커니즘을 살펴보면, query를 날려주게 되면, 우선 key들을 나열하여 동일한 것을 찾아낸다. 그 후에는 key-value쌍에서 동일한 부분의 value를 출력해준다. Attention mechanism은 \bkey-value 쌍이 있고, query를 날려서 query와 key를 유사한지 비교를 해준뒤, 유사도를 고려한 Value들을 섞어서 Aggregation(합성)을 해준것이 Attention value이다. 대부\b분의 Attention network에서는 key와 value를 같은 값을 사용한다. Seq2seq에서는 Encoder의 Hidden Layer들을 key와 value로 사용한다. 직관적으로 생각을 해보면, Decoder에서 어떤것을 찾고자 한다면, 찾고자 하는 것에 대한 정보는 Encoder에서 찾을 수 있을 것이다. 그렇기에 Key-value가 Encoder의 Hidden Layer가 되는 것이다. 또한, Seq2seq에서는 Decoder의 Hidden Layer들을 Query로 사용한다. 주의할 점은, Encoder와 달리 하나 앞선 time-step의 Hidden Layer를 사용한다는 점이다. 왜냐하면 현재의 출력을 내기 위해서는 현재의 출력이 사용될 수는 없기 때문에 즉, 미래(예측)를 가지고 현재 출력을 만들어낼 수는 없기 때문에 하나 앞선 Hidden Layer를 query로 사용하는 것이다. Key와 \bValue는 서로 동일한 Encoder의 Hidden Layer들이 사용되며, Query는 Decoder에 있는 각각의 Hidden Layer들이 될 것이며, i-번째 time step에 대한 Query를 날려서 Encoder에 있는 모든 Key와 유사도를 비교해서 최종적으로는 유사도를 고려한 Aggregation(종합)한 Attention Value를 출력해 준다. 이렇게 Attention Value($a_{0}$)를 입력으로 받아 $s_{0}$ Hidden Layer에서 LSTM 구조를 거쳐 $s_{1}$ Hidden Layer가 나올 것이다. 이 새롭게 얻어진 $s_{1}$ Hidden Layer에다가 $s_{0}$를 통해 얻어진 Attention Value($a_{0}$) Concatenate를 해서 출력을 내준다.($v_{1}$) 이전에는 Decoder에서 그대로 Hidden Layer가 나오던 것이 이제는 Encoder의 Hidden Layer들을 비교해서 만들어낸 Attention Value를 같이 출력으로 냄으로써, Encoder 부분\u001c의 value들을 잘 가져올 수 있도록 해주었다. Attention is all you need - Transformer 모델 RNN 같은 경우는 입력을 순서대로 넣어주기 때문에 입력된 단어의 위치를 따로 표시하지 않아도 되지만, Transformer 구조 같은 경우에는 병렬적으로 계산을 하기 때문에, 현재 계산하고 있는 단어가 어느 위치에 있는 단어인지를 표현을 해주어야 해서 positional encoding을 사용한다. 우선, Transformer와 Seq2seq 모델을 비교하자면, Seq2seq 모델은 Encoder와 Decoder가 있고 그 사이에 Context가 전달되는 구조를 가지고 있다. Transformer 모델의 경우는 Input쪽(왼쪽의 빨간색 박스)과 Output쪽(왼쪽의 파란색 박스)로 구성되며, Input쪽에서는 Input embedding이 들어가서 Encoding이 일어나고 Context가 전달이 되어 Output쪽의 Decoder부분에서 Decoding이 되서 출력이 나오게 된다. 전체적으로 구조는 비슷해보이지만, Seq2seq 모델은 RNN로 구성되어 있어서 순차적으로 이루어지게 되어있고, Transformer 모델은 병렬적으로 계산되므로 Input쪽이 동시에 계산되고 Output쪽이 동시에 계\u001c산되는 형태로 학습이되는 점이 차이점이다. Input을 먼저 보면, 아래의 노란색 Matrix 형태로 되어 있으며, 일반적으로는 입력 단어의 가짓수와 출력 단어의 가짓수는 동일할 것이다. 만약 기계번역처럼 2개의 언어가 다르다면, 다를 것이다! Output은 원래 Seq2seq의 구조에서 보았듯이 shift 시킨 입력을 넣어주었었던 것과 같이 SOS를 넣어주고 EOS를 빼준 형태로 Outputs에 넣어준다. One-hot encoding으로 되어있던 것들을 Embedding해서 각각의 Embedding에 넣어준다. Positional Encoding은 시간적으로 위치가 따르때마다 고유의 코드를 생성해서 Input Embedding에 더해주는 형태로 구성되어있다. 이렇게 해줌으로써, 전체 Sequence의 길이 중 상대적 위치에 따라서 고유의 벡터를 생성하여 Embedding된 벡터에 더해\u001c주게 된다. 예를들면, 보통 Embedding된 벡터들은 0를 기준으로 분포가되어있는데 Input Embedding에 sin과 cosine을 조합해서 만들어진 feature 벡터를 더해주는 것이라고 보면된다. 위에서 말했듯, Embedding들은 0을 기준으로 분포하므로 여기에 sin과 cosine을 조합해 만든 Feature 벡터를 더해준다해\b도 크게 손상이 가지 않기 때문에 걱정하지 않아도된다. Dot-Product는 우리가 알고있는 내적과 동일하다. 그리고 Mask를 이용해서 Illegal connection의 Attention을 금\b지한다는 의미는 self-attention에 대한 이야기인데, 일반적인 Attention 구조는 Decoder쪽에 Hidden Layer를 통해 Output을 내려면 Encoder 쪽에 Hidden Layer 전체와 비교해서 산출을해야하므로 이런 경우는 괜찮지만, Self-attention에서는 Decoder를 똑같은 Decoder 자기 자신과 Attention을 할 수가 있는데 여기서 Decoder 부분의 해당 Hidden Layer를 산출하려면 순차적으로 출력이 나온다고 했을때 해당 부분의 Decoder보다 이후 시점은 아직 결과가 산출되지 않았기 때문에 그보다 앞선 시점의 Decoder부분에서의 Hidden Layer들만을 사용할 수 있다는 이야기이다. 여기서 비교할 때 사용할 수 없는 Hidden Layer들을 Illegal connection이라고 한다. 이런 Illegal connection은 Mask를 통해 -inf로 보내버리면 Softmax에서 값이 0이되는 것을 이용하여 attention이 안되도록 구현하고 있다. Multi-Head Attention은 쉽게 말해 Scaled Dot-Product Attention을 h개를 모아서 병렬적으로 연산을 할 수 있게끔하는 것이다. h개를 사용함으로써 같은 입력에 대해서 더 풍부한 출력이 나타날 수 있다. 또한, 여기서 처음 Linear 연산(Matrix Mult)을 하는 것은 Query, Key, Value 각각 중 특정 차원만을 보겠다는 이야기이며, 차원을 줄여주어 병렬에 유리한 구조를 만드는 역할도 있다. 그러므로 이 연산을 한 후에 h가지로 병렬처리함으로써 풍부한 출력을 얻을 수 있는 것을 이해할 수 있다. 제일 아래 단계의 Linear연산을 통해Q,K,V의 차원을 감소(h개로 나눠짐)시킨다는 것이 중요하다. 또한 아래 수식에서 가중치 $W_{V,i}, W_{K,i}, W_{Q,i}$의 각각의 Dimension보다 더 작은 값으로 모델의 Dimension($d_{model}$)을 해준다. 이는 value, key, query의 차원을 모델에 사용하는 차원으로 차원을 변환시켜주는 의미이기도 하다. Mask는 RNN의 Decoder단을 생각해보았을때, context가 앞에서 뒤로 넘어가면서 이미 구한 것들만 참조를 할 수 있는데, Transformer구조에서는 병렬적으로 계산을 하기 때문에 self-attention을 할 경우에는 시간적으로 앞에\u001c서 일어난 것들에 대해서만 영향을 받게해주어야 RNN과 동일한 구조가 되기 때문\u001d에 Mask를 이용해서 예측하고자 하는 시점을 포함한 미래값들을 가려준다. Multi-Head Attention이 Transformer에 어떻게 적용되어 있는지 살펴보자. Self-Attention은 Decoder와 동일한 Decoder를 참조하므로 Key와 Query와 Value는 모두 같은 것이다. Encoding 같은 경우에는 causual system일 필요가 딱히 없기 때문에 Mask 없이 Key, Value, Query가 그대로 사용될 수 있지만, Decoder 부분같은 경우에는 현\b재 Query하려고 하는 것이 Key와 value가 Query보다 더 앞서서 나올수 없기 때문에 Mask를 활용한 Masked Multi-Head Attention을 사용한다. 이렇게 Encoder단의 Self-Attention을 통해서 Attention이 강조되어 있는 Feature들을 추출을 해주고 Decoder단에서는 Output Embedding(or 이전의 출력값)이 들어왔을 때 이것을 통해 Masked Multi-Head Attention을 통해 Feature 추출을 해주고, 그 다음에 붉은 색 박스 부분에서는 이 Decoder를 통해 추출된 Feature가 Query로 들어가고, 나머지 Key, Value는 Encoder를 통해 만들어진 출력을 가지고 입력을 받게된다. 결국에는 이런 구조는 Seq2seq 모델의 Attention과 동일한 구조가 되게 될 것이다. 실제로는 Multi-Head Attention이 병렬적으로 계산됨으로써 self-attention이 RNN을 대체해서 들어간다고 볼 수 있다. Position-wise Feed-Forward는 특별한 것은 아니고 앞서서 말했던 것과 같이 아래 초록색 박스는 가로가 문장의 길이, 세로가 One-hot vector를 크기로 갖는 행렬인데 병렬적을 처리되는 input 단어 하나마다 동일한 구조의 activation이 ReLu인 FC Layer층을 공유해서 사용하여 출력을 내보낸다는 의미이다. 이를 통해 병렬적으로 계산하지만 기존의 FeedForward propagation을 구현할 수 있다. Feed-Forward가 일어난 다음이나 Self-Attention이 일어난 다음에는 이전의 것(Skip connection)을 가져와서 더 해준 뒤 Layer Normalization을 수행해서 사용하고 있다. Layer Normalization은 Batch의 영향을 받지 않는 Normalization이라고 생각하면된다. 마지막 Feed-Forward에 의해서 마지막 Feature 출력이 나오게 되면 Linear 연산(Matrix Mult)을 사용해서 출력 단어 종류의 수에 맞추게 One-hot vector로 만들어준다. 그런 다음 Softmax를 이용해서 어떤 단어인지 classification을 한다. Attention 신경망의 수식적 이해 Attention mechanism은 Key와 Query를 비교하는 Comparison을 통해 그에 따른 유사도를 가중치처럼 사용하여 Key에 맞는 value들의 조합으로 Aggregation(가중합)을 통하여 Attention value를 만들어 주는 구조가 Attention mechanism이었다. 그러므로 결국 Query와 비슷하면 비슷할 수록 높은 가중치를 주어 출력을 주는 것이다. compare 함수로는 Dot-Product가 많이 쓰이며, 저기서 k와 q가 각각 벡터의 norm이 1이라면 결국 코사인 유사도를 구하는 것과 동일해 질 것이다. 그러나 길이가 1이 아닐 경우를 생각해서 Dot-product이후에 softmax를 사용하여 전체의 합을 1로 만들어 주게끔하여 각각의 가중치들을 하나의 확률로 사용할 수 있게끔 변환해 주어 사용한다. Seq2seq 모델은 Encoder구조를 통해 Feature들을 만들게 되고 최종적으로는 출력으로 Context를 생성하\u001c여 이 Context 하나에 의지해서 Decoder는 SOS(Start Of Sequence)를 시작으로 출력으로는 단어를 하나씩 내어주는 모델이다. Key-value쌍은 기존의 Context만을 보며 출력을 내주었던 것과 다르게 Decoder부분의 Hidden Layer에 대한 출력을 낼때, Encoder 부분에 중간중간 부분을 알게하기 위해서 사용되어진다. Query는 Decoder의 Hidden Layer들을 사용하는데, 해당 출력을 해야하는 RNN구조의 하나 이전의 time-step의 Hidden Layer를 Query로 사용한다는 점을 기억하자! 아래 그림에서는 $s_{0}, s_{1}, s_{2}$가 해당한다. i-th query가 들어오게되면 각각 Key와 비교를 하게되고, 앞에서 말한것과 같이 내적한 뒤 Softmax를 해줘 가중치로 만든뒤에 각각에 해당하는 Value와 곱해 가중합을 한 것을 Attention value로 산출한다. 아래 그림에서 출력과 RNN 구조사이에 실제로는 FC Layer가 하나 존재해서 출력을 One-hot vector로 만들어준다. $X\\in R^{BXLXN}$에서 B:Batch size, L:문장의 길이, \bN:One-hot vetor나 embedding Feature의 길이를 의미하며, 여기서 Decode 쪽으로 Context를 넘길때는 LSTM이라면 Hidden State와 Cell State 둘다 넘겨주어야 하기에 Batch_size X Hidden state의 Feature 갯수인 M을 사이즈로 갖는 tensor를 넘겨줄 것이다. Encoder의 Hidden State인 H가 Attention mechanism에 Key와 Value로 입력이 되고, Query에는 Decoder의 한 step 앞선 Hidden State를 사용하게 된다. 만약에 Input의 언어와 Output의 언어가 다르다면, 단어의 가짓수나 길이가 달라질 수있다는 점을 주의하자! sin법칙과 cosine법칙에 의해 각각 분리해서 쓸수 있는데 결국 덧셈과 뺄셈으로 이 Positional Encoding이 달라지기 때문\b에 FC Layer에서 학습하는데 용이하게 될 것이다. 전체적인 구조는 Attention mechanism을 적용한 Seq2seq 모델과 유사하지만 Scale이 되는 부분과 Mask를 사용하는 부분\b이 다르다. 또한, 가장 중요한 점은 아래 수식 중 Query와 key, value 부분을 모아서 하나의 metrics로 만들어 줌으로써 우리가 처음 배웠던 shallow NN과 같이 병렬적으로 처리할 수 있게끔 해주었다는 것이 가장 큰 Transformer 모델의 요소일 것이다. scale 처리를 해줌으로써 내적의 값이 너무 커져서 saturation되서 Softmax값이 차이가 많이나는 것을 방지할 수 있다. 제일 아래 단계의 Linear연산을 통해Q,K,V의 차원을 감소(h개로 나눠짐)시킨다는 것이 중요하다. 또한 아래 수식에서 가중치 $W_{V,i}, W_{K,i}, W_{Q,i}$의 각각의 Dimension보다 더 작은 값으로 모델의 Dimension($d_{model}$)을 해준다. 이는 value, key, query의 차원을 모델에 사용하는 차원으로 차원을 변환시켜주는 의미이기도 하다.","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"순환신경망(Vanilla RNN 및 LSTM 구현)","slug":"deep_learning_09","date":"2020-01-20T20:39:33.000Z","updated":"2020-01-21T08:10:51.058Z","comments":true,"path":"2020/01/21/deep_learning_09/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/21/deep_learning_09/","excerpt":"","text":"순환 신경망 구현 및 학습Vanilla RNN1!pip install tensorflow==2.0.0-beta1 1import tensorflow as tf tensorflow version 확인1print(tf.__version__) 하이퍼 파라미터 설정1234EPOCHS = 10# 우리가 분석할 때 10000개의 단어만 사용하겠다는 의미로 설정하였다.NUM_WORDS = 10000 모델정의123456789101112class MyModel(tf.keras.Model): def __init__(self): super(MyModel, self).__init__() # input_dim, output_dim self.emb = tf.keras.layers.Embedding(NUM_WORDS, 16) self.rnn = tf.keras.layers.SimpleRNN(32) self.dense = tf.keras.layers.Dense(2, activation='softmax') def call(self, x, training=None, mask=None): x = self.emb(x) x = self.rnn(x) return self.dense(x) 학습, 테스트 루프 정의1234567891011121314151617181920# Implement training loop@tf.functiondef train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy): with tf.GradientTape() as tape: predictions = model(inputs, training=True) loss = loss_object(labels, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train_loss(loss) train_accuracy(labels, predictions)# Implement algorithm test@tf.functiondef test_step(model, images, labels, loss_object, test_loss, test_accuracy): predictions = model(images, training=False) t_loss = loss_object(labels, predictions) test_loss(t_loss) test_accuracy(labels, predictions) 데이터셋 준비IMDB review를 보고 긍정인지 부정인지를 예측하는 문제이며, y(target value)는 binary value(0 or 1)를 가지지만 x_data(feature)에서 각각의 review의 길이가 다르므로 입력에서 출력이 나오는 기준을 맞추기 위해 zero-padding을 해주는 작업을 실행할 것이다. 아래의 ‘pad_sequence’함수에서 maxlen=32는 최대 길이를 32글자로 맞추겠다는 의미이다. maxlen=32로 함으로써 원래 본 데이터의 맨 뒤부분에서 시작해서 32번째 데이터 까지를 잘라서 사용하는 것이며, 이 부분에 데이터가 없을 시 0으로 padding 처리를 해주는 함수이다. 1234567891011121314151617imdb = tf.keras.datasets.imdb(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=NUM_WORDS)# padding='post' 뒤쪽으로 padding해준다.x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, value=0, padding='pre', maxlen=32)x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, value=0, padding='pre', maxlen=32)train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) 학습 환경 정의모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의12345678910111213# 모델 생성model = MyModel()# 손실함수 및 최적화 기법 정의loss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()# 성능 지표 정의train_loss = tf.keras.metrics.Mean(name=\"train_loss\")train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")test_loss = tf.keras.metrics.Mean(name=\"test_loss\")test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\") 학습 루프 동작12345678910111213for epoch in range(EPOCHS): for seqs, labels, in train_ds: train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy) for test_seqs, test_labels in test_ds: test_step(model, test_seqs, test_labels, loss_object, test_loss, test_accuracy) template = \"Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;\" print(template.format(epoch + 1, train_loss.result(), train_accuracy.result() * 100, test_loss.result(), test_accuracy.result() * 100)) LSTM keras는 고수준 API이므로 이미 내부에 구현이 되어있어 다음과 같이 변경해주는 것만으로 LSTM을 구현 할 수 있다. 123456789101112131415class MyModel(tf.keras.Model): def __init__(self): super(MyModel, self).__init__() # input_dim, output_dim self.emb = tf.keras.layers.Embedding(NUM_WORDS, 16) # tf.keras.layers.GRU(32)도 가능 # 참고로 RNN은 층을 쌓을수록 성능이 안좋아질 가능성이 높다는 점을 주의하자! self.rnn = tf.keras.layers.LSTM(32) self.dense = tf.keras.layers.Dense(2, activation='softmax') def call(self, x, training=None, mask=None): x = self.emb(x) x = self.rnn(x) return self.dense(x)","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"NLP 전처리","slug":"NLP_02","date":"2020-01-19T06:29:26.000Z","updated":"2020-02-03T10:53:23.888Z","comments":true,"path":"2020/01/19/NLP_02/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/19/NLP_02/","excerpt":"","text":"형태소 Tokenizing 라이브러리영어 Tokenizing 라이브러리 1) NLTK 파이썬에서 영어 텍스트 전처리 작업을 하는데 많 쓰이는 라이브러리로, 이 라이브러리는 50여 개가 넘는 말뭉치 리소를 활용해 영어 텍스트를 분석할 수 있게 제공 한다. 직관적으로 함수를 쉽게 사용할 수 있게 구성돼 있어 빠르게 텍스트 전처리를 할 수 있다. 또한 단어 단위 토크나이징과 문장 단위 토크나이징을 하는 모듈이 따로 있으며 ‘a’, ‘the’ 같은 관사나 ‘is’와 같이 자주 의미는 별로 없지만 자주 등장하는 단어인 불용어들을 모아 불용어 사전을 구성하고 있어 따로 불용어를 정의할 필요없이 바로 사용가능하다. 2) Spacy NLTK와 같은 오픈소스 라이브러리이다. 주로 상업용 목적으로 만들어졌다는 점이 NLTK와 다르며, 영어를 포함한 8개 언어에 대한 자연어 전처리 모듈을 제공하고, 빠른 속도로 전처리할 수 있다. 원하는 언어에 대한 전처리를 한 번에 해결할 수 있다는 장점이 있으며, 특히 딥러닝 언어 모델의 개발도 지원하고 있어 매력적이다. NLTK와 다르게 단어 단위, 문장 단위 토크나이징을 한가지 모듈을 통해 처리한다. 이러한 영어 토크나이징 도구는 한국어에 적용할 수 없다!! 한글 토크나이징 라이브러리 자연어 처리에서 각 언어마다 모두 특징이 다르기 때문에 천편일률적으로 동일한 방법을 적용하기는 어렵다. 한국어 자연어 처리에 많이 사용되는 파이썬 라이브러리 KoNLPy를 소개하겠다. 1) KoNLPy(지도학습 기법으로 학습) 한글 자연어 처리를 쉽고 간결하게 처리할 수 있도록 만들어진 오픈소스 라이브러리이다. 또한 국내에 이미 만드어져 사용되고 있는 여러 형태소 분석기를 사용할 수 있게 허용한다. 형태소 분석으로 형태소 단위의 토크나이징을 가능하게 할뿐만 아니라 구문 분석을 가능하게 해서 언어 분석을 하는 데 유용한 도구다. 한글 텍스트의 경우에는 형태소 단위 토크나이징이 필요할 때가 있다. KoNLPy에서는 여러 형태소 분석기를 제공하며, 각 형태소 분석기별로 분석한 결과가 다르므로 자신의 분석 데이터에 맞는 형태소 분석기를 선택해서 사용할 것을 권한다. Mecab의 경우 원도우에서는 사용할 수 없으니 참고해서 사용하자. Hannanum (한나눔) Kkma (꼬꼬마) Komoran (코모란) Mecab (메케브) Okt(Twitter) KAIST에서 개발된 한나눔은 다음과 같은 메소드를 제공한다. 서울대학교에서 개발된 한나눔은 다음과 같은 메소드를 제공한다. 코모란은 다음과 같은 메소드를 제공한다. mecab은 은전한닢이란 의미를 지니고 있으며(TMI인듯), 빠르고 성능이 우수한 것으로 알려져 있다. 다음과 같은 메소드를 제공한다. Okt는 구 Twitter로 불리며, Twitter에서 만들었다. 다음과 같은 메소들르 제공한다. macOS에서 설치123456789# JPype1은 파이썬에서 자바 클래스를 사용할 수 있도록 만들어주는 라이브러리이다.# 만약 window라면 https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype에서 맞는 사양을 설치# 64bit-python3.6 버전이라면 JPype1-0.63-cp36-cp36m-win_amd64.whl 을 설치하면된다.# pip install JPype1-0.63-cp36-cp36m-win_amd64.whl# MacOs에선conda install -c conda-forge jpype1pip install konlpy 형태소 분석기 사용법 각각의 형태소 분석기는 클래스생성만 다르고 나머지는 동일한 함수를 사용하므로 아래 예시에서는 형태소 분석기 중 제일 속도가 빠르다고 알려져 있는 Mecab을 사용할 것이다. pos를 통해 얻는 품사의 태깅의 의미를 알고 싶다면 클릭 tokenizer.morphs() 텍스트를 형태소 단위로 나눈다. 옵션으로는 norm과 stem이있다. 각각 True 혹은 False 값을 받으며, norm은 normalize의 약자로서 문장을 정규화하는 역힐을 하고, stem은 각 단어에서 어간을 추출하는 기능(예시: 해야지 -&gt; 하다)이다. 각각 True로 설정하면 각 기능이 적용 된다. 둘 다 default는 False이다. tokenizer.nouns() 텍스트에서 명사만 뽑아낸다. tokenizer.phrases() 텍스트에서 어절을 뽑아낸다. tokenizer.pos() 위의 세 함수는 추출기인 반면에, pos 함수는 태깅함수이다. 각 품사를 태깅하는 역할을 한다. norm, stem 옵션이 존재하며 join=True로 하게 되면 (형태소, 품사)의 형태에서 형태소/품사 형태로 붙여서 리스트화한다. 어떤 형태소 분석기를 사용할지는 자신이 가진 데이터로 실험 삼아 형태소 분석을 해보고 속도나 품질을 비교해서 고르는 것이 좋다. 자신의 분석에서 사전에 추가해야할 단어들이 있다면 사용자 사전에 추가해 주면 된다. 12345678910111213141516from konlpy.tag import Mecabtokenizer = Mecab()# 형태소 단위로 나누기tokenizer.morphs(\"아버지가방에들어가신다.\")# 결과[\"아버지\", \"가\", \"방\", \"에\", \"들어가\", \"신다\"]# 품사 태그tokenizer.pos('아버지가방에들어가신다.')``` ##### 결과``` bash[('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('신다', 'EP+EC')] Mecab에 사용자 사전 추가하기 Komoran은 추가할 내용의 txt를 만들어 객체 생성시 userdic 파라미터에 만든 path를 입력해 주면되고, Hannanum은 konlpy/java/data/kE/dic_user.txt에 존재하며 여기에 새로운 단어를 형식에 맞춰 추가해주면된다. Kkma는 ~/anaconda3/lib/python3.6/site-packages/konlpy/java/kkma-2.0.jar 압축파일 내의 .dic 형식의 파일들이 dictionary 파일이므로 압축을 푼뒤 해당 품사에 맞는 파일에 단어를 추가해주면된다. 그리고나서 다시 .jar로 압축을 해주고 원본은 만약을 위해 다른 곳에 보관하며 사용한다. 형태소 분석기를 사용하다 보면 가장 신경 써야 하는 점이 중요 token들을 어떻게 처리해야 할지다. 예를들면 우리가 ‘천리마전자’라는 기업의 데이터 분석 팀에 속해 있고 천리마저나에 관한 Corpus를 분석하거나 이로부터 임베딩을 만들어야 한다고 가정해보자. 이 경우 ‘천리마전자’라는 token은 섬세하게 처리해야한다. 만약 ‘천리마전자 텔레비전 정말 좋네요’라는 가상의 리뷰를 분석한다면 천리마 전자 보다 천리마전자로 분석됐을 때 임베딩 품질이 더 좋을 것이다. 이럴 경우 사용자 사전에 추가하여 하나의 토큰으로 분석될 수 있도록 강제해야한다. 2) Khaiii 사용법 (지도학습기법으로 학습) reference Khaiii(Kakao Hangul Analyzer iii)는 kakao가 2018년 말 공개한 오픈소스 한국어 형태소 분석기다. 국립국어원이 구축한 세종 코퍼스를 이용해 CNN 모델을 적용해 학습했다. Khaiii의 아키텍처는 입력 문장을 문자 단위로 읽어 들인 뒤 convolution filter가 이 문자들을 슬라이딩해 가면서 정보를 추출한다. 출력 노드에서는 이렇게 모은 정보들을 종합해 형태소의 경계와 품사 태그를 예측한다. 카카오 측 설명에 따르면 모델을 C++로 구현해 GPU 없이도 형태소 분석이 가능하며 실행 속도 역시 빠르다고 한다. 123456789101112131415from khaiii import KhaiiiApitokenizer = KhaiiiApi()data = tokenizer.analyze('아버지가방에들어가신다')tokens = []for word in data: token.extend([str(m).split(\"/\")[0] for m in word.morphs])# 결과['아버지', '가', '방에', '들', '어', '가', '시', 'ㄴ다']# 품사 정보 확인 taggingfor word in data: token.extend([str(m) for m in word.morphs]) 결과1['아버지/NNG', '가/JKS', '방에/NNG', '들/VV', '어/EC', '가/VV', '시/EP', 'ㄴ다/EC'] 3) soynlp (비지도학습으로 학습) 형태소 분석, 품사 판별 등을 지원하는 파이썬 기반 한국어 자연어 처리 패키지다. 데이터 패턴을 스스로 학습하는 비지도 학습 접근법을 지향하기 때문에 하나의 문장 혹은 문서에서보다는 어느 정도 규모가 있으면서 동질적인 문서 집합(homogeneous documents)에서 잘 작동한다. soynlp 패키지에 포함된 형태소 분석기는 데이터의 통계량을 확인해 만든 단어 점수 표로 작동한다. 단어 점수는 크게 응집확률(Cohesion Probability) 과 브랜칭 엔트로피(Branching Entropy)를 활용한다. 구체적으로는 주어진 문자령이 유기적으로 연결돼 함께 자주 나타나고(응집 확률이 높을 때), 그 단어 앞뒤로 다양한 조사, 어미 혹은 다른 단어가 등장하는 경우(브랜칭 엔트로피가 높을 때) 해당 문자열을 형태소로 취급한다. 예를 들어, 주어진 Corpus에서 ‘꿀잼’이라는 단어가 연결돼 자주 나타났다면 ‘꿀잼’을 형태소라고 본다(응집 확률이 높음). 한편 ‘꿀잼’ 앞에 ‘영화’, ‘정말’, ‘너무’ 등 문자열이, 뒤에 ‘ㅋㅋ’, ‘ㅎㅎ’, ‘!!’ 등 패턴이 다양하게 나타났다면 이 역시 ‘꿀잼’을 형태소로 취급한다.(브랜칭 엔트로피가 높음) Cohesion score + L-Tokenizer Cohesion score는 한국어의 단어 추출을 위하여 character n-gram 을 이용한다. 새로운 개념을 설명하기 위해 새로운 단어가 만들어지기 때문에 모든 단어를 포함하는 사전은 존재할 수 없다. 학습데이터를 이용하는 supervised algorithms 은 가르쳐주지 않은 단어를 인식하기가 어렵다. 실질적으로는 사전에 등록되지 않은 단어는 형태소 분석을 할 수 없다는 것이다. 통계 기반 단어 추출 기법은 ‘우리가 분석하려는 데이터에서 최대한 단어를 인식’하여 학습데이터를 기반으로 하는 supervised approach 를 보완하기 위한 방법이다. 단어의 경계에 가까워질수록 P(xy|x)의 값이 커지고, 단어의 경계를 넘어서면 P(xy|x)의 값이 줄어든다. 이 현상을 이용하여 L part 에서 단어를 추출할 수 있는 character n-gram 기반 score 를 정의한다. 필자가 이해하기로는 간단하게 말하면 corpus에서 사용되는 단어 중 의미를 갖는 단위를 나누는 기준으로써 Cohesion score가 높은 것을 사용한다는 것이다. 참조 1234567891011121314151617181920212223242526from soynlp.word import WordExtractorsentence = [데이터]word_extractor = WordExtractor(min_frequency=100, min_cohesion_forward=0.05, min_right_branching_entropy=0.0)word_extractor.train(sentence)# model 저장word_extractor.save(model_fname_and_path)# 위에서 저장한 모델 loadimport mathfrom soynlp.tokenizer import LTokenizermodel_fname = '위에서 저장했던 model path'word_extractor = WordExtractor(min_frequency=100, min_cohesion_forward=0.05, min_right_branching_entropy=0.0)word_extractor.load(model_fname)scores = word_extractor.word_scores()scores = &#123;key ; (scores[key].cohesion_forward * math.exp(scores[key].min_right_branching_entropy)) for key in scores.keys()&#125;tokenizer = LTokenizer(scores=scores)tokens = tokenizer.tokenize('애비는 종이었다.') 4) 구글 센텐스피스(sentencepiece) 구글에서 공개한 비지도 학습기반 형태소 분석 패키지이며, 1994년 제안된 바이트 페어 인코딩(BPE : Byte Pair Encoding)기법 등을 지원하며 pip 설치를 통해 파이썬 콘솔에서도 사용할 수 있다. BPE의 기본 원리 Corpus에서 가장 많이 등장한 문자열을 병합해 문자열을 압축하는 것 예시 aaabdaaabac 위의 문자열에서는 aa가 가장 많이 나타났다. 이를 Z로 치환하면 원래 문자열을 다음과 같이 압축할 수 있다. ZabdZabac 이번에는 ab가 가장 많이 나타났으므로 Y로 치환하겠다. ZYdZYac 자연어 처리에서 BPE가 처음 쓰인 것은 기계 번역 분야다. BPE를 활용해 토크나이즈하는 메커니즘의 핵심은 원하는 어휘 집합 크기가 될 때까지 반복적으로 고빈도 문자열들을 병합해 어휘 집합에 추가한다. 이것이 BPE학습이다. 학습이 끝난 이후의 예측과정은 문장 내 각 어절(띄어쓰기로 문장을 나눈 것)에 어휘 집합에 있는 subword가 포함돼 있을 경우 해당 subword를 어절에서 분리한다.(최장 일치 기준) 이후 어절의 나머지에서 어휘 집합에 있는 subword를 다시 찾고, 또 분리한다. 어절 끝까지 찾았는데 어휘 집합에 없으면 미등록 단어(Unknown word)로 취급한다. BERT 모델은 BPE로 학습한 어휘 집합을 쓴다. BPE는 문자열 기반의 비지도 학습 기법이기 때문에 데이터만 확보할 수 있다면 어떤 언어에든 적용이 가능하다. 물론 BERT 모델에 사용할 수 있는 어휘 집합으로 쓸 수 있게 하기 위해서는 언더바(_) 문자를 ‘##’로 바꾸고 [PAD], [UNK], [CLS], [MASK], [SEP] 등 스페셜 토큰을 추가한다. 구글이 공개한 BERT 모델 코드에서 BPE로 학습한 어휘 집합으로 토큰을 분리하는 클래스를 실행 1234567891011121314import sentencepiece as spmtrain =\"\"\"--input=input_file_path \\ --model_prefix=sentence \\ --vocab_size=32000 \\ --model_type=bpe --character_coverage=0.9995\"\"\"spm.SentencePieceTrainer.Train(train)from bert.tokenization import FullTokenizervocab_fname = \"vocabulary_file_path.vocab\"tokenizer = FullTokenizer(vocab_file=vocab_fname, do_lower_case=False)tokenizer.tokenize(\"집에좀 가자\") 결과1['집에', '##좀', '가자'] soynlp 형태소 분석이나 BPE 방식의 토크나이즈 기법은 띄어쓰기에 따라 분석 결과가 크게 달라지므로 이들 모델을 학습하기 전 띄어쓰기 교정을 먼저 적용하면 그 분석 품질이 개선될 수 있다.띄어쓰기 교정 soynlp에서는 띄어쓰기 교정 모듈도 제공한다. Corpus에서 띄어쓰기 패턴을 학습한 뒤 해당 패턴대로 교정을 수행한다. 예를 들어, 학습 데이터에서 ‘하자고’라는 문자 앞뒤로 다수의 공백이 발견됐다면 예측단계에서 ‘하자고’가 출현한다면 앞뒤를 띄어서 교정하는 방식이다. 123456789101112from soyspacing.countbase importCountSpace# corpus가 띄어쓰기가 이미 올바로 되어있어야 품질이 높아질 것이다.corpus_fname = 'corpus_path'model_fname = '저장할때 사용할 모델 path'model = CountSpace()model.train(corpus_fname)model.save(model_fname, json_format=False)model.load_model(model_fname, json_format=False)model.correct(\"어릴때보고 지금다시봐도 재밌어요\") 결과1[어릴때 보고 지금 다시봐도 재밌어요]","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"임베딩이란?","slug":"NLP_01","date":"2020-01-16T08:29:53.000Z","updated":"2020-02-03T14:23:01.815Z","comments":true,"path":"2020/01/16/NLP_01/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/16/NLP_01/","excerpt":"","text":"컴퓨터가 바라보는 문자 아래와 같이 문자는 컴퓨터가 해석할 때 그냥 기호일 뿐이다. 이렇게 encoding된 상태로 보게 되면 아래와 같은 문제점이 발생할 수 있다. 이 글자가 어떤 글자인지를 표시할 수 있고 그에 따른 특성을 갖게 하려면 우선 계산할 수 있게 숫자로 만들어 주어야 할 것이다. 그러한 방법 중 가장 단순한 방법이 One-hot encoding을 통한 것이다. 허나, 이러한 Sparse matrix를 통한 계산은 너무 비효율 적이다. 그렇다면 어떻게 dense하게 표현할 수 있을지를 고민하는 것이 바로 Embedding이라는 개념의 본질일 것이다. 임베딩(Embedding)이란? 자연어 처리(Natural Language Processing)분야에서 임베딩(Embedding)은 사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자형태인 vector로 바꾼 결과 혹은 그 일련의 과정 전체를 의미한다. 가장 간단한 형태의 임베딩은 단어의 빈도를 그대로 벡터로 사용하는 것이다. 단어-문서 행렬(Term-Document Matrix)는 row는 단어 column은 문서에 대응한다. 구분 메밀꽃 필 무렵 운수 좋은 날 사랑 손님과 어머니 삼포 가는 길 기차 0 2 10 7 막걸리 0 1 0 0 선술집 0 1 0 0 위의 표에서 운수좋은 날이라는 문서의 임베딩은 [2, 1, 1]이다. 막걸리라는 단어의 임베딩은 [0, 1, 0, 0]이다. 또한 사랑 손님과 어머니, 삼포 가는 길이 사용하는 단어 목록이 상대적으로 많이 겹치고 있는 것을 알 수 있다. 위의 Matrix를 바탕으로 우리는 사랑 손님과 어머니는 삼포 가는 길과 기차라는 소재를 공유한다는 점에서 비슷한 작품일 것이라는 추정을 해볼 수 있다. 또 막걸리라는 단어와 선술집이라는 단어가 운수 좋은 날이라는 작품에만 등장하는 것을 알 수 있다. 막걸리-선술집 간 의미 차이가 막걸리 기차 보다 작을 것이라고 추정해 볼 수 있다. 임베딩의 역할 1) 단어/문장 간 관련도 계산 단어-문서 행렬은 가장 단순한 형태의 임베딩이다. 현업에서는 이보다 복잡한 형태의 임베딩을 사용한다. 대표적인 임베딩 기법은 Word2Vec을 뽑을 수 있을 것이다. 이렇듯 컴퓨터가 계산하기 쉽도록 단어를 전체 단어들간의 관계에 맞춰 해당 단어의 특성을 갖는 벡터로 바꾸면 단어들 사이의 유사도를 계산하는 일이 가능해진다. 자연어일 때 불가능했던 유사도를 계산할 수코사인 유사도 계산이 임베딩 덕분에 가능하다는 것이다. 또한 임베딩을 수행하면 벡터 공간을 기하학적으로 나타낸 시각화 역시 가능하다. 2) 의미적/문법적 정보 함축 임베딩은 벡터인 만큼 사칙 연산이 가능하다. 단어 벡터 간 덧셈/뺄셈을 통해 단어들 사이의 의미적, 문법적 관계를 도출해낼 수 있다. 예를들면, 아들 - 딸 + 소녀 = 소년이 성립하면 성공적인 임베딩이라고 볼 수 있다. 아들 - 딸 사이의 관계와 소년 - 소녀 사이의 의미 차이가 임베딩에 함축돼 있으면 품질이 좋은 임베딩이라 말할 수 있다는 이야기이다. 이렇게 단어 임베딩을 평가하는 방법을 단어 유추 평가(word analogy test)라고 부른다. 3) 전이학습(Transfer learning) 품질 좋은 임베딩은 모형의 성능과 모형의 수렴속도가 빨라지는데 이런 품질 좋은 임베딩을 다른 딥러닝 모델의 입력값으로 사용하는 것을 transfer learning이라 한다. 예를 들면, 대규모 Corpus를 활용해 임베딩을 미리 만들어 놓는다. 임베딩에는 의미적, 문법적 정보 등이 녹아 있다. 이 임베딩을 입력값으로 쓰는 전이 학습 모델은 문서 분류라는 업무를 빠르게 잘 할 수 있게 되는 것이다. 임베딩 기법의 역사와 종류 통계 기반 -&gt; 뉴럴 네트워크 기반 통계 기반 기법 잠재 의미 분석(Latent Semantic Analysis) : 단어 사용 빈도 등 Corpus의 통계량 정보가 들어 있는 행렬에 특이값 분해등 수학적 기법을 적용해 행렬에 속한 벡터들의 차원을 축소하는 방법이다. 차원을 축소하는 이유는 예를 들어 Term-Document matrix 같은 경우는 row가 더 큰 sparse matrix일 확률이 높기 때문에 쓸데 없이 계산량과 메모리자원을 낭비하는 것을 예방하기 위해서이다. 여기서 차원 축소를 통해 얻은 행렬을 기존의 행렬과 비교했을 때 단어를 기준으로 했다면 단어 수준 임베딩, 문서를 기준으로 했다면 문서 임베딩이된다. 잠재 의미 분석 수행 대상 행렬은 여러 종류가 될 수 있으며, Term-Document Matrix, TF-IDF Matrix, Word-Context Matrix, PMI Matrix등이 있다. Neural Network 기반 기법 Neural Probabilistic Language Model이 발표된 이후 부터 Neural Network기반의 임베딩 기법들이 주목 받고 있다. Neural Network는 구조가 유연하고 표현력이 풍부하기 때문에 자연어의 무한한 문맥을 상당 부분 학습할 수 있다. 단어 수준 -&gt; 문장 수준 단어 수준 임베딩 기법 : 각각의 벡터에 해당 단어의 문맥적 의미를 함축하지만, 단어의 형태가 동일하다면 동일단어로 인식하고, 모든 문맥 정보를 해당 단어 벡터 투영하므로 동음이의어를 분간하기 어렵다는 단점이 있다. ex) NPLM, Word2Vec, GloVe, FastText, Swivel 등 문장 수준 임베딩 기법 : 2018년 초에 ELMo(Embedding from Language Models)가 발표된 이후 주목 받기 시작했다. 개별 단어가 아닌 단어 Sequence 전체의 문맥적 의미를 함축 하기 때문에 단어 임베딩 기법보다 Transfer learning 효과가 좋은 것으로 알려져 있다. 또한, 단어 수준 임베딩의 단점인 동음이의어도 문장수준 임베딩 기법을 사용하면 분리해서 이해할 수 있다. ex) BERT(Bidirectional Encoder Representations from Transformer), GPT(Generation Pre-Training) 등 Rule based -&gt; End to End -&gt; Pre-training/fine tuning 1990년대에는 자연어 처리 모델 대부분은 우리가 딥러닝과 달리 머신러닝처럼 사람이 Feature를 직접 뽑았다. 그렇기에 Feature를 추출할 때 언어학적인 지식을 활용해야 했다. 허나. 2000년대 중반 이후 NLP 분야에서도 딥러닝 모델이 주목받기 시작하여 Feature를 직접 뽑지 않아도 되었다. 데이터를 넣어주면 사람의 개입없이 모델 스스로 처음부터 끝까지 이해하는 End-to-End Model 기법을 사용하였다. 대표적으로는 기계번역에 널리 사용됐던 Sequence-to-Sequence 모델이 있다. 2018년 ELMo 모델이 제안된 이후 NLP 모델은 pre-training과 fine tuning 방식으로 발전하고 있다. 우선 대규모 Corpus로 임베딩을 만든다.(Pre-train) 이 임베딩에는 Corpus의 의미적, 문법적 맥락이 포함돼 있다. 이후 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만드로 우리가 풀고 싶은 구체적 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트한다.(fine tuning) ELMo, GPT, BERT등이 이 방식에 해당된다. 우리가 풀고 싶은 자연어 처리의 구체적 문제들(예시 : 품사 판별(Part-Of-Speech tagging), 개체명 인식(Named Entity Recognition), 의미역 분석(Semantic Role Labeling))을 다운 스트림 태스크(DownStream task)라고 한다. 다운스트림에 앞서 해결해야 할 과제라는 뜻의 업스트림 테스크(UpStream task)는 단어/문장 임베딩을 Pre-train하는 작업이 해당된다. 임베딩의 종류와 성능1) 행렬 분해 Corpus 정보가 들어 있는 원래 행렬을 Decomposition을 통해 임베딩하는 기법이다. Decomposition 이후엔 둘 중 하나의 행렬만 사용하거나 둘을 sum하거나 concatenate하는 방식으로 임베딩을 한다. ex) GloVe, Swivel 등 2) 예측 기반 어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전 단어들이 주어졌을 때 다음 단어가 무엇일지 예측하거나, 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 맞추는 과정에서 학습하는 방법 Neural Network기반 방법들이 속한다. ex) Word2Vec, FastText, BERT, ELMo, GPT 등 3) 토픽 기반 주어진 문서에 잠재된 주제를 추론하는 방식으로 임베딩을 수행하는 기법이며, 대표적으로 잠재 디리클레 할당(LDA)가 있다. LDA 같은 모델은 학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환하기 때문에 임베딩 기법의 일종으로 이해할 수 있다. NLP 용어 정리Corpus(말뭉치) 임베딩 학습이라는 특정한 목적을 가지고 수집한 표본이다. Collection(컬렉션) Corpus에 속한 각가의 집합을 칭한다. 예를 들어, 한국어 위키백과와 네이버 영화 리뷰를 말뭉치로 쓴다면 이들 각각이 컬렉션이 된다. Sentence(문장) 생각이나 감정을 말과 글로 표현할 때 완결된 내용을 나타내는 최소의 독립적인 형식 단위를 가리킨다. 실무에서는 주로 문장을 마침표(.)나 느낌표(!), 물음표(?)와 같은 기호로 구분된 문자열을 문장으로 취급한다. Document(문서) 생각이나 감정, 정보를 공유하는 문장 집합을 의미한다. 문서는 단락(Paragraph)의 집합으로 표현될 수 있다. 별도의 기준이 없다면 줄바꿈(\\n) 문자로 구분된 문자열을 문서로 취급한다. Token(토큰) 문장은 여러개의 토큰으로 구성된다. 토큰은 단어(Word), 형태소(Morpheme), 서브워드(subword)라고도 한다. 문장을 토큰 시퀀스로 분석하는 과정을 토크나이즈(tokenize)라고 한다. Vocabulary(어휘집합) Corpus에 있는 모든 Document를 Sentence로 나누고 여기에 Tokenize를 실행한 후 중복을 제거한 Token들의 집합이다. Vocabulary에 없는 token은 미등록 단어(Unknown word)라고 한다. 벡터가 어떻게 의미를 가지게 되는가 자연어의 의미를 임베딩에 녹여내는 방법은 자연어의 통계적 패턴 정보를 통째로 임베딩에 넣는 것이다. 자연어의 의미(문법적 의미, 단어의 의미등)는 그 언어를 사용하는 사람들의 일상 언어에 정보가 들어있기 때문이다. 임베딩을 만들 때 사용하는 통계 정보는 크게 3가지가 있다. 1) 문장에 어떤 단어가 많이 쓰였는지 -&gt; bag of words(백오브워즈) 가정 2) 단어가 어떤 순서로 등장하는지 -&gt; Language model(언어 모델) 가정 3) 문장에 어떤 단어가 같이 나타났는지 -&gt; distribution hypothesis(분포가정) 1) BOW(Bag-Of-Words) 가정 문서의 저자가 생각한 주제가 문서에서의 단어 사용에 녹아있다는 생각으로부터 단어의 순서 정보는 무시하고 어떤 단어가 많이 쓰였는지 정보를 중시한다. 경우에 따라서는 빈도 역시 단순화해 등장 여부(등장 시 1, 아니면 0)만을 사용하기도 한다. 간단한 아이디어지만 정보 검색(information Retrieval)분야에서 여전히 많이 쓰이고 있다. 사용자 질의에 가장 적절한 문서를 보여줄 때 질의를 BOW 임베딩으로 변환하고 질의와 검색 대상 문서 임베딩 간 코사인 유사도를 구해 가장 높은 문서를 사용자에게 노출 한다. 대표 통계량 : TF-IDF개념을 모른다면 클릭 대표 모델 : Deep Averaging Network 단어의 순서를 고려하지 않고 단어의 임베딩을 평균을 취해 만든다. 간단한 구조임에도 성능이 좋아서 현업에서도 자주 쓰인다. 2) Language model 가정- `시퀀스에 확률을 부여하여 단어 시퀀스를 명시적(순서를 고려)으로 학습하는 모델` 2-1) 통계 기반 언어 모델 단어가 n개 주어진 상황이라면 Language model은 n개 단어가 동시에 나타날 확률을 반환한다. 통계 기반의 언어 모델은 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습한다. 잘 학습된 언어 모델이 있다면 주어진 단어 시퀀스 다음 단어로 확률이 높은 자연스러운 단어를 선택할 것이다. 구체적인 방법은 한 상태의 확률은 그 직전 상태에만 의존한다는 Markov assumption에 기반하여 n-gram을 통해 확률을 계산할 수 있다. 허나 데이터에 한 번도 등장하지 않는 n-gram이 존재할 때 예측 단계에서는 확률값을 0으로 취하는 문제가 있다. P(w_{n}|w_{n-1} = \\frac{w_{n-1}}{w_{n}}) 위의 문제점들을 해결하기 위해 Back-off, Smoothing 등의 방식이 제안됐다. 1) Back-off n-gram 등장 빈도가 0인 단어들이 있을 수 있으므로 n-gram 등장빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사하는 방식이다. n을 크게 하면 할수록 등장하지 않는 케이스가 많아질 가능성이 높기 때문이다. $\\alpha, \\beta$는 실제 빈도와의 차이를 보정해주는 parameter이다. Freq(내 마음 속에 영원히 기억될 최고의 명작이다) \\approx \\alpha Freq(영원히 기억될 최고의 명작이다) + \\beta 2) (Add-k) Smoothing 등장 빈도 표에 모두 k 만큼 더하는 기법이다. 만약 k=1로 설정한다면 특별히 라플라스 스무딩(laplace smoothing)이라고 한다. 스무딩을 시행하면 높은 빈도를 가진 문자열 등장 확률을 일부 깎고 전혀 등장하지 않는 케이스들에는 약간의 확률을 부여하게 된다. 2-2) 뉴럴 네트워크 기반 언어 모델 Neural Network는 입력과 출력 사이의 관계를 유연하게 포착해낼 수 있고, 그 자체로 확률 모델로 기능할 수 있다. 주어진 단어 시퀀스를 가지고 다음 단어를 예측하는 과정에서 학습된다. 학습이 완료되면 이들 모델의 중간 혹은 말단 계산 결과물을 단어나 문자의 임베딩으로 활용한다. Language model 기반 기법은 순차적으로 입력받아 다음 단어를 맞춰야 하기 때문에 일방향(uni-directional)이지만 Masked language model은 문장 전체를 다 보고 중간에 있는 단어를 예측하기 때문에 양방향(bi-directional)학습이 가능하다. 그로인해 Masked Language model 기반의 방법들(예:BERT)은 기존 Language model 기법들 대비 임베딩 품질이 좋다. 대표 모델 : ELMo, GPT 등 3) Distribution hypothesis 자연어 처리에서 분포란 특정 범위, 즉 Window(해당 단어를 중심으로 범위에 포함시킬 앞뒤 단어 수, 예를 들어 윈도우가 2라면 타깃 단어 앞뒤로 2개의 문맥단어의 빈도를 계산) 내에 동시에 등장하는 이웃 단어 또는 문맥(context)의 집합을 가리킨다. 어떤 단어 쌍이 비슷한 문맥 환경에서 자주 등장한다면 그 의미 또한 유사할 것이라는 것이 Distribution hypothesis의 전제이다. 형태소의 경계를 정하거나 품사를 나누는 것과 같은 다양한 언어학적 문제는 말뭉치의 분포 정보와 깊은 관계를 갖고 있다. 이 덕분에 임베딩에 분포 정보를 함축하게 되면 해당 벡터에 해당 단어의 의미를 내제시킬 수 있는 것이다. 대표 통계량 : PMI(Pointwise Mutual Information : 점별 상호 정보량) 두 단어의 등장이 독립일 때 대비해 얼마나 자주 같이 등장하는지를 수치화한 것 PMI(A, B) = log\\frac{P(A,B)}{P(A)P(B)} Term-context matrix는 특정 단어 기준으로 Window에 존재하는 단어들을 count하는 방식으로 만들어지는데, 여기에서 PMI 수식을 적용시키면된다. 이렇게 구축한 PMI 행렬의 행 벡터 자체를 해당 단어의 임베딩으로 사용할 수도 있다. 대표 모델 : Word2Vec CBOW 모델 문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습된다. 1) 각 주변 단어들을 one-hot 벡터로 만들어 입력값으로 사용 (입력층 벡터) 2) 가중치 행렬을 각 one-hot 벡터에 곱해서 n-차원 벡터를 만든다. (N-차원 은닉층) 3) 만들어진 n-차원 벡터를 모두 더한 후 개수로 나눠 평균 n-차원 벡터를 만든다. (출력층 벡터) 4) n-차원 벡터에 다시 가중치 행렬을 곱해서 one-hot 벡터와 같은 차원의 벡터로 만든다. 5) 만들어진 벡터를 실제 예측하려고 하는 단어의 one-hot 벡터와 비교해서 학습한다. Skip-gram 모델 타깃 단어를 가지고 문맥 단어가 무엇일지 예측하는 과정에서 학습된다. 1) 하나의 단어를 one-hot 벡터로 만들어서 입력값으로 사용한다.(입력층 벡터) 2) 가중치 행렬을 one-hot 벡터에 곱해서 n-차원 벡터를 만든다.(N-차원 은닉층) 3) n-차원 벡터에 다시 가중치 행렬을 곱해서 one-hot 벡터와 같은 차원의 벡터로 만든다.(출력층 벡터) 4) 만들어진 벡터를 실제 예측하려는 주변 단어들 각각의 one-hot 벡터와 비교해서 학습한다. 두 모델의 확실한 차이점은 CBOW에서는 입력값으로 여러 개의 단어를 사용하고, 학습을 위해 하나의 단어와 비교하지만, Skip-gram에서는 입력값이 하나의 단어를 사용하고, 학습을 위해 주변의 여러 단어와 비교한다. 위의 학습 과정을 모두 끝낸 후 가중치 행렬의 각 행을 단어 벡터로 사용한다. 카운트 기반 방법(Bag of Words 가정 방법들)로 만든 단어 벡터보다 단어 간의 유사도를 잘 측정하며, 단어들의 복잡한 특징까지도 잘 잡아낸다는 장점이 있다. 보통 CBOW보다 Skip-gram의 성능이 더 좋아 자주 사용된다. 하지만 무조건적으로 좋은 것은 아니다!","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"Regression(03) - 회귀진단","slug":"machine_learning_04","date":"2020-01-15T09:24:28.000Z","updated":"2020-01-16T08:10:22.954Z","comments":true,"path":"2020/01/15/machine_learning_04/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/15/machine_learning_04/","excerpt":"","text":"교호작용 성별, 결혼여부, 혹은 소속 정치단체 등과 같은 질적(qualitative) 또는 범주형(categorical)요인들이 회귀분석에서 종속(반응)변수의 변화를 설명하는 데 매우 유용한 독립(설명) 변수 역할을 할 때가 있다. 이런 질적 독립(설명)변수로 이용할 경우 이들은 지시변수(Indicator variable) 또는 가변수(dummy variable)의 형식으로 표현해야한다. 가변수는 다양한 용도를 가지고 있으며, 회귀관계에 영향을 주는 질적 요인을 고려할 때마다 항상 사용할 수 있다. -여러 범주를 표현하기 위하여 가변수를 사용할 경우 필요한 가변수의 개수는 일반적으로 가능한 범주의 수보다 하나 작게 잡으면 된다.왜냐하면, 가변수를 종합하면 교육수준에 관한 3개의 범주를 나타낼 수 있기 때문이다. 게다가, 범주를 모두 다 지시변수로 사용하면 범주화된 변수들끼리 완벽한 선형관계가 성립되어 극단적인 다중공선성을 보일수 있다. 선형대수 측면에서도 각 Column vector들끼리 서로 linearly independent 해야 해를 갖을 수 있으므로 위의 방법으로 만드는 것이 옳은 방법이다. 여기서 지시변수 또는 가변수에 의하여 표현되지 않는 범주는 기저범주(base category) 또는 대조 그룹(control group)이라고 불리는데, 지시변수의 회귀계수가 대조 그룹에 대한 상대적인 값으로 해석되기 때문이다. 아래 표에서 만일, 최종학력이 대학원인 사람과 대학교인 사람의 평균적인 차이가 궁금할 경우는 $B_{2}-B-{1}=2,000$로 구할 수 있다. 또한 아래 해석은 다른 변수들을 고정시켰을 경우에 해당한다. 수입 = ( \\beta_{0} + 대학교 + 대학원 ) 변수 선택법 회귀 분석의 진단 : 모형 위반의 검출 주어진 데이터에 모형을 적합함에 있어서, 한 개 또는 몇 개의 관측 개체들에 의하여 적합이 과도하게 결정되는 것으 바람직 하지 않다. 앞서 말한 가설검정등은 표준적인 회귀의 가정들이 만족될 때만 유의미하다. 이들 가정이 위반된다면, 이전에 언급된 표준적인 결과들은 유효하지 않으며 결과의 응용이 심각한 오류를 야기할 수도 있다. 모형위반을 검토하기 위해 엄격한 수치적 규칙들을 적용하는 것 대신에 주로 그래프적인 방법들을 소개할 것이다. 회귀분석의 표준적인 가정\u001d들 1) 선형성 가정 : 종속(반응)변수 Y와 독립(설명)변수 X들을 관계시키는 모형이 회귀계수 $\\beta$들에 대하여 선형임을 가정한다. 만약, 선형성 가정이 만족되지 않는다면 종종 데이터에 대한 변환을 통해 선형성을 달성할 수 있다. 단순회귀에서는 이 가정을 Y와 X의 산점도를 통해 쉽게 확인 할 수 있으나, 다중회귀에서는 고차원성 때문에 산점도를 통해 확인이 어렵다. Y = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{p}X_{p} + \\epsilon 2) 잔차에 대한가정 : $\\epsilon_{i} \\sim^{i.i.d} N(0, \\sigma^{2})$ 이 가정을 통해 아래의 가정들을 만족해야한다. 1) 잔차의 정규성 : 잔차 $\\epsilon_{i}$는 정규분포를 따른다. 독립(설명)변수들의 값이 반복되어 있지 않다면 쉽게 위반되지 않는다. 2) 잔차의 등분산성 : 동일한 상수분산 $\\sigma^{2}$을 가져야 한다. 이 가정이 만족하지 않을 때 이분산성을 띈다는 문제가 있다고 한다. 3) 잔차의 독립성 : 잔차들이 서로 독립이므로 그들의 공분산은 모두 0이다. 이 가정이 만족되지 않으면 자기상관의 문제가 있다고 한다. 3) 독립(설명)변수들에 대한 가정(1,2는 실제로 평가 불가하므로 3이 중요!!) : 1) 독립(설명)변수들은 확률변수가 아니다. 만약 실험 설계에 의해서 얻어진 데이터 값들에 의한 것이 아닌 비실험 또는 관측의 상황에서는 이것이 만족되지 않을 거이라는 것은 명확하며, 이에 대한 해석도 수정되어야한다. 독립(설명)변수들이 확률변수이면 모든 추론은 관측된 데이터에 의존하여 조건부적이다. 2) 값 $x_{1j}, x_{2j}, \\cdots ,x_{nj}$는 오차 없이 측정된 것으로 가정된다. 허나 이 가정은 만족되기 쉽지 않다. 측정에서의 오차는 잔차의 분산, 다중상관계수, 회귀계수의 개별 추정치들에 영향을 줄 것이다. 추정된 회귀계수로부터 측정오차의 영향을 제거하는 것은 거의 기대하기 힘들다.그러므로 변수들이 오차를 가지고 있어서 회귀계수의 추정에 문제가 있더라도 회귀방정식이 예측을 위해 여전히 사용될 수 있다. 그러나 독립(설명)변수에 존재하는 오차는 예측의 정확도를 감소 시킬 것이다. 3) 독립(설명)변수는 선형종속이 아닌 것으로 가정된다. 즉, 위해서 언급했었던 linearly independent해야 한다는 의미이며 이 가정으로 인해 정규방정식의 해의 유일성을 보장받을 수 있다. 이 가정이 위반 되는 것이 공선성(collinearity)의 문제이다. 4) 관측개체에 대한 가정 : 모든 관측개체들은 동일하게 신뢰할 만하며, 회귀의 결과를 결정하고 결론을 도출함에 있어서 거의 동등한 역할을 한다. 최소제곱버의 특징 중 하나는 기본 가정에 대한 사소한 또는 작은 위반이 분석으로부터 도출된 추론이나 결론을 무효화할 만큼 큰 영향을 주지는 않는다는 것이다. 그러나 모형의 가정에 대한 큰 위반은 결론을 심각하게 왜곡 시키므로 결론적으로, 그래프를 통해서 잔차의 구조와 데이터의 패턴을 조사하는 것은 매우 중요하다. 다양한 유형의 잔차들 회귀분석에 있어서 모형이 가지는 가능한 결함을 찾아내는 데 가장 간단하고 효과적인 방법은 잔차플롯을 살펴보는 것이다. 더욱이, 분석이 요약통계량에만 근거할 경우 간과할지도 모를 데이터의 중요한 구조와 정보들을 잔차분석을 통해 발견할 수도 있다. \\hat{Y} = X\\hat{\\beta} = PYP = X(X^{T}X)X{T} 즉, Y를 $\\hat{Y}$로 만들기 위한 linear transform matrix를 모자(hat) 또는 사영(Projection) matrix P라고 한다. 여기서 $i=j일 때, p_{ii}=p_{ij}=p_{ji}=p_{jj}$는 사영행렬(P)의 i번째 대각원소이다. 이것은 i번째 관측개체에 대한 지레값(Leverage value)으로 불린다. 아래의 식에서 볼 수 있듯이 $\\hat{y}_{i}$은 Y의 모든 관측값들의 가중합이며, $p_{ii}$는 i번째 적합값 $\\hat{y}_{i}$을 결정함에 있어서 $y_{i}$에 부여되는 가중치(지레)이기 때문이다. \\hat{y}_{i} = p_{i1}y_{1} + p_{i2}y_{2} + \\cdots + p_{in}y_{n}, i=1,2,...,n 또한, 잔차($e_{i}$)의 분산은 그의 표준편차로 나누어 표준화하여 다음과 같이 평균 0과 표준편차 1을 가지는 표준화 잔차(standardized residual)을 얻을 수 있다. 자세하게는 $\\sigma$를 어떤 것을 사용하냐에 따라 내적 표준화잔차와 외적 표준화잔차로 나뉘어지지만, 결국 표본크기가 충분히 클때(30이상) 이 잔차들은 근사적으로 표준정규분포를 따른다. 또한 잔차들을 엄밀하게는 서로 독립이 아니지만, 표본크기가 크면 독립성의 문제는 무시 될 수 있다. 따라서, 잔차플롯을 작성함에 있어서 두가지 형태의 잔차 중 어느 것을 사용하는가는 별로 문제가 되지 않는다. z_{i} = \\frac{e_{i}}{\\sigma \\sqrt{1-p_{ii}}}그래프적 방법들 그래프적 방법들은 데이터 분석에서 중요한 역할을 하며, 특히 데이터에 선형모형을 적합할 때 더욱 중요하다. 분석이 수치적 결과에만 의존한다면 잘못된 결론에 도달할 수 있음을 볼 수 있다.그 대표적인 예로는 Anscombe의 데이터를 들 수 있다. 특정 그래프를 탐색하기에 앞서, 어떤 가정이 만족될 때 그 그래프가 어떻게 나타나야 하는지를 알아야 한다. 그러고 나서 그 그래프가 기대와 일치하는지를 살펴보아야 한다. 이렇게 함으로써 가정의 올바름 또는 그릇됨을 확인할 수 있을 것이다. 1) 모형을 적합하기 이전의 그래프 종속(반응)변수와 독립(설명)변수 사이의 관계를 나타내는 모형의 형태는 이론적 배경 또는 검정될 가설에 근거해야 한다. 1) 일차원 그래프 : 개별 변수의 분포를 개략적으로 살펴보기 위해 그린다. 이를 통해 어떤 변수가 매우 치우쳐져 있다면 변환이 수행되어야 한다. 비대칭의 정도가 심한 변수에 대하여 로그 변환이 추천된다. 일변량 그래프는 원래의 변수를 이용해야 할지 아니면 변환된 변수를 가지고 분석을 수행해야 하지에 대하여 정보를 제공한다. 또한 일변량 그래프는 변수에 있는 특이값의 존재 유무를 제시한다. 특이값은 그것이 입력오류 등에 의한 것인지(측정후 잘못 기입된 경우와 같은)를 알아보기 위해 조사되어야한다. 또한 특이값은 이후의 분석에서 문제를 발생시킬 수도 있기 때문에 분석을 수행할 때 주의깊게 다루어져야 한다. ex) histogram, stem-and-leaf display, dot plot, box plot 2) 이차원 그래프 : 변수의 수가 많은 경우 해당 차원과 같은 차원에서 변수들을 볼 수 없으므로, 각 변수들의 쌍에 대한 관계를 탐색하고 일반적인 패턴을 파악하기 위해 산점도를 통해 살펴볼 수 있다. 산점도행렬을 살펴볼때 주의할 점은 상관계수는 오직 선형관계만을 측정하며 robust하지 않으므로 쌍별 상관계수는 대응되는 산점도와 연관하여 해석해야 한다는 점이다. 단순회귀에서는 Y대 X의 산점도가 선형의 형태를 보일 것으로 기대되나, 다중회귀에서는 Y대 각 독립(설명)변수의 산점도가 선형의 형태를 보일 수도 있고 그렇지 않을 수도 있다. 즉, 선형의 형태가 보이지 않는다고 해서 주어진 선형모형이 옳지 않다는 것을 의미하지 않늗다. 또한, 각각의 독립(설명)변수들끼리 선형패턴을 보이지 않아야 한다. 산점도에 선형관계가 보이지 않는다는 것이 전체 독립(설명)변수들의 집합이 선형적으로 독립이라는 것을 의미하지는 않기 때문에 주의가 필요하다. 선형 관계는 두개 이상의 변수들을 포함하고 있을 수 있다. scatter plot을 통해서는 그런 다변량 관계를 검출하는 것이 쉽지 않다. 그러한 다중공선성 문제는 앞서 다룬 방법과 같이 해결하려고 해보아야 한다. 3) 회전도표 4) 동적그래프 2) 모형을 적합한 이후의 그래프 앞에서 소개된 그래프들은 데이터 검토와 모형설정 단계에서 유용하다. 데이터에 모형을 적합한 이후의 그래프들은 가정들을 검토하고 주어진 모형의 적합도를 평가하는 데 도움을 준다. 1) 선형성과 정규성 가정을 검토하기 위한 그래프표준화잔차의 정규확률 plot (Q-Q plot) 표준화 잔차의 분위수와 표준정규분포의 분위수의 scatter plot이라고 보면된다. 만약 잔차가 정규성을 띈다면 대각선과 최대한 비슷하게 그려져야한다. (Standardized) Residual vs Predictor(독립변수) 산점도 표준적인 가정 하에서 표준화잔차는 각 독립(설명)변수들과 상관되어 있지 않다. 이 가정이 만족된다면 이 플롯은 랜덤하게 흩어진 점들이 나타나야 한다. 이 plot에서 특정한 패턴이 발견된다면 어떤 가정들이 위반되었음을 의미한다. 아래 그림에\u001d서 (a)는 선형성 가정이 만족되지 않았을 때 나타나느 플롯 중 하나이며, 이 경우에는 Y 또는 특정 예측 변수에 대한 변환이 선형석을 위하여 필요할 수 있다. 그림 (b)는 이분산성을 의미하며 분산의 안정화를 위하여 데이터 변환이 필요할 것이다. (Standardized) Residual vs fitted-value plot 표준적인 가정 하에서 표준화잔차는 적합값과도 상관되어 있지 않다. 따라서 이 가정이 만족된다면 이 plot은 랜덤하게 흩어진 점들을 나타내야 한다. 표준화잔차의 인덱스 plot 표준화잔차 vs 관측개체 번호의 plot이다. 아래와 같이 해석할 수 있으며, 만일 관측개체의 취해진 순서가 중요한 의미를 가진다면, (예컨데, 개체가 시간 또는 공간 상의 순서에 따라 취해졌을 때), 연속적인 순서에 의한 잔차 plot은 오차의 독립성 가정을 검토하기 위해 사용될 수 있다. 독립성 가정 하에서 점들은 0 주위의 수평 띠(밴드) 안에서 랜덤하게 흩어져 있어야 한다. 2) 특이값과 영향력 있는 개체를 검출하기 위한 그래프지레점, 영햘력, 특이값 주어진 데이터에 모형을 적합함에 있어서 한두 개의 관측값들에 의해 적합이 과도하게 결정되면 분석이 제대로 이루어지지 않은 것이므로 이런 관측값들은 보통 잔차가 0에 가깝거나 0이기 때문에 특이값이 아니나 영향력있는 개체이다. 이런 상황에서는 잔차를 살펴보는 것은 거의 도움이 되지 않는다. 어떤 점이 제외되었을 때 혼자서 또는 다른 점들과 결합하여 적합모형(추정된 회귀계수, 적합값, t-통계량 등)에 큰 변화를 준다면 그 점을 영향력 있는 점이라고 한다. 일반적으로 어떤 점을 제외하면 약간이라도 적합에 변화가 있을 것이다. 여기에서의 관심은 그 점이 과도한 영향력이 있는가이다.따라서, 영향력이 있는 관측개체가 데이터에 존재한다면 그것을 파악하는 것이 중요하다. 영향력 있는 개체는 일반적으로 종속(반응)변수 Y 또는 독립(설명)변수 X 공간에 대하여 특이값이다. 반응(종속) 변수에 대한 특이값 : 잔차 plot을 통해 파악될 수 있으며, 잔차 plot은 존재하는 총체적인 모형위반들을 나타낼 것이며, 잔차 plot의 탐색은 분석에서 주요 도구 중 하나이다. 독립(설명) 변수에 대한 특이값 : 앞에서 설명한 지레값($p_{ii}$)는 X-공간에서 특이성을 측정하는 데 이용될 수 있다. 큰 지레값을 가지는 관측개체는 X-공간에서 특이값이기 때문이다. 반응변수에 대한 특이값(큰 표준화잔차를 가진 점)과 구별하기 위하여 높은 지레점(high leverage point)라고 한다. 위의 반응 변수에 대한 특이값은 잔차 plot을 통해 충분히 살펴 볼 수 있지만, 독립(설명)변수에 대한 특이값은 잔차 plot으로는 찾아보기 힘들다. 그 이유는 아래 잔차와 지레값의 관계에 대한 식을 살펴보면 높은 지레값을 갖는 점들은 잔차가 낮기 때문이다. 그러므로, 잔차 plot을 살펴보는 것만으로는 충분하지 않으므로 종속변수와 독립변수의 산점도에 회귀식을 그려보거나 지레값의 index plot을 그려 살펴 봐야 한다. 통상적으로 사용되는 $p_{ii}$에 대한 임계값은 $2(p+1)/n=0.2$이다. p_{ii} + \\frac{e^2_{i}}{SSE} \\leq 1영향력의 측도 Cook&#39;s distance : 전체 데이터로부터 얻은 회귀계수들과 i번째 개체를 제거하고 얻은 회귀계수(또는 적합값)들의 차이를 측정한다. C값에 대한 index plot을 그려 C값들이 비슷한 값을 가지지 않다면 돋보이는 C값들을 갖는 데이터들을 제외하고 모형을 적합에 보는 등의 방법을 검토해 봐야할 것이다 C_{i} = \\frac{sum^{n}_{j=1} (\\hat{y_{j}} - \\hat{y_{j}}_{i})^2}{\\hat{\\sigma^}^{2} (p+1)} , i=1,2, \\cdots ,n 이외의 Welsch &amp; Kuh의 측도(DFITS)와 Hadi의 영향력 측도가 있으나, Cook’s distance를 통해 충분히 검사가능하므로 생략하도록 한다. 다만, Welsh &amp; Kuh와 Cook’s distance는 잔차와 지레값에 대한 승법적(곱하는)함수인 반면에 Haid의 측도는 가법적(종속변수와 독립변수 각각에 대한 영향력의 수치를 더하는)함수이다. 특이값은 언제나 조심스럽게 조사되어야 되며 실무에서 분석시 함부로 제거해서는 안된다. 그 데이터 자체도 의미가 있을 수 있기 때문(예를 들면, 데이터가 모집단으로 부터 추출되지 않았다든가 또는 모형이 선형이 아니라는 것을 의미할 수 있기 때문)이다. 지레대 효과는 높으나 영향력이 작은 경우는 큰 문제를 일으키지는 않는다. 그러나 높은 지레값을 가지며 영향력이 큰 점들은 예측변수들의 공간에서 보통의 것들에 비해 멀리 떨어져 있으며 적합에 유의적인 영향을 끼치기 때문에 잘 검토할 필요가 있다. 즉, P값들의 index plot과 Cook&#39;s distance의 index plot과 종속변수와 독립변수 plot을 종합해서 비교해 보면서 각각의 지렛값이 높은 데이터와 영향력이 있는 값을 찾아야 할 것이다. criterion에는 cooks 와 DFITS를 사용할 수 있다. 아래 그래프의 해석은 몇가지 주의해야할 관측치들이 있는데, contractor와 reporter는 낮은 Leverage를 갖지\u001d만 큰 잔차를 갖는것을 볼 수 있다. RR.engineer는 작은 잔차와 낮은 Leverage를 갖는다. Conductor와 minister는 둘다 모두 높은 Leverage와 높은 잔차를 갖으므로 영향력있는 관측치이다. 특이값(잔차가 큰 관측치)과 영향력있는 관측개체(high leverage high residual)를 식별하는 데 유용한 다른 접근방법은 로버스트 회귀(robust regression)이다. 높은 지레값을 가지는 관측개체에 상대적으로 낮은 가중치를 주고 회귀직선을 적합시킨다. 다음에 더 자세한 설명을 할 것이다. 3) 변수들의 효과에 대한 진단플롯 회귀방정식의 어떤 변수를 보유해야 할 것인지 아니면 제거해야 할 것이지를 각각의 t-검정에 대한 보조도구로 사용될 수 있다. 첨가변수 plot 또는 편회귀 plot(added-variable plot 또는 partial regression plot) 회귀 모형에 대한 특정 독립(설명)변수를 포함시킬 것인지의 여부를 검토할 때, 그 대상이 되는 예측 변수에 대한 회귀계수의 크기를 그래프를 통하여 표현한다. plot에 나타나는 점들의 기울기는 곧 해당 독립(설명)변수에 대한 회귀계수를 나타낸다. 따라서 이 plot에 나타난 점들이 뚜렷한 기울기를 보이지 않는다면 이는 그 변수가 모형에서 별로 유용하지 않음을 의미한다. X축이 해당 예측변수 그 자체가 아니므로 비선형성의 여부를 나타내주지는 않는 점을 주의해야 한다. 또한 이 plot은 그 계수의 크기를 결정하는 데 중요한 역할을 하는 데이터 점을 제시해 주기도 한다. 첨가변수 plot은 Y-잔차$(X_{j}를 제외한 나머지 변수들로 설명되지 않은 Y의 부분) vs X_{j}-잔차(X_{j}를 종속변수로하여 나머지 변수들로 설명되지 않은 X_{j}의 부분)$을 그리는 plot이다. 이 두개의 잔차들을 최소 제곱벙으로 적합시켰을 때, 적합된 회귀직선의 기울기는 Xj를 포함한 모든 독립(설명)변수 얻은 회귀계수($\\hat{\\beta}_{j}$)와 같다. 개별로 그리는 것은 index로 식별을 할 수 있지만 아래에 여러가지를 한 꺼번에 그리는 방법은 인덱스를 볼 수 없으므로 처음에는 여러개를 다 같이 그린 후에 자세히 살펴봐야할 변수에 대해서만 개별로 그리는 방법을 사용하는 것이 좋을 것이다. 성분잔차 plot(component plus residual plot) 회귀 분석에서 가장 오래된 그래프적 기법 중의 하나이다. $(e + \\hat{\\beta}_{j} X_{j} VS X_{j})$에 대한 산점도이다. $\\hat{\\beta}_{j} X_{j}$은 j번째 독립(설명)변수가 적합값에 기여하는 공헌도(성분)임을 주목하자. 이 plot에서 기울기는 해당 독립변수에 대한 추정 회귀계수를 의미하며, 해당 예측변수의 기울기를 보여 줄 뿐 아니라 종속변수와 해당 독립변수사이의 비선형성의 존재도 알려줌으로써 필요할 경우 독립변수에 관한 구체적인 선형변환의 내용까지도 제시한다는 것이다. 이 또한, added-variable plot처럼 여러개를 그려본 뒤 필요한 변수에 대해서만 살펴보는 것을 추천. component plus residual plot vs added-variable plot 두 그래프 모두 회귀계수에 대한 추정치를 기울기로 보여주지만, added-variable plot은 어떤 데이터가 회귀계수를 추정하는데 많은 영향을 주었는지를 알 수 있게 도와준다. 반면에, component plus residual plot은 added-variable plot보다 특정 독립변수를 회귀모형에 도입해야 하느냐 하는 문제에 대한 답이나 그 독립변수가 가지는 비선형성의 여부를 탐색하는 데 더 민감한 것으로 알려져 있다. 추가적인 예측변수의 효과 회귀식에 새로운 변수를 도입하는 것의 효과에 대하여 다음의 두가지 질문을 고려해야할 것이다. (a) 새로운 변수의 회귀계수가 유의한가? (b)새로운 변수를 도입함으로써 회귀식에 이미 포함되어 있는 변수들의 회귀계수를 유의하게 변화시키는가? 이 두가지 질문에 대한 답으로 크게 4가지 유형이 있을 수 있다. 1) 새로운 변수가 유의하지 않은 회귀계수를 가지며, 다른 회귀계수들은 이전의 값에 비해 거의 변화가 없다. 어떤 다른 외부적인 조건(예컨대, 이론 또는 주제에 대한 고려)에 의하여 필요성이 있지 않다면, 새로운 변수는 회귀식에 포함되지 않아야 한다. 2) 새로운 변수가 유의한 회귀계수를 가지며, 이전에 도입된 다른 변수들의 회귀계수에 큰 변화가 있다. 이 경우 새로운 변수가 유지되어야 하며, 그러나 공선성에 대한 탐색이 필요하다. 공선성의 증거가 없다면, 그 변수는 방정식에 포함되어야 하며 다른 추가적인 변수의 도입에 대한 탐색이 수행되어야 한다. 3) 새로운 변수가 유의한 회귀계수를 가지며, 다른 회귀계수들은 이전의 값에 비하여 큰 변화가 없다. 이것은 이상적인 상황이며 새로운 변수가 이전에 도입된 변수들과 상관되어 있지 않을 때 발생한다. 이 경우 새로운 변수는 방정식에 포함되어야한다. 4) 새로운 변수가 유의하지 않은 회귀계수를 가지며, 이전에 도입된 다른 회귀계수에 큰 변화가 있다. 이것은 명백한 공선성의 증거이며, 회귀식에 새로운 변수를 포함시킬 것인지 아니면 제 제외시킬 것인지를 결정하기 전에 수정작업이 취해져야 한다.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"NLP란?","slug":"NLP_07","date":"2020-01-14T19:01:52.000Z","updated":"2020-02-04T07:55:06.549Z","comments":true,"path":"2020/01/15/NLP_07/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/15/NLP_07/","excerpt":"","text":"자연어란? NLP란? NLP의 어려움 우리가 실생활에서 사용하는 언어는 복잡성, 애매함, 그리고 의존성을 지니고 있기 때문이다. 복잡성이란 예를 들어 필자가 좋아하는 게임인 배틀그라운드로 예를 들어 보겠다. 몇 주전 PUBG에서는 배틀그라운드의 신맵인 카라킨에 대해 반응을 보기 위해 각 배틀그라운드 커뮤니티 사이트에 대한 댓글을 분석한다고 가정해보자. 그렇다면, 우선 게임 용어가 어떤 것들을 지칭하는 지 사전지식이 필요할 것이다. 이렇듯 복잡하게 연관되어 있는 Token간의 관계를 복잡성이라고 한다. 애매함은 다중모드라는 것이 있기 때문에 발생하는 것이다. 예를 들어, 우리가 실생활에서 사용하는 어구 중 ‘너 참 잘한다.’라는 문장은 여러가지 상황에서 사용되며, 상황에 따라 다른 의미를 갖는다. ‘데이터 전처리를 이렇게 잘해놨어? 와.. 너 참 잘한다.’와 ‘응? 이거 뭐야? 이거 왜 최소 글자수를 8자로했어? 전체 Corpus에 단어 평균 길이는 11인데??? 몰랐다고?? 참 잘한다~!’ 앞의 두 문장은 ‘참 잘한다’의 의미가 문맥적으로 다르다는 것이다. 의존성은 특정 질문에 대한 답변을 할때 정보가 부족해서 의존적인 부분들이 생기는 문제를 의미한다. 한가지 예로, ‘젤리 먹고싶은데 젤리 하나만 사다줄래’의 질문에 대한 답을 할때, 위의 질문만 들었을 땐 도대체 어떤 젤리를 사야할지 모를것이다. 이런 상황에 다시 ‘어떤 젤리 먹고싶은데’라는 질문을 통한 상호작용으로 해결할 수 있을 것이다. NLP분야에서의 Machine learning vs Deep learning 반복해서 얘기하지만 Feature의 추출을 사람이 직접하는 Machine learning같은 경우는 언어학에 대한 지식을 깊이 알고 있어야 가능할 것이다. 그에 반해, 상대적으로 deep learning은 feature를 만들어 줄 수 있는 구조를 만들면 그에따라 Feature를 알아서 생성해주므로 상대적으로 언어학에 대한 깊이 있는 지식이 없어도 분석이 가능하다. NLP의 Applicatio의 종류 네이버의 파파고, 구글의 구글 번역기 같은 번역 서비스를 예로 들수 있다. 여러 회사에서 특히 은행이나 카드사, 쇼핑몰등에서 많이 보았을 법한 챗봇 서비스도 NLP의 응용분야이다. 스캐터랩의 핑퐁이나, 심심이같은 챗봇 서비스들도 있다. 예를 들면, 구글 애드 센스 같은 서비스가 있다.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"순환 신경망(RNN) - 순차 데이터의 이해","slug":"deep_learning_08","date":"2020-01-12T06:25:39.000Z","updated":"2020-01-21T08:10:54.577Z","comments":true,"path":"2020/01/12/deep_learning_08/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/12/deep_learning_08/","excerpt":"","text":"순차 데이터의 이해 우리가 순환 신경망을 사용하는 이유는 입력을 순환 신경망으로 받거나 출력을 순환 신경망으로 내기 위해서이다. 일정한 시간차을 갖는 Time Series라면, x축이 특정 시간을 의미하는 Temporal Sequence와는 다르게 하나하나의 Step으로 간주한다. 일반적으로는 위에서 보는 것과 같이 Temporal Sequence를 보간하여 Time Series로 변환해 준 뒤에 사용한다. 개인 비서 서비스는 예를 들어, siri나 구글의 okay google 같은 서비스이다. 기본 적인 순환 신경망(Vanilla RNN) 앞서 말한 순차데이터를 입력받아 원하는 출력을 하려면, 기억시스템이 전제되어야 한다. CNN이나 Deep Neural Network, shallow NN은 Memoryless System이다. 다음 그림에서 볼 수 있듯이, RNN은 이전의 Network 구조와는 다르게 입력층에 n-1번째 step의 hidden layer를 n번째 데이터와 concatenation을 하여 사용하는 것이다. 이렇게 함으로써 이전의 모든 입력에 영향을 받는다. shallow Neural Network를 Deep Neural Network로 만들어 주었듯이, 동일하게 하여 Multi-Layer RNN을 만들 수 있다. 왼쪽의 노드들만 본다면 다음 Layer들의 이전 step의 hidden Layer를 가져온 것을 확인 할 수 있다. 하지만, 이런 구조는 Vanilla RNN과 다르게 Hidden Layer의 길이도 2배이상으로 늘어나기 때문에, 복잡도가 높아지게 되며, 현실적으로 학습이 잘 되지 않아 권장되지 않는다. 그 이유는 간단하게만 말하자면, 일반적인 Neural Network는 depth 방향으로만 gradient가 잘 학습되면 되지만, 이 구조는 input까지 Gradient의 영향을 주도록 해야하기 때문이다. 심화 순환 신경망 그렇다면 &#39;Vanilla RNN이 왜 잘 쓰이지 않는가?&#39;에 대한 가장 큰 이유를 앞서 언급했던 것과 같이 Gradient가 Input까지 타고 가서 학습을 잘 못하기 때문이라고 언급했다. 즉, Gradient Vanishing 문제라는 것이다. 검정색 선이 Input Gate이다. 빨간색 선은 Vanilla RNN에서의 Hidden State와 동일하다. 입력이 들어오고 이전 Hidden State를 받아서 같이 tanh activation function을 FC(Fully connected) Layer를 통과시켜 출력을 내주면 RNN의 Hidden Layer이기 때문이다. 해당 Layer에서 필요한 정보만을 출력층으로 내주고, 필요하지 않은 정보는 계속 기억하게끔 다음 time step으로 넘겨주어서 이전에 어떤 출력을 내주었었나를 Cell State와는 별개로 또 넘겨주어 기억하게끔 한다. 아래 그림에서 출력을 내보내기 위해 Cell State에서 tanh를 거쳐 주는데 이 작업은 다른 activation function들이 존재하는 노드들과 달리 FC layer로 이루어져 있지 않고 그냥 activation function만 거치게 된다. 그 이유는 Cell State가 Forget gate를 지나면서는 0~1사이의 값이 곱해지므로 크게 문제가 없지\u001d만 Input Gate를 지나면서 Feature가 추가적으로 더해질때 tanh를 지나면 -1~1사이의 값이 되므로 범위 -2~2로 늘어나게 되어 추후에 Gradient Explode가 일어날 수 있어 예방차원에서 tanh을 사용하는 것이기 때문이다. 아래 그림에서 $1 - Forget Gate$를 Input Gate로 사용하는 것은 Forget Gate에서 잊어버린 만큼만 Input Gate를 통해 채워 주는 의미로 해석할 수 있다. Input Gate를 통해 새로운 Feature가 추가되기에 앞서서 이전 Hidden State 정보를 얼마나 잊게 하느냐의 의미인데, 예를 들어 앞의 문장이 .을 통해 마쳐졌다면, 그 뒤의 문장은 다른 문장 구조를 띄게 되므로 0에 가깝게 하여 Reset을 시켜줄 것이다. 시간펼침 역전파 학습법(BPTT: Back Propagation Through Time) 순환신경망은 기존의 기본적인 역전파 학습법으로는 학습할 수 없다. 그렇다면, 어떻게 해야할까? 물론, 모델이 학습할때 언제 입력이 끝날지 모르기에 마지막 입력 같은 경우는 EOS(End Of Sequence)라는 특별한 미리정해준 하나의 토큰을 날려주는 경우가 많다. 아래의 그림은 Input의 시점에 따라 펼쳐져있다는 것을 이해하기 쉽게 펼쳐 놓은 것인데, 여기서 주의할 점은 아래의 RNN안의 Hyper parameter들은 모두 동일하다는 것이다. 즉 아래의 그림은 재귀적형태를 시간의 흐름상으로 나열해 놓은 것이라고 생각하면 된다. 위에서 언급했던 것과 같이 출력(또는 입력)의 길이가 정해져있지 않은 RNN의 경우, 아래는 마지막 출력에 EOS\u001d 출력을 내게끔 학습시켜야 모든 출력이 나왔다는 것을 알 수가 있다. 또한, Back propagation도 마찬가지로 각각의 출력에 대한 Loss값 부터 시작해서 Input지점까지 해주면 된다. 단일 입력, 다중 출력의 실제 모델에서는 입력이 없는 다른 층에서는 0을 입력하거나 미리 정해놓은 입력을 넣어주어 학습을 시키는 것이 일반적이다. 다중 입력에 대해서 다중 출력이 나오려면 2가지 상황이 있을 수 있다. 하나는 아래 그림에서와 같이 입력에 대해서 출력이 나오고 입력이 끝나면 출력도 끝나는 것이 있을 수 있다.이런 경우는 대표적으로, 동영상의 프레임 분류가 있다. 예를 들면, CF의 한 프레임이 입력으로 들어와 각 장면이 어떤 장면인지 서술하는 식으로의 분류를 들 수 있을 것이다. 또 다른 한 상황은 모든 입력을 받고 그 다음에 출력이 나오는 경우가 있다. 이 경우도 마찬가지로 입력의 길이가 언제 끝날지 모르므로 마지막 입력에 EOS를 날려 주어야 한다. 심화 순환 신경망의 수식적 이해 Vanilla RNN의 수식은 이전에 간단히 다루었다. 이제 LSTM과 GRU도 수식으로 접근해 보자. 특징이 여러차원으로 되어있는데, 이 Forget gate 또한 여러 차원으로 되어있어 특징별로 기억할지 말지를 결정할 수 있다. Reset gate는 Hidden state에서 바로 잊는 Forget gate와는 다르게 현재 Feature를 뽑을 때 얼만큼 잊어줄 것인가를 결정하는 부분이다. 큰 맥락에서는 기억하고 있어야 하지만, 현재 Feature를 뽑을 때는 방해가 될 수 있는 정보를 잊게하는 역할이다. 예를 들어 아래와 같은 상황일때, 마지막 박 아무개의 답을 추론하고자 한다면, 먼저 “나는 사과가 좋다.”, “너는 과일을 싫어한다.”라는 문장 2개는 \u001c분리가 된 문장이지만 “나는 사과가 좋다” 내에서는 ‘나’하고 ‘사과’는 잘 기억이 되어야 하지만 “너는 과일을 싫어한다”라는 문장은 다른 문장이므로 기억이 안되어야 할 것이다. 하지만, “나는 어떤 과일이 먹고 싶을까?”에 답을 하려면, 최근에 문장인 “너는 과일을 싫어한다”에서는 추론할 때 필요한 정보가 없기 때문에 그 이전 문장인 “나는 사과가 좋다.”는 context를 계속해서 가지고 있어야한다. 이 정보가 Hidden state를 타고 움직여야 하는 정보이\u001d고, 여기서 “나는 사과가 좋다.”와 “너는 과일을 싫어한다.”라는 문장을 구분하여 단계적으로 활용하지 않기 위한 작업이 Reset gate를 통한 작업이다. Reset gate와 다르게 Hidden state에 직접적으로 곱해져서 이전 time step Hidden State에서 기억을 잊어버리게 하는 역할을 한다. 잊어버린 부분만큼을 다시 새로운 정보로 보충하기 위해 1에서 뺀 만큼을 새로운 입력의 결과에 곱해준다. 이전 time step의 Hidden state가 들어왔을 때 reset gate를 통해 제어가 된 것을 가지고 현재 Feature들을 뽑아주게 되고, Forget gate에서 의해서 제어가 된 만큼 넘어오고 Forget gate에 의해서 상보적인 만큼 다시 새로 뽑은 Feature를 입력을 받아서 다음 출력으로 나가게 된다. 그렇기에 값이 -1~1로 bound되어있어 LSTM과 다르게 tanh함수가 필요하지 않다. 순차 신경망에서 Tensor의 이해 데이터가 Feature같은 경우에는 항상 꽉차게 되는데, 순차데이터 같은 경우에는 길이가 L보다 짧을 수 있다. 그런 경우에는 앞을 0으로 채워준다.(pre-padding) 출력이 나오는 시점은 고정되므로 일관되게 앞쪽으로 정렬된 출력이 나올 수 있게 하기 위해서 뒷부분을 0으로 채운다. 순환 신경망의 학습법 시간에 대해서 펼쳐있고, 추가적으로 Batch로도 펼쳐 주어야 하는데, 즉, 아래와 같은 구조가 Batch size만큼 더 있어주어야 한다는 의미이다. 그래서 시간적으로 펼칠 때 역전파를 위한 추가적인 메모리가 필요하다. 일반적인 CNN이나 DNN은 시간적으로 펼치는 것이 없기 때문에 Batch에 대해서 크게 엄격하지 않다. 하지만 RNN은 아래와 같이 시간적으로 펼치기 때문에 Batch size를 늘리는데 엄격하다. 순차 데이터의 길이 L이 매우 클 경우, 시간 펼침이 늘어나면서 필요 메모리가 L배 증가한다. 그 이유는 길이가 1개 씩 늘어날 때마\u001c다 펼침을 하나씩 더 해야하기 때문이다. 이 때 B(Batch)를 한번에 계산하므로, 얕은 신경망에 비해 훨씬 큰 메모리가 필요. 길이 L의 입력을 길이 T로 쪼개어 순서대로 학습한다. 즉, Time step이 T 이상 떨어진 입-출력 관계는 학습되지 않는다. Hidden state와 Cell state를 통해 Forward propagation에서는 잘 추론 할 수 있도록 넘겨 주지만, Back propagation에서는 그 관계를 서로 넘겨주지 못한다. 만약 전부다 연결시킨 관계를 학습시켜야 한다면 Truncated BPTT가 아닌 길이가 L인 모든 데이터를 학습시켜야 한다. 그러므로 Truncated BPTT를 사용할 시 반드시 영향을 주는 데이터 사이의 관계를 침해하지 않게 T로 적절하게 나누어졌는지, 우리가 학습하고자 하\b는 것이 어느 정도의 시간차이까지 우리가 연관성을 봐야 하는지를 염두해 두고 학습을 시켜야 한다. 만약 연관성이 있는 데이터의 주기(데이터간의 시점 차이)가 크고 Gradient가 끊기지 않고 연결되어 업데이트가 이루어져야 한다면, 최대한 Batch Size를 극단적 낮추고, 최대한 Memory가 큰 GPU를 사용해서 최대한 긴 길이를 학습해 주는 방법을 사용해야 할 것이다.","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"DenseNet 구현 및 학습","slug":"deep_learning_07","date":"2020-01-12T06:23:36.000Z","updated":"2020-01-20T16:27:52.245Z","comments":true,"path":"2020/01/12/deep_learning_07/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/12/deep_learning_07/","excerpt":"","text":"DenseNetwork 구현 및 학습 필자는 구글 colab을 통해 학습시켰으며, @tf.function을 사용하기위해 tensorflow 2.0 버젼을 시용하였다. 1!pip install tensorflow==2.0.0-beta1 사용할 라이브러리 import 12import tensorflow as tfimport numpy as np 위에서 설치한 tensorflow의 버전이 2.0인지 확인 1print(tf.__version__) Hyper parameter를 설정 1EPOCHS = 10 DenseUnit 구현1234567891011121314151617class DenseUnit(tf.keras.Model): def __init__(self, filter_out, kernel_size): super(DenseUnit, self).__init__() # batch normalization -&gt; ReLu -&gt; Conv Layer # 여기서 ReLu 같은 경우는 변수가 없는 Layer이므로 여기서 굳이 initialize 해주지 않는다. (call쪽에서 사용하면 되므로) # Pre-activation 구조는 똑같이 가져가되, concatenate 구조로 만들어주어야함에 주의하자!! self.bn = tf.keras.layers.BatchNormalization() self.conv = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same') self.concat = tf.keras.layers.Concatenate() def call(self, x, training=False, mask=None): # x : (Batch 갯수, Height, width, Channel_in) # training 꼭 잊어버리지 말자!! h = self.bn(x, training=training) h = tf.nn.relu(h) h = self.conv(h) # h : (Batch, height, width, filter_output) zero-padding을 했으므로 filter만 바뀜 return self.concat([x, h]) # (Batch, height, width, (channel_in + filter_output)) DenseLayer 구현1234567891011class DenseLayer(tf.keras.Model): def __init__(self, num_unit, growth_rate, kernel_size): super(DenseLayer, self).__init__() self.sequence = list() for idx in range(num_unit): self.sequence.append(DenseUnit(growth_rate, kernel_size)) def call(self, x, training=False, mask=None): for unit in self.sequence: x = unit(x, training=training) return x Transition Layer 구현 Maxpooling을 해줄 때 필요함. 예를 들어, DenseLayer를 사용을 하게 되면, Growth_rate=32, num_unit=8인 경우에는, 32X8 만큼 channel의 수가 급격하게 증가하기 때문에 너무 커질 수 있다. 이럴 경우 이 Transition Layer를 통해 Pooling 전에 channel의 수를 조절해주기 위해 사용되는 것이다. 123456789101112class TransitionLayer(tf.keras.Model): def __init__(self, filters, kernel_size): super(TransitionLayer, self).__init__() # transition을 할 경우에는 이렇게 convolution을 해서 단순히 filter 개수를 변경만 해준뒤 # 그 다음에 Maxpooling을 해주는 식으로 구현이 된다. self.conv = tf.keras.layers.Conv2D(filters, kernel_size, padding='same') self.pool = tf.keras.layers.MaxPool2D() def call(self, x, training=False, mask=None): # 여기서는 Batch normalization이 없기 때문에 training을 안써줘도 된다. x = self.conv(x) return self.pool(x) 모델 정의123456789101112131415161718192021222324252627282930313233class DenseNet(tf.keras.Model): def __init__(self): super(DenseNet, self).__init__() self.conv1 = tf.keras.layers.Conv2D(8, (3, 3), padding='same', activation='relu') # 28x28x8 # num_unit=2, growth_rate=4, kernel_size=(3,3) # num_unit은 ResNet과 동일 self.dl1 = DenseLayer(2, 4, (3, 3)) # 28x28x(8+2*4) self.tr1 = TransitionLayer(16, (3, 3)) # 14x14x16 self.dl2 = DenseLayer(2, 8, (3, 3)) # 14x14x(16 + 2*8) self.tr2 = TransitionLayer(32, (3, 3)) # 7x7x32 self.dl3 = DenseLayer(2, 16, (3, 3)) # 7x7x(32+2*16) self.flatten = tf.keras.layers.Flatten() self.dense1 = tf.keras.layers.Dense(128, activation='relu') self.dense2 = tf.keras.layers.Dense(10, activation='softmax') def call(self, x, training=False, mask=None): x = self.conv1(x) x = self.dl1(x, training=training) x = self.tr1(x) x = self.dl2(x, training=training) x = self.tr2(x) x = self.dl3(x, training=training) x = self.flatten(x) x = self.dense1(x) return self.dense2(x) 학습, 데스트 루프 정의1234567891011121314151617181920# Implement training loop@tf.functiondef train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy): with tf.GradientTape() as tape: predictions = model(images, training=True) loss = loss_object(labels, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train_loss(loss) train_accuracy(labels, predictions)# Implement algorithm test@tf.functiondef test_step(model, images, labels, loss_object, test_loss, test_accuracy): predictions = model(images, training=False) t_loss = loss_object(labels, predictions) test_loss(t_loss) test_accuracy(labels, predictions) 데이터셋 준비12345678910mnist = tf.keras.datasets.mnist(x_train, y_train), (x_test, y_test) = mnist.load_data()x_train, x_test = x_train / 255.0, x_test / 255.0x_train = x_train[..., tf.newaxis].astype(np.float32)x_test = x_test[..., tf.newaxis].astype(np.float32)train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) 학습 환경 정의모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의12345678910111213# 모델 생성model = DenseNet()# 손실함수 정의 및 최적화 기법 정의loss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()# 평가지표 정의train_loss = tf.keras.metrics.Mean(name='train_loss')train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')test_loss = tf.keras.metrics.Mean(name='test_loss')test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy') 학습 루프 동작 어떤 것은 DenseNet이 성능이 더 좋게 나오고 어떤 것은 ResNet이 더 좋게 나오기도 한다. 또한 parameter에 따라 다르다. Resnet은 좀 더 안정적이게 수렴하지만 Densenet은 좀 fluctuate하다는 점을 유의하자. 123456789101112131415161718for epoch in range(EPOCHS): for images, labels in train_ds: train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy) for test_images, test_labels in test_ds: test_step(model, test_images, test_labels, loss_object, test_loss, test_accuracy) template = \"Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;\" print(template.format(epoch+1, train_loss.result(), train_accuracy.result() * 100, test_loss.result(), test_accuracy.result() * 100))#train_loss.reset_states()#train_accuracy.reset_states()#test_loss.reset_states()#test_accuray.reset_states()","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"Residual Network 구현 및 학습","slug":"deep_learning_06","date":"2020-01-12T06:17:16.000Z","updated":"2020-01-20T15:58:35.374Z","comments":true,"path":"2020/01/12/deep_learning_06/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/12/deep_learning_06/","excerpt":"","text":"Residual Network GoogLeNet 이후에 나온 모델로 Residual 구조를 Skip connection 구조를 갖으며, pre-activation을 갖는 Residual unit을 먼저 만든 후에서 Resnet Unit을 연결하야 만들 ResnetLayer를 만들어 Residual Layer를 구현할 것이다. 그 후 전체적인 ResNet Model를 생성하여 앞서 구현해본 VGG-16과 성능을 비교하기 위해 동일한 mnist 데이터를 통해 학습과 검증을 해볼 것이다. DenseNetwork 구현 및 학습 필자는 구글 colab을 통해 학습시켰으며, @tf.function을 사용하기위해 tensorflow 2.0 버젼을 시용하였다. 1!pip install tensorflow==2.0.0-beta1 사용할 라이브러리 import 12import tensorflow as tfimport numpy as np 위에서 설치한 tensorflow의 버전이 2.0인지 확인 1print(tf.__version__) Hyper parameter를 설정 1EPOCHS = 10 Residual Unit 구현123456789101112131415161718192021222324252627282930313233class ResidualUnit(tf.keras.Model): def __init__(self, filter_in, filter_out, kernel_size): super(ResidualUnit, self).__init__() # batch normalization -&gt; ReLu -&gt; Conv Layer # 여기서 ReLu 같은 경우는 변수가 없는 Layer이므로 여기서 굳이 initialize 해주지 않는다. (call쪽에서 사용하면 되므로) self.bn1 = tf.keras.layers.BatchNormalization() self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding=\"same\") self.bn2 = tf.keras.layers.BatchNormalization() self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding=\"same\") # identity를 어떻게 할지 정의 # 원래 Residual Unit을 하려면 위의 순서로 진행한 뒤, 바로 X를 더해서 내보내면 되는데, # 이 X와 위의 과정을 통해 얻은 Feature map과 차원이 동일해야 더하기 연산이 가능할 것이므로 # 즉, 위에서 filter_in과 filter_out이 같아야 한다는 의미이다. # 하지만, 다를 수 있으므로 아래와 같은 작업을 거친다. if filter_in == filter_out: self.identity = lambda x: x else: self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding=\"same\") # 아래에서 batch normalization은 train할때와 inference할 때 사용하는 것이 달라지므로 옵션을 줄것이다. def call(self, x, training=False, mask=None): h = self.bn1(x, training=training) h = tf.nn.relu(h) h = self.conv1(h) h = self.bn2(h, training=training) h = tf.nn.relu(h) h = self.conv2(h) return self.identity(x) + h Residual Layer 구현12345678910111213141516171819class ResnetLayer(tf.keras.Model): # 아래 arg 중 filter_in : 처음 입력되는 filter 개수를 의미 # Resnet Layer는 Residual unit이 여러개가 있게끔해주는것이므로 # filters : [32, 32, 32, 32]는 32에서 32로 Residual unit이 연결되는 형태 def __init__(self, filter_in, filters, kernel_size): super(ResnetLayer, self).__init__() self.sequnce = list() # [16] + [32, 32, 32] # 아래는 list의 length가 더 작은 것을 기준으로 zip이 되어서 돌아가기 때문에 # 앞의 list의 마지막 element 32는 무시된다. # zip([16, 32, 32, 32], [32, 32, 32]) for f_in, f_out in zip([filter_in] + list(filters), filters): self.sequnce.append(ResidualUnit(f_in, f_out, kernel_size)) def call(self, x, training=False, mask=None): for unit in self.sequnce: # 위의 batch normalization에서 training이 쓰였기에 여기서 넘겨 주어야 한다. x = unit(x, training=training) return x 모델 정의1234567891011121314151617181920212223242526272829class ResNet(tf.keras.Model): def __init__(self): super(ResNet, self).__init__() self.conv1 = tf.keras.layers.Conv2D(8, (3,3), padding=\"same\", activation=\"relu\") # 28X28X8 self.res1 = ResnetLayer(8, (16, 16), (3, 3)) # 28X28X16 self.pool1 = tf.keras.layers.MaxPool2D((2,2)) # 14X14X16 self.res2 = ResnetLayer(16, (32, 32), (3, 3)) # 14X14X32 self.pool2 = tf.keras.layers.MaxPool2D((2,2)) # 7X7X32 self.res3 = ResnetLayer(32, (64, 64), (3, 3)) # 7X7X64 self.flatten = tf.keras.layers.Flatten() self.dense1 = tf.keras.layers.Dense(128, activation=\"relu\") self.dense2 = tf.keras.layers.Dense(10, activation=\"softmax\") def call(self, x, training=False, mask=None): x = self.conv1(x) x = self.res1(x, training=training) x = self.pool1(x) x = self.res2(x, training=training) x = self.pool2(x) x = self.res3(x, training=training) x = self.flatten(x) x = self.dense1(x) return self.dense2(x) 학습,테스트 루프 정의12345678910111213141516171819202122# Implement training loop@tf.functiondef train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy): with tf.GradientTape() as tape: # training=True 꼭 넣어주기!! predictions = model(images, training=True) loss = loss_object(labels, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train_loss(loss) train_accuracy(labels, predictions)# Implement algorithm test@tf.functiondef test_step(model, images, labels, loss_object, test_loss, test_accuracy): # training=False 꼭 넣어주기!! predictions = model(images, training=False) t_loss = loss_object(labels, predictions) test_loss(t_loss) test_accuracy(labels, predictions) 데이터셋 준비12345678910mnist = tf.keras.datasets.mnist(x_train, y_train), (x_test, y_test) = mnist.load_data()x_train, x_test = x_train / 255.0, x_test / 255.0x_train = x_train[..., tf.newaxis].astype(np.float32)x_test = x_test[..., tf.newaxis].astype(np.float32)train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) 학습 환경 정의모델 생성, 손실 함수, 최적화 알고리즘, 평가지표 정의12345678910111213# 모델 생성model = ResNet()# 손실함수 정의 및 최적화 기법 정의loss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()# 평가지표 정의train_loss = tf.keras.metrics.Mean(name='train_loss')train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')test_loss = tf.keras.metrics.Mean(name='test_loss')test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy') 학습 루프 동작12345678910111213for epoch in range(EPOCHS): for images, labels in train_ds: train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy) for test_images, test_labels in test_ds: test_step(model, test_images, test_labels, loss_object, test_loss, test_accuracy) template = \"Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;\" print(template.format(epoch+1, train_loss.result(), train_accuracy.result() * 100, test_loss.result(), test_accuracy.result() * 100))","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"Scrapy 웹 크롤링 02 - Spider, Scrapy selectors, Items","slug":"Crawling_01","date":"2020-01-10T18:39:47.000Z","updated":"2020-01-28T08:26:14.810Z","comments":true,"path":"2020/01/11/Crawling_01/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/11/Crawling_01/","excerpt":"","text":"Spider Spider의 종류 (참고로, 아래 3가지 종류의 Spider는 잘 사용되지 않는다.) CrawlSpider XMLFeedSpider CSVFeedSpider SitemapSpider 12# 여러사이트를 크롤링하기 위한 spider를 생성scrapy genspider many_site hub.scraping.com spider 폴더의 many_site.py파일에서 코드를 작성하기에 앞서 settings.py에서 naver와 daum은 robots.txt에서 크롤링을 불허하기에 다음과 같은 수정작업을 해주어야 크롤링이 가능하다. 아래의 코드처럼 여러 도메인을 크롤링할 수 있는 방법은 크게 3가지 정도가 있다. 다음과 같이 1번째 방법을 사용해서 로깅 및 분기처리로 여러 사이트를 크롤링할 수 있게끔 코드를 수정해 주었다. Selectorxpath selector 도움 사이트 https://docs.scrapy.org/en/latest/topics/selectors.html#working-with-xpath www.nextree.co.kr/p6278 css selector 도움 사이트 https://docs.scrapy.org/en/latest/topics/selectors.html#extension-to-css-selectors crawling시 활용 tip 타겟 데이터는 크롬 개발자 도구 사용 선택자 연습 팁 : scrapy shell 에서 테스트(효율성) scrapy shell 도메인 중요(완전 동치는 아니다!) get() == extract_first() getall() == extract() CSS 선택자 div#chan div : (자손) chan이라는 class속성값으로 갖는 div tag의 아래에 존재하는 모든 div div#chan &gt; div : (자식) chan이라는 class속성값으로 갖는 div tag의 직계자식 div들 ::text -&gt; 노드의 텍스트만 추출 ::attr(name) -&gt; 노드 속성값 추출 get(default=’’) : get으로 추출할 때 해당사항이 없다면 공백으로 출력 예시) response.css(‘title::text’).get() : title tag의 텍스트만 추출 response.css(‘div &gt; a::attr(href)’).getall() : div tag의 자식 a tag의 href속성값 전부 추출 Xpath 선택자 nodename : 이름이 nodename 선택 text() -&gt; 노드 텍스트만 추출 / : 루트부터 시작 // : 현재 node 부터 문서상의 모든 노드 조회 . : 현재 node .. : 현재 node의 부모노드 @ : 속성 선택자 예시) response.xpath(‘/div’) : 루트노드부터 모든 div tag 선택 response.xpath(‘//div[@id=”id”]/a/text()’).get() : div tag 중 id가 ‘id’인 자식 a tag의 텍스트 하나만 추출 혼합 사용 가능!! response.css(‘img’).xpath(‘@src’).getall() 실습) w3school(웹에 관한 정보들이 있는 사이트) 실습 목표 : nav 메뉴 이름 크롤링 실습 과정 : shell 실행 -&gt; 선택자 확인 -&gt; 코딩 -&gt; 데이터 저장(프로그램 테스트) Items 구조적으로 데이터를 크롤링할 수 있게 해주는 역할을 한다. 예를 들면 내가 크롤링할 데이터를 정확하게 구분(신문기사의 이름, 본문, 이미지 이렇게 구조적으로 구분)하게 구조적으로 규칙을 정하고 그 규칙들을 Items라는 파일안에 작성하여 나중에 Items를 return하\u001d면 명확하게 구분된 우리가 원하는 여러가지 형식으로 저장할 수 있다. spider는 직접 크롤링을 하는 역할, Items는 크롤링 될 타겟 데이터를 명확히 해주는 역할이라고 생각하면 될 것이다. Scrapy Item장점 1) 수집 데이터를 일관성있게 관리 가능 2) 데이터를 사전형(Dict)로 관리, 오타 방지 3) 추후 가공 및 DB 저장 용이 Items를 사용한 scrapy는 새로운 사이트를 크롤링할 것이므로 새로운 spider를 만들어준다. 1scrapy genspider using_items itnews.com items.py 파일에서 우리의 타켓 데이터를 정의해준다. items.py를 활용하기 위해 import를 할 경우 다음과 같이 절대경로를 사용한 path 추가 방법을 사용해야한다. 다음과 같이 items의 ItArticle class를 활용하여 spider를 좀 더 깔끔하게 작성할 수 있다.","categories":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/categories/crawling/"}],"tags":[]},{"title":"Scrapy 웹 크롤링 01 - 환경설정 및 기초","slug":"Crawling_00","date":"2020-01-09T12:08:12.000Z","updated":"2020-01-23T17:17:25.112Z","comments":true,"path":"2020/01/09/Crawling_00/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/09/Crawling_00/","excerpt":"","text":"Scrapy VS Beautiful SoupBeautiful Soup Beautiful Soup는 웹 상의 정보를 빠르게 크롤링 하기위한 도구이며, 정적인 정보를 가져 올 수 있다. 즉, 해당 API(URL)에 요청했을때 바로 가져올수 있는 정보들만 가져올 수 있다. 시간이 좀 더 걸린 후에 나오는 정보들은 가져올 수 없다는 것이다. 진입 장벽이 매우 낮고 간결해서, 입문 개발자에게 안성맞춤이다. 그리고 이 라이브러리는 스스로 크롤링을 하는 것이 아니라 urlib2 또는 requests 모듈을 통해 HTML 소스를 가져와야 한다. Scrapy Scrapy는 Python으로 작성된 Framework이며, spider(bot)을 작성해서 크롤링을 한다. Scrapy에서는 직접 Beautiful Soup 이나 lxml을 사용할 수 있다. 하지만 Beautiful Soup에서는 지원하지 않는 Xpath를 사용할 수 있다. 또한, Xpath를 사용함으롴써 복잡한 HTML소스를 쉽게 크롤링 할 수 있게 해준다. 또한 Xpath를 통한 crawling이 가능한 모듈로는 selenium도 존재한다. selenium도 Scrapy와 연동해서 가능하다. Anaconda env 먼저 사전에 anaconda를 통해 가상환경을을 만들어준다. 1234567891011121314# env 생성conda create -n env_name python=3.5# env 리스트 보기conda env list# env 활성화conda activate env_name# env 비활성화conda deavtivate# env 삭제conda env remove -n env_name Scrapy 환경설정 먼저 가상환경을 활성화시켜준 후에, spider bot을 만들 폴더의 상위 폴더에서 다음의 명령어를 실행시켜준다. 12345conda activate env_namecd ..scrapy startproject project_name 다음과 같이 설정한 project명을 갖는 폴더가 만들어지며, 필자는 section01_2라고 명명했다. 위의 단계까지 실행했다면, 다음과 같은 출력이 보일 것이다. 빨간줄 아래에 나와있는 예시 명령어를 따라서 실행시키면 spider bot을 만들수 있다. 또한, 모든 앞으로의 모든 명령어는 scrapy.cfg라는 파일이 존재하는 directory path에서 해야한다. genspider scrapy에서 가장 중요한 spider class를 만들어준다. 123456789# spider를 만들기 위해 명령어를 실행하려면 scrapy.cfg파일의 경로로 이동해야하기 때문에cd section01_2# scrapy.cfg파일이 존재하는지 다시 한번 확인ls# https://blog.scrapinghub.com/은 crawling 테스트를 위한 사이트로 유명하다.# https://blog.scrapinghub.com/이라는 사이트를 크롤링할 testspider라는 이름으로 spider 을 만들어라는 명령어scrapy genspider testspider blog.scrapinghub.com 다음과 같은 출력결과를 볼 수 있으며, section01_2에 spiders라는 폴더의 testspider라고 만들어졌다는 것을 의미한다. 12345678# 만들어진 spider 파일 확일을 위해 이동cd section01_2/spiders# 위에서 만들어 놓았던 testspider라는 spider가 있는지 확인ls# testspider.py 확인vim testspider.py 먼저 straturl과 우리가 크롤링하려는 URL endpoint가 https인지 확인한 후 고쳐준다.(여기서 필자는 vim으로 수정하였기에 pep8에 의거하여 space 4번으로 indent를 사용하였다. space와 tap을 번갈아가며 사용하면 python interpreter가 다르게 인식하므로 에러를 발생시킨다!) 앞으로의 실습에 헷갈림을 방지하기 위해서 name을 test1으로 변동해주었고, allowed_domains과 start_urls를 보면 설정해 놓은 대로 들어가 있는 것을 알 수 있다. 여기서 scrapy는 allowed_domains과 start_urls가 리스트 구조로 되어있는데 다른 URL과 도메인들을 추가하면 해당 사이트들을 돌아가며 크롤링을 할 수 있는 병렬처리가 가능하다는 것이 가장 큰 장점이다. 추후에 설명하겠지만, 눈치 빠르신 분들은 아래 parse함수에서 response를 parameter로 받는 함수이므로 이 함수에 크롤링하고 싶은 부분에 대한 코드를 만들면 크롤링이 가능하다는 것을 알 것이다!! 혹시 response에서 어떤 명령어가 사용가능한지 보고 싶다면 runspider vs crawl runspider와 crawl의 차이점은 runspider는 spiders폴더에서 실행할 수 있고, crawl은 scrapy.cfg파일이 존재하는 폴더에서 실행하여햐 한다는 점이 차이점이다!! runspider 명령어를 통해 spider bot을 실행시키는 것은 단위 테스트라고 소위 불리는 방식을 할 때 유용하고 crawl은 우리가 원하는 구조를 다 만들어 놓은 후 테스트를 할 때나 실제로 크롤링을 할 경우 사용하는 것이 유용하다. 12345# runspider는 spiders 폴더에서 실행하여야한다.scrapy runspider testspider.py# crawl은 scrapy.cfg파일이 존재하는 path에서 실행시켜주어야한다.scrapy crawl test1 --nolog settings.py spider의 속성에 관련된 parameter들이 있는 파일이라고 생각하면 된다. 예를 들면, 아래의 그림에서 볼 수 있듯이 SPIDER MODULES는 현재 SPIDER의 위치를 의미하고, NEWSPIDER MODULE은 Spider를 새로 생성시 어느 위치에 추가되는지를 의미한다. ROBOTSTXT_OBEY는 robots.txt의 규칙에 의거하여 crawling을 하겠다는 의미이며, DOWNLOAD_DELAY는 몇초간격으로 서버에 요청을 할지에 대한 수치이다. 필자는 1로 정했는데 여기서는 1초마다라는 의미이다. 만약에 0.2라고 하게 되면 0.2초마다 서버에 요청하게 되어 서버에 부하를 일으키게 되면 심할경우 영구 van을 당할 수도 있기에 간격을 1초이상으로 하는 것을 권장한다. 실습)blog.scrapinghub.com에서 기사 제목들만 크롤링 하기! 위의 실습주제로 실습을 진행하기 위해서는 앞서 만들어본 spider 파일에서 parse함수를 수정해야할 것이다. 그에 앞서 크롤링할 blog.scrapinghub.com의 제목에 해당하는 css path를 보면 전체 html의 body 부분에서 div element 중 class의 이름이 post-header인 부분에만 존재하는 것을 개발자 도구를 통해 알아내었다. 다른 부분에 동일한 element나 class명을 가질 수도 있으므로 find를 해보아야한다! 다음과 같이 제목을 크롤링하기 위해 testspider.py을 수정해 주었다. 참고로 결과를 파일로 저장할때 scrapy가 지원하는 파일 형식은 json, jsonlines, jl, csv, xml, marshal, pickle이다. 결과를 저장할때 동일한 파일명과 확장자명을 가진 파일이 이미 존재한다면 그 파일에 데이터를 추가해주므로 주의하자! 12345# 필자는 spider 폴더에서 실행함.scrapy runspider testspider.py -o result.csv -t csv# result 파일인 result.csv가 만들어진 것을 확인할 수 있다.ls Requests를 사용하여 페이지 순회하며 크롤링 우리가 예를 들어 어떤 페이지내에서 여러 항목에 대해 해당 url로 이동 후 크롤링하고 다시 그 전 페이지로 돌아가서 다음 항목의 url로 이동 후 크롤링하는 이런 순회를 거쳐야하는 작업은 request나 selenium으로 하게되면 iterable한 코드를 통해 가능하게 되며, 그렇지 않다면, 동일한 구성을 지닌 페이지들이라면 함수를 여러번 실행하는 등의 multiprocessing을 통해 병렬처리를 따로 해주어여하는 불편함이 있다. 허나, scrapy는 코드 몇 줄로 가능하다. 먼저 앞의 spider말고 새로운 spider를 만들어 사용할 것이다. spider를 만든 설정은 위에서 만든 것과 동일하다. spider 이름만 pagerutine이라고 명명했을 뿐이다. 이전에 blog.scrapinghub.com의 페이지에 해당 10개의 기사들에 대한 path가 “div.post-header h2 &gt; a” 이며, a tag의 href 속성 값들을 통해 각각의 기사에 해당 페이지로 이동이 가능할 수 있다는 사실을 확인 할 수 있다. 첫번째 parse 함수에서 미리 추출한 url을 request하여 얻은 response를 다른 함수로 전달해주기 위해 Request 명령어를 사용하였으며, urljoin을 사용한 이유는 절대주소가 아닌 상대주소로 되어있는 경우 위\u001d의 start_urls에 설정해 놓은 주소를 앞에 붙여 절대 주소로 바꿔주는 기능이다. 물론 절대주소인 경우는 이런 작업을 생략한다. Scrapy Shell 사용법 쉽게 말해 이전에 크롤링을 할때 Spider 폴더에서 파일을 수정하고 테스트해보는 방식으로는 작업의 효율성이 떨어지므로 그 전에 css나 xpath selector를 테스트해볼 수 있는 것이 Shell mode이다. 123456789101112131415161718192021222324252627282930313233343536# shell 모드 접속scrapy shell################ shell 모드 접속했다고 가정 ################ url 설정(request하는 대상을 바꾸는 역할)fetch('url')quit################# 다른 방법 ############################# 위의 단계를 한번에 하는 방법scrapy shell https://blog.scrapinghub.com# response data가 무엇이 있는지 확인 할 수 있다. 소스페이지를 현재 내 컴퓨저에 가져와서 보여주는 방식view(response)# 예시fetch(https://daum.net)view(response)# response가 사용할수있는 method 확인dir(response)# 현재 reponse의 url 확인response.url# 현재 response의 body정보 확인response.body# 현재 reponse의 status 확인response.status# robot.txt\u001d에 크롤링이 허용되지 않았으면 shell script가 실행되지 않는다.# settings 파일에서의 설정 파라미터들을 동적으로 설정하며 실행 가능!scrapy shell https://daum.net --set=\"ROBOTSTXT_OBEY=False\"","categories":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/categories/crawling/"}],"tags":[]},{"title":"모형 성능 평가 지표","slug":"machine_learning_03","date":"2020-01-09T06:12:42.000Z","updated":"2020-01-15T09:24:04.397Z","comments":true,"path":"2020/01/09/machine_learning_03/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/09/machine_learning_03/","excerpt":"","text":"회귀(regression) 평가 지표 회귀의 평가를 위한 지표는 실제 값과 회귀 예측값의 차이 값을 기반으로 한 지표가 중심이다. 실제값과 예측값의 차이를 그냥 더하면 잔차의 합은 0이므로 지표로 쓸 수 없다. 이 때문에 잔차의 절대값 평균이나 제곱, 또는 제곱한 뒤 다시 루트를 씌운 평균값을 성능 지표로 사용한다. 평가 지표 수식 MAE(Mean Absolute Error) $MAE = \\frac{1}{n} \\sum_{i=1}^{n} \\lvert Y_{i} - \\hat{Y_{i}} \\rvert$ MSE(Mean Squared Error) $MSE = \\frac{1}{n} \\sum_{i=1}^{n} (Y_{i} - \\hat{Y_{i}})$ RMSE(Root Mean Squared Error) $RMSE = $\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (Y_{i} - \\hat{Y_{i}})^{2}}$ $R^{2}$ $R^{2} = \\frac{예측값의 Variance}{실제값 Variance} = \\frac{SSR}{SST}$ 이전에도 언급했던 것처럼 변수가 추가된다면 당연히 SSR의 수치가 높아지기 때문에 $R^{2}$값은 올라갈 수 밖에 없다. 그러므로 변수 수에 영향을 받지 않고 서로 비교할 수 있게끔 만들어 준 것이 수정된 결정계수이다. 분류(classification) 성능 지표 특히 imbalanced data에서 모형의 성능을 정확도 하나만을 가지고 성능을 평가한다면, 예를 들어, 100개중 90개는 세모고 10개는 네모라고 할 때 100개 모두 세모라고 예측해버리게 되면 정확도는 90%이므로 좋은 성능 지표라고 할 수 없다. 그러므로 imbalanced data에서의 성능 지표는 정확도(accuarcy) 보다는 정밀도(precision), 재현율(Recall)를 더 선호한다. 정밀도(precision)와 재현율(recall) 지표 중에 분류 모델의 업무 특성에 따라서 특정 평가 지표가 더 중요한 지표로 간주 될 수 있다. 재현율(recall)이 중요 지표인 경우는 실제 Positive 양성 데이터를 Negative로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우(FN이 Critical한 경우)이다. 예를 들어 암 판단 모형은 재현율(recall)이 훨씬 중요한 지표이다. 왜냐하면 실제 Positive인 암 환자를 Positive 양성이 아닌 Negative 음성으로 잘못 판단했을 경우 오류의 대가가 생명을 앗아갈 정도로 심각하기 때문이다. 반면에 실제 Negative인 건강한 환자를 암 환자인 Positive로 예측한 경우면 다시 한번 재검사를 하는 수준의 비용이 소모될 것이다. 또 다른 예로는, 금융 사기 적발 모델을 들 수 있다. 물론 고객에게 금융 사기 혐의를 잘못 씌우면 문제가 될 수 있기에 정밀도(Precision)도 중요 평가 지표지만, 업무적인 특성을 고려하면 재현율(Recall)이 상대적으로 더 중요한 지표입니다. 보통은 재현율(Recall)이 정밀도(Precision)보다 상대적으로 중요한 업무가 많지만, 정밀도가 더 중요한 지표인 경우도 있다. 예를 들어, 스팸메일 여부를 판단하는 모형의 경우 실제 Positive인 스팸 메일을 Negative인 일반 메일로 분류하더라도 사용자가 불편함을 느끼는 정도이지만, 실제 Negative인 일반 메일을 Positive인 스팸메일로 분류할 경우에는 메일을 아예 받지 못하게 돼 업무에 차질이 생긴다. 정밀도(Precision)이 상대적으로 더 중요한 지표인 경우는 실제 Negative 음성인 데이터 예측을 Positive 양성으로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우(FP가 Critical한 경우)이다. 재현율(Recall)과 정밀도(Precision) 모두 TP를 높이는 데 동일하게 초점을 맞추지만, 재현율(Recall)은 FN를 낮추는데, 정밀도(Precision)는 FP를 낮추는데 초점을 맞춘다. 이 같은 특성 때문에 재현율(Recall)과 정밀도(Precision)은 서로 보완적인 지표로 분류의 성능을 평가하는데 적용된다. 가장 좋은 성능 평가는 재현율(Recall)과 정밀도(Precision) 모두 높은 수치를 얻는 것이다. 반면에 둘 중 어느 한 평가 지표만 매우 높고, 다른 수치는 매우 낮은 결과를 나타내는 경우에는 바람직하지 않다. 정밀도(Precision)/ 재현율(Recall) Trade-off 분류하려는 업무의 특성상 정밀도(Precision) 또는 재현율(Recall)이 특별히 강조돼야 할 경우 분류의 결정 임계값(Threshold)을 조정해 정밀도(Precision) 또는 재현율(Recall)의 수치를 높일 수 있다. 하지만 정밀도(Precision)와 재현율(Recall)은 상호 보완적인 평가 지표이기 때문에 어느 한쪽을 강제로 높이면 다른 하나의 수치는 떨어지기 쉽다. 이를 정밀도(Precision)/재현율(Recall)의 Trade-off라고 부른다. scikit-learn에서 각각의 분류모델들은 predict_proba의 결과를 Threshold(보통은 0.5)보다 같거나 작으면 0값으로, 크면 1값으로 변환해 반환하는 Binarizer 클래스를 사용하여 predict의 결과를 계산하여 반환해 준다. 만약 임계값을 낮추면 재현율(Recall)값이 올라가고 정밀도(Precision)가 떨어질 것이다. 그 이유는 임계값은 Positive 예측값을 결정하는 확률의 기준이 되는데 임계값을 0.5에서 0.4로 낮추면 그만큼 Positive 예측을 더 너그럽게 하기 떄문에 True로 예측하는 값이 많아지게 된다. Positive 예측을 많이 하다보니 실제 양성을 음성으로 예측하는 횟수가 상대적으로 줄어들기 때문이다. 정밀도(Precision)과 재현율(Recall)의 맹점 Positive 예측의 임계값을 변경함에 따라 정밀도(Precision)와 재현율(Recall)의 수치가 변경된다. 임계값의 이러한 변경은 업무 환경에 맞게 두 개의 수치를 상호 보완할 수 있는 수준에서 적용돼야 한다. 그렇지 않고 단 하나의 성능 지표 수치를 높이기 위한 수단으로 사용돼서는 안된다. 각각의 지표를 극단적으로 높일 수는 있고, 정밀도(Precision) 또는 재현율(Recall) 중 하나에 상대적인 중요도를 부여해 각 예측 상황에 맞는 분류 알고리즘을 튜닝할 수 있지만, 그렇다고 정밀도(Precision)/재현율(Recall) 중 하나에 상대적인 중요도를 부여해 각 예측 상황에 맞는 분류 알고리즘을 튜닝할 수 있지만, 그렇다고 정밀도(Precision)/재현율(Recall) 하나만 강조하는 상황이 돼서는 안된다. F1-score는 정밀도(Precision)와 재현율(Recall)을 결합한 지표로 어느 한쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가진다. 여기서 또 한가지 주의할 점은 정밀도(Precision)와 재현율(Recall)의 조화평균값이라 해서 무조건 F1-score가 높은 것이 좋은 모형은 아니라는 점이다. 정밀도(Precision)과 재현율(Recall) 그리고 F1-score 모두 구한 후 비교하여 적합한 모형을 선정하는 것이 중요하다! F1 = \\frac{2}{\\frac{1}{Recall} + \\frac{1}{Precision}} ROC Curve와 이에 기반한 AUC score는 이진 분류의 예측 성능 측정에서 중요하게 사용되는 지표이다. ROC Curve(Receiver Operation Characteristic Curve)는 일반적으로 의학분야에서 많이 사용되지만, 머신 러닝의 이진 분류 모델의 예측 성능을 판단하는 중요한 지표이다. ROC Curve는 FRR(False Positive Rate)이 변할 때 TPR(True Positive Rate)이 어떻게 변하는지를 나타내는 곡선이다. FPR을 X축으로 하고 FPR을 0부터 1까지 변경하면서, TPR을 Y축으로 잡아 FPR에 변화에 따른 TPR의 변화가 곡선 형태로 나타난다. 분류결정 임계값은 Positive 에측값을 결정하는 값이므로 FPR을 0으로 만들려면 1로 지정하면 된다. TPR은 재현율(Recall)과 동일하며, 민감도라고도 불린다. 가운데 직선은 ROC Curve의 최저값(AUC는 0.5)이다. ROC 곡선이 가운데 직선에 가까울수록 성능이 떨어지는 것이며, 멀어질수록 성능이 뛰어나다는 것이다. 일반적으로 ROC Curve 자체는 FPR과 TPR의 변화값을 보는 데 이용하며 분류의 성능 지표로 사용되는 것은 ROC Curve 면적에 기반한 AUC 값으로 결정한다. AUC(Area Under Curve)값은 ROC Curve 밑의 면적을 구한 것으로서 일반적으로 1에 가까울수록 좋은 수치이다. AUC 수치가 커지려면 FPR이 작음 상태에서 얼마나 큰 TPR을 얻을 수 있느냐가 관건이다. 가운데 직선을 랜덤 수준의(동전 던지기 수준) 이진 분류 AUC 값으로 0.5이다. 따라서 보통의 분류는 0.5이상의 AUC값을 가지낟. TPR(민감도) = \\frac{TP}{TP+FN}TNR(특이성) = \\frac{TN}{FP+TN})FPR = 1 - TNR = \\frac{FP}{FP+TN}","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"Regression(02) - 다중선형회귀 및 다중공선성","slug":"machine_learning_02","date":"2020-01-08T14:22:36.000Z","updated":"2020-01-15T13:27:04.785Z","comments":true,"path":"2020/01/08/machine_learning_02/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/08/machine_learning_02/","excerpt":"","text":"다중 선형 회귀 다중회귀방정식에서 회귀계수에 대한 해석은 자주 혼동되는 것 중 하나이다. 단순회귀방정식은 직선을 표현하지만 다중회귀방정식은 평면(독립(설명)변수가 두개인 경우) 혹은 초평면(독립(설명)변수가 두개보다 많은 경우)을 표현한다. 위의 예에서 회귀계수의 해석은 다른 변수들이 고정되어 있을때 TV가 1단위 증가할 때 매출액은 0.046단위 증가한다고 해석할 수 있다. 회귀 계수 $\\beta_{j}$는 $X_{j}$를 제외한 나머지 모든 예측 변수들을 상수로 고정시킨 상태에서 $X_{j}$의 한 단위 증가에 따른 Y의 증분으로 해석될 수 있다. 변화의 크기는 다른 예측 변수들이 어떤 값으로 고정되어 있는지에 의존하지 않는다. 또 다른 해석은 $\\beta_{j}$가 다른 독립(설명)변수들에 의하여 종속(반응)변수 Y가 조정된 후에 Y에 대한 $X_{j}$의 공헌도를 의미한다. 이는 예를 들어 $Y = \\beta{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\epsilon $일 때, $[Y~X_{1}로 얻은 잔차] ~ [X_{2}~X_{1}]$에서의 계수와 동일하다. Y와 $X_{2}$ 각각으로부터 $X_{1}$의 선형효과를 제거한 후 Y에 미치는 $X_{2}$의 효과를 나타내기 때문이다.다중 선형 회귀 계수 검정 단순 선형 회귀와 동일하게 각각의 회귀계수가 통계적으로 유의미한지를 검정하는 것은 동일하다. 하지만, 다중 선형 회귀는 다음과 같이 전체 회귀계수가 의미있는지에 대한 검정도 하게 된다. 여기서 회귀계수가 0에 가깝고 standard error를 더하고 뺀 범위내에 0이 포함된다면 그 변수 또한 유의미하더라도 제거해야할 것이다. 또한 여기서 잔차의 정규성이라는 가정을 만족할 때 t 통계량의 절대값이 크거나 대응되는 p-value가 더 작다면 독립(설명)변수와 종속(반응)변수 사이의 선형관계가 더 강함을 의미한다. 위에서 언급하고 있는 개별적인 회귀계수 $\\beta$에 대한 검정 이외에, 여러 가지 다른 형태의 가설들이 선형모형의 분석과 관련하여 고려될 수 있다. 통상적으로 고려될 수 있는 가설들은 다음과 같다. 1) 독립 변수의 모든 회귀 계수들이 0이다. 2) 독립 변수의 회귀 계수들 중 일부분이 0이다. 3) 회귀계수들 중 일부분이 서로 같은 값을 가진다. 4) 회귀모수들이 특정한 제약조건을 만족한다. 이런 가설들은 하나의 통합된 접근방법을 통해 동일한 방식으로 검정될 수 있다. 먼저, 모든 독립변수를 포함한 모형을 완전모형(FM)이라고 하자. 그리고 귀무가설에 가정된 내용들을 완전모형에 대입해서 얻은 모형을 축소모형(RM)이라고 하자. 완전모형의 변수들이 상대적으로 축소모형에 비해 많으므로 SSR값이 커져 잔차제곱합(SSE)을 감소시킬 것이므로 $SSE(RM) \\geq SSE(FM)$이 된다. 따라서 차이 $SSE(RM) - SSE(FM)$은 축소모형을 적합함으로써 증가하는 잔차제곱합(SSE)을 의미한다. 만약 이 차이가 크다면 축소모형은 적절하지 않다. F = \\frac{[SSE(RM) - SSE(FM) ]/(p + 1 - k)}{SSE(FM)/(n-p-1)} 위의 식을 통해 다 위에서 언급했던 가설들을 모두 검정할 수 있다. 가설 1)독립 변수의 모든 회귀 계수들이 0이다.의 귀무가설은 $H_{0}:\\beta_{1} = \\beta_{2} = \\cdots = \\beta_{p} = 0$이며 대립가설은 $H_{1}: 최소한 하나의 계수는 0이 아니다. $이다. 이 가설은 축소모형의 변수는 1개이므로 해당 모형을 fitting한 후에 나오는 분산분석표에서의 F 통계량 값을 보고 검정 할 수 있다. 가설 2)독립 변수의 회귀 계수들 중 일부분이 0이다.의 귀무가설은 $H_{0}:\\beta_{1} = \\beta_{3} = \\beta_{5} = 0 $ 이고, 대립가설은 $H_{1}:\\beta_{1}, \\beta_{3}, \\beta_{5} 중 최소한 하나는 0이 아니다. $ 이다. 이 가설은 위에서 F 통계량을 구하는 방식에 변형을 주어 생각해보면 $R^{2}$값을 통해 구할 수 있음을 알 수 있다. F = \\frac{({R_{p}}^{2}-{R_{q}}^{2})/(p-q)}{(1 - {R_{p}}^{2})/(n-p-q)}, df=(p-q,n-p-1) 여기서 주목할 점은 SST는 정해져 있어 고정되어 있는데, SSR은 변수를 추가할수록 점점 더 커지므로 귀무가설을 기각하기 더 쉬워진다는 것이며, 우리가 추후에 말할$R^{2}$값도 변수를 추가할수록 높아지므로 이값으로 모형의 성능을 평가할때 무조건 이값이 높다고 좋은 모형이라고 생각하지 않아야 한다. 또한 수정결정계수(adjusted R-squared) ${R_{adj}}^{2}$도 적합도를 평가하기 위해 사용될 수 있다. ${R_{adj}}^{2}$은 모형안에 있는 독립(설명)변수들의 수가 다르다는 것을 조정하므로 F값과 같이 서로 다른 모형들(포함된 독립변수가 다르거나 갯수가 다른)을 비교하기 위해 사용된다. 이 값은 결정계수 값과 다르게 Y의 전체 변이 중에서 독립변수들에 의하여 설명되는 비율로 해석 될 수 없다! {R_{adj}}^{2} = 1 - \\frac{SSE/(n-p-1)}{SST/(n-1)}원점을 통과하는 회귀선 일반적으로 고려되는 단순선형회귀모형은 $ Y = \\beta_{0} + \\beta_{1}X + \\epsilon $과 같이 절편항을 가지고 있다. 그러나 원점을 통과하는 다음과 같은 모형 $ Y = \\beta_{1}X + \\epsilon $ 에 데이터를 적합시킬 필요가 있을 때도 있다. 이 모형은 절편항이 없는 모형으로 불린다. 문제의 성격이나 외적 상황에 의해 회귀선이 원점을 지나야만하는 경우가 있다. 예를 들어, 시간(X)의 함수로서 여행 거리(Y)는 상수항을 가지지 않아야 한다. 이때는 SSE의 자유도가 확률 변수인 절편항이 하나 빠지므로 N-p(전체 확률변수가 설명변수p개 절편항 1개 이었던 N-p-1에서)로 바뀌게 된다. 또한 이때는 우리가 알고 있는 SST=SSR+SSE라는 공식이 더 이상 성립되지 않는다. 그러므로 $R^{2}$와 같은 절편항을 갖는 모형에 대한 몇몇 성능 평가 지표들은 절편항이 없는 모형에 대해서는 더 이상 적절하지 않다. 절편항이 없는 모형에 대한 적절한 항등식은 y의 평균을 0으로 대체함으로써 얻어진다. \\sum_{i = 1}^{n} y_{i}^{2} = \\sum_{i = 1}^{n} \\hat{y_{i}^{2}} + \\sum_{i = 1}^{n} e_{i}^{2} 그러므로 $R^{2}$ 또한 재정의 된다. R^{2} = \\frac{\\sum \\hat{ y_{i}^{2} }}{\\sum {y_{i}^{2}} = 1 - \\frac{\\sum e_{i}^{2}}{\\sum y_{i}^{2}} 절편항을 가진 모형의 경우 $R^{2}$가 Y를 그의 평균으로 조정한 후에 Y의 전체 변동성 중에서 독립(설명)변수 X에 의하여 설명되는 비율로 해석될 수 있다. 절편항이 없는 모형의 경우에는 Y에 대한 조정이 없다. 이 처럼 절편항이 없는 모형은 풀고자하는 문제와 관련된 이론 혹은 물리적 상황에 부합되는 경우에만 사용되어야만 한다! 그러나 몇몇 응용에서는 어떤 모형을 사용해야 할지가 분명하지 않을 수 있다. 이러한 경우 1) 관측값과 예측값의 가까운 정도를 측정하는 것이 잔차제곱평균이므로 두 모형에 의해 산출되는 잔차평균제곱(SSE를 각각의 모형에 대한자유도로 나눈 값)을 비교하여 평가한다. 2) 데이터 모형을 적합하고 절편항의 유의성을 검정하여(t통계량을 바탕으로) 검정이 유의하다면 절편항을 가진 모형을 사용하고 그렇지 않으면 절편항이 없는 모형을 사용한다. 그러나, 일반적으로 회귀모형에서는 상수항이 통계적으로 유의하지 않더라도, 강한 이론적 근거가 존재하지 않는다면, 상수항은 모형에 포함되어야 한다. 특히 분석에 사용되는 데이터가 원점을 포함하지 않는 경우 더욱 강조되는데 그 이유는 상수항이 종속(반응)변수의 기본적인 수준(평균)을을 나타내기 때문이다. 참고로 필자는 ANCOVA 분석 즉, 회귀식에서 설명변수들 중 질적인 변수(혹은 더미변수)가 포함되어 있어 그런 질적인 변수와 더미변수가 절편항(상수항)을 대신해줄 것이라고 착각하여 상수항을 생성하지 않고, 모형을 적합시켰던 경험이 있다. 프로젝트였는데 멘토분께서 왜 절편항을 포함하지 않았냐고 물어보보셨는데 위와 같은 답변을 했었는데 잘못된 접근법이라고 조언을 해주셨었다. 그 당시에는 이해가 가지 않았지만 이제는 나의 접근법이 말이 안된다는 것부터 깨달았다. 왜냐하면 SGD 방법으로 확률변수인 절편항과 계수항들을 업데이트해 나가는 방식으로 회귀모형을 짜는데 필자는 이미 상수항 취급을 하는 질적변수나 더미변수 자체를 절편항이라고 생각했으니 말 자체가 안되는 것이다. 모형에서 중심화(centering)와 척도화(scaling) 회귀분석에서는 회귀계수의 크기가 변수의 측정 단위에 영향을 받게 되므로 중심화와 척도화를 해야한다.예를 들어 달러 단위로 측정된 소득의 회귀계수가 5.123이라면, 소득이 1,000달러 단위로 측정 되었을때는 5123으로 바뀌게 된다. 절편항(상수항)이 있는 모형을 다룰 때는 변수에 대한 중심화와 척도화가 필요하지만, 절편이 없는 모형을 다룰 때는 변수의 척도화만 필요하다. 또한 이는 다른 선형성을 가정하는 모델(RBF kernel을 사용하는 SVM, logistic regression)에선 피처를 정규성을 띄게 해주어야하는 모형 뿐만아니라 피처 scaling을 하여 과적합을 방지하는 방법이므로 알고있어야한다. 중심화(centering) 변수는 각 관측값에서 모든 관측값의 평균을 빼는 것으로 얻어진다. 중심화된 변수 척도화 또한 가능하다. 두 가지 형태의 척도화(scaling)가 통상적으로 가능한데, 단위 길이 척도화(unit length scaling or normalization)와 표준화(Standardization)이다. 단위길이 척도화는 피처 벡터의 길이로 나누어주거나 min-max scaling 같은 것을 의미한다. 표준화는 말 그대로 편차를 표준편차로 나누어 표준정규분포를 띄게끔해주는 작업을 의미한다. 다중 공선성(Multi-collinearity) Ordinary Least Squares(OLS) 즉 최소 제곱법 기반의 회귀 계수 계산은 독립 변수(입력 피처)의 독립성에 많은 영향을 받는다. 피처간의 상관관계가 매우 높은 경우 분산이 매우 커져서 오류에 매우 민감해지며 선형대수의 관점에서 보면, 모든 컬럼들이 linearly independent해야 최소한 하나 이상의 해가 존재하기 때문이다. 만약 위의 말이 이해가 가지 않는다면, 필자가 추전하는 선형대수학 강의를 듣는 것을 권한다. 다음 페이지를 가면 찾을 수 있다. 선형대수학 강의 추천 위와 같은 다중 공선성 문제가 있을 경우, 일반적으로 상관관계가 높은 독립 변수(입력 피처)가 많은 경우 독립적인 중요한 독립 변수(입력 변수)만을 남기고 제거하거나 규제를 적용한다. 또한 매우 많은 피처가 다중 공선성 문제를 가지고 있다면 PCA를 통해 차원 축소를 수행하는 것도 고려해 볼 수 있다. 다중 공선성 검사하는 방법들 위에서 VIF가 10이상인 경우 다중공선성이 있는 변수라고 판단할 수 있다고 했는데, 그렇다면 다중공선성이 있다고 판단되는 변수를 무조건적으로 제거해야 하나라는 의문이 들 수 있을 것이다. 그에 대한 답은 무조건적으로 제거하면 안된다라는 것이다. VIF가 높더라도 통계적으로 유의미한 변수(p-value가 유의수준 보다 낮은 변수)라면 제거하지 않는 것이 적절하다. 추가적으로 다른 변수들 중에 VIF가 높고, 유의미하지 않은 변수가 있다면 그 변수들을 제거해 본 뒤 VIF를 계산해 보아야 할 것이다. 이 과정을 거쳐서도 아마도 VIF는 높은 수치이겠지만, 제거를 해서는 안된다. 다중 공선성을 검사하는 방법에는 VIF외에도 상관계수 행렬을 구해서 위와 같이 산점도와 같이 그려서 보아야한다. 상관계수는 공분산을 각각의 표준편차로 나누어준 수치인데, 공분산은 예를 들어 두 변수 X와 Y가 있다면, Y와 X 사이의 선형 관계에 대한 방향을 나타낸다. Cov(X, Y)는 측정단위의 변화에 영향을 받기 때문에 우리에게 관계의 강도가 얼마나 되는 지를 알려주지는 않고 방향만을 알려준다. 이러한 문제를 해결하기 위해 표준편차로 나누어 Standardization을 해주어 단위에 대한 영향을 없애준 것이 상관계수이다. 참고로 여기서 Corr(X, Y)=0 가 반드시 Y와 X 사이에 관계가 없음을 의미하는 것이 아님을 주의하자! 상관계수는 오직 선형 관계를 측정하기에 선형적으로 관계가 없음을 의미한다. 즉, X와 Y가 비선형적으로 관련되어 있을 때에도 Corr(X, Y)가 0이 될 수 있다.또한 상관계수도 평균과 분산과 마찬가지로 극단값에 민감하다. 그러므로 이러한 요약 통계량에만 의존하는 분석으로는 전체적인 패턴을 보는데에 있어서 차이를 발견할 수 없게 할 것이다. 따라서 필자는 개인적으로 데이터 EDA 과정에서 독립(설명)변수들과 반응 변수 사이의 관계를 꼭 산점도로 그려 확인한 뒤, 상관계수와 의미가 일치하는지 확인해보는 작업이 필수라고 여긴다. 통계를 공부하는 Beginner들이 많이들 오해할 만한 사실은 Corr(X, Y)는 한 변수의 값이 주어졌을 때 다른 변수의 값을 예측하기 위해서 사용할 수 없다. 단지 대응(pairwise)관계만 측정한다. 예측을 하고 관계를 설명하기 위해서 우리는 회귀분석을 하는 것!!!!","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"NLP를 공부하는데 도움되는 사이트 모음","slug":"NLP_00","date":"2020-01-07T11:57:26.000Z","updated":"2020-01-18T07:37:27.407Z","comments":true,"path":"2020/01/07/NLP_00/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/07/NLP_00/","excerpt":"","text":"자연어 처리 관련 자료 자연어 처리에 대해 공부할 수 있게 도움이 될 만한 사이트 자연어 처리 강의 딥러닝을 이용한 자연어 처리:https://www.edwith.org/deepnlp 자연어 처리 오프라인 스터디 모임 DeepNLP(모두의연구소 자연어 처리 스터디):http://www.modulabs.co.kr/information 바벨피쉬(싸이그래머 스터디) : https://www.facebook.com/groups/babelPish/ 온라인 참고 자료 스탠퍼드 자연어 처리 강의 : http://web.stanford.edu/class/cs224n Jacob Eisenstein 교수님의 자연어 처리 강의 : https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf YSDA 자연어 처리 : https://github.com/yandexdataschool/nlp_course 조경현 교수님의 자연어 처리 강의 노트 : https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf 파이썬 정규 표현식 라이브러리 re파이썬 정규 표현식 . 줄 바꿈을 제외한 모든 문자 ^ 문자열의 시작 $ 문자열의 끝 * 앞에 있는 문자가 0회 이상 반복된 문자열 + 앞에 있는 문자가 1회 이상 반복된 문자열 {m} 앞 문자를 m회 반복하는 문자열 {m, n} 앞 문자를 m~n회 반복하는 문자열 ? 앞 문자가 나오거나 나오지 않는 문자열 ({0, 1}와 동일) \\d 숫자 \\D 숫자가 아닌 문자 \\w 문자 혹은 숫자 \\W 문자 혹은 숫자가 아닌것 (…) 괄호 안의 모든 정규 표현식을 만족하는 문자 [abc] a, b, c 중 한 개의 문자와 일치 re 함수 re 라이브러리의 가장 기본적인 함수 4가지를 살펴 볼 것이다. 이 밖에도 re 라이브러리는 여러 가지 정규 표현식을 이용해 문자열을 다룰 수 있는 기능을 제공한다. re.compile(pattern) compile 함수는 특정 기호를 정규표현식 객체로 만들어준다. re 라이브러리를 사용하려면 정규표현식 패턴을 매번 작성해야하는데, 이 함수를 사용해 패턴을 컴파일하면 필요할 때마다 사용할 수 있다. 123#숫자나 문자가 아닌 것이 1회이상 반복되는 문자열pattern = ' \\W+'re_pattern = re.compile(pattern) re.search(pattern, string) search 함수는 해당 문자열에서 정규 표현식에 해당하는 첫 부분을 찾는다. 1234567# 문자나 숫자인 것이 1회 이상 반복되는 문자열# 즉, 탭, 줄바꿈, 공백이 아닌 문자를 모두 찾는 과정re.search(\"(\\w+)\"), \"wow, it is awesome\")# 결과# 범위가 (0,3), 찾은 문자는 'wow'로 그 뒤에 ,와 공백이 있으므로 그전까지의 문자를 출력&lt;_sre.SRE_Match object; span=(0,3), match='wow'&gt; re.split(pattern, string) split 함수는 해당 문자열에서 특정 패턴으로 문자열을 나눠서 리스트로 만든다. 12345# 문자 혹은 숫자가 아닌 것으로 문자열을 나눠서 리스트로 출력re.split('\\W', \"wow, it is world of word\")# 결과['wow', '', 'it', 'is', 'world', 'of', 'word'] re.sub(pattern, repl, string) 문자열에서 특정 패턴을 만족시키는 문자를 사용자가 정의한 문자(repl)로 치환한다. 12345# 숫자인 것을 number로 치환한 문자열 반환re.sub('\\d', 'number', '7 candy')# 결과'number candy' Kaggle Kaggle에서 API를 활용해서 데이터를 다운받는 방법을 소개하려고 한다. 캐글 API 연동을 위해서는 두가지 단계가 필요하다. 단 이 방법은 Local PC 환경에서의 방법이므로 Colab과 연동해서 사용하는 방법은 추가적인 작업들이 필요하다. 1conda install kaggle API가 성공적으로 설치되면 계정을 연동해야 한다. 캐글 홈페이지에서 회원가입 후 Account 탭으로 가서 ‘Create API Token’을 선택한 후 kaggle.json 파일을 내려받는다. 이 파일에는 본인의 인증서가 있고 이 파일을 다음의 위치로 이동시킨다. 12345# 윈도웅C:\\Users\\&lt;사용자명&gt;\\.kaggle# macOS, Linux$ /&lt;사용자 홈 디렉토리&gt;/.kaggle 이제 API를 활용해 데이터를 내려 받을 수 있다. 데이터를 내려받는 방법은 Data(데이터) 탭의 API 명령어를 복사한 후 커맨드 라인에서 다음과 같이 실행하면 된다. 1$ kaggle competitions download -c &lt;competition-name&gt; 데이터 목록 확인 1$ kaggle competitions files -c &lt;competition-name&gt; 데이터 제출 1kaggle competions submit &lt;competition-name&gt; -f &lt;file-name&gt; -m &lt;message&gt; 대회 목록 확인 1kaggle competition list 보다 더 자세한 기능은 캐글문서에서 확인해 보자.","categories":[{"name":"NLP","slug":"NLP","permalink":"https://heung-bae-lee.github.io/categories/NLP/"}],"tags":[]},{"title":"Regression(01) - 회귀의 종류 및 회귀계수","slug":"machine_learning_01","date":"2020-01-04T02:44:20.000Z","updated":"2020-01-15T13:15:06.113Z","comments":true,"path":"2020/01/04/machine_learning_01/","link":"","permalink":"https://heung-bae-lee.github.io/2020/01/04/machine_learning_01/","excerpt":"","text":"회귀분석이란? 지도 학습은 두 가지 유형으로 나뉘는데, 바로 분류(classification)와 회귀(regression)이다. 이 두 가지 기법의 가장 큰 차이는 분류는 예측값이 카테고리와 같은 이산형 클래스 값이고, 회귀는 연속형 숫자 값이라는 것이다. 회귀(regression)은 현대 통계학을 떠받치고 있는 주요 기중 중 하나이다. 여러분이 회귀분석시에 많이 들어봤을 예시는 부모의 키와 자식의 키에대한 예시가 있을 것이다. 부모의 키가 아주 크더라도 자식의 키가 부모보다 더 커서 세대를 이어가면서 무한정 커지는 것은 아니며, 부모의 키가 아주 작더라도 자식의 키가 부모보다 더 작아서 세대를 이어가며 무한정 작아지는 것이 아니라는 것이다. 즉, 사람의 키는 평균 키로 회귀하려는 경향을 가진다는 자연의 법칙이라는 의미이며, 회귀분석은 이처럼 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법이다. 머신러닝 관점에서 보면 독립변수는 피처에 해당되며, 종속변수는 결정 값이다. 머신러닝 회귀 예측의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 최적의 회귀 계수를 찾아내는 것이다. 회귀에서 가장 중요한 것은 바로 회귀 계수이다. 이 회귀 계수가 선형이나 아니냐에 따라 선형회귀와 비선형 회귀로 나눌수 있으며, 독립변수의 개수가 한개 인지 여러개인지에 따라 단일 회귀, 다중 회귀로 나뉜다. 선형(비선형)이라는 용어는 Y와 $X_{1},X_{2},…,X_{p}의 관계를 묘사하는 것이 아니라는 것에 주목하여라! 회귀계수가 방정식에 선형적(비선형적)으로 삽입되어 있다는 것과 관련이 있다. 선형 함수의 예 Y = \\beta_{0} + \\beta_{1}X_{1} + \\epsilon- `Y와 X 사이의 관계는 비선형이지만, 모수들이 선형적으로 삽입되어 있기 때문에 선형 함수` Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}{X_{2}}^{2} + \\epsilonY = \\beta_{0} + \\beta_{1}\\log X_{1} + \\epsilon 비선형 함수의 예 Y = \\beta_{0} + e^{\\beta_{1}X_{1}} + \\epsilon 각 독립변수들은 양적(quantitative) 혹은 질적(qualitative)으로 분류 될 수 있다. 양적 변수의 예) : 주택 가격, 침실의 개수, 연수, 세금 등 질적 변수의 예) : 이웃의 형태(좋은 혹은 나쁜 이웃), 집의 형태(정원이있는, 고풍스러운 등) 독립변수들은 양적 질적 변수 모두 취할 수 있는데, 질적 변수들이 있다면, 계산상의 이유로 더미 변수(dummy variable)로 코딩을 해주어야 한다. 단, 질적인 변수들도 예를 들어 전문가들에 의해 이미 규정되어 신뢰성 있는 공식이나 규칙을 통해 연속적인 수치로 변환될 수 있다면 더미 변수로 만들어 주지 않고 사용해도 된다. 모든 독립변수들이 질적인 경우 분산 분석(ANOVA : analysis of variance)기법이라고 한다. 분산 분석은 그 자신의 고유한 방법으로써 소개되고 통계학부생들이 통계적 자료분석이라는 주제로 수업을 수강할때 나오는 개념으로써 설명되고 있는데 회귀분석의 특별한 경우임을 알고있어라!!!! 또한, 어떤 예측변수들이 양적이고 반면에 다른 변수들이 질적이라면, 이러한 경우의 회귀분석을 공분산분석(ANCOVA : analysis of covariance)이라고 한다. 회귀의 유형 조건 - 일변량(Univariate) - 오직 하나의 양적 독립변수(설명변수) - 다변량(Multivariate) - 두 개 이상의 양적 독립변수(설명변수) - 단순(Simple) - 오직 하나의 종속변수(반응변수) - 다중(Multiple) - 두 개 이상의 종속변수(반응변수) - 선형(Linear) - 데이터에 대하여 가능한 변환을 취한 후, 모든 계수들이 방정식에 선형적으로 삽입되어 있음. - 비선형(Nonlinear) - 종속변수(반응변수)와 일부 독립변수들의 관계가 비선형이거나 일부 계수들이 비선형적으로 나타남. 계수들을 선형적으로 나타나게 하는 어떤 변환도 가능하지 않음. - 분산분석(ANOVA) - 모든 독립변수들이 질적 변수임. - 공분산분석(ANCOVA) - 어떤 독립변수들은 양적변수이고 다른 독립변수들은 질적변수임. - 로지스틱(Logistic) - 종속변수(반응변수)가 질적변수임. 대표적인 선형 회귀 모형은 다음과 같다.일반 선형 회귀 예측값과 실제값의 잔차 제곱합을 최소화할 수 있도록 회귀 계수를 최적화하며, 규제(Regularization)를 적용하지 않은 모델이다. 릿지(Ridge) Ridge 회귀는 선형 회귀에 L2 Regularization을 추가한 모형이다. Ridge 회귀는 L2 Regularization을 적용하는데, L2 Regularization은 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소 시키기 위해서 회귀 계수값을 더 작게 만드는 Regularization 모형이다. 라쏘(Lasso) Lasso 회귀는 선형 회귀에 L1 Regularization을 적용한 방식이다. L2 Regularization이 회귀 계수 값의 크기를 줄이는 데 반해, L1 Regularization은 예측 영향력이 작은 피처의 회귀계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 것이다. 이러한 특성 때문에 L1 Regularization은 피처 선택 기능으로도 불린다. 엘라스틱넷(ElasticNet) L2, L1 Regularization을 함께 결합한 모형이다. 주로 피처가 많은 데이터 세트에서 적용되며, L1 Regularization으로 피처의 개수를 줄임과 동시에 L2 Regularization으로 계수의 값의 크기를 조정한다. 로지스틱(Logistic) 로지스틱 회귀는 회귀라는 이름이 붙어 있지만, 사실은 분류에 사용되는 선형 모형이다. 로지스틱 회귀는 매우 강력한 분류 알고리즘이다. 일반적으로 이진 분류 뿐만아니라 희소 영역의 분류, 예를 들어 텍스트 분류와 같은 영역에서 뛰어난 예측 성능을 보인다. 가정에서 잔차( $\\epsilon_{i}$ )와 target 값인 Y가 정규분포를 따른다는 것이 중요하다. 회귀 계수 추정 회귀 계수의 의미 회귀 계수의 검정","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"머신러닝의 개요","slug":"machine_learning_00","date":"2019-12-30T08:20:19.000Z","updated":"2020-01-08T07:45:51.536Z","comments":true,"path":"2019/12/30/machine_learning_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/30/machine_learning_00/","excerpt":"","text":"Machine learning Machine Learning으로 할 수 있는 것들 독립변수 반응변수 모형 고객들의 개인 정보 및 금융 관련 정보 대출 연체 여부 대출 연체자 예측 탐지 모델, 대출 연체 관련 주요 feature 추출 게임 유저들의 게임 내 활동 정보 게임 이탈 여부. 어뷰징 여부 이상 탐지 모델(anomaly detection model) 숫자 손 글씨 데이터 숫자 라벨(0~9) 숫자 이미지 분류 모델 상품 구매 고객 특성 정보 군집화를 통한 고객 특성에 따른 Segmentation Segmentation 모델 고객들의 상품 구매내역 매장내 상품 진열 위치 리뉴얼을 통한 매출 증대 쇼핑몰 페이지 검색 및 클릭 로그 기록 맞춤 상품 추천 시스템 recommendation system SNS 데이터 및 뉴스 데이터 소셜 및 사회 이슈 파악 Supervised Learning VS Unspervised Learning 머신러닝과 딥러닝이 실무에서 사용되어지는 경우에 팀의 구분을 두고 회사에서 운영하는 경우가 많지만, 서로 구분해서 공부하거나 이해하진 않았으면 좋겠다는것이 필자의 바람이다. 물론, 최근에는 딥러닝의 유행으로 대부분의 분석직군에서 딥러닝을 위한 인원을 채용하고 있긴 하지만 머신러닝또한 중요하지 않은 것이 아니면, 아직도 많은 곳에서 머신러닝 엔지니어들을 뽑고 있다. 모형의 적합성 평가 및 실험 설계 전체적인 머신러닝의 작업과정은 다음과 같다. 전처리 Raw 데이터를 모델링 할 수 있도록 데이터를 병합 및 파생 변수 생성 실험 설계 위에서 실제로 우리가 모델을 적용을 한다는 것은 예를 들어 기업에서 상용화를 한다는 가정이라는 의미이다. test정보가 Train과 validation 데이터에 없어야 한다는 점은 쉽게 비유하면 시험을 보는데 우리가 이미 학습한 내용(training data)이 시험에 똑같이 나온다면 시험을 잘 볼 확률이 높아지기 때문이다. 우리가 data를 나누었던 이유는 training data를 통해 학습된 알고리즘 모형이 학습하지 않은 새로운 데이터에서도 잘 예측할 수 있도록 하기 위한 작업이었다는 것을 잊지 말아라! 또한, validation data는 parameter들을 조절하면서 최적의 모형을 선택하기 위한 데이터셋이라고 생각하면된다. 데이터가 잘 나누어졌는지 어느 한 쪽으로 치우쳐져 있는지 확인하는 방법으로 위의 오른쪽 그래프처럼 확인 할 수도 있을 것 같다! 무조건 위에서 언급했던 k-hold cross validation으로 training data set과 validation set을 나눠서 진행하는 것이 아니고 데이터의 성격에 따라 다르게 설계를 해야한다. 예를 들면 위의 반도체 두께를 예측하는 문제에 있어서는 반도체 두께가 각 개체가 만들어지는 순서에 의해 영향을 받는다. 즉 Y2는 Y1에 의해 영향을 받고 Y3는 Y1,Y2에 의해 영향을 받는다. 그러므로 이러한 데이터의 경우에는 무작정 k-hold cross validation으로 나누어주면 안된다. Y2를 예측하는데 Y3에 관한 정보를 사용하게 되기 때문이다. 또 다른 예로는 Imbalanced data에 대해 말할 수 있을 것이다. Imbalanced data는 target variable이 말 그대로 불균형한 데이터를 의미한다. 조금이라도 불균형하면 Imbalanced datas냐는 의문이 들겠지만 그런의미가 아니라 예를들면 보험회사의 보험사기라던지, 금융사기 같이 전체 데이터에서 target variable에 해당하는 데이터가 10% 정도로 희박하게 나타나는 데이터를 의미한다. 이런 데이터에서는 먼저 처음의 비율대로 train과 validation set으로 나누어준다. 그 다음 train data에서만 resampling을 하여 model을 학습시킨후에 추후에 validation data로 예측해보는 것이다. 그러므로 그냥 k-hold가 아닌 stratified k-Fold를 통해 데이터를 train과 validation set으로 나누어 주어야한다. 모형 학습 및 선택 과적합(Overfitting) 위의 편향과 분산의 트레이드 오프 관계에 의해 우리는 둘 중 하나를 좀더 생각해야만하는 상황에 놓이게될 것이다. 그렇다면 위의 4가지 그래프 중 어떤 모형을 선택하는 것이 좋은 것인가? 나의 개인적인 생각은 모형의 분산은 적고 편향이 높은 모형 같은 경우는 치우쳐 있는 데이터에 대한 예측성능만 높고 여러 다양한 데이터에 대한 예측 성능은 낮아지기 때문에 궁극적으로 목표해야할 모형은 분산은 높지만 편향이 낮은 모형이다.","categories":[{"name":"machine learning","slug":"machine-learning","permalink":"https://heung-bae-lee.github.io/categories/machine-learning/"}],"tags":[]},{"title":"data engineering (DB에 table 만들기)","slug":"data_engineering_05","date":"2019-12-17T08:30:11.000Z","updated":"2020-02-21T16:14:12.176Z","comments":true,"path":"2019/12/17/data_engineering_05/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/17/data_engineering_05/","excerpt":"","text":"Spotify가 국내에 음원 진출을 확정지었다는 기사를 보면서 다시 한번 이 토이프로젝트에 대해 동기부여가 되었다. Spotify API를 통해 AWS에 만들어 놓은 DB에 입력해 볼 것이다. Spotify API를 이용해서 DB 구축하기 먼저, 혹시라도 필자의 토이 프로젝트의 목표가 무엇인지 모르실 분들을 위해 말하자면, Spotify data를 통해서 자신의 취향을 입력하면 그 취향과 비슷한 장르의 음악과 뮤지션을 추천해주는 chat bot을 Facebook API를 통해서 만들고자 한다. 위와 같은 서비스를 만들기 위해서는 먼저 필요한 데이터를 얻기 위해 Spotify API의 response로 얻을 수 있는 값들이 무엇이 있고, 어떤 method를 통해 어떤 데이터를 얻을 수 있는지 정리를 해 보고 간단한 ERD를 통해 도식화 할 수 있을 것이다. Spotify developer를 클릭해서 아래 그림과 같이 DOCS 탭에서 WEB API reference를 클릭하면 여러 항목들이 나올 것이다. 여러 탭들 중 Artist와 관련된 사항들과 track에 따른 Audio feature들을 필요로 하기 때문에 Artists 탭과 Tracks 탭을 살펴보면 될 것이다. 확인해 본 결과 필요한 부분들을 정해 아래 그림과 같이 ERD를 도식화 할 수 있다. Get an Artist 탭에서 얻을 수 있는 테이블은 다음과 같다. 특히 genre는 가져올때는 python의 list형식으로 되어 있기 때문에 우리가 사용할 RDB에 저장할 때는 list로 저장할 수 없으므로 또 다른 테이블을 만들어 주어 하나의 id에 여러가지 genre 값을 갖을 수 있게끔 사용할 것이다. artist라는 테이블은 ‘id’, ‘Name’, ‘Followers’, ‘Popularity’, ‘External_Urls’(해당 가수 페이지의 url)을 넣어 줄 것이다. 그 중 ‘id’는 Not NuLL과 Unique한 것을 동시에 만족시키는 Primary Key로 지정할 것이다. 왜냐하면 고유한 식별자이며, 이를 통해 다른 테이블들과의 join하는 것이나 검색에 있어서 훨씬 빠른 검색을 할 수 있게 되기 때문이다. artist genres 테이블은 ‘Artist_Id’, ‘Genre’ 값을 갖게 할 것이며, 여기서는 Artist_Id가 Foreign Key 역할을 할 것이다. Get an Artist&#39;s Top Tracks에서는 top_tracks 테이블을 얻을 수 있다. ‘id’(track id), ‘Artist ID’, ‘Name’, ‘Popularity’(spotify에서 계산한 특정 알고리즘에 의해 산출된 0~100사이의 score), ‘URL’, ‘image_url’을 넣어 줄 것이다. 마지막으로 유일하게 Tracks탭에서 Get Audio Features for Several Tracks를 통해 audio feature인 ‘track_id’, ‘Key’, ‘Mode’, ‘Acousticness’, ‘Danceability’, ‘Energy’, ‘Instrumentalness’, ‘Liveness’, ‘Loudness’, ‘Speechiness’, ‘Valence’, ‘Tempo’를 넣어 줄 것이다. ‘Key’ : 음의 높낮이를 의미하며, 각각에 해당하는 음의 높낮이를 정수에 mapping했다. E.g. 0 = C, 1 = C♯/D♭, 2 = D등 ‘Mode’ : 장조(도레미파솔라시\b)로 이루어져있다면 1, 단조(라시도레미파솔)로 이루어져 있다면 0을 갖는다. ‘Acousticness’ : Acoustic적인 트랙인지를 말해주는 지표로서, 0~1값을 갖고, 해당 트랙이 Acoustic적인 음악일수록 1값을 갖는다. ‘Danceability’ : 음악적 요소인 템포, 리듬의 안정성등의 조합을 기반으로하여 춤에 적합한지를 나타내는 지표이다. 0~1사이의 값을 갖으며, 춤에 적합한 트랙일수록 1의 값을 갖는다. ‘Energy’ : 활동적이고 긴장을 줄 수 있는 트랙인지에 대한 지표로서 0~1사이의 값을 갖는다. ‘Instrumentalness’ : 트랙이 목소리가 주인지를 파악하는 지표로서, 여기서 말하는 목소리란, 추임새적인 부분들을 악기적인 요소로 보고 이런 악기적인 요소가 많을수록 1에 가깝운 값을 갖는다. ‘Liveness’ : 해당 트랙이 live음원인지를 파악하는 요소로서, 0.8을 넘는다면 live 트랙일 확률이 높다. ‘Loudness’ : 트랙 전반적인 평균 데시벨(dB)로서, 상대적으로 다른 트랙들과 비교할 수 있다. 일반적인 값은 -60에서 0 db 사이이다. ‘Speechiness’ : 토크쇼, 오디오북, 시같이 구어체 단어들의 존재를 탐지하는 지표로서 0.66 값 이상은 아마도 완전히 구어체로 만들어진 트랙을 의미한다고 하며, 1에 가까울수록 구어체를 많이 가지고 있다. ‘Valence’ : 트랙의 감성을 나타내는 지표로서, 0~1사이의 값을 갖는다. 1에 가까울수록 밝고 긍정적인 느낌의 트랙을 의미한다. ‘Tempo’ : 전반적인 트랙의 BPM을 추정하는 지표이다. DB Table 생성 Python Script 파일을 작성하기에 앞서, 먼저 SQL에 접속한 후 데이터를 insert할 경우 어떤 구문을 사용하여야 적합할지에 대해 알아 볼 것이다. 123456789101112## artists에 관한 table# 이모지까지 커버 하고 싶은 경우mysql&gt; create table artists (id VARCHAR(255), name VARCHAR(255), followers INTEGER, popularity INTEGER, url VARCHAR(255), image_url VARCHAR(255), PRIMARY KEY(id)) ENGINE=InnoDB DEFAULT CHARSET='utf8mb4' COLLATE 'utfmb4_unicode_ci'# 이모지는 제외하고 문자만 커버하는 경우mysql&gt; create table artists (id VARCHAR(255), name VARCHAR(255), followers INTEGER, popularity INTEGER, url VARCHAR(255), image_url VARCHAR(255), PRIMARY KEY(id)) ENGINE=InnoDB DEFAULT CHARSET='utf8'## artist_genres에 관한 테이블mysql&gt; create table artist_genres (artist_id VARCHAR(255), genre VARCHAR(255)) ENGINE=InnoDB DEFAULT CHARSET='utf8';# 위에서 만든 artists table에 대한 info를 보고 싶은 경우mysql&gt; show create table artists; 앞으로 artist_genres 테이블에 어떤 artist의 장르가 추가된다면, 데이터를 추가해주어야 하는데 지속적으로 추가해주어야하므로 자동화를 할 것이다. 추가해 주는 값이 테이블에 이미 있는 값을 갖는 데이터가 들어온다면 무의미할 것이다. 아무런 column에 제약을 주지 않았기에 insert를 통한 데이터 추가 방식은 무의미하다. 123456789101112131415161718# 데이터 추가mysql&gt; insert into artist_genres (artist_id, genre) values ('1234', 'pop');# 테이블 확인mysql&gt; select * from artist_genre;# 데이터 추가mysql&gt; insert into artist_genres (artist_id, genre) values ('1234', 'pop');# 테이블 확인mysql&gt; select * from artist_genre;# 테이블 값만 삭제mysql&gt; delete from artist_genres;# 테이블 자체를 삭제# 실무에서는 drop은 잘 사용하지 않고, Alter를 사용한다.mysql&gt; drop table artist_genres; 앞에서 말한 추후에 데이터 입력시 동일한 데이터를 계속 추가하는 문제를 방지하기 위해 column에 unique key 속성을 추가해 주었다. 이전의 방법과 동일하게 추가했을 경우 insert into구문은 오류를 발생하여서 추가 데이터를 저장하진 않지만, Python script가 도중에 멈추게 되는 문제가 생긴다는 점을 확인 할 수 있다. 12345678910111213141516# Unique key 속성 부여mysql&gt; create table artist_genres (artist_id VARCHAR(255), genre VARCHAR(255), unique key(artist_id, genre)) ENGINE=InnoDB DEFAULT CHARSET='utf8';# 속성 추가 확인mysql&gt; show create table artist_genres;# 데이터 추가mysql&gt; insert into artist_genres (artist_id, genre) values ('1234', 'pop');# 데이터 확인mysql&gt; select * from artist_genres;# 데이터 추가mysql&gt; insert into artist_genres (artist_id, genre) values ('1234', 'pop');ERROR 1062 (23000): Duplicate entry '1234-pop' for key 'artist_id' insert into 구문 대신 update set 구문을 사용해보았다. 우선 키값이 있기 때문에 중복되는 값이 저장되지는 않는다. update set 구문은 NULL데이터를 가지고 있었다면 변경되지 않는다는 문제점이 있다. replace into 구문을 통해 값이 변경되기 했지만 성능적인 측면에서, 데이터가 많을땐 먼저 키값이 있는지 찾고, 키 값이 존재한다면 지금처럼 그 행을 지우고 새로운 행으로 바꿔준다. 또한, primary key와 auto increment를 통해 설정되어있던 테이블이라면 원래 primary key로 인해 부여 받은 값이 아닌 새로운 값을 부여받게된다는 문제점이 있다. 예를 들면, auto increment로 DB에 입력되어 지는대로 번호를 부여했는데, DB에 입력된지 오래된 id에 genre를 추가하려고 한다면 기존의 행번호를 지우고 table의 맨뒤에 새로 입력된다는 것이다. on duplicate key update를 통해 문제점을 해결 할 수 있다! 1234567891011121314151617181920212223242526272829303132333435363738# 데이터 추가mysql&gt; update artist_genres set genre='pop' where artist_id='1234';# columns 추가mysql&gt; alter table artist_genres add column country VARCHAR(255);# update 되는 시점을 갖는 column 추가# 자동적으로 데이터가 추가 될때마다 그 시점이 저장됨mysql&gt; alter table artist_genres add column updated_at timestamp default current_timestamp on update current_timestamp;# 동일한 키값을 갖는 데이터가 이미 존재하므로 오류를 발생시킴.mysql&gt; INSERT INTO artist_genres (artist_id, genre, country) VALUES ('1234', 'pop', 'UK');# replace into 구문을 통해 값이 변경되기 했지만 성능적인 측면에서, 데이터가 많을땐 먼저 키값이 있는지 찾고, 존재한다면 지금처럼 그 행을 지우고 새로운 행으로 바꿔준다.mysql&gt; REPLACE INTO artist_genres (artist_id, genre, country) VALUES ('1234', 'pop', 'UK');# 그러므로 위의 조건에 부합하는 행위인 키값인 artist_id와 genre값이 있는 행을 찾기 때문에 2rows가된다.Query OK, 2 rows affected (0.28 sec)# 위에서 replace into 구문을 통해 값이 변경되었음을 확인 할 수 있다.mysql&gt; select * from artist_genres;# artist_id, genre가 Unique key인데 두 컬럼을 동시에 동일한 값을 갖는 row가 없으므로 새로 추가 해준다.mysql&gt; REPLACE INTO artist_genres (artist_id, genre, country) VALUES ('1234', 'rock', 'UK');mysql&gt; select * from artist_genres;# 말 그대로 동일한 키값이 있으면 그냥 insert into 구문은 오류를 발생시켰지만, insert ignore into 구문은 오류를 발생시키지 않고 무시한다.# unique한 key값인 artist_id와 genre가 동일한 행이 이미 테이블에 존재하기 때문이다.mysql&gt; insert ignore into artist_genres (artist_id, genre, country) VALUES ('1234', 'rock', 'FR');mysql&gt; select * from artist_genres;# on duplicate key updatemysql&gt; insert into artist_genres (artist_id, genre, country) values ('1234', 'rock', 'FR') on duplicate key update artist_id='1234', genre='rock', country='FR'# 우리가 입의로 넣어 주었던 country는 지워줄 것이다.mysql&gt; alter table artist_genres drop column country; 결론은 다음 구문을 query문으로 사용하겠다는 것이다.1mysql&gt; insert into artist_genres (artist_id, genre, country) values ('1234', 'rock', 'FR') on duplicate key update artist_id='1234', genre='rock', country='FR' create_artist_table.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141import sysimport requestsimport base64import jsonimport loggingimport pymysqlimport csvimport sys, os, argparsedef main(host, user, passwd, db, port, client_id, client_secret): try: conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") sys.exit(1) headers = get_headers(client_id, client_secret) ## Spotify Search API artists = [] with open('artist_list.csv') as f: raw = csv.reader(f) for row in raw: artists.append(row[0]) for a in artists: params = &#123; \"q\": a, \"type\": \"artist\", \"limit\": \"1\" &#125; r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) raw = json.loads(r.text) artist = &#123;&#125; try: artist_raw = raw['artists']['items'][0] if artist_raw['name'] == params['q']: artist.update( &#123; 'id': artist_raw['id'], 'name': artist_raw['name'], 'followers': artist_raw['followers']['total'], 'popularity': artist_raw['popularity'], 'url': artist_raw['external_urls']['spotify'], 'image_url': artist_raw['images'][0]['url'] &#125; ) insert_row(cursor, artist, 'artists') except: logging.error('something worng') continue conn.commit() sys.exit(0) try: r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) except: logging.error(r.text) sys.exit(1) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) if r.status_code != 200: logging.error(r.text) if r.status_code == 429: retry_after = json.loads(r.headers)['Retry-After'] time.sleep(int(retry_after)) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) ## access_token expired elif r.status_code == 401: headers = get_headers(client_id, client_secret) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) else: sys.exit(1)def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" # base64 encode 및 decode 함수는 모두 byte 형 객체를 필요로 한다. # 문자열을 byte로 가져 오려면, 파이썬의 내장 된 encode 함수를 사용하여 문자열을 encoding해주어야 한다. encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) # r.text가 type이 string이므로 json형태로 만들어 주기 위해 json 패키지를 사용한다. access_token = json.loads(r.text)['access_token'] # Spotif Web API를 접근 가능하게 하는 Access Token은 다음과 같은 형식으로 사용가능하고 Authrization 페이지에 나와있다. headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersdef insert_row(cursor, data, table): placeholders = ', '.join(['%s'] * len(data)) # \"%s, %s, %s, %s, %s, %s\" columns = ', '.join(data.keys()) # \"id, name, follwers, popularity, url, image_url\" key_placeholders = ', '.join(['&#123;0&#125;=%s'.format(k) for k in data.keys()]) # \"id=%s, name=%s, follwers=%s, popularity=%s, url=%s, image_url=%s\" sql = \"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s\" % (table, columns, placeholders, key_placeholders) cursor.execute(sql, list(data.values())*2)if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) 결과 이미지 python script를 짜고 추후에 script를 실행하여 바로 RDS에 저장하고 table이 제대로 생성됬는지 확인하였다. artist genre table artist genre table은 최대로 50개를 검색할수 있는 Spotify search API 메서드를 통해 batch 단위로 검색할 수 있도록 Python script를 구성하였다. 먼저 artist table에 있는 id들을 모두 fetch_all한 후에 batch size인 50개씩 묶어서 list로 만들어 둔 후 join을 통해 한꺼번에 search 하는 방법을 사용하였다. search API 사용법 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141import sysimport requestsimport base64import jsonimport loggingimport pymysqlimport csvimport sys, os, argparsedef main(host, user, passwd, db, port, client_id, client_secret): try: # use_unicode=True를 써야 한글같은 경우는 깨지지 않는다. conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset='utf8') cursor = conn.cursor() except: logging.error(\"could not connect to rds\") # 보통 문제가 없으면 0 # 문제가 있으면 1을 리턴하도록 안에 숫자를 넣어준다. sys.exit(1) headers = get_headers(client_id, client_secret) cursor.execute(\"SELECT id FROM artists\") artists = [] # 커서의 fetchall() 메서드는 모든 데이타를 한꺼번에 클라이언트로 가져올 때 사용된다. # 또다른 fetch 메서드로서 fetchone()은 한번 호출에 하나의 Row 만을 가져올 때 사용된다. for (id, ) in cursor.fetchall(): artists.append(id) artist_batch = [artists[i: i+50] for i in range(0, len(artists), 50)] artist_genres = [] for i in artist_batch: ids = ','.join(i) URL = \"https://api.spotify.com/v1/artists/?ids=&#123;&#125;\".format(ids) r = requests.get(URL, headers=headers) raw = json.loads(r.text) for artist in raw['artists']: for genre in artist['genres']: artist_genres.append( &#123; 'artist_id': artist['id'], 'genre': genre &#125; ) for data in artist_genres: insert_row(cursor, data, 'artist_genres') # commit을 해줘야 records를 볼 수 있다. conn.commit() cursor.close() sys.exit(0) try: r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) except: logging.error(r.text) sys.exit(1) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) if r.status_code != 200: logging.error(r.text) if r.status_code == 429: retry_after = json.loads(r.headers)['Retry-After'] time.sleep(int(retry_after)) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) ## access_token expired elif r.status_code == 401: headers = get_headers(client_id, client_secret) r = requests.get(\"https://api.spotify.com/v1/search\", params=params, headers=headers) else: sys.exit(1)def get_headers(client_id, client_secret): endpoint = \"https://accounts.spotify.com/api/token\" encoded = base64.b64encode(\"&#123;&#125;:&#123;&#125;\".format(client_id, client_secret).encode('utf-8')).decode('ascii') headers = &#123; \"Authorization\": \"Basic &#123;&#125;\".format(encoded) &#125; payload = &#123; \"grant_type\": \"client_credentials\" &#125; r = requests.post(endpoint, data=payload, headers=headers) access_token = json.loads(r.text)['access_token'] headers = &#123; \"Authorization\": \"Bearer &#123;&#125;\".format(access_token) &#125; return headersdef insert_row(cursor, data, table): \"\"\" insert into query문 작성및 실행을 좀 더 간편하게 하기 위해 만든 함수 \"\"\" placeholders = ', '.join(['%s'] * len(data)) columns = ', '.join(data.keys()) key_placeholders = ', '.join(['&#123;0&#125;=%s'.format(k) for k in data.keys()]) sql = \"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s\" % (table, columns, placeholders, key_placeholders) cursor.execute(sql, list(data.values())*2)if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--client_id', type=str, help='Spotify app client id') parser.add_argument('--client_secret', type=str, help='Spotify client secret') parser.add_argument('--host', type=str, help='end point host') parser.add_argument('--username', type=str, help='AWS RDS id') parser.add_argument('--database', type=str, help='DB name') parser.add_argument('--password', type=str, help='AWS RDS password') args = parser.parse_args() port = 3306 main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret) 결과 이미지 python script를 짜고 추후에 script를 실행하여 바로 RDS에 저장하고 table이 제대로 생성됬는지 확인하였다. API를 통해 크롤링한 데이터 DB에서 분석하기 DB를 통해 간단한 분석을 실행해 볼 것이다. 먼저 artists table에서 상위 10개의 records를 보여주는 SQL구문을 통해 데이터 구성을 살펴볼 것이다. 아래 그림과 같이 출력되며, 전체 column이 id, name, followers, popularity, image_url들이 있다. 1SELECT * FROM artists LIMIT 10; artist당 평균적으로 몇개의 장르를 갖고 있는지를 살펴보기 위해 아래와 같은 계산을 했다. 결과적으로 전체 artists는 488명을 search해서 가져왔고, artist_genres table에서는 전체 artists의 명수인 489로 나누어져 평균적인 artist 1명당 갖는 genre는 6개인 것을 확인할 수 있었다. 123SELECT COUNT(*) FROM artists;SELECT COUNT(*)/489 FROM artist_genres; 결과12345678910111213+----------+| COUNT(*) |+----------+| 488 |+----------+1 row in set (0.01 sec)+--------------+| COUNT(*)/489 |+--------------+| 6.4376 |+--------------+1 row in set (0.01 sec) artist_genres table 에서 genre와 count를 보여주는데, genre로 GROUPING을 하고 그에 따른 갯수에 내림차순으로 보여달라고 Query문을 작성하여 살펴 보았더니, 가장 많은 genre는 rock이며, 그 다음은 classic rock이다. 1234SELECT genre, COUNT(*) AS count_num FROM artist_genres GROUP BY genre ORDER BY count_num DESC LIMIT 20;# 동일한 결과를 출력# SELECT genre, COUNT(*) FROM artist_genres GROUP BY 1 ORDER BY 2 DESC LIMIT 20; 결과12345678910111213141516171819202122232425+-------------------+-----------+| genre | count_num |+-------------------+-----------+| rock | 155 || classic rock | 93 || album rock | 90 || mellow gold | 87 || folk rock | 70 || soft rock | 70 || art rock | 66 || singer-songwriter | 61 || adult standards | 51 || dance rock | 50 || hard rock | 50 || blues rock | 49 || soul | 45 || new wave pop | 44 || roots rock | 43 || permanent wave | 40 || folk | 37 || psychedelic rock | 37 || pop rock | 35 || pop | 35 |+-------------------+-----------+20 rows in set (0.16 sec) 가장 popularity가 높은 가수는 아래와 같이 Drake이다. 그 다음은 Ariana Grande, Taylor Swift 순이다. 1SELECT popularity, name FROM artists ORDER BY 1 DESC LIMIT 20; 결과12345678910111213141516171819202122232425+------------+-----------------------+| popularity | name |+------------+-----------------------+| 98 | Drake || 97 | Ariana Grande || 95 | Taylor Swift || 95 | Justin Bieber || 95 | Kanye West || 94 | Eminem || 93 | Maroon 5 || 93 | Nicki Minaj || 92 | Queen || 92 | Sam Smith || 92 | Imagine Dragons || 92 | Rihanna || 91 | Kendrick Lamar || 91 | The Beatles || 91 | Lil Wayne || 89 | Lana Del Rey || 88 | Frank Sinatra || 88 | Katy Perry || 87 | Red Hot Chili Peppers || 87 | Snoop Dogg |+------------+-----------------------+20 rows in set (0.01 sec) 이번에는 artist table에서 followers와 name 중 followers가 많은 순으로 내림차순하여 상위 20개의 records만 볼 것이다. 이 또한, Drake가 부동의 1위이다. 1SELECT followers, name FROM artists ORDER BY 1 DESC LIMIT 20; 결과12345678910111213141516171819202122232425+-----------+-----------------------+| followers | name |+-----------+-----------------------+| 42943467 | Drake || 39573173 | Ariana Grande || 35366176 | Rihanna || 31737977 | Justin Bieber || 31374657 | Eminem || 25029508 | Taylor Swift || 23906032 | Imagine Dragons || 22042155 | Queen || 21244706 | Maroon 5 || 16648595 | Nicki Minaj || 15813453 | Demi Lovato || 14486822 | The Beatles || 14319668 | Katy Perry || 13656683 | Kendrick Lamar || 12980433 | Michael Jackson || 12587845 | Metallica || 12205939 | Red Hot Chili Peppers || 11002859 | Pink Floyd || 10840912 | Kanye West || 10800211 | Sam Smith |+-----------+-----------------------+20 rows in set (0.02 sec) 이번에는 artists table에서 popularity가 80 초과인 id를 갖는 artist들의 장르들이 무엇인지 살펴 보았다. 그 중 pop이 17건으로 제일 많았고, 그 다음은 rock이었다. 이 부분은 필자에게는 약간의 의외였다. 물론 약 500명의 artist만을 가져왔기에 객관적이지는 못하겠지만, 필자가 개인적으로 rock음악을 안들어서 인지 아직까지 rock도 인기가 있는 것으로 미루어보아 듣는 분들이 많다는 사실을 알게 되었다. 1SELECT genre, COUNT(*) FROM artist_genres tb1 JOIN artists tb2 ON tb1.artist_id = tb2.id WHERE tb2.popularity &gt; 80 GROUP BY 1 ORDER BY 2 DESC LIMIT 20 결과12345678910111213141516171819202122232425+------------------+----------+| genre | COUNT(*) |+------------------+----------+| pop | 17 || rock | 16 || rap | 9 || dance pop | 9 || post-teen pop | 7 || hip hop | 7 || permanent wave | 7 || album rock | 6 || classic rock | 6 || adult standards | 5 || pop rap | 5 || modern rock | 4 || neo mellow | 4 || hard rock | 3 || alternative rock | 3 || g funk | 3 || pop rock | 3 || gangster rap | 3 || post-grunge | 3 || west coast rap | 3 |+------------------+----------+20 rows in set (0.01 sec)","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering (AWS로 DB 만들기)","slug":"data_engineering_04","date":"2019-12-15T06:16:36.000Z","updated":"2020-02-20T07:34:45.492Z","comments":true,"path":"2019/12/15/data_engineering_04/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/15/data_engineering_04/","excerpt":"","text":"AWS RDB 만들기 Spotify data를 크롤링 하고난 후에 AWS RDB에 저장하기 위해서 먼저 DB를 만들어 줄 것이다.Youtube 초보자를 위한 AWS 시작하기! AWS RDS 생성 개발자나 필자 처럼 데이터를 분석하는 분들을 제외한 분들은 아마 Amazon이라는 단어를 듣게 된다면, 물건을 사고파는 뭐 그런 웹사이트 페이지를 떠올리는 분들이 많을 것이다. 허나, Amazon Web Service(AWS)는 Amazon의 그런 이미지와는 다르다. Cloud Service를 제공해주는 것이다. 우선 가입을 해야한다. 참고로 대학생인 분들은 AWS educate로 가입하면 Credit을 받는 방법이 있는데, 뭐 꼭 현재 재학중이지 않아도 자신의 대학교 이메일로 인증이 가능하다면 AWS Educate에 Student신분으로 가입이 가능하다. 만약 대학생이 아닌 분들은 그냥 AWS(AWS와 AWS educate는 다르다.)를 가입해서 사용하면 된다. 참고로 1년 동안은 어느정도 free tier를 주어서 몇몇 서비스들은 무료로 오래 이용가능할 것이다. 필자도 작년 이용했었는데 기간이 만료되어 이번에 다시 다른 계정을 만들었는데, 새로운 계정을 만들면 또 free tier 이용이 가능한 것 같다.(개인적인 생각이지만 아마도 가입시에 적는 신용카드 번호가 다른 것이면 가능한듯 하다.) 또한, 가입시에 적는 신용카드는 결제 카드로 설정되며 free tier로 이용하는 것을 제외한 다른 이용료를 결제할 수 있다. 물론, 자동결제는 아니고 자신이 결제해줘야 하며, 결제를 해주지 않는다면 휴면 계정으로 전환시켜 서비스 이용이 불가능하다.(처음 결제되는 $1는 결제가 되는 카드인지 확인하는 확인용으로 알고있는데 나중에 결제 취소 해주므로 걱정하지 않아도 된다.) 다음의 서비스 중에 우선 RDM을 생성해 줄 것이다. step 1) Create database를 클릭, method에서 직접 Customize하려면 Standard를 체크!(easy 방법은 이미 Instancd Size와 ram 등 사양들을 AWS Image처럼 만들어 놓은 형태로 되어있다.) 또한, Python을 통해 사용할 것이므로 MySQL로 만들 것이다. version은 제일 stable한 5.7.22 version을 선택! Templete은 Free-Tier를 선택! 만약 바로 실무에서 사용해야 한다면 Production을 사용하면 된다. 필자는 연습용으로 만드는 것이므로 Free를 선택 !!! DB instance identifier는 DB의 이름이고, 그 아래 Credentials Settings의 Master username은 DB 접속시 Master 권한을 인증할 ID와 password이다. 아래로 내려 갈수록 저사양 DB이며, 아마 필자와 동일하게 Free-Tier를 선택했다면, 이미 맨 아래 단계로 설정 되어있을 것이다. Free-Tier의 경우에는 다른 사양을 선택할 수 없다. Storage type General Purpose : 주로 저장할 때 사용 Provisioned IOPS(Input Output Per Second): 데이터의 입출력을 빠르게 접근할 수 있게 해야할 경우 사용 Storage autoscaling Enable storage autoscaling은 할당한 자원이 초과되어 다른 여유자원이 있다면, 자동으로 여유자원을 가져와 사용할 수 있게끔해주는 설정이다. 아래에 있는 설정 사항들은 모두 기본값을 설정했다. 이제 맨 아래로 가서 생성을 클릭하면 된다. 참고) Multi-AZ deployment 접속하는 User의 지역에 상관없이 동일하게 Performance를 내도록 할 때 사용 왼쪽의 Database 탭을 클릭하면, 다음과 같이 본인의 DB에 대한 창이 나올 것이다. 아직 생성중일 것이다. 우선, 본인의 DB명을 클릭하자. Connectivity &amp; Security 탭을 살펴보면 Public accessibility가 No로 되어있을 텐데 이것을 Yes로 설정을 바꿔주어야 접속이 가능하다. 아래 Connectivity &amp; Security 탭에서 Security의 빨간네모칸 부분을 누르면 앞으로 DB에 접속이 가능한 프로토콜 설정하거나 관리할 수 있는 페이지로 이동한다. edit 버튼을 눌러 DB에 connect 할 때 접속가능한 프로토콜을 설정해준다. MySQL로 접속이 가능하게끔 설정을 추가해주고 저장한다. 다시 Database 탭으로 돌아가면, 아마도 여러분의 DB가 만들어졌을 것이다. Command Line으로 DB 접속하기command 도움말을 먼저 확인하여 접속시 필요한 옵션들을 알아보자.1mysql --help 접속시에 필요한 간단한 옵션들은 다음과 같다. -h, —host=name =&gt; Connect to host. -p,—password[=name] =&gt; Password to use when connecting to server. If password is not given it’s asked from the tty. -P, —port=# =&gt; Port number to use for connection or 0 for default to, in order of preference, my.cnf, $MYSQL_TCP_PORT, /etc/services, built-in default (3306). -u, —user=name =&gt; User for login if not current user. 접속!! -p 옵션까지만 치면 password를 입력하라고 할 텐데, 입력하면 접속이 된다. 아래에서의 end-point는 DB 생성한 후 해당 DB페이지를 보면 나와있다. 혹시라도 잘 이해가 안가시는 분들을 위해 아래 필자의 접속 커맨드를 예시로 들 것이다. 12345678910111213141516# mysql -h end-point -P 3306 -u userId -pmysql -h spotify.cgaj5rvtgf25.ap-northeast-2.rds.amazonaws.com -P 3306 -u hb0619 -p# ----- MySQL DB 접속 후 -----------# 생성되어 있는 DB목록 확인SHOW TABLES;# Create TableCREATE TABLE people(first_name VARCHAR(20), last_name VARCHAR(20), age INT);# 생성된 DB 사용USE people# 접속시 이미 만들어져 있는 테이블로 바로 사용가능하게 끔 하는 방법# 이 방법은 생성되어 있는 데이터베이스에 한해서만 작동 가능하다.mysql -h spotify.cgaj5rvtgf25.ap-northeast-2.rds.amazonaws.com -P 3306 -D people -u hb0619 -p","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"선형 대수 공부할 때 도움되는 사이트","slug":"linear_algebra_00","date":"2019-12-14T07:24:18.000Z","updated":"2020-02-04T17:54:33.939Z","comments":true,"path":"2019/12/14/linear_algebra_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/14/linear_algebra_00/","excerpt":"","text":"기초 선형 대수 공부할 때 도움되는 사이트 아래의 설명들은 사이트들에 대한 주관적인 의견이므로, 개인마다 차이가 있을 것이다. 영어가(도) 편하신 분들을 위한 자료 선형대수 cheat sheet 엄청 기본적인 개념들을 모르는 분들께만 추천 Gilbert Strang 교수님 강의 기본적인 개념부터 차근차근 그리고 조금은 직관적으로 선형대수를 공부하고 싶다면 추천 CS231n Numpy tutorial 기본 개념을 조금은 익힌 후 Python을 통해 실습해 보고 싶은 분들께 추천 한국어가 편하신 분들을 위한 자료 인공지능을 위한 선형대수 in edwith 위의 개념을 위주로 한 강의를 들은 후에 복습차원에서 빠르게 듣는 것을 추천한다. 개념에 대한 설명이 부족한 강의는 절대적으로 아니지만 위의 개념적인 강의를 들으면서 스스로 먼저 생각하고 고민한 뒤에 이 강의를 수강하면 더 효과적일 것 이다. 또한 중간 중간 Python에 의한 실습도 제공한다. 칸 아카데미 기본적인 개념부터 시작해서 그래프를 통해 기하학적인 부분을 많이 보여주는 강의.(영어 강의도 가능) 한양대 이상화 교수님 선형대수 강의(Kocw) 개인적으로 이 강의를 제일 먼저 접하게 되었었고, 추후에 길버트 교수님의 강의와 비교하면 개념에 대한 설명이 좀 더 공학적이지만, 예시를 통한 설명으로 극복이 가능하다. 필자는 머리가 뛰어난 편은 아니어서 처음 들었을 땐 솔직히 노트에 적으면서 공부했어도 이해를 하지 못했으나, 두번째 들으면서 각 개념들의 연결고리를 생각하고 이해하며 듣게되어 훨씬 좋았다.","categories":[{"name":"linear algebra","slug":"linear-algebra","permalink":"https://heung-bae-lee.github.io/categories/linear-algebra/"}],"tags":[]},{"title":"추천시스템(Recommendation System)","slug":"Recommendation_System_00","date":"2019-12-14T02:34:02.000Z","updated":"2019-12-15T05:43:25.915Z","comments":true,"path":"2019/12/14/Recommendation_System_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/14/Recommendation_System_00/","excerpt":"","text":"1) 추천 시스템(Recommendation System)이란?위키백과의 정의를 통해 먼저 정리해보자! 정보 필터링 (IF) 기술의 일종 특정 사용자가 관심을 가질만한 정보 (영화, 음악, 책, 뉴스, 이미지, 웹 페이지 등)를 추천하는 것 ㄴ.종류(Different Types of Recommendation Engines) :1) 협업 필터링 기법(Collaborative filtering) 기본적인 가정이 과거에 동의한 사람들이 미래에도 동의하고 그들이 과거에 좋아했던 것들을 좋아할 것이라는 가정을 바탕으로 다른 사용자와의 비슷함에 기초를 두고 사용자들이 무엇을 좋아할 지를 예측하는 것에 기초에 두고 있다. Linkedin, facebook과 같은 SNS는 collaboprative filtering을 친구 추천 등에 사용한다. 가장 큰 장점인 machine analyzable content에 의존하고 있지 않다는 점으로 인해 정확하게 item 그 자체를 이해하지 않고도 영화와 같은 복잡한 item들을 추천 할 수 있다. 주로 사용되는 알고리즘 : KNN, Pearson Correlation 모델을 만들 때, feature는 Explicit하거나 Implicit한 data collection 사이에서 만들어진다. Explicit data collection의 예 사용자에게 item을 평가하게하기, 검색하게 하기, 가장 선호하는 것과 가장 덜 선호하는 것을 순위매기게 하기 등 Implicit data collection의 예 사용자가 본 item을 관찰하고 분석하기, 사용자가 구매한 item을 기록하기, 사용자의 SNS를 분석하고 비슷한 likes와 dislike를 찾아내기! 2) 컨텐츠 기반 필터링(Content-based filtering) Keyword(item을 설명(describe)하는데 사용함.), Profile(사용자가 좋아하는 type의 item을 가리키게(indicate) 만들어짐.)을 통해 과거에 사용자자가 좋아했던 것들 (또는 현재 보고 있는 것들)과 비슷한 items을 추천하려고 한다. 다양한 후보 items는 사용자에 의해 평가되는(rated) items와 비교되고 그 중 best-matching items를 추천한다. Pandora Radio는 첫 seed와 같이 사용자에 의해 제공된 노래와 비슷한 특징의 음악을 재생해 주는 content-based recommendation System이다. 이 접근법은 집합적 정보로부터 원하는 내용이나 관련된 내용을 가져오는 Inforamtion retrieval과 필요없는 정보를 제거하는 Information filtering에 뿌리를 두고 있다. Items의 특징(Keyword)을 끌어내기 위해 TF-IDF(Term frequency-inverse document frequency)를 사용한다 User의 profil을 만들기 위해서, 그 시스템은 대게 두가지 정보에 집중한다. 1) 사용자의 선호의 model 2) 추천시스템과 사용자의 상호작용 정보(history) 기본적으로 이런 방법들은 시스템 안에서 item에 특성을 부여하면서 item profile(이산적 feature와 attributes)을 사용한다. 그 시스템은 item 특성의 weighted vector을 기반으로 한 사용자의 content-based profile을 만든다. Weights는 사용자에게 각각의 feature의 중요도를 나타내고 개별적으로 점수 매겨진(rated) content vector로 부터 다양한 방법으로 계산될 수 있다. 사용자가 좋아할 것 같 확률을 계산하기 위해 복잡한 방법들(베이지안 분류, 클러스터 분석, 결정트리, 그리고 인공 신경망 네트워크왁 같은 머신러닝 기술을 사용하는 반면에, 간단한 접근법들은 그 점수 매겨진 item vector의 평균 값을 사용한다. 보통 ‘좋아요’와 ‘싫어요’와 같은 형태로 사용자로부터 직접적인 피드백은 특정한 속성(attribute)의 중요도에 대한 더 높거나 낮은 weight를 할당하는데 사용될 수 있다. Content-based filtering의 중요한 문제점은, 하나의 content source에 관련된 사용자들 행동으로부터 사용자 선호도를 배울 수 있고 다른 content 종류(type)에 대해서도 배운 사용자 선호도들을 적용시킬 수 있는지에 대한 여부이다. 그 시스템이 다른 서비스의 다른 종류의 content를 추천할 수 있는 것보다 사용자가 이미 사용한 것과 같은 종류의 content를 추천하는 것에 한정돼 있다면 해당 추천 시스템의 가치는 상당히 낮을 것이다. 예를 들어, news browsing에 기반한 추천 뉴스 기사는 유용하지만, news browsing에 기반해 추천될 수 있는 다른 서비스의 음악, 비디오, 제품 토론에서 더 유용하다. 참고) TF-IDF 정보검색과 텍스트 마이닝에서 이용하는 가중치로, 여러 문서로 이루어진 문서군이 있을 때 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치이다. 문서의 핵심어를 추출하거나 검색엔진에서 검색 결과의 순위를 결정하거나,문서들 사이의 비슷한 정도를 구하는 등의 용도로 사용할 수 있다. IDF 값은 문서군의 성격에 따라 결정된다. 예를 들어, ‘원자’라는 낱말은 일반적으로 문서들 사이에서는 잘 나오지 않기 때문에 IDF 값이 높아지고 문서의 핵심어가 될 수 있지만, 원자에 대한 문서를 모아놓은 문서군의 경우 이 낱말은 상투어가 되어 각 문서들을 세분화하여 구분할 수 있는 다른 낱말들이 높은 가중치를 얻게 된다. 특정 문서 내에서 단어 빈도가 높을수록, 그리고 전체 문서들 중 그 단어를 포함한 문서가 적을수록 TF-IDF값이 높아진다. 따라서 이 값을 이용하면 모든 문서에 흔하게 나타나는 단어를 걸러내는 효과를 얻을 수 있다. IDF의 로그 함수값은 항상 1이상이므로, IDF값과 TF-IDF값을 항상 0 이상이 된다. 특정 단어를 포함하는 문서들이 많을 수록 로그 함수 안의 값이 1에 가까워지게 되고, 이 경우 IDF값과 TF-ID값은 0에 가까워지게 된다. TF-IDF = TF \\times IDF TF(Term Frequency, 단어 빈도) 특정한 단어가 문서 내에 얼마나 자주 등장하는지를 나타내는 값 산출 방식 ㄴ. TF :tf(t, d) = f(t, d) (f(t, d) : 문서 d 내에서 단어 t의 총 빈도) ㄴ. Boolean TF :tf(t, d) = t가 d에 한 번이라도 나타나면 1, 아니명 0 ㄴ. log scale TF :tf(t, d) = \\log(f(t, d) +1) ㄴ. 증가 빈도 TF : 일반적으로는 문서의 길이가 상대적으로 길 경우, 단어 빈도값을 조정하기 위해 사용 tf(t, d) = 0.5 + \\frac{0.5 \\times f(t, d)}{max{f(w, d) : w \\in d}} = 0.5 + \\frac{0.5 \\times target 단어에 대한 TF}{동일 문서(문장)내에서의 최빈단어의 빈도수} IDF(Inverse Document Frequency, 역문서 빈도) TF 값이 높을 수록 문서에서 중요하다고 생각될 수도 있지만 단순히 흔하게 등장하는 것일 수도 있다. 이값을 DF(Document Frequency, 문서 빈도)라고한다. 영어문장에서 예를 들자면 가령 I, you 같은 단어들을 예로 들 수 있을 것이다. DF의 역수를 IDF(Inverse Document Frequency, 역문서 빈도)라고 한다. 한 단어가 문서 집합 전체에서 얼마나 공통적으로 나타나는지를 나타내는 값 IDF 값은 문서군의 성격에 따라 결정된다. 예를 들어, ‘원자’라는 낱말은 일반적으로 문서들 사이에서는 잘 나오지 않기 때문에 IDF 값이 높아지고 문서의 핵심어가 될 수 있지만, 원자에 대한 문서를 모아놓은 문서군의 경우 이 낱말은 상투어가 되어 각 문서들을 세분화하여 구분할 수 있는 다른 낱말들이 높은 가중치를 얻게 된다. 산출 방식 $ \\mid D \\mid$ : 문서 집합 D의 크기, 또는 전체 문서의 수 $ \\mid d \\in D : t \\in d \\mid$ : 단어 t가 포함된 문서의 수(즉, $tf(t,0) \\neq 0$). 단어가 전체 말뭉치(Corpus)안에 존재하지 않을 경우 이는 분모가 0이 되는 결과를 가져온다. 이를 방지하기 위해 $1 + \\mid d \\in D : t \\in d \\mid $로 쓰는 것이 일반적이다. idf(t, D) = \\log \\frac{ \\mid D \\mid}{ \\mid d \\in D : t \\in d \\mid} = \\log \\frac{전체 문서의 수}{해당 Target단어를 포함한 문서의 수}3) Hybrid Recommendation Systems 위의 2가지 Recommendation System들은 각각의 장점과 단점이 존재함을 살펴보았다. 그래서 최근 연구는 Collaborative filtering과 content-based filtering을 섞은 Hybrid 접근법이 몇몇의 상황(Cold start(충분한 정보가 없어서 필요한 정보를 얻지 못하는 것)) Sparsity)에서 더 효과적일 수 있다고 설명한다. Hybrid 추천 시스템이란 용어는 아래의 각 시스템별 단점들을 보완하기 위해 다중의 추천 기술을 함께 섞는 어떠한 추천 시스템을 의미하며 다중의 추천 기술이 내포하고 있는 의미는 동일한 기술을 여러개 겹치는 것도 포함된다. Collaborative 이 시스템은 다른 사용자들과 items에 대한 profiles을 평가하는 정보만 사용하면서 추천을 한다. 이 시스템은 현재의 사용자나 items과 비슷한 평가 기록(history)와 함께 비슷한(peer) 사용자 또는 items을 배치하고, 이 근접이웃(neighborhood)를 이요해서 추천을 만든다. 사용자 기반과 item기반의 가장 가까운 이웃 알고리즘은 cold-start문제를 해결하기 위해 합쳐질 수 있고 추천 결과를 향상시킬 수 있다. Content-based 이 시스템은 사용자가 그들에게 준 평가와 제품들과 관련된 특징이라는 두가지 Sources로부터 추천을 만든다. Content-based 추천자는 추천을 user-specific 분류 문제처럼 다루고 제품의 특징에 기반한 사용자의 좋아요와 싫어요의 분류자를 학습한다. Demographic Demographic(인구 통계학적) 추천은 사용자의 인구통계학적 정보(profile)를 기반으로 추천을 제공한다. 추천된 제품은 그 영역의 사용자들의 평가들을 합침으로써 다른 인구통계학적 영역을 위해 만들어 질 수 있다. Knowledge-based 이 추천자는 사용자의 선호와 요구(needs)에 대한 추론을 기반으로 한 제품을 제안한다. 이 지식(knowledge)는 때때로 얼마나 특정한 제품 특징이 사용자의 요구를 충족시키는 지에 대한 뚜렷한 기능적(functional) 지식을 포함한다. Netflix는 Hybrid 추천 시스템의 좋은 예이다. 사용자가 높게 평가했던(Content-based)영화와 비슷한 feature를 띄는 영화를 추천하고, 비슷한 사용자(collaborate)들의 검색 습관과 시청을 비교함으로서 추천을 한다. 정확도를 넘어서전형적으로, 추천 시스템에 대한 연구는 가장 정확한 추천 알고리즘을 찾는 것에 관심을 둔다. 하지만, 많은 중요한 요소들이 있다. Diversity 사용자들은 자신이 선택한 Item과 유사성이 높은 intra-list에 포함된 다른 아티스들을 보이는 다양성을 갖춘 추천 시스템에 더 만족하는 경향을 보인다. Recommender persistence 어떤 상황에서, 추천시스템이 그 이전의 추천과 동일한 추천을 다시 보여주거나 사용자가 다시 items을 평가하게 하는 것이 더 효과적이다. Privacy 추천 시스템은 대게 privacy 문제를 해결해야 한다. 왜냐하면 사용자들은 민감한 정보를 공개해야하기 때문이다. Collaborative filtering을 사용해 사용자의 Profiles을 만드는 것은 privacy의 관점에서 문제가 될 수 있다. 많은 유럽 국가들은 data privacy에 대한 강한 문화를 가지고 있고, 사용자의 profile을 만드는 어떠한 단계를 소개하려는 시도는 부정적인 사용자 반응을 초래할 수 있다. 실제로 Netflix는 데이터 대회를 통해 데이터를 공개했다가 비식별화된 데이터와 다른 데이터를 연결함을써 개인을 식별할 수 있게 됨을 확인했고, 고소까지 당했었다. 그 이외의 주의사항 참고 문헌 및 사이트 웹 사이트 잔재미 코딩 leebaro blog svd의 활용에 관한 darkpgmr님의 블로그 NMF 알고리즘을 이용한 유사한 문서검색과 구현 Python을 이용한 행렬의 분해 예제 naver 검색엔진 추천시스템 airs개발기(2017 Deview) Movie recommendation system 예시 sanghyukchun님의 github blog(Recommendation System) 추천시스템을 위한 Deep learning 문헌 Building Recommendation Engines Recommender Systems in E-CommerceJ. Ben Schafer, Joseph Konstan, John Riedl State of the Art Recommender System. Laurent Candillier, Kris Jack Recommender Systems in E-Commerce. Sanjeevan Sivapalan, Alireza Sadeghian A Survey of e-Commerce Recommender Systems Farida Karimova, PhD Low-Rank Matrix Completion (2013) by Ryan Kennedy Exact Matrix Completion via Convex Optimization Emmanuel J. Cand`es","categories":[{"name":"Recommendation System","slug":"Recommendation-System","permalink":"https://heung-bae-lee.github.io/categories/Recommendation-System/"}],"tags":[]},{"title":"data engineering (API는 무엇인가?!?)","slug":"data_engineering_03","date":"2019-12-13T04:25:42.000Z","updated":"2020-02-20T07:34:05.544Z","comments":true,"path":"2019/12/13/data_engineering_03/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/13/data_engineering_03/","excerpt":"","text":"REST API의 정의와 예제들API(Application Programming Interface) 두 개의 시스템이 서로 상호 작용하기 위한 인터페이스 데이터를 주고 받는 인터페이스 API라고 하면 보통 REST API를 지칭 웹사이트는 HTTP(S)프로토콜을 사용하는 REST API 기반으로 구축 API 접근 권한Authentication VS Authorization Authentication : Identity(정체)가 맞다는 증명 Authorization : API를 통한 어떠한 액션을 허용 API가 Authentication으로 하여도 어떠한 액션에 대해서는 Authorization을 혀용하지 않을 수 있음API의 필수는 첫째도 둘째도 Security 어떠한 Security 방안이 없을 경우 DELETE request를 통해서 다른 이용자의 정보를 지울 수도 있음 제 3자에게 데이터 유출로 이어질 수 있음 누가 API를 사용하는지, 어떤 정보를 가져가는지 트래킹 할 수가 없음 API Key란 무엇인가? API Key란 보통 Request URL혹은 Request 헤더에 포함되는 긴 String Basic Auth OAuth 2.0 설명하자면 Web API는 우리가 직접 어떤 Action을 하는 것이기 때문에 해당 Web에만 접근 권한을 받으면 되지만, 그와는 다르게 어떤 Action을 취할 Web을 다른 앱에게 접근 권한을 주어 End User인 우리 대신 정보를 제공하게 해주는 방식이다. 예를들면 우리가 어떤 서비스를 가입하려고 할때 SNS로 가입이 가능하게 할 수 있는 접근 권한 방식이라고 할 수 있다. Endpoints &amp; Methods Resource는 API를 통해 리턴된 정보이며, 하나의 Resource 안에 여러개의 Endpoints가 존재한다. Parameters Parameters는 Endpoint를 통해 Requests 할때 같이 전달하는 옵션들 Request Body안에 포함되는 Parameter들은 post 방식에서 주로 많이 사용한다. Spotify 필자의 프로젝트의 주된 data를 제공받을 Spotify를 먼저 소개하기 전에, 왜 국내의 Melon과 Genie를 택하지 않았는지를 말하려고 한다. SK플래닛이 2012년부터 운영하던 개발자센터 내 오픈 API 서비스를 2018년 3월부터 중단한다고 발표함으로써 Melon의 API 서비스를 시기적으로 사용하지 못하게 되었으며, Genie 또한 API 서비스를 더 이상 제공하지 않고 있다. 단순히 곡명과 아티스트명, 해당 곡에 대한 댓글등 이런 것들은 Selenium이나 그냥 기본적인 requests 모듈을 통해 가능하지만, 이번 토이 프로젝트의 목표는 Spotify API를 통하여 엔터티간의 관계를 내가 직접 설계해 보고 이미 만들어 있긴 하지만 각 곡들의 특징을 수치적으로 분류해 놓은 특징들로 유사도를 계산하여 User에게 추천하는 Facebook 앱을 만들어 보고 싶어서이다. 그렇다면, Spotify는 무엇인가? 스포티파이는 프리웨어이다. 본래에는 무료로 이용하면 시간 제한이 있었으나, 2014년 폐지되었다. 스포티파이는 Spotify 웹사이트에서 바로 다운받을 수 있다. 제공되는 곡들은 음반사들이 라이선스하여 합법적으로 제공한 것이다. 하지만, 사용자가 한 달 9.99 유로의 서비스 사용료를 내지 않는다면, 소프트웨어 상에 광고가 표시되며, 곡과 곡 사이에 광고가 삽입된다. 가입자가 서비스 사용료를 냈다면, 가입자는 자동적으로 “프리미엄 사용자” 상태가 된다. 프리미엄 사용자들은 특별히 뉴스나 프리뷰를 들을 수 있다. 또한 developer API를 제공함으로써, API 사용법에 대한 상세한 설명이 있다.Spotify developer앞으로 이 사이트에 있는 API 사용법을 활용하여 데이터를 crawling한 후에 RDS에 저장할 것이다. Spotify for Developers API를 사용하기 위해서는 접근 권한이 있는 Access ID와 password를 발급받아야 하므로 먼저, APP을 만들것이다. 위의 탭란에서 DashBoard를 클릭해보면 다음과 같은 페이지로 이동할 것이다. login이 필요하므로 먼저 가입을 해야 할 것이다. 옆에 있는 Sign up for free Spotify account here 버튼을 눌러 필자가 사용할 Access 방법은 위에서 언급했던 Oauth 2.0을 활용한 방식은 아니고, 그냥 Access ID와 Password를 발급받은 후 Access하는 방식을 택했다. 약간의 주의사항은 발급받은 후 1시간 경과 후에는 Password를 재발급받은 후 사용하여야 한다. Client Credentials Flow 방식은 만들어 놓은 application(dash board)을 통해서 client id와 client secret을 spotify에게 주게 되는데, spotify는 다시 한번 Access Token을 반환해 주게된다. 이 Access Token을 통해 data를 가져 올 수 있게 된다. Client Credentials Flow는 POST https://accounts.spotify.com/api/token를 통해 request 할 수 있다. 아래 그림을 보면서 조금 더 자세하게 말하자면, requests를 post방식으로 얻게 되는데, request body parameter에는 client credential값을 꼭 필요로 하며, header parameter에는 Basic 다음 문자열로 을 넣어주어야한다. 그리고 받은 Access Token을 이용해\b서 “Authorization”: “Bearer {Access token}”형태로 headers에 넣어 request를 get 방식으로 해주\u001c면 하고싶은 메서드를 사용할 수 있다. 여기서 문제가 생겼다. 국제적인 서비스여서 국내에서도 이용 제한이 없을 것이라고 생각했지만, 국내에서는 서비스를 아직 하지 않고 있다한다……. 결국 방법을 찾아보다 VPN을 사용하여 우회한 후에 가입을 하면 서비스 이용이 가능하다는 사실을 알게 되었고, VPN을 이용하여 가입하였다. 가입한 후에 Application을 등록해보자. App의 이름과 설명 개발 용도를 입력해주는데, App 개발 용도는 미정이므로 모른다고 설정했다. 다 만들어 졌다면, 다음과 같이 새로운 앱이 생성되었음을 확인 할 수 있다. 우리가 API에 접근할 때 필요한 ID와 password 정보를 보려면 앱을 클릭하면 확인 할 수 있다. password는 hide되어있는데 hide를 풀면 볼 수 있다. 1시간 마다 password는 reset해서 사용해야한다. API를 사용법을 통해 결과적으로 만들 ERD(Entity Relationship Diagram)는 다음과 같다. 위의 ERD를 만들기에 앞서서 Spotify API를 통해 Access Token을 얻고 search할 수 있도록 script 파일을 만들어 본다면 다음과 같은 것이다. 지속적으로 유지를 하기 위해서 status code마다 처리하는 방식을 달리 해주어야 할 것이다.특히 그 중에 status code 중 429는 너무 많은 데이터를 요청 했을 때 출력값으로 받는 status code이며, 받은 출력안에 제한시간이 같이 존재하므로 그 시간동안 python 동작으로 멈추도록 sleep을 걸어놓게끔 코드를 작성할 것이다. Spotify의 status code 또한, status code = 401인 경우에는 access token이 expired되었기 때문에 일어나는 경우이기 때문에 새롭게 client secret을 받아야 할 것이다.","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"Basic ConvNN(VGG-16모방한 기본)구현","slug":"deep_learning_05","date":"2019-12-13T03:58:02.000Z","updated":"2020-01-12T16:26:13.503Z","comments":true,"path":"2019/12/13/deep_learning_05/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/13/deep_learning_05/","excerpt":"","text":"Basic ConvNN 구현 참고로 저는 mac을 사용하기에 local에서말고 GPU를 사용하게끔 Google Colab을 사용하였다. 제가 구현한 방식은 tensorflow 2.0 version이므로(tf.function을 사용하느라) colab의 tensorflow의 version이 뭔지 먼저 확인했습니다. 1.15 version이어서 2.0으로 설치를 진행한 후 코드를 실행하였습니다. 참고로 2.0으로 설치하고 난 후에는 꼭 반드시 런타임을 재시작 해주셔야 업데이트 한 2.0 version으로 사용하실 수 있습니다. 123import tensorflow as tfimport numpy as npprint(tf.__version__) 런타임 재시작 후1!pip install tensorflow==2.0.0-beta1 기본 합성곱 신경망 구현12import tensorflow as tfimport numpy as np 하이퍼 파라미터1EPOCHS = 10 참고로 conv layer을 통과한 출력의 dimension을 계산하는 것은 다음과 같다.padding : 2N+1 = kernel_size(Filter_size)로 N을 구한다.output dimension :\\frac{input\\hspace{0.1cm} size + (padding\\hspace{0.1cm} size * 2) - filter\\hspace{0.1cm}size}{strid} + 1모델 정의123456789101112131415161718192021222324252627282930313233class ConvNet(tf.keras.Model): def __init__(self): super(ConvNet, self).__init__() self.sequence = [] conv2d = tf.keras.layers.Conv2D max_pool = tf.keras.layers.MaxPool2D flatten = tf.keras.layers.Flatten # filters = 16 (출력되는 channel의 수) # kernel_size = 3 * 3 # padding의 default값인 'valid'는 zero-padding을 해주지 않음으로써 영상의 크기가 Conv layer를 통과함으로써 줄어들 수 있다. # 'same'은 zero-padding을 의미 여기서는 동일한 크기를 유지하기 위해 # input data는 (28x28x1)을 갖는 MNIST이다. # VGG-16의 가장 큰 특징은 Pooling을 하기 전에 동일한 Conv Layer를 반복해서 사용하는 것이다. self.sequence.append(conv2d(16, (3,3), padding='same', activation='relu')) # output dimension (28x28x16) self.sequence.append(conv2d(16, (3,3), padding='same', activation='relu')) # output dimension (28x28x16) # 2x2 pooling을 한다. maxpooling을 이용하여 영상의 크기를 줄여준다. self.sequence.append(max_pool((2,2))) # output dimension (14x14x16) self.sequence.append(conv2d(32, (3,3), padding='same', activation='relu')) # output dimension (14x14x32) self.sequence.append(conv2d(32, (3,3), padding='same', activation='relu')) # output dimension (14x14x32) self.sequence.append(max_pool((2,2))) # output dimension (7x7x32) self.sequence.append(conv2d(64, (3,3), padding='same', activation='relu')) # output dimension (7x7x64) self.sequence.append(conv2d(64, (3,3), padding='same', activation='relu')) # output dimension (7x7x64) self.sequence.append(flatten()) # 1568x1 self.sequence.append(tf.keras.layers.Dense(2028, activation='relu')) self.sequence.append(tf.keras.layers.Dense(10, activation='softmax')) def call(self, x, training=False, mask=None): for layer in self.sequence: x = layer(x) return x 학습, 테스트 루프 정의1234567891011121314151617181920# Implement training loop@tf.functiondef train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy): with tf.GradientTape() as tape: predictions = model(images) loss = loss_object(labels, predictions) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) train_loss(loss) train_accuracy(labels, predictions)# Implement algorithm test@tf.functiondef test_step(model, images, labels, loss_object, test_loss, test_accuracy): predictions = model(images) t_loss = loss_object(labels, predictions) test_loss(t_loss) test_accuracy(labels, predictions) 데이터셋 준비1234567891011121314151617181920212223mnist = tf.keras.datasets.mnist# 입력 영상이 총 8bit 즉, 0~255 사이의 값들로 이루어져 있으므로(x_train, y_train), (x_test, y_test) = mnist.load_data()# 0~1표현으로 바꿔준다.x_train, x_test = x_train / 255.0, x_test / 255.0# 입력 영상 하나의 사이즈는 28x28이므로 channel을 하나 더 늘려 주어야한다.print(x_train.shape)print(x_train[0].shape)# x_train : (NUM_SAMPLE, 28, 28) -&gt; (NUM_SAMPLE, 28, 28 , 1)# ...은 해당 데이터 객체의 모든 axis를 표현하는 것이다.# 위에서 255.0으로 나누어주게 되면 float64로 되므로 자료형을 float32로 해야 error가 없다.## x_train[:,:,:, tf.newaxis]x_train = x_train[..., tf.newaxis].astype(np.float32)x_test = x_test[..., tf.newaxis].astype(np.float32)# Numpy object나 Tensor로 부터 데이터셋을 구축할 수 있다.train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)# test data는 suffle할 필요없다.test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32) 학습 환경 정의모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의12345678910111213# Create modelmodel = ConvNet()# Define loss and optimizerloss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()# Define performance metricstrain_loss = tf.keras.metrics.Mean(name='train_loss')train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')test_loss = tf.keras.metrics.Mean(name='test_loss')test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy') 학습 루프 동작123456789101112131415161718for epoch in range(EPOCHS): for images, labels in train_ds: train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy) for test_images, test_labels in test_ds: test_step(model, test_images, test_labels, loss_object, test_loss, test_accuracy) template = 'Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;' print(template.format(epoch + 1, train_loss.result(), train_accuracy.result() * 100, test_loss.result(), test_accuracy.result() * 100)) # reset_state는 새로운 값들을 받기 위해 하는 건가? train_loss.reset_states() train_accuracy.reset_states() test_loss.reset_states() test_accuracy.reset_states()","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"Convolution Neural Network(1)","slug":"deep_learning_04","date":"2019-12-10T04:50:24.000Z","updated":"2020-01-20T05:53:41.787Z","comments":true,"path":"2019/12/10/deep_learning_04/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/10/deep_learning_04/","excerpt":"","text":"합성곱 연산과 이미지 필터 아날로그 신호처리는 선형이고 시불변인 시스템에 의존해서 개발이 되게 되는데, Noise가 있는 입력이 들어왔을 때 넣어주면 Noise가 제거된 출력이 나오는 이런 시스템을 모두 LTI system이라고 부른다. 디지털 신호가 아닌 아날로그 신호로부터 LTI system이 정의되어있다. 선형이라는 것은 대부분 알고 있듯이 선형대수에서 나오는 linearity를 만족시키면 되는 것이고, 시불변이라는 의미는 시간이 지나도 동일한 결과를 내보내준다는 의미이다. 확률과정에서 step에 영향을 받지 않는다라고 보면 좋을 것 같다. 사람은 대표적으로 LTI 시스템이 아닌 시스템이다. 수학적으로는 엄밀하진 않지만, 공학에선 많이 사용한다. 왼쪽의 삼각형을 모든 구간에 대해 전부해준다면 값은 1이 될 것이다. 여기서 $h\\to\\infty$가 되면, Dirac 델타 함수가 된다. 시간 t=0만 임의의 값을 갖고, 나머지 구간은 0을 갖는다. 모든 구간에서 적분한 값이 1 convolution을 한다는 것은 임의의 두 함수 중 한 함수를 좌우로 뒤집고 이동시키면서 두함수의 곱을 적분하여 계산한다. 합성곱 계산 animation vertical Sobel Filter가 왜 미분 필터이냐고 의문이 든다면, 1차원 신호를 data로 생각하고 앞서 했었던 수치 미분을 떠올려 보자. 그렇다면, 단숨에 이해가 갔을 것이다. 지금은 vertical Sobel Filter이므로 가로의 Edge는 추출하지 못한 것을 확인할 수 있다. 그에 반해 세로 성분들은 잘 검출된 것을 확인 할 수 있다. 만일 위의 필터를 90도 rotate해주게되면 가로로 미분하는 horizonal Sobel Filter가 되어 위의 결과와는 반대의 결과를 보여줄 것이다. 합성곱 계층 입력이 이제는 영상으로 여러개 들어오게 되어 각각의 입력층의 뉴런 하나 하나가 channel이라고 불린다. 필터 가중치는 보통 3x3, 5x5, 7x7등을 주로 사용한다. 2D signal과 2D signal을 곱(element-wise product or Hadamard product)해야하므로 합성곱을 사용한다. $kernel_{Height} \\times kernel_{Width} \\times channel_{in} \\times channel_{out}$ 만큼의 parameter가 필요하다. filter는 $Channel_{in} \\times Channel_{out}$개 만큼 있을 것이다. kernel(Filter)에 나타나는 모양과 유사한 모양을 한 위치가 높은 값으로 나타나게 된다. 기본적인 합성곱 신경망 stride요소를 넣지 않으면, 합성곱 계층에서는 영상의 크기는 그대로이며, 영상의 채널 수가 달라진다. kernel(filter)가 돌아다니면서 포착하는 형태이기 때문에 공간적인 특징이 있고, 따라서 Feature Map이라고 한다. classification에서는 Max Pooling이 주로 잘 먹힌다! -Convolutional Layer와 FC Layer를 연결해주기 위해 필요하다 먼저, 맨 처음 언급했던 머신러닝과 딥러닝의 가장 큰 차이는 사람이 feature를 넣어주느냐 그렇지 않느냐의 차이라고 했다. 크게 보면 위의 그래프에서 합성곱 계층과 활성함수의 과정을 N번 반복하는 것은 shallowNN의 input으로 넣어 줄 Feature를 뽑는 과정이라고 직관적으로 이해할 수 있다. 앞의 합성곱 계층에서 activation function 까지를 계속해서 진행 할수록 Feature Map의 크기(width와 height)는 Kernel과 Pooling에 의해서 줄어들고 channel(depth)는 늘어나게 될 것이다. 또한 처음부터 끝까지 동일한 크기의 kernel(Filter)을 사용한다고 가정한다면, 영상에서 더 넓은 영역을 커버하는 효과를 주는 것과 동일하다. 그래\u001d서 Feature Map을 한번 뽑을 때마다 Pooling을 해주면\u001c서 처음에는 좁은 영역을 점점 더 넓은 영역을 본다. 점점 더 넓은 영역을 본다는 의미는 Pooling을 함으로써 결국에는 더 넓은 범위를 대표하는 값들을 가진 2-D Matrix인 Feature Map이 될 것이기 때문이다. 98년도의 르쿤 교수님의 LeNet-5는 pooling 대신 subsampling을 사용하여 같은 Feature Map의 크기를 줄여주었다. 합성곱 신경망의 심화 이해 간단히 생각하면 $kernel_{height} \\times kernel_{width} \\times Channel_{in} \\times Channel_{out}$ 만큼 어마어마하게 많은 Parameter가 필요하므로 계산해야 할 Para\u001cmeter가 상대적으로 적은 FC Layer로 하는 것이 더 좋은 방법이지 않을까라고 생각하시는 분들이 있을 것이다. 허나, 그것은 잘못된 생각이다. Convolutional Layer를 사용하기 때문에 우리가 Image를 처리할 수 있는 것이다. FC Layer를 사용하게 되면 오히려 계산해야 할 Parameter의 개수가 어마어마하게 늘어난다.$(Height_{in} \\times Width_{in} \\times Channel_{in}) \\times (Height_{out} \\times Width_{out} \\times C_{out})$ 얼핏 보기엔 비슷해보이겠지만, 예를 들어보자. 입력으로 RGB channel을 갖는 1024 * 1024 image를 받는다면, FC Layer를 사용한다면, $(1024 \\times 1024 \\times 16) \\times (1024 \\times 1024 \\times 32)$이지만 Convolutional Layer를 사용하고 $3 \\times 3$ kernel을 사용한다면 $(3 \\times 3 \\times 16 \\times 32)$로 훨씬 적은 연산을 한다. 이러한 이유로 우리가 영상을 입력으로 하는 것은 절대로 FC Layer를 통해 해결할 수 없다. 위에서 W는 kernel들을 $C_{in} \\times C_{out}$ Matrix로 이루어진 tensor이다. 즉, $W_{i,j}$들이 각각의 kernel을 나타내고 $X_{i}$와 convolution operation을 해주므로 편향은 FC Layer와 동일하게 channel 1개마다 1개씩 존재한다. 위의 그림의 예를 보면 kernel size가 $3 = 2N+1$이므로 입력에 상하좌우 1개의 Zero-Padding을 해준 것이다. \bStride를 하는 것은 결과를 미리 Convolution을 Full로 다 연산을 한 다음에 하나씩 Subsample하는 것과 동일한 결과를 가져온다. 그러므로 다 연산한 후에 subsampling을 하면 연산은 다하지만 결국엔 버리는 값이 생기기 때문에 Stride를 사용한다. 학습 초반에는 위쪽의 Feature Map들 처럼 좁은 범위의 Feature들을 추출하지만, 학습의 후반 부에는 넓은 범위의 Feature들을 학습한다. Batch Normalization(배치 정규화) 일반 경사 하강법의 경우, Gradient를 한번 업데이트 하기 위해 모든 학습 데이터를 사용한다. 하지만 데이터가 엄청나게 많다면?? 그렇다면 Gradient를 업데이트하는데 오랜시간이 소요될 것이다. 그렇다면 SGD는??!! Stochastic은??? Epoch마다 데이터 순서를 섞어주기도 하는 이유는 random성을 더 강하게 해주기 위해서이다. 이런 현상을 해결하기 위한 것이 batch normalization이다. 또한, 동일한 scale과 동일한 zero-mean을 가지게 되기 때문에 학습률 결정에 유리하다 말의 의미는 학습을 할 때 더 scale이 큰 경우에는 학습이 많이 되고, scale이 작\u001d으면 학습이 적게 되는 문제가 발생할 수 있다. 학습률을 너무 크게 할 경\u001c우 Gradient가 크게 나오는 곳에 Gradient exploding이 발\u001d생할 수가 있고, 반대로 학습률을 너무 작게 할 경우 Gradient Vanishing이 발생되서 학습이 안되는 곳이 발생\u001c되는 문제가 있는데 Normalization을 해주게 되\u001c\u001c면 모든 각각의 계층들이 동일한 scale로 학습되기 때문에 학습률 결정에 유리하다는 것이다.(미분을 할때 입력에 대해서 출력값이 커지게 되면, Gradient값도 커질 것이다.) 각각의 batch를 normalization하면, 말 그대로 normalization이 된 것이므로 모수가 $\\mu=0, \\sigma^2=1$인 gaussian distribution을 갖게 될 것이다. 그런데 activation함수는 LeRu를 사용한다면 0미만인 것들은 모조리 0값으로 반환될것이다. 이미 연산을 해놓은 값들을 연산 하기 전이 아닌 연산후에 0으로 만들어 의미 없게 만드는 것 보다 그렇게 0으로 반환되는 개수를 조절하기 위해 추가 스케일링 계수인 $\\gamma$와 $\\beta$를 만들고, 역전파 알고리즘으로 학습시켜준다. 학습과정에서 이동평균을 구해놓는데, 최근 N개에 대한 이동평\u001c균을 사용한다. 최근 N개만 사용하고 그 전에 것들은 자연스럽게 날라가기 때문\u001c에 최근 N개가 충분한 sample이 아닐 경\u001c우$\\mu$,$\\sigma$가 적절하지 않게 결정되는 문제가 있는\u001c데 이런 상황을 해결하는 것은 지수평균을 사용한다. 심화 합성곱 신경망 GoogLeNet 2014년도에 GoogLeNet과 VGG-19가 나왔는데, GoogLeNet이 에러율이 좀 더 낮고 층이 더 \b깊은 것을 알 수 있다. GoogLeNet이 좀 더 복잡해서 VGG가 더 많이 알려져 있지만, 다양한 스킬들을 공부하려면 GoogLeNet을 조금 살펴보는 것도 좋을 것이다. Let&#39;s Go Deeper and Deeper라는 모토를 가지고 만들어진 것과 같이 좀더 hidden_layer가 깊어진 것을 알 수 있다. 1x1, 3x3, 5x5의 feature들을 다 나누어서 학습\b한다. 즉, 다양한 크기의 Filter들이 잘 학습된다. 또한 3x3 Max pooling 같은 경우는 convolution Layer를 거치지않고도 단순히 max pooling을 통한 후에도 다음 단계에서 의미있는 feature로 작동된다는 것을 보여주었다!! naive 한 Inception 모듈 구조에서 먼저 단순히 1x1 conv를 통과시켜 동일한 channel 영역(Receptive Field)을 가져가면서도 channel을 줄여 연산량을 줄여 주는 구\u001c조인 Bottleneck를 구현하고 있다. 맨 마지막 출력층에서만 inference를 한다면 Input에 가까운 층일수록 점점 더 Vanishing Gradient문제로 인해 학습이 저하 될 것을 우려하여 중간 feature들로도 classification을 하도록 하였다. GoogLeNet 중간 요약 Inception 구조 Battleneck 구조 중간 중간에 inference Residual Network 이제는 거의 일반적이고, 기본구조로 많이 사용하는 구조이다. 왼쪽 구조에서 표현가능한 것은 오른쪽 구조인 Residual 구조에서도 표현 가능함이 증명 되어 있다. 직관적으로 봤을때는 Feature를 뽑아서 이전 Feature와 더한 다는 것이 잘 이해가 안갈 수 도 있겠지만, 이런식으로 했을때도 좌측의 일반적인 Conv Layer의 구조와 수학적으로 동치를 이룬다는 것을 알고 있자. - 맨 왼쪽이 Original ResidualNN의 구조이고 가운데가 Pre-Activation을 사용하는 구조이다. Densely Connected ConvNets(DenseNet) 간단히 말하자면, 모든 Layer들이 다 연결되어 있는 구조라고 할 수 있다. 처음에 일반적인 Conv Layer를 통해 Feature Map을 만들고 그런 뒤에 Dense Block을 이용해서 다른 모든 Conv Layer들과 Dense하게 연결시켜준다. 그 다음 Conv Layer를 이용해서 channel 개수를 조정해주고, Max Pooling을 이용해서 영상크기를 줄여준다. 이런 과정을 여러번 반복해서 Feature를 추출한 후, 맨 마지막은 FC Layer로 구성해주었다. 위의 구조에서 Dense Block들이 residual block으로 바뀐다면 ResNet인 것이다. Pre-Activation구조를 사용한다는 것이 ResNet을 계승하고 있는다는 것을 알 수 있는 명확한 내용이다. 위의 그림을 설명하자면, 먼저 입력으로 3X3XChannel의 Feature map이 만들어지면 그것을 현재 Growth rate가 4이므로 3X3XChannelX4로 Feature map을 새롭게 만들어 주게 된다. 그 후 또다른 Convolution Layer를 통해 새로이 만들어진 Feature map은 3X3X4X4가 될 것이다. 왜냐하면 input channel은 이전의 만들어진 Feature map이 output channel의 수를 4로 정했기 때문에 4이고 output channel은 만들기 나름이지만 처음부터 4로 고정시켜 놓았다는 가정을 해보자. 그렇다면 이제 이 두개의 Feature map을 concatenate해보면 3X3XChannelX4 + 3X3X4X4 = 3X3X(Channel+4)X4 가 될 것이다. 이렇게 그 뒤의 Dense Block 내의 모든 convolution Layer가 진행될 때마다 해주면 위의 그림과 같이 될 것이며 결국 이전 Feature map들을 누적하는 의미라는 것을 알 수 있다. 위의 그림에서 C:처음 입력된 channel의 수, k:growth rate, l:몇번째 까지 Growth를 했는지를 의미한다. $C_{Bottle neck}$은 (C+K*l)이 커졌을 때를 대비해 줄이기위한 목적으로 곱해주는 것이다.","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"data engineering basic(SQL Basic)","slug":"data_engineering_02","date":"2019-12-10T02:23:24.000Z","updated":"2019-12-13T06:46:53.388Z","comments":true,"path":"2019/12/10/data_engineering_02/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/10/data_engineering_02/","excerpt":"","text":"SQL(Structured Query Language)DB (Database) 데이터를 통합하여 관리하는 데이터의 집합 DBMS (Database Management system) DB를 관리하는 미들웨어 시스템을 의미 Database 분류 RDBMS(Relational Database Management System) NoSQL - 데이터 테이블 사이에 키값으로 관계를 가지고 있는 데이터베이스 ex) Oracle, Mysql, Postgresql, Sqlite -데이터 사이의관계 설정으로 최적화된 스키마를 설계 가능 - 데이터 테이블 사이에 관계가 없이 저장하는 데이터베이스 - 데이터 사이의 관계가 없으므로 복잡성이 줄고 많은 데이터를 저장 가능 RDBMS table 행(row)과 열(column)로 이루어져 있는 데이터 베이스를 이루는 기본 단위 Storage Engine MyISAM : full text index 지원, table 단위 lock, select가 빠름, 구조 단순 InnoDB : transaction 지원, row 단위 lock, 자원을 많이 사용, 구조 복잡 Column 테이블의 세로축 데이터 Field, Attribute 라고도 불림 Row 테이블의 가로축 데이터 Tuple, Recode 라고도 불림 Value 행(row)과 열(column)에 포함되어있는 데이터 Key 행(row)의 식별자로 사용 Relationship Schema 스키마(schema)는 데이터 베이스의 구조를 만드는 디자인 NoSQL NoSQL(Not Only SQL) RDBMS의 의존적인 관계가 갖는 한계를 극복하기 위해 만들어진 데이터베이스 확장성이 좋음 데이터 분산처리 용이 데이터 저장이 유연함 RDMBS와 다르게 구조의 변경이 불필요 Schema 및 Join이 없음 Join 기능이 없으므로 각각의 테이블만 사용가능 collection 별로 관계가 없기 때문에 모든 데이터가 들어있어야 하므로 RDBMS보다 저장공간이 더 필요 저장되는 데이터는 Key-value 형태의 JSON 포멧을 사용 select는 RDBMS보다 느리지만 insert가 빨라 대용량 데이터 베이스에 많이 사용 트랜젝션(transaction)이 지원되지 않음(동시수정에 대한 신뢰성이 지원되지 않음) https://db-engines.com/en/ranking_trend Install MySQL(for Mac OS) 주의) 2가지 방법을 소개하지만, Python에서 MySQL을 활용할 User들에게는 1번 방법으로 설치를 해야한다는 것을 알려드립니다!!! brew(1번방법)로 설치해야 python의 mysql client를 사용할수 있습니다. 방법 1) reference https://gist.github.com/operatino/392614486ce4421063b9dece4dfe6c21 Install12345$ brew install mysql@version_num$ brew tap homebrew/services $ brew services start mysql@version_num$ brew services list$ brew link mysql@version_num --force$ mysql -V 앞으로 SQL 접속시 사용할 Password!!1$ mysqladmin -u root password 'yourpassword' Connect mysql server1$ mysql -u root -p 방법 2) dmg 파일 받아서 install step 1) https://dev.mysql.com/downloads/mysql/5.7.html#downloads에서 DMG 파일 다운로드 step 2) 다운 받은 DMG 파일을 실행 설치 중간에 임시 패스워드를 기억 step 3) 시스템 환경설정에 가면 MySQL이 설치 된것을 확인 MySQL 서버의 인스턴스를 정지 및 실행, 초기화, 제거등을 할수 있다. Start MySQL Server 버튼을 클릭하여 실행 아래의 경로로 이동1$ cd /usr/local/mysql/bin Mysql 서버에 접속1$ sudo ./mysql -p Password: (관리자 권한으로 실행을 위한 PC의 패스워드)Enter password: (임시로 발급받은 DB의 패스워드 입력) 아래의 mysql 프롬프트가 나오면 정상!! 설치 완료!!1mysql&gt; 패스워드 변경 ( qwer1234 로 변경할 경우 )12mysql&gt; ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;qwer1234&apos;;mysql&gt; FLUSH PRIVILEGES; mysql&gt; quit; 변경한 패스워드로 다시 로그인Mysql Basic Commandsystem12345678910111213# mysql명령어 리스트 확인mysql&gt; help# 현재 상태 보기mysql&gt; status# mysql 접속 종료mysql&gt; exitmysql&gt; quit# 패스워드 변경 ( qwer1234 로 변경하는 경우 )mysql&gt; ALTER USER 'root'@'localhost' IDENTIFIED BY 'qwer1234'2.2 Database Database1234567891011121314151617# DB 목록 보기mysql&gt; show databases;# DB 만들기 ( DB이름을 test라고 하려면 )mysql&gt; create database test;# DB 접속하기 ( DB 이름 test )mysql&gt; use test;# 현재 접속중인 DB 확인하기mysql&gt; select database();# DB 지우기mysql&gt; drop database test;# DB 삭제 확인mysql&gt; show databases; Table1234567891011121314151617181920212223242526272829# 테이블 만들기# 문자열 name 20자, age 숫자 3자 컬럼이 있는 테이블이 생성mysql&gt; create table user ( name char(20), age int(3) );# 테이블 목록 확인mysql&gt; show tables;# 테이블 구조 확인mysql&gt; desc user;mysql&gt; describe user;mysql&gt; explain user;# 테이블 이름 바꾸기(another로 바꾸기)mysql&gt; rename table user to another;# 테이블 이름 바뀐것 확인 mysql&gt; show tables;# 테이블에 데이터 추가하기mysql&gt; insert into another(name, age) values(\"alice\", 23);mysql&gt; insert into another(name, age) values(\"peter\", 30);# 추가된 데이터 확인하기mysql&gt; select * from anther;# 테이블 지우기mysql&gt; drop table anther;# 테이블 삭제된것 확인mysql&gt; show tables; Database Management Application for Mac OSstep 1) Install Sequel Pro https://www.sequelpro.com/ 경로에서 sequelpro를 다운 받아서 설치 step 2) Connect Database Server 아래와 같이 Host, Username, Password를 설정하여 연결 Sample Database Download https://dev.mysql.com/doc/index-other.html링크에서 Sample database 를 다운 혹시라도 앞으로 저의 블로그를 보시면서 따라해보실 분들은 world database, sakila database 를 다운받아 주세요. sql 파일 추가1234/usr/local/mysql/bin 디렉토리에서 아래와 같이 실행하면 sql 파일을 import - import 하기 전에 world 데이터 베이스가 있어야 함$ sudo ./mysql -p world &lt; (sql 파일 경로)- brew로 설치한 경우 아래와 같이 추가$ mysql -u root -p world &lt; (sql 파일 경로)","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"data engineering basic(Unix환경 및 커맨드)","slug":"data_engineering_01","date":"2019-12-09T08:48:07.000Z","updated":"2019-12-17T04:17:10.108Z","comments":true,"path":"2019/12/09/data_engineering_01/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/09/data_engineering_01/","excerpt":"","text":"Pipes and Filterscat : 해당 파일 전체를 printhead : 해당 파일 앞의 10줄 정도를 printtail : 해당 파일 뒤의 20줄 정도를 print command &gt; file : 기존의 파일 내용은 지우고 현재 command한 결과 파일에 저장command &gt;&gt; file : 기존의 파일에 덮붙여서 결과를 저장(python append같은 느낌!) 12345# example.py를 python3로 run하고 그 결과를 result.txt파일로 저장python3 example.py &gt; result.txt# example.py를 python3로 run하고 그 결과를 result.txt파일에 덮붙여서 저장python3 example.py &gt;&gt; result.txt Shell script terminal에서 바로 명령어를 여러개 사용하고 싶을때 shell script를 사용하면 된다. 예를들어 아래의 example.py를 실행시켜 위에서 command를 한번에 실행시키고 싶다면 다음과 같이 먼저 example.py를 작성한 후에 command.sh 파일에는 command들을 작성한 후에 shell script 파일을 run하면 된다. example.py12345678import sysdef main(): # command 뒤에 따라오는 첫번째 글자를 print print(sys.argv[1])if __name__==\"__main__\": main() command.sh123456#!/usr/bin/env bashpython3 example.py 1 &gt; result.txtpython3 example.py 2 &gt;&gt; result.txthead result.txtrm -rf result.txt example.py terminal창123#권한을 설정chmod +x command.sh./command.sh 보통은 우리가 deploy.sh라는 파일로 만들어 그 안에서 작업을 한다. 예를 들어서,1234567891011121314# zip형식으로 되어있는 모든(*)파일을 삭제해라rm *.zip# 모든 파일을 lisztfever라는 이름으로 압축해라. -r 옵션은 파일이 있을 수 있으므로 붙여준다.zip lisztfever.zip -r *# aws s3라는 storage에 s3://areha/lisztfever/lisztfever.zip 에 해당 path의 파일을 삭제aws s3 rm s3://areha/lisztfever/lisztfever.zip# s3에게 다시 copy해라aws s3 cp ./listzfever.zip s3://areha/lisztfever/listzfever.zip# aws lambda function을 update해라.aws lambda update-function-code --function-name listzfever --s3-buket areha --s3-key listzfever/listzfever.zip AWS Cloud Service먼저, IAM(Identity and Access Management)에 대해서 설명하겠다. 내가 누구이고 어떤 Access를 가지고 있는지를 관리하는 곳이라고 생각할 수 있다. 여기서 새로운 User를 등록 할 수 있다. 위의 Add User를 통해서 새로운 User를 등록해보자. Access type은 우리가 AWS cli를 통해서도 관리하므로 첫번째인 Programmatic access로 설정한다. Permission을 주는 방식에 대한 설정하는 부분이다. Add user to group : 한 Project를 여러명이 같이 진행하여 여러명이 관리할 경우 사용. Copy permissions from existing user : 말 그대로 이미 존재하는 user의 permission들 중 하나를 선택하여 Copy할 경우 사용 Attach existing policies directly : AWS에 존재하는 정책들 중 하나를 선택하여 바로 사용하는 경우 사용 예전에 있던 계정이 만료된걸 모르고 있다가 결제를 안해버려서…. ㅠㅠㅠ 새롭게 만든 계정으로 하느라 등록된 User들이 없다. 그러므로 3번째 설정으로 들어가서 모든 최상위 permission을 갖는 AdministratorAccess를 주었다. 다음으로 넘어가게 되면, tag를 설정하게 되는데, 우선 넘어가겠다.(이 부분은 나중에 설정할 것이다.) 앞에서 설정한 사항들을 확인하고, Create User를 누르게 되면 설정한대로 User를 생성하게 되는 것이다. Access key ID, Secret access key가 생성되어 나오는데, 이 창이 닫히면, 볼수 없으므로 Download csv를 하는 것을 권장한다. 설치 프로그램을 실행한다. /usr/local/aws에 AWS CLI를 설치하고 /usr/local/bin 디렉터리에 symlink aws를 생성한다. -b 옵션을 사용하여 symlink를 생성하면 사용자의 $PATH 변수에 설치 디렉터리를 지정할 필요가 없다. 이렇게 하면 모든 사용자가 임의 디렉터리에서 aws를 입력하여 AWS CLI를 호출 가능하게 한다. 123curl \"https://s3.amazonaws.com/aws-cli/awscli-bundle.zip\" -o \"awscli-bundle.zip\"unzip awscli-bundle.zipsudo ./awscli-bundle/install -i /usr/local/aws -b /usr/local/bin/aws 위의 설치가 다 끝나면, 이제 aws cli의 configure를 설정해 볼 것이다. 이를 통해 우리가 console에 접속하지 않고도 cli 환경에서도 aws를 조작할 수 있게 된다. 12345aws configureAWS Access Key ID [None]: Access key IDAWS Secret Access Key [None]: Secret access keyDefault region name [None]: ap-northeast-2Default output format [None]:","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"쉽게 배우는 경사하강 학습법","slug":"deep_learning_02","date":"2019-12-07T15:00:00.000Z","updated":"2019-12-13T03:55:29.137Z","comments":true,"path":"2019/12/08/deep_learning_02/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/08/deep_learning_02/","excerpt":"","text":"쉽게 배우는 경사하강 학습법 어떤 손실 함수를 사용하느냐에 따라서 학습이 어떻게 이루어질 것인지, 그리고 학습을 할 때 정답의 형태를 결정하기 때문에 손실 함수는 중요하다! Traning Data를 Model에 입력해 우리가 학습시키고자 하는 Trainable Parameters를 얻게 되는데 Trainable Parameters들을 inputs으로 보고 outputs을 학습결과인 Loss Function으로 생각하면, 알고리즘 학습은 입력을 바꿔가면서, 출력값이 점점 작아지게 하는 것이라고 볼 수 있다. 결국 알고리즘 학습은 입력을 바꿔가면서, 출력값이 점점 작아지게 하는 것이라는 관점에서 최적화 이론의 목표와 동일하다는 사실을 알 수 있다. 경사 하강 학습법 무차별 대입법은 범위를 알아야하고 범위를 안다해도 step을 촘촘히 조사해야 하므로 계산 복잡도가 높다. 적게 대입해 보고 답을 찾을 수 있는 방법을 생각하다 최적화 알고리즘이 발전 하게 되었다. 최적화 이론과 수학적 표현 수치적 방법의 대표적인 방법이 경사하강법이다. 심화 경사 하강 학습법 경사하강 학습법의 단점들을 극복한 알고리즘에 대해서 알아보자. 경사하강법은 안장점에서 기울기가 0이 되므로 벗어나지 못하게 되는 문제점이 있다. 이동 벡터가 이전 기울기에 영향을 받도록 하는 방\u001d법 이전의 속도에 영향을 받는 방법이라고 할 수 있다. 장점 : Local minimum과 noise에 대처 가능 단점 : 경사하강법은 단순히$x_{t-1}$이동벡터($v_{t}$)를 추가로 사용하므로, 경사 하강법 대비 2배의 메모리를 사용 변수별로 learning rate가 달라지게 조절한다. 예를 들어서 $x=[x_{1}, x_{2}, x_{3},…,x_{n}]$이 존재할때 어떤 변수는 기울기를 크게 가져가고 어떤 변수는 기울기를 작게 가져갈 경우 처음에 기울기를 크게 가져가지 못한다면 local minimum에 빠지기 쉬운 문제점이 있다. 이런 문제점을 해결하고자 변수별로 learning rate를 다르게 가져가는 알고리즘인 Ada Grad 탄생된 것이다. 장점 : $g_{t}$가 누적되어 커진 것은 학습이 그만큼 많이 된 것이므로 학습이 많이 변수는 학습율을 감소시켜, 다른 변수들이 잘 학습되도록 한다. 단점 : $g_{t}$ 가 계속해서 커져서 학습이 오래 진행되면 learning rate가 0ㅇ에 가까워지므로 더이상 학습이 이루어지지 않는 단점이 있다. gradient의 크기를 제곱한 벡터(gradient벡터의 L2-norm)를 누적합을 해서 적게 학습되는 변수들을 더 학습시켜 주도록했지만 epoch나 batchsize등 반복 시키는 parameter의 value가 높아질수록 오래 진행되어 누적합이 커지게 되면 더 이상 학습이 되지 않는 문제점을 개선한 방법이다. 위의 식에서 $\\gamma$값은 0~1값을 갖게 되며, 이 값을 통해 이전의 gradient 누적합을 감소시키는 효과를 주면서 새로운 gradient의 값을 쫓아갈 수 있도록 개선하였다. 그러므로, 변수 간의 상대적인 학습율 차이는 유지하면서$g_{t}$가 무한정 커지지 않아 학습을 오래 할 수 있다. RMSprop과 Momentum의 장점을 결합한 알고리즘이다. 대부분의 코드에 이 Adam optimization을 사용한다. 경사 하강법\u001d을 이용한 얕은 신경망 학습123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112# 경사 하강법을 이용한 얕은 신경망 학습import tensorflow as tfimport numpy as np## 하이퍼 파라미터 설정epochs = 1000## 네트워크 구조 정의### 얕은 신경망#### 입력 계층 : 2, 은닉 계층 : 128 (Sigmoid activation), 출력 계층 : 10 (Softmax activation)# keras의 모듈을 상속해서 Model을 구현class MyModel(tf.keras.Model): def __init__(self): # 상속을 한 경우에는 상속을 한 상위 class를 initialize하는 것을 잊어버리지 말자! super(MyModel, self).__init__() # 아래의 input_dim을 적어줄 필요는 없다. 실제 데이터가 들어올때 정의 되기 떄문이다. self.d1 = tf.keras.layers.Dense(128, input_dim=2, activation=\"sigmoid\") self.d2 = tf.keras.layers.Dense(10, input_dim=128, activation=\"softmax\") # Model이 실제 call이 될때 입력에서 출력으로 어떻게 연결이 될 것인지를 정의 def call(self, x, training=None, mask=None): x = self.d1(x) return self.d2(x)## 학습 루프 정의@tf.function# tensorflow의 Auto Graph를 통해 쉽게 구현가능하다.# function 내의 python 문법으로 입력된 모든 tensor 연산들을 tf.function에 의해서# 최적화된다.def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_metric): # Gradient를 계산하기위한 with tf.GradientTape() as tape: predictions = model(inputs) loss = loss_object(labels, predictions) # loss를 model의 trainable_variables(W,b)로 각각 미분해서 gradient를 구한것. # loss는 scalar이고, model.trainable_variables는 벡터이므로 결과 또한 벡터가 될 것이다. gradients = tape.gradient(loss, model.trainable_variables) # 각 gradient와 trainable_variables들이 optimizer로 학습 optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # loss를 종합 train_loss(loss) # matric train_metric(labels, predictions)## 데이터셋 생성, 전처리np.random.seed(0)pts = []labels = []center_pts = np.random.uniform(-8.0, 8.0, size=(10, 2))for label, center_pt in enumerate(center_pts): for _ in range(100): pts.append(center_pt + np.random.randn(*center_pt.shape)) labels.append(label)# GPU를 사용하게 된다면 위의 MyModel class에서 initialize 할때# Layer에 따로 dtype을 지정하지 않으면 float32로 설정되므로 동일하게 해주기 위해 type 재설정pts = np.stack(pts, axis=0).astype(np.float32)# 이미 integer이므로 바꿀 필요가 없음.labels = np.stack(labels, axis=0)# 위에서 만든 데이터를 train data set으로 변형# train_ds는 iterable한 object가 된다.# 1000개를 섞어 batch_size를 32개로 해서 구성해준다.train_ds = tf.data.Dataset.from_tensor_slices((pts, labels)).shuffle(1000).batch(32)print(pts.shape)print(labels.shape)## 모델 생성model = MyModel()## 손실 함수 및 최적화 알고리즘 설정### CrossEntropy, Adam Optimizerloss_object = tf.keras.losses.SparseCategoricalCrossentropy()optimizer = tf.keras.optimizers.Adam()## 평가 지표 설정### Accuracytrain_loss = tf.keras.metrics.Mean(name='train_loss')train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')## 학습 루프for epoch in range(epochs): #위에서 batch_size를 32로 했으므로 한번 실행시 32개씩 나옴. for x, label in train_ds: train_step(model, x, label, loss_object, optimizer, train_loss, train_accuracy) template = 'Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;' print(template.format(epoch+1, train_loss.result(), train_accuracy.result()*100))## 데이터셋 및 학습 파라미터 저장# 압축해서 여러개의 Numpy Object들을 저장할 수 있다.np.savez_compressed('ch2_dataset.npz', inputs=pts, labels=labels)W_h, b_h = model.d1.get_weights()W_o, b_o = model.d2.get_weights()# weight는 tensorflow에서 사용하고 있는 convention이랑# shallowNN을 구현할 때 사용했던 convention이 좀 다르다.W_h = np.transpose(W_h)W_o = np.transpose(W_o)np.savez_compressed('ch2_parameters.npz', W_h=W_h, b_h=b_h, W_o=W_o, b_o=b_o)","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"심층 신경망의 구조","slug":"deep_learning_03","date":"2019-12-07T15:00:00.000Z","updated":"2019-12-20T04:38:06.937Z","comments":true,"path":"2019/12/08/deep_learning_03/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/08/deep_learning_03/","excerpt":"","text":"심층 신경망의 구조 은닉 계층 추가 = 특징의 비선형 변환 추가!!선형 변환의 이해 선형대수의 선형 변환을 함수의 개념에서 보았을때, 입력 차원(n)이 출력 차원(m)보다 크다면 Onto(전사함수: 모든 공역이 치역이 되있는 상태)가 될 수 있지만, 그 반대인 경우는 적은 차원을 갖는 입력벡터의 차원으로 일부분의 출력 벡터의 차원을 커버하는 것이 되는 것이므로, 전사함수가 될 수 없다. 또한, 이런 입력차원(n)이 출력 차원(m)보다 작은 경우의 구조를 우리는 딥러닝 네트워크 구조에서도 볼 수 있다. 예를 들면 GAN이나 Auto Encoder의 decoder구조가 가장 쉬운 예시일 것이다. 여기서의 의문은 그렇다면, 일부분의 차원으로 피처를 잘 배울 수 있는지가 의문일 것이다. 허나, 그 일부의 차원이 원래 갖고 있던 특성에서 나올법한 특성들만을 생성해 주므로 걱정하지 않아도 된다. 또한, 선형시스템의 곱으로 노드들의 연산을 표현할 수 있는데, 여기서, 예를 들어, 특징벡터1과 특징벡터2간의 방향이 비슷한 즉, Orthogonal하지 않고 방향이 비슷한 벡터를 통해 연산을 진행하면 다음 층에서는 노드들 중에 비슷한 특징에 대한 정보를 포함하고 있을 것이다. Inner product를 projection의 개념에서 살펴보면, 어떠한 벡터가 다른 방향의 벡터에 projection을 하는 것은 그 projection한 벡터가 그 방향의 벡터가 어느 정도의 성분을 가지고 있는지를 의미하므로 선형대수 측면에서 위에서 각 피처들간의 곱의 연산들에 의한 새로운 피처들의 생성은 projection된 길이를 비교하는 행위와 동일할 것이다. 역전파 학습법의 개념 y를 구하려면 x와 z를 알아야 하는데, x와 z에는 중복된 연산이 있어서 비효율적이다. 처음 계산할 때 값을 저장해주어서 중복계산이 발생하지 않도록 해준다. 학습을 마친 후 validation set이나 test set에 적용할 때는 더 이상 학습을 하지 않으므로 이 순방향 추론만을 사용한다. 심층 신경망의 수학적 이해 역전파 학습의 필요성 (N+1번) 손실함수를 평가한다고 하는데 그 이유는 기준점이 되는 손실함수를 먼저 한번 계산하고 나머지 편미분시에 가각 N번 평가하기 때문이다. 합성함수와 연쇄 법칙 역전파 학습법의 수식적 이해 미분하고자 하는 경로 사이에 있는 모든 미분값을 알아야 원하는 미분을 구할 수 있다는 의미이다. 수치적 미분에서는 N+1번을 계산하여야 했지만, 역전파 알고리즘으로 인해 단 한번의 손실함수 평가로 미분을 구할 수 있다. 수치 미분을 이용한 심층 신경망 학습123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134## 수치 미분을 이용한 심층 신경망 학습import timeimport numpy as np## 유틸리티 함수epsilon = 0.0001def _t(x): return np.transpose(x)def _m(A, B): return np.matmul(A, B)def sigmoid(x): return 1 / (1 + np.exp(-x))def mean_squared_error(h, y): return 1 / 2 * np.mean(np.square(h - y))## 뉴런 구현class Neuron: def __init__(self, W, b, a): self.W = W self.b = b self.a = a # Gradient self.dW = np.zeros_like(self.W) self.db = np.zeros_like(self.b) def __call__(self, x): return self.a(_m(_t(self.W), x) + self.b) # activation((W^T)x + b)## 심층신경망 구현class DNN: \"\"\" hidden_depth : hidden_layer의 갯수 num_neuron : hidden_layer 하나당 neuron의 갯수 num_input : input_layer의 neuron의 갯수 num_output : output_layer의 neuron의 갯수 activation : activation funtion으로 사용할 함수 \"\"\" def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid): # W, b initialize def init_var(i, o): return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,)) self.sequence = list() # First hidden layer W, b = init_var(num_input, num_neuron) self.sequence.append(Neuron(W, b, activation)) # Hidden layers for _ in range(hidden_depth - 1): W, b = init_var(num_neuron, num_neuron) self.sequence.append(Neuron(W, b, activation)) # Output layer # 단순히 심층신경망 구현 후에 수치미분을 사용한 역전파학습을 보이기 위한 코드이므로 # Output layer의 activation function을 따로 바꾸지 않고 sigmoid로 사용하겠다. W, b = init_var(num_neuron, num_output) self.sequence.append(Neuron(W, b, activation)) def __call__(self, x): # layer를 call하는 것은 결국 위에서 정의한 Neuron의 call이 될 것이고 # x는 activation((W^T)x + b)이 될 것이다. for layer in self.sequence: x = layer(x) return x def calc_gradient(self, x, y, loss_func): def get_new_sequence(layer_index, new_neuron): # 특정한 변수하나(weight나 bias)만 변화를 줘서 그 때 loss가 얼마나 변하는지를 보고 # numerical gradient를 계산하려하기 때문에 변화된 변수가 있는 새로운 Sequence가 필요하다. new_sequence = list() for i, layer in enumerate(self.sequence): if i == layer_index: new_sequence.append(new_neuron) else: new_sequence.append(layer) return new_sequence def eval_sequence(x, sequence): for layer in sequence: x = layer(x) return x loss = loss_func(self(x), y) for layer_id, layer in enumerate(self.sequence): # iterate layer for w_i, w in enumerate(layer.W): # iterate W (row) for w_j, ww in enumerate(w): # iterate W (col) W = np.copy(layer.W) W[w_i][w_j] = ww + epsilon new_neuron = Neuron(W, layer.b, layer.a) new_seq = get_new_sequence(layer_id, new_neuron) h = eval_sequence(x, new_seq) num_grad = (loss_func(h, y) - loss) / epsilon # (f(x+eps) - f(x)) / epsilon layer.dW[w_i][w_j] = num_grad for b_i, bb in enumerate(layer.b): # iterate b b = np.copy(layer.b) b[b_i] = bb + epsilon new_neuron = Neuron(layer.W, b, layer.a) new_seq = get_new_sequence(layer_id, new_neuron) h = eval_sequence(x, new_seq) num_grad = (loss_func(h, y) - loss) / epsilon # (f(x+eps) - f(x)) / epsilon layer.db[b_i] = num_grad # gradient를 계산할 때 loss를 return해야 학습과정에 loss가 어떻게 되는지를 알 수 있기때문에 return 해준다. return loss## 경사하강법def gradient_descent(network, x, y, loss_obj, alpha=0.01): loss = network.calc_gradient(x, y, loss_obj) for layer in network.sequence: layer.W += -alpha * layer.dW layer.b += -alpha * layer.db return loss## 동작 테스트x = np.random.normal(0.0, 1.0, (10,))y = np.random.normal(0.0, 1.0, (2,))dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=sigmoid)t = time.time()for epoch in range(100): loss = gradient_descent(dnn, x, y, mean_squared_error, 0.01) print('Epoch &#123;&#125;: Test loss &#123;&#125;'.format(epoch, loss))print('&#123;&#125; seconds elapsed.'.format(time.time() - t)) 역전파 알고리즘을 이용한 심층 신경망 학습123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137## 역전파 학습을 이용한 심층 신경망 학습import timeimport numpy as np## 유틸리티 함수def _t(x): return np.transpose(x)def _m(A, B): return np.matmul(A, B)## Sigmoid 구현class Sigmoid: def __init__(self): # 곱의 형태로 나오게 되므로 처음에 1로해서 추후에 입력될 수치에 영향을 덜 주게 해준다. self.last_o = 1 def __call__(self, x): self.last_o = 1 / (1.0 + np.exp(-x)) return self.last_o def grad(self): # sigmoid(x) * (1- sigmoid(x)) return self.last_o*(1-self.last_o)## Mean Squared Error 구현class MeanSquaredError: def __init__(self): # chain rule을 할 때 MSE로 부터 gradient를 계속해서 가져와야하므로 저장해놓기 위해 self.dh = 1 self.last_diff = 1 def __call__(self, h, y): # 1/2 * mean((h - y)^2) self.last_diff = h - y return 1 / 2 * np.mean(np.square(h - y)) def grad(self): # h - y return self.last_diff## 뉴런 구현class Neuron: def __init__(self, W, b, a_obj): self.W = W self.b = b # activation이 이전과 다르게 class로 작성되었으므로 instanctiation을 해주어야한다. self.a = a_obj() # gradient self.dW = np.zeros_like(self.W) self.db = np.zeros_like(self.b) self.dh = np.zeros_like(_t(self.W)) ## 아래의 grad_W를 위해 저장해놓는다. ## W로 미분했을 경우 이전 입력을 갖고 있어야 바로 사용할 수 있으므로 self.last_x = np.zeros((self.W.shape[0])) self.last_h = np.zeros((self.W.shape[1])) def __call__(self, x): self.last_x = x self.last_h = _m(_t(self.W), x) + self.b return self.a(self.last_h) def grad(self): # dy/dh = W return self.W * self.a.grad() def grad_W(self, dh): grad = np.ones_like(self.W) grad_a = self.a.grad() for j in range(grad.shape[1]): # dy/dw = x grad[:, j] = dh[j] * grad_a[j] * self.last_x return grad def grad_b(self, dh): # dy/db = 1 return dh * self.a.grad() * 1## 심층신경망 구현class DNN: def __init__(self, hidden_depth, num_neuron, input, output, activation=Sigmoid): def init_var(i, o): return np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,)) self.sequence = list() # First hidden layer W, b = init_var(input, num_neuron) self.sequence.append(Neuron(W, b, activation)) # Hidden Layers for index in range(hidden_depth): W, b = init_var(num_neuron, num_neuron) self.sequence.append(Neuron(W, b, activation)) # Output Layer W, b = init_var(num_neuron, output) self.sequence.append(Neuron(W, b, activation)) def __call__(self, x): for layer in self.sequence: x = layer(x) return x def calc_gradient(self, loss_obj): loss_obj.dh = loss_obj.grad() # for문에서 한번에 처리하기 위해서 loss object를 넣어준다. self.sequence.append(loss_obj) # back_propagation loop for i in range(len(self.sequence) -1, 0 , -1): l1 = self.sequence[i] l0 = self.sequence[i - 1] l0.dh = _m(l0.grad(), l1.dh) l0.dw = l0.grad_W(l1.dh) l0.db = l0.grad_b(l1.dh) # loss object가 들어 있으면 출력을 얻지 못하고 loss 만 얻게 될 것이기 때문이다. self.sequence.remove(loss_obj)## 경사하강 학습법def gradient_descent(network, x, y, loss_obj, alpha=0.01): loss = loss_obj(network(x), y) # Forward inference network.calc_gradient(loss_obj) # Back-propagation for layer in network.sequence: layer.W += -alpha * layer.dW layer.b += -alpha * layer.db return loss## 동작 테스트x = np.random.normal(0.0, 1.0, (10,))y = np.random.normal(0.0, 1.0, (2,))t = time.time()dnn = DNN(hidden_depth=5, num_neuron=32, input=10, output=2, activation=Sigmoid)loss_obj = MeanSquaredError()for epoch in range(100): loss = gradient_descent(dnn, x, y, loss_obj, alpha=0.01) print('Epoch &#123;&#125;: Test loss &#123;&#125;'.format(epoch, loss))print('&#123;&#125; seconds elapsed.'.format(time.time() - t))","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"가장 단순한 신경망을 통한 작동원리","slug":"deep_learning_01","date":"2019-12-05T15:00:00.000Z","updated":"2019-12-08T12:33:35.852Z","comments":true,"path":"2019/12/06/deep_learning_01/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/06/deep_learning_01/","excerpt":"","text":"Node가 단일 뉴런 연산을 의미한다고 했는데 여기서의 단일 뉴런 연산이란 input에 가중치를 곱하고 합계를 낸 후에 activation function까지 통과시키는 과정을 의미한다. 위의 식에서 편향을 잊어버리지 말자!! 예를들면, 편향이 없다면 원점을 지나는 선만 표현할 수 있지만 편향을 통해 원점을 지나지 않는 선들도 표현할 수 있게 할 수 있다. 참고로 특별히 편향이 없는 경우도 있을 순 있다. 회귀 문제 어떤 입력이 들어왔을 떄 출력이 연속적인 값을 가질 때 Regression을 사용한다. 이진 분류 문제 다중 분류 문제 Softmax의 분모에 의해서 다른 클래스에 대한 학습에도 영향을 준다는 의미이다. 분모는 다른 클래스로 예측한 확률또한 더해주기 때문이다. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273# 얕은 신셩망을 이용한 다중 분류 문제import numpy as npimport matplotlib.pyplot as plt## 함수 구현# Sigmoid 함수def sigmoid(x): return 1/(1+np.exp(-x))# Softmax 함수def softmax(x): return np.exp(x)/np.sum(np.exp(x))# 네트워크 구조 정의class ShallowNN: # 아래의 W와 b에 적절한 값은 추후에 넣어주기 때문에 현재는 0으로 잡음 def __init__(self, num_input, num_hidden, num_output): self.W_h = np.zeros((num_hidden, num_input), dtype=np.float32) self.b_h = np.zeros((num_hidden, 1), dtype=np.float32) self.W_o = np.zeros((num_output, num_hidden), dtype=np.float32) self.b_o = np.zeros((num_output, 1), dtype=np.float32) # NN의 연산을 call 형태로 해서 작성 def __call__(self, x): h = sigmoid(np.matmul(self.W_h, x) + self.b_h) return softmax(np.matmul(self.W_o, h) + self.b_o)# 데이터셋 불러오기dataset = np.load('ch2_dataset.npz')inputs = dataset['inputs']labels = dataset['labels']print(labels.shape)print(inputs.shape)# ShallowNN Model 생성model=ShallowNN(num_input=inputs.shape[1], num_hidden=128, num_output=10)# 사전에 학습된 파라미터 불러오기weights = np.load('ch2_parameters.npz')model.W_h = weights['W_h']model.b_h = weights['b_h']model.W_o = weights['W_o']model.b_o = weights['b_o']# 모델 결과 프린트outputs = []for point, label in zip(inputs, labels): output = model(point) outputs.append(np.argmax(output)) print(np.argmax(output), label)outputs = np.stack(outputs, axis=0)# 정답 클래스 scatter plotplt.figure()for idx in range(10): mask = labels == idx plt.scatter(inputs[mask, 0], inputs[mask, 1])plt.title('True Label')# plt.grid()plt.show()# 모델 출력 클래스 scatter plotplt.figure()for idx in range(10): mask = outputs == idx plt.scatter(inputs[mask, 0], inputs[mask, 1])plt.title('Model output')# plt.grid()plt.show()","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"딥러닝이 무엇인가?","slug":"deep_learning00","date":"2019-12-04T15:00:00.000Z","updated":"2019-12-08T12:41:32.567Z","comments":true,"path":"2019/12/05/deep_learning00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/12/05/deep_learning00/","excerpt":"","text":"딥러닝의 이해 기계학습의 경우에는 위의 고양이와 개를 구분하기 위해서 이진 분류기를 구현할 것인데, 이런 이진 분류기를 구현하기 위해서는 Feature Extractor\u001d가 필요하다. 여기서 말하는 Feature Extractor\u001d 란 구분에 용이한 특징을 추출하여 feature vector를 만드는 데 사용하는 것이다. 이렇게 잘 추출한 특징 벡터를 가지고 분류기를 개와 고양이를 구분한다. 특징 추출기를 통해 사람이 직접 Feature vector들을 만들고 Classifier 부분만 기계가 학습하는 방식이 Machine Learning이다. 반면에 딥러닝은 개와 고양이의 row data를 받아서 Feature Extractor\u001d가 네트워크 구조 내부에 포함되어 있다. 특징 추출도 컴퓨터가 하고 classifier 부분도 컴퓨터가 알아서 분류하므로 전체 네트워크 구조가 학습대상이 된다. 딥러닝은 과거 몇번의 고비(XOR문제를 다층 퍼셉트론으로 극복, 기울기 소실문제는 심층믿음 신경망을 통해 극복)을 극복하고 현재는 많은 이들의 관심 속에 발전해가고 있다. 딥러닝의 대중화를 이끈 요소들을 다음 그림들에서 볼 수있다.","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"data engineering basic","slug":"data_engineering_basic","date":"2019-11-29T10:57:36.000Z","updated":"2019-12-08T14:31:09.223Z","comments":true,"path":"2019/11/29/data_engineering_basic/","link":"","permalink":"https://heung-bae-lee.github.io/2019/11/29/data_engineering_basic/","excerpt":"","text":"데이터 분석가와 엔지니어링 차이점 데이터 분석가는 갖춰진 데이터 시스템과 데이터를 통해서 다양한 분석을 하는 업무이며, 엔지니어링은 그와 다르게 비즈니스에 맞는 데이터를 추출하고 그에 따라 분석하는 환경을 만들어 나가는 업무라고 생각할 수 있을 것이다. 특히, 데이터 전처리나 추출, 정제를 담당하는 업무이다. 데이터 엔지니어링이 중요한 이유 비즈니스 모델과 가장 연관이 깊은 업무이다. 왜냐하면 회사의 비즈니스 모델에 맞는 데이터를 가져와야하고 가져온 데이터를 통해 어떤 환경을 갖출 것인지, 그에따라 데이터 분석가들이 전략을 짤 때 기반을 갖출 수 있도록 해주려면 어떻게 해야할지가 중요하기 때문이다.그래서 엔지니어링을 뽑을 경우 해당 비즈니스의 Knowledge가 어느 정도 있는 것이 좋을 거라고 생각이들고 실제로 그렇게 면접도 보는(?)것 같다. 페이스북은 User와 관련 세밀한 데이터가 중요했지만 e-commerce는 User 관련 데이터 보다는 마케팅, CRM, 물류 데이터가 상대적으로 더 중요할 수도 있다. 데이터 아키텍쳐시 고려사항1.비즈니스 모델 상 가장 중요한 데이터는 무엇인가? 발생되는 데이터 양 대비 초점을 맞춰야 하는 데이터는 어떤 것인지 즉, 비용 대비 비즈니스 임팩트가 가장 높으 데이터를 확보하는 것이 제일 중요하다. 2.Data Governance 3.유연하고 변화 가능한 환경 구축 특정 기\b술 및 솔루션에 얽매여져 있지 않고 새로운 테크를 빠르게 적용할 수 있는 아키텍쳐를 만드는 것 생성되는 데이터의 형식이 변화할 수 있는 것처럼 그에 맞는 Tool들과 solution들도 빠르게 변화할 수 있는 시스템을 구축하는 \u001d것 4. Real Time(실시간) 데이터 핸들링이 가능한 시스템 밀리세컨 단위의 스트리밍 데이터가 됐건 하루에 한번 업데이트 되는 데이터든 데이터 아키텍쳐는 모든 스피드의 데이터를 핸들링 해야한다. Real Time Streaming Data Processing Cronjob Serverless Triggered Data Processing 5. 시큐리티 내부와 외부 모든 곳에서부터 발생할 수 있는 위험요소들을 파악하여 어떻게 데이터를 안전하게 관리할 수 있는지 아키텍쳐 안에 포함 6. 셀프 서비스 환경 구축 데이터 엔지니어 한명만 엑세스가 가능한 데이터 시스템은 확장성이 없는 데이터 분석 환경이다. 이런 환경에서는 예를 들어, 데이터 분석가들이라던지, 데이터 사이언티스트들, 비즈니스팀들 등 다른 사람들도 BI Tool, Query System for Analysis, Front-end application등 이 가능하게끔 확장성이 있도록 환경을 구축하는 것이 중요하다. 데이터 시스템의 옵션들API시대 현재 마케팅, CRM, ERP등 다양한 플랫폼 및 소프트웨어들은 API라는 송신방법을 통해 데이터를 주고 받을 수 있는 환경을 구축하여 생태계를 생성되어있다. 예를들면, facebook, google, Spotify같은 서비스들이 회사자체에 DB시스템을 구축하고 있는데, 이런 데이터들을 API를 통해 바로 DB에 엑세스해서 서비스를 제공할 수도 있고, 아니면 DB를 새로 생성해 거기에 받아서 저장해놓은 후 정제 및 분석 환경을 구축하여 다양한 서비스를 제공할 수 있다. 이런 환경에서 현재 많은 서비스들이 있으며, 새로운 서비스를 개발하는 입장에서는 필요한 여러가지 서비스들이 있는데, 일일이 다 만들수 없으므로 만들어져 있는 것들, 써드 파티라고 하는 서비스들을 이용하는 것이다. 이러한 서비스를 이용하는 데이터를 가지고도 또다른 분석환경을 구\b축해야 한다. ex) CaFe24(호스팅업체), facebook Ads,Google Ads(마케팅분야) Relational Databases 데이터의 관계도를 기반으로 한 디지털 데이터베이스로 데이터의 저장을 목적으로 생겨났다. SQL이라고 하는 스탠다드 방식을 통해 자료를 열람하고 유지한다. 현재 대부분의 서비스들이 가장 많이 쓰고 있는 데이터 시스템. NoSQL Databases 관계형 데이터 베이스에서는 Schema 형식에 맞춰 데이터를 추출 및 저장했다면, 이제는 너무나 다양한 형식이 없는 데이터 부터 틀에 맞출 수 없는 데이터들이 생성되어 NoSQL이 대두되었다. 예를 들면 메신저에서 많이 사용된다. Not Only NoSQL Unstructured, Schema Less Databases Scale horizontally Highly scalable Haddop / Spark / Presto 등 빅데이터 처리Distribtion Storage System / MapReduce를 통한 병렬 처리 Spark Hadoop의 진화된 버전으로 빅데이터 분석 환경에서 Real Time 데이터를 프로세싱하기에 더 최적 java, Python, Scala를 통한 API를 제공하여 Application 생성 SQL Query 환경을 서포트하여 분석가들에세 더 각광 서버리스 프레임워크 Triggered by http requests, database events, queuing services DB가 됐건, 어떤 server가 됐\u001d건 어떠한 하나의 가상 클라우드상에서 server가 필요하게 되는데, 서버를 생성하고 유지및 관리할 때 데이터가 발생하는 event가 발생할 때 Trigger가 되는 부분들을 처리하기 위해 사용한다. Pay as you User 항상 Server를 띄워놓고 있지 않기 때문에 쓰는 만큼만 비용을 지불하기에 좋다. Form of functions 하나의 Function이라고 생각하는 것이 좋다. 예를 들어서, 서버리스 프레임 워크를 통해서 어떠한 event가 들어왔을 경우, 어떤 것으로 Trigger가 됐을때, 어떠한 Algorithm을 실행시키는 function이라고 생각하면 된다. 3rd Party 앱들 및 다양한 API를 통해 데이터를 수집 정제하는데 유용 데이터 파이프라인데이터 파이프라인 데이터를 한 장소에서 다른 장소로 옮기는 것을 의미 ex) API -&gt; DB, DB -&gt; DB, DB -&gt; BI Tool 데이터 파이프라인이 필요한 경우 1) 다양한 데이터 소스들로부터 많은 데이터를 생성하고 저장하는 서비스를 구축할 경우 필요하다! 2) 데이터 사일로: 마케팅, 어카운팅, 세일즈, 오퍼레이션 등 각 영역의 데이터가 서로 고립되어 있는 경우 (ex)대기업의 각 부서를 생각해보면 이해하기 쉬울 것이다.즉, 각각의 팀들이 따로 존재하여 공유가 어려운경우) 3) 실시간 혹은 높은 수준의 데이터 분석이 필요한 비즈니스 모델 ex)facebook등 4) 클라우드 환경으로 데이터 저장 데이터 파이프라인 구축시 고려사\u001d항 Scalability : 데이터가 기하급수적으로 늘어났을때도 작동하는가? Stability : 에러, 데이터플로우 등 다양한 모니터링 관리 Security : 데이터 이동간 보안에 대한 리스크는 무엇인가? 데이터 프로세싱 자동화란? 데이터 프로세싱 자동화란 필요한 데이터를 추출, 수집, 정제하는 프로세싱을 최소한의 사람 인풋으로 머신이 운영하는 것을 의미 ex) Spotify 데이터를 하루에 한번 API를 통해서 클라우드 데이터베이스로 가져온다고 했을 때 매번 사람이 데이터 파이프라인을 작동하는 것이 아니라 Crontab 등 머신 스케쥴링을 통해 자동화 자동화를 위해 고려할 사항 error가 뜨는 것이든, 추출을 했으면 분석을 한다던지 사람이 하면 순서나 여러가지 고랴를 할 수 있지만, 자동으로 헀을경우는 머신이 모르기 떄문에 다음과 같은 사항들을 고려해야한다. 1) 데이터 프로세싱 스텝들 Spotify API에서 어떠한 데이터를 가져와야되고, 그 중에서 어떠한 것들은 걸러내고, 어떤 알고리즘을 돌리고, 그 후에 시각화를 한겠다는 말 그대로 프로세싱 스텝을 의미. 2) 에러 핸들링 및 모니터링 에러가 생성이 됐을때, 어떻게 반응을 하게끔할 것인지, 에러나 퍼포먼스 또는 데이터 추출이 얼마나 걸렸는지 같은 사항을 모니터링 할수 있게끔 구축해야한다. exampl.log라는 파일에 다양한 로그들을 저장하는데, 그 생성된 로그들도 CloudWatch에도 생성되어 모니터링이 가능하다. 3) Trigger/ Scheduling 다음 단계를 실행하기 위해 어떻게 Trigger가 되어서 실행을 시킬지, 하루에 한번 돌릴지, 아니면 한달에 한번 돌릴지 등에 관한 스케줄을 고려해야한다. Spotify 프로젝트 데이터 아키텍쳐Ad hoc VS Automated Ad hoc 분석 환경 구축은 서비스를 지속적으로 빠르게 변화시키기 위해 필수적인 요소 Ad hoc 분석은 쉽게 말해 분석을 하고 싶을 때만 하는 것이다. 이런 Ad hoc 분석이 필수적인 이유는 구축한 분석환경을 통해서 다양한 사람들이 분석을 할 수 있게끔해야 하기 때문이다. 이니셜 데이터 삽입, 데이터 Backfill 등을 위해 Ad hoc 데이터 프로세싱 시스템 구축 필요 Automated : 이벤트, 스케쥴 등 트리거를 통해 자동화 시스템 구축","categories":[{"name":"data engineering","slug":"data-engineering","permalink":"https://heung-bae-lee.github.io/categories/data-engineering/"}],"tags":[]},{"title":"Requests 모듈 사용하기(HTTP 통신)","slug":"Request","date":"2019-09-28T07:20:55.000Z","updated":"2019-11-21T10:39:17.411Z","comments":true,"path":"2019/09/28/Request/","link":"","permalink":"https://heung-bae-lee.github.io/2019/09/28/Request/","excerpt":"","text":"Requests 모듈 http request/response를 위한 모듈 HTTP method를 메소드 명으로 사용하여 request 요청 예)GET, POST 가장 우리가 흔하게 크롤링을 하는데 사용하는 방식이며, API만 알고 있다면 쉽게 사용할 수 있다!","categories":[],"tags":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/tags/crawling/"}]},{"title":"웹 기본 지식 이해하기 01(chrome을 이용하여 웹페이지 분석하기)","slug":"HTTP_method","date":"2019-09-28T05:52:18.000Z","updated":"2019-12-06T04:56:15.576Z","comments":true,"path":"2019/09/28/HTTP_method/","link":"","permalink":"https://heung-bae-lee.github.io/2019/09/28/HTTP_method/","excerpt":"","text":"HTTP HyperText Transfer Protocol: HTML(HyperText Markup language) 문서 등의 리소스를 전송하는 프로토콜(규약) 클라이언트에서 서버로 HTTP 요청을 하는데 이 요청 방식으로는 Get, Post, Put, delete, Option등이 있는데 이 중 크롤링에서 가장 많이 쓰이는 두가지(Get, Post)를 알아볼 것이다. GET 요청 : 데이터를 URL에 포함하여 전달이 가능하다. 그러므로 정보의 공유가 가능하다. 공유가 가능하다는 의미는 가장 쉬운 예를 들자면 URL 클릭하면 그 정보를 담고있는 웹 페이지를 요청하여 우리의 웹 브라우져에 랜더링을 거쳐 보여주는 것이 가능하다는 의미이다.(주로 리소스 요청에 사용) ex)https://search.naver.com/search.naver?sm=top hty&amp;fbm=1 POST 요청 : 데이터를 Form data에 포함하여 전달 그래서 대부분 우리 눈에 안보이는 요청이다.(주로 로그인에 사용) 예를 들어 설명하자면, 먼저 로그인 페이지가 있다면, 참고로 이 로그인 페이지는 리소스를 요청하는 것이므로 GET 방식이고 로그인 페이지에서 로그인을 하는 행위를 할 경우 이 떄 사용된다. ex) https://www.kangco.com/meber/member_check.asp 어떠한 방식으로 해당 웹 페이지가 서버에 요청을 하는지는 개발자 도구의 Network 탭에서 특정 Name을 클릭 후 Request Method를 보면서 확인할 수 있다. HTML element 이해하기(tag, attribute, value) HTML(Hyper Text Markup Language) 웹사이트를 생성하기 위한 언어로 문서와 문서가 링크로 연결되어 있고, tag를 사용하는 언어 태그(Tag)란? HTML 문서의 기본 블락 &lt;태그명 속성1=&quot;속성값1&quot; 속성2=&quot;속성값2&quot;&gt;Value&lt;/태그명&gt; (Value가 있는 경우) &lt;태그명 속성1=&quot;속성값1&quot; 속성2=&quot;속성값2/&quot;&gt; (Value가 없는 경우) 크롤링을 단 한번이라도 직접 해보신 분들은 우리가 원하는 값을 추출하기 위해서는 어느 정도의 HTML 지식이 있어야한다. 즉, Value를 추출하기 위해 해당 Value가 포함되어 있는 tag의 구조를 알아야 한다는 것이다. 위의 단순한 tag의 구조는 그런 관점에서 혹시라도 HTML의 구조를 모르시분들을 위해 간단히 설명하고 넘어가는 것이다. Requests 모듈 http request/response를 위한 모듈 HTTP method를 메소드 명으로 사용하여 request 요청 예)GET, POST 가장 우리가 흔하게 크롤링을 하는데 사용하는 방식이며, API만 알고 있다면 쉽게 사용할 수 있다!","categories":[],"tags":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/tags/crawling/"}]},{"title":"colab & Kaggle 연동 및 기초 사용법","slug":"kaggle_00","date":"2019-08-01T07:26:46.000Z","updated":"2019-09-28T05:51:41.381Z","comments":true,"path":"2019/08/01/kaggle_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/08/01/kaggle_00/","excerpt":"","text":"Kaggle Korea에서 진행중인 대회에서 Kaggle Kernel을 사용하다 보니 커널이 자꾸 죽는 이유는 도대체 무엇인지… competition에 늦게 참여한 관계로 더 시간이 촉박하기만 한데…. 그래서 google colab으로 바꾸려고 생각하였다. 캐글은 예측모델 및 분석 대회를 하는 플랫폼이다. 개인 및 단체에서 해결하고 싶은 과제와 데이터를 등록하면, 캐글에 가입한 데이터 과학자들이 모델을 개발하고 결과를 등록한다. 예측력을 순위로 하여 가장 좋은 순위에게는 상금도 주워진다. 그만큼 데이터 사이언스에 관한 분들은 모를 수 없는 사이트라고 생각한다. Google Colaboratory는 Google Drive + Jupyter Notebook의 기능을 가지고 있으며, Google Drive처럼 협업 가능(동시에 수정 가능)하다고 한다. https://colab.research.google.com/로 접속시 사용 가능하다. 무엇보다 가장 좋았던 점은 캐글의 커널은 9시간이 최장 이용시간인 반면에, colab은 12시간이다. 3시간 차이에 얼마나 더 바뀌겠냐라는 분들도 계시 겠지만 GPU가 없는 나에겐 3시간은 엄청난 시간이다. 더 자세한 사항은 [https://zzsza.github.io/data/2018/08/30/google-colab/][https://zzsza.github.io/data/2018/08/30/google-colab/] 이 블로그를 참조하는 것을 추천한다! 개인적으로 데이터 사이언스에 관해 많은 것에 대해 자세히 다루고 있다고 생각하며 강추한다!(절대 홍보글 아님.) 구글 드라이브와 Colab 연동 매번 session이 끊기거나 종료되면 이 작업을 해주어야 한다. 그래도 kaggle 보단 내 컴퓨터에선 덜 끊긴다. 먼저, 구글 드라이브와 연동을 시키는 이유는 로컬에서 Colab working directory로 파일을 업로드하게 되면 차후 다시 접속할 때 다시 업로드를 해주어야하지만 구글 드라이브에선 바로 읽을 수 있기 때문이다. 1234567from google.colab import authauth.authenticate_user()# colab에서 drive란 폴더를 만든 후, 우리 구글 드라이브의 root와 drive 폴더를 연결(mount)from google.colab import drivedrive.mount('/content/gdrive') 구글 드라이브와 로컬 연동 파일을 하나씩 업로드하지 말고 대량의 파일을 한꺼번에 업로드하고 싶은 경우 [BackupAndSync](https://www.google.com/drive/download/)를 사용해 로컬과 구글 드라이브를 연동 1) 위 링크를 클릭해 백업 및 동기화 다운로드 2) InstallBackupAndSync.dmg라는 파일을 클릭한 후, (열리지 않으면 우클릭 후 열기) 프로그램 설치 3)맥북 환경이 한글이신 분은 Google에서 백업 및 동기화라는 응용 프로그램이 추가됨(이것도 실행이 안되면 클릭 후 실행) 환경 설정에서 동기화할 폴더 선택 (단, 크기가 큰 파일은 동기화 시간이 오래 걸릴 수 있음) Kaggle 연동하기- 1) Kaggle beta API Json Key 다운- Kaggle - My Account - Dataset 옆에 있는 …을 클릭한 후, Account로 이동 - 하단에 API 부분에 Create New API Token을 클릭하면 Json Key가 다운로드 됨 - 이 Json 키를 매번 Colab에서 올려서 할 수도 있지만, 더 편하게 사용하고 싶어서 Google Storage에 Json 파일을 올리고, 거기서 키를 복사해오는 방법으로 진행합니다 - 2) Google Storage에 Json Key 저장- Google Storage로 이동한 후, Storage 버킷 선택 (버킷이 없다면 생성!) - Colab에서 아래 명령어 입력 123456from google.colab import authauth.authenticate_user()!mkdir -p ~/.kaggle!mv ./kaggle.json ~/.kaggle/!chmod 600 ~/.kaggle/kaggle.json 우선 여기까지! 내일 다시 시작합니다!!!!!","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"https://heung-bae-lee.github.io/categories/Kaggle/"}],"tags":[]},{"title":"[CS231n]Lecture10-Recurrent Neural Networks","slug":"cs231n_10","date":"2019-07-29T07:22:55.000Z","updated":"2019-07-31T13:40:54.829Z","comments":true,"path":"2019/07/29/cs231n_10/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/29/cs231n_10/","excerpt":"","text":"","categories":[],"tags":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/tags/CS231n/"}]},{"title":"[수리통계학] 나혼자 정리하는 통계생의 수리통계학 00","slug":"Statistics_00","date":"2019-07-29T07:12:38.000Z","updated":"2019-07-31T13:40:58.126Z","comments":true,"path":"2019/07/29/Statistics_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/29/Statistics_00/","excerpt":"","text":"먼저, 이 글은 data science라는 분야를 공부하며 원래 통계학을 전공했던 나였지만 학부과정에서 배웠던 그리고 부끄럽지만 석사 때 배웠다고 기억하고 있는 조각들을 모아 수리통계학을 간단히 정리하기 위해 쓰는 글이다. 확률과 확률분포여러 가지 조사 연구들은 동일한 조건에서의 반복 실험이 대체적으로 표준이 된다는 것이 특징이다. 예를 들어, 의학 연구에서는 복용된 약의 효과에 관심이 있을 것이다. 우리가 어떤 결론을 내리기 까지 그에 따른 근거를 뒷받침하기 위한 정보를 얻을 수 있는 유일한 방법은 실험이다. 이런 실험을 통해 얻은 실험값들은 예측할 수 없다는 것이 실험의 특성이다. 여기서 통계를 공부하다 보면 제일 먼저 알게되는 sample space를 정의할 것이다. sample space : 같은 조건에서 반복할 수 있는 실험에서 실현 가능한 결과의 집합","categories":[{"name":"Statistics - Mathematical Statistics","slug":"Statistics-Mathematical-Statistics","permalink":"https://heung-bae-lee.github.io/categories/Statistics-Mathematical-Statistics/"}],"tags":[]},{"title":"[CS231n]Lecture09-CNN_Architectures","slug":"cs231n_09","date":"2019-07-28T03:43:10.000Z","updated":"2019-07-31T13:40:52.676Z","comments":true,"path":"2019/07/28/cs231n_09/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/28/cs231n_09/","excerpt":"","text":"LeNet 산업에 아주 성공적으로 적용된 최초의 ConvNet이다. 이미지를 입력으로 받아 Stride = 1인 5x5필터를 거치고 몇 개의 Conv Layer와 Pooling Layer를 거친다. 그리고 마지막 출력 노드 전에 Fully Connected Layer가 붙는다. 엄청 간단한 모델이지만 숫자 인식에서 엄청난 성공을 거두었다. AlexNet 2012년에 등장한 최초의 Large scale CNN이며 Image Classification Benchmark의 우승 모델이다. ConvNet 연구의 부흥을 일으킨 장본인이다. 수년 전까지 대부분의 CNN 아키텍쳐의 베이스 모델로 사용되어 왔다. AlexNet은 다양한 Task의 transfer learning에 많이 사용되었다. AlexNet은 기본적으로 conv - pool - normalization 구조가 2번 반복된다. 그리고 conv layer가 조금 더 붙고 (CONV 3,4,5) 그 뒤에 pooling layer가 있다.(Max POOL3) 그리고 마지막에는 Fully connected layer가 몇 개 붙는다.(FC 6,7,8) 생긴 것만 봐서는 기존의 LeNet과 상당히 유사하며 layer만 늘어 났다고 생각이 들 것이다. AlexNet은 5개의 Conv Layer와 2개의 FC-Layer(마지막 FC Layer 전까지)로 구성된다. 다른 Conv Net의 다이어그램과 유사하긴 하지만 한 가지 차이점이 있다. 모델이 두개로 나누어져서 서로 교차한다는 것이다. AlexNet을 학습할 당시에 3GB GTX850으로 학습시켰다. 그래서 전체 Layer를 GPU에 다 넣을 수 없어서 분산 시켜 넣을 수 밖에 없었다. 각 GPU가 모델의 뉴런과 Feature Map을 반반씩 나눠가진다. Conv 1,2,4,5를 살펴보면 같은 GPU 내에 있는 Feature Map만 사용할 수 있다. 즉, 전체 96개의 feature Map을 볼 수 없다. 그래서 다이어그램을 보면 각 Conv Layer의 Depth가 48인 것이다. Conv 3, FC 6,7,8를 보면 이 Layer들은 이전 계층의 전체 Feature Map과 연결되어 있다. 이 Layer들에서는 GPU간의 통신을 하기 때문에 이전 입력 Layer의 전체 Depth를 전부 가져올 수 있는 것이다. AlexNet 논문의 아키텍쳐와 관련한 조그만 이슈는 그림을 자세히 보면 첫 Layer가 224x224라고 되어 있는데, 실제 입력은 227x227이다. 질문 Pooling layer에는 파라미터가 없는가? 파라미터는 우리가 학습시키는 가중치이다. Conv Layer에는 학습할 수 있는 가중치가 있다. 반면 Pooling의 경우에는 가중치가 없고 그저 특정 지역에서 큰 값을 뽑아내는 역할만 한다. 따라서 학습시킬 파라미터가 없는 것이다. 각각의 Layer의 파라미터 사이즈를 계산해 보자!!!! ZFNet 2013년 우승 모델이며 AlexNet의 하이퍼 파라미터를 개선한 모델이다. AlexNet과 같은 Layer 수이고, 기존의 구조도 같다. 다만, stride size, 필터 수 같은 하이퍼 파라미터를 조절해서 AlexNet의 Error rate를 좀 더 개선시켰다. 앞으로 언급할 두가지 모델은 Batch normalization이 없던 시절이었기 때문에 깊은 모델을 학습시키는 일이 어려웠다. 그렇기에 깊은 모델을 수렴시키기 위해서 각종 테크닉을 사용해야 했다.\u001d 먼저, VGG는 초기에 11 Layer이었는 11 Layer가 모델이 잘 수렴하는 한계였기 때문이다. 그리고 나서 11 Layer 중간에 Layer를 무작위로 추가해서 VGG-16, VGG-19를 학습시켰다. GoogLeNet의 경우에는 auxiliary classifiers를 도입하여 단지 네트워크의 초기 Layer에 gradient를 직접 흘려 보내기 위한 수단이었다.(성능을 올리기 위해 도입한것이 아니다! 그리고 Batch Norm이 있다면 굳이 이런식의 테크닉들은 더이상 필요치 않다!!!) VGGNet 훨씬 더 깊어졌고 더 작은 필터를 사용한다. 더 깊게 쌓이므로써 Non-Linearity를 더 추가 할 수 있고 파라미터 수도 더 적어지게 되었다. AlexNet에서는 8개의 Layer -&gt; VGGNet 16~19개의 Layer 3x3필터만 사용 (이웃픽셀을 포함할 수 있는 가장 작은 필터) 이유 : 필터의 크기가 작으면 파라미터의 수가 더 적어서 Depth를 더 키울 수 있다. 3x3 필터를 여러 개 쌓은 것은 결국 7x7 필터를 사용하는 것과 실질적으로 동일한 Receptive Filter를 가지는 것이다. 작은 필터를 유지해 주고 주기적으로 Pooling을 수행하면서 전체 네트워크를 구성하게 된다. fc7 은 4096 사이즈의 Layer인데 아주 좋은 feature representation을 가지고 있는 것으로 알려졌으며 \b다른 데이터에서도 feature 추출이 잘되며 다른 Task에서도 일반화 능력이 뛰어난 것으로 알려져있다. VGG19의 경우 VGG16과 유사한 아키텍쳐이지만 Conv Layer가 조금 더 추가 되었다. VGG19가 아주 조금 더 좋다. 그러나 보통 VGG16을 더 많이 사용한다. AlexNet에서 처럼 모델 성능을 위해서 앙상블 기법을 사용했다. 질문) 3x3인 Stride가 1인 필터 3개를 쌓게 되면 실질적인 Receptive Field가 어떻게 될까? Receptive Field은 필터가 한번에 볼 수 있는 입력의 Spatial area이다. 첫번째 Layer의 Receptive Field는 3x3이다. 두 번째 Layer의 경우는 각 뉴런이 첫 번째 Layer 출력의 3x3 만큼을 보게 될 것이다. 그리고 3x3 중에 각 사이드는 한 픽셀씩 더 볼 수 있게 된다. 따라서 두번째 Layer의 경우는 실제로 5x5의 receptive filed를 가지게 된다. 3번째 Layer의 경우 2번째 Layer의 3x3을 보게된다. 그리고 이 과정을 피라미드처럼 그려보면 입력 Layer의 7x7을 보게 되는 것이다.따라서 실질적인 Receptive Field는 여기에서 7x7이 된다. 하나의 7x7 필터를 사용하는 것과 동일하다. 질문) 하나의 Conv Layer 내에 여러개의 필터가 존재하는 이유는? 각 필터가 존재하는 이유는 서로 다른 패턴을 인식하기 위해서라고 할 수 있다. 각 필터는 각각의 Feature Map을 만들게 된다. 질문) Localization은 무엇인가? task 중에서 예를 들면 “이미지에 고양이가 있는가?”를 분류하는 것 뿐만 아니라 정확히 고양이가 어디에 있는지 네모박스를 그리는 것이다. Detection은 이미지 내에 다수의 객체가 존재할 수 있다. 그에 반해 localization은 이미지에 객체가 하나만 있다고 가정하고 이미지를 분류하고 추가적으로 네모박스도 쳐야한다. 질문) 네트워크가 깊어질수록 Layer의 필터 갯수를 늘려야 하는지?(Channel Depth를 늘려야 하는\u001d지) 디자인하기 나름이고 반드시 그럴 필요는 없다. 하지만, 실제로 사람들이 Depth를 많이 늘리는 경우가 많다. Depth를 늘리는 이유 중 하나는 계산량을 일정하게 유지시키기 위해서이다. 왜냐하면 보통 네트워크가 깊어질수록 각 Layer의 입력을 Down sampling하게 된다. 즉, 네트워크를 통해 나가면서 점점 정보를 잃어나가는 현상이 발생될 수 있다는 것이다. 그러므로 Spatial area가 작아질수록 필터의 depth를 조금씩 늘려주게 된다. Width나 Height가 작아지기 때문에 Depth를 늘려도 부담이 없다. GoogLeNet 2014년 Classification Challenge에서 우승한 모델이다. 22개의 Layer를 가진 깊은 네트워크이다. 가장 중요한 것은 효율적인 계산에 관한 그들의 특별한 관점이 있다는 것과 높은 계산량을 아주 효율적으로 수행하도록 네트워크를 디자인했다는 점이다. “a good local network typology”를 디자인 하고 싶었다. 그리고 “network within a network”라는 개념으로 local topology를 구현했고 이를 쌓아올렸다. Inception module을 여러개 쌓아서 만든다. 또한 파라미터를 줄이기 위해 FC-Layer를 사용하지 않는다. 전체 파라미터 수가 60M인 AlexNet에 비해 GoogLeNet은 전체 파라미터 수가 5M 정도이다. Inception Module 내부에는 동일한 입력을 받는 서로 다른 다양한 필터들이 병렬로 존재한다. 이전 Layer의 입력을 받아서 다양한 Conv 연산을 수행 한 후 각 Layer에서 각각의 출력 값들이 나온다. 그 출력들을 모두 Depth 방향으로 합친다.(concatenate) 그렇게 합치면 하나의 tensor로 출력이 결정되고 이 하나의 출력을 다음 레이어로 전달하는 것이다. 질문) 이러한 다양한 연산을 수행하고 이를 하나로 합쳐주는 아주 단순한 방식이 갖는 문제점은 무엇일까? 계산 비용에 문제가 있다. 1x1 conv의 경우 입력에 맞춰 depth는 256이다. 그리고 128개의 필터 하나다. 그리고 128개의 필터 하나 당 28x28 Feature map을 생성하게 될 것이다. 이런식으로 다른 Layer의 출력값을 계산해 보면 다음 그림과 같을 것이다. 참고로 이런 계산이 나온 이유는 spatial dimension을 유지하기 위해서 zero padding을 하였기 때문이다.Stride를 잘 조절해서 Spatial dimension를 유지하면 입력과 출력의 크기는 같게 된다. 즉 28x28은 동일하고 depth가 점점 쌓이게 된다는 것을 의미한다. 그림에서는 최종적으로 28 x 28 x 672 가 된다. Inception module의 입력은 28x28x256 이었으나 출력은 28x28x672이 된 것이다. Spatial dimension은 변하지 않았지만 depth가 엄청나게 불어난 것이다. 연산량이 \u001c많다는 것이 문제이며, Pooling Layer는 Depth를 그대로 유지하기 때문에 문제를 악화 시킨다. 위와 같은 문제를 해결하기 위한 key insight bottleneck layer를 이용하는 것이다. Conv 연산을 수행하기에 앞서 입력을 더 낮은 차원으로 보내는 것이다.(depth를 더 낮은 차원으로 projection하는 \u001d것) input feature map들 간의 선형 결합(linear combination)이라고 할 수 있다. 주요 아이디어는 바로 입력의 depth를 줄이는 것이다.각 Layer의 계산량은 1x1 conv를 통해 줄어든다. 질문) 1x1 Conv를 수행하면 일부 정보손실이 발생하지 않는다? 정보 손실이 발생할 순 있지만 동시에 redundancy가 있는 imput features를 선형결합 한다고 볼 수 있다. 1x1 conv로 선형결합을 하고 non-Linearity(ReLU같은)를 추가하면 네트워크가 더 깊어지는 효과도 있다. 일반적으로 1x1 con를 추가하면 여러모로 도움이 되고 더 잘 동작한다. 위의 그림에서 파란색 네모 박스는 추가시킨 보조 분류기(auxiliary classifier)이다. 그 구조는 Average pooling과 1x1 conv가 있고 FC-layer도 몇개 붙는 우리가 알고있는 작은 네트워크들이다. SoftMax로 1000개의 ImageNet class를 구분한다. 또한 네트워크가 깊기 때문애 이곳에서도 loss를 계산하\b고 추가적인 gradient를 얻을 수 있고 따라서 중간 Layer의 학습을 도울 수 있다. 질문) 보조분류기에서 나온 결과를 최종 분류 결과에 이용할 수 있는가?? GoogLeNet 학습 시, 각 보조분류기의 Loss를 모두 합친 평균을 계산한다. 아마도 도움이 될 것이다. 질문) bottleneck layer를 구성할 때 1x1 conv 말고 다른 방법으로 차원을 축소시켜도 되는가?? 여기에서 1x1 conv를 쓴 이유는 차원 축소의 효과도 있고 다른 Layer들 처럼 conv Layer이기 때문이다. 차원 축소 과정에서 이전의 feature map과 연관이 있는지 학습하려면 전체 네트워크를 Backprop으로 학습시킬 수 있어야 한다. 네트워크가 엄청 깊은 경우에서는 gradient 신호가 점점 작아지게 되고 결국에는 0에 가깝게 될 수 있다. 그렇기 때문에 보조 분류기를 이용해서 추가적인 gradient 신호를 흘려준다. 이 때 backprop은 각 보조분류기 별로 실행하는 것이 아닌 네트워크 전체가 한번에 작동시킨다. 질문) 각 Layer가 가중치를 공유하는가 아닌가?? 모든 Layer를 가중치를 공유하지 않는다. ResNet 2015년도 우승 모델이며, 혁명적으로 네트워크의 깊이가 깊어진 모델이다.(152개의 Layer) residual connection 이라는 방법을 사용한다. residual block들을 쌓아올리는 구조이다. short connection과 residual block이 도입된 모\u001d델 모델 Depth가 50이상일 때 Bottleneck Layers를 도입 ‘CNN을 깊고 더 깊게 쌓게 되면 어떤 일이 발생할까?’라는 의문에서 부터 시작된다. 예를 들어, VGG에 conv pool Layer를 깊게만 쌓는다고 과연 성ㄴ능이 더 좋아지는 것이 맞는지를 보자면 아니라는 것이다. 다음 그림에서 56-Layer와 20-Layer를 비교해서 설명하고 있다. 우리는 56-Layer는 상대적으로 더 많은 파라미터를 가지고 있기에 20-Layer보다 좀더 overfitting이 일어날 확률이 높다고 생각할 수 있을 것이다. 허나 우리 예상처럼 test error는 56-layer가 더 낮지만 training error 또한 더 낮기에 overfitting이 원인이 아니라는 것을 확실히 알 수 있다. ResNet 저자들아 내린 가설은 더 깊은 모델 학습 시 Optimization에 문제가 생긴다는 것이다. 그렇다면 모델이 더 깊다면 적어도 더 얕은 모델만큼은 성능이 나와야 하지 않는가라는 생각으로 인해 더 얕은 모델의 가중치를 깊은 모델의 일부 Layer에 복사한다. 그리고 나머지 Layer는 identity mapping을 하여 Deeper Model의 학습이 제대로 안되더라도 적어도 shallower Model 만큼의 성능을 보장하게끔 디자인 한다. 이런 아이디어를 모델에 적용시키기 위해 가중치가 없으면 입력을 identity mapping을 시켜 출력으로 내보내는 Skip Connection을 도입하게 된다. 실제 Layer는 변화량(delta)만 학습하면 된다. 입력 X에 대한 잔차(residual)이라고 할 수 있다. 질문) Layer의 출력과 Skip Connection의 출력이 같은 차원인가? 그렇다. 두 경로의 출력 값 모두 같은 차원이다. 일반적으로는 같은 차원이 맞지만, 그렇지 않은 경우에는 Depth-wise padding으로 차원을 맞춰준다. 질문) Layer의 출력인 Residual의 의미는 무엇인가? 아래 그림을 보면, 전체 출력 값은 F(x)+X 이고, F(x)는 Layer의 출력 값이다. X는 그저 입력값이다. 왼쪽의 평범한 네트워크는 H(x)를 학습시키고 있지만 아주 깊은 네트워크에서는 H(x)를 학습시키는 것은 너무 어렵다. 그래서 ResNet의 아이디어는 H(x)=F(x)+x이므로 F(x)를 학습시켜보면 어떨까라는 것이다. 즉, H(x)를 직접 배우는 대신에 X에 얼마의 값을 더하고 빼야할까를 배우는 것이 쉬울것이라고 생각한 것이다. 이것은 단지 가설일 뿐이지만 가설이 참이라면 모델의 일부가 학습된 shallow layers이고 나머지 layer들은 identity로 구성되어진 상황에서는 잘 동작할 것이다. 이는 대부분의 layer가 잘 동작하려면 layer의 출력이 identity에 가까워야 할지 모른다는 것을 암시한다. 이 때문에 Identity(Input) + 변화량(delta)만 학습시키면 된다. 예를들어, Output = Input(Identity)이어야만 하는 상황이\u001d면 F(x) = 0 이 되면 그만이다. 이는 상대적으로 학습시키기 쉽다고 볼 수 있다. ResNet에서는 Layer의 출력은 입력 + residual block의 출력이다. 우선 residual block의 가중치가 0이면 이 block은 identity mapping을 한다. 이러한 속성으로 모델이 필요없는 Layer를 사용하지 않도록 학습하는데 아주 유용하다. ResNet의 관점에서 L2 Regularization을 해석해 볼 수도 있다. Layer에 L2 Regularization을 추가시키면 L2는 모든 파라미터가 0이 되도록 노력할 것이다. 사실 CNN Architectures의 관점에서 보면 모든 파라미터가 0이면 이상하다. 하지만 ResNet의 관점에서는 파라미터를 0으로 만드려는 속성은 모델이 불필요한 Layer를 사용하지 않도록 해줄 수 있다.","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"내가 정리하는 C/C++ 00","slug":"data_structure_00","date":"2019-07-24T08:03:14.000Z","updated":"2020-03-19T11:07:46.234Z","comments":true,"path":"2019/07/24/data_structure_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/24/data_structure_00/","excerpt":"","text":"개발환경 구축하기C와 C++1) C++는 기본적으로 C의 기능을 확장한 형태의 프로그래밍 언어이다.2) 따라서 C 언어의 기능을 포함하고 있다는 점에서 C++프로젝트로 .C 확장자를 갖는 파일을 생성하여 코딩해도 정상적으로 동작한다. 개발환경 구축하기 나는 개인적으로 IDE(Integrated Development Environment) 중에서 Atom을 이미 설치하고 있기에 따로 대표적인 Visual Studio를 설치하진 않았다. Atom에서는 따로 gpp-compiler 라는 패키지를 설치해주면 끝난다! 단축키 또한 자신이 커스터마이징 할 수 있는데, 나는 default인 f5가 compile f6이 디버깅으로 되어있는 상태에서 사용할 것이다. Development_environment 전통적인 프로그램은 전처리기 -&gt; 컴파일러 -&gt; 링커를 거쳐 실행파일로 만들어진다. 역시! 모든 언어의 기초를 배울때 하는 Hello World를 언급하며 시작해보자! 12345678#include &lt;stdio.h&gt;int main(void) &#123; printf(\"Hello world\\n\"); // system(\"pause\"); system( \"read -n 1 -s -p \\\"Press any key to continue...\\\"\" ); return 0;&#125; 명령문 하나하나씩 설명을 하자면 처음, #include 명령어를 이용해 다양한 라이브러리를 불러 올 수 있다. 위에서 불러온 stdio.h(standard io)는 여러 기본적인 기능을 포함하고 있지만 그 중 대표적으로 위에서 사용한 printf가 있다.main함수는 다양한 함수가 사용 될 수 있겠지만 처음 시작할 때는 main함수 이후에 사용한다. 또한 가장 큰 특징은 함수는 반환값이 없을 수도 있지만 main 함수에서는 항상 0을 반환하는 것이 일반적이다. 나와 같이 R과 python으로 프로그래밍을 배운 사람이라면 ;이 낯설을 것이다. C/C++에서는 하나의 명령어가 끝났음을 알리기 위해 ;을 붙인다. 위의 명령 프롬프트에서 pause 명령어를 실행시키면 키보드를 입력 전까지 대기하는 기능을 사용할 수 있다. system 함수를 이용하여 운영체제의 기본적인 기능을 이용할 수 있다. 허나, 나의 mac북은 window의 프롬프트 창과는 다른 운영체제이므로 당연히 pause가 걸리지 않는다! 실행하면 찾을 수 없는 명령어라고 나오므로 약간의 편법으로 구사할 수 있을 것 같다! 혹시라도 mac에서 pause를 걸려한다면 저런식으로 만들어 보는 방법도 있다.","categories":[{"name":"C/C++/자료구조","slug":"C-C-자료구조","permalink":"https://heung-bae-lee.github.io/categories/C-C-자료구조/"}],"tags":[]},{"title":"[CS231n]Lecture08-Deep learning Software","slug":"cs231n_08","date":"2019-07-23T05:13:30.000Z","updated":"2019-07-24T07:35:38.400Z","comments":true,"path":"2019/07/23/cs231n_08/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/23/cs231n_08/","excerpt":"","text":"GPU Graphics card 또는 Graphics Processing Unit이라고 하는데 결국엔 우리가 아는 사실 처럼 computer graphics를 랜더링하기 위해 더 와닿게 말하자면 게임을 더 최적의 환경에서 하기 위해 만들어 졌다고 할 수 있다. # Cores Clock speed Memory Price CPU 4 4.4 GHz Shared with system $339 CPU 10 3.5 GHz Shared with system $1723 GPU 3840 1.6 GHz 12GB GDDR5X $1200 GPU 1920 1.68 GHz 8GB GDDR5 $399 위의 표에서 볼 수 있듯이 CPU의 경우 core의 수가 적다. GPU는 CPU보다 훨씬 더 많은 core를 가지고 있지만 각각의 코어가 더 느린 clock speed에서 동작하며 그 코어들이 CPU처럼 독립적으로 동작하지 않으며 많은 일을 처리할 수 없다. GPU는 코어마다 독립적인 테스크가 있는 것이 아니라 많은 코어들이 하나의 테스크를 병렬적으로 수행한다. GPU의 코어의 수가 많다는 것은 어떤 테스크가 있을 때 그 테스크에 대해 병렬로 수행하기 아주 적합하다는 것을 알 수 있다. CPU에도 캐시가 있지만 비교적 작다. 대부분의 memory는 RAM에서 끌어다 쓴다. 실제 RAM과 GPU간의 통신은 상당한 보틀넥을 초래한다. 그렇기 때문에 GPU는 보통 RAM이 내장되어 있다. GPU는 내장되어있는 메모리와 코어 사이의 캐싱을 하기 위한 일종의 다계층 캐싱 시스템을 가지고 있다. 이는 CPU의 캐싱구조와 매우 유사하다. CPU는 범용처리에 적합하고, GPU는 병렬처리에 더 특화되어 있다. GPU에서 정말 잘 동작하고 아주 적합한 알고리즘은 바로 행렬곱 연산이다. 실제로 GPU로 학습을 할 때 생기는 문제 중 하나는 바로 Model과 Model의 가중치는 전부 GPU RAM에 상주하고 있는 반면에 Train data는 하드드라이브(SSD)에 있다는 것이다. 때문에 Train time에 디스크에 디스크에서 데이터를 읽어들이는 작업을 세심하게 신경쓰지 않으면 보틀넥이 발생할 수 있다. 즉,GPU는 forward/backward 가 아주 빠른 것은 사실이지만, 디스크에서 데이터를 읽어들이는 것이 보틀넥(병목현상)이 되는 것이다. 이러한 문제를 해결하기 위한 해결책 중 하나는 데이터셋이 작다면 데이터 전체를 RAM에 올려 놓는 것이다. 또는 데이터셋이 작지 않더라도, 서버에 RAM 용량이 크다면 가능 할 수도 있을 것이다. 또한 기본적으로 HDD 대신 SSD를 사용하는 것이 좋다. 또 다른 방법으로는 CPU의 multiple CPU threads(CPU의 다중스레드)를 이용해서 데이터를 RAM에 미리 올려 놓는 것이다.(Pre-fetching)GPU는 빠른데 데이터 전송 자체가 충분히 빠르지 못하면 보틀넥이 생길수 밖에 없다. Deep learning frameworkDeep learning framework를 사용하는 이유1) 딥러닝 프레임워크를 이용하면 엄청 복잡한 그래프를 우리가 직접 만들지 않아도 된다.2) forward pass만 잘 구현해 놓는다면 Back propagation은 알아서 구성되어 gradient를 쉽게 계산할 수 있다.3) cuBLAS, cuDNN, CUDA 그리고 memory등을 직접 세심하게 다루지 않고 GPU를 효율적으로 사용할 수 있다. framework의 존재 목표는 forward pass 코드를 Numpy스럽게 작서을 해 놓으면 GPU에서도 동작하고 gradient도 알아서 계산해 주는 것이다. [그림1][그림2] Tensorflow placeholder는 그래프 밖에서 데이터를 넣어주는 변수이고, variable은 그래프 내부에 있는 변수이다. Tensorflow는 분산처리도 지원하기 떄문에 서로 다른 머신을 이용해 graph를 쪼개서 실행시킬 수도 있다. 혹 분산처리를 계획한다면 Tensorflow가 유일한 선택지가 될 것이다. Pytorch Facebook에서 나온 PyTorch는 TensorFlow와는 다르게 3가지 추상화 레벨을 정의해 놓았다. 이미 고수준의 추상화를 내장하고 있기에 (Module 객체) TensorFlow 처럼 어떤 모듈을 선택할 지 고민할 필요가 없다. tensor : Numpy array와 유사한 tensor object가 있으며 GPU에서 작동한다. tensorflow의 Numpy array variable : 그래프의 노드(그래프를 구성하고 gradient 등을 계산) tensorflow의 Tensor, Variable, Placeholder Module : 전체 Neural network를 구성 tensorflow의 tf.layers, TFSlim, TFLearn 등 Static computational graph vs Dynamic graphPytorch와 TensorFlow의 주된 차이점 중 하나이다. TensorFlow는 두단계로 나누어진다.(Static computational graph - 그래프가 단 하나만 고정적으로 존재하기 때문이다.) 1) 그래프를 구성하는 단계 2) 구성된 그래프를 반복적으로 돌리는 단계 그래프를 한번 구성해 놓으면 학습시에는 동일한 그래프를 아주 많이 재사용하게 된다. 그러므로 그런 그래프를 최적화시킬 기회가 주어질 수 있다. 처음 최적화 시킬 때 까지 시간이 소요된다 하더라도 최적화된 그래프를 여러번 사용한다는 것을 고려해보면 그에 따른 소요된 시간은 중요치 않을 수 도 있다. 또한 메모리내에 그 네트워크 구조를 갖고 있다는 것이되므로 \b네트워크 구조를 파일 형태로 저장할 수 있다. 그래프의 모든 전체적인 연산들을 다 고려해서 만들어 주어야한다.(ex.loop문) TensorFlow Fold라는 TF 라이브러리가 static graph으로 만든 트릭으로 dynamic graphs를 작성하게 해준다. Pytorch는 하나의 단계이다.(Dynamic computational graph) 매번 forward pass 할 때 마다 새로운 그래프를 구성한다. 또한, 그래프 구성과 그래프 실행하는 과정이 얽혀 있기에 모델을 재사용하기 위해서는 항상 원본 코드가 필요하다. 코드가 훨씬 깔끔하고 작성하기 더 쉽다. tensorflow와는 다르게 python 명령어들을 활용할 수 있다. 다양한 데이터에도 유동적인 네트워크를 만들 수 있다.(RNN사용 - NLP에서 문장을 파싱하는 문제 중 트리를 파싱하기 위해 recursive한 네트워크가 필요할 수 있다.) Recurrent network, Recursive network, Modular Networks(이미지와 질문을 던지면 적절한 답을 하는 구조)를 구성할 때 조금 더 편할 수 있다.","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"[CS231n]Lecture07-Training Neural Networks2","slug":"cs231n_07","date":"2019-07-22T03:24:40.000Z","updated":"2019-07-23T04:21:56.948Z","comments":true,"path":"2019/07/22/cs231n_07/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/22/cs231n_07/","excerpt":"","text":"지난 6강에서는 activation function을 중점적으로 다루어 보았는데, 10년전 까지만 해도 sigmoid가 아주 유명했다. 허나, Vanishing gradient가 생기는 문제로 인해 최근에는 Sigmoid와 tanh 보다는 ReLU를 쓴다라고 했다. 대부분의 경우 normalize나 zero-centered로 데이터를 처리해 주지 않으면, Loss가 파라미터에 너무 민감하기 때문에 학습시키기에 어렵다. 하이퍼파라미터를 몇 개씩 선택하는지에 따른 고민은 보통 모델에 따라 다르며, 하이퍼파라미터의 수가 많을 수록 기하급수적으로 경우의 수가 늘어난다. 많은 하이퍼 파라미터 중 learning rate가 가장 중요할 것이라고 본다. regularization, learning rate decay, model size 같은 것들은 Learning rate보단 덜 중요하다. 그렇기에 Block Coordinate Descent(BCD) 같은 방법을 쓸 수도 있다. 우선 learning rate를 정해놓은 다음에 다양한 모델 사이즈를 시도해 보는 것이다. 이 방법을 사용하면 기하급수적으로 늘어나는 Search space를 조금은 줄일 수 있다. 하지만 정확히 어떤 순서로 어떻게 찾아야 할지 정해야 하는 것이 가장 큰 문제이다. 우리가 어떤 하이퍼파라미터 값을 변경할 시에 다른 하이퍼파라미터의 최적 값이 변해버리는 경우는 가끔 발생한다. 이런 경우 더 좋은 최적화 방법을 사용하면 모델잉 learning rate에 덜 민감하도록 할 수 있다. Fancier OptimizationSGD(Stochastic Gradient Descent)의 문제점 1)가중치가 움직일수 있는 방향 중 불균형한 방향이 존재한다면 SGD는 잘 동작하지 않을 것이다. 아래의 그림을 보고 수평축과 수직축 이 두가지에 대해 가중치의 변화에 따른 손실함수의 변화량이라고 생각해 보자.(우리가 쉽게 어릴적 보았던 지도에서 등고선을 떠올린다면 더 쉽게 이해할 수 있을 것이다.) 그렇다면, 수평축의 가중치 보다 수직축의 가중치가 훨씬 더 손실함수의 변화하는 속도가 빠를 것이다.(왜? 기울기가 더 가파르니까!!!) 즉, Loss는 수직 방향의 가중치 변화에 훨씬 더 민감하게 반응한다. 아래의 그림에서 red point가 현재 Loss라고 가정했을때, 현재 지점의 Hessian matrix의 최대/최소 singular values값의 비율이 매우 안좋다는 뜻이므로 Loss는 bad condition number를 지니고 있다고 말할 수 있을 것이다.[그림0] 아래와 같은 손실함수에서 SGD를 시행한다면, gradient의 방향이 고르지 못하기 때문에 지그재그 모양으로 gradient의 방향이 그려지게 된다. Loss에 영향을 덜 주는 수평방향 차원의 가중치는 업데이트가 아주 느리게 진행된다. 즉, 이렇게 가중치가 움직일수 있는 방향 중 불균형한 방향이 존재한다면 SGD는 잘 동작하지 않을 것이다. [그림1] 2)local minima와 saddle points X축은 어떤 하나의 가중치를 나타내고, Y축은 Loss를 나타내고 있다. 위의 그림은 local minima에 관한 그림이고, 아래의 그림은 saddle point와 관련된 그림이다. saddle point가 의미하는 것은 어떤 방향은 Loss가 증가하고 몇몇 방향은 Loss가 감소하고 있는 곳을 생각해 볼 수 있다. 그에 반해 local minima는 한 방향으로 Loss가 상승하는 방향이다. 이런 saddle point 문제는 고차원 공간에서는 더욱 더 빈번하게 발생하며, 지난 몇 년간 알려진 사실은 very large neural network가 local minima 보다는 saddle point에 취약하다는 것이다. saddle point 뿐만 아니라 saddle point 근처에서도 문제가 발생하는데 근처에서 gradient가 0은 아니지만 기울기가 아주 작은 곳들이 보일 것이다. 그것이 의미하는 바는 gradient를 계산해서 업데이트를 해도 기울기가 아주 작기 때문에 현재 가중치의 위치가 saddle point 근처라면 업데이트는 아주 느리게 진행된다는 점 또한 문제점이다. 3) mini-batch로 인한 가중치 업데이트는 추정값이다. 손실함수를 계산할 때는 엄청 많은 Training Set 각각의 loss를 전부 계산해야 한다. 매번 이렇게 전부를 계산하는 것은 어렵기 때문에 실제로는 mini-batch의 데이터들만 가지고 실제 Loss를 추정하기만 한다. 이는 gradient의 부정확한 추정값만을 구할 뿐이라는 것이다. 위에서 말한 위험요소들을 다루기 위해서 더 좋은 최적화 알고리즘이 필요하다. 아래에서 소개하는 최적화 알고리즘들의 velocity은 하이퍼 파라미터가 아니며 초기값을 항상 0으로 둔다!1) SGD + momentum 아이디어는 gradient의 방향으로만 움직이는 SGD에 velocity를 유지하는 것이다. 즉, gradient를 계산할 때 velocity를 이용한다. 현재 mini-batch의 gradient 방향만 고려하는 것이 아니라 velocity를 같이 고려하는 것이다.아래의 수식을 보면 velocity의 영향력을 rho의 비율로 맞춰주는데 보통 0.9 또는 0.99 같은 높은 값으로 맞춰준다. gradient vector 그대로의 방향이 아닌 velocity vector의 방향으로 나아가게 된다. [그림2]local minima와 saddle points 문제는 local minima에 도달해도 여전히 velocity를 가지고 있기 때문에 gradient가 0이라도 움직일 수 있으며 계속해서 내려갈 수 있다. saddle point 주변의 gradient가 작더라도, 굴러내려오는 속도가 있기 때문에 velocity를 가지게 되어 이 또한 잘 극복해 내고 계속 밑으로 내려올 수 있는 것이다. 기존의 SGD만을 사용했을 경우처럼 지그재그로 움직이는 상황을 momentum으로 인해 그러한 변동을 서로 상쇄시켜 버린다. noise가 평균화 되버리는 의미를 갖는다. 이를 통해서 Loss에 민감한 수직 방향의 변동은 줄여주고 수평방향의 움직임은 점차 가속화 될 것이다. momentum을 추가하게 되면 high condition number problem을 해결하는데 도움이 되는 것이다! 직관적으로 보면 velovity는 이전 gradients의 weighted sum이다. 더 최근의 gradients에 가중치가 더 크게 부여되고 계산되는 과정이 일좀의 smooth moving average라고 볼 수 있다. 시간이 지날수록 이전의 gradient들은 exponentially하게 감소한다.[그림3] 2) Nesterov momentumSGD momentum은 현재지점에서의 gradient를 계산한 뒤에 velocity와 곱해주었지만 Nesterov momentum은 계산 순서만 변형을 시켜 주었다고 보면 된다. 아래의 왼쪽 그림을 보면, 빨간 점에서 시작해서 우선은 Velocity 방향으로 움직인다. 그리고 그 지점에서의 gradient를 계산한 후 다시 원점으로 돌아가서 이 둘을 합치는 것이다. velocity의 방향이 잘못되었을 경우에 현재 gradient의 방햐을 좀 더 활용할 수 있도록 해준다. Nesterov는 Convex Optimization 문제에서는 뛰어난 성능을 보이지만 하지만 Neural network와 같은 Non-convex problem에서는 성능이 보장되는 않는다.[그림4]Nesterov의 첫번째 수식은 기존의 momentum과 동일하다. 아래 그림에서 재배열한 수식을 보면 기존과 동일하게 velocity와 계산한 gradient를 일정 비율로 섞어주는 역할을 한다. 그리고 두 번째 수식에서 마지막 부분을 보면 현재 점과 velocity를 더해주며, 현재 velocity - 이전 velocity를 계산해서 일정 비율(rho)을 곱하고 더해줍니다. 현재/이전의 velocity간의 에러 보정(error-correcting term)이 추가되었다.[그림5]이전의 velocity의 영향을 받기 때문에 momentum 방법들은 minima를 그냥 지나쳐 버리는 경향이 있다. 하지만 스스로 경로를 수정하고는 결국 minima에 수렴한다. 3) AdaGrad 학습 도중에 계산되는 gradient에 제곱을 해서 계속 더해준다. 가중치를 업데이트 할때 gradient로 나눠주는 작업을 수행한다. 한 차원은 항상 gradient가 높은 차원이고 다른 하나는 항상 작은 gradient를 가지는 2차원 좌표가 있다고 가정하자. small dimension에서는 gradient의 제곱 값 합이 작은데 이 작은 값으로 나워지므로 가속도가 붙게된다. Large dimension에서는 gradient가 큰 값 이므로 큰 값이 나워지게 되어 속도가 점점 줄어든다. 하지만 학습이 계속 진행될수록 학습 횟수가 늘어난다는 문제가 있다. 학습 횟수가 많아질수록 AdaGrad의 값은 점점 작아진다. 이러한 점은 Convex한 Loss인 경우에 좋은 특징이 될 수 있다. minimum에 근접하면 서서히 속도를 줄여서 수렴할 수 있게 해 줄 수 있기 때문이다. 하지만 saddle point problem과 같은 non-convex 문제에서는 AdaGrad가 멈춰 버리는 상황이 발생할 수 도 있어 문제가 있다. 일반적으로 NN을 학습시킬 때는 잘 사용하지 않는다.[그림6] 4) RMSProp 위와 같은 문제를 보완하기 위한 알고리즘이다. AdaGrad의 gradient 제곱 항을 그대로 사용한다. 점점 속도가 줄어드는 문제를 제곱항을 그저 누적시키는 것이 아니라 기존의 누적 값에 decay_rate를 곱해주는 방식을 통해 해결하였다. decay_rate는 보통 0.9 또는 0.99를 자주 사용한다. gradient 제곱을 계속 나눠준다는 점은 AdaGrad와 유사하다. 이를 통해 step의 속도를 가속/감속 시킬 수 있다. [그림7] 5) Adam momentum + RMSProp 으로 위에서 종합한 momentum계열의 알고리즘과 Ada계열의 알고리즘의 특징을 합한 것이다. 빨간색 부분은 gradient의 가중합이다. 파란색 부분은 AdaGrad나 RMSProp처럼 gradients의 제곱을 이용하는 방법이다. 초기 Step이 엄청 커져 버릴 수 있고 이로 인해 잘못될 수도 있다. 이런 문제를 해결하기 위해 보정하는 항을 추가한다.(bias correction term) Adam은 다양한 문제에도 정말 잘 동작한다. 하지만 예를들어 손실함수가 타원형이고 축 방향으로 정렬되어 있지 않고 기울어져 있다고 생각해 보자. 회전된 타원(poor conditioning) 문제는 Adam을 비롯한 다른 여러 알고리즘들도 다를 수 없는 문제이다.[그림8] learning rates decay 전략 처음에는 learning rate를 높게 설정한 다음에 학습이 진행될수록 learning rates를 점점 낮추는 것이다. 예를 들면, 100,000 iter에서 learning rates를 낮추고 학습시키는 것이다.(step decay) 또는 exponential decay 처럼 학습과정 동안에 꾸준히 learning rate를 낮출 수도 있다. learning rate가 너무 높아서 더 깊게 들어가지 못하는 상황에 learning rate를 낮추게 되면 속도가 줄어들며 지속해서 Loss를 내려갈 수 있을 것이다. learning rate decay는 부차적으로 생각해보는 것이지 학습 초기부터 고려하지는 않는다. 또한 Adam 보다는 SGD Momentum을 사용할 때 자주 사용한다.[그림9] 위에서 언급한 알고리즘들은 1차 미분값을 사용하여 1차 근사함수를 실제 손실함수라고 가정하고 가중치의 업데이트를 진행하였다. 이런 방식은 근사 시킨 값이므로 정확성이 떨어져 스텝의 사이즈를 키워 멀리 갈수가 없다. 2차 미분값을 활용하여 근사 시키는 방법을 통해 그러한 문제를 해결할 수 있을 것이다.[그림10][그림11] Second-Order Optimization 위의 2차원을 다차원으로 확장시켜보면 이를 Newton step이라고 한다. Hessian matrix의 inverse matrix를 이용하게 되면 실제 손실함수의 2차 근사를 이용해 곧바로 minima로 이동할 수 있다는 것이다. 다른 알고리즘들과 달리 learning rate를 사용하지 않는다. 실제로는 2차 근사도 완벽하지 않기에 learning rate가 필요하다. 어디까지나, minima로 이동하는게 아니라 minima의 방향으로 이동하기 때문이다. 허나, inverse matrix를 구하기 힘들고 메모리에 대량의 파라미터를 저장할 방법이 없기에 이 알고리즘은 NN에서 사용되지 않는다. 1) Quasi-Newton methods(BGFS most popular) Full Hessian을 그대로 사용하기 보다 근사시킨다. Low-rank approximations하는 방법이다. 2) L-BFGS Hessian을 근사시켜 사용하는 second-order optimizer이다. 사실상 DNN에서는 잘 사용하지 않는다. 왜냐하면 L-BFGS에서 2차 근사가 stochastic case에서 잘 동작하지는 않으며, Non-convex 문제에도 적합하지 않기 때문이다. 단지 full batch update가 가능하고 stochasticity가 적은 경우라면, L-BFGS가 좋은 선택이 될 수 있다. NN을 학습시키는데 많이 사용되지는 않지만 Style transfer 같은 stochasticity와 파라미터가 적은 경우에서 Optimization을 해야할 경우에 종종 사용할 수 있다. 위에서 살펴본 방법들은 학습과정의 error를 줄이기 위한 방법들이었다. 허나 우리가 진정으로 관심을 갖고 보아야 할 것은 test set의 error이다. 그런 test set의 error를 줄이기 위한 방안들을 다음에서 제시한다. Model Ensemble model을 train 시킨 후 우리가 가장 기대하는 바는 새롭게 들어온 test set에 대한 성능. 즉, test set에 대한 error가 작기를 기대한다. 그렇게 하는 가장 쉽고 빠른 방법이 바로 Model Ensemble이다. machine learning 분야에서 종종 사용하는 기법으로 예를 들어 설명하자면 모델을 하나만 학습시키지 말고 10개의 모델을 독립적으로 학습시키는 것이다. 결과는 10개 모델 결과의 평균을 이용한다. 모델의 수가 늘어날수록 Overfitting이 줄어들고 성능이 조금씩 향상된다. 보통 2%정도 증가한다. ImageNet이나 Kaggle competition 같이 모델의 성능을 최대화 시키는 것이 주된 목표일 경우 많이 사용한다. 허나 개인적으로나 주변의 조언들을 종합해보자면 우선 기본적인 base모델의 성능을 높이는데 주력하는 것이 더 좋을 듯하다. 실제로 실무에서는 Ensemble이나 Stacking같은 기법을 자주 사용하지는 않는다고 하기 때문이다. 하지만 한번 쯤은 만들어 보는 것도 좋은 것 같다. 또한 학습 도중 중간 모델들을 저장하고 앙상블로 사용할 수도 있다. 그리고 Test시에는 여러 중간 모델들을 통해 나온 예측값들을 평균을 내서 사용한다. 만약 모델간의 Loss 차이가 크면 한쪽이 Overfitting 일수 있고, 차이가 작아도 안좋은 것은 아닐까라는 생각에 의해 좋은 앙상블 결과를 위해서라면 모델 간의 최저그이 갭을 찾는 것이 중요하지 않는냐는 생각이 들 수도 있겠지만, 언제나 말하듯 우리에게 중요한 것은 validation set의 성능을 최대화시키는 것이다. 앙상블시에 다양한 모델 사이즈, learning rate, 그리고 다양한 regularization 기법 등을 앙상블 할 수 있다. 이런 모델을 독립적으로 학습시키는 방법외에도 학습하는 동안에 파라미터의 exponentially decaying average를 계속 계산한다. 이 방법은 학습중인 네트워크의 smooth ensemble 효과를 얻을 수 있다. 즉, checkpoints에서의 파라미터를 그대로 사용하지 않고 smoothly decaying average를 사용하는 방법이다.(Polyak averaging) Regularization 모델에 어떤 조건들을 추가할 텐데 그 term들은 모델이 Training data에 fit하는 것을 막아줄 것이다. 그리고 한번도 보지 못한 데이터에서의 성능을 향상시키는 방법이다. L2 regularization은 NN에는 잘 어울리지 않는다. 왜??? NN에서 가장 많이 사용하는 Regularization은 바로 dropout이다!!! Dropout Forward pass 과정에서 한 layer씩 출력(activation = previous activation * weight)을 전부 구한 후에 임의로 일부 뉴런을 0으로 만들어 주는데 매번 Forward pass마다 0이 되는 뉴런을 바꿔주어 특징들 간의 상호작용을 방지한다고 볼 수 있다. 즉, 네트워크가 어떤 일부 feature에만 의존하지 못하게 해준다. 다양한 feature를 골고루 이용할 수 있도록 하여 Overfitting을 방지한다고 볼 수 있다..보통은 0.5로 준다. [그림12] 단일 모델로 앙상블 효과를 가질 수 있다는 것이다. 서로 다른 파라미터를 공유하는 서브네트워크 앙상블을 동시에 학습시키는 것이라고도 생각할 수도 있다. 그러나, 뉴런의 수에 따라서 앙상블 가능한 서브네트워크의 수가 기하급수적으로 증가하기 때문에 가능한 모든 서브네트워크를 사용하는 것은 사실상 불가능하다. Dropout을 사용하면 Test time에는 어떤 일이 발생되나? Dropout을 사용하면 기본적으로 NN의 동작자체가 변하게 된다. 기존의 NN은 w와 x에 대한 함수였다. 그러나 Dropout을 사용한면 Network에 z라는 입력이 추가된다. z는 random dropout mask이다. test시에는(예측시) 임의의 값을 부여하는 것은 좋지 않다. 왜냐하면 예측할때마다 결과가 바뀔수 있기 때문이다. 이런 randomness를 average out시키는데 적분을 통해 marginalize out시키는 것으로 생각해볼 수 있다. 허나 실제로는 까다로운 문제이므로 z를 여러번 샘플링해서 예측시에 이를 average out시키는 것이다. 하지만 이 방법도 test time에서의 randomness을 만들어 내기 때문에 좋지 않은 방법이다. 허나 다음 그림과 같이 test time에서 stochasticity를 사용하지 않고 할 수 있는 값 싼 방법 중 하나는 dropout probability를 네트워크의 출력에 곱하여 test time과 train time의 기대값을 같게 해주는 것이다. [그림13] 실제로 코드에서는 아래와 같이 예측시에 dropout probability를 곱해주거나 tip으로 train에서는 연산이 GPU에 의해 계산되어 추가되는 것에 별로 신경쓰지 않지만 Test time에서는 효율적으로 동작하길 바라므로 train시에 오히려 역으로 dropout probability를 나누어주는 식으로 수행할 수 있다. [그림14] dropout을 사용하게 되면 Train time에서 gradient에는 어떤 일이 일어나는지 궁금할 것이다. 결론은 우리가 생각하던 Dropout이 0으로 만들지 않은 노드에서만 Backpropagation이 발생하게 된다. Dropout을 사용하게 되면 각 스텝마다 업데이트되는 파라미터의 수가 줄어들기 때문에 전체 학습시간은 늘어나지만 모델이 수렴한 후에는 더 좋은 일반화 능력을 얻을 수 있다. 기본적으로 Dropout은 일반적인 regularization전략을 구체화시킨 하나의 예시에 불과하다. 이 전략은 Train time에는 네트워크에 randomness를 추가해 네트워크를 마구잡이로 흩뜨려 놓으므로써 Training data에 너무 fit하지 않게 해준다. 그리고 Test time에서는 randomness를 평균화 시켜서 generalization 효과를 주는 것이다. Dropout이 Regularization에 가장 대표적인 예이긴 하지만 Batch normalization 또한 비슷한 동작을 할 수 있다. 왜냐하면 mini-batch로 하나의 데이터가 샘플링 될 때 매번 서로 다른 데이터들과 만나게 된다. Train time에서는 각 데이터에 대해서 이 데이터를 얼마나 어떻게 정규화시킬 것인지에 대한 stochasticity이 존재했다. 하지만 test time에서는 정규화를 mini-batch 단위가 아닌 global 단위로 수행함으로써 stochasticity를 평균화 시킨다. 이러한 특성 때문에 Batch-Normalization은 Dropout과 유사한 Regularization 효과를 얻을 수 있다. 실제로 Batch-Normalization을 사용할 때는 Dropout을 사용하지 않는다. Batch-Normalization에도 충분히 regularization 효과가 있기 때문이다.[그림15] Batch-Normalization과는 다르게 자유롭게 조절할 수 있는 파라미터 p가 있기 때문에 Batch-Normalization은 여전히 쓸모있다. data augmentation Regularization 패러다임에 부합하는 전략 중 하나이다. 예를 들어 고양이 사진을 classification 문제로 풀려고 할때, 이미지의 반전을 주어 입력한다던지 아니면, 임의의 다야한 사이즈로 잘라서(crop) 사용할 수 있다. 또한, color jittering도 있는데 간단한 방법으로는 학습시 이미지의 contrast 또는 brightness를 바꿔준다. 복잡한 방법으로는 PCA의 방향을 고려하여 color offset을 조절하는 방법이다. 이런 방법은 color jittering을 좀 더 data-dependent한 방법으로 진행하는 것으로 자주 사용하는 방법은 아니다. data augmentation은 어떤 문제에도 적용해 볼 수 있는 아주 일반적인 방법이라고 할 수 있다. 어떤 문제를 풀려고 할 때, 이미지의 label을 바꾸지 않으면서 이미지를 변환시킬 수 있는 많은 방법들을 생각해 볼 수 있다.train time에 입력 데이터에 임의의 변환을 시켜주게 되면 일종의 regularization 효과를 얻을 수 있다. 그 이유는 위에서 언급하고 강조한 것과 같이 train time에는 stochasticity가 추가되고 test time에는 marginalize out 되기 때문이다. DropConnect Dropout과 다르게 activation이 아닌 weight matrix를 임의적으로 0으로 만들어주는 방법이다. fractional max pooling 보통의 경우, 2x2 max pooling 연산은 고정된 2x2 지역에서만 수행하지만 fractional max pooling에서는 그렇게 하지 않고 pooling 연산을 수행할 지역이 임의로 선정된다. 예를 들면 아래의 그림에서 Train time에 샘플링 될 수 있는 임의의 pooling region을 볼 수 있다. 그리고 test time에 stochasticity를 average out 시키려면 pooling regions를 고정시켜 버리거나 혹은 여러개의 pooling regions를 만들고 averaging over를 시킨다. 많이 사용하지는 않지만 좋은 방법이다.[그림16] Stochastic Depth Train time에는 layer 중 일부를 제거해 버리고 일부만 사용해서 학습한다. Test time에는 전체 네트워크를 다 사용한다. 최신의 연구이며 실제로 잘 사용하진 않지만 아주 좋은 아이디어이다. 보통 하나 이상의 regularization 방법을 사용하는데 그 중에서도 Batch-Normalization만으로도 충분하다. 다만 Overfitting이 발생한다 싶으면 Dropout과 같은 다양한 방법을 추가해 볼 수 있다. 이를 가지고 blind cross-validation을 수행하지는 않는다. 대신에 네트워크에 overfit의 조짐이 보일때 하나씩 추가시켜 본다. Transfer Learning overfitting이 일어날 수 있는 상황 중 하나는 충분한 데이터가 없을 때이다. 우리는 이런 상황에서 Transfer learning을 사용하여 문제를 해결 할 수 있다. 또한, 흔히들 CNN 학습에는 엄청많은 데이터가 필요하다고 생각할 수 있는데 그런한 사고를 무너뜨려 버려준다. 1) 먼저 ImageNet 같은 아주 큰 데이터셋으로 학습을 한번 시킨다.ImageNet에서 학습된 feature를 우리가 가진 작은 데이터셋에 적용하는 것이다.이제는 1000개의 ImageNet 카테고리를 분류하는 것이 아니라 10종의 강아지를 분류하는 문제이다. 데이터는 엄청 적다. 이 데이터 셋은 오직 C(예:10개)개의 클래스만 가지고 있다. 2) 가장 마지막의 Fully connected layer는 최종 feature와 class scores간의 연결인데 이를 초기화시킨다. 또한, 기존의 ImageNet은 4,096 x 1,000 차원의 matrix였지만 이제는 우리가 10개의 클래스를 갖으므로 4,096 x 10 차원의 matrix로 바꿔준다. 나머지 이전의 모든 레이어드르이 가중치는 그대로 둔다. 이렇게 되면 linear classifier를 학습시키는 것과 같다. 왜냐하면 오로지 마지막 레이어만 가지고 우리 데이터를 학습시키는 것이기 때문이다. 이러한 방법을 사용하면 아주 작은 데이터 셋일지라도 아주 잘 동작하는 모델을 만들 수 있다. 만일, 데이터가 조금 더 있다면 전체 네트워크를 fine-tuning 할 수 있다. 최종 레이어들을 학습시키고 나면, 네트워크의 일부만이 아닌 네트워크 전체의 학습을 고려해 볼 수도 있을 것이다 데이터가 더 많이 있다면 네트워크의 더 많은 부분을 업데이트 시킬 수 있을지도 모른다. 이 부분에서는 보통 기존의 Learning rate보다는 낮춰서 학습시킨다. 왜냐하면 기존의 가중치들이 이미 ImageNet으로 잘 학습되어 있고 이 가중치들이 대게는 아주 잘 동작하기 때문이다. 우리가 가진 데이터셋에서의 성능을 높히기 위해서라면 그 가중치들을 아주 조금씩만 수저정하면 될 것이다. transfer learning은 거의 일상적인 수준이 되었다. 대부분은 ImageNet pretrained model을 사용하고 현재 본인의 task에 맞도록 fine tuning한다. captioning의 경우 word vectors를 pretrain하기도 한다. pretrained CNN 뿐만 아니라 큰 규모의 코퍼스로 부터 학습된 pretrained word vectors도 함께 이용할 수 있다. 허나, captioning task에서는 pretrained word vectors을 사용하는 경우가 많지 않고 크게 중요하지 않다. 문제에 대한 데이터셋이 크지 않은 경우라면 풀려는 문제와 유사한 데이터셋으로 학습된 pretrained model을 다운로드 받아라. 그리고 이 모델의 일부를 초기\b화시키고 가지고있는 데이터로 모델을 fine-tuning한다. TensorFlow : https://github.com/tensorflow/modelsPytorch : https://github.com/pytorch/vision","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"[CS231n]Lecture06-Training Neural Networks","slug":"cs231n_06","date":"2019-07-19T14:15:00.000Z","updated":"2019-07-23T04:18:10.010Z","comments":true,"path":"2019/07/19/cs231n_06/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/19/cs231n_06/","excerpt":"","text":"Optimization을 통해서 네트워크의 파라미터를 학습시킬 수 있다. Loss가 줄어드는 방향으로 이동하는데 이것은 gradient의 반대 방향으로 이동하는 것과 같다. Mini-batch SGD 알고리즘으로 가중치들(네트워크의 파라미터)을 업데이트하는 과정은 다음과 같다. Mini-batch SGD로 가중치 업데이트 과정Loop: 데이터의 batch size를 정한 후 그만큼의 크기의 데이터만 가져온다. Forward prop을 통해 Loss값을 산출한다. gradient를 계산하기 위해서 Backpropagation을 시행한다. 계산하여 얻은 gradient를 이용해서 파라미터를 업데이트 한다. Training Neural NetworksNeural Networks의 학습을 처음 시작할때 필요한 기본설정에 대해 말하자면 다음과 같다. activation function 선택, preprocessing, weight initialization, regularization, gradient checking Activation function1)Sigmoid $\\sigma(x) = \\frac{1}{1+e^{-x}}$ 각 입력을 받아서 그 입력을 [0,1]사이의 값이 되도록 해준다. 입력의 값이 크면 Sigmoid의 출력값이 1에 가까울 것이고 작으면 0에 가까울 것이다. neuron의 firing rate를 saturation 시키는 것으로 해석할 수 있다. 왜냐하면 어떤 값이 0에서 1사이의 값을 가지면 이를 fireing rate라고 생각할 수 있기 때문이다. ReLu가 생물학적 타당성이 더 크기에 최근에는 대부분 ReLu를 사용한다.(이 부분은 생물학적인 내용인것 같다.) 참조 firing rate 문제점 Saturation되는것이 gradient를 없앤다. Sigmoid에서 x가 0이면 잘 작동할 것이고, gradient도 잘 얻게 될 것이다. 허나, 음의 큰 값이거나 양의 큰값이면 sigmoid가 flat하게 되고 gradient가 0이 될 것이다. 거의 0에 가까운 값이 backprob이 되는 것이다. 그렇다면 결국 출력 노드에서는 가중치의 업데이트 의미가 있는 것이 되겠지만 밑으로 계속 Backpropagation 과정을 하다보면 0이 계속 전달되게 되므로 의미가 없어진다. sigmoid의 출력이 zero centered 하지 않다는 것이다. sigmoid의 입력이 항상 양수라고 가정해보자. 그런 layer에서 dL/df를 계산하면서 local gradient를 생각해보자. local gradient는 전부 양수가 되거나 전부 음수가 된다. 결론적으로는 gradient의 부호가 계속 동일하게 되기 때문에 가중치가 모두 같은 방향으로만 움직일 것임을 의미한다. 파라미터를 업데이트 할 때 다 같이 증가하거나 다같이 감소하거나 할 수 밖에 없다. 이런 gradient 업데이트는 아주 비효율적이다. 위에서 전부 양수 또는 음수로 업데이트된다는 것을 해석해보면, gradient가 이동할 수 있는 방향은 4분면 중 두 영역만 해당 될 것이다. 이러한 이유가 zero-mean data를 우리가 만들어주는 이유이다. 입력 X가 양수/음수를 모두 가지고 있으면 전부 같은 방향으로 움직이는 일은 발생하지 않을 것이기 때문이다. exponential로 인해 계산비용이 크다는 것이다.(내적의 계산이 비싸다.) 2)tanh sigmoid와는 비슷한 모양을 가지지만 출력값의 범위가 [-1,1]이다. 가장 큰 차이라면 zero-centered라는 것이다. 이를 통해 sigmoid의 두번째 문제는 해결되지만, saturation 때문에 여전히 Gradient는 죽는다. 여전히 flat한 구간이 있기 때문이다.-3)ReLU f(x) = max(0,x) 적어도 양의 값에서는 saturetion되지 않는다. sigmoid와는 다르게 exponential같은 연산이 없는 단순한 max 연산이므로 계산이 매우 빠르다.(sigmoid나 tanh보다 수렴속도가 6배정도로 훨씬 빠르다.) 생물학적 타당성도 ReLU가 sigmoid보다 크다. ImageNet 2012에서 우승한 AlexNet이 처음 ReLU를 사용하기 시작했다. 문제점 더 이상 zero-centered가 아니라는 점이다. tanh가 이 문제는 해결했는데 ReLU는 다시 이 문제를 가지고 있게 된다. 양의 수에서는 saturation이 되지 않지만 음의 경우에서는 그렇지 않다. Dead ReLU가 발생되는 문제의 이유 initialization을 잘못한 경우 learning rate가 지나치게 높은 경우 4)Leaky ReLU f(x) = max(0.01x, x) ReLU와 유사하지만 음수 영역에서 더이상 0이 아니며, 여전히 계산이 효율적이어서 sigmoid나 tanh보다 수렴을 빨리 할 수 있다. Dead ReLU가 없다! 5)Parametric ReLU f(x) = max(alpha*x, x) Leaky ReLU와 유사하게 음의 영역에서도 기울기를 가지고 있다는 것을 확인할 수 있다. 다만 alpha라는 파라미터를 정해 놓는 것이 아니라, backprob으로 학습시키는 점이 다르다. 6)Exponential Linear Units(ELU) ReLU의 장점을 그대로 가져오는데, 추가적으로 zero-mean에 가까운 출력을 보여준다. zero-mean에 가까운 출력은 앞서 Leaky ReLU, PReLU가 가진 이점이있다. 하지만 Leaky ReLU와 비교해보면 ELU는 negative에서 기울기를 가지는 것 대신 또 다시 saturation이 된다.ELU가 주장하는건 이런 saturation이 좀더 noise에 강인할 수 있다는 것이다. ReLU와 Leaky ReLU의 중간 정도이다. Leaky ReLU처럼 zero-mean의 출력을 내지만 Saturation의 관점에서 ReLU의 특성도 가지고 있다. 6)Maxout “Neuron” maxout는 ReLU와 leaky ReLU의 좀 더 일반화된 형태이다. 선형함수이기 때문에 saturation되지 않으며 gradient가 죽지 않을 것이다. 문제점은 뉴런당 파라미터의 수가 두배가 된다는 것이다. w1과 w2를 지니고 있어야하기 때문이다. 실제로 가장 많이들 쓰는 것은 바로 ReLU이다. 다만 ReLU를 사용하려면 learning rate를 아주 조심스럽게 결정해야 할 것이다. Leaky ReLU, Maxout, ELU와 같은 것들도 써볼 수 있지만 아직 실험단계이긴 하다. 여러분들의 문제에 맞춰 어떤 활성함수가 잘 동작하는지 확인해 볼 수 있을 것이다. tanh도 써볼 수 있지만, 대게는 ReLU, ReLU의 변종들이 좀 더 잘 동작한다고 생각하면 된다. Data Preprocessing가장 대표적인 전처리 과정은 zero-mean으로 만들고 normalize하는 것이다. 이런 과정은 왜 거치는 것일까? zero-centering은 가중치의 업데이트시에 좀 더 다양한 방향의 gradient를 얻기 위함이며, normalization을 해주는 이유는 모든 차원이 동일한 범위한에 있게 해줘 동등한 기여를 하게 하기 위해서이다. 이미지의 경우 실제로는 전처리로 zero-centering 정도만 해준다. 왜냐하면 이미지는 이미 각 차원간에 스케일이 어느정도 맞춰져 있기 때문에 normalization이 필요치 않기 때문이다. 다른 PCA나 whitening같은 더 복잡한 전처리 과정도 잘 사용치 않는다. -AlexNet은 전체 이미지의 평균을 빼주는 방식으로 zero-centering을 해주지만 VGGNet은 각 채널별 평균을 빼주는 방식으로 진행한다. 이런 방식의 결저을 각자의 판단에 맡겨진다. weight Initialization모든 가중치를 0으로 세팅한다면 어떻게 될까? 가중치가 0이라서 모든 뉴런은 모두 다 같은 연산을 수행한다. 출력도 같을 것이고, 결국 gradient도 서로 같을 것이다. 모든 가중치가 똑같은 값으로 업데이트 된다. 이것이 모든 가중치를 동일하게 초기화시키면 발생하는 일이다. 위와 같은 문제들을 해결하기 위해 다음과 같은 방법들을 제시한다. 표준 정규 가우시안 분포에서 임의의 작은 값으로 초기화한다. 이 방법은 깊은 네트워크에서 문제가 생길 수 있다. 왜냐하면 가중치를 업데이트 하는데 있어서 가중치의 값이 작기 때문에 가중치가 0으로 수렴되는 상황이 발생될 수 있기 때문이다. 적절한 가중치를 얻는 것은 너무 어렵다. 너무 작으면 사라져버리고 너무 크면 saturation(가중치의 gradient가 0)이 되어버린다. 1)Xavier Initialization-Standard gaussian으로 뽑은 값을 입력의 수로 스케일링해준다. 기본적으로 Xavier initialization가 하는 일은 입/출력의 분산을 맞춰주는 것이다. 입력의 수가 작으면 더 작은 값으로 나누고 좀 더 큰 값을 얻는다. 더 큰 가중치를 얻는 이유는 작은 입력의 수와 가중치가 곱해지기 때문에 가중치가 커야만 출력의 분산 만큼 큰 값을 얻을수 있기 때문이다. 반대로 입력의 수가 많은 경우에는 더 작은 가중치가 필요하다. 각 레이어의 입력이 Unit gaussian이길 원한다면 이런 류의 초기화 기법을 사용해 볼 수있다. 이런 결과는 Linear activation이 있다고 가정하는 것이다. tanh의 경우를 예를 들면 active region안에 있다고 가정하는 것이다. 하지만 ReLU를 쓰면 잘 작동하지 않는다. ReLU는 출력의 절반이 0이되어 죽는다. 결국 출력의 분산을 반토막 내버린다. 그러므로 값이 너무 작아지는 것이다. 이런 문제를 해결하기 위해 추가적으로 2로 나눠주는 즉 입력의 반밖에 들어가지 않는 점을 추가해주는 방법을 사용하기도 한다. 2)Batch Normalizationgaussian의 범위로 activation을 유지시키는 것에 관련한 아이디어 중 하나이다. 우리는 layer로 부터 나온 activation의 값들이 Unit gaussian이기를 바란다. 가중치를 잘 초기화 시키는 것 대신에 학습 할 때 마다 각 레이어에 이런 일을 해줘서 모든 layer가 Unit gaussian이 되도록 해준다. Batch size 만큼의 데이터에서 각 피처별로 평균과 분산을 계산한후 Normalization을 해준다. 이러한 연산은 Fully connected layer나 Conv Layer직후에 넣어준다. 깊은 네트워크에서 각 layer의 가중치가 지속적으로 곱해져서 Bad scaling effect가 발생했지만, Normalization은 그런 Bad effect를 상쇄시켜 버린다. Batch Normalization은 입력의 스케일만 살짝 조정해 주는 역할이기 때문에 Fully connected와 Conv layer 어디에든 적용가능하다. Conv layer에서 차이점이 있다면 Normalization을 차원마다 독립적으로 수행하는 것이 아니라 같은 Activation Map의 같은 채널에 있는 요소들은 같이 Normalize해 준다. 왜냐하면 Conv 특성상 같은 방식으로 normalize 시켜야 하기 때문이다. 즉, Conv Layer의 경우 Activation map(채널, Depth)마다 평균과 분산을 하나만 구한다. 그리고 현재 Batch에 있는 모든 데이터로 Normalize를 해준다. 이처럼 layer의 입력이 unit gaussian이 되도록 강제하는 것이다. tanh를 예시로 생각해 보면 입력데이터가 tanh로 인해 얼마나 saturation 될지를 조절하고 싶은 경우 같이 유연성을 줄 수도 있다. 학습가능한 감마와 베타를 통해 normalization 단계 이전으로 다시 복원 시켜주는 것과 같은 작업을 통해 유연성을 갖게할 수 있다. 때문에 batch 단위로 normalization을 일단 해주고 파라미터를 다시 학습시키는 것이다. 또한 Batch Normalization은 regularization의 역할도 한다. 각 layer의 출력은 해당 데이터 하나 뿐만 아니라 batch 안에 존재하는 모든 데이터들에 영향을 받는다. 더 이상 layer의 출력은 deterministic하지 않고 조금씩 바뀌게 되고 이는 regularization effect를 준다. 우리가 입력을 강제로 gaussian 분포로 만들어 버리게 되면 기존의 구조를 잃는 것은 아닌지에 대한 의문이 생길 수 도 있다. 특히 CNN의 경우 입력의 공간적 특성을 유지시켜 계산하는 방법이므로 더 강한 의문이 들수도 있겠지만, 예를 들어 설명하자면 데이터의 전처리를 할 때도 gaussian을 사용하는데 그럴 경우도 모든 피처들을 가우시안 분포로 만든다고 해도 어떠한 구조도 잃어버리지 않는다. 단지, 데이터에 연산이 잘 수행되도록 선형변환(스케일링, 시프트)을 해주는 것이다. 또한, shift와 scale요소를 추가시켜 학습을 시켜버리면 결국 identity mapping이 되서 Batch normalization이 사라지는 것이 아닌지의문이 들 수 있다. 실제로 감마와 베타를 학습시키게 되면 identity가 되지는 않는다. sift와 scale이 일정량 변하긴 하지만 보통은 identity mapping이 될 정도는 아니다. 그러므로 여전히 batch normalization의 효과를 얻을 수 있다. Batch Noramalization에서 평균과 분산은 학습데이터에서 구한것이며, test시에는 추가적인 계산은 하지않는다. 하이퍼 파라미터 최적화 시에는 Log scale로 값을 주는 것이 좋다. 파라미터 값을 샘플링할때 10^-3 ~10^-6을 샘플링하지 말고 10의 차수 값만 샘플링하는 것이 좋다. 최적의 값이 내가 정한 범위의 중앙 쯤에 위치하도록 범위를 잘 설정해 주는 것이 중요하다. 실제로는 grid search보다는 random search를 하는 것이 더 좋다. 왜냐하면 실제로는 어떤 파라미터가 더 중요할 수도 있는데 그러한 important variable에서 더 다양한 값을 샘플링 할 수 있기 때문이다.","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"[CS231n]Lecture05-Convolution Neural Network","slug":"cs231n_05","date":"2019-07-18T18:00:00.000Z","updated":"2019-12-13T03:43:27.468Z","comments":true,"path":"2019/07/19/cs231n_05/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/19/cs231n_05/","excerpt":"","text":"CNN의 역사는 생략하겠다. CNN의 기본적인 구조기존의 Fully connected Layer와 CNN의 주된 차이점은 기존의 이미지 구조를 보존시킨다는 점이다. 그리고 필터가 가중치 역할을 하는 것이라고 생각하면 될 것이다. 간단히 표현하자면, 필터를 통해 이미지를 슬라이딩하면서 공간적으로 내적을 수행하는 방식이 CNN 구조이다. 더 자세히 말하자면, 우선 필터의 크기는 입력의 크기보다는 작지만 필터의 깊이는 항상 입력의 깊이 만큼 항상 확장되어야 한다. 그러한 필터를 전체 이미지의 일부 공간에 겹쳐놓고 내적을 수행한다. 필터의 W(가중치)와 이에 상응하는 위치에 있는 입력 이미지의 픽셀을 곱해준다. 이러한 내적을 할 때는 fully connected layer의 연산과 동일하게 필터 크기에 상응하는 입력 데이터의 텐서를 하나의 긴 벡터로 보고 벡터 끼리의 내적으로 생각하면 된다. 왜냐하면 각 원소끼리 Convolution을 하는 것과 텐서를 쭉 펴서 내적을 하는 것은 동일한 작업의 결과물을 보여주기 때문이다. 신호처리 분야에서의 convolution은 실제로 필터를 뒤집은 다음에 연산을 수행한다는 내용이 있었는데 이 부분은 아직 정확한 의미를 알지 못해 좀 더 찾아보며 공부한 후 추후에 다시 내용을 업데이트 해야할 것 같다. 이제부터는 CNN에서의 필터가 작동하는 방식을 말해 보면, Convolution은 이미지의 좌상단부터 시작하게 된다. 그리고 필터와 입력 이미지 데이터의 내적으로인해 각 해당 위치에 activation map을 산출하게 된다. 예를 들어 2칸씩 띄어서 필터를 슬라이딩 한다던지 출력 activation map 행렬의 크기는 필터를 어떻게 슬라이드를 하느냐에 따라 다를 것이다. 기본적으로는 하나씩 연산을 수행한다. 보통 Convolution Layer에서는 여러개의 필터를 사용하는데 그 이유는 필터마다 다른 특징을 추출하고 싶기 때문이다. activation map의 갯수는 사용하는 필터갯수에 따라 달라진다. CNN(ConvNet)layer들 사이에 보통의 NN 처럼 activation function, Convolution, pooling(뒤에 더 자세한 설명을 하겠지만 pooling은 activation map의 사이즈를 줄이는 역할을 한다.) 같은 처리를 한 후 이러한 layer 구조를 여러번 쌓아준다. 이러한 여러 Layer들을 쌓고나서 보면 결국 각 필터들이 계층적으로 학습을 하는것을 보게된다. 보통은 CNN 전체의 구조에서 앞쪽에 있는 필터들은 low-level feature(Edge와 같은)를 학습하게 되고, 중간은 Mid-level feature을 가지게 되어 cornor나 blobs등과 같은 특징들이 보인다. 뒤쪽으로 갈수록 high-level feature(좀 더 객체와 닮은 것들이 출력으로 나오는)를 학습하게 된다. 이렇게 필터에 따라 특징을 추출하는 과정이 중요하기에 필터의 Depth를 늘리는 데 어떤 직관을 가져야하는지 의문이 들수도 있을 것이다. 이는 어떻게 모델을 디자인해야 되는지의 문제이므로 실제로는 어떤것이 더 좋은지를 찾아내야한다. Conv Layer를 계층적으로 쌓아서 단순한 특징을 뽑고 그서을 또 조합해서 더 복잡한 특징으로 활용했다. 이는 강제로 학습시킨 것이 아니라 계층적 구조를 설계하고 역전파로 학습시킨 것 뿐이지만 필터는 이렇게 학습되는 것이다. 우리가 각 필터를 통해 얻은 activation map을 시각화하면 이미지가 어떻게 생겨야 해당 뉴런의 활성을 최대화시킬 수 있는지는 나타내는 것이다. 즉, 이미지가 필터와 비슷하게 생겼으면 출력값이 커지게 된다! 그러므로 우리는 이런 시각화를 통해 어떤 필터에 대한 activation map인지를 역으로 추론해 볼 수 있을 것이다. 마치 이미지 중 어느 위치에서 필터가 크게 반응하는 지를 볼 수 있기 때문이다.시각화는 Backpropagation을 통해 해볼 수 있다. 보통 필터의 갯수는 2의 제곱수로 한다. ex) 32, 64, 128, 512등 Spatial dimension먼저, 한가지 주의할 점은 우리의 입력데이터를 잘 커버할 수 없는 필터 사이즈로 CNN 구조를 설계하게 되면, 그로인한 불균형한 결과를 볼 수도 있기 때문에 잘 동작하지 않을 수 있다는 문제가 있다. 그러므로 필터의 사이즈를 정할떄는 입력데이터를 커버할 수 있는 사이즈로 정해주어야 할 것이다. 우리가 필터를 슬라이딩한 후의 activation map의 사이즈(가로,세로 크기)는 (원래 image dimension - 필터 dimension)/stride + 1 이를 이용해서 어떤 필터 크기를 사용해야하는 지를 알 수가 있다. 어떤 스트라이드를 사용했을 때 이미지에 꼭 맞는지, 그리고 몇 개의 출력값을 낼 수 있는지도 알 수 있다. Padding가장 흔히 쓰는 기법은 zero-pad이다. 입력 image data의 코너 즉 가장자리는 필터를 적용할 때 필터의 중심보다는 덜 영향을 주진 않을지의 의문을 가질 수도 있을 것이다. 그러한 의문을 해소하기 위해 zero padding을 사용하는 것이다. 다음 그림처럼, 가장자리에 0을 채워 덫붙여 넣는 것이다. 그렇게하면 좌상단의 자리에서도 필터 연산을 수행할 수 있게 된다. 혹시 Padding을 모르는 사람들을 위해 말을 하자면, 예를들어 image data의 가장자리에 픽셀을 덫붙여주는 작업이다. zero padding을 하면 모서리에 필요없는 특징을 추가하게 되는 것은 아닌지라는 의문을 갖을 수 있는데, 우리의 본 목적은 이미지나 영상 내에서 어떤 edge 부분의 위치(값)을 얻고 싶은 것이고, zero-padding은 이를 할 수 있는 하나의 방법일 뿐이라는 것을 명심하자! 왜냐하면 우리는 지금 필터가 닿지 않는 모서리 부분에서도 값을 뽑을 수 있게 되기 때문이다. (zero가 아닌 mirror나 extend하는 방법도 있다!! 허나, zero-padding 제법 잘 동작하는 방법 중 하나이다.) 물론 모서리 부분에 약간의 artifact가 생길 순 있다. 당연히 고려해야 하는 부분이다. zero-padding을 하는 또 다른 이유는 Layer를 거치면서도 입력의 사이즈를 유지하기 위해서이다. padding을 해줌으로서 컬럼이 두개가 더 생성되어 결국에는 입력받은 데이터의 크기와 동일한 출력 크기를 얻을 수 있다는 점을 앞서 배운 공식에 적용시켜 생각해보면 이해가 갈 것이다. 즉, 요약하면 Padding을 하게되면 출력 사이즈를 유지시켜주고 필터의 중앙이 닿지 않는 곳도 연산할 수 있다는 것을 알 수 있다. 다음은 내가 강의를 들으면서 접했던 질문 중 나 또한 궁금했던 질문이다. 만약 이미지가 square matrix가 아니고 rectangular matrix라면 수평, 수직방향의 stride를 다르게 적용해야 하는지 이다. 이런 질문은 나왁 같은 입문자들에게는 궁금할 만한 질문이라고 생각한다. 담은 물론 가능하지만, 보통은 square matrix로 이미지를 처리한후 사용하여 같은 stride를 적용한다는 것이다. 이미지 해상도 문제와도 관련이 있다. 아마 우리는 입력 이미지 원본 상태의 비율을 유지하고 싶겠지만 대부분은 정사각형으로 잘라서 사용한다는 것이다. 일반적으로 어떤 stride와 filter를 쓸건지를 정하는 방법이 있다. filter size stride 3x3 1 5x5 2 7x7 3 만약 Layer가 여러겹 쌓인 구조를 설계한다고 가정했을때, zero-padding을 하지 않는 다면 출력 사이즈는 아주 빠르게 줄어 들게 될 것이다. 그 의미는 결국 일부 정보를 잃게 되는 것이고 원본 이미지를 표현하기에 너무 작은 값을 사용하게 될 것이다. 또한, 그 이유는 매번 필터를 적용하여 convolution을 할 때 각 코너에 있는 값들을 계산하지 못하기 때문이다. 예를 들어 보자. input volume이 32x32x3이고, 10개의 5x5 필터들을 stride 1, padding을 2로 주었을 경우, output volume size는 어떻게 될 것인가? 정답은 32x32x10이다.(32+(2x2)-5/1)+1=32이고 필터의 갯수가 10이므로) 그렇다면 이 Layer의 parameter는 총 몇개 일까? 정답은 5x5x3=75개라는 가중치와 bias인 1개를 더한 75+1=76개를 각 필터당 가지므로 총 10개의 필터가 있기에 76*10=760개이다. 1x1 Convolution도 의미가 있다. 필터의 크기가 1초과인 다른 필터들 처럼 공간적인 정보를 이용하진 않지만 여전히 Depth만큼 연산을 수행한다. 그러니 1x1 ConvNet는 입력의 전체 Depth에 대한 내적을 수행하는 것과 같다. &#39;stride를 선택하는 데 있어 가질 수 있는 직관은 무엇인가&#39;라는 의문에 stride를 크게 가져갈수록 출력은 점점 작아질 것이라는 것이다. 즉, 이미지를 다운샘플링하는 것인데 Pooling을 하는 것과 비슷하다. 엄밀히 말하자면 둘은 다른 얘기이며, Pooling 보다 더 좋은 성능을 보이기도 한다. Pooling 처럼 다운 샘플링하는 동일한 효과를 얻으면서도 더 좋은 성능을 낼 수도 있다. 그리고 activation map의 사이즈를 줄이는 것은 추후 모델의 전체 파라미터의 갯수에도 영향을 미친다. 왜냐하면 출력 노드 전 마지막 단계에서 Fully connected Layer를 보게 되면 Conv의 출력 모두와 연결되어 있음을 알 수 있다. 즉, Conv Layer의 출력이 작을수록 Fully connected Layer에서 필요한 파라미터의 수가 더 작을 것이라는 점이다. 파라미터의 수, 모델의 사이즈, 그리고 Overfitting 과 같은 것들에는 다양한 trade-off가 있다. 이러한 trade-off는 stride를 몇 으로 할지를 결정할때 고려해야 하는 문제이다. Conv Layer를 Brain Neuron의 관점에서 살펴보자. 뉴련과 Conv Layer의 가장 큰 차이점은 우리의 뉴런은 Local connectivity를 가지고 있다는 점이다. Conv Layer처럼 슬라이딩을 하는 것이 아니라 특정 부분에만 연결되어 있다. 하나의 뉴런은 한 부분만 처리하고, 그런 뉴런들이 모여서 전체 이미지를 처리하는 것이다. 이런 식으로 spatial structure를 유지한 채로 Layer의 출력인 activation map을 만드는 것이다. 또한, 필터는 이미지에서 같은 지역을 돌더라도 서로 다른 특징을 뽑아낸다고 볼 수 있다. Pooling layerPooling Layer는 Representation들을 더 작고 관리하기 쉽게 해준다. 즉 파라미터의 수를 줄인다는 것이다.그리고 일종의 공간적인 불변성을 얻을 수 도 있다. 결국 Pooling Layer를 관리하는 일은 Downsampling하는 것이다. 여기서 중요한 점은 Depth에는 아무런 영향을 주지 않는다는 것이다. 그리고 일반적으로 Max pooling이 사용된다. Pooling에도 필터 크기를 정할 수 있다. 얼마만큼의 영역을 한 번에 묶을지를 정하는 것이다. 기본적으로 Downsampling을 하고 싶은 것이기 때문에 Pooling을 할 때는 겹치지 않는 것이 일반적이다. 또한, 우리가 다루는 값들은, 얼마나 이 뉴런이 활성화되었는 지를 나타내는 값이다. 즉, 이 필터가 각 위치에서 얼마나 활성화되었는지 이다. Max pooling은 그 지역이 어디든, 어떤 신호에 대해 “얼마나” 그 필터가 활성화 되었는지를 알려준다고 알 수 있다. \b인식에 대해 생각해 보았을때 인식은 그 값이 어디에 있었다는 것 보다는 그 값이 얼마나 큰지가 중요한 것이다. 그러므로 average pooling 보다 더 좋다. Pooling도 일종의 stride 기법이라고 볼 수 있다. 사람들은 Downsampling을 할때 Pooling을 하기보단 stride를 많이 사용하고 있는 추세이고 성능도 좋다. Pooling Layer도 Conv Layer에서 사용했던 수식을 그대로 이용해서 Design choice를 할 수 있다.한가지 특징이 있다면 pooling layer에서는 보통 padding을 하지 않는다. padding의 목적은 주로 사이즈를 유지하기 위한 것이지만 pooling의 목적은 down sampling이고 Conv layer에서 처럼 코너의 값을 계산하지 못하는 경우도 없기 때문이다. filter size stride 2x2 2 3x3 2","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"[CS231n]Lecture02-Image classification pipeline","slug":"cs231n_02","date":"2019-07-18T07:00:00.000Z","updated":"2019-12-08T05:34:11.115Z","comments":true,"path":"2019/07/18/cs231n_02/","link":"","permalink":"https://heung-bae-lee.github.io/2019/07/18/cs231n_02/","excerpt":"","text":"기본적으로 Computer vision에서 가장 핵심이 될 수 있는 작업은 Image Classification이라고 할 수 있을 것이다. Image classification이 가능하면 detection, segmentation, captioning 작업들이 수월하게 작업이 가능해진다. 이미지는 기본적으로 3차원의 배열 형태로 [0, 255] 사이의 수로 이루어져 있다. 3차원은 기본적으로 이미지가 컬러인 경우이며, 각각의 차원은 RGB채널로 불린다. 이미지는 카메라 앵글이라던지 보는 시각에 따라 달리 보이며, 밝기도 다를 수 있으며, 형태의 변형, 그리고 은폐, 은닉 등등 여러 문제들이 존재한다. 그렇다면, 우리는 이런 문제들을 가진 상태에서 classification을 하는데 있어서 숫자를 정렬하는 알고리즘과 같은 명백한 알고리즘이 존재하지 않는다는 것이 문제점으로 떠오를 것이다. 물론 이미지를 보고 이미지의 edge, chunk를 찾아서 library화 하고 이미지가 어떻게 배열되어 있는지와 같은 feature들을 가지고 들어온 이미지를 classification하는 방식으로 진행해왔지만, 이 정도만으로는 역부족이었다. 이제는, 데이터 기반의 접근방식을 사용한다. - 1) Labeling 되어있는 data를 준비한다. - 2) Image classifer를 훈련시키기 위해 Machine Learning을 사용한다. - 3) test image set을 활용해서 classifier를 평가한다. 먼저 지금은 잘 사용되지 않는 Nearest Neighbor Classifier를 설명해 볼 것이다.방법은 간단하다. 먼저 모든 train data와 각 image data의 label을 읽어 메모리상에 기억하게 해놓는다.그런 후에 가장 비슷한 train image의 label으로 예측하게끔 하는 방법이다. 위에서 말한 가장 비슷한 image라는 것은 소위 생각해보면 데칼코마니 같이 서로 겹쳐 놓았을때 같게 되면 두 image는 동일한 image라고 생각하기 때문에 각각의 동일한 위치의 픽셀값을 빼 절대값의 합을 취하는 L1 norm을 사용하는 방법과 ConvNet(CNN)은 data가 엄청 많아야 학습 시킬 수 있다라는 생각은 잘못된 생각일 수 있다. 왜냐하면 우리 모델을 처음부터 학습시키는 것이 아닌 학습되어 있는 모델의 가중치를 가져와 쓰는 Fine Tune이 있기 때문이다.","categories":[{"name":"CS231n","slug":"CS231n","permalink":"https://heung-bae-lee.github.io/categories/CS231n/"}],"tags":[]},{"title":"growth_hacking_01","slug":"growth_hacking_01","date":"2019-05-17T23:38:10.000Z","updated":"2019-07-20T03:36:16.967Z","comments":true,"path":"2019/05/18/growth_hacking_01/","link":"","permalink":"https://heung-bae-lee.github.io/2019/05/18/growth_hacking_01/","excerpt":"","text":"결국 문제는 Distribution 모바일 앱 시장을 생각해 보면 알수 있듯이 초창기에는 신기해서 앱을 다운로드하던 시정이 있었지만, 현재는 앱을 다운로드하지 않는 트렌드를 알 수 있다. 클라우드 서비스의 성장과 SaaS의 성장으로 인래 많은 스타트업이 생겼지만, 그 중 대부분이 실패를 한다. 그 이유는 유통때문이다. 즉, 빈약한 유통채널을 가지고 있기 때문이다. 그러한 대표적인 사례로 TiVo를 꼽을 수 있다. 즉, 컨텐츠가 있어야 디지털 비디오 레코드 상품을 판매하는 것이 의미있는 것인데, 컨텐츠들은 모두 케이블 TV 사업자와 위성 TV 사업자가 컨텐츠 유통을 쥐고있기 때문에 유통이 어려웠다. 결국 Tivo는 Patent Troll(특허 전쟁)로 전락하였다. 성공적인 기술 기업의 일반적인 모델은, 제품 중심기업이 아니라 유통 중심 기업이 되는 것이다. 구글, 페이스북등의 기업은 그 자체가 유통 플랫폼이 되어, 계속해서 새로운 제품을 고객들에게 선보이고 있다. 허나 위의 유통 중심의 기업으로 성장하기가 어려운 현실이 되었다. 시장에 많은 제품과 서비스가 나와있으며 소비자와 고객들이 가진 시간과 돈은 한정되어있으므로 모든 제품과 서비스가 고객들에게 도달하지 못하기 때문이다. 또한, 페이스북, 인스타그램 등 광고 플랫폼의 지면에는 한계가 있지만, 광고하고 싶은 기업은 점점 늘어나기 때문에 광고비는 상승한다. 사람들은 점점 광고를 클릭하지 않으며, 사람들이 기업의 마케팅 활동에 적응하고 피로감을 느끼고있는 상황은 더욱 더 유통이 이루어지기 힘들게 하고 있다.","categories":[{"name":"growth hacking","slug":"growth-hacking","permalink":"https://heung-bae-lee.github.io/categories/growth-hacking/"}],"tags":[]},{"title":"growth_hacking_00","slug":"growth_hacking_00","date":"2019-05-17T22:36:16.000Z","updated":"2019-07-20T03:36:19.296Z","comments":true,"path":"2019/05/18/growth_hacking_00/","link":"","permalink":"https://heung-bae-lee.github.io/2019/05/18/growth_hacking_00/","excerpt":"","text":"Growth hacking outlineStart up? 에어비앤비, 우버, 비바리퍼블리카(토스), 우아한형제들(배달의 민족), VCNC의 타다 등의 회사 확장가능하고 반복가능한 사업모델을 탐색하기 위해 고안되어진 임시조직 아직 잘 작동하는 사업 모델을 찾아내지 못한 기업을 의미하며, 사업 모델이 잘 작동한다면 Established company라고 부를 수 있다. 스타트업의 숙명은 결국 사업 모델을 찾기 전까지 여러 가지 시도를 하고 시행착오를 겪을 수 밖에 없다! 스타트업의 특징 한정된 자원(Runway) ex) 제한된 돈 Runway의 끝에 도달하기 전에 해야할 일 시장, 고객 등에 대한 가설을 확인하며 고객 확보 매출, 후속 투자 유치 등 이런 스타트업과 가설검증의 관계는 무엇일까? - 가설은, 팀이 가진 경험이나 직관을 기반으로한 학습된 추측이다. 결국 스타트업은 끊임없이 가설을 검증해야하는 조직이다!! 그러므로, 스타트업은 빠른 속도로(Lean하게) 가설을 검증해야 한다. 왜냐하면 많은 가설들을 주어진 자원내에 해결하여 이익을 창출해야하기 때문이다. 그로스 역시, 끊임 없이 가설을 세우고 검증하는 “테스트”를 반복하는 것이다. 리스크와 가설 위에서 가설의 검증 중요성을 언급하였다. 그렇다면 스타트업이 가지고 있는 가설 중 어떤 가설 부터 확인해야 하는지를 말하자면, 틀렸을 경우, 사업이 실패할 확률이 큰 리스크가 높은 가설을 먼저 확인해야 함이 논리적일 것이다. 그렇다면 그 리스크가 큰 가설은 제일 무엇일지를 고민해보면 수익을 창출해낼수 있으려면 고객이 있어야 하므로 잠재적인 고객이 없다는 것이 가장 리스크가 높을 것이다. 고객 리스크고객에 대한 가설을 확인하는 방법 고객 인터뷰 (가장 효과적인 방법) 포커스 그룹 인터뷰(FGI) 5~6명 정도되는 인원들을 한 방에 모아놓고 인터뷰 진행을 하므로 인터뷰 중 자신의 주관적인 의견을 말하기 보다는 그 인터뷰의 흐름에 따라 의견을 낼 수 있는 집단 사고의 위험이 있을 수 있다. 또한 목소리 큰 사람에세 동조하는 경향을 보이므로 이런 문제를 해결하기 위해 전문 모더레이터를 고용하는 방법이 있지만 이 방법은 비용이 높으므로 스타트업에서 추천하는 방법은 아니다. 1대 1 심층 인터뷰 위의 포커스 그룹 인터뷰와는 다르게 집단 사고의 위험이 없고 비교적 쉽게 수행할 수 있다. 온전히 한명의 고객에 집중할 수 있다는 장점이 있다. 여기까지 보게 되면 그로스 해킹은 데이터를 보아야만 하는 것이 아니지 않느냐는 의문이 생길 것이다. 이에 대한 답은 아니다라는 결론을 내릴 수 있게 된다. 왜냐하면 그로스라는 것은 결국 고객을 파악하여 고객이 어떤 문제를 가지고 있는지 같은 고객의 잠재적인 니즈를 파악하여 그를 해결하므로서 수익을 창출하는 것이기 때문이다. 참고로 데이터 기반 회사의 끝판왕인 구글도 고객 인터뷰를 중시한다. [참고] 고객 인터뷰 공부하기 애쉬 모리아의 책 “린 스타트업”, “스케일링” Google Ventures - User Research, Quick n’Dirtyhttps://library.gv.com/user-research-quick-and-dirty-1fcfa54c91c4 고객이 존재한다면 추후에 확인해야할 리스트 고객이 존재하긴 하더라도, 적은 수의 고객만 존재한다면 시장의 파이가 크지 않으므로 사업모델을 만들어내기 적합하지 않다. 이런 시장의 크기는 충분히 여러명(50명, 100명)과 인터뷰를 하며, 반복적인 패턴을 관찰해야 하며, 또 다른 방법으로는 실제 제품을 개발하지 않고 아이디어를 검증하는 방법인 Smoke Test를 실행헤 보는 것이다. 예를 들면, 더미 화면을 만들어 그곳으로 트래픽을 유입시\u001d켜서 테스트를 하는 방법이다. 이렇게 함으로써 우리 제품의 시장에서의 수요를 확인할 수 있다. 솔루션 리스크 Problem-Solution Fit이 맞지 않다라는 말을 들어본적이 있을 것이다. 즉, 솔루션이 문제에 적합하지 않다는 것이다. 고객의 문제도 존재하고, 지갑을 열 고객도 충분히 많지만 우리의 제품이 그 문제에 대한 효과적인 해결책이 아닌 경우에 위의 말을 사용한다. 솔루션 리스크에 대비하려면 프로토타입을 만들어서 고객 반을을 먼저 확인해야 한다. 가설을 확인하지 않은 채, 제품 개발에 너무 많은 자원을 들이지 않아야 한다.","categories":[{"name":"growth hacking","slug":"growth-hacking","permalink":"https://heung-bae-lee.github.io/categories/growth-hacking/"}],"tags":[]},{"title":"프로그래머를 위한 베이지안 with 파이썬(1)","slug":"Bayes","date":"2019-03-08T14:52:20.000Z","updated":"2019-07-20T03:36:10.633Z","comments":true,"path":"2019/03/08/Bayes/","link":"","permalink":"https://heung-bae-lee.github.io/2019/03/08/Bayes/","excerpt":"","text":"베이지안 심리 상태 베이지안 추론은 불확실성을 유지한다는 점에서 기존의 전통적인 통계적 추론과 다르다. 위의 말을 읽었을때 개인적으로 통게학을 전공한 나로써는 이해가 잘 가지 않았다. 왜냐하면 통계라는 학문 자체가 불확실성을 기반으로 하는 것이 아닌가라는 의문이 들었기 때문이다. random한 것에서 확률적으로 높은 것을 찾아내는 것인데 말이다. 우선 확률을 해석하는 방법은 크게 두가지로 구분 될 수 있다. 고전적인 빈도주의적 관점 : 사건이 장기적으로 일어나는 빈도로 해석ex) 항공기의 사고 확률을 오랫동안 발생한 비행기 사고의 빈도로 해석한다. 위의 빈도주의적 관점은 논리적으로는 오류가 없어 보이나, 오랫동안이라는 전제조건이 필요하다. 만일 오랫동안 즉, 충분한 시도가 없었을 경우의 확률은 어떠한 근거로 이야기 해야할 것인가의 의문을 갖게 된다. 그것이 바로 베이지안 방법이다. 베이지안 관점 : 사건 발생에 대한 믿음 또는 확신의 척도로 해석 간단히 말하면, 확률은 의견이나 견해를 요약한 것이라는 의미일 것이다. 주목할 점은 믿음의 정도나 확신의 척도가 개인의 주관에 달려 있다는 점이다. 그렇다면 객관적이지 않은데 이것을 확률이라고 말할 수 있는가? 좀더 근본적인 얘기를 해보자면 믿음이기 때문에 누군가가 틀렸다는 것을 말할 수 없지 않은가? 베이지안관점에서의 확률을 이야기하자면 베이즈 정리 또한 빼놓고 이야기 할수 없을 것이다.베이즈 정리의 사전확률과 사후확률","categories":[{"name":"Bayes","slug":"Bayes","permalink":"https://heung-bae-lee.github.io/categories/Bayes/"}],"tags":[]},{"title":"비주얼 디자인에서 제목(Headings), 단락(Paragraph)구조 도출하기","slug":"front_end4","date":"2019-03-05T14:05:59.000Z","updated":"2019-07-20T03:36:22.826Z","comments":true,"path":"2019/03/05/front_end4/","link":"","permalink":"https://heung-bae-lee.github.io/2019/03/05/front_end4/","excerpt":"","text":"제목(Hedings), 단락(Paragraph) 도출하는 과정단락(Paragraph) 사용자가 가장 많이 읽는 콘텐츠는 단락으로 단락은 p요소로 구성된다. 제목의 단계(Hedings Level 1 - 6) 사용자가 가장 먼저 읽은 콘텐츠는 제목으로 제목은 h요소로 구성된다. h1 요소는 문서에서 단 한번만 사용하며 HTML5에서는 섹션 콘텐츠 마다 사용 가능하다. HTML 제목 HTML 단락 HTML 주\u001d석 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=\"ko-KR\" dir=\"ltr\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"&gt; &lt;title&gt;HTML 제목(Headings) 그리고 단락(Paragraph)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; \b오늘의 나이, 대체로 맑음 &lt;h1&gt;이제 좀 살아본 사람들의 마흔, 자기 생의 날씨를 적절히 대처하기 알맞은 나이.&lt;/h1&gt; &lt;h2&gt;이 나이에도 여전히 미숙하고 꾸준히 실수한다.&lt;/h2&gt; &lt;img src=\"sbs-drama__do-you-want-to-kiss-first.png\" alt=\"SBS 드라마 &lt;키스 먼저 할까요?&gt;의 한 장면: 배우 김선아가 홀로 겨울바다를 걷는 중...\"&gt; &lt;/body&gt;&lt;/html&gt; HTML 이미지 &amp; 피규어 &amp; 캡션 HTML 문서에 연결되어 화면에 표시되는 이미지 요소와 도표, 차트, 표 이미지 등을 캡션과 함께 묶어주는 피규어 요소에 알아본다. 위에서 img 요소의 alt 속성을 해두는 이유는 링크가 깨질 경우에는 화면에 alt 속성 값이 출력 되어 어떤 이미지 였는지 정보를 제공할 수 있으며, 또한 시각 장애인의 경우 이미지를 보지 못하므로 이런 대체 속성을 통해서 정보를 얻을 수 있기 때문이다. 또한, HTML의 이미지 같은 경우는 서버를 통해 연결되어 있다. 즉, 한글이나 기타 문서 작업의 프로그램에서 처럼 이미지를 실제로 포함하고 있는 것(임베딩)이 아니라는 점이 크게 다른 점이라고 할 수 있다. 웹 문서에 주로 사용되는 이미지 포멧은 다음과 같다. 비트맵 그래픽 파일 : JPG, GIF, PNG 포멧이 사용 벡터 그래픽 파일 : SVG 포멧이 사용 (위의 비트맵 그래픽 파일은 픽셀로 구성되어 있기 때문에 크기를 키울 경우 뿌옇게 흐려진다.) JPG 이미지JPG 이미지는 압축률이 높고, 다양한 색상을 처리하도록 설계되었으며, 그러므로 사진 또는 복잡한 그래픽(그레디언트와 같은) 이미지에 많이 사용된다. 하지만, GIF, PNG와 달리 투명한 픽셀을 허용하지 않는다. PNG 이미지사진의 경우 동일한 품질의 PNG 파일 크기가 일반적으로 JPG보다 크기 때문에 PNG는 사진이나 애니메이션을 제외한 모든 유형에 적합하다. 하지만 JPG와 달리 투명 처리가 가능해 아이콘, 로고, 다이어그램 등에 사용하면 좋다. GIF 이미지GIF는 표현 가능한 색상이 256색으로 제한되어 있기에 사진에는 적합하지 않다. 하지만, 애니메이션을 적용할 수 있는 포멧으로 단순한 그래픽의 애니메이션에 사용하면 좋다. 투명하게 처리가 가능하긴 하지만, PNG 포멧 보다 표현력이 떨어진다. SVG 이미지SVG는 벡터 기반 그래픽 포멧으로 품질 손실없이 확대, 축소 할 수 있다. 오늘날 처럼 다양한 스크린에 대응하는 반응형 웹 디자인에 매우 적합하다. HTML 이미지HTML 피규어HTML 피규어 캡션 HTML 문법 유효성 검사 &amp; 엔티티(Entity) 코드HTML 문법 유효성 검사(Validator) HTML 문서의 문법의 유효성 검사 방법과 각 괄호(Angle Bracket,&lt; &gt;) 모양의 문자를 브라우저가 태그로 인식하지 않도록 이스케이프(Escape) 처리하는 엔티티(Entity)코드에 대해서 알아본다. 참고자료vaildator.w3.orgentitycode.com 위에서 첫번째 사이트의 경우 HTML 문법을 검사할수 있는 온라인 서비스를 제공하고 있어서 서비스를 이용하면 좋다. 또한, 이스케이프 처리를 할려는 문자들은 두번째 사이트에서 찾아서 수정하면 된다. 위의 코드에서 이스케이프 처리를 해준다면, 다음과 같이 &lt;&gt; 부분이 변경된다. 1234567891011121314&lt;!DOCTYPE html&gt;&lt;html lang=\"ko-KR\" dir=\"ltr\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\"&gt; &lt;title&gt;HTML 제목(Headings) 그리고 단락(Paragraph)&lt;/title&gt; &lt;/head&gt; &lt;body&gt; \b오늘의 나이, 대체로 맑음 &lt;h1&gt;이제 좀 살아본 사람들의 마흔, 자기 생의 날씨를 적절히 대처하기 알맞은 나이.&lt;/h1&gt; &lt;h2&gt;이 나이에도 여전히 미숙하고 꾸준히 실수한다.&lt;/h2&gt; &lt;img src=\"sbs-drama__do-you-want-to-kiss-first.png\" alt=\"SBS 드라마 &amp;lt키스 먼저 할까요?&amp;gt의 한 장면: 배우 김선아가 홀로 겨울바다를 걷는 중...\"&gt; &lt;/body&gt;&lt;/html&gt; HTML 목록 디자인HTML 문서 작성 시, 목록은 매우 빈번하게 사용 되는 요소이다. 비순차 목록(Unorderd List) : 순서가 없는 목록 순차 목록(Ordered List) : 순서가 중요한 목록 여기서 가장 중요한 것은 ol이나 ul요소의 자식은 꼭 li요소로 감싸고 시작해야한다는 점이다!!! 참고 자료 HTML 앵커(Anchor) &amp; 하이퍼링크(Hyperlink)하이퍼링크(Hyperlink)현재 페이지에서 다른 페이지로 이동하게 해주는 것을 하이퍼링크라고 한다. 이렇게 하이퍼 링크를 할때 사용하는 요소는 앵커요소이며, 다음과 같다. 앵커(Anchor) : 페이지를 다른 페이지로 전환하지 않고, 현재 페이지 하단으로 이동할 때 사용한다.123456789101112131415161718192021222324&lt;!DOCTYPE html&gt;&lt;html lang=\"en\" dir=\"ltr\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt;&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;h2&gt;목차(Table of Contents)&lt;/h2&gt; &lt;!-- Depth 1: ul --&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=\"#intro\"&gt;소개&lt;/a&gt;&lt;/li&gt; &lt;!-- Depth 2: ul --&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=\"\"&gt;영상소개&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"\"&gt;어렵진 않을까 걱정되시나요&gt;?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"\"&gt;GSAP에 대해 간략하게 정리해볼까요?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"\"&gt;TweenLite&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;li&gt;&lt;a href=\"\"&gt;다운로드 &amp;amp; CDN&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=\"intro\"&gt;소개&lt;/h3&gt; &lt;/body&gt;&lt;/html&gt; 예를들어, 위의 코드에서는 앵커를 사용하여 소개를 누르게 되면, 주소가 file:///Users/heungbaelee/workspace/myBlog/source/_posts/day.html에서 file:///Users/heungbaelee/workspace/myBlog/source/_posts/day.html#intro로 바뀌게 되는 것을 확인 할 수 있다. 참고자료하이퍼링 요소 웹사이트는 폴더로 구성된 HTML 파일의 모음일 뿐이다. 다른 파일 내부에서 이들 파일을 참조하기 위해 인터넷은 URL(uniform resource locators)를 사용한다. URL은 웹 사이트의 리소스 위치 경로를 말하며 다음과 같이 구성된다. 예를들어 다음과 같은 주소가 있을때, https://Developer.mozila.org/EN-US/DOCS/WEB/HTML 에서 https:// : SCHEME Developer.mozila.org : DOMAIN /EN-US/DOCS/WEB/HTML : PATH 웹 문서에 URL을 입력하는 방법은 3가지 정도가 있다. 절대 경로 : 현재 HTML 문서와 상관없이 URL 주소를 사용해 리소스를 찾는 상대 경로 : 현재 HTML 문서에서 상대적인 위치를 설정하는 것을 말한다. ex) “../mics/extra.html”과 같이 사용한다. 루트 상대경로 : 현재 HTML 문서가 존재하는 영역의 최상위 루트 경로에서 대상을 찾는 것 ex)”/images.html”","categories":[{"name":"Front end","slug":"Front-end","permalink":"https://heung-bae-lee.github.io/categories/Front-end/"}],"tags":[]},{"title":"HTML의 DOCTYPE?","slug":"frontent3","date":"2019-02-12T01:57:00.000Z","updated":"2019-03-05T13:56:18.570Z","comments":true,"path":"2019/02/12/frontent3/","link":"","permalink":"https://heung-bae-lee.github.io/2019/02/12/frontent3/","excerpt":"","text":"우선 코드를 작성할때 HTML 코드는 tag name에 대문자로도 작성이 가능하나, 소문자로 작성하는 것을 권하며 코드 간 들여 쓰기를 하는 이유는 HTML 요소 간 관계(부모, 자식, 형제)를 구분하기 용이 함에 들여쓰나, 브라우저는 코드가 한 줄 이어도 해석하는데 아무런 문제가 없다. DTD(표준 호환모드)문서 유형 정의(Document Type Definition)를 말하며 브라우저의 렌더링 모드를 표준으로 작동하게 만들어 준다. 여기서 중요한 것인 DTD는 반드시 HTML 문서의 최상 위에 위치해야 한다. 비표준(quirks) 모드와 표준(standards) 호환모드 또한, 브라우저에서 해당 웹의 DOCTYPE을 알고 싶은 경우는 console 창에 document.doctype을 입력하면 출력되게 되어있어서 확인 할 수 있다. 웹 표준 기술 규격에 따르면 html 요소는 오직 1개의 head와 뒤 따르는 1개의 body 요소만 자식으로 포함할 수 있다는 사실을 기억하자. 문서에 사용된 주 언어가 무엇인지 설정하는 lang 속성루트 요소에 lang 속성을 사용하여 문서에 주요 사용되는 언어(language)를 설정한다.사용되는 언어 코드는 ISO 639 (전 세계 언어 명칭에 고유 부호를 부여하는 국제 표준)에서 찾을 수 있다. 12345678910&lt;!doctype html&gt;&lt;html lang=\"ko-KR\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;title&gt; HTML 문서 작성을 위한 기본 문법&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;p title=\"Development Tools\"&gt;개발 도구(Devtools)&lt;/p&gt; &lt;/body&gt;&lt;/html&gt; 위의 코드에서 html 태그의 lang이라는 attribute를 통해 작성하는 코드에서 주로 사용되는 언어를 선택 해주면 된다. “ko-KR”은 대한민국에서 주로 사용언어는 korean이라는 의미이다. ko, en 은 korean, english 의 약자로 언어(language) 정보를 말한다. 반면 KR은 Republic of Korea 지역(locale) 정보를 말한다. ko 만 사용하면 한국어를 통칭하지만, 언어를 지역마다 구분해야 할 경우, 지역 정보를 추가하여 사용하는 것이 좋다. 예를들어, ko-KR은 대한민국(남한)에서 사용하는 한국어를 의미하지만, ko-KP는 조선 민주주의 인민공화국(북한, 북조선)에서 사용하는 한국어를 의미하게 된다. 영문권에서는 지역마다 사용 되는 영어가 달라 다음과 같이 표기하여 구분한다. en-GB ⇒ 영국 영어en-US ⇒ 미국 영어en-CA ⇒ 캐나다 영어 언어 조회는 Language subtag lookup 에서 할 수 있다.","categories":[{"name":"Front end","slug":"Front-end","permalink":"https://heung-bae-lee.github.io/categories/Front-end/"}],"tags":[]},{"title":"deep learning_01","slug":"deep_learning1","date":"2019-01-16T13:59:30.000Z","updated":"2019-12-08T05:35:02.857Z","comments":true,"path":"2019/01/16/deep_learning1/","link":"","permalink":"https://heung-bae-lee.github.io/2019/01/16/deep_learning1/","excerpt":"","text":"우선 지금부터 설명하는 내용은 패스트캠퍼스의 교육과정에서 배운 내용과 제 나름대로 책을 읽어가며 정리하는 내용을 바탕으로 작성하는 글입니다. 딥러닝 기초딥러닝? ML분야에서나 AI분야에서 활발하게 이용하고 있는데 간단히 말하자면, 여기서 말하는 ‘deep’은 학습을 수행하는 신경망(혹은 그에 상응하는 다른 것)의 층수가 ‘깊다’라는 의미이다. 즉, 여러층을 가진 구조를 사용한 학습을 말한다. 신경망 기초 이론 신경망(neural network) 모형은 기저 함수(basis function)의 형태를 모수(parameter)값으로 변화 시킬 수 있는 적응형 기저 함수 모형(adaptive basis function model)이며 구조적으로는 복수의 퍼셉트론을 쌓아놓은 형태이므로 MLP(multi-layer perceptron)로도 불린다. 시그모이드 활성화 함수 일반적으로 활성화 함수 h로는 위와 아래가 막혀있는(bounded) 시그모이드 함수 \\( \\sigma \\)를 사용하는데 가장 많이 사용하는 활성화 함수는 로지스틱 함수이다. 비선형 기저 함수퍼셉트론에서 x 대신 기저함수 \\( \\phi(x) \\)를 사용하면 XOR 문제 등의 비선형 문제를 해결할 수 있다. 그러나 고정된 기저 함수를 사용해야 하므로 문제에 맞는 기저 함수를 찾아야 한다는 단점이 있다. 따라서 많은 기저 함수를 사용할 수 밖에 없는 것이 보통이다. 하이퍼 파라미터에 의해 모양이 바뀌는 비선형 기저 함수만약 기저 함수 \\( \\phi(x) \\)의 형태를 추가적인 모수 \\( \\theta \\)를 사용하여 조절할 수 있다면 즉, 기저함수 \\( \\phi(x;\\theta) \\)를 사용하면 \\( \\theta \\)값을 바꾸는 것만으로 다양한 시도를 하여 다양한 모양의 기저함수를 테스트해 볼 수 있을 것이다. 앞으로 설명할 신경망 즉, MLP(Multi-Layer-Perceptron)은 퍼셉트론이 사용하고 있는 로지스틱 시그모이드 함수를 기저 함수로 사용하는 모형이다. 기저 함수의 형태는 하이퍼 파라미터인 w","categories":[{"name":"deep learning","slug":"deep-learning","permalink":"https://heung-bae-lee.github.io/categories/deep-learning/"}],"tags":[]},{"title":"HTML이란?","slug":"Foront_end2","date":"2019-01-15T03:36:00.000Z","updated":"2019-03-05T13:54:47.784Z","comments":true,"path":"2019/01/15/Foront_end2/","link":"","permalink":"https://heung-bae-lee.github.io/2019/01/15/Foront_end2/","excerpt":"","text":"HTML 문서 작성을 위해 알아야 할 기본 문법웹 페이지는 head 영역과 body영역으로 구성 된다. Head Page title(웹 페이지의 제목으로 브라우저 탭에 표시된다.) 만약 위의 값이 한글인데 깨지게 된 경우는 console창에서 document.characterSet을 쳐보면 인코딩 방식을 확인 할 수 있으며 meta 태그에 charset=”utf-8” 속성값을 입력해주면된다. CSS Links Other Abstract things Body Headings Paragraph Other things(추상적이지 않고 우리 눈에 볼수 있는 것) HTML 용어- element(요소) - tag - open tag - close tag - attribute - value 기본 문법HTML 요소는 대소문자를 구분하지 않는다!!! 하지만 대게 가독성과 기타 이유 때문에 소문자로 작성한다.HTML은 elements로 구성되어 있으며, 이들은 적절한 방법으로 나타내고 실행하기 위해 각 컨텐츠의 여러 부분들을 감싸고 마크업 한다. 만약 문장을 그냥 자체로 표현하고 싶다면?? 다음과 같이 로 감싸면 된다. 1$ &lt;p&gt;My cat is very grumpy.&lt;/p&gt; 여기서 는 opening tag &lt;/p&gt;는 closing tag 안에 감싸져 있는 My cat is very grumpy는 Content 위의 코드 전체를 Element라고 한다. 닫는 태그가 없는 HTML 요소 Empty Element는 contents를 감싸지 않아 비어있다는 의미로써 contents를 감싸지 않기 때문에 닫는 태그를 갖지 않는다. empty element 중첩된 요소(Nesting elements)요소 안에 다른 요소가 들어갈 수 있다. 그런 요소는 중첩되어 있다고 표현한다. 예를 들어 “고양이가 매우 사납다”라는 문단을 강조하기 위해서 ‘매우’라는 단어를 강조하는 요소를 중첩해서 사용할 수 있다.1$ &lt;p&gt;My cat is &lt;strong&gt;very&lt;/strong&gt; grumpy.&lt;/p&gt; 블럭 레벨 요소 vs 인라인 요소(Block vs inline elements)HTML의 두 종류의 Element는 Block level element와 Inline element이다. Block-level elements는 웹페이지 상에 Block을 만드는 요소이다. 앞뒤 요소 사이에 새로운 줄(Line)을 만들고 나타난다. 즉 블록 레벨 요소 이전과 이후 요소사이의 줄을 바꾼다. 블록 레벨 요소는 일반적으로 페이지의 구조적 요소를 나타낼 때 사용된다. 예를 들어 개발자는 블록 레벨 요소를 사용하여 Paragraph, list, Navigation Menus(네비게이션 메뉴), Footers(꼬리말)등을 표현할 수 있다. 블록 레벨 요소는 인라인 요소에 중첩될 수 없다. 그러나 블록 레벨 요소는 다른 블록 레벨 요소에 중첩될수 있다. Inline elements는 항상 블록 레벨 요소내에 포함되어 있다. 인라인 요소는 문서의 한 단락같은 큰 범위에는 적용 될 수 없고 문장, 단어 같은 작은 부분에 대해서만 적용될 수 있다. 가장 큰 차이점은 인라인 요소는 새로운 줄을 만들지 않는다.즉 인라인 요소를 작성하면 그것을 작성한 단락내에 나타나게 된다. 예를 들어, 인라인 요소에는 하이퍼링크를 정의하는 요소인 , 텍스트(Text)를 강조하는 요소인 , 등이 있다. 123&lt;em&gt;first&lt;/em&gt;&lt;em&gt;second&lt;/em&gt;&lt;em&gt;third&lt;/em&gt;&lt;p&gt;forth&lt;/p&gt;&lt;p&gt;fifth&lt;/p&gt;&lt;p&gt;sixth&lt;/p&gt; 위의 코드를 실행 시켜 보면 알 수 있듯이 은 인라인 요소여서 서로 같은 줄에 공백이 없이 위치 하지만, 는 블록 레벨 요소이어서, 각 요소들은 새로운 줄에 나타나며, 위와 아래에 여백이 있다.(정확히는 여백은 브라우저가 문단에 적용하는 기본 CSS styling 때문에 적용된다.) 빈 요소(Empty elements)모든 요소가 위에 언급된 Open tag, contents, closed tag의 패턴을 따르는 것은 아니다. 주로 문서에 무언가를 첨부하기 위해 단일 태그(Single tag)를 사용하는 요소도 있다. 예들들어 요소는 해당 위치에 이미지를 삽입하기 위한 요소이다.(빈 요소는 가끔 Void 요소로 불리기도 한다.) 1&lt;img src=\"https://raw.githubusercontent.com/mdn/beginner-html-site/gh-pages/images/firefox-icon.png\"&gt; 속성(attributes)element는 Attribute를 가질 수 있다. Attribute는 element에 실제론 나타내고 싶지 않지만 추가적인 내용을 담고 싶을 때 사용한다. 아래의 코드는 나중에 스타일에 관련된 내용이나 기타 내용을 위해 해당 목표를 구분할 수 있는 class attribute를 부여 했다. Attribute를 사용할 때 지켜야 할 점 element 이름 다름에 바로오는 attribute는 element이름과 attribute 사이에 공백이 있어야 되고, 하나 이상의 attribute가 있는 경우엔 attribute 사이에 공백이 있어야 한다. attribute 이름 다음엔 등호(=)가 붙는다. attribute 값은 열고 닫는 따옴표로 감싸야 한다.","categories":[{"name":"Front end","slug":"Front-end","permalink":"https://heung-bae-lee.github.io/categories/Front-end/"}],"tags":[]},{"title":"HTML이란?","slug":"Front_end","date":"2019-01-14T08:36:00.000Z","updated":"2019-03-05T13:54:51.234Z","comments":true,"path":"2019/01/14/Front_end/","link":"","permalink":"https://heung-bae-lee.github.io/2019/01/14/Front_end/","excerpt":"","text":"HTML(Hyper Text Markup Language)이란?? Hyper Text란? text의 밑줄이 거지게되어 사용자가 이걸 눌르면 연결되어있는 링크로 연결된다. 구조를 설계할 때 사용되는 언어이며, 하이퍼링크 시스템으로 이루어져 있다. 확장자는 htm, html으로 가지며, 이 파일은 단순한 텍스트 파일에 불과하지만, 이 파일을 웹브라우져가 화면에 렌더링(그림을 그려준다라고 생각하면됨)을 통해 사용자가 볼수 있도록 해준다. HTML 표준 기술 사양 HTML 문서 파일과 웹 브라우저의 해석 &amp; 시멘틱 마크업 시멘틱 마크업(Semantic Markup) HTML은 웹사이트 콘텐츠의 의미를 설명하는 유일한 목적을 가진다. 즉, 비주얼 디자인(모양, 색, 크기 등)이 아니라(이 요소들은 CSS나 Javascript의 목표) 전체적인 구조 설계(Structure Design)를 목표로 한다. Semantic Markup은 종종 POSH(Plain Old Semantic HTML)라고도 불리우는데 해석 그대로 평범하고 오핸된 의미론적인 HTML이라는 뜻이다. 적절한 HTML요소를 올바르게 사용하는 것에서 시작한다. 1$ &lt;div id=\"heading\" style=\"font-size: 300%; padding: 10px;\"&gt;시멘틱 마크업이란?&lt;/div&gt; 위의 마크업은 확실히 제목같아 보이지만 의미와 용도면에서는 제목으로서의 기능은 없다. 이것은 검색엔진의 최적화, 접근성, 개발면에서 다음과 같은 사항들을 고려해야된다. 최적화 : 검색엔진을 최적화하는 것에서 매우 중요한 것은 headings 요소 안의 키워드이다. 접근성 : 스크린리더기는 네비게이션 길잡이로 headings요소를 참고한다. 개발 : 적절한 시멘틱 요소를 사용하지 않게 될시 스크립트와 스타일을 주기 위해 해당 요소를 설정하는데 많이 까다롭다. 시멘틱 마크업은 가능한 가벼워야 하기에 중첩된 모든 div요소와 실타래 처럼 얽혀 있는 스파게티 코드는 제거되어야 파일 사이즈가 작아지고 코딩이 더 쉬워진다. 결론적으로 HTML, CSS, Javascript는 각각 분리되어야 개발 및 유지 보수 측면에서 상당한 수고를 덜어 줄 수 있다. 1$ &lt;h1&gt;시멘틱 마크업이란?&lt;/h1&gt; 2005년 Andy Clark(세계적인 웹디자이너)도 데이터 구조, 스타일 정보, 그리고 스크립트 정보를 각각 분리시켜야 한다고 말한다. 1) 시멘틱 HTML은 정확하고 접근이 용이한 컨텐츠의 데이타 구조를 형성한다. 바로 HTML5가 이런 부분을 잘 지원하고 있다. 그래서 우리는 어떠한 스크립트를 추가 하지 않고도 가능한 접근이 용이하며 쓸모있게 데이터 구조를 만들어 내야한다. CSS는 스타일 정보를 제공한다. 즉, 데이터 구조에 우리가 원하는대로 시각적인 효과를 더해 주는 것이다. CSS3는 이전의 CSS2보다 더 강력한 툴이다. HTML5와 그 외 다른 것에 정의되어진 스크립팅 APIs와 베이스언어를 포함한 자바스크립트는 우리가 제작하는 사이트에 풍부한 기능과 유용성의 증대를 더해 주는 스크립팅을 제공한다. 텍스트 에디터로 HTML 파일을 열어서 마크업 한 후에 HTML 파일울 웹 브라우저로 열어 새로고침하면 결과를 확인할 수 있다. 브라우저는 어떻게 동작하는가?","categories":[{"name":"Front end","slug":"Front-end","permalink":"https://heung-bae-lee.github.io/categories/Front-end/"}],"tags":[]},{"title":"크롤링과 스크래핑 및 unix명령어 기초지식(1)","slug":"unix","date":"2018-12-23T09:34:53.000Z","updated":"2019-01-16T12:08:35.899Z","comments":true,"path":"2018/12/23/unix/","link":"","permalink":"https://heung-bae-lee.github.io/2018/12/23/unix/","excerpt":"","text":"크롤링과 스크래핑 웹페이지에의 정보를 추출하기 위한 프로그램을 웹 크롤러(Web Crawler) 또는 단순하게 크롤러(Crawler)라고 한다. 크롤러는 스파이더(Spider) 또는 봇(Bot)이라고 부르기도 하는데, 예를 들어 구글에서 우리가 검색할 경우 빠르게 검색 할 수 있는 이유 중의 하나는 바로 웹검색 엔진이 미리 전 세계의 웹사이트를 수집하고 저장함으로 가능한 일이다. 또한 RSS 리더는 사람 대신 크롤러가 RSS 피드를 확인하고, 변경 사항이 있는 경우에 업데이트된 항목이 있다고 알려주기도 한다. 가장 공감할 수 있는 부분은 바로 트위터, 페이스북 등의 SNS에서 웹 페이지 URL을 공유하면 페이지의 제목과 이미지를 미리 보기로 출력해 주는데, 이것도 크롤러가 해당 페이 지를 방문해서 관련된 정보를 추출하기 때문에 가능한 것이다. 그렇다면 크롤러와 스크래핑 두 용어의 차이점은 무엇인가??? 크롤링 웹 페이지의 하이퍼링크를 순회하면서 웹 페이지를 다운로드 하는 작업 스크래핑 다운로드한 웹 페이지에서 필요한 정보를 추출하는 작업 그렇다면 우리는 결국 크롤링한 후 스크래핑까지 하게 되는 일련의 작업을 앞으로 할 것이라고 생각한다. Wget으로 크롤링하기가장 먼저 Wget으로 크롤링하는 법을 소개 할 것이다. 크롤링과 스크래핑이 무엇인지 감을 잡기 위함이다. W","categories":[{"name":"crawling","slug":"crawling","permalink":"https://heung-bae-lee.github.io/categories/crawling/"}],"tags":[]},{"title":"hexo를 이용한 블로그 만들기","slug":"1day","date":"2018-12-21T15:00:00.000Z","updated":"2020-01-10T19:58:16.175Z","comments":true,"path":"2018/12/22/1day/","link":"","permalink":"https://heung-bae-lee.github.io/2018/12/22/1day/","excerpt":"","text":"Hexo로 나만의 블로그 만들기! Hexo는 쉽고 빠르고 강력한 블로그 프레임워크입니다. Node.js만 조금 알고 있다면 자신의 취향에 맞게 커스터마이징 할 수 있어서 좋습니다. 또한 템플릿 엔진으로는 Swig 또는 EJS 를 주로 사용합니다. 작성 시 사용할 수 있는 언어로는 HTML, Markdown, AsciiDoc 등이 있습니다. 이 글은 그 중에서 Markdown으로 작성할 예정입니다. 아래의 주소는 Hexo의 공식 홈페이지로서 기본적인 사용법을 알려주고 있습니다. Hexo 만약, 바로 사용법을 알고 싶다면 documentation을 클릭해주세요! 요점만 간략히 요약하자면 Hexo는 다음과 같은 특징이 있습니다. 커맨드라인(cmd)으로 간편하게 포스트 생성 및 관리 마크다운(Markdown) 지원 SEO, 반응형 웹을 지원하는 다양한 테마 npm 을 이용한 간편한 플러그인 적용 Github Pages, Netlify 등을 이용한 호스팅","categories":[{"name":"hexo","slug":"hexo","permalink":"https://heung-bae-lee.github.io/categories/hexo/"}],"tags":[]}]}