<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DataLatte&#39;s IT Blog</title>
    <link>https://heung-bae-lee.github.io/</link>
    
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>DataLatte가 Data Science 공부를 하면서 정리해 놓는 블로그</description>
    <pubDate>Tue, 12 May 2020 14:53:02 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Elements in linear algebra</title>
      <link>https://heung-bae-lee.github.io/2020/05/12/post/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/12/post/</guid>
      <pubDate>Tue, 12 May 2020 13:41:59 GMT</pubDate>
      <description>
      
        
        
          &lt;ul&gt;
&lt;li&gt;아래 내용은 &lt;a href&gt;김도형 박사님의 선형대수 강의안&lt;/a&gt; &lt;a href&gt;edwith의 인공지능을 위한 선형대수&lt;/a&gt; 강의와 &lt;a href&gt;KOCW의 한양대학교 이상화 교수님의 선형대수학&lt;/a&gt; 강의를 보고 정리한 내용이다.&lt;
        
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>아래 내용은 <a href>김도형 박사님의 선형대수 강의안</a> <a href>edwith의 인공지능을 위한 선형대수</a> 강의와 <a href>KOCW의 한양대학교 이상화 교수님의 선형대수학</a> 강의를 보고 정리한 내용이다.</li></ul><h1 id="Elements-of-Linear-Algebra"><a href="#Elements-of-Linear-Algebra" class="headerlink" title="Elements of Linear Algebra"></a>Elements of Linear Algebra</h1><p><img src="/image/linear_algebra_elements.png" alt="선형대수의 요소"></p><ul><li>선형대수에서 다루는 데이터는 개수나 형태에 따라 크게 스칼라(scalar), 벡터(vector), 행렬(matrix), 텐서(tensor) 유형으로 나뉜다. 스칼라는 숫자 하나로 이루어진 데이터이고, 벡터는 여러 숫자로 이루어진 데이터 레코드(data record)이며, 행렬은 이러한 벡터, 즉 데이터 레코드가 여럿인 데이터 집합이라고 볼 수 있다. 텐서는 같은 크기의 행렬이 여러 개 있는 것이라고 생각하면 된다.</li></ul><h3 id="스칼라-Scalar"><a href="#스칼라-Scalar" class="headerlink" title="스칼라(Scalar)"></a>스칼라(Scalar)</h3><ul><li>스칼라는 하나의 숫자만으로 이루어진 데이터를 말한다. 스칼라는 보통 $x$와 같이 알파벳 소문자로 표기하며 실수(real number)인 숫자 중의 하나이므로 실수 집합  $𝐑$의 원소라는 의미에서 다음처럼 표기한다.</li></ul><script type="math/tex; mode=display">\begin{align} x \in \mathbf{R} \end{align}</script><h3 id="벡터-Vector"><a href="#벡터-Vector" class="headerlink" title="벡터(Vector)"></a>벡터(Vector)</h3><ul><li><p>벡터는 여러 개의 숫자가 특정한 순서대로 모여 있는 것을 말한다. 사실 대부분의 데이터 레코드는 여러 개의 숫자로 이루어진 경우가 많다. 하나의 묶음(tuple)으로 묶어놓는 것이 좋다. 이때 숫자의 순서가 바뀌면 어떤 숫자가 어떤 피처에 매핑되는 값인지알 수 없으므로 <code>숫자의 순서를 유지하는 것이 중요</code>하다. 이런 데이터 묶음을 선형대수에서는 벡터라고 부른다.</p></li><li><p>이때 벡터는 복수의 가로줄, 즉 행(row)을 가지고 하나의 세로줄, 즉 열(column)을 가지는 형태로 위에서 아래로 내려써서 표기해야 한다. 하나의 벡터를 이루는 데이터의 개수가  $𝑛$개이면 이 벡터를 $n$-차원 벡터($n$-dimensional vector)라고 하며 다음처럼 표기한다.</p></li></ul><script type="math/tex; mode=display">\begin{align} x = \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{N} \\ \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} x \in \mathbf{R}^N \end{align}</script><ul><li>여기서 순서는 차원을 의미한다. 즉, 위의 presentation에서 벡터 $x$는 1차원에 1 2차원에 0 3차원에 2라는 값을 갖는다는 의미이다. 참고로 순서가 정해져 있지 않은 배열(array)는 집합(set)이다. 또한 벡터라고 하는 것은 한방향으로만 있는 1-dimensional로 존재하는 배열 또는 하나의 숫자가 된다.</li></ul><h3 id="행렬-Matrix"><a href="#행렬-Matrix" class="headerlink" title="행렬(Matrix)"></a>행렬(Matrix)</h3><ul><li>행렬(Matrix)는 기본적으로 2-Dimensional 배열(array)를 의미한다. 행렬은 복수의 차원을 가지는 데이터 레코드가 다시 여러 개 있는 경우의 데이터를 합쳐서 표기한 것이다. 행렬은 보통  $𝑋$와 같이 알파벳 대문자로 표기한다.</li></ul><script type="math/tex; mode=display">\begin{align} X = \begin{bmatrix} \boxed{\begin{matrix} x_{1, 1} & x_{1, 2} & x_{1, 3} & x_{1, 4}\end{matrix}}  \\ \begin{matrix} x_{2, 1} & x_{2, 2} & x_{2, 3} & x_{2, 4}\end{matrix} \\ \begin{matrix} x_{3, 1} & x_{3, 2} & x_{3, 3} & x_{3, 4}\end{matrix} \\ \begin{matrix} x_{4, 1} & x_{4, 2} & x_{4, 3} & x_{4, 4}\end{matrix} \\ \begin{matrix} x_{5, 1} & x_{5, 2} & x_{5, 3} & x_{5, 4}\end{matrix} \\ \begin{matrix} x_{6, 1} & x_{6, 2} & x_{6, 3} & x_{6, 4}\end{matrix} \\ \end{bmatrix} \end{align}</script><ul><li>스칼라와 벡터도 수학적으로는 행렬에 속한다. 스칼라는 열과 행의 수가 각각 1인 행렬이고 벡터는 열의 수가 1인 행렬이다. 그래서 스칼라는 $ \begin{align} a \in \mathbf{R}^{1\times 1} \end{align}$ 벡터는 $\begin{align} x \in \mathbf{R}^{n\times 1} \end{align}$</li></ul><h3 id="텐서-Tensor"><a href="#텐서-Tensor" class="headerlink" title="텐서(Tensor)"></a>텐서(Tensor)</h3><p>-</p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/12/post/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Ensemble Learning - 01</title>
      <link>https://heung-bae-lee.github.io/2020/05/02/machine_learning_14/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/02/machine_learning_14/</guid>
      <pubDate>Sat, 02 May 2020 12:00:10 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Ensemble-Learning이란&quot;&gt;&lt;a href=&quot;#Ensemble-Learning이란&quot; class=&quot;headerlink&quot; title=&quot;Ensemble Learning이란?&quot;&gt;&lt;/a&gt;Ensemble Learning이란?&lt;/h2&gt;&lt;ul
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Ensemble-Learning이란"><a href="#Ensemble-Learning이란" class="headerlink" title="Ensemble Learning이란?"></a>Ensemble Learning이란?</h2><ul><li>모형 결합(model combining)방법은 앙상블 방법론(ensemble methods)라고도 한다. 이는 특정한 하나의 예측 방법이 아니라 복수의 예측모형을 결합하여 더 나은 성능의 예측을 하려는 시도이다.</li></ul><ul><li><p>모형 결합 방법을 사용하면 <code>일반적으로 계산량은 증가하지만 다음과 같은 효과가 있다.</code></p><ul><li>단일 모형을 사용할 때 보다 성능 분산이 감소하기에 <code>과최적화(overfitting)을 방지</code>한다.</li><li><code>개별 모형이 성능이 안좋을 경우에는 결합 모형의 성능이 더 향상</code>된다.</li></ul></li><li><p>Ensemble Learning은 캐글이나 다른 대회에서 높은 성능을 자랑하며 여러 차례 우승을 차지한 알고리즘으로 그만큼 강력하지만, 현업에서는 Ensemble Learning을 사용하지 않을 가능성이 매우 높다. 왜냐하면 굉장히 강력하지만 다른 모델들과의 성능차이가 엄청나게 차이나는 것이 아니며, 실제 Domain에서 중요한 변수가 무엇인지와 같은 원인을 찾는 feature selection 부분이 더 중요할 수 있기 때문이다. 물론, 성능측면이 중요한 Domain분야에서는 Ensemble Learning이 중요할 것이다.</p></li><li><p>Ensemble이라는 의미의 사전적 정의는 ‘합창단’, ‘조화’라는 의미를 지닌다. 즉, 머신러닝에서의 개념은 여러개의 모델을 조합을 시킨다라는 의미로 받아들일 수 있다.</p></li></ul><p><img src="/image/Ensemble_dictionary_mean.png" alt="Ensemble의 의미"></p><ul><li>통계학에서의 대수의 법칙이라는 개념이 있는데, 큰 모집단에서 무작위로 뽑은 표본의 수가 많아 질수록(보통은 30개이상의 관측이) 모집단의 평균에 가까울 확률이 높아진다는 개념이다. <code>많은 시행의 결과가 수학적으로 합리적인 결과를 보여준다</code>는 것을 의미하는데, Ensemble learning에 적용하여 생각해보면 다수의 모델이 더 합리적인 성능을 가져올 수 있다는 것으로 해석할 수도 있다.</li></ul><p><img src="/image/the_law_of_large_number.png" alt="대수의 법칙과 Ensemble"></p><ul><li>하지만 아래에서 <code>합치는 모델의 성능 자체가 떨어지는 모델을 가지고 Ensemble learning을 진행한다고 해도 성능을 올릴 수는 없다.</code></li></ul><p><img src="/image/Ensemble_learning_conception_01.png" alt="Ensemble learning의 개념 - 01"></p><p><img src="/image/Ensemble_learning_conception_02.png" alt="Ensemble learning의 개념 - 02"></p><ul><li>아래에서는 이진분류에 대해서만 언급했지만, 대부분의 classification 문제에서는 One VS Rest 방식으로 문제를 풀기에 이진 분류 뿐만아니라, class가 여러개인 multi class 문제에서도 적용되는 내용이다.</li></ul><p><img src="/image/why_performence_prob_higher_than_half.png" alt="Ensemble learning에서 base model의 성능의 전제조건"></p><ul><li>아래에서와 같이 각각의 성능이 0.5인 분류기들을 voting을 통해 결과를 내게 되는데, 각각의 weak한 분류기들의 조합을 통해 최종적으로는 0.625라는 성능을 내게 된다.</li></ul><p><img src="/image/how_to_decision_predicted_value_on_ensemble.png" alt="Ensemble을 통한 결과 도출"></p><ul><li>다수결 모형이 개별 모형보다 더 나은 성능을 보이는 이유는 다음 실험에서도 확인 할 수 있다. 만약 개별 모형이 정답을 출력할 확률이 $p$인 경우에 <code>서로 다르고 독립적인 모형</code> $N$개를 모아서 다수결 모형을 만들면 정답을 출력할 확률이 다음과 같아진다.</li></ul><script type="math/tex; mode=display">\sum_{k>\frac{N}{2}}^N \binom N k p^k (1-p)^{N-k}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def total_error(p, N):</span><br><span class="line">    te = 0.0</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(int(np.ceil(N/2)), N + 1):</span><br><span class="line">        te += sp.misc.comb(N, k) * p**k * (1-p)**(N-k)</span><br><span class="line">    <span class="built_in">return</span> te</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.linspace(0, 1, 100)</span><br><span class="line">plt.plot(x, x, <span class="string">'g:'</span>, lw=3, label=<span class="string">"개별 모형"</span>)</span><br><span class="line">plt.plot(x, total_error(x, 10), <span class="string">'b-'</span>, label=<span class="string">"다수결 모형 (N=10)"</span>)</span><br><span class="line">plt.plot(x, total_error(x, 100), <span class="string">'r-'</span>, label=<span class="string">"다수결 모형 (N=100)"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"개별 모형의 성능"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"다수결 모형의 성능"</span>)</span><br><span class="line">plt.legend(loc=0)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/each_model_and_voting_medel_performance_differ.png" alt="개별모델과 앙상블 모델의 성능 비교"></p><ul><li>각각의 분류기(모델)를 통해 최종적으로는 해당 데이터들의 decision boundary의 평균을 사용하는 것과 동일한 결과를 얻을 수 있다.</li></ul><p><img src="/image/Ensemble_learning_classifier_decision_boundary_visualization.png" alt="Ensemble을 통한 예측의 decision boundary"></p><ul><li>아래에서와 같이 <code>Test data에 대해 일정부분 bias되는 부분을 줄이기 위해 overfitting이 잘 되는 트리기반의 모형을 주로 베이스 모델로 사용</code>한다.</li></ul><p><img src="/image/base_line_model_why_is_based_on_tree.png" alt="Ensemble 모델에서 트리기반을 주로 사용하는 이유"></p><ul><li>Ensemble Learning의 종류는 아래 그림과 같이 나눌 수 있다. 쉽게 말하면 <code>Bagging</code>은 여러 개의 모델을 만들기 위해서는 Tree기반이나 선형회귀 분석 같은 경우는 동일한 feature와 동일한 data를 사용했을 경우 동일한 결과를 내주기 때문에, <code>모델을 여러 개 만들기 위해서 데이터를 나누어서 각 모델에 fitting시키는 것을 의미</code>한다. <code>Random Forest</code>는 데이터 뿐만 아니라 feature들의 선택도 각 모델별로 달리하여 fitting하는 것이며, <code>Boosting</code>은 분류기가 틀리게 예측한 데이터들에 대해 그 다음 학습기는 좀 더 학습을 잘 할 수 있도록 가중치를 주는 개념이다. 마지막으로 <code>Stacking</code>은 성능순으로 점수를 매기는 캐글에서는 0.1%라도 올리는 것이 중요하기 때문에 사용되어 지는데, 다른 Ensemble 기법들 보다 많은 성능을 높이지는 못하여 잘 사용되지는 않는다. 굉장히 많은 학습 연산량을 필요로 하기 때문에 실제 Domain에서 사용되어지기에는 쉽지 않다.</li></ul><p><img src="/image/Ensmble_learning_types.png" alt="Ensemble learning의 종류"></p><ul><li>위에서 언급한 것과 같이 모형 결합 방법은 크게 나누어 취합(aggregation) 방법론과 부스팅(boosting)방법론으로 나눌 수 있다.<ul><li>취합 방법론은 사용할 모형의 집합이 이미 결정되어있다.</li><li>부스팅 방법론은 사용할 모형을 점진적으로 늘려간다.</li></ul></li></ul><ul><li><p>각 방법론의 대표적인 방법들은 아래와 같다.</p><ul><li><p>취합 방법론</p><ul><li>다수결(Majority Voting)</li><li>배깅(Bagging)</li><li>랜덤 포레스트(Random Forests)</li></ul></li><li><p>부스팅 방법론</p><ul><li>에이다부스트(AdaBoost)</li><li>그레디언트 부스트(Gradient Boost)</li></ul></li></ul></li></ul><h2 id="다수결-방법-Voting"><a href="#다수결-방법-Voting" class="headerlink" title="다수결 방법(Voting)"></a>다수결 방법(Voting)</h2><ul><li>다수결 방법은 가장 단순한 모형 결합 방법으로 전혀 다른 모형도 결합할 수 있다. 다수결 방법은 Hard Voting과 Soft Voting 두 가지로 나뉘어진다.<ul><li>hard voting: 단순 투표. 개별 모형의 결과 기준</li><li>soft voting: 가중치 투표. 개별 모형의 <code>조건부 확률의 합</code> 기준</li><li><code>일반적으로 hard voting보다는 soft voting이 예측 성능이 좋아서 더 많이 사용된다.</code></li></ul></li></ul><ul><li><p>Scikit-Learn의 ensemble 서브 패키지는 다수결 방법을 위한 <code>VotingClassifier</code>클래스를 제공한다. 입력인수는 다음과 같다.</p><ul><li><code>estimators</code>: 개별 모형 목록, 리스트나 named parameter 형식으로 입력</li><li><code>voting</code>: 문자열 {hard, soft} hard voting과 soft voting 선택. 디폴트는 hard</li><li><code>weights</code>: 사용자 가중치 리스트</li></ul></li><li><p>다음과 같은 예제 데이터를 가지는 이진 분류 문제를 생각해보자.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([[0, -0.5], [-1.5, -1.5], [1, 0.5], [-3.5, -2.5], [0, 1], [1, 1.5], [-2, -0.5]])</span><br><span class="line">y = np.array([1, 1, 1, 2, 2, 2, 2])</span><br><span class="line">x_new = [0, -1.5]</span><br><span class="line">plt.scatter(X[y == 1, 0], X[y == 1, 1], s=100, marker=<span class="string">'o'</span>, c=<span class="string">'r'</span>, label=<span class="string">"클래스 1"</span>)</span><br><span class="line">plt.scatter(X[y == 2, 0], X[y == 2, 1], s=100, marker=<span class="string">'x'</span>, c=<span class="string">'b'</span>, label=<span class="string">"클래스 2"</span>)</span><br><span class="line">plt.scatter(x_new[0], x_new[1], s=100, marker=<span class="string">'^'</span>, c=<span class="string">'g'</span>, label=<span class="string">"테스트 데이터"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.title(<span class="string">"이진 분류 예제 데이터"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/Ensemble_voting_method_data.png" alt="Ensemble 방법 중 Voting 방법을 사용할 데이터"></p><ul><li>먼저, 이 문제를 3가지 다른 방법으로 풀어볼 것이다.<ul><li>로지스틱 회귀모형</li><li>QDA 모형</li><li>가우시안 나이브베이즈 모형</li></ul></li></ul><ul><li>마지막으로 3가지 모형을 다수결로 합친 모형을 <code>VotingClassifier</code>클래스로 만들었다. 다만 3가지 모형의 가중치가 각각 1,1,2로 가우시안 나이브베이즈 모형의 가중치를 높였다.</li></ul><ul><li>결과는 다음과 같이, 로지스틱 회귀모형과 가우시안 나이브베이즈 모형은 클래스 1이라는 결과를 보이지만 QDA모형은 클래스 2라는 결과를 보였다. 소프트 방식의 다수결 모형은 클래스 2라는 결론을 보인다. 만약 하드 방식의 다수결 모형이었다면 예측 결과는 클래스 1이 될 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis</span><br><span class="line">from sklearn.ensemble import VotingClassifier</span><br><span class="line"></span><br><span class="line">model1 = LogisticRegression(random_state=1)</span><br><span class="line">model2 = QuadraticDiscriminantAnalysis()</span><br><span class="line">model3 = GaussianNB()</span><br><span class="line">ensemble = VotingClassifier(estimators=[(<span class="string">'lr'</span>, model1), (<span class="string">'qda'</span>, model2), (<span class="string">'gnb'</span>, model3)], voting=<span class="string">'soft'</span>)</span><br><span class="line"></span><br><span class="line">probas = [c.fit(X, y).predict_proba([x_new]) <span class="keyword">for</span> c <span class="keyword">in</span> (model1, model2, model3, ensemble)]</span><br><span class="line">class1_1 = [pr[0, 0] <span class="keyword">for</span> pr <span class="keyword">in</span> probas]</span><br><span class="line">class2_1 = [pr[0, 1] <span class="keyword">for</span> pr <span class="keyword">in</span> probas]</span><br><span class="line"></span><br><span class="line">ind = np.arange(4)</span><br><span class="line">width = 0.35  <span class="comment"># bar width</span></span><br><span class="line">p1 = plt.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color=<span class="string">'green'</span>)</span><br><span class="line">p2 = plt.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width, color=<span class="string">'lightgreen'</span>)</span><br><span class="line">p3 = plt.bar(ind, [0, 0, 0, class1_1[-1]], width, color=<span class="string">'blue'</span>)</span><br><span class="line">p4 = plt.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color=<span class="string">'steelblue'</span>)</span><br><span class="line"></span><br><span class="line">plt.xticks(ind + 0.5 * width, [<span class="string">'로지스틱 회귀 모형'</span>, <span class="string">'QDA 모형'</span>, <span class="string">'가우시안 나이브베이즈'</span>, <span class="string">'소프트 다수결 모형'</span>])</span><br><span class="line">plt.ylim([0, 1.1])</span><br><span class="line">plt.title(<span class="string">'세가지 다른 분류 모형과 소프트 다수결 모형의 분류 결과'</span>)</span><br><span class="line">plt.legend([p1[0], p2[0]], [<span class="string">'클래스 1'</span>, <span class="string">'클래스 2'</span>], loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/soft_voting_method_on_data_test.png" alt="소프트 보팅을 사용한 결과와 개별모델 예측 결과와의 비교"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from itertools import product</span><br><span class="line"></span><br><span class="line">x_min, x_max = -4, 2</span><br><span class="line">y_min, y_max = -3, 2</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.005),</span><br><span class="line">                     np.arange(y_min, y_max, 0.005))</span><br><span class="line">f, axarr = plt.subplots(2, 2)</span><br><span class="line"><span class="keyword">for</span> idx, clf, tt <span class="keyword">in</span> zip(product([0, 1], [0, 1]),</span><br><span class="line">                        [model1, model2, model3, ensemble],</span><br><span class="line">                        [<span class="string">'로지스틱 회귀'</span>, <span class="string">'QDA'</span>, <span class="string">'가우시안 나이브베이즈'</span>, <span class="string">'다수결 모형'</span>]):</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.2, cmap=mpl.cm.jet)</span><br><span class="line">    axarr[idx[0], idx[1]].scatter(</span><br><span class="line">        X[:, 0], X[:, 1], c=y, alpha=0.5, s=50, cmap=mpl.cm.jet)</span><br><span class="line">    axarr[idx[0], idx[1]].scatter(x_new[0], x_new[1], marker=<span class="string">'x'</span>)</span><br><span class="line">    axarr[idx[0], idx[1]].set_title(tt)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li>아래 각 모형별로 decision boundary를 살펴 보았을 경우 어떠한 생각이 드는가? 필자의 생각엔 물론 전제조건이 아래 train 데이터가 모집단의 분포를 대표할 수 있는 데이터들이라는 가정하에 soft 방식으로 한 결과는 옳지 못한 결과라고 생각한다. 2클래스로 분류되기엔 1 클래스가 많은 영역에 존재하기 때문이다. 이렇게 시각화를 통해 살펴보는 방법도 결과에 대한 검증을 위해 필요할 것이다. 허나, 다변량인 경우는 몇가지 중요한 변수들에 대해서만 시각화를 해 본다던가 아니면 해당 조합들에 대해 모두 그려보는 것도 때론 좋은 방법일 수 있을 것이다.</li></ul><p><img src="/image/each_model_decision_boundary_differ_result.png" alt="모델 별 decision boundary"></p><ul><li>앞서 모형 결합에서 사용하는 독립적인 모형의 수가 많을 수록 성능 향상이 일어날 가능성이 높다는 것을 알았다. <code>각각 다른 확률 모형을 사용하는데에는 한계가 있으므로 보통은 배깅 방법을 사용하여 같은 확률 모형을 쓰지만 서로 다른 결과를 출력하는 다수의 모형을 만든다.</code></li></ul><h2 id="배깅-Bagging"><a href="#배깅-Bagging" class="headerlink" title="배깅(Bagging)"></a>배깅(Bagging)</h2><ul><li><p>Bagging은 Bootstrap Aggregating의 약자로 Sampling을 하는 방식이 Bootstrap방식을 사용하기 때문이다. 아래 그림에서 볼 수 있듯이 복원추출의 방식이라고 생각하면된다. 하나의 모델에 대하여 데이터를 추출할 경우 해당 모델에 들어가있는 데이터는 중복된 데이터가 있을 수 있다.(오른쪽 첫번째 데이터세트에서와 같이)</p><ul><li>같은 데이터 샘플을 중복사용(replacement)하지 않으면: Pasting</li><li>같은 데이터 샘플을 중복사용(replacement)하면: Bagging</li><li>데이터가 아니라 다차원 독립 변수 중 일부 차원을 선택하는 경우에는: Random Subspaces</li><li>데이터 샘플과 독립 변수 차원 모두 일부만 랜덤하게 사용하면: Random Patches</li></ul></li></ul><p><img src="/image/What_is_bagging_01.png" alt="Bagging의 개념 - 01"></p><ul><li>이렇게 추출하는 데이터는 전체 데이터 중 약 63%정도만 추출을 하게 된다. 아래 첫 번째 그림에서는 밑줄이 그러진 원의 데이터는 추출되지 않는 데이터들이다. 2번째 그림은 Bootstrap size가 5라면 5개씩 12개의 데이터 set를 복원추출을 통하여 뽑는 것이다. 여기서의 $k$는 임의로 정할 수 있다.</li></ul><p><img src="/image/What_is_bagging_02.png" alt="Bagging의 개념 - 02"></p><p><img src="/image/What_is_bagging_03.png" alt="Bagging의 개념 - 03"></p><ul><li>위에서 언급했듯이 추출되지 않은 데이터 set이 있는 것은 학습에 활용되지 않았으므로 그대로 두면 데이터를 낭비하는 것과 동일하다. 물론 Test set을 미리 나누어 놓고 해당 Test set을 prediction한 결과를 voting하여 성능을 측정하지만, 사용되지 않은 데이터(Out-of_Bag data)에 대해서도 모델별 성능을 계산한다.</li></ul><p><img src="/image/What_is_bagging_04.png" alt="Bagging의 개념 - 04"></p><ul><li>트리(Tree)와 배깅(Bagging)을 비교하자면 깊이 성장한 트리는 overfitting이 굉장히 심해지기 때문에 분산이 증가하기 때문에 편향은 줄어들 것이다. 그러나 배깅은 이러한 트리들을 결합시키므로 <code>편향이 유지되며, 분사은 감소하는 모델</code>이 될 것이다. <code>학습데이터의 noise에 robust</code>하다. 그러나 모형해석이 어려워지는 단점이 있다. 이러한 단점이 실제 Domain에서 사용되지 못하는 이유가 될 수 있다.</li></ul><p><img src="/image/What_is_bagging_05.png" alt="Bagging의 개념 - 05"></p><ul><li>Scikit-Learn의 ensemble 서브 패키지는 배깅 모형 결합을 위한 <code>BaggingClassifier</code> 클래스를 제공한다. 사용법은 아래와 같다. 참고로 <code>BaggingRegressor</code>도 존재하며 사용법은 동일하다.<ul><li><code>base_estimator</code>: 기본모형</li><li><code>n_estimators</code>: 모형 갯수. default=10</li><li><code>bootstrap</code>: 데이터 중복 사용 여부. default=True</li><li><code>max_samples</code>: 데이터 샘플 중 선택할 샘플의 수 혹은 비율. default=1.0</li><li><code>bootstrap_features</code>: feature의 중복 사용 여부. default=False</li><li><code>max_features</code>: 다차원 독립 변수 중 선택할 차원의 수 혹은 비율. default=1.0</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.ensemble import BaggingClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data[:, [0, 2]], iris.target</span><br><span class="line"></span><br><span class="line">model1 = DecisionTreeClassifier(max_depth=10, random_state=0).fit(X, y)</span><br><span class="line">model2 = BaggingClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100, random_state=0).fit(X, y)</span><br><span class="line"></span><br><span class="line">x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">Z1 = model1.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z1, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"개별 모형"</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">Z2 = model2.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z2, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"배깅 모형"</span>)</span><br><span class="line">plt.suptitle(<span class="string">"붓꽃 데이터의 분류 결과"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li>왼쪽은 단일 모형으로 max_depth=10으로 설정한 Decision tree의 decision boundary의 모습이다. 트리의 깊이가 깊으므로 과최적화(overfitting)이 발생되었다. 오른쪽의 그림은 max_depth=2로 설정한 Decision tree모형을 100개 결합한 배깅 모형의 decision boundary 모습이다. 물론 depth를 작게하여 기본적인 모형자체도 과최적화(overfitting)를 방지하였지만, 배깅을 함으로써 모형의 분산이 줄어들며 더 train data에 robust하게 decision boundary가 그려진것을 확인할 수 있다.</li></ul><p><img src="/image/bagging_and_single_model_performance_differ_result.png" alt="배깅과 단일모형간의 decision boundary의 차이"></p><h2 id="랜덤-포레스트-Random-Forest"><a href="#랜덤-포레스트-Random-Forest" class="headerlink" title="랜덤 포레스트(Random Forest)"></a>랜덤 포레스트(Random Forest)</h2><ul><li>아래 그림과 같이 배깅은 여러 모델들을 결합하지만 부트스트랩 방식을 사용하여 모델들간의 사용되어지는 데이터가 동일한 집합들이 있을 수 있다. 그러므로 <code>Ensemble Learning의 개념에서 언급했었던 각 모델별 독립이라는 가정에 크게 위반</code>되어진다. 결국 비슷한 트리가 만들어지게 되어 모델들간의 공분산이 크게 되어 모델이 많아짐에 따라 점점 전체 Ensemble 모델의 분산은 커진다는 것이다. 분산이 커진다면 편향이 감소되어 더 좋은것이 아닌가라고 생각이 들수도 있겠지만, 모델의 예측 성능의 변동폭이 너무 크게 되면(분산이 크게 되어) 그만큼 불확신성도 높아지기 때문이다. 게다가, 애초에 다양한 모델에 대한 결합을 한 Ensemble 모델을 만들려고 한 의도조차 변질되어진다.</li></ul><p><img src="/image/why_is_randomforest_better_than_bagging.png" alt="배깅의 유의점 보완을 위한 방안"></p><ul><li>랜덤포레스트(Random Forest)는 의사 결정 나무(Decision Tree)를 개별 모형으로 사용하는 모형 결합 방법을 말한다. 랜덤 포레스트는 데이터 특징차원의 일부만 선택하여 사용한다. 하지만 <code>노드 분리시 모든 독립 변수들을 비교하여 최선의 독립 변수를 선택하는 것이 아니라 독립 변수 차원을 랜덤하게 감소시킨 다음 그 중에서 독립 변수를 선택</code>한다. 이렇게 하면 <code>개별 모형들 사이의 상관관계가 줄어들기 때문에 모형 성능의 변동이 감소하는 효과</code>가 있다. 이러한 방법을 극단적으로 적용한 것이 Extremely Randomized Trees 모형으로 이 경우에는 각 노드에서 랜덤하게 독립 변수를 선택한다.</li></ul><p><img src="/image/random_forest_conception.png" alt="랜덤포레스트의 개념"></p><p><img src="/image/random_forest_how_to_decide_parameter.png" alt="랜덤 포레스트의 개념 및 특징"></p><ul><li>랜덤 포레스트와 Extremely Randomized Trees 모형은 각각 <code>RandomForestClassifier</code> 클래스와 <code>ExtraTreesClassifier</code> 클래스로 구현되어 있다.</li></ul><ul><li>랜덤 포레스트는 CPU 병렬 처리도 효과적으로 수행되어 빠른 학습이 가능하기 때문에 뒤에 소개할 그레디언트 부스팅보다 예측 성능이 약간 떨어지더라도 랜덤 포레스트로 일단 기반 모델을 먼저 구축하는 경우가 많다. 멀티 코어 환경에서는 RandomForestClassifier 생성자와 GridSearchCV 생성 시 n_jobs = -1 파라미터를 추가하면 모든 CPU 코어을 이용해 학습할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data[:, [0, 2]], iris.target</span><br><span class="line"></span><br><span class="line">model1 = DecisionTreeClassifier(max_depth=10, random_state=0).fit(X, y)</span><br><span class="line">model2 = RandomForestClassifier(max_depth=2, n_estimators=100, random_state=0).fit(X, y)</span><br><span class="line"></span><br><span class="line">x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">Z1 = model1.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z1, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"개별 모형"</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">Z2 = model2.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z2, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"배깅 모형"</span>)</span><br><span class="line">plt.suptitle(<span class="string">"붓꽃 데이터의 분류 결과"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li>아래 그림에서 오른쪽은 max_depth=2로 설정하고 모형의 수를 100개로 한 RandomForest 모델의 decision boundary의 시각화한 것이다.</li></ul><p><img src="/image/randomforest_decision_boundary_reuslt_viz.png" alt="랜덤포레스트와 단일 Decision Tree 모형의 decision boundary 비교"></p><ul><li>랜덤 포레스트의 장점 중 하나는 각 독립 변수의 중요도(feature importance)를 계산할 수 있다는 점이다. 포레스트 안에서 사용된 모든 노드에 대해 어떤 독립 변수를 사용하였고 그 노드에서 얻은 information gain을 구할 수 있으므로 각각의 독립 변수들이 얻어낸 information gain의 평균을 비교하면 어떤 독립 변수가 중요한지를 비교할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_classification</span><br><span class="line">from sklearn.ensemble import ExtraTreesClassifier</span><br><span class="line"></span><br><span class="line">X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0,</span><br><span class="line">                           n_classes=2, random_state=0, shuffle=False)</span><br><span class="line"></span><br><span class="line">forest = ExtraTreesClassifier(n_estimators=250, random_state=0)</span><br><span class="line">forest.fit(X, y)</span><br><span class="line"></span><br><span class="line">importances = forest.feature_importances_</span><br><span class="line"></span><br><span class="line">std = np.std([tree.feature_importances_ <span class="keyword">for</span> tree <span class="keyword">in</span> forest.estimators_], axis=0)</span><br><span class="line">indices = np.argsort(importances)[::-1]</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"특성 중요도"</span>)</span><br><span class="line">plt.bar(range(X.shape[1]), importances[indices],</span><br><span class="line">        color=<span class="string">"r"</span>, yerr=std[indices], align=<span class="string">"center"</span>)</span><br><span class="line">plt.xticks(range(X.shape[1]), indices)</span><br><span class="line">plt.xlim([-1, X.shape[1]])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/feature_importance_using_random_forests.png" alt="랜덤포레스트를 통한 특성 중요도"></p><ul><li>다음은 올리베티 얼굴 사진을 Extreme 랜덤 포레스트로 구한 뒤 특징(pixel) 중요도를 이미지로 나타낸 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_olivetti_faces</span><br><span class="line">from sklearn.ensemble import ExtraTreesClassifier</span><br><span class="line"></span><br><span class="line">data = fetch_olivetti_faces()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line"></span><br><span class="line">forest = ExtraTreesClassifier(n_estimators=1000, random_state=0)</span><br><span class="line">forest.fit(X, y)</span><br><span class="line"></span><br><span class="line">importances = forest.feature_importances_</span><br><span class="line">importances = importances.reshape(data.images[0].shape)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8, 8))</span><br><span class="line">plt.imshow(importances, cmap=plt.cm.bone_r)</span><br><span class="line">plt.grid(False)</span><br><span class="line">plt.title(<span class="string">"픽셀 중요도(pixel importance)"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/pixel_importance_using_random_forests.png" alt="랜덤포레스트를 이용해 시각화한 픽셀 중요도"></p><h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><ul><li>앞에서 언급했던 Bagging이나 Random Forests는 부트스트랩 방식으로 데이터를 뽑긴해도 각 모델에 대해 독립적이라고 가정하지만, <code>Boosting은 resampling을 할 때 오분류된 데이터에 더 가중치를 주어서 오분류된 데이터가 뽑힐 확률이 높도록 하여 복원 추출을 하고 다시 학습하기 때문에 모델들이 Sequential한 것이다.</code></li></ul><p><img src="/image/Boosting_conception.png" alt="Boosting 개념"></p><ul><li>부스트(boost) 방법은 미리 정해진 갯수의 모형 집합을 사용하는 것이 아니라 하나의 모형에서 시작하여 모형 집합에 포함할 개별 모형을 하나씩 추가한다. 모형의 집합은 위원회(commit) $C$라고 하고 $m$개의 모형을 포함하는 위원회를 $C_{m}$으로 표시한다. 위원회에 들어가는 개별 모형을 약 분류기(weak classifier)라고 하며 $k$로 표시한다.</li></ul><ul><li><code>부스트 방법의 특징은 한번에 하나씩 모형을 추가한다는 것</code>이다.</li></ul><script type="math/tex; mode=display">C_1 = \{ k_1 \}</script><script type="math/tex; mode=display">C_2 = C_1 \cup k_2 = \{ k_1, k_2 \}</script><script type="math/tex; mode=display">C_3 = C_2 \cup k_3 = \{ k_1, k_2, k_3 \}</script><script type="math/tex; mode=display">\vdots</script><script type="math/tex; mode=display">C_m = C_{m-1} \cup k_m = \{ k_1, k_2, \ldots, k_m \}</script><ul><li>그리고 m번째로 위원회에 추가할 개별 모형 $k_{m}$의 선택 기준은 그 전단계의 위원회 $C_{m-1}$의 성능을 보완하는 것이다. <code>위원회 $C_{m}$의 최종 결정은 다수결 방법을 사용하지 않고 각각의 개별 모형의 출력을 가중치 $\alpha$로 가중 선형조합한 값을 판별함수로 사용</code>한다. 또한 부스트 방법은 이진 분류에만 사용할 수 있으며 $y$값은 1 또는 -1의 값을 가진다.</li></ul><script type="math/tex; mode=display">y = -1 \text{ or } 1</script><script type="math/tex; mode=display">C_{m}(x_i) =  \text{sign} \left( \alpha_1k_1(x_i) + \cdots + \alpha_{m}k_{m}(x_i) \right)</script><h2 id="AdaBoost-에이다부스트"><a href="#AdaBoost-에이다부스트" class="headerlink" title="AdaBoost(에이다부스트)"></a>AdaBoost(에이다부스트)</h2><p><img src="/image/Adaboosting_conception.png" alt="AdaBoost 개념"></p><ul><li>에이다 부스트(adaboost)라는 이름은 적응 부스트(adaptive boost)라는 용어에서 나왔다. 에이다부스트는 위원회에 넣을 개별 모형 $k_{m}$을 선별하는 방법으로학습데이터 집합의 $i$번째 데이터에 가중치 $w_{i}$를 주고 분류 모형이 틀리게 예측한 데이터의 가중치를 합한 값을 손실함수 $L$로 사용한다. 이 손실함수를 최소화하는 모형이 k_{m}으로 선택된다.</li></ul><script type="math/tex; mode=display">L_m = \sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)</script><ul><li>위 식에서 $I$는 $k(x_{i}) \neq y_{i}$라는 조건이 만족되면 1, 아니면 0을 갖는 indicator function이다. 즉 예측을 틀리게한 데이터들에 대한 가중치의 합이다. 위원회 $C_{m}$에 포함될 개별 모형 $k_{m}$이 선택된 후에는 가중치 $\alpha_{m}$을 결정해야 한다. 이 값은 다음처럼 계산한다.</li></ul><script type="math/tex; mode=display">\epsilon_m = \dfrac{\sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)}{\sum_{i=1}^N w_{m,i}}</script><script type="math/tex; mode=display">\alpha_m = \frac{1}{2}\log\left( \frac{1 - \epsilon_m}{\epsilon_m}\right)</script><ul><li>데이터에 대한 가중치 $w_{m, i}$는 최초에는$(m=1)$ 모든 데이터에 대해 동일한 값을 갖지만, 위원회가 증가하면서 값이 바뀐다. 가중치의 값은 지시함수를 사용하여 위원회 $C_{m-1}$이 맞춘 문제는 작게, 틀린 문제는 크게 확대(boosting)된다.</li></ul><script type="math/tex; mode=display">w_{m,i} = w_{m-1,i}  \exp (-y_iC_{m-1}) = \begin{cases} w_{m-1,i}e^{-1}  & \text{ if } C_{m-1} = y_i\\ w_{m-1,i}e & \text{ if } C_{m-1} \neq y_i \end{cases}</script><ul><li>$m$번째 멤버의 모든 후보에 대해 위 손실함수를 적용하여 가장 값이 작은 후보를 $m$번째 멤버로 선정한다.</li></ul><ul><li>에이다 부스팅은 사실 다음과 같은 손실함수를 최소화하는 $C_{m}$을 찾아가는 방법이라는 것을 증명할 수 있다.</li></ul><script type="math/tex; mode=display">L_m = \sum_{i=1}^N \exp(−y_i C_m(x_i))</script><ul><li>개별 멤버 $k_{m}$과 위원회 관계는</li></ul><script type="math/tex; mode=display">C_m(x_i) = \sum_{j=1}^m \alpha_j k_j(x_i) = C_{m-1}(x_i) + \alpha_m k_m(x_i)</script><ul><li>이고 이 식을 대입하면</li></ul><script type="math/tex; mode=display">\begin{eqnarray} L_m &=& \sum_{i=1}^N \exp(−y_i C_m(x_i)) \\ &=& \sum_{i=1}^N \exp\left(−y_iC_{m-1}(x_i) - \alpha_m y_i k_m(x_i) \right) \\ &=& \sum_{i=1}^N \exp(−y_iC_{m-1}(x_i)) \exp\left(-\alpha_m y_i k_m(x_i)\right) \\ &=& \sum_{i=1}^N w_{m,i} \exp\left(-\alpha_m y_i k_m(x_i)\right) \\ \end{eqnarray}</script><ul><li>$y_{i}$와 $k_{M}(x_{i})$ 1 또는 -1값만 가질 수 있다는 점을 이용하면,</li></ul><script type="math/tex; mode=display">\begin{eqnarray} L_m &=& e^{-\alpha_m}\sum_{k_m(x_i) = y_i} w_{m,i} + e^{\alpha_m}\sum_{k_m(x_i) \neq y_i} w_{m,i} \\ &=& \left(e^{\alpha_m}-e^{-\alpha_m}\right) \sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right) + e^{-\alpha_m}\sum_{i=1}^N w_{m,i} \end{eqnarray}</script><ul><li>$L_{m}$을 최소화하려면 $\sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)$을 최소화하는 $k_{m}$ 함수를 찾은 다음 $L_{m}$을 최소화하는 $\alpha_{m}$을 찾아야 한다.</li></ul><script type="math/tex; mode=display">\dfrac{d L_m}{d \alpha_m} = 0</script><ul><li>이 조건으로부터 $\alpha_{m}$ 공식을 유도할 수 있다.</li></ul><p><img src="/image/cost_function_of_adaboost.png" alt="Adaboost 비용함수"></p><ul><li>다음은 Scikit-Learn의 ensemble 서브패키지가 제공하는 <code>AdaBoostClassifier</code> 클래스를 사용하여 분류 예측을 하는 예이다. 약분류기로는 깊이가 1인 단순한 의사결정나무를 채택하였다. 여기에서는 각 표본 데이터의 가중치 값을 알아보기 위해 기존의 <code>AdaBoostClassifier</code> 클래스를 서브 클래싱하여 가중치를 속성으로 저장하도록 수정한 모형을 사용하였다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_gaussian_quantiles</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.ensemble import AdaBoostClassifier</span><br><span class="line"></span><br><span class="line">X1, y1 = make_gaussian_quantiles(cov=2.,</span><br><span class="line">                                 n_samples=100, n_features=2,</span><br><span class="line">                                 n_classes=2, random_state=1)</span><br><span class="line">X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,</span><br><span class="line">                                 n_samples=200, n_features=2,</span><br><span class="line">                                 n_classes=2, random_state=1)</span><br><span class="line">X = np.concatenate((X1, X2))</span><br><span class="line">y = np.concatenate((y1, - y2 + 1))</span><br><span class="line"></span><br><span class="line">class MyAdaBoostClassifier(AdaBoostClassifier):</span><br><span class="line"></span><br><span class="line">    def __init__(self,</span><br><span class="line">                 base_estimator=None,</span><br><span class="line">                 n_estimators=50,</span><br><span class="line">                 learning_rate=1.,</span><br><span class="line">                 algorithm=<span class="string">'SAMME.R'</span>,</span><br><span class="line">                 random_state=None):</span><br><span class="line"></span><br><span class="line">        super(MyAdaBoostClassifier, self).__init__(</span><br><span class="line">            base_estimator=base_estimator,</span><br><span class="line">            n_estimators=n_estimators,</span><br><span class="line">            learning_rate=learning_rate,</span><br><span class="line">            random_state=random_state)</span><br><span class="line">        self.sample_weight = [None] * n_estimators</span><br><span class="line"></span><br><span class="line">    def _boost(self, iboost, X, y, sample_weight, random_state):</span><br><span class="line">        sample_weight, estimator_weight, estimator_error = \</span><br><span class="line">        super(MyAdaBoostClassifier, self)._boost(iboost, X, y, sample_weight, random_state)</span><br><span class="line">        self.sample_weight[iboost] = sample_weight.copy()</span><br><span class="line">        <span class="built_in">return</span> sample_weight, estimator_weight, estimator_error</span><br><span class="line"></span><br><span class="line">model_ada = MyAdaBoostClassifier(DecisionTreeClassifier(max_depth=1, random_state=0), n_estimators=20)</span><br><span class="line">model_ada.fit(X, y)</span><br><span class="line"></span><br><span class="line">def plot_result(model, title=<span class="string">"분류결과"</span>, legend=False, s=50):</span><br><span class="line">    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02), np.arange(x2_min, x2_max, 0.02))</span><br><span class="line">    <span class="keyword">if</span> isinstance(model, list):</span><br><span class="line">        Y = model[0].predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(model) - 1):</span><br><span class="line">            Y += model[i + 1].predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Y = model.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)</span><br><span class="line">    cs = plt.contourf(xx1, xx2, Y, cmap=plt.cm.Paired, alpha=0.5)</span><br><span class="line">    <span class="keyword">for</span> i, n, c <span class="keyword">in</span> zip(range(2), <span class="string">"01"</span>, <span class="string">"br"</span>):</span><br><span class="line">        idx = np.where(y == i)</span><br><span class="line">        plt.scatter(X[idx, 0], X[idx, 1], c=c, s=s, alpha=0.5, label=<span class="string">"Class %s"</span> % n)</span><br><span class="line">    plt.xlim(x1_min, x1_max)</span><br><span class="line">    plt.ylim(x2_min, x2_max)</span><br><span class="line">    plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'x2'</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.colorbar(cs)</span><br><span class="line">    <span class="keyword">if</span> legend:</span><br><span class="line">        plt.legend()</span><br><span class="line">    plt.grid(False)</span><br><span class="line"></span><br><span class="line">plot_result(model_ada, <span class="string">"에이다부스트(m=20) 분류 결과"</span>)</span><br></pre></td></tr></table></figure><p><img src="/image/Adaboost_m_20_result.png" alt="Adaboost 20번째 모델까지의 결과"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(10, 15))</span><br><span class="line">plt.subplot(421);</span><br><span class="line">plot_result(model_ada.estimators_[0], <span class="string">"1번 분류모형의 분류 결과"</span>, s=10)</span><br><span class="line">plt.subplot(422);</span><br><span class="line">plot_result(model_ada.estimators_[1], <span class="string">"2번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[0]).astype(int))</span><br><span class="line">plt.subplot(423);</span><br><span class="line">plot_result(model_ada.estimators_[2], <span class="string">"3번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[1]).astype(int))</span><br><span class="line">plt.subplot(424);</span><br><span class="line">plot_result(model_ada.estimators_[3], <span class="string">"4번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[2]).astype(int))</span><br><span class="line">plt.subplot(425);</span><br><span class="line">plot_result(model_ada.estimators_[4], <span class="string">"5번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[3]).astype(int))</span><br><span class="line">plt.subplot(426);</span><br><span class="line">plot_result(model_ada.estimators_[5], <span class="string">"6번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[4]).astype(int))</span><br><span class="line">plt.subplot(427);</span><br><span class="line">plot_result(model_ada.estimators_[6], <span class="string">"7번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[5]).astype(int))</span><br><span class="line">plt.subplot(428);</span><br><span class="line">plot_result(model_ada.estimators_[7], <span class="string">"8번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[6]).astype(int))</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="/image/Adaboost_m_20_result_01.png" alt="Adaboost 모델의 가중치의 변화에 따른 decision boundary의 변화 - 01"></p><p><img src="/image/Adaboost_m_20_result_02.png" alt="Adaboost 모델의 가중치의 변화에 따른 decision boundary의 변화 - 02"></p><h3 id="Adaboost-모형의-정규화"><a href="#Adaboost-모형의-정규화" class="headerlink" title="Adaboost 모형의 정규화"></a>Adaboost 모형의 정규화</h3><ul><li>Adaboost 모형이 과최적화(overfitting)가 되는 경우에는 학습 속도(learning rate)를 조정하여 정규화를 할 수 있다. 이는 <code>필요한 멤버의 수를 강제로 증가시켜서 과최적화를 막는 역할을 한다.</code> 즉, 새롭게 적용되는 모형에 대한 가중치를 줄여서 동일한 모형의 횟수를 거치더라도 가중치가 크게 영향을 받지 않도록 하여 과최적화를 없애는 방법이다.</li></ul><script type="math/tex; mode=display">C_m = C_{m-1} + \mu \alpha_m k_m</script><ul><li><code>AdaBoostClassifier</code> 클래스에서는  <code>learning_rate</code>인수를 1보다 적게 주면 새로운 멤버의 가중치를 강제로 낮춘다.</li></ul><h3 id="그레디언트-부스팅-Gradient-boosting"><a href="#그레디언트-부스팅-Gradient-boosting" class="headerlink" title="그레디언트 부스팅 (Gradient boosting)"></a>그레디언트 부스팅 (Gradient boosting)</h3><ul><li>기본적으로 부스팅은 다음 Round에서 이전에 잘못 예측한 데이터들에 대한 처리를 어떻게 하느냐에 따라 종류별로 차이가 존재한다. <code>Gradient Boosting은 이전 Round의 분류기로 예측한 error를 다음 Round의 분류기가 예측할 수 있도록 학습하면서 진행</code>한다.</li></ul><p><img src="/image/what_is_gradient_boosting.png" alt="그레디언트 부스팅의 개념"></p><ul><li>이전 모델의 error를 다음 모델이 예측할 수 있게끔 학습시켜 해당 분류기들의 학습된 결과를 계속해서 합해 나가면 마지막에는 최소한의 error만 남으므로, error를 최대한 줄일 수 있게 된다.</li></ul><p><img src="/image/gradient_boosting_method_principal.png" alt="그레디언트 부스팅의 원리"></p><ul><li>위에서 언급했던 것과 같이 error를 예측하게 하므로 이해하기 쉽게 regression을 통한 예시로 설명하겠다. 처음 모델의 error를 다음 모델은 예측하도록 학습하므로 이전 모델보다 오차가 더 줄어들 것이다. 그 다음 모델도 이전 모델의 오차를 학습하게 되므로 더 오차가 줄어들 것이다. 이렇게 최종적으로는 error가 최대한 0에 가까워질 때 까지 학습하여 train set에 대해서는 과최적화가 이루어 질 것이다.</li></ul><p><img src="/image/gradient_boosting_steps.png" alt="그레디언트 부스팅의 이해"></p><ul><li>최종적으로는 학습 데이터에 대한 error를 작게 하는 것이므로 아래 그림에서와 같이 negative gradient를 최소화시키면서 학습 될 것이다.</li></ul><p><img src="/image/cost_function_with_gradient_boosting.png" alt="그레디언트 부스팅의 cost function"></p><ul><li>위의 그림에서 볼 수 있듯이 <code>그레디언트 부스트 모형은 변분법(calculus of variations)을 사용한 모형</code>이다. 학습 $f(x)$를 최소화하는 $x$는 다음과 같이 gradient descent 방법으로 찾을 수 있다.</li></ul><script type="math/tex; mode=display">x_{m} = x_{m-1} - \alpha_m \dfrac{df}{dx}</script><ul><li>그레디언트 부스트 모형에서는 손실 범함수(loss functional) $L(y, C_{m-1})$을 최소화하는 개별 분류함수 $k_{m}$를 찾는다. 이론적으로 가장 최적의 함수는 범함수의 미분이다.</li></ul><script type="math/tex; mode=display">C_{m} = C_{m-1} - \alpha_m \dfrac{\delta L(y, C_{m-1})}{\delta C_{m-1}} = C_{m-1} + \alpha_m k_m</script><ul><li><code>따라서 그레디언트 부스트 모형은 분류/회귀 문제에 상관없이 개별 멤버 모형으로 회귀분석 모형을 사용</code>한다. 가장 많이 사용되는 회귀분석 모형은 의사결정 회귀나무(decision tree regression model)모형이다.</li></ul><ul><li><p>그레디언트 부스트 모형에서는 다음과 같은 과정을 반복하여 멤버와 그 가중치를 계산한다.</p><ul><li><ol><li>$-\tfrac{\delta L(y, C_m)}{\delta C_m}$를 목표값으로 개별 멤버 모형 $k_{m}$을 찾는다.</li></ol></li><li><ol><li>$ \left( y - (C_{m-1} + \alpha_m k_m) \right)^2 $ 를 최소화하는 스텝사이즈 $\alpha_{m}$을 찾는다.</li></ol></li><li><ol><li>$ C_m = C_{m-1} + \alpha_m k_m $ 최종 모형을 갱신한다.</li></ol></li></ul></li><li><p>만약 손실 범함수가 오차 제곱 형태라면</p></li></ul><script type="math/tex; mode=display">L(y, C_{m-1}) = \dfrac{1}{2}(y - C_{m-1})^2</script><ul><li>범함수의 미분은 실제 목표값 $y$와 $C_{m-1}$과의 차이 즉, 잔차(residual)가 된다.</li></ul><script type="math/tex; mode=display">-\dfrac{dL(y, C_m)}{dC_m} = y - C_{m-1}</script><ul><li><p>Scikit-Learn의 GradientBoostingClassifier는 약한 학습기의 순차적인 예측 오류 보정을 통해 학습을 수행하므로 멀티 CPU 코어 시스템을 사용하더라도 병렬처리가 지원되지 않아서 대용량 데이터의 경우 학습에 매우 많은 시간이 필요하다. 또한 일반적으로 GBM이 랜덤 포레스트보다는 예측 성능이 조금 뛰어난 경우가 많다. 그러나 수행시간이 오래 걸리고, 하이퍼 파라미터 튜닝 노력도 더 필요하다.</p></li><li><p><code>loss</code>: 경사 하강법에서 사용할 비용 함수를 저장한다. 특별한 이유가 없으면 default인 ‘deviance’를 그대로 적용한다.</p></li></ul><ul><li><code>learning_rate</code>: GBM이 학습을 진행할 때마다 적용하는 학습률이다. Weak learner가 순차적으로 오류 값을 보정해 나가는 데 적용하는 계수이다. 0~1 사이의 값을 지정할 수 있으며 default=0.1이다. 너무 작은 값을 적용하면 업데이트 되는 값이 작아져서 최소 오류 값을 찾아 예측 성능이 높아질 가능성이 높다. 하지만 많은 weak learner는 순차적인 반복이 필요해서 수행 시간이 오래 걸리고, 또 너무 작게 설정하면 모든 weak learner의 반복이 완료돼도 최소 오류 값을 찾지 못할 수 있다. 반대로 큰 값을 적용하면 최소 오류 값을 찾지 못하고 그냥 지나챠 버려 예측 성능이 떨어질 가능성이 높아지지만, 빠른 수행이 가능하다. <code>이러한 특성 때문에 learning_rate는 n_estimators와 상호 보완적으로 조합해 사용한다. learning_rate를 작게하고 n_estimators를 크게 하면 더 이상 성능이 좋아지지 않는 한계점까지는 예측 성능이 조금씩 좋아질 수 있다.</code></li></ul><ul><li><code>subsample</code>: weak learner가 학습에 사용하는 데이터의 샘플링 비율이다. default=1이며, 이는 전체 학습 데이터를 기반으로 학습한다는 의미이다.(0.5이면 학습데이터의 50%를 의미) 과적합이 염려되는 경우 subsample을 1보다 작은 값으로 설정한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line">model_grad = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=0)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">model_grad.fit(X, y)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CPU <span class="built_in">times</span>: user 50 ms, sys: 0 ns, total: 50 ms</span><br><span class="line">Wall time: 50.4 ms</span><br><span class="line"></span><br><span class="line">GradientBoostingClassifier(criterion=<span class="string">'friedman_mse'</span>, init=None,</span><br><span class="line">                           learning_rate=0.1, loss=<span class="string">'deviance'</span>, max_depth=2,</span><br><span class="line">                           max_features=None, max_leaf_nodes=None,</span><br><span class="line">                           min_impurity_decrease=0.0, min_impurity_split=None,</span><br><span class="line">                           min_samples_leaf=1, min_samples_split=2,</span><br><span class="line">                           min_weight_fraction_leaf=0.0, n_estimators=100,</span><br><span class="line">                           n_iter_no_change=None, presort=<span class="string">'auto'</span>,</span><br><span class="line">                           random_state=0, subsample=1.0, tol=0.0001,</span><br><span class="line">                           validation_fraction=0.1, verbose=0,</span><br><span class="line">                           warm_start=False)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_result(model_grad)</span><br></pre></td></tr></table></figure><p><img src="/image/result_of_gradient_boost_plot_decision_boundary.png" alt="그레디언트 부스트의 decision boundary"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(121)</span><br><span class="line">plot_result(model_grad.estimators_[3][0])</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_result([model_grad.estimators_[0][0],</span><br><span class="line">             model_grad.estimators_[1][0],</span><br><span class="line">             model_grad.estimators_[2][0],</span><br><span class="line">             model_grad.estimators_[3][0]])</span><br></pre></td></tr></table></figure><p><img src="/image/result_of_gradient_boost_plot_decision_boundary_01.png" alt="그레디언트 부스트에 사용된 모형들의 4번째 까지의 각각의 decision decision boundary"></p><h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p><img src="/image/XGBoost_conception.png" alt="XGBoost 개념"></p><ul><li>XGboost는 GBM에 기반하고 있지만, GBM의 단점인 느린 수행 시간 및 과적합 규제(Regularization) 부재 등의 문제를 해결해서 매우 각광을 받고 있다. 특히 XGBoost는 병렬 CPU 환경에서 병렬 학습이 가능해 기존 GBM보다 빠르게 학습을 완료할 수 있다. 다음은 XGboost의 장점이다.</li></ul><div class="table-container"><table><thead><tr><th>항목</th><th>설명</th></tr></thead><tbody><tr><td>뛰어난 예측성능</td><td>일반적으로 분류와 회귀 영역에서 뛰어난 예측 성능을 발휘한다.</td></tr><tr><td>GBM 대비 빠른 수행 시간</td><td>일반적인 GBM은 순차적으로 Weak Learner가 가중치를 증감하는 방법으로 학습하기 때문에 전반적으로 \\ 속도가 느리다. 하지만 XGBoost는 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행 성능을 보장한다.\\ 아쉽게도 XGBoost가 일반적인 GBM에 비해 수행 시간이 빠르다는 것이지, 다른 머신러닝 알고리즘\\ (예를 들어 랜덤 포레스트)에 비해서 빠르다는 의미는 아니다.</td></tr><tr><td>과적합 규제\\ (Regularization)</td><td>표준 GBM의 경우 과적합 규제 기능이 없으나 XGBoost는 자체에 과적합 규제 기능으로 과적합에 \\ 좀 더 강한 내구성을 가질 수 있다.</td></tr><tr><td>Tree pruning (나무 가지치기)</td><td>일반적으로 GBM은 분할 시 부정 손실이 발생하면 분할을 더 이상 수행하지 않지만, 이러한 방식도 자칫\\ 지나치게 많은 분할을 발생할 수 있다. 다른 GBM과 마찬가지로 XGBoost도 max_depth 파라미터로 \\ 분할 깊이를 조정하기도 하지만, tree pruning으로 더 이상 긍정 이득이 없는 분할을 가지치기 해서\\ 분할 수를 더 줄이는 추가적인 장점을 가지고 있다.</td></tr><tr><td>자체 내장된 교차 검증</td><td>XGBoost는 반복 수행 시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차 검증을 수행해\\ 최적화된 반복 수행 횟수를 가질 수 있다. 지정된 반복 횟수가 아니라 교차 검증을 통해 평가 데이터 세트의\\ 평가값이 최적화 되면 반복을 중간에 멈출 수 있는 조기 중단 기능이 있다.</td></tr><tr><td>결손값 자체 처리</td><td>XGBoost는 결손값을 자체 처리할 수 있는 기능을 가지고 있다.</td></tr></tbody></table></div><p><img src="/image/XGboost_better_than_gbm.png" alt="XGBoost의 장점"></p><ul><li>XGBoost의 핵심 라이브러리는 C/C++로 작성돼 있다. XGBoost 개발 그룹은 파이썬에서도 XGBoost를 구동할 수 있도록 파이썬 패키지를 제공한다. 이 파이썬 패키지의 역할은 대부분 C/C++ 핵심 라이브러리를 호출하는 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># window용</span></span><br><span class="line"><span class="comment"># conda install -c anaconda py-xgboost</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Linux용</span></span><br><span class="line">conda install -c conda-forge xgboost</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import xgboost</span><br><span class="line"></span><br><span class="line">model_xgb = xgboost.XGBClassifier(n_estimators=100, max_depth=1, random_state=0)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/02/machine_learning_14/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 05 - 트리(Tree)</title>
      <link>https://heung-bae-lee.github.io/2020/05/02/data_structure_06/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/02/data_structure_06/</guid>
      <pubDate>Sat, 02 May 2020 10:27:21 GMT</pubDate>
      <description>
      
        
        
          &lt;hr&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;
        
      
      </description>
      
      
      <content:encoded><![CDATA[<hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><hr><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><hr>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/02/data_structure_06/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 04 - 해쉬 테이블</title>
      <link>https://heung-bae-lee.github.io/2020/04/30/data_structure_05/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/30/data_structure_05/</guid>
      <pubDate>Thu, 30 Apr 2020 14:32:54 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;대표적인-데이터-구조6-해쉬-테이블-Hash-Table&quot;&gt;&lt;a href=&quot;#대표적인-데이터-구조6-해쉬-테이블-Hash-Table&quot; class=&quot;headerlink&quot; title=&quot;대표적인 데이터 구조6: 해쉬 테이블 (Hash Table
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="대표적인-데이터-구조6-해쉬-테이블-Hash-Table"><a href="#대표적인-데이터-구조6-해쉬-테이블-Hash-Table" class="headerlink" title="대표적인 데이터 구조6: 해쉬 테이블 (Hash Table)"></a>대표적인 데이터 구조6: 해쉬 테이블 (Hash Table)</h1><h2 id="1-해쉬-구조"><a href="#1-해쉬-구조" class="headerlink" title="1. 해쉬 구조"></a>1. 해쉬 구조</h2><ul><li>Hash Table: <code>키(Key)에 데이터(Value)를 저장하는 데이터 구조</code><ul><li>Key를 통해 바로 데이터를 받아올 수 있으므로, 속도가 획기적으로 빨라짐</li><li><code>파이썬 딕셔너리(Dictionary) 타입이 해쉬 테이블의 예</code>: Key를 가지고 바로 데이터(Value)를 꺼냄</li><li>보통 배열로 미리 Hash Table 사이즈만큼 생성 후에 사용 (공간과 탐색 시간을 맞바꾸는 기법)<ul><li>배열을 Hash Table을 만드는데 사용하지만, 여러 키에 해당하는 주소가 동일할 경우 충돌을 해결하기 위해 해쉬테이블의 공간을 늘림으로인해서 배열보단 많은 저장공간이 필요할 수 있기 때문</li></ul></li><li><font color="#BF360C">단, 파이썬에서는 해쉬를 별도 구현할 이유가 없음 - 딕셔너리 타입을 사용하면 됨</font></li></ul></li></ul><h2 id="2-알아둘-용어"><a href="#2-알아둘-용어" class="headerlink" title="2. 알아둘 용어"></a>2. 알아둘 용어</h2><ul><li>해쉬(Hash): 임의 값(데이터)을 고정 길이로 변환하는 것</li><li>해쉬 테이블(Hash Table): 키 값의 연산에 의해 직접 접근이 가능한 데이터 구조</li><li>해싱 함수(Hashing Function): Key에 대해 산술 연산을 이용해 데이터 위치를 찾을 수 있는 함수</li><li>해쉬 값(Hash Value) 또는 해쉬 주소(Hash Address): Key를 해싱 함수로 연산해서, 해쉬 값을 알아내고, 이를 기반으로 해쉬 테이블에서 해당 Key에 대한 데이터 위치를 일관성있게 찾을 수 있음</li><li>슬롯(Slot): 한 개의 데이터를 저장할 수 있는 공간</li><li>저장할 데이터에 대해 Key를 추출할 수 있는 별도 함수도 존재할 수 있음<br><img src="https://www.fun-coding.org/00_Images/hash.png" width="400"></li></ul><h3 id="3-간단한-해쉬-예"><a href="#3-간단한-해쉬-예" class="headerlink" title="3. 간단한 해쉬 예"></a>3. 간단한 해쉬 예</h3><h4 id="3-1-hash-table-만들기"><a href="#3-1-hash-table-만들기" class="headerlink" title="3.1. hash table 만들기"></a>3.1. hash table 만들기</h4><ul><li>참고: 파이썬 list comprehension - <a href="https://www.fun-coding.org/PL&amp;OOP5-2.html" target="_blank" rel="noopener">https://www.fun-coding.org/PL&amp;OOP5-2.html</a></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([i <span class="keyword">for</span> i <span class="keyword">in</span> range(10)])</span><br><span class="line">hash_table</span><br></pre></td></tr></table></figure><hr><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure><hr><h4 id="3-2-이번엔-초간단-해쉬-함수를-만들어보자"><a href="#3-2-이번엔-초간단-해쉬-함수를-만들어보자" class="headerlink" title="3.2. 이번엔 초간단 해쉬 함수를 만들어보자."></a>3.2. 이번엔 초간단 해쉬 함수를 만들어보자.</h4><ul><li>다양한 해쉬 함수 고안 기법이 있으며, 가장 간단한 방식이 Division 법 (나누기를 통한 나머지 값을 사용하는 기법)이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def hash_func(key):</span><br><span class="line">    <span class="built_in">return</span> key % 5</span><br></pre></td></tr></table></figure><hr><h4 id="3-3-해쉬-테이블에-저장해보겠다"><a href="#3-3-해쉬-테이블에-저장해보겠다" class="headerlink" title="3.3. 해쉬 테이블에 저장해보겠다."></a>3.3. 해쉬 테이블에 저장해보겠다.</h4><h2 id="데이터에-따라-필요시-key-생성-방법-정의가-필요함"><a href="#데이터에-따라-필요시-key-생성-방법-정의가-필요함" class="headerlink" title="- 데이터에 따라 필요시 key 생성 방법 정의가 필요함"></a>- 데이터에 따라 필요시 key 생성 방법 정의가 필요함</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data1 = <span class="string">'Andy'</span></span><br><span class="line">data2 = <span class="string">'Dave'</span></span><br><span class="line">data3 = <span class="string">'Trump'</span></span><br><span class="line">data4 = <span class="string">'Anthor'</span></span><br><span class="line"><span class="comment">## ord(): 문자의 ASCII(아스키)코드 리턴</span></span><br><span class="line"><span class="built_in">print</span> (ord(data1[0]), ord(data2[0]), ord(data3[0]))</span><br><span class="line"><span class="built_in">print</span> (ord(data1[0]), hash_func(ord(data1[0])))</span><br><span class="line"><span class="built_in">print</span> (ord(data1[0]), ord(data4[0]))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">65 68 84</span><br><span class="line">65 0</span><br><span class="line">65 65</span><br></pre></td></tr></table></figure><hr><ul><li>3.3.2. 해쉬 테이블에 값 저장 예<ul><li>data:value 와 같이 data 와 value를 넣으면, 해당 data에 대한 key를 찾아서, 해당 key에 대응하는 해쉬주소에 value를 저장하는 예</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def storage_data(data, value):</span><br><span class="line">    key = ord(data[0])</span><br><span class="line">    hash_address = hash_func(key)</span><br><span class="line">    hash_table[hash_address] = value</span><br></pre></td></tr></table></figure><hr><h4 id="3-4-해쉬-테이블에서-특정-주소의-데이터를-가져오는-함수도-만들어보자"><a href="#3-4-해쉬-테이블에서-특정-주소의-데이터를-가져오는-함수도-만들어보자" class="headerlink" title="3.4. 해쉬 테이블에서 특정 주소의 데이터를 가져오는 함수도 만들어보자."></a>3.4. 해쉬 테이블에서 특정 주소의 데이터를 가져오는 함수도 만들어보자.</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">storage_data(<span class="string">'Andy'</span>, <span class="string">'01055553333'</span>)</span><br><span class="line">storage_data(<span class="string">'Dave'</span>, <span class="string">'01044443333'</span>)</span><br><span class="line">storage_data(<span class="string">'Trump'</span>, <span class="string">'01022223333'</span>)</span><br><span class="line"><span class="comment"># storage_data('Anthor', '01046723456')</span></span><br></pre></td></tr></table></figure><hr><h4 id="3-5-실제-데이터를-저장하고-읽어보겠다"><a href="#3-5-실제-데이터를-저장하고-읽어보겠다" class="headerlink" title="3.5. 실제 데이터를 저장하고, 읽어보겠다."></a>3.5. 실제 데이터를 저장하고, 읽어보겠다.</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def get_data(data):</span><br><span class="line">    key = ord(data[0])</span><br><span class="line">    hash_address = hash_func(key)</span><br><span class="line">    <span class="built_in">return</span> hash_table[hash_address]</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_data(<span class="string">'Andy'</span>)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'01046723456'</span></span><br></pre></td></tr></table></figure><hr><h3 id="4-자료-구조-해쉬-테이블의-장단점과-주요-용도"><a href="#4-자료-구조-해쉬-테이블의-장단점과-주요-용도" class="headerlink" title="4. 자료 구조 해쉬 테이블의 장단점과 주요 용도"></a>4. 자료 구조 해쉬 테이블의 장단점과 주요 용도</h3><ul><li>장점<ul><li><code>데이터 저장/읽기 속도가 빠르다. (검색 속도가 빠르다.)</code></li><li>해쉬는 키에 대한 데이터가 있는지(중복) 확인이 쉬움</li></ul></li><li>단점<ul><li><code>일반적으로 저장공간이 좀더 많이 필요</code>하다.</li><li><strong><code>여러 키에 해당하는 주소가 동일할 경우 충돌을 해결하기 위한 별도 자료구조가 필요함</code></strong></li></ul></li><li>주요 용도<ul><li>검색이 많이 필요한 경우</li><li>저장, 삭제, 읽기가 빈번한 경우</li><li>캐쉬 구현시 (중복 확인이 쉽기 때문)</li></ul></li></ul><h3 id="5-프로그래밍-연습"><a href="#5-프로그래밍-연습" class="headerlink" title="5. 프로그래밍 연습"></a>5. 프로그래밍 연습</h3><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">연습1: 리스트 변수를 활용해서 해쉬 테이블 구현해보기</font></strong><br>1. 해쉬 함수: key % 8<br>2. 해쉬 키 생성: hash(data)</div><ul><li>밑에서 사용되는 hash함수는 python을 새로 시작할 때마다 출력되는 값이 변화되므로 주의하자.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([0 <span class="keyword">for</span> i <span class="keyword">in</span> range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">hash</span>(data)</span><br><span class="line"></span><br><span class="line">def hash_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    hash_address = hash_function(get_key(data))</span><br><span class="line">    hash_table[hash_address] = value</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    hash_address = hash_function(get_key(data))</span><br><span class="line">    <span class="built_in">return</span> hash_table[hash_address]</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_data(<span class="string">'Dave'</span>, <span class="string">'0102030200'</span>)</span><br><span class="line">save_data(<span class="string">'Andy'</span>, <span class="string">'01033232200'</span>)</span><br><span class="line">read_data(<span class="string">'Dave'</span>)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'0102030200'</span></span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hash_table</span><br></pre></td></tr></table></figure><hr><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0, 0, <span class="string">'01033232200'</span>, 0, 0, 0, <span class="string">'0102030200'</span>, 0]</span><br></pre></td></tr></table></figure><hr><h3 id="6-충돌-Collision-해결-알고리즘-좋은-해쉬-함수-사용하기"><a href="#6-충돌-Collision-해결-알고리즘-좋은-해쉬-함수-사용하기" class="headerlink" title="6. 충돌(Collision) 해결 알고리즘 (좋은 해쉬 함수 사용하기)"></a>6. 충돌(Collision) 해결 알고리즘 (좋은 해쉬 함수 사용하기)</h3><blockquote><p><code>해쉬 테이블의 가장 큰 문제</code>는 <code>충돌(Collision)의 경우</code>이다.<br>이 문제를 충돌(Collision) 또는 해쉬 충돌(Hash Collision)이라고 부릅니다.</p></blockquote><h4 id="6-1-Chaining-기법"><a href="#6-1-Chaining-기법" class="headerlink" title="6.1. Chaining 기법"></a>6.1. Chaining 기법</h4><ul><li><strong>개방 해슁 또는 Open Hashing 기법</strong> 중 하나: <code>해쉬 테이블 저장공간 외의 공간을 활용하는 기법</code></li><li>충돌이 일어나면, 링크드 리스트라는 자료 구조를 사용해서, 링크드 리스트로 데이터를 추가로 뒤에 연결시켜서 저장하는 기법</li></ul><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">연습2: 연습1의 해쉬 테이블 코드에 Chaining 기법으로 충돌해결 코드를 추가해보기</font></strong><br>1. 해쉬 함수: key % 8<br>2. 해쉬 키 생성: hash(data)</div><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([0 <span class="keyword">for</span> i range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">hash</span>(data)</span><br><span class="line"></span><br><span class="line">def hashing_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    key=get(data)</span><br><span class="line">    hash_address=hashing_function(key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        hash_table[hash_address]=value</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prev_value = hash_table[hash_address]</span><br><span class="line">        hash_table[hash_address] = [prev_value, value]</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    key=get(data)</span><br><span class="line">    hash_address=hashing_function(key)</span><br><span class="line">    <span class="built_in">return</span> hash_table[hash_address]</span><br></pre></td></tr></table></figure><hr><ul><li>내가 만든 함수는 동일한 키값에 여러 데이터를 가지고 있으므로 Chaining 기법으로 충동해결을 방지한 코드가 아니다. 리스트를 사용하는 것은 좋았으나, 아래와 같이 <code>해당 key값을 갖고있어야 동일한 키값이 아닌 각각 고유의 키값을 가져 데이터를 읽어들일 경우에도 정확하게 특정 키값에 해당하는 데이터만을 불러올 수 있기 때문</code>이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([0 <span class="keyword">for</span> i <span class="keyword">in</span> range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">hash</span>(data)</span><br><span class="line"></span><br><span class="line">def hash_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="comment"># 해당 hash_address값에 이미 데이터가 존재하는 경우 키값과 동일한 것이라면</span></span><br><span class="line">        <span class="comment"># 값을 저장하지만 그렇지 않다면 리스트로 [index, value]형식으로 저장</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(hash_table[hash_address])):</span><br><span class="line">            <span class="keyword">if</span> hash_table[hash_address][index][0] == index_key:</span><br><span class="line">                hash_table[hash_address][index][1] = value</span><br><span class="line">                <span class="built_in">return</span></span><br><span class="line">        hash_table[hash_address].append([index_key, value])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        hash_table[hash_address] = [[index_key, value]]</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(hash_table[hash_address])):</span><br><span class="line">            <span class="keyword">if</span> hash_table[hash_address][index][0] == index_key:</span><br><span class="line">                <span class="built_in">return</span> hash_table[hash_address][index][1]</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">return</span> None</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'Dave'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'Da'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'Data'</span>) % 8)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">6</span><br><span class="line">6</span><br><span class="line">3</span><br></pre></td></tr></table></figure><hr><ul><li>동일한 hash함수 값을 갖지만 동일 주소에서 저장을하지만 키값을 다르게 하여 저장한다. 또한, 현재는 Chaning 기법을 사용했으므 링크드리스트로 동일한 주소안에 저장되어 연결되어있음을 확인하자.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">save_data(<span class="string">'Da'</span>, <span class="string">'1201023010'</span>)</span><br><span class="line">save_data(<span class="string">'Dave'</span>, <span class="string">'3301023010'</span>)</span><br><span class="line"><span class="built_in">print</span>(read_data(<span class="string">'Da'</span>))</span><br><span class="line"><span class="built_in">print</span>(read_data(<span class="string">'Dave'</span>))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1201023010</span><br><span class="line">3301023010</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hash_table</span><br></pre></td></tr></table></figure><hr><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[0,</span><br><span class="line"> 0,</span><br><span class="line"> 0,</span><br><span class="line"> 0,</span><br><span class="line"> 0,</span><br><span class="line"> 0,</span><br><span class="line"> [[-8338113502661437674, <span class="string">'1201023010'</span>], [909867193312922558, <span class="string">'3301023010'</span>]],</span><br><span class="line"> 0]</span><br></pre></td></tr></table></figure><hr><h4 id="6-2-Linear-Probing-기법"><a href="#6-2-Linear-Probing-기법" class="headerlink" title="6.2. Linear Probing 기법"></a>6.2. Linear Probing 기법</h4><ul><li><strong>폐쇄 해슁 또는 Close Hashing 기법</strong> 중 하나: <code>해쉬 테이블 저장공간 안에서 충돌 문제를 해결하는 기법</code></li><li>충돌이 일어나면, 해당 hash address의 다음 address부터 맨 처음 나오는 빈공간에 저장하는 기법<ul><li><code>저장공간 활용도를 높이기 위한 기법</code></li></ul></li></ul><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">연습3: 연습1의 해쉬 테이블 코드에 Linear Probling 기법으로 충돌해결 코드를 추가해보기</font></strong><br>1. 해쉬 함수: key % 8<br>2. 해쉬 키 생성: hash(data)</div><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([0 <span class="keyword">for</span> i <span class="keyword">in</span> range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">hash</span>(data)</span><br><span class="line"></span><br><span class="line">def hash_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    index_key=get_key(data)</span><br><span class="line">    hash_address=hash_function(index_key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] !=0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                hash_table[index] = [index_key, value]</span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                hash_table[index][1] = value</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        hash_table[hash_address] = [index_key, value]</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    index_key=get_key(data)</span><br><span class="line">    hash_address=hash_function(index_key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] == 0:</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                <span class="built_in">return</span> None</span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                <span class="built_in">return</span> hash_table[index][1]</span><br></pre></td></tr></table></figure><hr><ul><li><code>read_data</code>함수에서 linear probling 기법은 해당 hash_address에 값이 저장되어있다면(동일한 키값은 업데이트하지만) 다음 주소에 값을 저장하므로 만약 다음 주소 중 0값이 있다면 해당 데이터를 저장한적이 없다는 것을 의미하므로 그에 해당하는 코드도 작성해주어야 함을 유의하자!</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([0 <span class="keyword">for</span> i <span class="keyword">in</span> range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">hash</span>(data)</span><br><span class="line"></span><br><span class="line">def hash_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                hash_table[index] = [index_key, value]</span><br><span class="line">                <span class="built_in">return</span></span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                hash_table[index][1] = value</span><br><span class="line">                <span class="built_in">return</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        hash_table[hash_address] = [index_key, value]</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                <span class="built_in">return</span> None</span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                <span class="built_in">return</span> hash_table[index][1]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">return</span> None</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'dk'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'dw'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'dc'</span>) % 8)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">2</span><br><span class="line">2</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">save_data(<span class="string">'dk'</span>, <span class="string">'01200123123'</span>)</span><br><span class="line">save_data(<span class="string">'dw'</span>, <span class="string">'3333333333'</span>)</span><br><span class="line">save_data(<span class="string">'dc'</span>, <span class="string">'23456781234'</span>)</span><br><span class="line">read_data(<span class="string">'dc'</span>)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'23456781234'</span></span><br></pre></td></tr></table></figure><hr><h4 id="6-3-빈번한-충돌을-개선하는-기법"><a href="#6-3-빈번한-충돌을-개선하는-기법" class="headerlink" title="6.3. 빈번한 충돌을 개선하는 기법"></a>6.3. 빈번한 충돌을 개선하는 기법</h4><ul><li>해쉬 함수을 재정의 및 해쉬 테이블 저장공간을 확대<ul><li>예를 들어 저장하고자하는 데이터가 8개라면 이에 2배에 해당하는 공간을 해쉬 테이블 구조로 저장공간을 만들어 두는 것이 일반적이다.</li></ul></li><li>예:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">16</span>)])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hash_function</span><span class="params">(key)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> key % <span class="number">16</span></span><br></pre></td></tr></table></figure><h3 id="참고-해쉬-함수와-키-생성-함수"><a href="#참고-해쉬-함수와-키-생성-함수" class="headerlink" title="참고: 해쉬 함수와 키 생성 함수"></a>참고: 해쉬 함수와 키 생성 함수</h3><ul><li><code>파이썬의 hash() 함수는 실행할 때마다, 값이 달라질 수 있음</code></li><li>유명한 해쉬 함수들이 있음: SHA(Secure Hash Algorithm, 안전한 해시 알고리즘)<ul><li>어떤 데이터도 유일한 고정된 크기의 고정값을 리턴해주므로, 해쉬 함수로 유용하게 활용 가능</li></ul></li></ul><h4 id="SHA-1"><a href="#SHA-1" class="headerlink" title="SHA-1"></a>SHA-1</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import hashlib</span><br><span class="line"></span><br><span class="line">data = <span class="string">'test'</span>.encode()</span><br><span class="line">hash_object = hashlib.sha1()</span><br><span class="line">hash_object.update(data)</span><br><span class="line">hex_dig = hash_object.hexdigest()</span><br><span class="line"><span class="built_in">print</span> (hex_dig)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a94a8fe5ccb19ba61c4c0873d391e987982fbbd3</span><br></pre></td></tr></table></figure><hr><h4 id="SHA-256"><a href="#SHA-256" class="headerlink" title="SHA-256"></a>SHA-256</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import hashlib</span><br><span class="line"></span><br><span class="line">data = <span class="string">'test'</span>.encode()</span><br><span class="line">hash_object = hashlib.sha256()</span><br><span class="line">hash_object.update(data)</span><br><span class="line">hex_dig = hash_object.hexdigest()</span><br><span class="line"><span class="built_in">print</span> (hex_dig)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08</span><br></pre></td></tr></table></figure><hr><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">연습4: 연습2의 Chaining 기법을 적용한 해쉬 테이블 코드에 키 생성 함수를 sha256 해쉬 알고리즘을 사용하도록 변경해보기</font></strong><br>1. 해쉬 함수: key % 8<br>2. 해쉬 키 생성: hash(data)</div><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import hashlib</span><br><span class="line"></span><br><span class="line">hash_table = list([0 <span class="keyword">for</span> i <span class="keyword">in</span> range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">        hash_object = hashlib.sha256()</span><br><span class="line">        hash_object.update(data.encode())</span><br><span class="line">        hex_dig = hash_object.hexdigest()</span><br><span class="line">        <span class="built_in">return</span> int(hex_dig, 16)</span><br><span class="line"></span><br><span class="line">def hash_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                hash_table[index] = [index_key, value]</span><br><span class="line">                <span class="built_in">return</span></span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                hash_table[index][1] = value</span><br><span class="line">                <span class="built_in">return</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        hash_table[hash_address] = [index_key, value]</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                <span class="built_in">return</span> None</span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                <span class="built_in">return</span> hash_table[index][1]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">return</span> None</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (get_key(<span class="string">'dw'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (get_key(<span class="string">'deo'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (get_key(<span class="string">'dh'</span>) % 8)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">0</span><br><span class="line">0</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_data(<span class="string">'deo'</span>, <span class="string">'01200123123'</span>)</span><br><span class="line">save_data(<span class="string">'dh'</span>, <span class="string">'3333333333'</span>)</span><br><span class="line">read_data(<span class="string">'dh'</span>)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'3333333333'</span></span><br></pre></td></tr></table></figure><hr><h3 id="7-시간-복잡도"><a href="#7-시간-복잡도" class="headerlink" title="7. 시간 복잡도"></a>7. 시간 복잡도</h3><ul><li><code>일반적인 경우(Collision이 없는 경우)는 O(1)</code></li><li>최악의 경우(Collision이 모두 발생하는 경우)는 O(n)</li></ul><blockquote><p>해쉬 테이블의 경우, 일반적인 경우를 기대하고 만들기 때문에, 시간 복잡도는 O(1) 이라고 말할 수 있음</p></blockquote><h3 id="검색에서-해쉬-테이블의-사용-예"><a href="#검색에서-해쉬-테이블의-사용-예" class="headerlink" title="검색에서 해쉬 테이블의 사용 예"></a>검색에서 해쉬 테이블의 사용 예</h3><ul><li>16개의 배열에 데이터를 저장하고, 검색할 때 O(n)</li><li>16개의 데이터 저장공간을 가진 위의 해쉬 테이블에 데이터를 저장하고, 검색할 때 O(1)</li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/30/data_structure_05/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 03 - 시간복잡도</title>
      <link>https://heung-bae-lee.github.io/2020/04/30/data_structure_04/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/30/data_structure_04/</guid>
      <pubDate>Thu, 30 Apr 2020 09:12:50 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;알고리즘-복잡도-표현-방법&quot;&gt;&lt;a href=&quot;#알고리즘-복잡도-표현-방법&quot; class=&quot;headerlink&quot; title=&quot;알고리즘 복잡도 표현 방법&quot;&gt;&lt;/a&gt;알고리즘 복잡도 표현 방법&lt;/h2&gt;&lt;h3 id=&quot;1-알고리즘-복잡도-계산이-필요
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="알고리즘-복잡도-표현-방법"><a href="#알고리즘-복잡도-표현-방법" class="headerlink" title="알고리즘 복잡도 표현 방법"></a>알고리즘 복잡도 표현 방법</h2><h3 id="1-알고리즘-복잡도-계산이-필요한-이유"><a href="#1-알고리즘-복잡도-계산이-필요한-이유" class="headerlink" title="1. 알고리즘 복잡도 계산이 필요한 이유"></a>1. 알고리즘 복잡도 계산이 필요한 이유</h3><h4 id="하나의-문제를-푸는-알고리즘은-다양할-수-있음"><a href="#하나의-문제를-푸는-알고리즘은-다양할-수-있음" class="headerlink" title="하나의 문제를 푸는 알고리즘은 다양할 수 있음"></a>하나의 문제를 푸는 알고리즘은 다양할 수 있음</h4><ul><li>정수의 절대값 구하기<ul><li>1, -1 -&gt;&gt; 1</li><li>방법1: 정수값을 제곱한 값에 다시 루트를 씌우기</li><li>방법2: 정수가 음수인지 확인해서, 음수일 때만, -1을 곱하기</li></ul></li></ul><blockquote><p>다양한 알고리즘 중 어느 알고리즘이 더 좋은지를 분석하기 위해, 복잡도를 정의하고 계산함</p></blockquote><h3 id="2-알고리즘-복잡도-계산-항목"><a href="#2-알고리즘-복잡도-계산-항목" class="headerlink" title="2. 알고리즘 복잡도 계산 항목"></a>2. 알고리즘 복잡도 계산 항목</h3><ol><li><strong>시간 복잡도</strong>: 알고리즘 실행 속도</li><li><strong>공간 복잡도</strong>: 알고리즘이 사용하는 메모리 사이즈</li></ol><blockquote><p>가장 중요한 시간 복잡도를 꼭 이해하고 계산할 수 있어야 함</p></blockquote><h3 id="알고리즘-시간-복잡도의-주요-요소"><a href="#알고리즘-시간-복잡도의-주요-요소" class="headerlink" title="알고리즘 시간 복잡도의 주요 요소"></a>알고리즘 시간 복잡도의 주요 요소</h3><blockquote><p><code>반복문이 지배</code>한다.</p></blockquote><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">생각해보기: 자동차로 서울에서 부산을 가기 위해, 다음과 같이 항목을 나누었을 때, 가장 총 시간에 영향을 많이 미칠 것 같은 요소는?</font></strong><br> 5번!!!* 예:  - 자동차로 서울에서 부산가기    1. 자동차 문열기    2. 자동차 문닫기    3. 자동차 운전석 등받이 조정하기    4. 자동차 시동걸기    5. `자동차로 서울에서 부산가기`    6. 자동차 시동끄기    7. 자동차 문열기    8. 자동차 문닫기</div><h3 id="마찬가지로-프로그래밍에서-시간-복잡도에-가장-영향을-많이-미치는-요소는-반복문"><a href="#마찬가지로-프로그래밍에서-시간-복잡도에-가장-영향을-많이-미치는-요소는-반복문" class="headerlink" title="마찬가지로, 프로그래밍에서 시간 복잡도에 가장 영향을 많이 미치는 요소는 반복문"></a>마찬가지로, 프로그래밍에서 시간 복잡도에 가장 영향을 많이 미치는 요소는 반복문</h3><ul><li>입력의 크기가 커지면 커질수록 반복문이 알고리즘 수행 시간을 지배함</li></ul><h3 id="알고리즘-성능-표기법"><a href="#알고리즘-성능-표기법" class="headerlink" title="알고리즘 성능 표기법"></a>알고리즘 성능 표기법</h3><ul><li><p><code>Big O (빅-오) 표기법</code>: O(N)</p><ul><li>알고리즘 <code>최악의 실행 시간을 표기</code></li><li><strong>가장 많이/일반적으로 사용함</strong></li><li><strong>아무리 최악의 상황이라도, <code>이정도의 성능은 보장한다는 의미이기 때문</code></strong></li></ul></li><li><p>Ω (오메가) 표기법:  Ω(N)</p><ul><li>오메가 표기법은 알고리즘 최상의 실행 시간을 표기</li></ul></li><li><p>Θ (세타) 표기법: Θ(N)</p><ul><li>오메가 표기법은 알고리즘 평균 실행 시간을 표기</li></ul></li></ul><blockquote><p>시간 복잡도 계산은 반복문이 핵심 요소임을 인지하고, 계산 표기는 최상, 평균, 최악 중, 최악의 시간인 Big-O 표기법을 중심으로 익히면 됨</p></blockquote><h3 id="3-대문자-O-표기법"><a href="#3-대문자-O-표기법" class="headerlink" title="3. 대문자 O 표기법"></a>3. 대문자 O 표기법</h3><ul><li>빅 오 표기법, Big-O 표기법 이라고도 부름</li><li><p>O(입력)</p><ul><li>입력 n 에 따라 결정되는 시간 복잡도 함수</li><li>O(1), O($log n$), O(n), O(n$log n$), O($n^2$), O($2^n$), O(n!)등으로 표기함</li><li>입력 n 의 크기에 따라 기하급수적으로 시간 복잡도가 늘어날 수 있음<ul><li><strong>O(1) &lt; O($log n$) &lt; O(n) &lt; O(n$log n$) &lt; O($n^2$) &lt; O($2^n$) &lt; O(n!)</strong><ul><li>참고: log n 의 베이스는 2 : $log_2 n$</li></ul></li></ul></li></ul></li><li><p>단순하게 입력 n에 따라, 몇번 실행이 되는지를 계산하면 됩니다.</p><ul><li><strong>표현식에 가장 큰 영향을 미치는 n 의 단위로 표기합니다.</strong></li><li><p>n이 1이든 100이든, 1000이든, 10000이든 실행을</p><ul><li><p>무조건 2회(상수회) 실행한다: O(1)</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> n &gt; <span class="number">10</span>:</span><br><span class="line">     print(n)</span><br></pre></td></tr></table></figure></li><li><p>n에 따라, n번, n + 10 번, 또는 3n + 10 번등 실행한다: O(n)</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">variable = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(n):</span><br><span class="line">         print(index)</span><br></pre></td></tr></table></figure></li><li><p>n에 따라, $n^2$번, $n^2$ + 1000 번, 100$n^2$ - 100, 또는 300$n^2$ + 1번등 실행한다: O($n^2$)</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">            variable = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">                <span class="keyword">for</span> num <span class="keyword">in</span> range(n):</span><br><span class="line">                    <span class="keyword">for</span> index <span class="keyword">in</span> range(n):</span><br><span class="line">                         print(index)</span><br><span class="line">       ```    </span><br><span class="line"></span><br><span class="line">&lt;img src=<span class="string">"http://www.fun-coding.org/00_Images/bigo.png"</span> width=<span class="number">400</span>/&gt;</span><br><span class="line"></span><br><span class="line">* 빅 오 입력값 표기 방법</span><br><span class="line">  - 예:</span><br><span class="line">    - 만약 시간 복잡도 함수가 <span class="number">2</span>$n^<span class="number">2</span>$ + <span class="number">3</span>n 이라면</span><br><span class="line">      - 가장 높은 차수는 <span class="number">2</span>$n^<span class="number">2</span>$</span><br><span class="line">      - 상수는 실제 큰 영향이 없음</span><br><span class="line">      - 결국 빅 오 표기법으로는 O($n^<span class="number">2</span>$) (서울부터 부산까지 가는 자동차의 예를 상기)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 4. 실제 알고리즘을 예로 각 알고리즘의 시간 복잡도와 빅 오 표기법 알아보기</span></span><br><span class="line"></span><br><span class="line">&lt;div class="alert alert-block alert-warning"&gt;</span><br><span class="line">&lt;strong&gt;&lt;font color="blue" size="3em"&gt;연습1:  1부터 n까지의 합을 구하는 알고리즘 작성해보기&lt;/font&gt;&lt;/strong&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">### 알고리즘1: 1부터 n까지의 합을 구하는 알고리즘1</span></span><br><span class="line">* 합을 기록할 변수를 만들고 <span class="number">0</span>을 저장</span><br><span class="line">* n을 <span class="number">1</span>부터 <span class="number">1</span>씩 증가하면서 반복</span><br><span class="line">* 반복문 안에서 합을 기록할 변수에 <span class="number">1</span>씩 증가된 값을 더함</span><br><span class="line">* 반복이 끝나면 합을 출력</span><br><span class="line"></span><br><span class="line">--------</span><br><span class="line">``` bash</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum_all</span><span class="params">(n)</span>:</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        total += num</span><br><span class="line">    <span class="keyword">return</span> total</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">sum_all(100)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4.62 µs ± 217 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)</span><br></pre></td></tr></table></figure><h3 id="시간-복잡도-구하기"><a href="#시간-복잡도-구하기" class="headerlink" title="시간 복잡도 구하기"></a>시간 복잡도 구하기</h3><ul><li>1부터 n까지의 합을 구하는 알고리즘1<ul><li>입력 n에 따라 덧셈을 n 번 해야 함 (반복문!)</li><li>시간 복잡도: n, 빅 오 표기법으로는 <strong>O(n)</strong></li></ul></li></ul><ul><li>위의 함수의 시간복잡도는 O(n)이다. 그렇다면, 이보다 더 빠르게 함수를 작성할 순 없을까 우리는 이미 어렸을때 n번째 까지 수의 합을 구하는 공식을 알고있다.</li></ul><h3 id="알고리즘2-1부터-n까지의-합을-구하는-알고리즘2"><a href="#알고리즘2-1부터-n까지의-합을-구하는-알고리즘2" class="headerlink" title="알고리즘2: 1부터 n까지의 합을 구하는 알고리즘2"></a>알고리즘2: 1부터 n까지의 합을 구하는 알고리즘2</h3><ul><li><font size="5em">$\frac { n (n + 1) }{ 2 }$</font></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def sum_all(n):</span><br><span class="line">    <span class="built_in">return</span> int(n * (n + 1) / 2)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">sum_all(100)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">248 ns ± 3.71 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)</span><br></pre></td></tr></table></figure><hr><h3 id="시간-복잡도-구하기-1"><a href="#시간-복잡도-구하기-1" class="headerlink" title="시간 복잡도 구하기"></a>시간 복잡도 구하기</h3><ul><li>1부터 n까지의 합을 구하는 알고리즘2<ul><li>입력 n이 어떻든 간에, 곱셈/덧셈/나눗셈 하면 됨 (반복문이 없음!)</li><li>시간 복잡도: 1, 빅 오 표기법으로는 <strong>O(1)</strong></li></ul></li></ul><h3 id="어느-알고리즘이-성능이-좋은가요"><a href="#어느-알고리즘이-성능이-좋은가요" class="headerlink" title="어느 알고리즘이 성능이 좋은가요?"></a>어느 알고리즘이 성능이 좋은가요?</h3><ul><li>알고리즘1 vs 알고리즘2</li><li>O(n) vs O(1)</li></ul><blockquote><p>이와 같이, 동일한 문제를 푸는 알고리즘은 다양할 수 있음<br>어느 알고리즘이 보다 좋은지를 객관적으로 비교하기 위해, 빅 오 표기법등의 시간복잡도 계산법을 사용함</p></blockquote><h4 id="이후-자료구조-알고리즘부터는-빅-오-표기법으로-성능을-계산해보면서-빅-오-표기법과-계산방법에-익숙해지기로-하자"><a href="#이후-자료구조-알고리즘부터는-빅-오-표기법으로-성능을-계산해보면서-빅-오-표기법과-계산방법에-익숙해지기로-하자" class="headerlink" title="이후 자료구조, 알고리즘부터는 빅 오 표기법으로 성능을 계산해보면서, 빅 오 표기법과 계산방법에 익숙해지기로 하자."></a>이후 자료구조, 알고리즘부터는 빅 오 표기법으로 성능을 계산해보면서, 빅 오 표기법과 계산방법에 익숙해지기로 하자.</h4>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/30/data_structure_04/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 02 Linked List</title>
      <link>https://heung-bae-lee.github.io/2020/04/28/data_structure_03/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/28/data_structure_03/</guid>
      <pubDate>Mon, 27 Apr 2020 19:41:46 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;대표적인-데이터-구조-링크드-리스트-Linked-List&quot;&gt;&lt;a href=&quot;#대표적인-데이터-구조-링크드-리스트-Linked-List&quot; class=&quot;headerlink&quot; title=&quot;대표적인 데이터 구조: 링크드 리스트 (Linked L
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="대표적인-데이터-구조-링크드-리스트-Linked-List"><a href="#대표적인-데이터-구조-링크드-리스트-Linked-List" class="headerlink" title="대표적인 데이터 구조: 링크드 리스트 (Linked List)"></a>대표적인 데이터 구조: 링크드 리스트 (Linked List)</h1><h2 id="1-링크드-리스트-Linked-List-구조"><a href="#1-링크드-리스트-Linked-List-구조" class="headerlink" title="1. 링크드 리스트 (Linked List) 구조"></a>1. 링크드 리스트 (Linked List) 구조</h2><ul><li>연결 리스트라고도 함</li><li>배열은 순차적으로 연결된 공간에 데이터를 나열하는 데이터 구조<ul><li>그렇기 때문에 미리 연결된 공간을 예약을 해놓아야 한다는 것이 단점!</li></ul></li><li>링크드 리스트는 위와 같은 배열의 단점을 보완하고자 떨어진 곳에 존재하는 데이터를 화살표로 연결해서 관리하는 데이터 구조</li><li><font color="#BF360C">본래 C언어에서는 주요한 데이터 구조이지만, 파이썬은 리스트 타입이 링크드 리스트의 기능을 모두 지원</font></li><li><p>링크드 리스트 기본 구조와 용어</p><ul><li>노드(Node): 데이터 저장 단위 (데이터값, 포인터) 로 구성</li><li>포인터(pointer): 각 노드 안에서, 다음이나 이전의 노드와의 연결 정보를 가지고 있는 공간</li></ul></li></ul><p><br></p><ul><li>일반적인 링크드 리스트 형태<br><img src="https://www.fun-coding.org/00_Images/linkedlist.png"><br>(출처: wikipedia, <a href="https://en.wikipedia.org/wiki/Linked_list" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Linked_list</a>)</li></ul><h2 id="2-간단한-링크드-리스트-예"><a href="#2-간단한-링크드-리스트-예" class="headerlink" title="2. 간단한 링크드 리스트 예"></a>2. 간단한 링크드 리스트 예</h2><h3 id="Node-구현"><a href="#Node-구현" class="headerlink" title="Node 구현"></a>Node 구현</h3><ul><li>보통 파이썬에서 링크드 리스트 구현시, 파이썬 클래스를 활용함<ul><li>파이썬 객체지향 문법 이해 필요</li><li>참고: <a href="https://www.fun-coding.org/PL&amp;OOP1-3.html" target="_blank" rel="noopener">https://www.fun-coding.org/PL&amp;OOP1-3.html</a></li></ul></li></ul><h5 id="간단한-노드-1"><a href="#간단한-노드-1" class="headerlink" title="간단한 노드 1"></a>간단한 노드 1</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.next = None</span><br></pre></td></tr></table></figure><hr><h5 id="간단한-노드-2"><a href="#간단한-노드-2" class="headerlink" title="간단한 노드 2"></a>간단한 노드 2</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data, next=None):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.next = next</span><br></pre></td></tr></table></figure><hr><h5 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line">        self.__next=None</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def next(self):</span><br><span class="line">        <span class="built_in">return</span> self.__next</span><br><span class="line"></span><br><span class="line">    @next.setter</span><br><span class="line">    def next(self, n):</span><br><span class="line">        self.__next=n</span><br></pre></td></tr></table></figure><hr><h3 id="Node와-Node-연결하기-포인터-활용"><a href="#Node와-Node-연결하기-포인터-활용" class="headerlink" title="Node와 Node 연결하기 (포인터 활용)"></a>Node와 Node 연결하기 (포인터 활용)</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">node1 = Node(1)</span><br><span class="line">node2 = Node(2)</span><br><span class="line"><span class="built_in">print</span>(node1.data)</span><br><span class="line">node1.data=2</span><br><span class="line"><span class="built_in">print</span>(node1.data)</span><br><span class="line"></span><br><span class="line">node1.next = node2</span><br><span class="line"><span class="built_in">print</span>(node1.next.data)</span><br></pre></td></tr></table></figure><hr><h6 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">2</span><br></pre></td></tr></table></figure><hr><h3 id="3-링크드-리스트의-장단점-전통적인-C언어에서의-배열과-링크드-리스트"><a href="#3-링크드-리스트의-장단점-전통적인-C언어에서의-배열과-링크드-리스트" class="headerlink" title="3. 링크드 리스트의 장단점 (전통적인 C언어에서의 배열과 링크드 리스트)"></a>3. 링크드 리스트의 장단점 (전통적인 C언어에서의 배열과 링크드 리스트)</h3><ul><li>장점<ul><li>미리 데이터 공간을 미리 할당하지 않아도 됨<ul><li>배열은 <strong>미리 데이터 공간을 할당</strong> 해야 함</li></ul></li></ul></li><li>단점<ul><li>연결을 위한 별도 데이터 공간이 필요하므로, <code>저장공간 효율이 높지 않음</code></li><li>연결 정보를 찾는 시간이 필요하므로 <code>접근 속도가 느림</code> <strong>그에 반해 배열은 인덱싱을 통해 빠르게 접근할 수 있음</strong></li><li><code>중간 데이터 삭제시, 앞뒤 데이터의 연결을 재구성해야 하는 부가적인 작업 필요</code></li></ul></li></ul><h3 id="4-링크드-리스트의-복잡한-기능1-링크드-리스트-데이터-사이에-데이터를-추가"><a href="#4-링크드-리스트의-복잡한-기능1-링크드-리스트-데이터-사이에-데이터를-추가" class="headerlink" title="4. 링크드 리스트의 복잡한 기능1 (링크드 리스트 데이터 사이에 데이터를 추가)"></a>4. 링크드 리스트의 복잡한 기능1 (링크드 리스트 데이터 사이에 데이터를 추가)</h3><ul><li>링크드 리스트는 유지 관리에 부가적인 구현이 필요함</li></ul><p><img src="https://www.fun-coding.org/00_Images/linkedlistadd.png"><br>(출처: wikipedia, <a href="https://en.wikipedia.org/wiki/Linked_list" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Linked_list</a>)</p><h4 id="링크드-리스트의-사이에-데이터를-추가하기"><a href="#링크드-리스트의-사이에-데이터를-추가하기" class="headerlink" title="링크드 리스트의 사이에 데이터를 추가하기"></a>링크드 리스트의 사이에 데이터를 추가하기</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data, next=None):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.next = next</span><br><span class="line"></span><br><span class="line">def add(data):</span><br><span class="line">    node = head</span><br><span class="line">    <span class="keyword">while</span> node.next:</span><br><span class="line">        node = node.next</span><br><span class="line">    node.next = Node(data)</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">node1 = Node(1)</span><br><span class="line">head = node1</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(2, 10):</span><br><span class="line">    add(index)</span><br></pre></td></tr></table></figure><hr><h4 id="링크드-리스트-데이터-출력하기-검색하기"><a href="#링크드-리스트-데이터-출력하기-검색하기" class="headerlink" title="링크드 리스트 데이터 출력하기(검색하기)"></a>링크드 리스트 데이터 출력하기(검색하기)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">node = head</span><br><span class="line"><span class="keyword">while</span> node.next:</span><br><span class="line">    <span class="built_in">print</span>(node.data)</span><br><span class="line">    node = node.next</span><br><span class="line"><span class="built_in">print</span> (node.data)</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td></tr></table></figure><hr><ul><li>node3이 들어갈 자리를 아래와 같이 1의 데이터값을 갖는 노드 다음에 위치시키려면 1의 값을 갖는 노드의 next값을 먼저 저장해준뒤, 다음에 node3을 연결해준다. 그리고 다시 node3의 next가 이전 1의 값을 갖는 node의 next를 가리키게 하면된다.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">node3 = Node(1.5)</span><br><span class="line"></span><br><span class="line">node = head</span><br><span class="line">search = True</span><br><span class="line"><span class="keyword">while</span> search:</span><br><span class="line">    <span class="keyword">if</span> node.data == 1:</span><br><span class="line">        search = False</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        node = node.next</span><br><span class="line"></span><br><span class="line">node_next = node.next</span><br><span class="line">node.next = node3</span><br><span class="line">node3.next = node_next</span><br></pre></td></tr></table></figure></li></ul><hr><ul><li>이제 원하는대로 node가 삽입 되었는지 살펴보자.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">node = head</span><br><span class="line"><span class="keyword">while</span> node.next:</span><br><span class="line">    <span class="built_in">print</span>(node.data)</span><br><span class="line">    node = node.next</span><br><span class="line"><span class="built_in">print</span> (node.data)</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">1.5</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td></tr></table></figure><hr><h3 id="Pseudo-Code-for-Single-Linked-List"><a href="#Pseudo-Code-for-Single-Linked-List" class="headerlink" title="Pseudo Code for Single Linked List"></a>Pseudo Code for Single Linked List</h3><ul><li><p>위에서 언급한 기능들을 포함한 Single linked List를 객체로 만들어 본다.</p></li><li><ol><li>먼저, Single linked List에 필요한 기능 중 하나인 next를 찾으려면 시작점을 정해주어야 할 것이다. 그러므로 <code>head</code>와 리스트의 전체 사이즈를 나타내는 <code>n_size</code>를 만들어준다.</li></ol></li><li><ol><li>리스트를 추가해주는 <code>add</code>와 해당 데이터 값을 갖고있는 노드의 위치와 데이터값을 출력해주는 <code>search</code>를 만든다. <code>add</code>를 만들 때 주의할 점은head가 새롭게 추가되는 데이터가 되도록만들어 준다는 것을 생각해주어야 한다. next로 연결해주는 순서를 통해 만들어준다. <code>search</code>를 만들때 주의점은 while문을 돌면서 head 부터 시작하여 차례대로 찾는값과 비교해 주는데 찾는값이 해당 Linked list에 꼭 있다는 보장이 없기 때문에 data.next = None (마지막 노드)인 경우에는 cur.data가 없어 error를 발생시키게 된다. 그러므로 우선 cur의 데이터가 존재하는지 부터 생각하자. 또한, Node 자체를 반환해주는 식으로 작성해야 할 것이다. Linked list는 노드의 연결체이기 때문이다.</li></ol></li><li><ol><li><code>delete</code>함수는 python의 garbage collection을 통해 head였던 node의 연결을 끊어 줌으로써 구현할 수 있다.</li></ol></li><li><ol><li><code>traverse</code>함수는 제너레이터를 사용하여 전체적인 노드값을 보여주는 제너레이터는 제너레이터 객체에서 <strong>next</strong> 메서드를 호출할 때마다 함수 안의 yield까지 코드를 실행하며 yield에서 값을 발생시킨다.(generate) <code>yield를 사용하면 값을 함수 바깥으로 전달하면서 코드 실행을 함수 바깥에 양보하게된다. 따라서 yield는 현재 함수를 잠시 중단하고 함수 바깥의 코드가 실행되도록 만든다.</code></li></ol></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line">        self.__next=None</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def next(self):</span><br><span class="line">        <span class="built_in">return</span> self.__next</span><br><span class="line"></span><br><span class="line">    @next.setter</span><br><span class="line">    def next(self, n):</span><br><span class="line">        self.__next=n</span><br><span class="line"></span><br><span class="line">class Single_Linked_List:</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.head=None</span><br><span class="line">        self.n_size=0</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.n_size==0:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def size(self):</span><br><span class="line">        <span class="built_in">return</span> self.n_size</span><br><span class="line"></span><br><span class="line">    def add(self, data):</span><br><span class="line">        node=Node(data)</span><br><span class="line">        <span class="keyword">if</span> self.head==None:</span><br><span class="line">            self.head=node</span><br><span class="line">            self.n_size+=1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur=self.head</span><br><span class="line">            <span class="keyword">while</span> cur:</span><br><span class="line">                <span class="keyword">if</span> cur.next == None:</span><br><span class="line">                    cur.next=node</span><br><span class="line">                    self.n_size += 1</span><br><span class="line">                    cur=node.next</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    cur = cur.next</span><br><span class="line"></span><br><span class="line">    def add_after(self, data, target):</span><br><span class="line">        node=Node(data)</span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data == target:</span><br><span class="line">                cur_next=cur.next</span><br><span class="line">                cur.next=node</span><br><span class="line">                node.next=cur_next</span><br><span class="line">                <span class="built_in">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur=cur.next</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def search(self, data):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        이 함수는 리스트의 search 명령과 같이 동일한 값을 갖는 처음 만나는 노드를 반환한다.</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data == data:</span><br><span class="line">                <span class="built_in">return</span> cur</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur=cur.next</span><br><span class="line"></span><br><span class="line">    def delete(self, data):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        이 함수는 python의 garbage collection의 개념을 통해 연결을 끊어 제거해주는 방식으로 구현하였다.</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        cur = self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data == data:</span><br><span class="line">                previous.next =cur.next</span><br><span class="line">                <span class="built_in">break</span></span><br><span class="line">            previous = cur</span><br><span class="line">            cur = cur.next</span><br><span class="line">        self.n_size -= 1</span><br><span class="line"></span><br><span class="line">    def traverse(self):</span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            yield cur</span><br><span class="line">            cur=cur.next</span><br></pre></td></tr></table></figure><hr><h4 id="print-the-list"><a href="#print-the-list" class="headerlink" title="print the list"></a>print the list</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def show_list(slist):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'data size : &#123;&#125;'</span>.format(slist.size()))</span><br><span class="line">    g=slist.traverse()</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> g:</span><br><span class="line">        <span class="built_in">print</span>(node.data, end= <span class="string">'  '</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure><hr><h4 id="리스트에-삽입-및-데이터-확인"><a href="#리스트에-삽입-및-데이터-확인" class="headerlink" title="리스트에 삽입 및 데이터 확인"></a>리스트에 삽입 및 데이터 확인</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">slist=Single_Linked_List()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삽입'</span>)</span><br><span class="line">slist.add(3)</span><br><span class="line">slist.add(1)</span><br><span class="line">slist.add(5)</span><br><span class="line">slist.add(2)</span><br><span class="line">slist.add(7)</span><br><span class="line">slist.add(8)</span><br><span class="line">slist.add(3)</span><br><span class="line">show_list(slist)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">데이터 삽입</span><br><span class="line">data size : 7</span><br><span class="line">3  1  5  2  7  8  3</span><br></pre></td></tr></table></figure><hr><h4 id="리스트-중간에-삽입"><a href="#리스트-중간에-삽입" class="headerlink" title="리스트 중간에 삽입"></a>리스트 중간에 삽입</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slist.add_after(7, 5)</span><br><span class="line">show_list(slist)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data size : 7</span><br><span class="line">3  1  5  7  2  7  8  3</span><br></pre></td></tr></table></figure><hr><h4 id="search"><a href="#search" class="headerlink" title="search"></a>search</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 탐색'</span>)</span><br><span class="line">target=7</span><br><span class="line">res=slist.search(target)</span><br><span class="line"><span class="keyword">if</span> res:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'데이터 &#123;&#125; 검색 성공'</span>.format(res.data))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'데이터 &#123;&#125; 탐색 실패'</span>.format(target))</span><br><span class="line">res=None</span><br><span class="line"><span class="built_in">print</span>()</span><br></pre></td></tr></table></figure><hr><h6 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">데이터 탐색</span><br><span class="line">데이터 7 검색 성공</span><br></pre></td></tr></table></figure><hr><h4 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삭제'</span>)</span><br><span class="line">slist.delete(5)</span><br><span class="line">slist.delete(7)</span><br><span class="line">slist.delete(8)</span><br><span class="line">show_list(slist)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">데이터 삭제</span><br><span class="line">data size : 4</span><br><span class="line">3  1  2  7  3</span><br></pre></td></tr></table></figure><hr><h3 id="7-다양한-링크드-리스트-구조"><a href="#7-다양한-링크드-리스트-구조" class="headerlink" title="7. 다양한 링크드 리스트 구조"></a>7. 다양한 링크드 리스트 구조</h3><ul><li>더블 링크드 리스트(Doubly linked list) 기본 구조<ul><li>이중 연결 리스트라고도 함</li><li>장점: 양방향으로 연결되어 있어서 노드 탐색이 양쪽으로 모두 가능<br><br><br><img src="https://www.fun-coding.org/00_Images/doublelinkedlist.png"><br>(출처: wikipedia, <a href="https://en.wikipedia.org/wiki/Linked_list" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Linked_list</a>)</li></ul></li></ul><ul><li>위에서 언급한 것과 같이 양방향에서 모두 탐색이 가능하다는 점을 주의하자.<ul><li>양방향을 사용하기 위해선 위에서 정의했던 노드의 방향을 추가하여 재정의해주어야한다.</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self,data=None):</span><br><span class="line">        self.__data = data</span><br><span class="line">        self. __next = None</span><br><span class="line">        self.__before = None</span><br><span class="line">   <span class="comment"># 소멸자 : 객체가 메모리에서 사라질 때 반드시 한번 호출하는 것을 보장</span></span><br><span class="line">    def __del__(self):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'&#123;&#125; is deleted'</span>.format(self.__data))</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data = data</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def next(self):</span><br><span class="line">        <span class="built_in">return</span> self.__next</span><br><span class="line"></span><br><span class="line">    @next.setter</span><br><span class="line">    def next(self, next):</span><br><span class="line">        self.__next = next</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def before(self):</span><br><span class="line">        <span class="built_in">return</span> self.__before</span><br><span class="line"></span><br><span class="line">    @before.setter</span><br><span class="line">    def before(self, before):</span><br><span class="line">        self.__before = before</span><br></pre></td></tr></table></figure><hr><h3 id="Pseudo-Code-for-Single-Linked-List-1"><a href="#Pseudo-Code-for-Single-Linked-List-1" class="headerlink" title="Pseudo Code for Single Linked List"></a>Pseudo Code for Single Linked List</h3><ul><li><p>가장 먼저 single linked list와 다르게 head만 존재하는 것이 아니라 <code>tail</code>도 존재한다는 것을 유의하자.</p></li><li><p>필요한 기능을 나열해보자. 대략적으로 나누어서 생각해보면 다음과 같은 기능들이 필요할 것이다.</p><ul><li>size</li><li>empty</li><li>add<ul><li>head after</li><li>tail before</li><li>mid</li></ul></li><li>search<ul><li>head -&gt; tail</li><li>tail -&gt; head</li></ul></li><li>delete<ul><li>head</li><li>tail</li><li>mid</li></ul></li></ul></li><li><p><code>insert_after</code>, <code>insert_before</code>는 if문의 순서를 바꾸어 주어 링크드리스트내에 노드가 1개짜리인 경우에는 각각 <code>add_first</code>와 <code>add_last</code>를 하는 것과 동일하므로 함수로 계산하는 것으로 대체해주었다.</p></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line">class DoubleLinkedList:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.head = None</span><br><span class="line">        self.tail = None</span><br><span class="line">        self.d_size = 0</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.d_size==0:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def size(self):</span><br><span class="line">        <span class="built_in">return</span> self.d_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def add_first(self, data):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            new=Node(data)</span><br><span class="line">            self.head=new</span><br><span class="line">            self.tail=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new=Node(data)</span><br><span class="line">            new.next=self.head</span><br><span class="line">            self.head.before=new</span><br><span class="line">            self.head=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def add_last(self, data):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            new=Node(data)</span><br><span class="line">            self.head=new</span><br><span class="line">            self.tail=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new=Node(data)</span><br><span class="line">            self.tail.next=new</span><br><span class="line">            new.before=self.tail</span><br><span class="line">            self.tail=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def insert_after(self, data, node):</span><br><span class="line">        <span class="keyword">if</span> node == self.tail:</span><br><span class="line">            self.add_last(data)</span><br><span class="line">        <span class="keyword">elif</span> node == self.head:</span><br><span class="line">            new=Node(data)</span><br><span class="line">            new.next=self.head.next</span><br><span class="line">            new.before=self.head</span><br><span class="line">            self.head.next.before=new</span><br><span class="line">            self.head.next=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new=Node(data)</span><br><span class="line">            new.next=node.next</span><br><span class="line">            new.before=node</span><br><span class="line">            node.next.before=new</span><br><span class="line">            node.next=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line"></span><br><span class="line">    def insert_before(self, data, node):</span><br><span class="line">        <span class="keyword">if</span> node == self.head:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"this"</span>)</span><br><span class="line">            self.add_first(data)</span><br><span class="line">        <span class="keyword">elif</span> node == self.tail:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"is"</span>)</span><br><span class="line">            new=Node(data)</span><br><span class="line">            new.next=self.tail</span><br><span class="line">            new.before=self.tail.before</span><br><span class="line">            self.tail.before.next=new</span><br><span class="line">            self.tail.before=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"shit"</span>)</span><br><span class="line">            new=Node(data)</span><br><span class="line">            new.next=node</span><br><span class="line">            new.before=node.before</span><br><span class="line">            node.before.next=new</span><br><span class="line">            node.before=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line"></span><br><span class="line">    def search_forward(self, data):</span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data==data:</span><br><span class="line">                <span class="built_in">return</span> cur</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur=cur.next</span><br><span class="line">        <span class="keyword">if</span> cur == None:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"리스트 안에 찾는 데이터가 존재하지 않습니다."</span>)</span><br><span class="line"></span><br><span class="line">    def search_backward(self, data):</span><br><span class="line">        cur=self.tail</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data == data:</span><br><span class="line">                <span class="built_in">return</span> cur</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur=cur.before</span><br><span class="line">        <span class="keyword">if</span> cur == None:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"리스트 안에 찾는 데이터가 존재하지 않습니다."</span>)</span><br><span class="line"></span><br><span class="line">    def delete_first(self):</span><br><span class="line">        new_head=self.head.next</span><br><span class="line">        self.head.next=None</span><br><span class="line">        self.head=new_head</span><br><span class="line">        new_head.before=None</span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line">    def delete_last(self):</span><br><span class="line">        new_tail=self.tail.before</span><br><span class="line">        self.tail.before=None</span><br><span class="line">        self.tail=new_tail</span><br><span class="line">        self.tail.next=None</span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def delete_node(self, node):</span><br><span class="line">        <span class="keyword">if</span> node == self.head:</span><br><span class="line">            self.delete_first()</span><br><span class="line">        <span class="keyword">elif</span> node == self.tail:</span><br><span class="line">            self.delete_last()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node.before.next=node.next</span><br><span class="line">            node.next.before=node.before</span><br><span class="line"></span><br><span class="line">    def traverse(self, start=True):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        start=True --&gt; from head</span></span><br><span class="line"><span class="string">        start=False --&gt; from tail</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="keyword">if</span> start:</span><br><span class="line">            cur=self.head</span><br><span class="line">            <span class="keyword">while</span> cur:</span><br><span class="line">                yield cur</span><br><span class="line">                cur=cur.next</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur=self.tail</span><br><span class="line">            <span class="keyword">while</span> cur:</span><br><span class="line">                yield cur</span><br><span class="line">                cur=cur.before</span><br></pre></td></tr></table></figure><hr><h4 id="print-the-linked-list"><a href="#print-the-linked-list" class="headerlink" title="print the linked list"></a>print the linked list</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def show_list(dlist, start=True):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'data size : &#123;&#125;'</span>.format(dlist.size()))</span><br><span class="line">    g=dlist.traverse(start)</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> g:</span><br><span class="line">        <span class="built_in">print</span>(node.data, end=<span class="string">'  '</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure><hr><h2 id="make-a-double-linked-list"><a href="#make-a-double-linked-list" class="headerlink" title="make a double linked list"></a>make a double linked list</h2><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dlist=DoubleLinkedList()</span><br></pre></td></tr></table></figure><hr><h4 id="insert-1-add-first"><a href="#insert-1-add-first" class="headerlink" title="insert 1 - add_first"></a>insert 1 - add_first</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삽입 -add_first'</span>)</span><br><span class="line">dlist.add_first(1)</span><br><span class="line">dlist.add_first(2)</span><br><span class="line">dlist.add_first(3)</span><br><span class="line">dlist.add_first(5)</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">데이터 삽입 -add_first</span><br><span class="line">data size : 4</span><br><span class="line">5  3  2  1</span><br></pre></td></tr></table></figure><hr><h4 id="insert-2-add-last"><a href="#insert-2-add-last" class="headerlink" title="insert 2 - add_last"></a>insert 2 - add_last</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삽입 -add_last'</span>)</span><br><span class="line">dlist.add_last(1)</span><br><span class="line">dlist.add_last(2)</span><br><span class="line">dlist.add_last(3)</span><br><span class="line">dlist.add_last(5)</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">데이터 삽입 -add_last</span><br><span class="line">data size : 8</span><br><span class="line">5  3  2  1  1  2  3  5</span><br></pre></td></tr></table></figure><hr><h4 id="insert-3-insert-after"><a href="#insert-3-insert-after" class="headerlink" title="insert 3 - insert_after"></a>insert 3 - insert_after</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삽입 - insert_after'</span>)</span><br><span class="line">dlist.insert_after(4, dlist.search_forward(5))</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">데이터 삽입 - insert_after</span><br><span class="line">data size : 9</span><br><span class="line">5  4  3  2  1  1  2  3  5</span><br></pre></td></tr></table></figure><hr><h4 id="insert-4-insert-before"><a href="#insert-4-insert-before" class="headerlink" title="insert 4 - insert_before"></a>insert 4 - insert_before</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삽입 - insert_before'</span>)</span><br><span class="line">dlist.insert_before(4, node=dlist.search_forward(5))</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">데이터 삽입 - insert_before</span><br><span class="line">this</span><br><span class="line">data size : 10</span><br><span class="line">4  5  4  3  2  1  1  2  3  5</span><br></pre></td></tr></table></figure><hr><h4 id="search-1"><a href="#search-1" class="headerlink" title="search"></a>search</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 탐색'</span>)</span><br><span class="line">target=2</span><br><span class="line"><span class="comment">#res=dlist.search_forward(target)</span></span><br><span class="line">res=dlist.search_backward(target)</span><br><span class="line"><span class="keyword">if</span> res:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'데이터 &#123;&#125; 탐색 성공'</span>.format(res.data))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'데이터 &#123;&#125; 탐색 실패'</span>.format(target))</span><br><span class="line">res=None</span><br></pre></td></tr></table></figure><hr><h6 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">데이터 탐색</span><br><span class="line">데이터 2 탐색 성공</span><br></pre></td></tr></table></figure><hr><h4 id="delete-first"><a href="#delete-first" class="headerlink" title="delete_first"></a>delete_first</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dlist.delete_first()</span><br><span class="line">dlist.delete_first()</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4 is deleted</span><br><span class="line">5 is deleted</span><br><span class="line">data size : 8</span><br><span class="line">4  3  2  1  1  2  3  5</span><br></pre></td></tr></table></figure><hr><h4 id="delete-last"><a href="#delete-last" class="headerlink" title="delete_last"></a>delete_last</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dlist.delete_last()</span><br><span class="line">dlist.delete_last()</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">5 is deleted</span><br><span class="line">3 is deleted</span><br><span class="line">data size : 6</span><br><span class="line">4  3  2  1  1  2</span><br></pre></td></tr></table></figure><hr><h4 id="delete-node"><a href="#delete-node" class="headerlink" title="delete_node"></a>delete_node</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dlist.delete_node(dlist.search_backward(1))</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-14"><a href="#결과-14" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 is deleted</span><br><span class="line">data size : 6</span><br><span class="line">4  3  2  1  2</span><br></pre></td></tr></table></figure><hr><ul><li>위의 테스트와 동일한 결과를 출력하나 head와 tail을 Node로 만들어 head와 tail에 관해 좀더 직관적으로 확인할 수 있고, Node 추가시 조금의 이점이 더 있게끔 구현해본 또 다른 코드이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">class DoubleLinkedList:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.head = Node()</span><br><span class="line">        self.tail = Node()</span><br><span class="line">        self.head.next = self.tail</span><br><span class="line">        self.tail.before = self.head</span><br><span class="line">        self.d_size = 0</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.d_size==0:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def size(self):</span><br><span class="line">        <span class="built_in">return</span> self.d_size</span><br><span class="line"></span><br><span class="line">    def add_first(self, data):</span><br><span class="line">        <span class="comment">#새 노드를 생성</span></span><br><span class="line">        new_node = Node(data)</span><br><span class="line"></span><br><span class="line">        new_node.next = self.head.next</span><br><span class="line">        new_node.before = self.head</span><br><span class="line">        <span class="comment">#위의 두가지는 순서가 바뀌어도 됨!</span></span><br><span class="line">        self.head.next.before = new_node</span><br><span class="line">        self.head.next = new_node</span><br><span class="line"></span><br><span class="line">        self.d_size+=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def add_last(self, data):</span><br><span class="line">        new_node = Node(data)</span><br><span class="line"></span><br><span class="line">        new_node.next = self.tail</span><br><span class="line">        new_node.before = self.tail.before</span><br><span class="line"></span><br><span class="line">        self.tail.before.next = new_node</span><br><span class="line">        self.tail.before = new_node</span><br><span class="line"></span><br><span class="line">        self.d_size+=1</span><br><span class="line"></span><br><span class="line">    def insert_after(self, data, node):</span><br><span class="line">        new_node = Node(data)</span><br><span class="line"></span><br><span class="line">        new_node.next = node.next</span><br><span class="line">        new_node.before = node</span><br><span class="line">        <span class="comment">#new_node.before = node.next.before</span></span><br><span class="line">        node.next.before = new_node</span><br><span class="line">        node.next = new_node</span><br><span class="line"></span><br><span class="line">        self.d_size+=1</span><br><span class="line"></span><br><span class="line">    def insert_before(self, data, node):</span><br><span class="line">        new_node = Node(data)</span><br><span class="line"></span><br><span class="line">        new_node.next = node</span><br><span class="line">        new_node.before = node.before</span><br><span class="line">        node.before.next = new_node</span><br><span class="line">        node.before = new_node</span><br><span class="line"></span><br><span class="line">        self.d_size+=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def search_forward(self, data):</span><br><span class="line">        current = self.head.next</span><br><span class="line">        <span class="keyword">while</span>(current is not self.tail):</span><br><span class="line">            <span class="keyword">if</span>(current.data == data):</span><br><span class="line">                <span class="built_in">return</span> current</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                current = current.next</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line"></span><br><span class="line">    def search_backward(self, data):</span><br><span class="line">        current = self.tail.before</span><br><span class="line">        <span class="keyword">while</span>(current is not self.head): <span class="comment"># 주소값을 비교하기 때문에 !=이 아닌 is not으로 하는 것이 더 좋음!!</span></span><br><span class="line">            <span class="keyword">if</span>(current.data == data):</span><br><span class="line">                <span class="built_in">return</span> current</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                current = current.before</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line"></span><br><span class="line">    def delete_first(self):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            <span class="built_in">return</span></span><br><span class="line">        self.head.next = self.head.next.next</span><br><span class="line">        self.head.next.before = self.head  </span><br><span class="line"></span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line">    def delete_last(self):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            <span class="built_in">return</span></span><br><span class="line">        self.tail.before=self.tail.before.before</span><br><span class="line">        self.tail.before.next=self.tail</span><br><span class="line"></span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line">    def delete_node(self, node):</span><br><span class="line">        node.before.next=node.next <span class="comment">#reference count가 0이되어 사라짐!</span></span><br><span class="line">        node.next.before=node.before</span><br><span class="line"></span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line">    def traverse(self, start=True):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        start=True --&gt; from head</span></span><br><span class="line"><span class="string">        start=False --&gt; from tail</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="keyword">if</span> start:</span><br><span class="line">            cur=self.head</span><br><span class="line">            <span class="keyword">while</span> cur is not self.tail:</span><br><span class="line">                yield cur</span><br><span class="line">                cur=cur.next</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur=self.tail.before</span><br><span class="line">            <span class="keyword">while</span> cur is not self.head:</span><br><span class="line">                yield cur</span><br><span class="line">                cur=cur.before</span><br></pre></td></tr></table></figure><hr><blockquote><p>위의 클래스를 사용하여 Python의 리스트를 구현해보았다.</p></blockquote><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">from double_linked_list import DoubleLinkedList</span><br><span class="line"></span><br><span class="line">class PseudoList(DoubleLinkedList):</span><br><span class="line">    <span class="comment">#pos는 파이썬 리스트의 인덱스와 비슷하게</span></span><br><span class="line">    <span class="comment">#데이터의 위치를 나타냄.</span></span><br><span class="line">    <span class="comment">#인덱스처럼 0이 첫번째 위치를 의미</span></span><br><span class="line">    def __init__(self, *args):</span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="keyword">for</span> elem <span class="keyword">in</span> args:</span><br><span class="line">            self.add_last(elem)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#전역함수 len(list)을 호출할 때 이 함수가 호출</span></span><br><span class="line">    def __len__(self):</span><br><span class="line">        <span class="built_in">return</span> self.size()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#파이썬 리스트의 append는 맨 뒤에 데이터를 추가</span></span><br><span class="line">    <span class="comment">#더블 링크드 리스트에 있는 add_last 함수를 사용</span></span><br><span class="line">    def append(self, data): <span class="comment">#래핑 함수</span></span><br><span class="line">        self.add_last(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#인자로 pos를 받으면 pos에 위치한 노드를 반환한다.</span></span><br><span class="line">    def __find_position(self, pos):</span><br><span class="line">        <span class="keyword">if</span> pos &gt;=self.size():</span><br><span class="line">            raise IndexError(<span class="string">'list index out of range'</span>)</span><br><span class="line">        cur = self.head.next</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(pos):</span><br><span class="line">            cur = cur.next</span><br><span class="line">        <span class="built_in">return</span> cur</span><br><span class="line"></span><br><span class="line">    <span class="comment">#pos에 위치한 노드를 구한 뒤 --&gt; __find_position()</span></span><br><span class="line">    <span class="comment">#그 노드의 앞에 데이터를 삽입 --&gt; insert_before()</span></span><br><span class="line">    def insert(self, pos, data):    </span><br><span class="line">        node = self.__find_position(pos)</span><br><span class="line">        self.insert_before(data, node)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#리스트에 있는 data의 개수를 카운트</span></span><br><span class="line">    <span class="comment">#리스트를 순회하면서 데이터가 있으면 cnt 변수를 1씩 증가시킴</span></span><br><span class="line">    def count(self, data):</span><br><span class="line">        cnt = 0</span><br><span class="line">        cur = self.head.next</span><br><span class="line">        <span class="keyword">while</span> cur is not self.tail:</span><br><span class="line">            <span class="keyword">if</span> cur.data == data:</span><br><span class="line">                cnt += 1</span><br><span class="line">            cur = cur.next</span><br><span class="line">        <span class="built_in">return</span> cnt</span><br><span class="line"></span><br><span class="line">    <span class="comment">#인자 data가 위치한 인덱스를(여기에서는 pos) 반환한다</span></span><br><span class="line">    <span class="comment">#start는 데이터를 찾기 시작하는 위치</span></span><br><span class="line">    def index(self, data, start=0):</span><br><span class="line">        cur = self.__find_position(start)</span><br><span class="line">        index = start</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data == data:</span><br><span class="line">                <span class="built_in">return</span> index</span><br><span class="line">            cur = cur.next</span><br><span class="line">            index += 1</span><br><span class="line"></span><br><span class="line">        raise ValueError(<span class="string">'&#123;&#125; is not in the list'</span>.format(data))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#print(li[3]) 처럼 [] 연산자를 통해 값을 가져올 때 내부에서 호출</span></span><br><span class="line">    <span class="comment">#index는 pos를 의미</span></span><br><span class="line">    <span class="comment">#index에 위치한 노드의 데이터를 반환</span></span><br><span class="line">    def __getitem__(self, index):<span class="comment">#연산자 오버로딩</span></span><br><span class="line">        node = self.__find_position(index)</span><br><span class="line">        <span class="built_in">return</span> node.data</span><br><span class="line"></span><br><span class="line">    <span class="comment">#li[3]=10 처럼 [] 연산자로 값을 대입할 때 내부에서 호출</span></span><br><span class="line">    <span class="comment">#index는 pos를 의미</span></span><br><span class="line">    <span class="comment">#index에 위치한 노드의 값을 data로 바꿈</span></span><br><span class="line">    def __setitem__(self, index, data):<span class="comment">#연산자 오버로딩</span></span><br><span class="line">        node = self.__find_position(index)</span><br><span class="line">        node.data = data</span><br><span class="line"></span><br><span class="line">    <span class="comment">#pos에 있는 데이터를 삭제하면서 반환</span></span><br><span class="line">    <span class="comment">#파이썬 리스트처럼 인자를 주지 않으면</span></span><br><span class="line">    <span class="comment">#리스트의 맨 마지막 데이터를 삭제하면서 반환</span></span><br><span class="line">    def pop(self, pos=None):</span><br><span class="line">        <span class="comment">#인자 pos가 있는 경우</span></span><br><span class="line">        <span class="keyword">if</span> pos:</span><br><span class="line">            node = self.__find_position(pos)</span><br><span class="line">        <span class="comment">#pos가 비어 있는 경우</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node = self.tail.before</span><br><span class="line">        <span class="comment">#delete_node 함수를 이용해 노드 삭제</span></span><br><span class="line">        cur = node</span><br><span class="line">        self.delete_node(node)</span><br><span class="line">        <span class="built_in">return</span> cur.data</span><br><span class="line"></span><br><span class="line">    <span class="comment">#리스트에 있는 데이터를 삭제</span></span><br><span class="line">    <span class="comment">#만약 같은 데이터가 여러개라면</span></span><br><span class="line">    <span class="comment">#리스트에서 위치상 첫번째 데이터가 삭제된다</span></span><br><span class="line">    <span class="comment">#반환은 하지 않는다</span></span><br><span class="line">    def remove(self, data):</span><br><span class="line">        <span class="comment">#delete_node와 search_forward 함수 이용</span></span><br><span class="line">        self.delete_node(self.search_forward(data))</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        string = <span class="string">'['</span></span><br><span class="line">        cur = self.head.next</span><br><span class="line">        <span class="keyword">while</span> cur is not self.tail:</span><br><span class="line">            string+=str(cur.data)</span><br><span class="line">            <span class="keyword">if</span> cur.next is not self.tail:</span><br><span class="line">                string+=<span class="string">', '</span></span><br><span class="line">            cur= cur.next</span><br><span class="line"></span><br><span class="line">        string+=<span class="string">']'</span></span><br><span class="line">        <span class="built_in">return</span> string</span><br></pre></td></tr></table></figure><hr><h3 id="객체-생성"><a href="#객체-생성" class="headerlink" title="객체 생성"></a>객체 생성</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">initial = [1, 2, 3, 4]</span><br><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li = PseudoList(*initial)</span><br><span class="line"><span class="comment">#python list</span></span><br><span class="line">py_li = initial</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-15"><a href="#결과-15" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pseudo list : [1, 2, 3, 4]</span><br><span class="line">python list : [1, 2, 3, 4]</span><br></pre></td></tr></table></figure><hr><h3 id="append"><a href="#append" class="headerlink" title="append"></a>append</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li.append(2)</span><br><span class="line">li.append(1)</span><br><span class="line">li.append(2)</span><br><span class="line">li.append(7)</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li.append(2)</span><br><span class="line">py_li.append(1)</span><br><span class="line">py_li.append(2)</span><br><span class="line">py_li.append(7)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-16"><a href="#결과-16" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pseudo list : [1, 2, 3, 4, 2, 1, 2, 7]</span><br><span class="line">python list : [1, 2, 3, 4, 2, 1, 2, 7]</span><br></pre></td></tr></table></figure><hr><h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">target = 2</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'count of &#123;&#125; : &#123;&#125; in pseudo_list'</span>.format(target, li.count(target)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'count of &#123;&#125; : &#123;&#125; in python_list'</span>.format(target, py_li.count(target)))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-17"><a href="#결과-17" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">count of 2 : 3 <span class="keyword">in</span> pseudo_list</span><br><span class="line">count of 2 : 3 <span class="keyword">in</span> python_list</span><br></pre></td></tr></table></figure><hr><h3 id="pop"><a href="#pop" class="headerlink" title="pop"></a>pop</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li.pop(2)</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li.pop(2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-18"><a href="#결과-18" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data of 7 is deleted</span><br><span class="line">data of 2 is deleted</span><br><span class="line">pseudo list : [1, 2, 3, 4, 2, 1]</span><br><span class="line">python list : [1, 2, 3, 4, 2, 1]</span><br></pre></td></tr></table></figure><hr><h3 id="pop-index"><a href="#pop-index" class="headerlink" title="pop(index)"></a>pop(index)</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li.pop(2)</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li.pop(2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-19"><a href="#결과-19" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data of 3 is deleted</span><br><span class="line">pseudo list : [1, 2, 4, 2, 1]</span><br><span class="line">python list : [1, 2, 4, 2, 1]</span><br></pre></td></tr></table></figure><hr><h3 id="insert"><a href="#insert" class="headerlink" title="insert"></a>insert</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li.insert(3, 9)</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li.insert(3, 9)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-20"><a href="#결과-20" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pseudo list : [1, 2, 4, 9, 2, 1]</span><br><span class="line">python list : [1, 2, 4, 9, 2, 1]</span><br></pre></td></tr></table></figure><hr><h3 id="index"><a href="#index" class="headerlink" title="index"></a>index</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">target = 9</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"index of &#123;&#125; : &#123;&#125; in pseudo_list"</span>.format(target, li.index(target)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"index of &#123;&#125; : &#123;&#125; in python_list"</span>.format(target, py_li.index(target)))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-21"><a href="#결과-21" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index of 9 : 3 <span class="keyword">in</span> pseudo_list</span><br><span class="line">index of 9 : 3 <span class="keyword">in</span> python_list</span><br></pre></td></tr></table></figure><hr><h3 id="indexing"><a href="#indexing" class="headerlink" title="indexing"></a>indexing</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li[3]=7</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li[3]=7</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-22"><a href="#결과-22" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pseudo list : [1, 2, 4, 7, 2, 1]</span><br><span class="line">python list : [1, 2, 4, 7, 2, 1]</span><br></pre></td></tr></table></figure><hr><h3 id="remove"><a href="#remove" class="headerlink" title="remove"></a>remove</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li.remove(9)</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li.remove(9)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-23"><a href="#결과-23" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data of 9 is deleted</span><br><span class="line">pseudo list : [1, 2, 4, 2, 1]</span><br><span class="line">python list : [1, 2, 4, 2, 1]</span><br></pre></td></tr></table></figure><hr>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/28/data_structure_03/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 01 Stack</title>
      <link>https://heung-bae-lee.github.io/2020/04/27/data_structure_02/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/27/data_structure_02/</guid>
      <pubDate>Mon, 27 Apr 2020 12:38:39 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Stack&quot;&gt;&lt;a href=&quot;#Stack&quot; class=&quot;headerlink&quot; title=&quot;Stack&quot;&gt;&lt;/a&gt;Stack&lt;/h1&gt;&lt;h2 id=&quot;꼭-알아둬야-할-자료-구조-스택-Stack&quot;&gt;&lt;a href=&quot;#꼭-알아둬야-할-자료-구조-스택-
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Stack"><a href="#Stack" class="headerlink" title="Stack"></a>Stack</h1><h2 id="꼭-알아둬야-할-자료-구조-스택-Stack"><a href="#꼭-알아둬야-할-자료-구조-스택-Stack" class="headerlink" title="꼭 알아둬야 할 자료 구조: 스택 (Stack)"></a>꼭 알아둬야 할 자료 구조: 스택 (Stack)</h2><ul><li>데이터를 제한적으로 접근할 수 있는 구조<ul><li>한쪽 끝에서만 자료를 넣거나 뺄 수 있는 구조</li></ul></li><li><p>가장 나중에 쌓은 데이터를 가장 먼저 빼낼 수 있는 데이터 구조</p><ul><li>큐: FIFO 정책 -&gt; 줄 세우기</li><li>스택: <code>LIFO 정책</code> -&gt; 책 쌓기</li></ul><h3 id="1-스택-구조"><a href="#1-스택-구조" class="headerlink" title="1. 스택 구조"></a>1. 스택 구조</h3><ul><li><p>스택은 LIFO(Last In, Fisrt Out) 또는 FILO(First In, Last Out) 데이터 관리 방식을 따름</p><ul><li>LIFO: 마지막에 넣은 데이터를 가장 먼저 추출하는 데이터 관리 정책</li><li>FILO: 처음에 넣은 데이터를 가장 마지막에 추출하는 데이터 관리 정책</li><li>참고로 Queue는 FIFO라고 많이 얘기하지만, Stack은 그냥 Stack이라고 한다.</li></ul></li><li><p>대표적인 스택의 활용</p><ul><li><code>컴퓨터 내부의 프로세스 구조의 함수 동작 방식</code></li></ul></li><li><p>주요 기능</p><ul><li>push(): 데이터를 스택에 넣기</li><li>pop(): 데이터를 스택에서 꺼내기</li></ul></li><li><p><font color="#BF360C">Visualgo 사이트에서 시연해보며 이해하기 (push/pop 만 클릭해보며): <a href="https://visualgo.net/en/list" target="_blank" rel="noopener">https://visualgo.net/en/list</a><br><br><br><img src="http://www.fun-coding.org/00_Images/stack.png"></font></p></li></ul><blockquote><p>그림으로 이해해보기</p></blockquote></li></ul><h2 id="2-스택-구조와-프로세스-스택"><a href="#2-스택-구조와-프로세스-스택" class="headerlink" title="2. 스택 구조와 프로세스 스택"></a>2. 스택 구조와 프로세스 스택</h2><ul><li>스택 구조는 <code>프로세스 실행 구조의 가장 기본</code><ul><li>함수 호출시 프로세스 실행 구조를 스택과 비교해서 이해 필요</li><li>stack이랑 queue는 일시적으로 자료를 보관할때 사용!!!</li></ul></li></ul><h1 id="재귀-함수"><a href="#재귀-함수" class="headerlink" title="재귀 함수"></a>재귀 함수</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def recursive(data):</span><br><span class="line">    <span class="keyword">if</span> data &lt; 0:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">"ended"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(data)</span><br><span class="line">        recursive(data - 1)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"returned"</span>, data)</span><br></pre></td></tr></table></figure><h5 id="실행"><a href="#실행" class="headerlink" title="실행"></a>실행</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recursive(4)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">0</span><br><span class="line">ended</span><br><span class="line">returned 0</span><br><span class="line">returned 1</span><br><span class="line">returned 2</span><br><span class="line">returned 3</span><br><span class="line">returned 4</span><br></pre></td></tr></table></figure><ul><li>Process에서 함수가 어떻게 동작하는지 그리고 그것이 어떻게 Stack이라는 자료구조와 연결이 되는지 중점적으로 설명하면 다음과 같다. Program이 실행되는 상태를 Process라고 하는데 그 Process안에서 함수가 호출이 된 것이므로 Process Stack에 아래 그림과 같이 쌓이게 된다. recursive 함수가 최종적으로 끝나게 되면, Stack 구조와 같이 제일 마직막 실행된 함수부터 실행을 마치게 되어 위와 같은 결과가 출력이 되는 것이다.</li></ul><p><img src="/image/Stack_process.png" alt="Stack Process"></p><h2 id="3-자료-구조-스택의-장단점"><a href="#3-자료-구조-스택의-장단점" class="headerlink" title="3. 자료 구조 스택의 장단점"></a>3. 자료 구조 스택의 장단점</h2><ul><li>장점<ul><li>구조가 단순해서, 구현이 쉽다.</li><li>데이터 저장/읽기 속도가 빠르다.</li></ul></li><li>단점 (일반적인 스택 구현시)<ul><li><code>데이터 최대 갯수를 미리 정해야 한다.</code><ul><li>파이썬의 경우 재귀 함수는 1000번까지만 호출이 가능함</li></ul></li><li><code>저장 공간의 낭비가 발생할 수 있음</code><ul><li>미리 최대 갯수만큼 저장 공간을 확보해야 함</li></ul></li></ul></li></ul><blockquote><p>스택은 단순하고 빠른 성능을 위해 사용되므로, 보통 배열 구조를 활용해서 구현하는 것이 일반적임.<br>이 경우, 위에서 열거한 단점이 있을 수 있음</p></blockquote><h2 id="4-파이썬-리스트-기능에서-제공하는-메서드로-스택-사용해보기"><a href="#4-파이썬-리스트-기능에서-제공하는-메서드로-스택-사용해보기" class="headerlink" title="4. 파이썬 리스트 기능에서 제공하는 메서드로 스택 사용해보기"></a>4. 파이썬 리스트 기능에서 제공하는 메서드로 스택 사용해보기</h2><ul><li>append(push), pop 메서드 제공</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_stack = list()</span><br><span class="line"></span><br><span class="line">data_stack.append(1)</span><br><span class="line">data_stack.append(2)</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_stack</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 2]</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_stack.pop()</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure><h2 id="5-프로그래밍-연습"><a href="#5-프로그래밍-연습" class="headerlink" title="5. 프로그래밍 연습"></a>5. 프로그래밍 연습</h2><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">연습1: 리스트 변수로 스택을 다루는 pop, push 기능 구현해보기 (pop, push 함수 사용하지 않고 직접 구현해보기)</font></strong><br></div><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">stack_list = list()</span><br><span class="line"></span><br><span class="line">def push(data):</span><br><span class="line">    stack_list.append(data)</span><br><span class="line"></span><br><span class="line">def pop():</span><br><span class="line">    data = stack_list[-1]</span><br><span class="line">    del stack_list[-1]</span><br><span class="line">    <span class="built_in">return</span> data</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(10):</span><br><span class="line">    push(index)</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pop()</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9</span><br></pre></td></tr></table></figure><h2 id="또-다른-방식으로-구현하는-Stack-구조"><a href="#또-다른-방식으로-구현하는-Stack-구조" class="headerlink" title="또 다른 방식으로 구현하는 Stack 구조"></a>또 다른 방식으로 구현하는 Stack 구조</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 많은 언어들이 실제 구현되어 있는 것을 보면 실제 자료는 container라는 stack안의 공간에 있고, stack은 인터페이스만 제공</span></span><br><span class="line">class Stack:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="comment"># 실제 데이터를 가지고 있는 자료구조</span></span><br><span class="line">        self.container=list() <span class="comment"># 빈 list 객체가 만들어져서 container에 저장.</span></span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> not self.container: <span class="comment"># 비어있으면 true가되니까</span></span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def push(self, data):</span><br><span class="line">        self.container.append(data)</span><br><span class="line"></span><br><span class="line">    def pop(self):</span><br><span class="line">        <span class="built_in">return</span> self.container.pop()</span><br><span class="line"></span><br><span class="line">    def peek(self):</span><br><span class="line">        <span class="built_in">return</span> self.container[-1]</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">s=Stack()</span><br><span class="line"></span><br><span class="line">s.push(1)</span><br><span class="line">s.push(2)</span><br><span class="line">s.push(3)</span><br><span class="line">s.push(4)</span><br><span class="line">s.push(5)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> not s.empty():</span><br><span class="line">    <span class="built_in">print</span>(s.pop(), end=<span class="string">'  '</span>)</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5  4  3  2  1</span><br></pre></td></tr></table></figure><h2 id="Node를-활용한-Stack-구조-구현하기"><a href="#Node를-활용한-Stack-구조-구현하기" class="headerlink" title="Node를 활용한 Stack 구조 구현하기"></a>Node를 활용한 Stack 구조 구현하기</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data=None):</span><br><span class="line">        self.__data=data</span><br><span class="line">        self.__next=None</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def next(self):</span><br><span class="line">        <span class="built_in">return</span> self.__next</span><br><span class="line"></span><br><span class="line">    @next.setter</span><br><span class="line">    def next(self, n):</span><br><span class="line">        self.__next=n</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">class LStack:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="comment">#인스턴스 멤버</span></span><br><span class="line">        self.top=None</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.top is None:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def push(self, data):</span><br><span class="line">        new_node = Node(data)</span><br><span class="line"><span class="comment">#         new_node.__data=data</span></span><br><span class="line"><span class="comment">#         if self.empty():</span></span><br><span class="line"><span class="comment">#             self.top = new_node</span></span><br><span class="line"><span class="comment">#             return</span></span><br><span class="line"></span><br><span class="line">        new_node.next = self.top</span><br><span class="line">        self.top = new_node</span><br><span class="line"></span><br><span class="line">    def pop(self):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            <span class="built_in">return</span> None</span><br><span class="line">        cur = self.top</span><br><span class="line">        self.top = self.top.next</span><br><span class="line">        <span class="built_in">return</span> cur.data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def peek(self):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            <span class="built_in">return</span> None</span><br><span class="line">        cur = self.top</span><br><span class="line">        <span class="built_in">return</span> cur.data</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">s = LStack()</span><br><span class="line"></span><br><span class="line">s.push(1)</span><br><span class="line">s.push(2)</span><br><span class="line">s.push(3)</span><br><span class="line">s.push(4)</span><br><span class="line">s.push(5)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> not s.empty():</span><br><span class="line">    <span class="built_in">print</span>(s.pop(), end=<span class="string">"  "</span>)</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5  4  3  2  1</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">s = LStack()</span><br><span class="line"></span><br><span class="line">s.push(1)</span><br><span class="line">s.push(2)</span><br><span class="line">s.empty()</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/27/data_structure_02/#disqus_thread</comments>
    </item>
    
    <item>
      <title>의사결정나무</title>
      <link>https://heung-bae-lee.github.io/2020/04/26/machine_learning_13/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/26/machine_learning_13/</guid>
      <pubDate>Sat, 25 Apr 2020 18:27:27 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Decision-tree-배경&quot;&gt;&lt;a href=&quot;#Decision-tree-배경&quot; class=&quot;headerlink&quot; title=&quot;Decision tree 배경&quot;&gt;&lt;/a&gt;Decision tree 배경&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;의사결정나무의 
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Decision-tree-배경"><a href="#Decision-tree-배경" class="headerlink" title="Decision tree 배경"></a>Decision tree 배경</h1><ul><li>의사결정나무의 장점은 해석력이 좋다. 우리가 모델을 만들때 성능이 좋은 것도 중요하지만, 어떻게 사람들한테 메세지를 줄 수 있는가처럼 어떻게 활용할 수 있는가가 더 중요한 경우도 있다. 예측력이 조금 떨어지더라도 이야기로 풀어서 어떠한 근거로 인해 Y는 이렇게 된다는 식으로 풀어서 설명할 수 있다는 의미이다.</li></ul><ul><li>결정트리는 매우 쉽고 유연하게 적용될 수 있는 알고리즘이다. 또한 <code>데이터의 Scaling이나 정규화(normalize) 등의 사전 가공의 영향이 매우 적다. 하지만, 예측 성능을 향상시키기 위해 복잡한 규칙 구조를 거쳐야 하며, 이로 인한 과적합(overfitting)이 발생해 반대로 예측 성능이 저하될 수도 있다는 단점이있다.</code> 이러한 단점이 앙상블 기법에서는 오히려 장점으로 작용한다. 앙상블은 매우 많은 여러개의 예측 성능이 상대적으로 떨어지는 학습 알고리즘을 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트하면서 예측 성능을 향상시키는데, 결정트리가 좋은 약한 학습기가 되기 때문이다.</li></ul><ul><li>결정 트리(Decision Tree)는 ML 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘이다. <code>데이터에 있는 규칙을 학습을 통해 자동으로 찾아내는 트리(Tree) 기반의 분류 규칙을 만드는 것</code>이다. 따라서, <code>데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인각가 알고리즘의 성능을 크게 좌우</code>한다. 의사결정나무(decision tree)는 여러 가지 규칙을 순차적으로 적용하면서 독립 변수 공간을 분할하는 분류 모형이다. 분류(classification)와 회귀 분석(regression)에 모두 사용될 수 있기 때문에 CART(Classification And Regression Tree)라고도 한다.</li></ul><p><img src="/image/decision_tree_concept.png" alt="결정 트리"></p><ul><li>아래 그림은 결정 트리의 구조를 간략하게 나타낸 것이다. 데이터 세트에 feature가 있고 이러한 feature가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어지며 새로운 규칙 조검마다 서브 트리(Sub tree)가 생성된다. 하지만 <code>많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 얘기이고, 이는 곧 과적합(overfitting)으로 이어지기 쉽다. 즉, 트리의 깊이(Depth)가 깊어질수록 결정 트리의 예측 성능이 저할될 가능성이 높아진다는 의미</code>이다.</li></ul><p><img src="/image/consist_of_decision_tree_node.png" alt="결정 트리 용어 - 01"></p><p><img src="/image/consist_of_decision_tree_node_01.png" alt="결정 트리 용어 - 02"></p><ul><li>결정트리는 다음과 같이 종속변수(반응 변수, target 값)의 자료형에 의해서 다음과 같이 분류될 수 있다. 아래 그림에서 오른쪽 그림이 분류트리이고 왼쪽 그림이 회귀 트리이다.</li></ul><p><img src="/image/what_kinds_of_decision_tree_for_dependent_variable.png" alt="결정 트리 종류"></p><h3 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h3><ul><li><p>그렇다면, 가능한 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙이 정해져야 한다. 이를 위해서는 어떻게 트리를 분할(Split)할 것인가가 중요한데 최대한 균일한 데이터 세트를 구성할 수 있도록 분할하는 것이 필요하다.</p></li><li><p>엔트로피는 섞여있는 상태를 의미한다고 생각하면 이해하기 쉽다. 섞여있는 상태면 엔트로피가 높은 것이고 물리적인 힘을 써서 분리는 해놓은 경우는 엔트로피가 낮은 상태이다. 아래 그림에서 $x$축의 $P+$가 의미하는 것이 노란 곡물이 나올 확률이라고 가정해 보자. $P+$가 0인 상황은 노란 곡물이 없는 상태를 의미하고, 1인 경우는 노란 곡물만 있는 상태일 것이다. 이 때의 엔트로피는 잘 분리되어있기 때문에 0의 값을 갖게된다. 허나 $P+$가 0.5일 경우는 노란곡물이 존재하거나 하지 않을 확률이 각각 절반이기 때문에 엔트로피가 가장 높게 된다.</p></li></ul><p><img src="/image/what_is_entropy.png" alt="Entropy"></p><ul><li><p>확률론에서의 <code>엔트로피</code> 개념은 <code>확률분포의 모양을 설명하는 특징값이며 확률분포가 가지고 있는 정보의 양을 나타내는 값</code>이기도 하다. 엔트로피는 두 확률분포의 모양이 어떤 관계를 가지는지 혹은 유사한지를 표현하는 데도 쓰인다. 조건부엔트로피는 한 확률분포에 의해 다른 확률분포가 받는 영향을 설명한다. 교차엔트로피와 쿨백-라이블러 발산은 두 확률분포가 얼마나 닮았는지를 나타낸다. 마지막으로 두 확률분포의 독립 및 상관관계를 나타내는 상호정보량에 대해서 설명할 것이다.</p></li><li><p>$Y\;=\;0$ 또는 $Y\;=\;1$인 두 가지 값을 가지는 확률변수의 확률분포가 다음과 같이 세 종류가 있다고 하자.</p><ul><li>확률분포 $Y_{1}$ : $P(Y=0)=0.5, P(Y=1)=0.5$</li><li>확률분포 $Y_{2}$ : $P(Y=0)=0.8, P(Y=1)=0.2$</li><li>확률분포 $Y_{3}$ : $P(Y=0)=1.0, P(Y=0)=0.0$</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(9, 3))</span><br><span class="line">plt.subplot(131)</span><br><span class="line">plt.bar([0, 1], [0.5, 0.5])</span><br><span class="line">plt.xticks([0, 1], [<span class="string">"Y=0"</span>, <span class="string">"Y=1"</span>])</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">plt.title(<span class="string">"<span class="variable">$Y_1</span>$"</span>)</span><br><span class="line">plt.subplot(132)</span><br><span class="line">plt.bar([0, 1], [0.8, 0.2])</span><br><span class="line">plt.xticks([0, 1], [<span class="string">"Y=0"</span>, <span class="string">"Y=1"</span>])</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">plt.title(<span class="string">"<span class="variable">$Y_2</span>$"</span>)</span><br><span class="line">plt.subplot(133)</span><br><span class="line">plt.bar([0, 1], [1.0, 0.0])</span><br><span class="line">plt.xticks([0, 1], [<span class="string">"Y=0"</span>, <span class="string">"Y=1"</span>])</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">plt.title(<span class="string">"<span class="variable">$Y_3</span>$"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/probability_distribution_ploting.png" alt="확률분포 그래프"></p><ul><li><p>베이지안 관점에서 위 확률분포는 다음과 같은 정보를 나타낸다.</p><ul><li>확률분포 $Y_{1}$은 $y$값에 대해 아무것도 모르는 상태</li><li>확률분포 $Y_{2}$은 $y$값이 0이라고 믿지만 아닐 가능성도 있다는 것을 아는 상태</li><li>확률분포 $Y_{2}$은 $y$값이 0이라고 100% 확신하는 상태</li></ul></li><li><p>확률 분포가 가지는 이러한 차이를 하나의 숫자로 나타낸 것이 바로 엔트로피이다.</p></li></ul><h3 id="Entropy-정의"><a href="#Entropy-정의" class="headerlink" title="Entropy 정의"></a>Entropy 정의</h3><ul><li><p>엔트로피(Entropy)는 <code>확률분포가 가지는 정보의 확신도 혹은 정보량을 수치로 표현한 것</code>이다. 확률분포에서 특정한 값이 나올 확률이 높아지고 나머지 값의 확률은 낮아진다면 엔트로피가 작아진다. 반대로 여러가지 값이 나올 확률이 대부분 비슷한 경우에는 엔트로피가 높아진다. 엔트로피는 확률분포의 모양이 어떤지를 나타내는 특성값 중 하나로 볼 수도 있다. <code>확률 또는 확률밀도가 특정값에 몰려있으면 엔트로피가 작다고 하고 반대로 여러가지 값에 골고루 퍼져 있다면 엔트로피가 크다고 한다.</code> 확률분포의 엔트로피는 물리학의 엔트로피 용어를 빌려온 것이다. 물리학에서는 물질의 상태가 분산되는 정도를 엔트로피로 정의한다. 물체의 상태가 여러가지로 고루 분산되어 있으면 엔트로피가 높고 특정한 하나의 상태로 몰려있으면 엔트로피가 낮다. 수학적으로 엔트로피는 확률분포함수를 입력으로 받아 숫자를 출력하는 범함수(functional)로 정의한다.</p></li><li><p>확률변수 $Y$가 카테고리분포와 같은 이산확률변수이면 다음처럼 정의한다. 이 식에서 $K$는 $X$가 가질 수 있는 클래스의 수이고 p(y)는 확률질량함수이다. 확률의 로그값이 항상 음수이므로 음수 기호를 붙여서 양수로 만들었다.</p></li></ul><script type="math/tex; mode=display">\begin{align} H[Y] = -\sum_{k=1}^K p(y_k) \log_2 p(y_k) \end{align}</script><ul><li>확률변수 $Y$가 연속확률변수이면 다음처럼 정의한다. 아래 수식에서 $p(y)$는 pdf이다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y] = -\int_{-\infty}^{\infty} p(y) \log_2 p(y) \; dy \end{align}</script><ul><li>로그의 밑(base)이 2로 정의된 것은 정보통신과 관련을 가지는 역사적인 이유 때문이다. 엔트로피 계산에서 $p(y)\;=\;0$인 경우에는 로그값이 정의되지 않으므로 다음과 같은 극한값을 사용한다.</li></ul><script type="math/tex; mode=display">\begin{align} \lim_{p\rightarrow 0} \; p\log_2{p} = 0 \end{align}</script><ul><li>이 값은 로피탈의 정리에서 구할 수 있다. 위에서 예를 든 $Y_{1}, Y_{2}, Y_{3}$ 3개의 이산확률분포에 대해 엔트로피를 구하면 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y_1] = -\dfrac{1}{2} \log_2 \dfrac{1}{2} -\dfrac{1}{2} \log_2 \dfrac{1}{2} = 1 \end{align}</script><script type="math/tex; mode=display">\begin{align} H[Y_2] = -\dfrac{8}{10} \log_2 \dfrac{8}{10} -\dfrac{2}{10} \log_2 \dfrac{2}{10} \approx 0.72 \end{align}</script><script type="math/tex; mode=display">\begin{align} H[Y_3] = -1 \log_2 1 -0 \log_2 0 = 0 \end{align}</script><ul><li>다음은 Numpy로 엔트로피를 계산한 결과다. 확률값이 0일 때는 가장 작은 값인 <code>eps</code>를 대신 사용한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-0.5 * np.log2(0.5) - 0.5 * np.log2(0.5)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-0.8 * np.log2(0.8) - 0.2 * np.log2(0.2)</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7219280948873623</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">eps = np.finfo(<span class="built_in">float</span>).eps</span><br><span class="line">-1 * np.log2(1) - eps * np.log2(eps)</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.1546319456101628e-14</span><br></pre></td></tr></table></figure><blockquote><p>연습문제) 베르누이분포에서 확률값 $P(Y=1)$은 0부터 1까지의 값을 가질 수 있다. 각각의 값에 대해 엔트로피를 계산하여 가로축이 P(Y=1)이고 세로축이 H(Y)인 그래프를 그려라.</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.font_manager as fm</span><br><span class="line"></span><br><span class="line">path = <span class="string">'/Library/Fonts/NanumGothic.ttf'</span></span><br><span class="line">font_name = fm.FontProperties(fname=path, size=50).get_name()</span><br><span class="line">plt.rc(<span class="string">'font'</span>, family=font_name)</span><br><span class="line"></span><br><span class="line">P_Y = np.linspace(0, 1, 100)</span><br><span class="line">ls=[]</span><br><span class="line"><span class="keyword">for</span> p_y <span class="keyword">in</span> P_Y:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (p_y != 0) and (1-p_y != 0):</span><br><span class="line">        ls.append(- p_y * np.log2(p_y) -(1 - p_y) * np.log2(1-p_y))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (p_y == 0) and (1 - p_y == 1):</span><br><span class="line">        p_y = np.finfo(<span class="built_in">float</span>).eps</span><br><span class="line">        ls.append(- p_y * np.log2(p_y) -(1 - p_y) * np.log2(1-p_y))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (p_y == 1) and (1 - p_y == 0):</span><br><span class="line">        ls.append(- p_y * np.log2(p_y) -(np.finfo(<span class="built_in">float</span>).eps) * np.log2(np.finfo(<span class="built_in">float</span>).eps))</span><br><span class="line"></span><br><span class="line">plt.plot(P_Y, ls, <span class="string">"-"</span>, label=<span class="string">"엔트로피"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"엔트로피"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"베르누이 분포의 모수"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/bernoulli_distribution_parameters_entropy.png" alt="베르누이 분포의 모수에 따른 엔트로피"></p><p><img src="/image/ENTROPY_dost_plot.png" alt="엔트로피의 직관적 해석"></p><blockquote><p>다음 확률분포의 엔트로피를 계산하라.</p></blockquote><script type="math/tex; mode=display">\begin{align} H[Y] = -\sum_{k=1}^K p(y_k) \log_2 p(y_k) \end{align}</script><ul><li>엔트로피의 정의에따라 풀면 다음과 같다.</li></ul><script type="math/tex; mode=display">(1) P(Y=0)=\dfrac{1}{8}, P(Y=1)=\dfrac{1}{8}, P(Y=2)=\dfrac{1}{4}, P(Y=3)=\dfrac{1}{2}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- 1/8 * np.log2(1/8) -(1/8) * np.log2(1/8) -(2/8) * np.log2(2/8) -(4/8) * np.log2(4/8)</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.75</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">(2) P(Y=0)=1, P(Y=1)=0, P(Y=2)=0, P(Y=3)=0</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- 1 * np.log2(1) -(np.finfo(<span class="built_in">float</span>).eps) * np.log2(np.finfo(<span class="built_in">float</span>).eps) -(np.finfo(<span class="built_in">float</span>).eps) * np.log2(np.finfo(<span class="built_in">float</span>).eps) -(np.finfo(<span class="built_in">float</span>).eps) * np.log2(np.finfo(<span class="built_in">float</span>).eps)</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3.4638958368304884e-14</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">(3) P(Y=0)=\dfrac{1}{4}, P(Y=1)=\dfrac{1}{4}, P(Y=2)=\dfrac{1}{4}, P(Y=3)=\dfrac{1}{4}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- 1/4 * np.log2(1/4) -(1/4) * np.log2(1/4) -(1/4) * np.log2(1/4) -(1/4) * np.log2(1/4)</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2.0</span><br></pre></td></tr></table></figure><h3 id="엔트로피의-성질"><a href="#엔트로피의-성질" class="headerlink" title="엔트로피의 성질"></a>엔트로피의 성질</h3><ul><li>확률변수가 결정론적이면 확률분포에서 특정한 하나의 값이 나올 확률이 1이다. 이 때 엔트로피는 0이 되고 이 값은 엔트로피가 가질 수 있는 최솟값이다. 반대로 엔트로피의 최대값은 이산 확률변수의 클래스의 갯수에 따라 달라진다. 만약 이산확률분포가 가질 수 있는 값이 $2^{K}$개면 엔트로피의 최대값은 각 값에 대한 확률이 모두 같은 값인 $\frac{1}/{2^{K}}$이다. 엔트로피의 값은 아래의 수식과 같을 것이다.</li></ul><script type="math/tex; mode=display">\begin{align} H = -2^K \cdot \frac{1}{2^K}\log_2\dfrac{1}{2^K} = K \end{align}</script><h3 id="엔트로피의-추정"><a href="#엔트로피의-추정" class="headerlink" title="엔트로피의 추정"></a>엔트로피의 추정</h3><ul><li>이론적인 확률밀도함수가 없고 실제 데이터가 주어진 경우에는 데이터에서 확률질량함수를 추정한 후, 이를 기반으로 엔트로피를 계산한다. 예를 들어 데이터가 모두 80개가 있고 그 중 $Y=0$인 데이터가 40개, $Y=1$ 데이터가 40개 있는 경우는 엔트로피가 1이다.</li></ul><script type="math/tex; mode=display">\begin{align} P(y=0) = \dfrac{40}{80} = \dfrac{1}{2} \end{align}</script><script type="math/tex; mode=display">\begin{align} P(y=1) = \dfrac{40}{80} = \dfrac{1}{2} \end{align}</script><script type="math/tex; mode=display">\begin{align} H[Y] = -\dfrac{1}{2}\log_2\left(\dfrac{1}{2}\right) -\dfrac{1}{2}\log_2\left(\dfrac{1}{2}\right) = \dfrac{1}{2} + \dfrac{1}{2}  = 1 \end{align}</script><ul><li>Scipy의 stats 서브패키즈는 엔트로피를 구하는 <code>entropy</code>함수를 제공한다. <code>base</code>인수값은 2가 되어야 한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = [0.5, 0.5]</span><br><span class="line">sp.stats.entropy(p, base=2)</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure><blockquote><p>연습 문제)</p></blockquote><ul><li>(1) 데이터가 모두 60개가 있고 그 중 $Y=0$인 데이터가 20개, $Y=1$인 데이터가 40개 있는 경우의 엔트로피를 계산하라.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = [1/3, 2/3]</span><br><span class="line">sp.stats.entropy(p, base=2)</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9182958340544894</span><br></pre></td></tr></table></figure><ul><li>(2) 데이터가 모두 40개가 있고 그 중 $Y=0$인 데이터가 30개, $Y=1$인 데이터가 10개 있는 경우의 엔트로피를 계산하라.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = [3/4, 1/4]</span><br><span class="line">sp.stats.entropy(p, base=2)</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.8112781244591328</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = [1, 0]</span><br><span class="line">sp.stats.entropy(p, base=2)</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.0</span><br></pre></td></tr></table></figure><h3 id="가변길이-인코딩"><a href="#가변길이-인코딩" class="headerlink" title="가변길이 인코딩"></a>가변길이 인코딩</h3><ul><li>엔트로피는 원래 통신 분야에서 데이터가 가지고 있는 정보량을 계산하기 위해 고안되었다. 예를 들어 4개의 글자 A, B, C, D로 씌여진 다음과 같은 문서가 있다고 하자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">N = 200</span><br><span class="line">p = [1/2, 1/4, 1/8, 1/8]</span><br><span class="line">doc0 = list(<span class="string">""</span>.join([int(N * p[i]) * c <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(<span class="string">"ABCD"</span>)]))</span><br><span class="line">np.random.shuffle(doc0)</span><br><span class="line">doc = <span class="string">""</span>.join(doc0)</span><br><span class="line">doc</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'BDABABACBABBAACADAADAAADAAAAAABBAABADAAAAABBACAABACBBACDBAAACBCABBAABAAAAADDBABCBDBBDDBAABBBADCACAADAADCABADCAAAAACADBAABABCBAACAAABCDAADDCCCAAABABBDACACAAAAAABABBADABBABDBADBACAABDCAAABAAABACCDABAABA'</span></span><br></pre></td></tr></table></figure><ul><li><p>이 문서를 0과 1로 이루어진 이진수로 변환해야 하면 보통 다음처럼 인코딩한다.</p><ul><li>A=”00”</li><li>A=”01”</li><li>A=”10”</li><li>A=”11”</li></ul></li><li><p>이렇게 인코딩을 하면 200글자로 이루어진 문서는 이진수 400개가 된다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">encoder = &#123;<span class="string">"A"</span>: <span class="string">"00"</span>, <span class="string">"B"</span>: <span class="string">"01"</span>, <span class="string">"C"</span>: <span class="string">"10"</span>, <span class="string">"D"</span>: <span class="string">"11"</span>&#125;</span><br><span class="line">encoded_doc = <span class="string">""</span>.join([encoder[c] <span class="keyword">for</span> c <span class="keyword">in</span> doc])</span><br><span class="line">encoded_doc</span><br></pre></td></tr></table></figure><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'0111000100010010010001010000100011000011000000110000000000000101000001001100000000000101001000000100100101001011010000001001100001010000010000000000111101000110011101011111010000010101001110001000001100001110000100111000000000001000110100000100011001000010000000011011000011111010100000000100010111001000100000000000000100010100110001010001110100110100100000011110000000010000000100101011000100000100'</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(encoded_doc)</span><br></pre></td></tr></table></figure><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">400</span><br></pre></td></tr></table></figure><ul><li>그런데 이진수로 변환할 때 더 글자수를 줄일 수 있는 방법이 있다. 우선 위 글자의 분포를 조사하자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sns.countplot(list(doc), order=<span class="string">"ABCD"</span>)</span><br><span class="line">plt.title(<span class="string">"글자수의 분포"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/count_plot_with_characters_count.png" alt="글자수의 분포"></p><ul><li>글자수의 분포가 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} P(Y=A)=\dfrac{1}{2}, P(Y=B)=\dfrac{1}{4}, P(Y=C)=\dfrac{1}{8}, P(Y=D)=\dfrac{1}{8} \end{align}</script><ul><li><p>지프의 법칙(Zipf’s law)에 따르면 이러한 분포는 현실의 글자 빈도수에서도 흔히 나타난다. 확률분포가 위와 같을 때는 다음처럼 인코딩하면 인코딩된 후의 이진수 수를 줄일 수 있다.</p><ul><li>A=”0”</li><li>B=”10”</li><li>C=”110”</li><li>D=”111”</li></ul></li><li><p>이 방법은 글자마다 인코딩하는 이진수의 숫자가 다르기 때문에 가변길이 인코딩(variable length encoding)이라고 한다. 가장 많이 출현하는 ‘A’는 두 글자가 아닌 한 글자이므로 인코딩 후의 이진수 수가 감소한다. 반대로 ‘C’, ‘D’는 이진수의 수가 3개로 많지만 글자의 빈도가 적어서 영향이 적다.</p></li></ul><ul><li>만약 문서의 분포가 위에서 가정한 분포와 정확하게 같다면 인코딩된 이진수의 숫자는 다음 계산에서 350개가 됨을 알 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} \left(200 \times \dfrac{1}{2}\right) \cdot 1 + \left(200 \times \dfrac{1}{4}\right) \cdot 2 + \left(200 \times \dfrac{1}{8}\right) \cdot 3 + \left(200 \times \dfrac{1}{8}\right) \cdot 3 = 350 \end{align}</script><ul><li>따라서 알파벳 한 글자를 인코딩하는데 필요한 평균 비트(bit)수는 $350 \div 200 = 1.75$이고 이 값은 확률변수의 엔트로피 값과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} H = -\dfrac{1}{2}\log_2\dfrac{1}{2} -\dfrac{1}{4}\log_2\dfrac{1}{4} -\dfrac{2}{8}\log_2\dfrac{1}{8} = 1.75 \end{align}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vl_encoder = &#123;<span class="string">"A"</span>: <span class="string">"0"</span>, <span class="string">"B"</span>: <span class="string">"10"</span>, <span class="string">"C"</span>: <span class="string">"110"</span>, <span class="string">"D"</span>: <span class="string">"111"</span>&#125;</span><br><span class="line">vl_encoded_doc = <span class="string">""</span>.join([vl_encoder[c] <span class="keyword">for</span> c <span class="keyword">in</span> doc])</span><br><span class="line">vl_encoded_doc</span><br></pre></td></tr></table></figure><h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'10111010010011010010100011001110011100011100000010100010011100000101001100010011010100110111100001101011001010001000000111111100101101011110101111111000101010011111001100011100111110010011111000000110011110001001011010001100001011011100111111110110110000100101011101100110000000100101001110101001011110011110011000101111100001000010011011011101000100'</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(vl_encoded_doc)</span><br></pre></td></tr></table></figure><h5 id="결과-14"><a href="#결과-14" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">350</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sp.stats.entropy([1/2, 1/4, 1/8, 1/8], base=2)</span><br></pre></td></tr></table></figure><h5 id="결과-15"><a href="#결과-15" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.75</span><br></pre></td></tr></table></figure><h3 id="지니-불순도"><a href="#지니-불순도" class="headerlink" title="지니 불순도"></a>지니 불순도</h3><ul><li>엔트로피와 유사한 개념으로 지니불순도(Gini impurity)라는 것이 있다. 지니불순도는 엔트로피처럼 확률분포가 어느쪽에 치우쳐져있는가를 재는 척도지만 로그를 사용하지 않으므로 계산량이 더 적어 엔트로피 대용으로 많이 사용된다. 경젠학에서도 사용되지만 지니계수(Gini coefficient)와는 다른 개념이라는 점에 주의해야 한다.</li></ul><script type="math/tex; mode=display">\begin{align} G[Y] = \sum_{k=1}^K P(y_k) (1 - P(y_k)) \end{align}</script><ul><li>다음 그림은 값이 두 개인 이산확률분포에서 지니불순도와 엔트로피를 비교한 결과이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">P0 = np.linspace(0.001, 1 - 0.001, 1000)</span><br><span class="line">P1 = 1 - P0</span><br><span class="line">H = - P0 * np.log2(P0) - P1 * np.log2(P1)</span><br><span class="line">G = 2 * (P0 * (1 - P0) + P1 * (1 - P1))</span><br><span class="line"></span><br><span class="line">plt.plot(P1, H, <span class="string">"-"</span>, label=<span class="string">"엔트로피"</span>)</span><br><span class="line">plt.plot(P1, G, <span class="string">"--"</span>, label=<span class="string">"지니불순도"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">"P(Y=1)"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/difference_with_Gini_and_entropy.png" alt="엔트로피와 지니 불순도 비교"></p><h3 id="엔트로피-최대화"><a href="#엔트로피-최대화" class="headerlink" title="엔트로피 최대화"></a>엔트로피 최대화</h3><ul><li><p>기대값 $0$, 분산 $\sigma^{2}$이 주어졌을 때 엔트로피 $\text{H}[p(x)]$를 가장 크게 만드는 확률밀도함수 $p(x)$는 정규분포가 된다. 이는 아래와 같이 증명한다. 우선 확률 밀도함수가 지켜야 할 제한조건은 다음과 같다.</p></li><li><p>(1) 확률밀도함수의 총면적은 1</p></li></ul><script type="math/tex; mode=display">\begin{align} \int_{-\infty}^{\infty} p(x) dx = 1 \end{align}</script><ul><li>(2) 기댓값(평균)은 0</li></ul><script type="math/tex; mode=display">\begin{align} \int_{-\infty}^{\infty} xp(x) dx = 0 \end{align}</script><ul><li>(3) 분산은 $\sigma^{2}$</li></ul><script type="math/tex; mode=display">\begin{align} \int_{-\infty}^{\infty} x^2 p(x) dx = \sigma^2 \end{align}</script><ul><li>최대화할 목적범함수(objective functional)은 엔트로피이다.</li></ul><script type="math/tex; mode=display">\begin{align} \text{H}[p(x)] = -\int_{-\infty}^{\infty} p(x)\log p(x) dx \end{align}</script><ul><li>라그랑주 승수법으로 제한조건을 추가하면 다음과 같아진다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \text{H}[p(x)] &= -\int_{-\infty}^{\infty} p(x)\log p(x) dx + \lambda_1 \left( \int_{-\infty}^{\infty} p(x) dx - 1 \right) \\ & + \lambda_2 \left( \int_{-\infty}^{\infty} xp(x) dx\right) + \lambda_3 \left( \int_{-\infty}^{\infty} x^2 p(x) dx - \sigma^2 \right) \\ &= \int_{-\infty}^{\infty} \left(-p(x)\log p(x) + \lambda_1 p(x) + \lambda_2 xp(x) + \lambda_3 x^2p(x) - \lambda_1 - \lambda_3 \sigma^2 \right) dx \end{aligned} \end{align}</script><ul><li>변분법에서 도함수는 다음과 같이 계산된다.</li></ul><p><a href="https://datascienceschool.net/view-notebook/006a077ee4964eb58c6b08b213126f8f/" target="_blank" rel="noopener">참고 : 변분법</a></p><script type="math/tex; mode=display">\begin{align} \dfrac{\delta H}{\delta p(x)} = -\log p(x) - 1 + \lambda_1 + \lambda_2 x + \lambda_3 x^2 = 0 \end{align}</script><ul><li>따라서 확률밀도함수의 형태는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} p(x) = \exp \left( - 1 + \lambda_1 + \lambda_2 x + \lambda_3 x^2 \right) \end{align}</script><ul><li>적분을 통해 위 형태의 확률밀도함수의 면적, 기대값, 분산을 계산하고 주어진 제한조건을 만족하도록 연립방정식을 풀면 라그랑주 승수를 다음처럼 구할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \lambda_1 &= 1-\dfrac{1}{2} \log{2\pi\sigma^2} \\ \lambda_2 &= 0 \\ \lambda_3 &= -\dfrac{1}{2\sigma^2} \\ \end{aligned} \end{align}</script><ul><li>이 값을 대입하면 정규분포라는 것을 알 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} p(x) = \dfrac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\dfrac{x^2}{2\sigma^2} \right) \end{align}</script><ul><li>따라서 정규분포는 기댓값과 표준편차를 알고있는 확률분포 중에서 가장 엔트로피가 크고 따라서 가장 정보가 적은 확률분포이다. <code>정규분포는 베이즈 추정에 있어서 사실상의 무정보 사전확률분포로 사용되는 경우가 많다.</code></li></ul><h3 id="의사결정나무를-사용한-분류예측"><a href="#의사결정나무를-사용한-분류예측" class="headerlink" title="의사결정나무를 사용한 분류예측"></a>의사결정나무를 사용한 분류예측</h3><ul><li>의사결정나무에 전체 트레이닝 데이터를 모두 적용해 보면 각 데이터는 특정한 노드를 타고 내려가게 된다. 각 노드는 그 노드를 선택한 데이터 집합을 갖는다. 이 때 노드에 속한 데이터의 클래스의 비율을 구하여 이를 그 노드의 조건부 확률분포 $P(Y\;=\;k|X)_{node}$라고 정의한다.</li></ul><script type="math/tex; mode=display">P(Y=k|X)_{\text{node}} \approx \dfrac{N_{\text{node},k}}{N_{\text{node}}}</script><ul><li>테스트 데이터 $X_{test}$의 클래스를 예측할 때는 가장 상위의 노드부터 분류 규칙을 차례대로 적용하여 마지막에 도달하는 노드의 조건부 확률 분포를 이용하여 클래스를 예측한다.</li></ul><script type="math/tex; mode=display">\hat{Y} = \text{arg}\max_k P(Y=k|X_{\text{test}})_{\text{last node}}</script><h3 id="분류-규칙을-정하는-방법"><a href="#분류-규칙을-정하는-방법" class="headerlink" title="분류 규칙을 정하는 방법"></a>분류 규칙을 정하는 방법</h3><ul><li>분류 규칙을 정하는 방법은 부모 노드와 자식 노드간의 엔트로피를 가장 낮게 만드는 최상의 독립 변수와 기준값을 찾는 것이다. 이러한 기준을 정량화한 것이 정보획득량(Information Gain)이다. 기본적으로 모든 독립변수와 모든 가능한 기준값에 대해 정보획득량을 구하여 가장 정보획들량이 큰 독립 변수와 기준값을 선택한다.</li></ul><h3 id="Information-Gain"><a href="#Information-Gain" class="headerlink" title="Information Gain"></a>Information Gain</h3><ul><li><p>데이터 세트의 균일도는 데이터를 구분하는 데 필요한 정보의 양에 영향을 미친다. 결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만든다. 즉, 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고, 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트로 쪼개는 방식을 자식 트로 내려가면서 반복하는 방식으로 데이터 값을 예측하게 된다. 이러한 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한  Information Gain과 Gini 계수가 있다. 즉, <code>(이전 엔트로피 - 이후 엔트로피)의 차가 많이 나는 규칙부터 실행하여 균일도를 높이는 방식</code>이다. 규칙노드를 통해 이전 엔트로피와의 차이가 클수록 유의미한 규칙이 될 것이다.</p></li><li><p>정보획득량(Information Gain)는 $X$라는 조건에 의해 확률 변수 $Y$의 엔트로피가 얼마나 감소하였는가를 나타내는 값이다. 다음처럼 $Y$의 엔트로피에서 $X$에 대한 $Y$의 조건부 엔트로피를 뺀 값으로 정의된다.</p></li></ul><script type="math/tex; mode=display">IG[Y,X] = H[Y] - H[Y|X]</script><p><img src="/image/information_Gain_concept.png" alt="Information Gain의 개념"></p><h3 id="결합-엔트로피"><a href="#결합-엔트로피" class="headerlink" title="결합 엔트로피"></a>결합 엔트로피</h3><ul><li>결합엔트로피(joint entropy)는 결합확률분포를 사용하여 정의한 엔트로피를 말한다. 이산 확률변수 $X,\;Y$에 대해 결합엔트로피는 다음 처럼 정의한다. 아래 식에서 $K_{X}, K_{Y}$는 각각 $X$와 $Y$가 가질 수 있는 값의 개수이고, $p$는 결합 확률질량함수이다.</li></ul><script type="math/tex; mode=display">\begin{align} H[X, Y] = - \sum_{i=1}^{K_X} \sum_{j=1}^{K_Y} \,p(x_i, y_j) \log_2 p(x_i, y_j) \end{align}</script><ul><li>연속확률변수 $X,\;Y$에 대한 결합엔트로피는 다음처럼 정의한다. 아래 식에서 $p$는 결합 확률밀도함수이다.</li></ul><script type="math/tex; mode=display">\begin{align} H[X, Y] = - \int_{x} \int_{y} \,p(x, y) \log_2 p(x, y)  \; dxdy \end{align}</script><ul><li>결합엔트로피도 결합확률분포라는 점만 제외하면 일반적인 엔트로피와 같다. 모든 경우에 대해 골고루 확률이 분포되어 있으면 엔트로피값이 커지고 특정한 한 가지 경우에 대해 확률이 모여있으면 엔트로피가 0에 가까워진다.</li></ul><h3 id="조건부-엔트로피"><a href="#조건부-엔트로피" class="headerlink" title="조건부 엔트로피"></a>조건부 엔트로피</h3><ul><li>조건부 엔트로피(conditional entropy)는 어떤 확률 변수 $X$가 다른 확률변수 $Y$의 값을 예측하는데 도움이 되는지를 측정하는 방법 중의 하나이다. 만약 확률변수 X의 값이 어떤 특정한 하나의 값을 가질 때 확률변수 $Y$도 마찬가지로 특정한 값이 된다면 $X$로 $Y$를 예측할 수 있다. 반대로 확률변수 $X$의 값이 어떤 특정한 하나의 값을 가져도 확률변수 $Y$가 여러 값으로 골고루 분포되어 있다면 $X$는 $Y$의 값을 예측하는데 도움이 안된다.</li></ul><ul><li>조건부 엔트로피의 정의는 다음과 같이 유도한다. 확률변수 $X,\;Y$가 모두 이산확률변수라고 가정하고 $X$가 특정한 값 $x_{i]}$를 가질 떄의 $Y$의 엔트로피 $H[Y \mid X=x_i]$는 다음처럼 조건부확률분포의 엔트로피로 정의한다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y \mid X=x_i] = - \sum_{j=1}^{K_Y} p(y_j \mid x_i)  \log_2 p(y_j \mid x_i) \end{align}</script><ul><li>조건부 엔트로피는 확률변수 $X$가 가질 수 있는 모든 경우에 대해 $H[Y \mid X=x_i]$를 가중평균한 값으로 정의한다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y \mid X] &= \sum_{i=1}^{K_X} \,p(x_i)\,H[Y \mid X=x_i]  \\ &= - \sum_{i=1}^{K_X} \sum_{j=1}^{K_Y} p(y_j \mid x_i)p(x_i)  \log_2 p(y_j \mid x_i)  \\ &= - \sum_{i=1}^{K_X} \sum_{j=1}^{K_Y} p(x_i, y_j)  \log_2 p(y_j \mid x_i)  \\ \end{align}</script><ul><li>연속확률변수의 경우에는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y \mid X=x] = - \int_{y} p(y \mid x)  \log_2 p(y \mid x)\; dy \end{align}</script><script type="math/tex; mode=display">\begin{align} H[Y \mid X] &= - \int_{x} \,p(x) \,H[Y \mid X=x] \; dx \\ &= - \int_{x} p(x) \left( \int_{y} p(y \mid x)  \log_2 p(y \mid x)\; dy \right) \; dx   \\ &= - \int_{x} \int_{y} p(y \mid x_i) p(x) \log_2 p(y \mid x) \; dxdy \\ &= - \int_{x} \int_{y} \,p(x, y) \log_2 p(y \mid x) \; dxdy \\ \end{align}</script><ul><li>따라서 조건부엔트로피의 최종적인 수학적 정의는 다음과 같다.</li></ul><ul><li>이산확률변수의 경우에는 다음과 같이 정의한다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y \mid X] = - \sum_{i=1}^{K_X} \sum_{j=1}^{K_Y} \,p(x_i, y_j) \log_2 p(y_j \mid x_i) \end{align}</script><ul><li>연속확률변수의 경우에는 다음과 같이 정의한다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y \mid X] = - \int_{x} \int_{y} \,p(x, y) \log_2 p(y \mid x)  \; dxdy \end{align}</script><ul><li>다시 돌아와 Decision Tree를 이야기하자면 Decision Tree의 일반적인 알고리즘은 데이터 세트를 분할하는 데 가장 좋은 조건, 즉 Information Gain이나 Gini coefficient가 높은 조건을 찾아서 자식 트리 노등에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정한다.</li></ul><p><img src="/image/how_to_calculate_information_gain_01.png" alt="Information gain 계산 방법"></p><p><img src="/image/how_to_calculate_information_gain.png" alt="Information gatin 계산 예시"></p><p><img src="/image/Decision_tree_using_information_gain.png" alt="Inforamtion gain을 이용한 Decision Tree 분류"></p><p><img src="/image/how_to_split_node_next_step_using_information_gain.png" alt="Decision Tree의 분할과정"></p><p><img src="/image/decision_tree_process_cal.png" alt="Decision Tree의 분할"></p><h3 id="Information-Gain-예시"><a href="#Information-Gain-예시" class="headerlink" title="Information Gain 예시"></a>Information Gain 예시</h3><p><img src="/image/conditional_entropy_calculate_example.png" alt="정보획득량 예시"></p><ul><li>예를 들어 A,B 두 가지의 다른 분류 규칙을 적용했더니 위에서 처럼 서로 다르게 데이터가 나뉘어 졌다고 가정하자.</li></ul><ul><li>A방법과 B방법 모두 노드 분리전에는 Y=0인 데이터의 수와 Y=1인 데이터의 수가 모두 40개였다.</li></ul><ul><li>A방법으로 노드를 분리하면 다음과 같은 두 개의 자식 노드가 생긴다.<ul><li>자식 노드 A1은 Y=0인 데이터가 30개, Y=1인 데이터가 10개</li><li>자식 노드 A2은 Y=0인 데이터가 10개, Y=1인 데이터가 30개</li></ul></li></ul><ul><li>B방법으로 노드를 분리하면 다음과 같은 두 개의 자식 노드가 생긴다.<ul><li>자식 노드 B1은 Y=0인 데이터가 20개, Y=1인 데이터가 40개</li><li>자식 노드 B2은 Y=0인 데이터가 20개, Y=1인 데이터가 30개</li></ul></li></ul><ul><li>우선 부모 노드의 엔트로피를 계산하면 다음과 같다.</li></ul><script type="math/tex; mode=display">H[Y] = -\dfrac{1}{2}\log_2\left(\dfrac{1}{2}\right) -\dfrac{1}{2}\log_2\left(\dfrac{1}{2}\right) = \dfrac{1}{2} + \dfrac{1}{2}  = 1</script><ul><li>A 방법에 대해 IG를 계산하면 다음과 같다.</li></ul><script type="math/tex; mode=display">H[Y|X=X_1] = -\dfrac{3}{4}\log_2\left(\dfrac{3}{4}\right) -\dfrac{1}{4}\log_2\left(\dfrac{1}{4}\right) = 0.81</script><script type="math/tex; mode=display">H[Y|X=X_2] = -\dfrac{1}{4}\log_2\left(\dfrac{1}{4}\right)  -\dfrac{3}{4}\log_2\left(\dfrac{3}{4}\right) = 0.81</script><script type="math/tex; mode=display">H[Y|X] = \dfrac{1}{2} H[Y|X=X_1] + \dfrac{1}{2} H[Y|X=X_2] = 0.81</script><script type="math/tex; mode=display">IG = H[Y] - H[Y|X] = 0.19</script><ul><li>B 방법에 대해 IG를 계산하면 다음과 같다.</li></ul><script type="math/tex; mode=display">H[Y|X=X_1] = -\dfrac{1}{3}\log_2\left(\dfrac{1}{3}\right) - \dfrac{2}{3}\log_2\left(\dfrac{2}{3}\right) = 0.92</script><script type="math/tex; mode=display">H[Y|X=X_2] = 0</script><script type="math/tex; mode=display">H[Y|X] = \dfrac{3}{4} H[Y|X=X_1] + \dfrac{1}{4} H[Y|X=X_2] = 0.69</script><script type="math/tex; mode=display">IG = H[D] - H[Y|X] = 0.31</script><ul><li><code>따라서 B 방법이 더 나은 방법임을 알 수 있다.</code></li></ul><ul><li>위와 같이 model을 fitting하였으면, 이제 어떻게 해당 영역에 예측할 데이터가 포함된다면 어떻게 클래스를 정하는지에 대해서 알아볼 것이다. 간단히 말하자면 해당 영역에 포함된 클래스의 개수가 많은 것으로 예측한다.</li></ul><p><img src="/image/how_to_precidct_data_class_01.png" alt="Decision tree의 predict - 01"></p><p><img src="/image/how_to_precidct_data_class_02.png" alt="Decision tree의 predict - 01"></p><p><img src="/image/how_to_precidct_data_class_03.png" alt="Decision tree의 predict - 01"></p><h3 id="Decision-Tree의-특징"><a href="#Decision-Tree의-특징" class="headerlink" title="Decision Tree의 특징"></a>Decision Tree의 특징</h3><ul><li>결정 트리의 가장 큰 장점은 정보의 균일도라는 률을 기반으로 하고 있어서 <code>알고리즘이 쉽고 직관적</code>이라는 점이다. 결정 트리가 룰이 매우 명확하고, 이에 기반해 어떻게 규칙 노드와 리프 노드가 만들어지는지 알 수 있고, 시각화로 표현까지 할 수 있다. <code>또한 정보의 균일도만 신경쓰면 되므로 특별한 경우를 제외하고는 각 feature의 스케일링과 normalization과 같은 작업이 크게 영향을 미치지 않는다.</code> 반면에 결정 트리 모델의 가장 큰 단점은 <code>과적합(overfitting)으로 정확도가 떨어진다는 점</code>이다. 복잡한 학습 모델은 결국에는 실제 상황(테스트 데이터)에 유연하게 대처할 수 없어서 예측 성능이 떨어질 수 밖에 없다. <code>트리의 크기를 사전에 제한하는 것이 오히려 성능 튜닝에 더 도움이 될 것</code>이다.</li></ul><h3 id="Scikit-Learn의-의사결정나무-클래스"><a href="#Scikit-Learn의-의사결정나무-클래스" class="headerlink" title="Scikit-Learn의 의사결정나무 클래스"></a>Scikit-Learn의 의사결정나무 클래스</h3><h2 id="Scikit-Learn에서-의사결정나무는-DecisionTreeClassifier클래스로-구현되어있다-여기에서는-붓꽃-분류-문제를-예를-들어-의사결정나무를-설명한다-이-예제에서는-독립변수-공간을-공간상에-표시하기-위해-꽃의-길이와-폭만을-독립변수로-사용하였다"><a href="#Scikit-Learn에서-의사결정나무는-DecisionTreeClassifier클래스로-구현되어있다-여기에서는-붓꽃-분류-문제를-예를-들어-의사결정나무를-설명한다-이-예제에서는-독립변수-공간을-공간상에-표시하기-위해-꽃의-길이와-폭만을-독립변수로-사용하였다" class="headerlink" title="- Scikit-Learn에서 의사결정나무는 DecisionTreeClassifier클래스로 구현되어있다. 여기에서는 붓꽃 분류 문제를 예를 들어 의사결정나무를 설명한다. 이 예제에서는 독립변수 공간을 공간상에 표시하기 위해 꽃의 길이와 폭만을 독립변수로 사용하였다."></a>- Scikit-Learn에서 의사결정나무는 <code>DecisionTreeClassifier</code>클래스로 구현되어있다. 여기에서는 붓꽃 분류 문제를 예를 들어 의사결정나무를 설명한다. 이 예제에서는 독립변수 공간을 공간상에 표시하기 위해 꽃의 길이와 폭만을 독립변수로 사용하였다.</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line"></span><br><span class="line">data = load_iris()</span><br><span class="line">y = data.target</span><br><span class="line">X = data.data[:, 2:]</span><br><span class="line">feature_names = data.feature_names[2:]</span><br><span class="line"></span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">tree1 = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, max_depth=1, random_state=0).fit(X, y)</span><br></pre></td></tr></table></figure><hr><ul><li>다음은 의사결정나무를 시각화하기 위한 코드이다. <code>draw_decision_tree</code>함수는 의사결정나무의 의사 결정 과정의 세부적인 내역을 다이어그램으로 보여주고 <code>plot_decision_regions</code>함수는 이러한 의사 결정에 의해 데이터의 영역이 어떻게 나뉘어졌는지를 시각화하여 보여준다.</li></ul><ul><li>아래 방법으로 draw_decision_tree함수를 동일하게 출력할 수 있다.<ul><li><code>filled</code>: 그래프에 각 클래스별로 색상을 입힘.</li><li><code>rounded</code>: 반올림 시켜주는 역할</li><li><code>special_characters</code>: 특수문자가 있을 경우 제외시켜줌.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dot_data=tree.export_graphviz(out_file=None, decision_tree=tree1,feature_names=iris.feature_names,</span><br><span class="line">                             class_names=iris.target_names,</span><br><span class="line">                             filled=True, rounded=True, special_characters=True)</span><br><span class="line">graphviz.Source(dot_data)</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import io</span><br><span class="line">import pydot</span><br><span class="line">from IPython.core.display import Image</span><br><span class="line">from sklearn.tree import export_graphviz</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_decision_tree(model):</span><br><span class="line">    dot_buf = io.StringIO()</span><br><span class="line">    export_graphviz(model, out_file=dot_buf, feature_names=feature_names)</span><br><span class="line">    graph = pydot.graph_from_dot_data(dot_buf.getvalue())[0]</span><br><span class="line">    image = graph.create_png()</span><br><span class="line">    <span class="built_in">return</span> Image(image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot_decision_regions(X, y, model, title):</span><br><span class="line">    resolution = 0.01</span><br><span class="line">    markers = (<span class="string">'s'</span>, <span class="string">'^'</span>, <span class="string">'o'</span>)</span><br><span class="line">    colors = (<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'lightgreen'</span>)</span><br><span class="line">    cmap = mpl.colors.ListedColormap(colors)</span><br><span class="line"></span><br><span class="line">    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),</span><br><span class="line">                           np.arange(x2_min, x2_max, resolution))</span><br><span class="line">    Z = model.predict(</span><br><span class="line">        np.array([xx1.ravel(), xx2.ravel()]).T).reshape(xx1.shape)</span><br><span class="line"></span><br><span class="line">    plt.contour(xx1, xx2, Z, cmap=mpl.colors.ListedColormap([<span class="string">'k'</span>]))</span><br><span class="line">    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)</span><br><span class="line">    plt.xlim(xx1.min(), xx1.max())</span><br><span class="line">    plt.ylim(xx2.min(), xx2.max())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, cl <span class="keyword">in</span> enumerate(np.unique(y)):</span><br><span class="line">        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8,</span><br><span class="line">                    c=[cmap(idx)], marker=markers[idx], s=80, label=cl)</span><br><span class="line"></span><br><span class="line">    plt.xlabel(data.feature_names[2])</span><br><span class="line">    plt.ylabel(data.feature_names[3])</span><br><span class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> Z</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_decision_tree(tree1)</span><br></pre></td></tr></table></figure><hr><h2 id><a href="#" class="headerlink" title></a><img src="/image/iris_data_first_divide_on_plot_01.png" alt="max_depth=1인 경우의 decision tree의 분할 - 01"></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_regions(X, y, tree1, <span class="string">"Depth 1"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><h2 id="-1"><a href="#-1" class="headerlink" title></a><img src="/image/iris_data_first_divide_on_plot_02.png" alt="max_depth=1인 경우의 decision tree의 분할 - 02"></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line"></span><br><span class="line">confusion_matrix(y, tree1.predict(X))</span><br></pre></td></tr></table></figure><h5 id="결과-16"><a href="#결과-16" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[50,  0,  0],</span><br><span class="line">       [ 0, 50,  0],</span><br><span class="line">       [ 0, 50,  0]])</span><br></pre></td></tr></table></figure><hr><h2 id="depth-2로-변경한-후의-결과"><a href="#depth-2로-변경한-후의-결과" class="headerlink" title="- depth=2로 변경한 후의 결과"></a>- depth=2로 변경한 후의 결과</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tree2 = DecisionTreeClassifier(</span><br><span class="line">    criterion=<span class="string">'entropy'</span>, max_depth=2, random_state=0).fit(X, y)</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_decision_tree(tree2)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_03.png" alt="max_depth=2로 한 후의 분할 - 01"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_regions(X, y, tree2, <span class="string">"Depth 2"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_04.png" alt="max_depth=2로 한 후의 분할 - 02"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusion_matrix(y, tree2.predict(X))</span><br></pre></td></tr></table></figure><h5 id="결과-17"><a href="#결과-17" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[50,  0,  0],</span><br><span class="line">       [ 0, 49,  1],</span><br><span class="line">       [ 0,  5, 45]])</span><br></pre></td></tr></table></figure><hr><ul><li>max_depth=3으로 변경한 후의 결과</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tree3 = DecisionTreeClassifier(</span><br><span class="line">    criterion=<span class="string">'entropy'</span>, max_depth=3, random_state=0).fit(X, y)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_decision_tree(tree3)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_05.png" alt="max_depth=3로 한 후의 분할 - 01"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_regions(X, y, tree3, <span class="string">"Depth 3"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_06.png" alt="max_depth=3로 한 후의 분할 - 01"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusion_matrix(y, tree3.predict(X))</span><br></pre></td></tr></table></figure><h5 id="결과-18"><a href="#결과-18" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[50,  0,  0],</span><br><span class="line">       [ 0, 47,  3],</span><br><span class="line">       [ 0,  1, 49]])</span><br></pre></td></tr></table></figure><hr><ul><li>max_depth=5로 변경한 후 결과</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tree5 = DecisionTreeClassifier(</span><br><span class="line">    criterion=<span class="string">'entropy'</span>, max_depth=5, random_state=0).fit(X, y)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_decision_tree(tree5)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_07.png" alt="max_depth=5로 한 후의 분할 - 01"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_regions(X, y, tree5, <span class="string">"Depth 5"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_08.png" alt="max_depth=5로 한 후의 분할 - 02"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusion_matrix(y, tree5.predict(X))</span><br></pre></td></tr></table></figure><h5 id="결과-19"><a href="#결과-19" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[50,  0,  0],</span><br><span class="line">       [ 0, 49,  1],</span><br><span class="line">       [ 0,  0, 50]])</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import classification_report</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classification_report(y, tree5.predict(X)))</span><br></pre></td></tr></table></figure><h5 id="결과-20"><a href="#결과-20" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       1.00      1.00      1.00        50</span><br><span class="line">           1       1.00      0.98      0.99        50</span><br><span class="line">           2       0.98      1.00      0.99        50</span><br><span class="line"></span><br><span class="line">    accuracy                           0.99       150</span><br><span class="line">   macro avg       0.99      0.99      0.99       150</span><br><span class="line">weighted avg       0.99      0.99      0.99       150</span><br></pre></td></tr></table></figure><hr><ul><li>교차검증을 통해 최종모형의 성능을 살펴보면 아래와 같다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold, cross_val_score</span><br><span class="line"></span><br><span class="line">cv = KFold(5, shuffle=True, random_state=0)</span><br><span class="line">model = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, max_depth=5,</span><br><span class="line">                               random_state=0)</span><br><span class="line">cross_val_score(model, X, y, scoring=<span class="string">"accuracy"</span>, cv=cv).mean()</span><br></pre></td></tr></table></figure><h5 id="결과-21"><a href="#결과-21" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9466666666666667</span><br></pre></td></tr></table></figure><hr><h3 id="Regression-tree"><a href="#Regression-tree" class="headerlink" title="Regression tree"></a>Regression tree</h3><ul><li>regression tree는 종속변수가 연속형인 것만 제외했을땐 아래에서 보는 것과 같이 동일한 개념을 가진다.</li></ul><p><img src="/image/Regression_Tree_concept_01.png" alt="Regression Tree의 개념"></p><ul><li>Regression Tree는 <code>예측시에 각각의 영역에 대해 특정 실수값을 주는 방식</code>이다. 아래 수식에서 $c_{m}$은 오른쪽 그림에서와 같이 해당 영역의 높이라고 할 수 있다.</li></ul><p><img src="/image/Regression_Tree_concept_02.png" alt="Regression Tree 시각적으로 이해하기"></p><ul><li>classification의 경우에는 entropy로 정했지만, Regression의 경우에는 아래 2번째 수식에서 볼 수 있듯이 기본적인 회귀의 성능지표를 사용하여 변수를 선택하게 된다.</li></ul><p><img src="/image/Regression_Tree_concept_03.png" alt="Regression Tree 변수 선택방법"></p><ul><li>이렇게 결정된 영역에 속하는 예측값은 해당 영역의 평균값을 출력해주며, 영역별로(층별로) 존재한다.</li></ul><p><img src="/image/Regression_Tree_concept_04.png" alt="Regression Tree 예측"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line">rng = np.random.RandomState(1)</span><br><span class="line">X = np.sort(5 * rng.rand(80, 1), axis=0)</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line">y[::5] += 3 * (0.5 - rng.rand(16))</span><br><span class="line"></span><br><span class="line">regtree = DecisionTreeRegressor(max_depth=3)</span><br><span class="line">regtree.fit(X, y)</span><br><span class="line"></span><br><span class="line">X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]</span><br><span class="line">y_hat = regtree.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the results</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, s=20, edgecolor=<span class="string">"black"</span>, c=<span class="string">"darkorange"</span>, label=<span class="string">"데이터"</span>)</span><br><span class="line">plt.plot(X_test, y_hat, color=<span class="string">"cornflowerblue"</span>, linewidth=2, label=<span class="string">"예측"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x"</span>)</span><br><span class="line">plt.ylabel(r<span class="string">"<span class="variable">$y</span>$ &amp; $\hat&#123;y&#125;$"</span>)</span><br><span class="line">plt.title(<span class="string">"회귀 나무"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/Regression_tree_plot.png" alt="회귀나무"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/26/machine_learning_13/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Support Vector Machine(SVM) - 02</title>
      <link>https://heung-bae-lee.github.io/2020/04/25/machine_learning_12/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/25/machine_learning_12/</guid>
      <pubDate>Sat, 25 Apr 2020 11:03:51 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;커널-서포트-벡터-머신-SVM의-심화적-이해&quot;&gt;&lt;a href=&quot;#커널-서포트-벡터-머신-SVM의-심화적-이해&quot; class=&quot;headerlink&quot; title=&quot;커널 서포트 벡터 머신 - SVM의 심화적 이해&quot;&gt;&lt;/a&gt;커널 서포트 벡터 머신
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="커널-서포트-벡터-머신-SVM의-심화적-이해"><a href="#커널-서포트-벡터-머신-SVM의-심화적-이해" class="headerlink" title="커널 서포트 벡터 머신 - SVM의 심화적 이해"></a>커널 서포트 벡터 머신 - SVM의 심화적 이해</h1><p><img src="/image/svm_deep_mind.png" alt="SVM 심화적 이해"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(0)</span><br><span class="line">X_xor = np.random.randn(200, 2)</span><br><span class="line">y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &gt; 0)</span><br><span class="line">y_xor = np.where(y_xor, 1, 0)</span><br><span class="line">plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1],</span><br><span class="line">            c=<span class="string">'b'</span>, marker=<span class="string">'o'</span>, label=<span class="string">'클래스 1'</span>, s=50)</span><br><span class="line">plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1],</span><br><span class="line">            c=<span class="string">'r'</span>, marker=<span class="string">'s'</span>, label=<span class="string">'클래스 0'</span>, s=50)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.title(<span class="string">"XOR 문제"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/XOR_problem_with_svm.png" alt="XOR 문제"></p><h3 id="XOR-문제"><a href="#XOR-문제" class="headerlink" title="XOR 문제"></a>XOR 문제</h3><ul><li>위의 그림과같이 퍼셉트론이나 서포트 벡터 머신과 같은 선형판별함수 분류 모형은 다음과 같은 XOR(exclusive OR) 문제를 풀지 못한다는 단점이 있다. 이러한 경우에는 위의 그림에서 볼 수 있듯이 선형 판별평면(decision hyperplane)으로 영역을 나눌 수 없기 때문이다. 따라서 일반적인 SVM을 사용하면 XOR문제를 풀 수 없다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def plot_xor(X, y, model, title, xmin=-3, xmax=3, ymin=-3, ymax=3):</span><br><span class="line">    XX, YY = np.meshgrid(np.arange(xmin, xmax, (xmax-xmin)/1000),</span><br><span class="line">                         np.arange(ymin, ymax, (ymax-ymin)/1000))</span><br><span class="line">    ZZ = np.reshape(model.predict(</span><br><span class="line">        np.array([XX.ravel(), YY.ravel()]).T), XX.shape)</span><br><span class="line">    plt.contourf(XX, YY, ZZ, cmap=mpl.cm.Paired_r, alpha=0.5)</span><br><span class="line">    plt.scatter(X[y == 1, 0], X[y == 1, 1], c=<span class="string">'b'</span>,</span><br><span class="line">                marker=<span class="string">'o'</span>, label=<span class="string">'클래스 1'</span>, s=50)</span><br><span class="line">    plt.scatter(X[y == 0, 0], X[y == 0, 1], c=<span class="string">'r'</span>,</span><br><span class="line">                marker=<span class="string">'s'</span>, label=<span class="string">'클래스 0'</span>, s=50)</span><br><span class="line">    plt.xlim(xmin, xmax)</span><br><span class="line">    plt.ylim(ymin, ymax)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line"></span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line"></span><br><span class="line">svc = SVC(kernel=<span class="string">"linear"</span>).fit(X_xor, y_xor)</span><br><span class="line">plot_xor(X_xor, y_xor, svc, <span class="string">"선형 SVC 모형을 사용한 XOR 분류 결과"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/XOR_problem_with_svm_not_solved.png" alt="선형 SVC 모형을 사용한 XOR 분류 결과"></p><h3 id="변환함수를-사용한-비선형-판별-모형"><a href="#변환함수를-사용한-비선형-판별-모형" class="headerlink" title="변환함수를 사용한 비선형 판별 모형"></a>변환함수를 사용한 비선형 판별 모형</h3><ul><li>이러한 경우 도움이 되는 것이 원래의 $D$차원 독립 변수 벡터 $x$ 대신 비선형 함수로 변환한 $M$차원 벡터 $\phi(x)$를 독립 변수로 사용하는 방법이다.</li></ul><script type="math/tex; mode=display">\phi(\cdot): {R}^D \rightarrow {R}^M</script><script type="math/tex; mode=display">x=(x_1, x_2, \cdots, x_D) \;\;\; \rightarrow \;\;\; \phi(x) = (\phi_1(x), \phi_2(x), \cdots, \phi_M(x))</script><ul><li>앞서 XOR 문제를 풀기 위해 다음과 같이 상호 곱 (cross-multiplication) 항을 추가한 변환함수를 사용해 보자.</li></ul><script type="math/tex; mode=display">(x_1, x_2) \;\;\; \rightarrow \;\;\; \phi(x) = (x_1^2, \sqrt{2}x_1x_2, x_2^2)</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(6).reshape(3, 2)</span><br><span class="line">X</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[0, 1],</span><br><span class="line">       [2, 3],</span><br><span class="line">       [4, 5]])</span><br></pre></td></tr></table></figure><ul><li><code>FunctionTransformer</code> 전처리 클래스로 위와 변환함수를 이용한 변환을 할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import FunctionTransformer</span><br><span class="line"></span><br><span class="line">def basis(X):</span><br><span class="line">    <span class="built_in">return</span> np.vstack([X[:, 0]**2, np.sqrt(2)*X[:, 0]*X[:, 1], X[:, 1]**2]).T</span><br><span class="line"></span><br><span class="line">FunctionTransformer(basis).fit_transform(X)</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.        ,  0.        ,  1.        ],</span><br><span class="line">       [ 4.        ,  8.48528137,  9.        ],</span><br><span class="line">       [16.        , 28.28427125, 25.        ]])</span><br></pre></td></tr></table></figure><ul><li>위와 같은 변환함수를 써서 XOR 문제의 데이터를 변환하면 특성 $\phi_2$를 사용하여 클래스 분류를 할 수 있다는 것을 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X_xor2 = FunctionTransformer(basis).fit_transform(X_xor)</span><br><span class="line">plt.scatter(X_xor2[y_xor == 1, 0], X_xor2[y_xor == 1, 1], c=<span class="string">"b"</span>, marker=<span class="string">'o'</span>, s=50)</span><br><span class="line">plt.scatter(X_xor2[y_xor == 0, 0], X_xor2[y_xor == 0, 1], c=<span class="string">"r"</span>, marker=<span class="string">'s'</span>, s=50)</span><br><span class="line">plt.ylim(-6, 6)</span><br><span class="line">plt.title(<span class="string">"변환 공간에서의 데이터 분포"</span>)</span><br><span class="line">plt.xlabel(r<span class="string">"$\phi_1$"</span>)</span><br><span class="line">plt.ylabel(r<span class="string">"$\phi_2$"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/transformation_space_dist_data_plot.png" alt="변환 공간에서의 데이터 분포"></p><ul><li>다음 코드는 <code>Pipeline</code>클래스로 변환함수 전처리기와 <code>SVC</code> 클래스를 합친 모형의 분류 결과이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">basismodel = Pipeline([(<span class="string">"basis"</span>, FunctionTransformer(basis)),</span><br><span class="line">                       (<span class="string">"svc"</span>, SVC(kernel=<span class="string">"linear"</span>))]).fit(X_xor, y_xor)</span><br><span class="line">plot_xor(X_xor, y_xor, basismodel, <span class="string">"변환함수 SVC 모형을 사용한 XOR 분류 결과"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/SVC_model_using_transformation_function_xor_problem_result.png" alt="변환함수 SVC 모형을 사용한 XOR 분류 결과"></p><h3 id="커널-트릭"><a href="#커널-트릭" class="headerlink" title="커널 트릭"></a>커널 트릭</h3><p><img src="/image/svm_deep_mind_01.png" alt="커널 도입"></p><ul><li>서포트 벡터 머신의 경우 목적 함수(비용 함수)와 예측 모형은 다음과 같이 dual form으로 표현할 수 있다.</li></ul><script type="math/tex; mode=display">L =  \sum_{n=1}^N a_n - \dfrac{1}{2}\sum_{n=1}^N\sum_{m=1}^N a_n a_m y_n y_m x_n^T x_m</script><script type="math/tex; mode=display">y = w^T x - w_0 = \sum_{n=1}^N a_n y_n x_n^T x - w_0</script><ul><li>이 수식에서 $x$를 변환함수 변환으로 $\phi(x)$로 바꾸면 아래와 같이 된다. 즉, 모든 변환함수는 $\phi(x_i)^T\phi(x_j)$의 형태로만 사용되며 독립적으로 사용되지 않는다. 따라서 두 개의 변환된 독립 변수 벡터를 내적(inner product)한 값 $\phi(x_i)^T\phi(x_j)$를 하나의 함수로 나타낼 수 있다.</li></ul><script type="math/tex; mode=display">L =  \sum_{n=1}^N a_n - \dfrac{1}{2}\sum_{n=1}^N\sum_{m=1}^N a_n a_m y_n y_m \phi(x_n)^T \phi(x_m)</script><script type="math/tex; mode=display">y = w^T x - w_0 = \sum_{n=1}^N a_n y_n \phi(x_n)^T \phi(x) - w_0</script><ul><li><p>이렇게 하나의 함수로 나타낸 것을 커널(kernel)이라고 한다. 대응하는 변환함수가 존재할 수만 있다면 변환함수를 먼저 정의하고 커널을 정의하는 것이 아니라 커널을 먼저 정의해도 상관없다.</p></li><li><p>커널이 제 역할을 하려면 $x_i$와 $x_j$의 유사도를 측정하는 함수여야한다. 또한 커널함수에서 도로 기저함수 포맷으로 만들어질수도 있어야한다.</p></li></ul><h3 id="커널의-의미"><a href="#커널의-의미" class="headerlink" title="커널의 의미"></a>커널의 의미</h3><ul><li>서포트 벡터 머신의 목적 함수와 예측 모형은 커널을 사용하여 표현하면 다음과 같다.</li></ul><script type="math/tex; mode=display">L =  \sum_{n=1}^N a_n - \dfrac{1}{2}\sum_{n=1}^N\sum_{m=1}^N a_n a_m y_n y_m k(x_n, x_m)</script><script type="math/tex; mode=display">y = w^T x - w_0 = \sum_{n=1}^N a_n y_n k(x_n, x) - w_0</script><ul><li><p>커널을 사용하지 않는 경우 $k(x,y) = x^{T}y$라는 점을 고려하면 커널은 다음과 같은 특징을 보인다.</p><ul><li>$x$와 $y$가 동일한 벡터일 때 가장 크고</li><li>두 벡터간의 거리가 멀어질수록 작아진다.</li></ul></li><li><p><code>즉, 두 표본 데이터간의 유사도(similarity)를 측정하는 기준으로 볼 수도 있다.</code></p></li></ul><h3 id="커널-사용의-장점"><a href="#커널-사용의-장점" class="headerlink" title="커널 사용의 장점"></a>커널 사용의 장점</h3><ul><li><p>커널을 사용하면 basis 함수를 하나씩 정의하는 수고를 덜 수 있을뿐더러 변환과 내적에 들어가는 계산량이 줄어든다. 예를 들어, 다음과 같은 변환함수의 경우 커널방법을 쓰지 않을 경우에 $\phi(x_i)^T \phi(x_j)$를 계산하려면 $4\;+\;4\;+\;3 \;=\;11$번의 곱셈을 해야 한다.</p><ul><li>$\phi(x_1)$ 계산 : 곱셈 4회</li><li>$\phi(x_2)$ 계산 : 곱셈 4회</li><li>내적 계산 : 곱셈 3회</li></ul></li></ul><script type="math/tex; mode=display">\phi(x_i) = \phi([x_{i,1}, x_{i,2}]) = (x_{i,1}^2, \sqrt{2}x_{i,1}x_{i,2}, x_{i,2}^2)</script><ul><li>그런데 이 변환함수는 다음과 같은 커널로 대체가능하다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} k(x_1, x_2) &=& (x_1^Tx_2)^2 \\ &=& (x_{1,1}x_{2,1} + x_{1,2}x_{2,2})^2 \\ &=& x_{1,1}^2x_{2,1}^2 + 2x_{1,1}x_{2,1}x_{1,2}x_{2,2} + x_{1,2}^2y_{2,2}^2 \\ &=& (x_{1,1}^2, \sqrt{2}x_{1,1}x_{1,2}, x_{1,2}^2)  (x_{2,1}^2, \sqrt{2}x_{2,1}x_{2,2}, x_{2,2}^2)^T \\ &=& \phi(x_1)^T \phi(x_2) \end{eqnarray}</script><ul><li><p>커널을 사용하면 $\phi(x_1)^T \phi(x_2)$을 계산하는데 $2\;+\;1\;=\;3$ 번의 곱셈이면 된다.</p><ul><li>$x_1^Tx_2$: 곱셈 2회</li><li>제곱 : 곱셈 1회</li></ul></li></ul><h3 id="커널의-확장-생성"><a href="#커널의-확장-생성" class="headerlink" title="커널의 확장 생성"></a>커널의 확장 생성</h3><ul><li><p><code>어떤 함수가 커널함수가 된다는 것을 증명하기 위해서는 변환함수를 하나 하나 정의할 필요없이 변환함수의 내적으로 표현할 수 있다는 것만 증명</code>하면 된다. 하지만 실제로는 다음 규칙을 이용하면 이미 만들어진 커널 $k_1(x_1, x_2), k_2(x_1, x_2)$로 부터 새로운 커널을 쉽게 만들 수 있다.</p></li><li><ol><li>커널함수를 양수배한 함수는 커널함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = ck_1(x_1, x_2)\;\;(c > 0)</script><ul><li><ol><li>커널함수에 양수인 상수를 더한 함수는 커널 함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = k_1(x_1, x_2) + c\;\;(c > 0)</script><ul><li><ol><li>두 커널함수를 더한 함수는 커널함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = k_1(x_1, x_2) + k_2(x_1, x_2)</script><ul><li><ol><li>두 커널함수를 곱한 함수는 커널함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = k_1(x_1, x_2)k_2(x_1, x_2)</script><ul><li><ol><li>커널함수를 $x\geq0$에서 단조증가(monotonically increasing)하는 함수에 적용하면 커널함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = (k_1(x_1, x_2))^n \;\; (n=1, 2, \cdots)</script><script type="math/tex; mode=display">k(x_1, x_2) = \exp(k_1(x_1, x_2))</script><script type="math/tex; mode=display">k(x_1, x_2) = \text{sigmoid}(k_1(x_1, x_2))</script><ul><li><ol><li>$x_{1}, x_{2}$ 각각의 커널함수값의 곱도 커널함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = k_1(x_1, x_1)k_2(x_2, x_2)</script><h3 id="많이-사용되는-커널"><a href="#많이-사용되는-커널" class="headerlink" title="많이 사용되는 커널"></a>많이 사용되는 커널</h3><p><img src="/image/svm_deep_mind_02.png" alt="커널의 종류"></p><ul><li>다음과 같은 커널들이 많이 사용되는 커널들이다. 이 커널들은 대부분 변환함수로 변환하였을 때 무한대의 차원을 가지는 변환함수가 된다. 따라서 대부분의 비선형성을 처리할 수 있다. 비교를 위해 선형 서포트 벡터 머신의 경우도 추가하였다.</li></ul><blockquote><p>선형 서포트 벡터 머신</p></blockquote><script type="math/tex; mode=display">k(x_1, x_2) = x_1^Tx_2</script><blockquote><p>다항 커널 (Polynomial Kernel)</p></blockquote><script type="math/tex; mode=display">k(x_1, x_2) = (\gamma (x_1^Tx_2) + \theta)^d</script><blockquote><p>RBF(Radial Basis Function) 또는 가우시안 커널(Gaussian Kernel)</p><pre><code>- $\gamma = \frac{1}{2\sigma^{2}}$인 경우 가우시안 분포를 따르게 된다.</code></pre></blockquote><script type="math/tex; mode=display">k(x_1, x_2) = \exp \left( -\gamma ||x_1-x_2||^2 \right)</script><blockquote><p>시그모이드 커널 (Sigmoid Kernel)</p></blockquote><script type="math/tex; mode=display">k(x_1, x_2) = \tanh(\gamma (x_1^Tx_2) + \theta)</script><ul><li>앞에서 사용한 변환함수는 $\gamma \;=\;1,,\; \theta\;=\;0, \;d\;=\;2$인 다항 커널임을 알 수 있다.</li></ul><h3 id="다항-커널"><a href="#다항-커널" class="headerlink" title="다항 커널"></a>다항 커널</h3><ul><li>다항 커널은 벡터의 내적으로 정의된 커널을 확장하여 만든 커널이다. 아래에서 다항 커널이 어떤 변환함수로 되어 있는지 알아볼 것이다.</li></ul><ul><li>간단한 경우로 $\gamma \;=\;1,,\; \theta\;=\;1, \;d\;=\;4$이고 $x$가 스칼라인 경우에는 아래와 같으며, 마지막 수식에서 볼 수 있듯이 변환함수의 내적이 된다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} k(x_1, x_2) &=& (x_1^Tx_2 + 1)^4 \\ &=& x_1^4x_2^4 + 4x_1^3x_2^3 + 6x_1^2x_2^2 + 4x_1x_2 + 1 \\ &=& (x_1^4, 2x_1^3, \sqrt{6}x_1, 2x_1, 1)^T (x_2^4, 2x_2^3, \sqrt{6}x_2, 2x_2, 1) \ \\ \end{eqnarray}</script><ul><li>즉, 변환함수는 다음 5개가 된다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} \phi_1(x) &=& x^4 \\ \phi_2(x) &=& 2x^3 \\ \phi_3(x) &=& \sqrt{6}x^2 \\ \phi_4(x) &=& 2x \\ \phi_5(x) &=& 1 \\ \end{eqnarray}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">x1 = 1</span><br><span class="line">x2 = np.linspace(-3, 3, 100)</span><br><span class="line"></span><br><span class="line">def poly4(x1, x2):</span><br><span class="line">    <span class="built_in">return</span> (x1 * x2 + 1) ** 4</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8, 4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.plot(x2, poly4(x1, x2), ls=<span class="string">"-"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.title(<span class="string">"4차 다항커널의 예"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(122)</span><br><span class="line">plt.plot(x2, x2 ** 4)</span><br><span class="line">plt.plot(x2, 2 * x2 ** 3)</span><br><span class="line">plt.plot(x2, np.sqrt(6) * x2 ** 2)</span><br><span class="line">plt.plot(x2, 2 * x2)</span><br><span class="line">plt.plot(x2, np.ones_like(x2))</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.title(<span class="string">"4차 다항커널의 변환함수들"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/4th_polynomial_function_kernel_and_transformation_functions.png" alt="4차 다항 커널과 그에 따른 변환함수들"></p><h3 id="RBF-커널"><a href="#RBF-커널" class="headerlink" title="RBF 커널"></a>RBF 커널</h3><ul><li>RBF 커널은 가우시안 커널이라고도 한다. 문제를 간단하게 하기 위해 다음과 같이 가정할 것이다.</li></ul><script type="math/tex; mode=display">\gamma=\frac{1}{2}</script><script type="math/tex; mode=display">\|x_1\| = \|x_2\| = 1</script><ul><li>그러면 RBF 커널은 아래와 같은 차수가 무한대인 다항커널과 같아진다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} k(x_1, x_2) &=& \exp{\left(-\frac{||x_1 - x_2||^2}{2}\right)} \\ &=& \exp{\left(-\frac{x_1^Tx_1}{2} - \frac{x_2^Tx_2}{2} + 2x_1^Tx_2 \right)} \\ &=& \exp{\left(-\frac{x_1^Tx_1}{2}\right)}\exp{\left(-\frac{x_2^Tx_2}{2}\right)}\exp{(x_1^Tx_2)} \\ &=& C \exp{(x_1^Tx_2)} \\ &\approx& C \left( 1 + (x_1^Tx_2) + \dfrac{1}{2!}(x_1^Tx_2)^2 +  \dfrac{1}{3!}(x_1^Tx_2)^3 + \cdots \right) \\ \end{eqnarray}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">x1 = 0.0</span><br><span class="line">x2 = np.linspace(-7, 7, 100)</span><br><span class="line"></span><br><span class="line">def rbf(x1, x2, gamma):</span><br><span class="line">    <span class="built_in">return</span> np.exp(-gamma * np.abs(x2 - x1) ** 2)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8, 4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.plot(x2, rbf(x1, x2, 1), ls=<span class="string">"-"</span>, label=<span class="string">"gamma = 1"</span>)</span><br><span class="line">plt.plot(x2, rbf(x1, x2, 0.5), ls=<span class="string">":"</span>, label=<span class="string">"gamma = 0.5"</span>)</span><br><span class="line">plt.plot(x2, rbf(x1, x2, 5), ls=<span class="string">"--"</span>, label=<span class="string">"gamma = 5"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2 - x1"</span>)</span><br><span class="line">plt.xlim(-3, 7)</span><br><span class="line">plt.legend(loc=1)</span><br><span class="line">plt.title(<span class="string">"RBF 커널"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(122)</span><br><span class="line">plt.plot(x2, rbf(-4, x2, 1))</span><br><span class="line">plt.plot(x2, rbf(-2, x2, 1))</span><br><span class="line">plt.plot(x2, rbf(0, x2, 1))</span><br><span class="line">plt.plot(x2, rbf(2, x2, 1))</span><br><span class="line">plt.plot(x2, rbf(4, x2, 1))</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.title(<span class="string">"RBF 커널의 변환함수들"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/RBF_kernel_function_for_many_parameters.png" alt="RBF 커널과 그에따른 변환함수들"></p><h3 id="scikit-learn의-커널-SVM"><a href="#scikit-learn의-커널-SVM" class="headerlink" title="scikit-learn의 커널 SVM"></a>scikit-learn의 커널 SVM</h3><ul><li>scikit-learn의 <code>SVM</code> 클래스는 <code>kernel</code>인수를 지정하여 커널을 설정할 수 있다.<ul><li><code>kernel = &quot;linear&quot;</code> : 선형 SVM. $k(x_{1},\;x_{2})\;=\;x_{1}^{T}x_{2}$  </li><li><code>kernel = &quot;poly&quot;</code> : 다항 커널. $k(x_{1},\;x_{2})\;=\;(\gamma \;(x_{1}^{T} x_{2})\; +\; \theta)^{d}$<ul><li><code>gamma</code>: $\gamma$</li><li><code>coef0</code>: $\theta$</li><li><code>degree</code>: $d$</li></ul></li><li><code>kernel = &quot;rbf&quot;</code> 또는 <code>kernel = None</code>: RBF 커널. $k(x_1, x_2) = \exp \left( -\gamma ||x_1-x_2||^2 \right)$<ul><li><code>\gamma</code></li></ul></li><li><code>kernel = &quot;sigmoid&quot;</code> 시그모이드 커널. $k(x_1, x_2) = \tanh(\gamma (x_1^Tx_2) + \theta)$<ul><li><code>gamma</code> : $\gamma$</li><li><code>coef0</code> : $\theta$</li></ul></li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">polysvc = SVC(kernel=<span class="string">"poly"</span>, degree=2, gamma=1, coef0=0).fit(X_xor, y_xor)</span><br><span class="line">rbfsvc = SVC(kernel=<span class="string">"rbf"</span>).fit(X_xor, y_xor)</span><br><span class="line">sigmoidsvc = SVC(kernel=<span class="string">"sigmoid"</span>, gamma=2, coef0=2).fit(X_xor, y_xor)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8, 12))</span><br><span class="line">plt.subplot(311)</span><br><span class="line">plot_xor(X_xor, y_xor, polysvc, <span class="string">"다항커널 SVC를 사용한 분류 결과"</span>)</span><br><span class="line">plt.subplot(312)</span><br><span class="line">plot_xor(X_xor, y_xor, rbfsvc, <span class="string">"RBF커널 SVC를 사용한 분류 결과"</span>)</span><br><span class="line">plt.subplot(313)</span><br><span class="line">plot_xor(X_xor, y_xor, sigmoidsvc, <span class="string">"시그모이드커널 SVC를 사용한 분류 결과"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/XOR_problem_result_of_using_polynomial_kernel.png" alt="다항커널 SVC를 사용한 분류 결과"></p><p><img src="/image/XOR_problem_result_of_using_RBF_kernel.png" alt="RBF커널 SVC를 사용한 분류 결과"></p><p><img src="/image/XOR_problem_result_of_using_sigmoid_kernel.png" alt="Sigmoid 커널 SVC를 사용한 분류 결과"></p><h3 id="커널-파라미터의-영향"><a href="#커널-파라미터의-영향" class="headerlink" title="커널 파라미터의 영향"></a>커널 파라미터의 영향</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(8, 8))</span><br><span class="line">plt.subplot(221)</span><br><span class="line">plot_xor(X_xor, y_xor, SVC(kernel=<span class="string">"rbf"</span>, gamma=2).fit(X_xor, y_xor), <span class="string">"RBF SVM (gamma=2)"</span>)</span><br><span class="line">plt.subplot(222)</span><br><span class="line">plot_xor(X_xor, y_xor, SVC(kernel=<span class="string">"rbf"</span>, gamma=10).fit(X_xor, y_xor), <span class="string">"RBF SVM (gamma=10)"</span>)</span><br><span class="line">plt.subplot(223)</span><br><span class="line">plot_xor(X_xor, y_xor, SVC(kernel=<span class="string">"rbf"</span>, gamma=50).fit(X_xor, y_xor), <span class="string">"RBF SVM (gamma=50)"</span>)</span><br><span class="line">plt.subplot(224)</span><br><span class="line">plot_xor(X_xor, y_xor, SVC(kernel=<span class="string">"rbf"</span>, gamma=100).fit(X_xor, y_xor), <span class="string">"RBF SVM (gamma=100)"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li>$\gamma$의 값이 커질수록 hyperplane의 경계가 더 둥글어지면서, 그 값이 너무 높아지면 overfitting이 되게 된다.</li></ul><p><img src="/image/RBF_kernel_parameters_difference_plot.png" alt="RBF 커널의 감마값의 비교"></p><h3 id="iris-데이터에-적용"><a href="#iris-데이터에-적용" class="headerlink" title="iris 데이터에 적용"></a>iris 데이터에 적용</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data[:, [2, 3]]</span><br><span class="line">y = iris.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)</span><br><span class="line">sc = StandardScaler()</span><br><span class="line">sc.fit(X_train)</span><br><span class="line">X_train_std = sc.transform(X_train)</span><br><span class="line">X_test_std = sc.transform(X_test)</span><br><span class="line">X_combined_std = np.vstack((X_train_std, X_test_std))</span><br><span class="line">y_combined = np.hstack((y_train, y_test))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot_iris(X, y, model, title, xmin=-2.5, xmax=2.5, ymin=-2.5, ymax=2.5):</span><br><span class="line">    XX, YY = np.meshgrid(np.arange(xmin, xmax, (xmax-xmin)/1000),</span><br><span class="line">                         np.arange(ymin, ymax, (ymax-ymin)/1000))</span><br><span class="line">    ZZ = np.reshape(model.predict(np.array([XX.ravel(), YY.ravel()]).T), XX.shape)</span><br><span class="line">    plt.contourf(XX, YY, ZZ, cmap=mpl.cm.Paired_r, alpha=0.5)</span><br><span class="line">    plt.scatter(X[y == 0, 0], X[y == 0, 1], c=<span class="string">'r'</span>, marker=<span class="string">'^'</span>, label=<span class="string">'0'</span>, s=100)</span><br><span class="line">    plt.scatter(X[y == 1, 0], X[y == 1, 1], c=<span class="string">'g'</span>, marker=<span class="string">'o'</span>, label=<span class="string">'1'</span>, s=100)</span><br><span class="line">    plt.scatter(X[y == 2, 0], X[y == 2, 1], c=<span class="string">'b'</span>, marker=<span class="string">'s'</span>, label=<span class="string">'2'</span>, s=100)</span><br><span class="line">    plt.xlim(xmin, xmax)</span><br><span class="line">    plt.ylim(ymin, ymax)</span><br><span class="line">    plt.xlabel(<span class="string">"꽃잎의 길이"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"꽃잎의 폭"</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model1 = SVC(kernel=<span class="string">'linear'</span>).fit(X_test_std, y_test)</span><br><span class="line">model2 = SVC(kernel=<span class="string">'poly'</span>, random_state=0,</span><br><span class="line">             gamma=10, C=1.0).fit(X_test_std, y_test)</span><br><span class="line">model3 = SVC(kernel=<span class="string">'rbf'</span>, random_state=0, gamma=1,</span><br><span class="line">             C=1.0).fit(X_test_std, y_test)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8, 12))</span><br><span class="line">plt.subplot(311)</span><br><span class="line">plot_iris(X_test_std, y_test, model1, <span class="string">"선형 SVC"</span>)</span><br><span class="line">plt.subplot(312)</span><br><span class="line">plot_iris(X_test_std, y_test, model2, <span class="string">"다항커널 SVC"</span>)</span><br><span class="line">plt.subplot(313)</span><br><span class="line">plot_iris(X_test_std, y_test, model3, <span class="string">"RBF커널 SVM"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/iris_problem_result_of_using_linear_SVC.png" alt="선형 SVC를 사용한 분류 결과"></p><p><img src="/image/iris_problem_result_of_using_polynomial_kernel.png" alt="다항커널 SVC를 사용한 분류 결과"></p><p><img src="/image/iris_problem_result_of_using_RBF_kernel.png" alt="RBF커널 SVC를 사용한 분류 결과"></p><p><img src="/image/svm_deep_mind_03.png" alt="커널 사용시 유의할 점"></p><p><img src="/image/svm_deep_mind_04.png" alt="SVM과 LDA 비교"></p><h3 id="One-Class-Support-Vector-Machine"><a href="#One-Class-Support-Vector-Machine" class="headerlink" title="One Class Support Vector Machine"></a>One Class Support Vector Machine</h3><ul><li>종속변수가 없다는 의미이다. 고로 다 같은 범주이므로 classifying 정보가 없다는 것과 동일한 의미를 갖는다. <code>One Class SVM은 우리가 가진 자료들을 요약하는데 사용</code>한다. 마치 Unsupervised learning의 clustering 처럼 활용하는 방법이다. 기본적으로 원을 활용한 모델을 사용한다. 간단히 말해서 원안에는 자료가 있고, 원 바깥은 자료가 없다는 식으로 활용한다는 의미이다.  </li></ul><p><img src="/image/one_class_svm_concept.png" alt="one class svm 개념 - 01"></p><ul><li>SVM에서 서포트 벡터를 통해 $\beta$를 구해 hyperplane을 구했듯이 해당 원에 가장 가까운 $x_{k}$ 서포트 벡터를 통해 $R^{2}$ (반지를)을 계산한다.</li></ul><p><img src="/image/one_class_svm_concept_01.png" alt="one class svm 개념 - 02"></p><ul><li>아래의 그림처럼 바나나모양으로 데이터가 있는 경우에 임의의 반지름을 갖는 원안에 데이터를 모두 넣고 싶은데, 최소한의 반지름 값을 갖는 원을 찾는 문제가 된다.</li></ul><p><img src="/image/one_class_svm_concept_02.png" alt="one class svm 개념 - 03"></p><ul><li>내적을 kernel로 바꾼 후 polynomial kernel을 사용한 경우 차원이 높아 질수록 자유도도 높아짐(원보다는 데이터 하나하나에 영향을 더 많이 받음)을 확인할 수 있다.</li></ul><p><img src="/image/one_class_svm_concept_03.png" alt="one class svm 개념 - 04"></p><ul><li>또한, RBF 커널에서도 마찬가지로 C값이 높아질수록 허용하는 error가 낮아져 support vector안에 데이터들이 존재하게되며, 감마를 낮출수록($\sigma$를 높일수록) 두 벡터 $x_{i}$와 $x_{j}$간의 차이의 정도에 덜 민감해 지기 때문에 모양이 단순해 지게 된다.</li></ul><p><img src="/image/one_class_svm_concept_04.png" alt="one class svm 개념 - 05"></p><h3 id="SVR"><a href="#SVR" class="headerlink" title="SVR"></a>SVR</h3><ul><li>종속변수가 범주형이 아닌 연속형인 경우에는 SVR을 사용해야 한다. 아래 수식과 같이 margin과 hyperplane을 정의하는 것은 동일하다. 이번에는 <code>margin 밖에 존재하는 데이터들과 decision boundary와의 차이(error)를 최소화 하도록하는 방식</code>으로 동작한다. 그러므로 margin 안에 많은 데이터를 포함하고 있을 수록 error가 최소화 될 것이다. 결과적으론 아래와 같은 목적함수를 최소화하는데 마지막 수식과 같이 직관적으로 margin과의 차이만을 error로 보는 함수를 사용할 수 있다.</li></ul><p><img src="/image/svr_conception.png" alt="SVR의 정의"></p><ul><li>허나, 이전 그림에서의 error를 정의하는데 수학적인 계산에 용이성을 더하기 위해 아래와 같은 수식으로 error를 계산한다. 앞에서와 같이 미분불가능한 점이 없고 계산하기 편하기 때문에 아래와 같은 수식으로 사용한다.</li></ul><p><img src="/image/svr_conception_01.png" alt="SVR의 정의"></p><p><img src="/image/svr_conception_02.png" alt="SVR의 정의"></p><ul><li>SVM과 마찬가지로 SVR도 커널을 사용하여 다음과 같은 곡선의 형태로 fitting할 수 있다. 기본적으로 linear model을 사용했을 때는 전반적인 패턴을 잡아주고 다른 커널들에 대해서는 각 커널의 특성에 맞게 적합시켜 준다.</li></ul><p><img src="/image/svr_conception_03.png" alt="SVR의 정의"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/25/machine_learning_12/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Support Vector Machine(SVM) - 01</title>
      <link>https://heung-bae-lee.github.io/2020/04/22/machine_learning_11/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/22/machine_learning_11/</guid>
      <pubDate>Wed, 22 Apr 2020 09:04:12 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Support-Vector-Machine-SVM&quot;&gt;&lt;a href=&quot;#Support-Vector-Machine-SVM&quot; class=&quot;headerlink&quot; title=&quot;Support Vector Machine(SVM)&quot;&gt;&lt;/a&gt;Support
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Support-Vector-Machine-SVM"><a href="#Support-Vector-Machine-SVM" class="headerlink" title="Support Vector Machine(SVM)"></a>Support Vector Machine(SVM)</h1><ul><li>데이터의 분포가 정규분포를 띈다고 보이지 않는다면 이전에 말했던 LDA나 QDA를 사용하기 힘들다. 이런 경우 클래스간 분류를 하려고 할 때 사용될 수 있는 방법 중 하나가 SVM이다. 아래 그림과 같이 클래스 집단을 분류할 수 있는 벡터를 기준으로 최대한의 마진을 주어 분류하는 방식이다.</li></ul><p><img src="/image/background_of_svm_concept_01.png" alt="Support Vector Machine의 배경 - 01"></p><ul><li>아래 그림과 같이 두 클래스 집단간의 데이터 분포가 혼용되어있다면 어떤 방식으로 접근해야 할까? 두 클래스 집단간의 거리를 최대화하면서 혼용되어있는 데이터들에 대한 error를 적당히 허용하는 선에서 decision boundary를 결정해야 할 것이다.</li></ul><p><img src="/image/background_of_svm_concept_02.png" alt="Support Vector Machine의 배경 - 02"></p><ul><li>SVM은 regression 문제도 사용하지만 보통은 범주형 변수에 대한 classification 문제에 많이 사용한다. Support vector regression(SVR)은 최대한 많은 데이터를 margin 안에 포함하고자 하는 것이다. 이에 대해 margin 바깥에 존해하는 데이터에 대해 error를 줄어서 그 error를 최소화하는 방향으로 회귀를 진행한다. 일반적인 Regression은 해당 데이터를 설명할 수 있는 선에 대해 error를 계산하는 방식이라면, SVR은 margin 바깥에 존재하는 데이터들에 대해서만 error를 계산하는 방식이다.</li></ul><p><img src="/image/background_of_svm_concept_03.png" alt="Support Vector Machine의 배경 - 03"></p><p><img src="/image/background_of_svm_concept_04.png" alt="Support Vector Machine의 배경 - 04"></p><ul><li>초평면에 부등호를 도입하면 다음과 같이 영역으로 데이터를 구분지을 수 있게 된다.</li></ul><p><img src="/image/how_to_make_decision_boundary_with_svm_01.png" alt="Support Vector Machine의 Decision boundary - 01"></p><p><img src="/image/how_to_make_decision_boundary_with_svm_02.png" alt="Support Vector Machine의 Decision boundary - 02"></p><p><img src="/image/how_to_make_decision_boundary_with_svm_03.png" alt="Support Vector Machine의 Decision boundary - 03"></p><h2 id="나그랑주-승수-Lagrange-multiplier"><a href="#나그랑주-승수-Lagrange-multiplier" class="headerlink" title="나그랑주 승수(Lagrange multiplier)"></a>나그랑주 승수(Lagrange multiplier)</h2><ul><li>최적화 문제(예를 들어서 극대값이나 극소값)를 푸는데 특정조건하에서 문제를 풀 수 있도록 하는 방법이다. 아래 그림에서 보면, 아무런 제한이 없었다면 x와 y의 값에 따라 $-\infty$에서 $\infty$로 움직일 수 있을 것이다. 허나, $g(x,y)=c$라는 함수 범위 내에서만 움직일 수 있다고 제한을 주면 해당 제한영역하에서의 최적화를 풀어야할 것이다.</li></ul><p><img src="/image/Lagrange_multiplier_01.png" alt="Lagrange multiplier - 01"></p><p><img src="/image/Lagrange_multiplier_02.png" alt="Lagrange multiplier - 02"></p><p><img src="/image/Lagrange_multiplier_03.png" alt="Lagrange multiplier - 03"></p><ul><li>위에서 언급하는 최적화 문제를 수학적으로 살펴보려면, 아래와 같이 최적화문제에 대한 설명이 필요하다.</li></ul><h3 id="제한조건이-있는-최적화-문제"><a href="#제한조건이-있는-최적화-문제" class="headerlink" title="제한조건이 있는 최적화 문제"></a>제한조건이 있는 최적화 문제</h3><ul><li>제한조건(constraint)을 가지는 최적화 문제를 풀어본다. 제한 조건은 연립방적식 또는 연립부등식이다. <code>연립방정식 제한조건이 있는 경우에는 라그랑주 승수법을 사용하여 새로운 최적화 문제를 풀어야</code> 한다. <code>연립부등식 제한조건의 경우에는 KKT조건이라는 것을 만족하도록 하는 복잡한 과정을 거쳐야 한다.</code></li></ul><h4 id="등식-제한조건이-있는-최적화-문제"><a href="#등식-제한조건이-있는-최적화-문제" class="headerlink" title="등식 제한조건이 있는 최적화 문제"></a>등식 제한조건이 있는 최적화 문제</h4><ul><li>현실의 최적화 문제에서는 여러가지 제한조건이 있는 최적화(constrained optimization) 문제가 많다. 가장 간단한 경우는 다음과 같이 연립방정식 제한조건이 있는 경우다. 등식(equality)제한 조건이라고도 한다.</li></ul><script type="math/tex; mode=display">\begin{align} x^{\ast} = \text{arg} \min_x f(x) \end{align}</script><script type="math/tex; mode=display">\begin{align} x \in \mathbf{R}^N \end{align}</script><script type="math/tex; mode=display">\begin{align} g_j(x) = 0 \;\; (j=1, \ldots, M) \end{align}</script><ul><li>첫 번째 식만 보면 단순히 목적함수 $f(x)$를 가장 작게 하는 N차원 벡터 x값을 찾는 문제다. 하지만 마지막 식에 있는 M개의 등식 제한 조건이 있으면 M개 연립 방정식을 동시에 모두 만족시키면서 목적함수 $f(x)$를 가장 작게하는 $x$값을 찾아야 한다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} g_1(x) &= 0 \\ g_2(x) &= 0 \\ &\vdots \\ g_M(x) &= 0 \\ \end{aligned} \end{align}</script><h5 id="예제"><a href="#예제" class="headerlink" title="예제"></a>예제</h5><blockquote><p>목적 함수 $f$와 등식 제한조건 $g$가 다음과 같은 경우를 생각하자. 이 문제는 다음 그림 처럼 $g(x_{1}, x_{2}) = 0$으로 정의되는 직선상에서 가장 $f(x_{1},x_{2})$값이 작아지는 점 $(x_1^{\ast}, x_2^{\ast})$을 찾는 문제가 된다.</p></blockquote><script type="math/tex; mode=display">\begin{align} f(x_1, x_2) = x_1^2 + x_2^2 \end{align}</script><script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = x_1 + x_2 - 1 = 0 \end{align}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 목적함수 f(x) = x1^2 + x2^2</span></span><br><span class="line">def f1(x1, x2):</span><br><span class="line">    <span class="built_in">return</span> x1 ** 2 + x2 ** 2</span><br><span class="line"></span><br><span class="line">x1 = np.linspace(-5, 5, 100)</span><br><span class="line">x2 = np.linspace(-3, 3, 100)</span><br><span class="line">X1, X2 = np.meshgrid(x1, x2)</span><br><span class="line">Y = f1(X1, X2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 등식 제한조건 방정식 g(x) = x1 + x2 - 1 = 0</span></span><br><span class="line">x2_g = 1 - x1</span><br><span class="line"></span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>, levels=[0.5, 2, 8, 32])</span><br><span class="line">plt.plot(x1, x2_g, <span class="string">'g-'</span>)</span><br><span class="line"></span><br><span class="line">plt.plot([0], [0], <span class="string">'rP'</span>)</span><br><span class="line">plt.plot([0.5], [0.5], <span class="string">'ro'</span>, ms=10)</span><br><span class="line"></span><br><span class="line">plt.xlim(-5, 5)</span><br><span class="line">plt.ylim(-3, 3)</span><br><span class="line">plt.xticks(np.linspace(-4, 4, 9))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"등식 제한조건이 있는 최적화 문제"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/Lagrange_multiplier_optimization_problem.png" alt="최적화 문제 예시"></p><h4 id="라그랑주-승수법"><a href="#라그랑주-승수법" class="headerlink" title="라그랑주 승수법"></a>라그랑주 승수법</h4><ul><li>이렇게 등식 제한조건이 있는 최적화 문제는 라그랑주 승수법(Lagrange multiplier)을 사용하여 최적화할 수 있다. 라그랑주 승수 방법에서는 목적함수를 원래의 목적함수 $f(x)$를 사용하지 않는다. 대신 제한조건 등식에 $\lambda$라는 새로운 변수를 곱해서 더한 함수를 목적함수로 간주하여 최적화한다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} h(x, \lambda) &= h(x_1, x_2, \ldots , x_N, \lambda_1, \ldots , \lambda_M) \\ &= f(x) + \sum_{j=1}^M \lambda_j g_j(x) \end{aligned} \end{align}</script><ul><li>이때 제한조건 등식 하나마다 새로운 $\lambda_{i}$를 추가해주어야 한다. 따라서 만약 제한조건이 $M$개이면 $\lambda_{1}, \cdots, \lambda_{M}$개의 변수가 새로 생긴 것과 같다. 이렇게 확장된 목적함수 $h$는 입력변수가 더 늘어났기 때문에 그레디언트 벡터를 영벡터로 만드는 최적화 필요 조건이 다음처럼 $N\;+\;M$개가 된다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= \dfrac{\partial f}{\partial x_1} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_1} = 0 \\ \dfrac{\partial h}{\partial x_2} &= \dfrac{\partial f}{\partial x_2} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_2} = 0 \\ & \vdots  \\ \dfrac{\partial h}{\partial x_N} &= \dfrac{\partial f}{\partial x_N} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_N} = 0 \\ \dfrac{\partial h}{\partial \lambda_1} &= g_1 = 0 \\ & \vdots  \\ \dfrac{\partial h}{\partial \lambda_M} &= g_M = 0 \end{aligned} \end{align}</script><ul><li>이 $N\;+\;M$개의 연립 방정식을 풀면 $N\;+\;M$개의 미지수를 구할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} x_1, x_2, \ldots, x_N, , \lambda_1, \ldots , \lambda_M \end{align}</script><ul><li>구한 결과에서 찾는 최소값 $x$를 구할 수 있다. 라그랑주 승수값은 필요없다.</li></ul><blockquote><p>예제) 위에서 제시한 예제를 라그랑주 승수법으로 풀어보자. 새로운 목적함수는 다음과 같다.</p></blockquote><script type="math/tex; mode=display">\begin{align} h(x_1, x_2, \lambda) = f(x_1, x_2) + \lambda g(x_1, x_2) = x_1^2 + x_2^2 + \lambda ( x_1 + x_2 - 1 )  \end{align}</script><ul><li>라그랑주 승수법을 적용하여 그레디언트 벡터가 영벡터인 위치를 구한다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= 2{x_1} + \lambda = 0 \\ \dfrac{\partial h}{\partial x_2} &= 2{x_2} + \lambda = 0 \\ \dfrac{\partial h}{\partial \lambda}  &= x_1 + x_2 - 1 = 0 \end{aligned} \end{align}</script><ul><li>방정식의 해는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} x_1 = x_2 = \dfrac{1}{2}, \;\;\; \lambda = -1 \end{align}</script><blockquote><p>연습문제 제한조건이 $x_{1}+x_{2}\; = \; 1$일 때 목적 함수 $f(x) = - log x_{1} - log x_{2} x_{1},x_{2} &gt; 0$ 을 최소화하는 x_{1}, x_{2}값을 라그랑주 승수법으로 계산하라.</p></blockquote><ul><li>위의 문제에서 목저함수는 아래와 같다.</li></ul><script type="math/tex; mode=display">\begin{align} h(x_1, x_2, \lambda) = f(x_1, x_2) + \lambda g(x_1, x_2) = - log x_1 - log x_2 + \lambda ( x_1 + x_2 - 1 ) \end{align}</script><ul><li>라그랑주 승수법은 적용하여 그레디언트 벡터가 영벡터인 위치를 구한다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= \lambda x_1 -1 = 0 \\ \dfrac{\partial h}{\partial x_2} &= \lambda x_2 -1 = 0 \\ \dfrac{\partial h}{\partial \lambda} &= x_1 + x_2 - 1 = 0 \end{aligned} \end{align}</script><ul><li>위 방정식을 풀면 해는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} x_1 = x_2 = \dfrac{1}{2}, \;\;\; \lambda = 2 \end{align}</script><h5 id="scipy를-사용하여-등식-제한조건이-있는-최적화-문제-계산하기"><a href="#scipy를-사용하여-등식-제한조건이-있는-최적화-문제-계산하기" class="headerlink" title="scipy를 사용하여 등식 제한조건이 있는 최적화 문제 계산하기"></a>scipy를 사용하여 등식 제한조건이 있는 최적화 문제 계산하기</h5><ul><li>scipy의 optimize 서브패키지는 제한조건이 있는 최적화 문제를 푸는 <code>fmin_slsqp()</code>명령을 제공한다. 목적함수와 초기값, 그리고 제한조건 함수의 리스트를 인수로 받는다. 목적함수는 배열인 인수를 받도록 구현되어야 하고 제한조건 함수의 경우에는 항상 <code>eqcons</code>인수를 명시해야 한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fmin_slsqp(func_objective, x0, eqcons=[func_constraint1, func_constraint2])</span><br></pre></td></tr></table></figure><ul><li>위에서의 두 문제를 scipy를 통해서 풀어보겠다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def f1array(x):</span><br><span class="line">    <span class="built_in">return</span> x[0] ** 2 + x[1] ** 2</span><br><span class="line"></span><br><span class="line">def eq_constraint(x):</span><br><span class="line">    <span class="built_in">return</span> x[0] + x[1] - 1</span><br><span class="line"></span><br><span class="line">scipy.optimize.fmin_slsqp(f1array, np.array([1, 1]), eqcons=[eq_constraint])</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.    (Exit mode 0)</span><br><span class="line">            Current <span class="keyword">function</span> value: 0.5000000000000002</span><br><span class="line">            Iterations: 2</span><br><span class="line">            Function evaluations: 8</span><br><span class="line">            Gradient evaluations: 2</span><br><span class="line">array([0.5, 0.5])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def f1array(x):</span><br><span class="line">    <span class="built_in">return</span> -np.log(x[0]) -np.log(x[1])</span><br><span class="line"></span><br><span class="line">def eq_constraint(x):</span><br><span class="line">    <span class="built_in">return</span> x[0] + x[1] - 1</span><br><span class="line"></span><br><span class="line">scipy.optimize.fmin_slsqp(f1array, np.array([1, 1]), eqcons=[eq_constraint])</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.    (Exit mode 0)</span><br><span class="line">            Current <span class="keyword">function</span> value: 1.3862943611198901</span><br><span class="line">            Iterations: 2</span><br><span class="line">            Function evaluations: 8</span><br><span class="line">            Gradient evaluations: 2</span><br><span class="line">array([0.5, 0.5])</span><br></pre></td></tr></table></figure><h3 id="라그랑주-승수의-의미"><a href="#라그랑주-승수의-의미" class="headerlink" title="라그랑주 승수의 의미"></a>라그랑주 승수의 의미</h3><ul><li>만약 최적화 문제에서 등식 제한조건 $g_{i}가 있는가 없는가에 따라 해의 값이 달라진다면 이 등식 제한조건에 대응하는 라그랑주 승수 $\lambda_{i}$는 0이 아닌 값이어야 한다. $\lambda_{i} = 0$일 때만 원래의 문제와 제한조건이 있는 문제의 최적화 조건이 같아지므로 최적화 해의 위치도 같게 나오기 때문이다.</li></ul><script type="math/tex; mode=display">\begin{align} \lambda_i \neq 0 \end{align}</script><h4 id="예제-1"><a href="#예제-1" class="headerlink" title="예제"></a>예제</h4><ul><li>목적함수가 아래와 같은 최소화 문제의 답은 $x_{1} = x_{2} = 0$이다.</li></ul><script type="math/tex; mode=display">\begin{align} f(x) = x_1^2 + x_2^2 \end{align}</script><ul><li>여기에 다음 제한 조건이 있다고 하자.</li></ul><script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = x_1 + x_2 = 0 \end{align}</script><ul><li>라그랑주 승수법에서 새로운 목적함수는 아래와 같다.</li></ul><script type="math/tex; mode=display">\begin{align} h(x_1, x_2, \lambda) = f(x_1, x_2) + \lambda g(x_1, x_2) = x_1^2 + x_2^2 + \lambda ( x_1 + x_2) \end{align}</script><ul><li>이에 따른 최적화 조건은 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= 2{x_1} + \lambda = 0 \\ \dfrac{\partial h}{\partial x_2} &= 2{x_2} + \lambda = 0 \\ \dfrac{\partial h}{\partial \lambda} &= x_1 + x_2 = 0 \end{aligned} \end{align}</script><ul><li>이에 대한 해는 $x_{1} = x_{2} = \lambda = 0$으로 <code>제한조건이 있으나 없으나 해는 동일하며, 라그랑주 승수는 0이 된다.</code> 즉, 제한조건이 의미가 없는 경우는 라그랑주 승수가 0이된다는 의미이다.</li></ul><h3 id="부등식-제한조건이-있는-최적화-문제"><a href="#부등식-제한조건이-있는-최적화-문제" class="headerlink" title="부등식 제한조건이 있는 최적화 문제"></a>부등식 제한조건이 있는 최적화 문제</h3><ul><li>이번에는 다음과 같이 부등식(inequality) 제한조건이 있는 최적화 문제를 생각하자.</li></ul><script type="math/tex; mode=display">\begin{align} x^{\ast} = \text{arg} \min_x f(x) \end{align}</script><script type="math/tex; mode=display">\begin{align} x \in \mathbf{R}^N \end{align}</script><script type="math/tex; mode=display">\begin{align} g_j(x) \leq 0 \;\; (j=1, \ldots, M) \end{align}</script><ul><li>만약 부등식이 $g_j(x) \geq 0$과 같다면 양변에 $-1$을 곱하여 부등호의 방향을 바꾼다. 이렇게 부등식 제한조건이 있는 최적화 문제도 라그랑주 승수 방법과 목적함수를 다음처럼 바꾸어 푼다. 이렇게 부등식 제한 조건이 있는 최적화 문제도 라그랑주 승수 방법과 목적함수를 다음처럼 바꾸어 푼다.</li></ul><script type="math/tex; mode=display">\begin{align} h(x, \lambda) = f(x) + \sum_{j=1}^M \lambda_j g_j(x) \end{align}</script><h5 id="다만-이-경우-최적화-해의-필요조건은-방정식-제한조건이-있는-최적화-문제와-다르게-KKT-Karush-Kuhn-Tucker-조건이라고-하며-다음처럼-3개의-조건으로-이루어진다"><a href="#다만-이-경우-최적화-해의-필요조건은-방정식-제한조건이-있는-최적화-문제와-다르게-KKT-Karush-Kuhn-Tucker-조건이라고-하며-다음처럼-3개의-조건으로-이루어진다" class="headerlink" title="다만, 이 경우 최적화 해의 필요조건은 방정식 제한조건이 있는 최적화 문제와 다르게 KKT(Karush-Kuhn-Tucker)조건이라고 하며 다음처럼 3개의 조건으로 이루어진다."></a>다만, 이 경우 최적화 해의 필요조건은 방정식 제한조건이 있는 최적화 문제와 다르게 <code>KKT(Karush-Kuhn-Tucker)조건</code>이라고 하며 다음처럼 3개의 조건으로 이루어진다.</h5><ul><li>1) 모든 독립변수 $x_{1}, x_{2}, \ldots, \x_{N}$에 대한 미분값이 0이다.<ul><li>첫 번째 조건은 방정식 제한조건의 경우와 같다. 다만 변수 $x$들에 대한 미분값만 0이어야 한다. 라그랑주 승수 $\lambda$에 대한 미분은 0이 아니어도 된다.</li></ul></li></ul><script type="math/tex; mode=display">\begin{align} \dfrac{\partial h(x, \lambda)}{\partial x_i} = 0 \end{align}</script><ul><li>2) 모든 라그랑주 승수 $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{M}$과 제한조건 부등식($\lambda$에 대한 미분값)의 곱이 0이다.<ul><li>두 번째 조건을 보면 확장된 목적함수를 나그랑주 승수로 미분한 값은 변수 $x$들에 대한 미분값과는 달리 반드시 0이 될 필요는 없다는 것을 알 수 있다. 이렇게 하려면 두 경우가 가능한데 등식 제한조건의 경우처럼 라그랑주 승수 $\lambda$에 대한 미분값이 0이어도 되고 아니면 라그랑주 승수 $\lambda$값 자체가 0이 되어도 된다.</li></ul></li></ul><script type="math/tex; mode=display">\begin{align} \lambda_j \cdot \dfrac{\partial h(x, \lambda)}{\partial \lambda_j} = \lambda_j \cdot g_j = 0  \end{align}</script><ul><li>3) 라그랑주 승수는 음수가 아니어야 한다.<ul><li>마지막 조건은 KKT 조건이 실제로 부등식 제한조건이 있는 최적화 문제와 같은 문제임을 보장하는 조건이다.</li></ul></li></ul><script type="math/tex; mode=display">\begin{align} \lambda_j \geq 0 \end{align}</script><h4 id="예제-2"><a href="#예제-2" class="headerlink" title="예제"></a>예제</h4><ul><li>부등식 제한조건을 가지는 최적화의 예를 풀어보자. 목적함수는 아래와 같다.</li></ul><script type="math/tex; mode=display">\begin{align} f(x_1, x_2) = x_1^2 + x_2^2 \end{align}</script><ul><li>이 예제에서 두 가지 제한 조건을 고려해 볼 텐데 하나는 다음 그림 중 왼쪽 그림처럼 부등식 제한조건이 아래와 같은 경우이다.</li></ul><script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = x_1 + x_2 - 1 \leq 0 \end{align}</script><ul><li>다른 하나의 제한조건은 아래와 같고 이에 대한 그림은 오른쪽에 해당한다.</li></ul><script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = -x_1 - x_2 + 1 \leq 0 \end{align}</script><ul><li>아래 그림에서 제한조건을 만족하는 영역을 어둡게 표시했다. 최적점의 위치는 점으로 표시했다. 첫 번째 제한조건의 경우에는 부등식 제한조건이 있기는 하지만 원래의 최적화 문제의 해가 부등식 제한조건이 제시하는 영역 안에 있기 때문에 최적점의 위치가 달라지지 않는다. 두 번째 제한조건의 경우에는 원래의 최적화 문제의 해가 부등식 제한조건이 제시하는 영역 바깥에 있기 때문에 최적점의 위치가 달라졌다. 하지만 최적점의 위치가 영역의 경계선(boundary line)에 있다는 점에 주의하라.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(13, 7))</span><br><span class="line">ax1 = plt.subplot(121)</span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>, levels=[0.5, 2, 8])</span><br><span class="line">plt.plot(x1, x2_g, <span class="string">'g-'</span>)</span><br><span class="line">ax1.fill_between(x1, -20, x2_g, alpha=0.5)</span><br><span class="line">plt.plot([0], [0], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.xlim(-3, 3)</span><br><span class="line">plt.ylim(-5, 5)</span><br><span class="line">plt.xticks(np.linspace(-4, 4, 9))</span><br><span class="line">plt.yticks(np.linspace(-5, 5, 11))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"최적해가 부등식과 관계없는 경우"</span>)</span><br><span class="line">ax2 = plt.subplot(122)</span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>, levels=[0.5, 2, 8])</span><br><span class="line">plt.plot(x1, x2_g, <span class="string">'g-'</span>)</span><br><span class="line">ax2.fill_between(x1, 20, x2_g, alpha=0.5)</span><br><span class="line">plt.plot([0.5], [0.5], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.xlabel(<span class="string">"x_1"</span>)</span><br><span class="line">plt.xlim(-3, 3)</span><br><span class="line">plt.ylim(-5, 5)</span><br><span class="line">plt.xticks(np.linspace(-4, 4, 9))</span><br><span class="line">plt.yticks(np.linspace(-5, 5, 11))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"최적해가 부등식에 의해 결정되는 경우"</span>)</span><br><span class="line">plt.suptitle(<span class="string">"부등식 제한조건이 있는 최적화 문제"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/inequality_having_constraint_optimization_problem.png" alt="부등식 제한조건이 있는 최적화 문제"></p><ul><li><p>그림에서 보듯이 부등식 제한조건이 있는 최적화 문제를 풀면 그 제한조건은 다음 두 가지 경우의 하나가 되어 버린다.</p><ul><li>최적화 결과에 전혀 영향을 주지 않는 <code>쓸모없는</code> 제한조건</li><li>최적화 결과에 영향을 주는 <code>등식(equality)</code>인 제한조건</li></ul></li><li><p>어느 경우이든 부등식 제한조건 문제로 시작했지만 결과는 제한조건이 없거나 등식 제한조건 문제를 푸는 것과 같아진다. KKT조건 중 두 번째 조건이 뜻하는 바는 다음과 같다. 다음 식에서 $x^{\ast}, \lambda^{\ast}$는 KKT 조건을 풀어서 구한 최적해의 값이다.</p></li></ul><script type="math/tex; mode=display">\begin{align} \lambda^{\ast} = 0 \;\; \text{or} \;\;  g(x^{\ast}) = 0 \end{align}</script><ul><li>만약 $g_{i} = 0$이면 이 조건은 부등식 제한조건이 아닌 등식 제한조건이 된다. 그리고 등식 제한조건에서 말한 바와 같이 (이 제한조건이 있으나 없으나 해가 바뀌지 않는 특수한 경우를 제외하면) 라그랑주 승수는 0이 아닌값을 가진다.</li></ul><script type="math/tex; mode=display">\begin{align} g_i = 0 \;\; \rightarrow \;\; \lambda_i \neq 0 \; (\lambda_i > 0) \end{align}</script><ul><li>반대로 $g_i \neq 0 \; (g_i &lt; 0)$이면 해가 $g_{i}$가 표현하는 곡선으로부터 떨어져 있기 때문에 부등식 제한조건이 아무런 의미가 없어진다. 즉, 제한조건이 있을 때와 없을 때의 해가 같다. 따라서 목적함수 $h(x,\lambda)$는 $\lambda_{i}g_{i}(g_{i} \neq 0)$항이 있으나 없으나 상관없이 같은 해를 가진다. 따라서 $\lambda_{i} = 0$이 된다.</li></ul><script type="math/tex; mode=display">\begin{align} g_i \neq 0 \;\; \rightarrow \;\; \lambda_i = 0  \end{align}</script><ul><li>따라서 <code>부등식 제한조건이 있는 최적화 문제는 각 제한조건에 대해 위의 두 가지 경우를 가정하여 각각 풀어보면서 최적의 답을 찾는다</code>.</li></ul><blockquote><p>예제) 다음은 복수의 부등식 제한조건이 있는 또다른 2차원 최적화 문제의 예이다.</p></blockquote><script type="math/tex; mode=display">\begin{align} \text{arg} \min_x \; (x_1-4)^2 + (x_2-2)^2 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_1(x) = x_1 + x_2 - 1\leq 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_2(x) = -x_1 + x_2 - 1\leq 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_3(x) = -x_1 - x_2 - 1\leq 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_4(x) = x_1 - x_2 - 1\leq 0 \end{align}</script><ul><li>위의 4가지 제한조건은 다음과 같은 하나의 부등식으로 나타낼 수도 있다.</li></ul><script type="math/tex; mode=display">\begin{align} g(x) = \left\vert\, x_1 \right\vert + \left\vert\, x_2 \right\vert - 1 = \sum_{i=1}^{2} \left\vert\, x_i \right\vert - 1 \leq 0 \end{align}</script><ul><li><p>아래 예제에서 최적해가 $x_{1}\;=\;1,  x_{2}\; = \;0$이라는 사실을 이용하여 라그랑주 승수 $\lambda_{1}, \lambda_{2}, \lambda_{3}, \lambda_{4}$ 중 어느 값이 0이 되는지 말해보자.</p><ul><li>이에 대한 답은 $\lambda_{2} = \lambda_{3} = 0$이 될 것이다. 해를 찾는데 아무런 영향을 미치지 않기 때문이다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def f2plt(x1, x2):</span><br><span class="line">    <span class="built_in">return</span> np.sqrt((x1 - 4) ** 2 + (x2 - 2) ** 2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x1 = np.linspace(-2, 5, 100)</span><br><span class="line">x2 = np.linspace(-1.5, 3, 100)</span><br><span class="line">X1, X2 = np.meshgrid(x1, x2)</span><br><span class="line">Y = f2plt(X1, X2)</span><br><span class="line"></span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>,</span><br><span class="line">            levels=np.arange(0.5, 5, 0.5) * np.sqrt(2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 제한 조건의 상수</span></span><br><span class="line">k = 1</span><br><span class="line">ax = plt.gca()</span><br><span class="line">x12 = np.linspace(-k, 0, 10)</span><br><span class="line">x13 = np.linspace(0, k, 10)</span><br><span class="line">ax.fill_between(x12, x12 + k, -k - x12, color=<span class="string">'g'</span>, alpha=0.5)</span><br><span class="line">ax.fill_between(x13, x13 - k, k - x13, color=<span class="string">'g'</span>, alpha=0.5)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 최적점 위치</span></span><br><span class="line">x1_sol = 1</span><br><span class="line">x2_sol = 0</span><br><span class="line">plt.plot(x1_sol, x2_sol, <span class="string">'ro'</span>, ms=20)</span><br><span class="line"></span><br><span class="line">plt.xlim(-2, 5)</span><br><span class="line">plt.ylim(-1.5, 3)</span><br><span class="line">plt.xticks(np.linspace(-2, 5, 8))</span><br><span class="line">plt.yticks(np.linspace(-1, 3, 5))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"$|x_1| + |x_2| \leq &#123;&#125;$ 제한조건을 가지는 최적화 문제"</span>.format(k))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/inequality_optimization_problem_plot.png" alt="부등식 최적화 문제"></p><h4 id="Scipy를-사용하여-부등식-제한조건이-있는-최적화-문제-계산하기"><a href="#Scipy를-사용하여-부등식-제한조건이-있는-최적화-문제-계산하기" class="headerlink" title="Scipy를 사용하여 부등식 제한조건이 있는 최적화 문제 계산하기"></a>Scipy를 사용하여 부등식 제한조건이 있는 최적화 문제 계산하기</h4><ul><li><code>fmin_slsqp()</code>명령은 이렇게 부등식 제한조건이 있는 경우에도 사용할 수 있다. 제한조건 인수의 이름이 <code>ieqcons</code>로 달라졌다. 단, <code>ieqcons</code> 인수에 들어가는 부등호는 우리가 지금까지 사용한 방식과 달리 0 또는 양수이어야 한다.</li></ul><script type="math/tex; mode=display">\begin{align} g \geq 0 \end{align}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fmin_slsqp(func_objective, x0, ieqcons=[func_constraint1, func_constraint2])</span><br></pre></td></tr></table></figure><ul><li>이렇듯, <code>fmin_slsqp()</code> 명령은 등식 제한조건과 부등식 제한조건을 동시에 사용할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def f2(x):</span><br><span class="line">    <span class="built_in">return</span> np.sqrt((x[0] - 4) ** 2 + (x[1] - 2) ** 2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 제한 조건 상수</span></span><br><span class="line">k = 1</span><br><span class="line">def ieq_constraint(x):</span><br><span class="line">    <span class="built_in">return</span> np.atleast_1d(k - np.sum(np.abs(x)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sp.optimize.fmin_slsqp(f2, np.array([0, 0]), ieqcons=[ieq_constraint])</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.    (Exit mode 0)</span><br><span class="line">            Current <span class="keyword">function</span> value: 3.6055512804550336</span><br><span class="line">            Iterations: 11</span><br><span class="line">            Function evaluations: 77</span><br><span class="line">            Gradient evaluations: 11</span><br><span class="line"></span><br><span class="line">array([9.99999982e-01, 1.79954011e-08])</span><br></pre></td></tr></table></figure><h1 id="서포트-벡터-머신"><a href="#서포트-벡터-머신" class="headerlink" title="서포트 벡터 머신"></a>서포트 벡터 머신</h1><ul><li>변수가 1개있다면, $x^T$는 N차원을 갖는 헹벡터라면, $\beta$도 같은 차원을 갖는 열벡터로서 내적을 통해 그에 따른 벡터의 영역이 초평면(hyperplane)을 이루게 될 것이다.</li></ul><p><img src="/image/SVM_conception_hyperplan_01.png" alt="SVM 정의"></p><ul><li><code>퍼셉트론</code>은 가장 단순하고 빠른 판별 함수 기반 분류 모형이지만 <code>판별 경계선(decision hyperplane)이 유니크하게 존재하지 않는다</code>는 특징이 있다. 서포트 벡터 머신(SVM: Support vector machine)은 퍼셉트론 기반의 모형에 가장 안정적인 판별 경계선을 찾기 위한 제한 조건을 추가한 모형이라고 볼 수 있다.</li></ul><p><img src="/image/svm_decision_boundary_hyperplane.png" alt="SVM의 decision boundary"></p><h3 id="서포트와-마진"><a href="#서포트와-마진" class="headerlink" title="서포트와 마진"></a>서포트와 마진</h3><ul><li>다음과 같이 $N$개의 학습용 데이터가 있다고 하자.</li></ul><script type="math/tex; mode=display">(x_{1}, y_{1}), (x_{2}, y_{2}), \ldots, (x_{i}, y_{i}), \ldots,(x_{N}, y_{N})</script><ul><li>판별함수 모형에서 $y$는 $+1,\; -1$ 두 개의 값을 가진다.</li></ul><script type="math/tex; mode=display">y = \begin{cases} +1 \\ -1 \end{cases}</script><ul><li>$x$ 데이터 중에서 $y$값이 $+1$인 데이터를 $x_{+}$, $y$값이 $-1$인 데이터를 $x_{-}$라고 하자. 판별함수 모형에서 직선인 판별 함수 $f(x)$는 다음과 같은 수식으로 나타낼 수 있다.</li></ul><script type="math/tex; mode=display">f(x) = w^Tx-w_0</script><ul><li>혹시라도 판별함수의 직선의 방정식이 어떻게 나온건지 이해가 안가시는 분들에게 설명을 드리고자 잠깐 선형대수의 직선의 방정식에 관한 설명을 하도록 하겠다.</li></ul><h3 id="직선의-방정식"><a href="#직선의-방정식" class="headerlink" title="직선의 방정식"></a>직선의 방정식</h3><ul><li><p>어떤 벡터 $w$가 있을 때</p><ul><li>원점에서 출발한 벡터 $w$가 가리키는 점을 지나면서</li><li>벡터 $w$에 수직인</li></ul></li><li><p>직선의 방정식을 구해보자.</p></li></ul><ul><li>위 두 조건을 만족하는 직선상의 임의의 점을 가리키는 벡터를 $x$라고 하면, 벡터 $x$가 가리키는 점과 벡터 $w$가 가리키는 점을 이은 벡터 $x - w$는 조건에 따라 벡터 $w$와 직교해야 한다. 따라서 다음 식이 성립한다.</li></ul><script type="math/tex; mode=display">\begin{align} w^T(x - w) = 0 \end{align}</script><ul><li>정리하면 다음과 같아진다.</li></ul><script type="math/tex; mode=display">\begin{align} w^T(x - w) = w^Tx - w^Tw = w^Tx - \| w \|^2 \end{align}</script><script type="math/tex; mode=display">\begin{align} w^Tx - \| w \|^2 = 0 \end{align}</script><ul><li>이 직선과 원점 사이의 거리는 벡터 $w$의 norm $|w|$이다.</li></ul><blockquote><p>연습문제) 만약 $v$가 원점을 지나는 직선의 방향을 나타내는 단위벡터라고 하자. 이때 그 직선 위에 있지 않는 어떤 점 $x$와 그 직선과의 거리의 제곱이 다음과 같음을 증명하라.</p></blockquote><script type="math/tex; mode=display">\begin{align} \| x \|^2 - (x^Tv)^2 \end{align}</script><ul><li><p>$ x \; - \; v \perp v$이기 때문에</p></li><li><p>$a^{\Vert b} = | x | cos \theta = \frac{| v | | x | cos \theta}{| v |} = \frac{x^{T} v}{| v |}$</p></li><li><p>벡터 $v$는 단위벡터이므로 $a^{\Vert b} = x^{T}v$가 된다. 여기서 피타고라스 정리를 사용하면 우리가 증명해야 하는 식을 구할 수 있다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">v = np.array([2, 1]) / np.sqrt(5)</span><br><span class="line">x = np.array([1, 3])</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=v, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.plot([-2, 8], [-1, 4], <span class="string">'b--'</span>, lw=2)</span><br><span class="line">plt.plot([1, 2], [3, 1], <span class="string">'g:'</span>, lw=2)</span><br><span class="line">plt.plot(x[0], x[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.text(0.1, 0.5, <span class="string">"<span class="variable">$v</span>$"</span>)</span><br><span class="line">plt.text(0.6, 3.2, <span class="string">"<span class="variable">$x</span>$"</span>)</span><br><span class="line">plt.xticks(np.arange(-3, 15))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-3, 7)</span><br><span class="line">plt.ylim(-1, 5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/line_with_distance_dot.png" alt="벡터 v 의 스칼라배한 직선위에 존재하지 않는 점과의 거리"></p><ul><li>예를 들어 아래와 같을 때</li></ul><script type="math/tex; mode=display">\begin{align} w = \begin{bmatrix}1 \\ 2\end{bmatrix} \tag{3.1.49} \end{align}</script><script type="math/tex; mode=display">\begin{align} \| w \|^2 = 5 \end{align}</script><script type="math/tex; mode=display">\begin{align} \begin{bmatrix}1 & 2\end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \end{bmatrix} - 5 = x_1 + 2x_2 - 5 = 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} x_1 + 2x_2 = 5 \end{align}</script><ul><li>이 방정식은 벡터 $w$가 가리키는 점 (1,2)를 지나면서 벡터 $w$에 수직인 직선을 뜻한다. 이 직선과 원점 사이의 거리는 $ |w|=\sqrt{5} $이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([1, 2])</span><br><span class="line">x1 = np.array([3, 1])</span><br><span class="line">x2 = np.array([-1, 3])</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=w, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x1, xytext=(0, 0), arrowprops=green)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x2, xytext=(0, 0), arrowprops=green)</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.plot(w[0], w[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot(x1[0], x1[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot(x2[0], x2[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot([-3, 5], [4, 0], <span class="string">'r-'</span>, lw=5)</span><br><span class="line">plt.text(-0.2, 1.5, <span class="string">"벡터 <span class="variable">$w</span>$"</span>)</span><br><span class="line">plt.text(1.55, 0.25, <span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.text(-0.9, 1.40, <span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.text(1.8, 1.8, <span class="string">"<span class="variable">$x_1</span> - w$"</span>)</span><br><span class="line">plt.text(-0.2, 2.8, <span class="string">"<span class="variable">$x_2</span> - w$"</span>)</span><br><span class="line">plt.text(3.6, 0.8, <span class="string">"직선 <span class="variable">$x</span>$"</span>)</span><br><span class="line">plt.xticks(np.arange(-2, 5))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-2, 5)</span><br><span class="line">plt.ylim(-0.6, 3.6)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/starting_zero_point_orthogonal_vector_distance.png" alt="원점에서 출발하는 벡터가 가리키는 점을 지나는 수직인 직선과의 거리"></p><ul><li>이번에는 벡터 $w$가 가리키는 점을 지나야 한다는 조건을 없애고 단순히<ul><li>벡터 $w$에 수직인</li></ul></li></ul><ul><li>직선 $x$의 방정식을 구하면 이때는 직선이 $w$가 아니라 $w$와 방향이 같고 길이가 다른 벡터 $w’=cw$을 지날 것이다. c는 양의 실수이다. 위에서 했던 방법으로 다시 직선의 방정식을 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} w'^Tx - \| w' \|^2 =  cw^Tx - c^2 \| w \|^2 = 0  \end{align}</script><script type="math/tex; mode=display">\begin{align} w^Tx - c \| w \|^2 = 0 \end{align}</script><ul><li>여기에서 $c | w |^2$는 임의의 수가 될 수 있으므로 단순히 벡터 $w$에 수직인 직선의 방정식은 다음과 같이 나타낼 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} w^Tx - w_0 = 0 \end{align}</script><ul><li>이 직선과 원점 사이의 거리는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} c \| w \| = \dfrac{w_0}{\|w\|}  \end{align}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([1, 2])</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=w, xytext=(0, 0), arrowprops=gray)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=0.5 * w, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.plot(0.5 * w[0], 0.5 * w[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot([-2, 5], [2.25, -1.25], <span class="string">'r-'</span>, lw=5)</span><br><span class="line">plt.text(-0.7, 0.8, <span class="string">"벡터 <span class="variable">$cw</span>$"</span>)</span><br><span class="line">plt.text(-0.1, 1.6, <span class="string">"벡터 <span class="variable">$w</span>$"</span>)</span><br><span class="line">plt.text(1, 1, <span class="string">"직선 <span class="variable">$x</span>$"</span>)</span><br><span class="line">plt.xticks(np.arange(-2, 5))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-2, 5)</span><br><span class="line">plt.ylim(-0.6, 3.6)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/orthogonal_vector_with_w.png" alt="벡터 w에 수직인 직선"></p><ul><li>예를 들어 $c=0.5$이면 벡터 $w=[1, 2]^T$에 수직이고 원점으로부터의 거리가 $\frac{\sqrt{5}}{2}$인 직선이 된다.</li></ul><script type="math/tex; mode=display">\begin{align} x_1 + 2x_2 - 2.5 = 0 \end{align}</script><h4 id="직선과-점의-거리"><a href="#직선과-점의-거리" class="headerlink" title="직선과 점의 거리"></a>직선과 점의 거리</h4><ul><li>이번에는 직선 $w^Tx - |w|^2 = 0$과 이 직선 위에 있지 않은 점 $x’$ 사이의 거리를 구해볼 것이다. 벡터 $w$에 대한 벡터 $x’$의 투영성분 $x’^{\Vert w}$의 길이는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} \|x'^{\Vert w}\| = \dfrac{w^Tx'}{\|w\|} \end{align}</script><ul><li>직선과 점 $x’$ 사이의 거리는 이 길이에서 원점에서 직선까지의 거리 $|w|$를 뺀 값의 절대값이다.</li></ul><script type="math/tex; mode=display">\begin{align} \left|  \|x'^{\Vert w}\| - \|w\| \right| = \left| \dfrac{w^Tx'}{\|w\|} - \|w\| \right| = \dfrac{\left|w^Tx' - \|w\|^2 \right|}{\|w\|} \end{align}</script><ul><li>직선의 방정식이 $w^Tx - w_0 = 0$이면 직선과 점의 거리는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} \dfrac{\left|w^Tx' - w_0 \right|}{\|w\|} \end{align}</script><ul><li><code>이 공식은 아래 내용중 SVM의 판별함수의 직선과 서포트벡터간의 거리를 계산하는데에서 사용</code>된다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([1, 2])</span><br><span class="line">x1 = np.array([4, 3])</span><br><span class="line">x2 = np.array([1, 2]) * 2</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x1, xytext=(0, 0), arrowprops=gray)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x2, xytext=(0, 0), arrowprops=gray)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=w, xytext=(0, 0), arrowprops=red)</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.plot(w[0], w[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot(x1[0], x1[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot([-3, 7], [4, -1], <span class="string">'r-'</span>, lw=5)</span><br><span class="line">plt.plot([2, 4], [4, 3], <span class="string">'k:'</span>, lw=2)</span><br><span class="line">plt.plot([3, 4], [1, 3], <span class="string">'k:'</span>, lw=2)</span><br><span class="line">plt.text(0.1, 0.9, <span class="string">"<span class="variable">$w</span>$"</span>)</span><br><span class="line">plt.text(4.2, 3.1, <span class="string">"<span class="variable">$x</span>'$"</span>)</span><br><span class="line">plt.text(1.5, 2.4, <span class="string">"<span class="variable">$x</span>'^&#123;\Vert w&#125;$"</span>)</span><br><span class="line">plt.xticks(np.arange(-3, 15))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-3, 7)</span><br><span class="line">plt.ylim(-1, 5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/distance_between_dots_and_line.png" alt="점과 직선사이의 거리"></p><ul><li>다시 SVM 판별함수를 살펴보면 정의에 따라 $y$값이 $+1$인 그 데이터 $x_{+}$에 대한 판별함수 값은 양수가 된다.</li></ul><script type="math/tex; mode=display">f(x_+) = w^Tx_+ - w_0 > 0</script><ul><li>반대로 y값이 -1인 그 데이터 $x_{-}$에 대한 판별함수 값은 음수가 된다.</li></ul><script type="math/tex; mode=display">f(x_-) = w^Tx_- - w_0 < 0</script><ul><li>$y$ 값이  $+1$인 데이터 중에서 판별 함수의 값이 가장 작은 데이터를 $x^{+}$라고 하고 $y$값이 $-1$인 데이터 중에서 판별함수의 값이 가장 큰 데이터를 $x^{-}$라고 하자. 이 데이터들은 각각의 클래스에 속한 데이터 중에서 가장 경계선에 가까이 붙어있는 최전방(most front)의 데이터들이다. 이러한 데이터를 서포트(support) 혹은 서포트 벡터(support vector)라고 한다. 물론 이 서포트에 대해서도 부호 조건은 만족되어야 한다.</li></ul><script type="math/tex; mode=display">f(x^+) = w^Tx^+ - w_0 > 0</script><script type="math/tex; mode=display">f(x^-) = w^Tx^- - w_0 < 0</script><ul><li>서포트에 대한 판별 함수의 값 $f(x^{+}), f(x^{-})$값은 부호 조건만 지키면 어떤 값이 되어도 괜찮다, 따라서 다음과 같은 조건을 만족하도록 판별 함수를 구한다.</li></ul><script type="math/tex; mode=display">f(x^+) = w^T x^{+} - w_0 = +1</script><script type="math/tex; mode=display">f(x^-) = w^T x^{-} - w_0 = -1</script><p><img src="/image/decision_function_value_of_SVM.png" alt="Support vector의 판별함수 값"></p><ul><li>이렇게 되면 모든 Support vector $(x_{+}, x_{-})$  데이터들에 대한 판별함수의 값의 절대값이 1보다 커지므로 다음 부등식이 성립한다.</li></ul><script type="math/tex; mode=display">w^Tx_+ - w_o \geq 1</script><script type="math/tex; mode=display">w^Tx_- - w_o \leq -1</script><ul><li>판별 경계선 $w^{T}x -  w_{0} = 0$과 점 $x^{+}, x^{-}$ 사이의 거리는 다음과 같이 계산할 수 있다.</li></ul><script type="math/tex; mode=display">\dfrac{w^T x^{+} - w_0}{\| w \|} = \dfrac{1}{\| w \|}</script><script type="math/tex; mode=display">-\dfrac{w^T x^{-} - w_0}{\| w \|} = \dfrac{1}{\| w \|}</script><ul><li>이 거리의 합을 마진(margin)이라고 하며 마진값이 클 수록 더 경계선이 안정적이라고 볼 수 있다. 그런데 위에서 정한 스케일링에 의해 마진은 다음과 같이 정리된다.</li></ul><script type="math/tex; mode=display">\dfrac{w^T x^{+} - w_0}{\| w \|}  -\dfrac{w^T x^{-} - w_0}{\| w \|} = \dfrac{2}{\| w \|}</script><ul><li>마진 값이 최대가 되는 경우는 $| w |$ 즉, $| w |^{2}$가 최소가 되는 경우와 같다. 다음과 같은 목적함수를 최소화하면 된다.</li></ul><script type="math/tex; mode=display">L = \dfrac{1}{2} ||w||^2 = \dfrac{1}{2} w^T w</script><ul><li>또한 모든 표본 데이터에 대해 분류는 제대로 되어야 하므로 모든 데이터 $x_{i}, y_{i} (i = 1,\ldots, N)$에 대해 다음 조건을 만족해야 한다. 위에서 스케일링을 사용하여 모든 데이터에 대해 $f(x_i) = w^Tx_i - w_o$가 $1$보다 크거나 $-1$보다 작게 만들었다는 점을 이용한다.</li></ul><script type="math/tex; mode=display">y_i \cdot f(x_i) = y_i \cdot( w^Tx_i - w_o) \geq 1 \;\;\; ( i = 1, \ldots, N )</script><script type="math/tex; mode=display">y_i \cdot ( w^Tx_i - w_o) - 1 \geq 0 \;\;\; ( i = 1, \ldots, N )</script><ul><li><code>라그랑주 승수법을 사용하면 최소화 목적함수를 다음과 같이 고치면 된다.</code> 즉, 위의 조건을 만족하는 w의 최소화 문제를 푸는 것과 같게 된다. $a_{i}$은 각각의 부등식에 대한 라그랑주 승수이다. 이 최적화 문제를 풀어 $w, w_{0}, a$를 구하면 판별함수를 얻을 수 있다.</li></ul><script type="math/tex; mode=display">L = \dfrac{1}{2} w^T w - \sum_{i=1}^N a_i \{ y_i \cdot ( w^Tx_i - w_o) - 1 \}</script><ul><li>KKT(Karush-Kuhn-Tucker) 조건에 따르면 부등식 제한 조건이 있는 경우에는 등식 제한조건을 가지는 라그랑주 승수 방법과 비슷하지만 $i$번째 부등식이 있으나 없으나 답이 같은 경우에는 해당 라그랑주 승수의 값이 $a_{i}=0$이 된다. 이 경우는 판별함수의 값 $w^Tx_i - w_o$이 $-1$보다 작거나 1보다 큰 경우이다.<br>즉, <code>마진안에 포함되지 않고 바깥에 있는 데이터들 같은 경우는 해당 조건식이 해를 찾는데 영향을 주지 않아 등식이 있는 최적화 문제를 푸는 것과 같다는 의미</code>이다.</li></ul><script type="math/tex; mode=display">y_i(w^Tx_i - w_o) - 1  > 0</script><ul><li>학습 데이터 중에서 최전방 데이터인 서포트 벡터가 아닌 모든 데이터들에 대해서는 이 조건이 만족되므로 서포트 벡터가 아닌 데이터는 라그랑지 승수가 $0$이라는 것을 알 수 있다.</li></ul><script type="math/tex; mode=display">a_i = 0 \;\; \text{if} \;\; x_i \notin \{ x^{+}, x^{-} \}</script><h3 id="듀얼-형식"><a href="#듀얼-형식" class="headerlink" title="듀얼 형식"></a>듀얼 형식</h3><ul><li>최적화 조건은 목적함수 $L$을 $w, w_{0}$로 미분한 값이 0이 되어야 하는 것이다.</li></ul><script type="math/tex; mode=display">\dfrac{\partial L}{\partial w} = 0</script><script type="math/tex; mode=display">\dfrac{\partial L}{\partial w_0} = 0</script><ul><li>이 식을 풀어서 정리하면 다음과 같아진다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} \dfrac{\partial L}{\partial w} &=& \dfrac{\partial}{\partial w} \left( \dfrac{1}{2} w^T w \right) - \dfrac{\partial}{\partial w} \sum_{i=1}^N \left( a_i y_i w^Tx_i - a_i y_i w_o - a_i \right) \\ &=& w - \sum_{i=1}^N  a_i y_i x_i \\ &=& 0 \end{eqnarray}</script><script type="math/tex; mode=display">\begin{eqnarray} \dfrac{\partial L}{\partial w_0} &=& \dfrac{\partial}{\partial w_0} \left( \dfrac{1}{2} w^T w \right) - \dfrac{\partial}{\partial w_0} \sum_{i=1}^N \left( a_i y_i w^Tx_i - a_i y_i w_o - a_i \right) \\&=& \sum_{i=1}^N  a_i y_i \\ &=& 0 \end{eqnarray}</script><ul><li>정리해보면, 다음과 같다.</li></ul><script type="math/tex; mode=display">w = \sum_{i=1}^N a_i y_i x_i</script><script type="math/tex; mode=display">0 = \sum_{i=1}^N a_i y_i</script><ul><li>이 두 수식을 원래의 목적함수에 대입하여 $w, w_{0}$을 없애면 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} L &=& \dfrac{1}{2} w^T w - \sum_{i=1}^N a_i \{ y_i \cdot ( w^Tx_i - w_o) - 1 \}  \\ &=& \dfrac{1}{2} \left( \sum_{i=1}^N a_i y_i x_i \right)^T \left( \sum_{j=1}^N a_j y_j x_j \right) - \sum_{i=1}^N a_i \left\{ y_i \cdot \left( \left( \sum_{j=1}^N a_j y_j x_j \right)^Tx_i - w_o \right) - 1 \right\}  \\ &=& \dfrac{1}{2} \sum_{i=1}^N \sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j - \sum_{i=1}^N \sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j + w_0 \sum_{i=1}^N a_i y_i + \sum_{i=1}^N a_i   \\ &=& \sum_{i=1}^N a_i - \dfrac{1}{2}\sum_{i=1}^N\sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j \end{eqnarray}</script><script type="math/tex; mode=display">L = \sum_{i=1}^N a_i - \dfrac{1}{2}\sum_{i=1}^N\sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j</script><ul><li>이 떄 $a$는 다음 조건을 만족한다.</li></ul><script type="math/tex; mode=display">\sum_{i=1}^N a_i y_i = 0</script><script type="math/tex; mode=display">a_i \geq 0 \;\;\;  ( i = 1, \ldots, N )</script><ul><li>이 문제는 $w$를 구하는 문제가 아니라 $a$만을 구하는 문제로 바뀌었으므로 듀얼형식(dual form)이라고 한다. 듀얼형식으로 바꾸면 수치적으로 박스(Box)제한 조건이 있는 이차프로그래밍(QP: Quadratic programming)문제가 되므로 원래의 문제보다는 효율적으로 풀 수 있다.</li></ul><p><a href="https://datascienceschool.net/view-notebook/0fca28c71c13460fb7168ee2adb9a8be/" target="_blank" rel="noopener">선형계획법 문제와 이차계획법 문제</a></p><ul><li>듀얼 형식 문제를 풀어 함수 $L$을 최소화하는 $a$를 구하면 예측 모형을 다음과 같이 쓸 수 있다.</li></ul><script type="math/tex; mode=display">f(x) = w^T x - w_0 = \sum_{i=1}^N a_i y_i x_i^T x - w_0</script><ul><li>$w_{0}$는 아래와 같이 구한다.</li></ul><script type="math/tex; mode=display">w_0 = w^T x^{+} - 1 또는 w_0 = w^T x^{-} + 1 또는 w_0 = \dfrac{1}{2} w^T (x^+ + x^{-})</script><ul><li>라그랑주 승수 값이 0 즉, $a_{i} = 0$이면 해당 데이터는 예측 모형, 즉 $w$ 계산에 아무런 기여를 하지 않으므로 위의 식은 실제로는 다음과 같다.</li></ul><script type="math/tex; mode=display">f(x) = a^+ x^T x^+ - a^- x^T x^- - w_0</script><ul><li>여기에서 $x^{T}x^{+}$는 $x$와 $x^{+}$ 사이의 코사인 유사도, $x^{T}x^{-}$는 $x$와 $x^{-}$ 사이의 코사인 유사도이므로 결국 두 <code>서포트 벡터와의 유사도를 측정해서 값이 큰쪽으로 판별</code>하게 된다.</li></ul><h3 id="Scikit-Learn의-서포트-벡터-머신"><a href="#Scikit-Learn의-서포트-벡터-머신" class="headerlink" title="Scikit-Learn의 서포트 벡터 머신"></a>Scikit-Learn의 서포트 벡터 머신</h3><ul><li>Scikit-Learn의 <code>svm</code> 서브패키지는 서포트 벡터 머신 모형인 <code>SVC</code>(Support Vector Classifier) 클래스를 제공한다. 이와 동시에 SVR(Support Vector Regressor)도 제공을 하지만 SVR은 추후에 설명하고 먼저, SVC에 대해 다루어 볼 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">X, y = make_blobs(n_samples=50, centers=2, cluster_std=0.5, random_state=4)</span><br><span class="line">y = 2 * y - 1</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y == -1, 0], X[y == -1, 1], marker=<span class="string">'o'</span>, label=<span class="string">"-1 클래스"</span>)</span><br><span class="line">plt.scatter(X[y == +1, 0], X[y == +1, 1], marker=<span class="string">'x'</span>, label=<span class="string">"+1 클래스"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">"학습용 데이터"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/Support_Vector_Machine_train_data_set.png" alt="Support Vector Machine train data set"></p><ul><li><code>SVC</code> 클래스는 커널(Kernel)을 선택하는 인수 <code>kernel</code>과 슬랙변수 가중치(slack variable weight)를 선택하는 인수 <code>C</code>를 받는데 지금까지 공부한 서포트 벡터 머신을 사용하려면 인수를 다음처럼 넣어준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.svm import SVC</span><br><span class="line">model = SVC(kernel=<span class="string">'linear'</span>, C=1e10).fit(X, y)</span><br></pre></td></tr></table></figure><ul><li><code>SVC</code>를 사용하여 모형을 구하면 다음과 같은 속성값을 가진다.<ul><li><code>n_support</code> : 각 클래스의 서포트 벡터의 개수</li><li><code>support</code> : 각 클래스의 서포트 벡터의 인덱스</li><li><code>support_vectors_</code> : 각 클래스의 서포트의 $x$값.$(x^{T}, x^{-})$</li><li><code>coef</code> : $w$벡터</li><li><code>intercept</code> : $- w_{0}$</li><li><code>dual_coef</code> : 각 원소가 $a_{i} \dot y_{i}$로 이루어진 벡터</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.n_support_</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1, 1], dtype=int32)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.support_</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([42,  1], dtype=int32)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.support_vectors_</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[9.03715314, 1.71813465],</span><br><span class="line">       [9.17124955, 3.52485535]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y[model.support_]</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-1,  1])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">xmin = X[:, 0].min()</span><br><span class="line">xmax = X[:, 0].max()</span><br><span class="line">ymin = X[:, 1].min()</span><br><span class="line">ymax = X[:, 1].max()</span><br><span class="line">xx = np.linspace(xmin, xmax, 10)</span><br><span class="line">yy = np.linspace(ymin, ymax, 10)</span><br><span class="line">X1, X2 = np.meshgrid(xx, yy)</span><br><span class="line"></span><br><span class="line">Z = np.empty(X1.shape)</span><br><span class="line"><span class="keyword">for</span> (i, j), val <span class="keyword">in</span> np.ndenumerate(X1):</span><br><span class="line">    x1 = val</span><br><span class="line">    x2 = X2[i, j]</span><br><span class="line">    p = model.decision_function([[x1, x2]])</span><br><span class="line">    Z[i, j] = p[0]</span><br><span class="line">levels = [-1, 0, 1]</span><br><span class="line">linestyles = [<span class="string">'dashed'</span>, <span class="string">'solid'</span>, <span class="string">'dashed'</span>]</span><br><span class="line">plt.scatter(X[y == -1, 0], X[y == -1, 1], marker=<span class="string">'o'</span>, label=<span class="string">"-1 클래스"</span>)</span><br><span class="line">plt.scatter(X[y == +1, 0], X[y == +1, 1], marker=<span class="string">'x'</span>, label=<span class="string">"+1 클래스"</span>)</span><br><span class="line">plt.contour(X1, X2, Z, levels, colors=<span class="string">'k'</span>, linestyles=linestyles)</span><br><span class="line">plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, alpha=0.3)</span><br><span class="line"></span><br><span class="line">x_new = [10, 2]</span><br><span class="line">plt.scatter(x_new[0], x_new[1], marker=<span class="string">'^'</span>, s=100)</span><br><span class="line">plt.text(x_new[0] + 0.03, x_new[1] + 0.08, <span class="string">"테스트 데이터"</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">"SVM 예측 결과"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/prediction_of_test_data_svm.png" alt="SVM 모델 test data 예측 결과"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = [10, 2]</span><br><span class="line">model.decision_function([x_new])</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-0.61101582])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.coef_.dot(x_new) + model.intercept_</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-0.61101582])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dual_coef_ = a_i * y_i</span></span><br><span class="line">model.dual_coef_</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[-0.60934379,  0.60934379]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.dual_coef_[0][0] * model.support_vectors_[0].dot(x_new) + \</span><br><span class="line">    model.dual_coef_[0][1] * model.support_vectors_[1].dot(x_new) + \</span><br><span class="line">    model.intercept_</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-0.61101582])</span><br></pre></td></tr></table></figure><blockquote><p>iris 문제를 서포트 벡터 머신으로 풀어보자. 다음과 같은 데이터만 사용한 이진 분류 문제로 바꾸어 풀어본다. 위의 예제와 마찬가지로 커널 인수 <code>kernel</code>과 슬랙변수 가중치 인수 <code>C</code>는 각각 <code>linear</code>, <code>1e10</code>으로 한다.</p></blockquote><pre><code>- 특징 변수를 꽃받침의 길이와 폭만 사용한다.- 붓꽆 종을 Setosa와 Versicolour만 대상으로 한다.</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix, classification_report</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X_data = iris.data[(iris.target == 0) | (iris.target == 1), :2]</span><br><span class="line">y = iris.target[(iris.target == 0) | (iris.target == 1)]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.3)</span><br><span class="line">svm = SVC(kernel=<span class="string">"linear"</span>, C=1e10)</span><br><span class="line">svm.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,</span><br><span class="line">    decision_function_shape=<span class="string">'ovr'</span>, degree=3, gamma=<span class="string">'auto_deprecated'</span>,</span><br><span class="line">    kernel=<span class="string">'linear'</span>, max_iter=-1, probability=False, random_state=None,</span><br><span class="line">    shrinking=True, tol=0.001, verbose=False)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred_y = svm.predict(X_test)</span><br><span class="line">confusion_matrix(pred_y, y_test)</span><br></pre></td></tr></table></figure><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[14,  0],</span><br><span class="line">       [ 0, 16]])</span><br></pre></td></tr></table></figure><ul><li><code>위의 조건에서 kernel=&quot;linear&quot;로 유지한채 C값만 [0.01, 0.1, 1, 10, 100]으로 변화를 주며 결과를 살펴보니 C값이 높아지면 Slack 변수로 줄 수 있는 값이 줄어들어 서포트 벡터의 수가 줄어든다. 반대로 C값을 낮추어 줄수록 Slack 변수가 갖는 값이 크게 되어 서포트 벡터는 많아지며 마진이 줄어든다.</code></li></ul><h3 id="슬랙변수"><a href="#슬랙변수" class="headerlink" title="슬랙변수"></a>슬랙변수</h3><ul><li><p>만약 데이터가 직선인 판별 경계선으로 나누어지지 않는 즉, 선형분이(linear seperable)가 불가능한 경우에는 다음과 같이 슬랙변수(slack variable)를 사용하여 개별적인 오차를 허용할 수 있다.</p></li><li><p>원래 판별 함수의 값은 클래스 $x^{T}$ 영역의 샘플 $x_{+}$에 대해선 첫번째 수식과 같고, 클래스 -1 영역의 샘플 $x_{-}$에 대해서는 두 번째 수식과 같아야한다.</p></li></ul><script type="math/tex; mode=display">w^Tx_+ - w_0 \geq 1</script><script type="math/tex; mode=display">w^Tx_- - w_0 \leq -1</script><ul><li>양수인 슬랙변수 $\xi \geq 0$를 사용하면 이 조건을 다음과 같이 완화할 수 있다.</li></ul><script type="math/tex; mode=display">w^Tx_+ - w_0 \geq +1-\xi_i</script><script type="math/tex; mode=display">w^Tx_- - w_0 \leq -1+\xi_i</script><ul><li>모든 슬랙변수는 0보다 같거나 크다.</li></ul><script type="math/tex; mode=display">\xi_i \geq 0 \;\;\; (i=1, \ldots, N)</script><ul><li>위의 부등식 조건을 모두 고려한 최적화 목적함수는 다음과 같아진다. 아래 식에서 $C \sum_{i=1}^N \xi_i$ 항은 슬랙변수의 합이 너무 커지지 않도록 제한하는 역할을 한다.</li></ul><script type="math/tex; mode=display">L = \dfrac{1}{2} ||w||^2 - \sum_{i=1}^N a_i (y_i \cdot ( w^Tx_i - w_o) - 1 + \xi_i ) - \sum_{i=1}^N \mu_i \xi_i  + C \sum_{i=1}^N \xi_i</script><p><img src="/image/Slack_variable_C_difference_each_value.png" alt="슬랙변수의 C값에 따른 오차 허용의 차이"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(0)</span><br><span class="line">X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]</span><br><span class="line">Y = [-1] * 20 + [1] * 20</span><br><span class="line"></span><br><span class="line">plotnum = 1</span><br><span class="line"><span class="keyword">for</span> name, penalty <span class="keyword">in</span> ((<span class="string">'C=10'</span>, 10), (<span class="string">'C=0.1'</span>, 0.1)):</span><br><span class="line">    clf = SVC(kernel=<span class="string">'linear'</span>, C=penalty).fit(X, Y)</span><br><span class="line">    xx = np.linspace(-5, 5)</span><br><span class="line"></span><br><span class="line">    x_jin = -5</span><br><span class="line">    x_jax = 5</span><br><span class="line">    y_jin = -9</span><br><span class="line">    y_jax = 9</span><br><span class="line">    XX, YY = np.mgrid[x_jin:x_jax:200j, y_jin:y_jax:200j]</span><br><span class="line"></span><br><span class="line">    levels = [-1, 0, 1]</span><br><span class="line">    linestyles = [<span class="string">'dashed'</span>, <span class="string">'solid'</span>, <span class="string">'dashed'</span>]</span><br><span class="line">    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])</span><br><span class="line">    Z = Z.reshape(XX.shape)</span><br><span class="line"></span><br><span class="line">    plt.subplot(1, 2, plotnum)</span><br><span class="line">    plt.contour(XX, YY, Z, levels, colors=<span class="string">'k'</span>, linestyles=linestyles)</span><br><span class="line">    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=120, linewidth=4)</span><br><span class="line">    plt.scatter(X[:, 0], X[:, 1], c=Y, s=60, linewidth=1, cmap=plt.cm.Paired)</span><br><span class="line">    plt.xlim(x_jin, x_jax)</span><br><span class="line">    plt.ylim(y_jin, y_jax)</span><br><span class="line">    plt.title(name)</span><br><span class="line"></span><br><span class="line">    plotnum += 1</span><br><span class="line"></span><br><span class="line">plt.suptitle(<span class="string">"슬랙변수 가중치 C의 영향"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/influence_of_weighted_value_C_with_slack_variables.png" alt="슬랙변수 가중치 C의 영향"></p><ul><li>다시 한번 정리하자면, 아래 그림에서 살펴보면 직선의 방정식 $x^{T} \beta + \beta_{0}$와 각 서포트 벡터 $x(x^{+}, x^{-})$와의 거리가 $\frac{1}{\beta$}이므로 결굴 마진을 크게 하는 것은 $\beta$를 작게 하는 것과 동일한 의미이다. 그러한 측면에서도 위에서 자세히 언급했듯이 Cost function(목적함수, 비용함수)가 아래와 같이 나온다는 것을 알 수 있다. 여기서 동시에 error를 최소화하고 싶으므로 라그랑주 승수 $C$를 크게가져가면서 error를 허용하는 slack변수를 최소화하는 동시에 $\beta$ 의 값도 최소화하는 optimization 문제를 풀 수 있다.</li></ul><p><img src="/image/calculate_svm.png" alt="SVM 계산 - 01"></p><ul><li>위에서의 조건하에 최적화를 하는 것이므로 라그랑주 승수를 도입하여 부등식이 있는 최적화 문제를 풀게 된다.</li></ul><p><img src="/image/calculate_svm_01.png" alt="SVM 계산 - 02"></p><p><img src="/image/calculate_svm_02.png" alt="SVM 계산 - 03"></p><ul><li>KKT조건을 만족함으로서, <code>global minimum을 보장</code>받을 수 있다.</li></ul><p><img src="/image/calculate_svm_03.png" alt="SVM 계산 - 04"></p><ul><li>즉, KKT의 2번째 조건에 의해서 서포트벡터인 경우는 조건식이 의미가 있기 때문에 라그랑지 승수 $\alpha_{i} \neq 0$이 된다는 의미이다.</li></ul><p><img src="/image/calculate_svm_04.png" alt="SVM 계산 - 05"></p><blockquote><p>얼굴 이미지 인식</p></blockquote><ul><li>총 40명이 각각 10장의 조금씩 다른 표정이나 모습으로 찍은 이미지 데이터이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_olivetti_faces</span><br><span class="line">faces = fetch_olivetti_faces()</span><br><span class="line"></span><br><span class="line">N = 2</span><br><span class="line">M = 5</span><br><span class="line">np.random.seed(0)</span><br><span class="line">fig = plt.figure(figsize=(9, 5))</span><br><span class="line">plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05)</span><br><span class="line">klist = np.random.choice(range(len(faces.data)), N * M)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(M):</span><br><span class="line">        k = klist[i * M + j]</span><br><span class="line">        ax = fig.add_subplot(N, M, i * M + j + 1)</span><br><span class="line">        ax.imshow(faces.images[k], cmap=plt.cm.bone)</span><br><span class="line">        ax.grid(False)</span><br><span class="line">        ax.xaxis.set_ticks([])</span><br><span class="line">        ax.yaxis.set_ticks([])</span><br><span class="line">        plt.title(faces.target[k])</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/fetch_olivetti_faces_01.png" alt="랜덤하게 뽑은 이미지"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(faces.data, faces.target, test_size=0.4, random_state=0)</span><br><span class="line"></span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">svc = SVC(kernel=<span class="string">'linear'</span>).fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">N = 2</span><br><span class="line">M = 5</span><br><span class="line">np.random.seed(4)</span><br><span class="line">fig = plt.figure(figsize=(9, 5))</span><br><span class="line">plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05)</span><br><span class="line">klist = np.random.choice(range(len(y_test)), N * M)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(M):</span><br><span class="line">        k = klist[i * M + j]</span><br><span class="line">        ax = fig.add_subplot(N, M, i * M + j + 1)</span><br><span class="line">        ax.imshow(X_test[k:(k + 1), :].reshape(64, 64), cmap=plt.cm.bone)</span><br><span class="line">        ax.grid(False)</span><br><span class="line">        ax.xaxis.set_ticks([])</span><br><span class="line">        ax.yaxis.set_ticks([])</span><br><span class="line">        plt.title(<span class="string">"%d =&gt; %d"</span> %</span><br><span class="line">                  (y_test[k], svc.predict(X_test[k:(k + 1), :])[0]))</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/fetch_olivetti_faces_02.png" alt="랜덤하게 뽑은 이미지의 예측"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import classification_report, accuracy_score</span><br><span class="line"></span><br><span class="line">y_pred_train = svc.predict(X_train)</span><br><span class="line">y_pred_test = svc.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_train, y_pred_train))</span><br></pre></td></tr></table></figure><h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       1.00      1.00      1.00         4</span><br><span class="line">           1       1.00      1.00      1.00         5</span><br><span class="line">           2       1.00      1.00      1.00         6</span><br><span class="line">           3       1.00      1.00      1.00         8</span><br><span class="line">           4       1.00      1.00      1.00         8</span><br><span class="line">           5       1.00      1.00      1.00         5</span><br><span class="line">           6       1.00      1.00      1.00         4</span><br><span class="line">           7       1.00      1.00      1.00         7</span><br><span class="line">           8       1.00      1.00      1.00         8</span><br><span class="line">           9       1.00      1.00      1.00         7</span><br><span class="line">          10       1.00      1.00      1.00         4</span><br><span class="line">          11       1.00      1.00      1.00         6</span><br><span class="line">          12       1.00      1.00      1.00         6</span><br><span class="line">          13       1.00      1.00      1.00         6</span><br><span class="line">          14       1.00      1.00      1.00         4</span><br><span class="line">          15       1.00      1.00      1.00         4</span><br><span class="line">          16       1.00      1.00      1.00         8</span><br><span class="line">          17       1.00      1.00      1.00         4</span><br><span class="line">          18       1.00      1.00      1.00         9</span><br><span class="line">          19       1.00      1.00      1.00         4</span><br><span class="line">          20       1.00      1.00      1.00         9</span><br><span class="line">          21       1.00      1.00      1.00         6</span><br><span class="line">          22       1.00      1.00      1.00         7</span><br><span class="line">          23       1.00      1.00      1.00         5</span><br><span class="line">          24       1.00      1.00      1.00         6</span><br><span class="line">          25       1.00      1.00      1.00         5</span><br><span class="line">          26       1.00      1.00      1.00         5</span><br><span class="line">          27       1.00      1.00      1.00         8</span><br><span class="line">          28       1.00      1.00      1.00         6</span><br><span class="line">          29       1.00      1.00      1.00         4</span><br><span class="line">          30       1.00      1.00      1.00         6</span><br><span class="line">          31       1.00      1.00      1.00         5</span><br><span class="line">          32       1.00      1.00      1.00         6</span><br><span class="line">          33       1.00      1.00      1.00         7</span><br><span class="line">          34       1.00      1.00      1.00         4</span><br><span class="line">          35       1.00      1.00      1.00         7</span><br><span class="line">          36       1.00      1.00      1.00         6</span><br><span class="line">          37       1.00      1.00      1.00         6</span><br><span class="line">          38       1.00      1.00      1.00         9</span><br><span class="line">          39       1.00      1.00      1.00         6</span><br><span class="line"></span><br><span class="line">    accuracy                           1.00       240</span><br><span class="line">   macro avg       1.00      1.00      1.00       240</span><br><span class="line">weighted avg       1.00      1.00      1.00       240</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred_test))</span><br></pre></td></tr></table></figure><h5 id="결과-14"><a href="#결과-14" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.86      1.00      0.92         6</span><br><span class="line">           1       1.00      1.00      1.00         5</span><br><span class="line">           2       1.00      1.00      1.00         4</span><br><span class="line">           3       0.50      1.00      0.67         2</span><br><span class="line">           4       1.00      0.50      0.67         2</span><br><span class="line">           5       1.00      1.00      1.00         5</span><br><span class="line">           6       0.83      0.83      0.83         6</span><br><span class="line">           7       1.00      0.67      0.80         3</span><br><span class="line">           8       0.67      1.00      0.80         2</span><br><span class="line">           9       1.00      1.00      1.00         3</span><br><span class="line">          10       1.00      1.00      1.00         6</span><br><span class="line">          11       1.00      1.00      1.00         4</span><br><span class="line">          12       0.67      1.00      0.80         4</span><br><span class="line">          13       1.00      1.00      1.00         4</span><br><span class="line">          14       1.00      1.00      1.00         6</span><br><span class="line">          15       1.00      0.33      0.50         6</span><br><span class="line">          16       0.67      1.00      0.80         2</span><br><span class="line">          17       1.00      1.00      1.00         6</span><br><span class="line">          18       1.00      1.00      1.00         1</span><br><span class="line">          19       1.00      1.00      1.00         6</span><br><span class="line">          20       1.00      1.00      1.00         1</span><br><span class="line">          21       1.00      0.75      0.86         4</span><br><span class="line">          22       1.00      1.00      1.00         3</span><br><span class="line">          23       0.71      1.00      0.83         5</span><br><span class="line">          24       1.00      1.00      1.00         4</span><br><span class="line">          25       1.00      1.00      1.00         5</span><br><span class="line">          26       1.00      1.00      1.00         5</span><br><span class="line">          27       1.00      1.00      1.00         2</span><br><span class="line">          28       1.00      1.00      1.00         4</span><br><span class="line">          29       1.00      1.00      1.00         6</span><br><span class="line">          30       1.00      1.00      1.00         4</span><br><span class="line">          31       1.00      1.00      1.00         5</span><br><span class="line">          32       1.00      1.00      1.00         4</span><br><span class="line">          33       1.00      1.00      1.00         3</span><br><span class="line">          34       1.00      0.83      0.91         6</span><br><span class="line">          35       1.00      0.67      0.80         3</span><br><span class="line">          36       1.00      1.00      1.00         4</span><br><span class="line">          37       1.00      1.00      1.00         4</span><br><span class="line">          38       0.50      1.00      0.67         1</span><br><span class="line">          39       0.67      0.50      0.57         4</span><br><span class="line"></span><br><span class="line">    accuracy                           0.93       160</span><br><span class="line">   macro avg       0.93      0.93      0.91       160</span><br><span class="line">weighted avg       0.95      0.93      0.92       160</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/22/machine_learning_11/#disqus_thread</comments>
    </item>
    
    <item>
      <title>퍼셉트론</title>
      <link>https://heung-bae-lee.github.io/2020/04/21/machine_learning_10/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/21/machine_learning_10/</guid>
      <pubDate>Tue, 21 Apr 2020 06:21:17 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;퍼셉트론&quot;&gt;&lt;a href=&quot;#퍼셉트론&quot; class=&quot;headerlink&quot; title=&quot;퍼셉트론&quot;&gt;&lt;/a&gt;퍼셉트론&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;퍼셉트론(perceptron)은 가장 오래되고 단순한 형태의 판별함수기반 분류모형 중 하나이다.&lt;/l
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="퍼셉트론"><a href="#퍼셉트론" class="headerlink" title="퍼셉트론"></a>퍼셉트론</h1><ul><li>퍼셉트론(perceptron)은 가장 오래되고 단순한 형태의 판별함수기반 분류모형 중 하나이다.</li></ul><p><img src="/image/perceptron_basic.png" alt="퍼셉트론"></p><ul><li>퍼셉트론은 입력 $x = (1, x_{1}, \cdots, x_{m})$에 대해 $1$ 또는 $-1$의 값을 가지는 $y$를 출력하는 비선형 함수이다. 1을 포함하는 입력 요소 $x_{i}$에 대해 가중치 $w_{i}$를 곱한 값 $a = w^{T}x$을 활성화값(activations)이라고 하며 이 값이 판별함수의 역할을 한다.</li></ul><script type="math/tex; mode=display">a = w^{T}x</script><ul><li>판별 함 값이 활성화함수(activation function) $h(a)$를 지나면 분류 결과를 나타내는 출력 $\hat{y}$가 생성된다.</li></ul><script type="math/tex; mode=display">\hat{y} = h(w^{T}x)</script><ul><li>퍼셉트론의 활성화 함수는 부호 함수(sign function) 또는 단위 계단 함수(Heaviside step function)라고 부르는 함수이다.</li></ul><script type="math/tex; mode=display">h(a) = \begin{cases} -1, & a < 0, \\ 1, & a \ge 0 \end{cases}</script><h2 id="퍼셉트론-손실함수"><a href="#퍼셉트론-손실함수" class="headerlink" title="퍼셉트론 손실함수"></a>퍼셉트론 손실함수</h2><p>다음과 같이 N개의 학습용 데이터가 있다고 하자.</p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/21/machine_learning_10/#disqus_thread</comments>
    </item>
    
    <item>
      <title>LDA, QDA</title>
      <link>https://heung-bae-lee.github.io/2020/04/19/machine_learning_09/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/19/machine_learning_09/</guid>
      <pubDate>Sun, 19 Apr 2020 13:26:35 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;LDA-선형판별분석법-QDA-이차판별분석법&quot;&gt;&lt;a href=&quot;#LDA-선형판별분석법-QDA-이차판별분석법&quot; class=&quot;headerlink&quot; title=&quot;LDA(선형판별분석법), QDA(이차판별분석법)&quot;&gt;&lt;/a&gt;LDA(선형판별분석법), 
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="LDA-선형판별분석법-QDA-이차판별분석법"><a href="#LDA-선형판별분석법-QDA-이차판별분석법" class="headerlink" title="LDA(선형판별분석법), QDA(이차판별분석법)"></a>LDA(선형판별분석법), QDA(이차판별분석법)</h1><ul><li>선형판별 분석법(Linear discriminant analysis, LDA)과 이차판별 분석법(quadratic discriminant analysis, QDA)는 대표적인 확률론적 생성모형이다. 가능도 y의 클래스값에 따른 x의 분포에 대한 정보를 먼저 알아낸 후, 베이즈 정리를 사용하여 주어진 x에 대한 y의 확률분포를 찾아낸다.</li></ul><h3 id="생성모형"><a href="#생성모형" class="headerlink" title="생성모형"></a>생성모형</h3><ul><li>생성모형에서는 베이즈 정리를 사용하여 조건부 확률 $p(y = k \mid x)$를 계산한다.</li></ul><script type="math/tex; mode=display">P(y = k \mid x) = \dfrac{P(x \mid y = k)\; P(y = k)}{P(x)}</script><ul><li>분류 문제를 풀기 위해서는 각 클래스 $k$에 대한 확률을 비교하여 가장 큰 값을 선택한다. 따라서 모든 클래스에 대해 값이 같은 분모 $P(x)$은 굳이 계산하지 않아도 괜찮다.</li></ul><script type="math/tex; mode=display">P(y = k \mid x) \;\; \propto \;\; P(x \mid y = k) \; P(y = k)</script><ul><li>여기에서 사전확률 $P(y=k)$는 특별한 정보가 없는 경우, 다음처럼 계산한다.</li></ul><script type="math/tex; mode=display">P(y = k) \approx \frac{ y = k \;\;인\;데이터\;수\;}{모든\;데이터의\;수}</script><ul><li><p><code>만약 다른 지식이나 정보로 알고있는 사전확률 값이 있다면 그 값을 사용하면 된다.</code></p></li><li><p>$y$에 대한 $x$의 조건부확률인 가능도는 다음과 같이 계산한다.</p><ul><li>1) $P(x \mid y = k)$가 특정한 확률분포 모형을 따른다고 가정한다. 즉, 확률밀도 함수의 형태를 가정한다.</li><li>2) $k$번째 클래스에 속하는 학습 데이터 $\{x_1, \cdots, x_N\}$을 사용하여 이 모형의 모수값을 구한다.</li><li>3) 모수값을 알고 있으므로 $P(x \mid y = k)$의 확률 밀도 함수를 구한 것이다. 즉, 새로운 독립변수 값 x이 어떤 값이 되더라도 $P(x \mid y = k)$의 값을 계산할 수 있다.</li></ul></li></ul><h3 id="LDA-Linear-Discriminant-Analysis"><a href="#LDA-Linear-Discriminant-Analysis" class="headerlink" title="LDA(Linear Discriminant Analysis)"></a>LDA(Linear Discriminant Analysis)</h3><ul><li><p>3가지(1,2,3) 범주의 클래스를 가지고 있는 데이터가 아래와 같을 때, 그림을 보면 <code>중심을 기점으로 마치 정규분포와 비슷하게 중심과의 거리가 멀어질수록 분포의 밀도가 떨어지는 듯해 보인다</code>면 LDA로 문제를 풀 수 있을 것 같다는 생각이 들어야 한다.</p></li><li><p>LDA(Linear Discriminant Analysis)는 선형 판별 분석법으로 불리며, PCA와 매우 유사하다. LDA는 PCA와 유사하게 입력 데이터 세트를 저차원 공간에 projection하여 차원을 축소하는 기법이지만, 중요한 차이는 <code>LDA는 지도학습의 분류(Classification)에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소</code>한다. <code>PCA는 입력 데이터의 변동성의 가장 큰 축을 찾았지만 LDA는 입력 데이터의 결정 값 클래스를 최대한 분리 할 수 축을 찾는다.</code></p></li></ul><p><img src="/image/Linear_discriminant_analysis_method_how_to_cal.png" alt="Linear Discriminant Analysis 배경"></p><ul><li>먼저, LDA도 이전의 나이브 베이지안 모형과 같이 특정 가정이 존재한다.<ul><li>클래스 집단별로 동일한 공분산 구조를 지녔다는 가정</li><li>클래스 집단별로 정규분포를 따른다는 가정</li></ul></li></ul><ul><li>아래 그림에서 가정을 적용한 후의 그림을 보면 3가지 영역으로 나누어져 있는데, 이러한 영역은 어떻게 나뉠수 있는지를 설명할 것이다.</li></ul><p><img src="/image/backgroud_with_LDA.png" alt="LDA의 가정"></p><ul><li>아래 그림과 같이 2차원(두가지 독립변수)의 두 가지 범주를 갖는 데이터를 분류하는 문제에서 LDA는 먼저 하나의 차원에 projection을 하여 차원을 축소시킨다. 그 후에 <code>클래스별 분포의 분산 대비 평균의 차이가 크게 나는 지점(즉, 두 집단의 평균의 평균점)을 decision boundary로 설정</code>한다.</li></ul><ul><li>아래 두 그림 중 어떠한 decision boundary를 갖는 것이 더 분류를 잘 한다고 생각이드는가? 아무래도 오른쪽을 선택하는 분들이 많을 것이다. <code>이렇게 데이터가 동일하여도 projection되는 축에 따라 decision boundary가 달라진다.</code>그러므로 <code>projection되는 축을 정하는 것이 중요</code>하다.</li></ul><p><img src="/image/LDA_decision_boundary_features.png" alt="LDA decision boundary"></p><ul><li>우선 분류를 하려면 각 클래스 집단의 평균의 차이가 큰 지점을 decision boundary로 찾는다면 쉽게 분류가 가능할 것이므로 아래 그림과 같이 두 평균 벡터의 차이에 평행한 축에 projection을 하여 두 클래스를 비교하게 된다. 그러나, 두 집단간의 분산은 크기 때문에(분산이 크다는 것은 변동성이 크다는 얘기이므로 데이터의 분포를 두 집단으로 나누기에 무리가 있다.) 아직까진 분류의 모형으로 부족해 보인다.</li></ul><p><img src="/image/how_to_decide_decision_boundary.png" alt="LDA decision boundary 결정 방법 - 01"></p><ul><li>위에서와 같이 각 클래스 집단의 평균의 차이만을 고려하는 것이 아니라, 각 클래스 집단의 분산은 작게끔하는 방향으로 projection을 시켜야 할 것이다. 분산이 작다면 그만큼 데이터의 분포가 밀집되어있으므로 분류의 예측 성능 또한 높아질 가능성이 커지기 때문이다.</li></ul><p><img src="/image/consider_with_variance_on_distribution_LDA.png" alt="LDA decision boundary 결정 방법 - 02"></p><h3 id="LDA의-수학적-개념-이해-다변량-정규분포"><a href="#LDA의-수학적-개념-이해-다변량-정규분포" class="headerlink" title="LDA의 수학적 개념 이해 - 다변량 정규분포"></a>LDA의 수학적 개념 이해 - 다변량 정규분포</h3><ul><li>이변량 정규 분포는 두 변수간의 상관계수 $\rho$가 0이라면 두 변수는 독립이므로 결국 위의 정규분포의 식을 곱한 값이 될 것이다. 그러나, 상관계수 $\rho$가 0이 아닌 값으로 존재한다. 아래 수식과 같은 jointed probability distribution을 갖는다. 3D 이미지로 살펴본다면 $\rho$의 절대값이 높아질수록 더 강한 선형성을 갖으며 전체 feature 공간에 대해서는 비대칭적이게 되어진다. 즉, 각 축의 방향에서 보았을 경우 분포의 이미지가 달라지게 된다.</li></ul><p><img src="/image/bivariate_normal_dist_formular.png" alt="이변량 정규 분포"></p><ul><li>다변량 정규 분포로 확장하기에 앞서서, 먼저 이변량 정규분포에 대해 정리한 후 그 개념을 확장시킬 것이다. 이변량 정규분포의 수식은 아래 그림과 같다. 만약 $\rho$가 0이 라면 위에서 언급했던 것과 같이 두 정규분포 수식을 단순한 곱한 것과 동일하다는 것을 확인 할 수 있다. 다변량 정규분포의 식에 $2X2$ 공분산행렬을 대입하여 계산한다면, 이변량 정규분포식이 나오게 된다.</li></ul><p><img src="/image/mutivariate_normal_dist_formular_01.png" alt="다변량 정규 분포"></p><ul><li>먼저 LDA는 확률적 생성모형이므로 확률값을 계산해본다면, 다음과 같이 k번째 범주 집단의 분포함수는 정의 될 수 있다.</li></ul><p><img src="/image/mutivariate_normal_dist_formular_02.png" alt="로그 다변량 정규 분포 - 01"></p><ul><li>그렇다면, 새롭게 들어오는 관측치에 대해 클래스를 분류할 경우 어떻게 확률값을 계산해야 할까? 다음과 같이 범주에서 2개를 한 쌍으로 뽑는 경우의 가지수$_{4}C_{2}$ 만큼의 계산을 통해 비교하여 최종적으로 확률값이 가장 높은 클래스로 분류를 할 것이다. 아래 수식에서 가장 마지막 수식은 이차식 처럼 모양이 나오게 되는데, 그 이유는 두 벡터의 내적이 되기 때문에 어떤 벡터를 먼저 곱하는지는 상관이 없기 때문이다. 즉, $x^{T} \sum^{-1} \mu_{k} = \mu_{k}^{T} \sum^{-1} x $이기 때문이다.</li></ul><p><img src="/image/mutivariate_normal_dist_formular_03.png" alt="로그 다변량 정규 분포 - 02"></p><ul><li><code>Linear Discriminant Analysis로 불리는 이유는 아래와 같이 feature x에 대한 1차식의 형태(선형구조)이기 때문</code>이다. 여기서 유추할 수 있는 점은 위의 식을 얻기 위해선 <code>LDA의 중요한 가정 중 하나인 클래스 집단 별 동일한 공분산 구조를 갖고있는다는 가정 때문</code>이었다. 만약, 클래스 집단 별 동일하지 않은 공분산 구조를 갖는다면 위의 식에서 공분산 행렬을 기준으로 곱해지는 2차형식(Quadratic form)이 없어지지 않을 것이다. <code>클래스 집단 별 다른 공분산 구조를 갖는다는 가정</code>을 한다면 <code>QDA</code>가 된다.</li></ul><p><img src="/image/mutivariate_normal_dist_formular_04.png" alt="LDA 확률 계산"></p><ul><li>LDA(Linear Discriminant Analysis)에서는 각 $Y$ 클래스에 대한 독립변수 $X$의 조건부 확률 분포가 공통된 공분산 행렬을 가지는 다변수 정규분포(multivariate Gaussian normal distribution)이라고 가정한다.</li></ul><script type="math/tex; mode=display">\Sigma_k = \Sigma \;\;\; \text{ for all } k</script><ul><li>이 때는 조건부확률분포를 다음과 같이 정리할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} \log p(x \mid y = k) &=& \log \dfrac{1}{(2\pi)^{D/2} |\Sigma|^{1/2}} -  \dfrac{1}{2} (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) \\ &=& C_0 - \dfrac{1}{2} (x-\mu_k)^T \Sigma^{-1} (x-\mu_k) \\ &=& C_0 - \dfrac{1}{2} \left( x^T\Sigma^{-1}x - 2\mu_k^T \Sigma^{-1}x + \mu_k^T \Sigma^{-1}\mu_k \right) \\ &=& C(x)  + \mu_k^T \Sigma^{-1}x - \dfrac{1}{2} \mu_k^T \Sigma^{-1}\mu_k \\ \end{eqnarray}</script><script type="math/tex; mode=display">\begin{eqnarray} p(x \mid y = k) &=& C'(x)\exp(w_k^Tx + w_{k0}) \\ \end{eqnarray}</script><ul><li>이 식에서 $C’(x)=exp C(x)$이다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} P(y=k \mid x) &=& \dfrac{p(x \mid y = k)P(y=k)}{\sum_l p(x \mid y = l)P(y=l) } \\ &=& \dfrac{C'(x)\exp(w_k^Tx + w_{k0}) P(y=k)}{\sum_l C'(x)\exp(w_l^Tx + w_{l0})P(y=l) } \\ &=& \dfrac{C'(x)\exp(w_k^Tx + w_{k0}) P(y=k)}{C'(x)\sum_l \exp(w_l^Tx + w_{l0})P(y=l) } \\ &=& \dfrac{P(y=k) \exp(w_k^Tx + w_{k0}) }{\sum_l P(y=l) \exp(w_l^Tx + w_{k0})} \\ &=& \dfrac{P(y=k) \exp(w_k^Tx + w_{k0}) }{P(x)} \\ \end{eqnarray}</script><ul><li>이 식에서 $P(x)$는 $y$클래스값에 영향을 받지 않는다. 따라서</li></ul><script type="math/tex; mode=display">\log P(y=k \mid x) = \log P(y=k) + w_k^Tx + w_{k0} - \log{P(x)} = w_k^Tx + C''_k</script><ul><li>모든 클래스 k에 대해 위와 같은 식이 성립하므로 클래스 $k_{l}$과 클래스 $k_{m}$의 경계선, 즉 두 클래스에 대한 확률값이 같아지는 $x$의 위치를 찾으면 다음과 같다.</li></ul><script type="math/tex; mode=display">w_{k_1}^Tx + C''_{k_1} = w_{k_2}^Tx + C''_{k_2}</script><script type="math/tex; mode=display">(w_{k_1} - w_{k_2})^Tx + (C''_{k_1} - C''_{k_2}) = 0</script><script type="math/tex; mode=display">w^Tx + C = 0</script><ul><li>즉, <code>판별함수가 x에 대한 선형방정식이 되고 경계선의 모양이 직선</code>이 된다.</li></ul><p><img src="/image/LDA_important_assumptions.png" alt="정리 - LDA의 가정"></p><p><img src="/image/how_to_calculate_probability_of_prediction_calss_on_LDA.png" alt="정리 - LDA의 추정"></p><p><img src="/image/LDA_estimation_course.png" alt="정리 - LDA의 확률 추정"></p><h2 id="LDA와-eigen-value와-eigen-vector와의-연관성"><a href="#LDA와-eigen-value와-eigen-vector와의-연관성" class="headerlink" title="LDA와 eigen value와 eigen vector와의 연관성"></a>LDA와 eigen value와 eigen vector와의 연관성</h2><ul><li>벡터 $a$를 다른 벡터 $b$에 직교하는 성분과 벡터 $b$에 평행한 성분으로 분해할 수 있는데, 평행한 성분을 벡터 $b$에 대한 투영성분(projection), 벡터  $b$에 대한 직교성분(rejection)이라고 하며 각각을 다음과 같이 표기한다.</li></ul><script type="math/tex; mode=display">\begin{align} a^{\Vert b} \tag{3.1.37} \end{align}</script><script type="math/tex; mode=display">\begin{align} a^{\perp b} \tag{3.1.38} \end{align}</script><ul><li><code>투영성분의 길이</code>는 다음처럼 구할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} \| a^{\Vert b} \| = \|a\|\cos\theta = \dfrac{\|a\|\|b\|\cos\theta}{\|b\|}  = \dfrac{a^Tb}{\|b\|} = \dfrac{b^Ta}{\|b\|} = a^T\dfrac{b}{\|b\|} \end{align}</script><ul><li>만약 벡터 $b$자체가 이미 단위벡터(Unit vector)이면 <code>단위벡터에 대한 투영길이는 내적</code>이 된다.</li></ul><script type="math/tex; mode=display">\begin{align} \| a^{\Vert b} \| = a^Tb \end{align}</script><ul><li><code>투영성분 성분 벡터</code>는 투영성분 길이와 벡터 $b$방향의 단위벡터의 곱이다.</li></ul><script type="math/tex; mode=display">\begin{align} a^{\Vert b} = \dfrac{a^Tb}{\|b\|} \dfrac{b}{\|b\|}= \dfrac{a^Tb}{\|b\|^2}b  \end{align}</script><ul><li>직교성분 벡터는 원래의 벡터에서 투영성분 성분 벡터를 뺀 나머지이다.</li></ul><script type="math/tex; mode=display">\begin{align} a^{\perp b} = a - a^{\Vert b} \end{align}</script><p><a href="https://datascienceschool.net/view-notebook/dd1680bfbaab414a8d54dc978c6e883a/" target="_blank" rel="noopener">참고 : projection과 rejection</a><br><a href="https://playground10.tistory.com/74" target="_blank" rel="noopener">참고 : 제2 코사인법칙을 사용한 내적과의 연관성</a></p><p><img src="/image/projeciton_related_with_LDA.png" alt="Projection"></p><ul><li>이러한 Projection이 LDA와의 관계를 설명하면 먼저, LDA의 목표인 클래스 집단 간의 평균은 크게하고 분산은 작게하는 decision boundary가 존재하는 Projection 공간을 말 할 수 있을 것이다.</li></ul><p><img src="/image/purpose_of_LDA.png" alt="LDA의 목표에 연관된 투영의 개념"></p><ul><li>아래 그림에서 말하는 사영시킬 벡터 $a$라는 것은 <code>decision boundary를 찾기위해 해당 데이터들을 투영시킨 벡터라는 의미</code>이다. 위의 그림에서는 분포로 존재하는 공간의 축을 의미한다. 단 아래 그림의 수식들이 정확하기 위해선 먼저 각 데이터의 feature 벡터들이 Unit vector이고 사영된 벡터도 Unit vector인 경우에 한해서 아래의 수식이 정확하다고 할 수 있다.</li></ul><p><img src="/image/purpose_of_LDA_01.png" alt="LDA의 decision boundary를 찾기 위한 projection - 01"></p><ul><li>LDA의 목표에 맞게 클래스 집단의 평균의 차이는 크게하면서 분산은 최소화 시키는 사영을 찾는것이므로 아래와 같은 분수식을 사용할 수 있다. 각 평균과 분산은 projection된 성분을 가리킨다. 행렬표현으로 묶어서 다르게 표현한다면 맨 마직막의 수식들로 표현 가능하다.</li></ul><p><img src="/image/purpose_of_LDA_02.png" alt="LDA의 decision boundary를 찾기 위한 projection - 02"></p><ul><li>행렬식에 대해 가장 큰 a값을 찾기 위해서는 미분을 해야 할 것이다. 미분을 통해 아래 그림과 같은 형태로 변형시킬 수 있다. 맨 마지막 수식을 살펴보면 eigen value와 eigen vector의 정의와 동일하게 볼 수 있음을 확인 할 수 있다.</li></ul><p><img src="/image/purpose_of_LDA_03.png" alt="LDA의 decision boundary를 찾기 위한 projection - 02"></p><ul><li>위의 식을 다시한번 정리하면 eigen vector와 eigen value를 구할 수 있는데, <code>해당 데이터를 축소하는데 방향은 변하지 않고 길이만 변하는 벡터인 eigen vector를 찾으면 LDA의 decision boundary가 존재하는 공간인 벡터</code>이다.</li></ul><p><img src="/image/eigen_vector_eigen_value_related_with_LDA.png" alt="LDA의 decision boundary를 찾기 위한 projection - 03"></p><p><img src="/image/eigen_vector_eigen_value_related_with_LDA_01.png" alt="LDA의 decision boundary에서 eigen vector의 역할"></p><ul><li>이전에 decision boundary를 찾는 공식에서 살펴보면, 찾아낸 축이 기울기 역할을 하고 클래스 집단의 평균을 지나는 초평면이 decision boundary라는 사실을 확인할 수 있다.</li></ul><p><img src="/image/eigen_vector_eigen_value_related_with_LDA_02.png" alt="LDA의 수학적 개념 총 정리"></p><ul><li>Scikit-Learn은 선형판별분석법을 위한 <code>LinearDiscriminantAnalysis</code> 클래스를 제공한다.<ul><li>참고로 데이터는 아래 QDA와 동일한 데이터를 사용한다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis</span><br><span class="line"></span><br><span class="line">lda = LinearDiscriminantAnalysis(n_components=3, solver=<span class="string">"svd"</span>, store_covariance=True).fit(X, y)</span><br></pre></td></tr></table></figure><ul><li><code>선형판별 분석법</code>에서는 <code>기대값 벡터만 클래스에 따라 달라지고 공분산 행렬은 공통으로 추정</code>한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lda.means_</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[-8.01254084e-04,  1.19457204e-01],</span><br><span class="line">       [ 1.16303727e+00,  1.03930605e+00],</span><br><span class="line">       [-8.64060404e-01,  1.02295794e+00]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lda.covariance_</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.7718516 , 0.13942905],</span><br><span class="line">       [0.13942905, 0.7620019 ]])</span><br></pre></td></tr></table></figure><ul><li>결과는 다음처럼 직선인 경계선을 가진다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x1min, x1max = -5, 5</span><br><span class="line">x2min, x2max = -4, 5</span><br><span class="line">XX1, XX2 = np.meshgrid(np.arange(x1min, x1max, (x1max-x1min)/1000),</span><br><span class="line">                       np.arange(x2min, x2max, (x2max-x2min)/1000))</span><br><span class="line">YY = np.reshape(lda.predict(np.array([XX1.ravel(), XX2.ravel()]).T), XX1.shape)</span><br><span class="line">cmap = mpl.colors.ListedColormap(sns.color_palette([<span class="string">"r"</span>, <span class="string">"g"</span>, <span class="string">"b"</span>]).as_hex())</span><br><span class="line">plt.contourf(XX1, XX2, YY, cmap=cmap, alpha=0.5)</span><br><span class="line">plt.scatter(X1[:, 0], X1[:, 1], alpha=0.8, s=50, marker=<span class="string">"o"</span>, color=<span class="string">'r'</span>, label=<span class="string">"클래스 1"</span>)</span><br><span class="line">plt.scatter(X2[:, 0], X2[:, 1], alpha=0.8, s=50, marker=<span class="string">"s"</span>, color=<span class="string">'g'</span>, label=<span class="string">"클래스 2"</span>)</span><br><span class="line">plt.scatter(X3[:, 0], X3[:, 1], alpha=0.8, s=50, marker=<span class="string">"x"</span>, color=<span class="string">'b'</span>, label=<span class="string">"클래스 3"</span>)</span><br><span class="line">plt.xlim(x1min, x1max)</span><br><span class="line">plt.ylim(x2min, x2max)</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">"LDA 분석 결과"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/LDA_analysis_result.png" alt="LDA 분석의 결과"></p><h4 id="선형판별분석법을-사용하여-붓꽃-분류문제를-풀고-성능을-분류결과표와-분류보고서를-출력하라-그리고-각-클래스에-대한-ROC-커브를-그려라"><a href="#선형판별분석법을-사용하여-붓꽃-분류문제를-풀고-성능을-분류결과표와-분류보고서를-출력하라-그리고-각-클래스에-대한-ROC-커브를-그려라" class="headerlink" title="선형판별분석법을 사용하여 붓꽃 분류문제를 풀고 성능을 분류결과표와 분류보고서를 출력하라. 그리고 각 클래스에 대한 ROC 커브를 그려라."></a>선형판별분석법을 사용하여 붓꽃 분류문제를 풀고 성능을 분류결과표와 분류보고서를 출력하라. 그리고 각 클래스에 대한 ROC 커브를 그려라.</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score</span><br></pre></td></tr></table></figure><ul><li>클래스별 고루 학습하기 위해 stratify하게 설정하였다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1234, stratify=y)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">LDA = LinearDiscriminantAnalysis()</span><br><span class="line">LDA.fit(train_X, train_y)</span><br><span class="line">pred_LDA = LDA.predict(test_X)</span><br><span class="line">confusion_matrix(pred_LDA, test_y)</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[15,  0,  0],</span><br><span class="line">       [ 0, 14,  0],</span><br><span class="line">       [ 0,  1, 15]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(pred_LDA, test_y))</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       1.00      1.00      1.00        15</span><br><span class="line">           1       0.93      1.00      0.97        14</span><br><span class="line">           2       1.00      0.94      0.97        16</span><br><span class="line"></span><br><span class="line">    accuracy                           0.98        45</span><br><span class="line">   macro avg       0.98      0.98      0.98        45</span><br><span class="line">weighted avg       0.98      0.98      0.98        45</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">roc_auc_score(label_binarize(test_y, classes=[0,1,2]), LDA.predict_proba(test_X))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">label_test_y = label_binarize(test_y, [0, 1, 2])</span><br><span class="line"></span><br><span class="line">fpr = [None] * 3</span><br><span class="line">tpr = [None] * 3</span><br><span class="line">thr = [None] * 3</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(3):</span><br><span class="line">    fpr[i], tpr[i], thr[i] = roc_curve(label_test_y[:, i], LDA.predict_proba(test_X)[:, i])</span><br><span class="line">    plt.plot(fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Fall-Out'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/LDA_analysis_result_on_iris.png" alt="iris 데이터에 대한 LDA 분석 결과의 ROC curve"></p><h2 id="QDA-Quadratic-Discriminant-Analysis"><a href="#QDA-Quadratic-Discriminant-Analysis" class="headerlink" title="QDA(Quadratic Discriminant Analysis)"></a>QDA(Quadratic Discriminant Analysis)</h2><ul><li>나이브 베이즈 모형은 조건부독립이라는 가정이 있었기 때문에 사실상 각 설명변수들간의 공분산 구조를 반영하진 않는다. 그에 반해, LDA는 설명변수간 비슷한 수준의 공분산 구조를 갖는다는 가정하에 모형을 실행하는데, <code>공분산 구조의 차이가 심하게</code> 난다면, LDA 가정을 실행할 수 없다. 이런 경우에 사용하는 것이 바로 QDA이다. <code>LDA는 가장 작은 그룹의 샘플 수가 독립(설명)변수의 개수보다 많아야 하는데 나이브 베이즈 모형은 조건부 독립을 가정하기에 훨씬 더 적은 샘플이더라도 추정이 가능</code>하다.</li></ul><p><img src="/image/limitation_of_LDA.png" alt="LDA 가정 총 정리"></p><ul><li>이차판별분석법(QDA)에서는 <code>독립변수 x가 실수이고 확률분포가 다변량 정규분포라고 가정한다. 단 x분포의 위치와 형태는 클래스에 따라 달라질 수 있다.</code>즉, <code>클래스별로 다른 공분산 구조를 갖는다는 의미</code>이다. 공분산 구조가 동일하지 않아 이차형식(Quadratic form)이 남게 된다.</li></ul><p><img src="/image/QDA_conception.png" alt="QDA의 개념"></p><script type="math/tex; mode=display">p(x \mid y = k) = \dfrac{1}{(2\pi)^{D/2} |\Sigma_k|^{1/2}} \exp \left( -\dfrac{1}{2} (x-\mu_k)^T \Sigma_k^{-1} (x-\mu_k) \right)</script><ul><li>이 분포들을 알고 있으면 독립변수 $x$에 대한 $y$ 클래스의 조건부확률분포는 다음과 같이 베이즈 정리와 전체 확률의 법칙으로 구할 수 있다.</li></ul><script type="math/tex; mode=display">P(y=k \mid x) = \dfrac{p(x \mid y = k)P(y=k)}{p(x)} = \dfrac{p(x \mid y = k)P(y=k)}{\sum_l p(x \mid y = l)P(y=l) }</script><ul><li>LDA는 Decision boundary가 선형으로 되기 때문에 아래 그림에서 보는 것과 같은 데이터 분포이면 구분하는데 어렵다. 허나 변수의 제곱을한 추가적인 변수들을 통해 이를 보완할 수 있다. 그렇지 않고 현재 존재하는 변수들만 사용해 문제를 풀기 위해선 QDA를 사용하는 것이 좋다. 중간에 있는 그림은 클래스 별 같은 공분산 구조를 가진다고 가정을 하고 변수를 추가해 주어 계산되는 반면에 오른쪽 그림은 처음 데이터 변수들만 사용하게 된다. 허나, QDA는 클래스별 서로 다른 공분산 구조를 가진다는 가정이 있기 때문에 모수를 추정하는 횟수가 그 만큼 더 많이 늘어나는 문제도 있다.</li></ul><p><img src="/image/what_is_difference_with_LDA_and_QDA_plot.png" alt="LDA와 QDA의 비교 - 01"></p><p><img src="/image/what_is_diiference_with_LDA_and_QDA_on_feature.png" alt="LDA와 QDA의 비교 - 02"></p><ul><li>예를 들어 $y$가 1,2,3 이라는 3개의 클래스를 가지고 각 클래스에서의 $x$의 확률분포가 다음과 같은 기대값 및 공분산 행렬을 가진다고 가정하자.</li></ul><script type="math/tex; mode=display">\mu_1 = \begin{bmatrix}  0 \\ 0 \end{bmatrix}, \;\; \mu_2 = \begin{bmatrix}  1 \\ 1 \end{bmatrix}, \;\; \mu_3 = \begin{bmatrix}-1  \\ 1 \end{bmatrix}</script><script type="math/tex; mode=display">\Sigma_1 = \begin{bmatrix} 0.7 &   0 \\ 0   & 0.7 \end{bmatrix}, \;\; \Sigma_2 = \begin{bmatrix} 0.8 & 0.2 \\ 0.2 & 0.8 \end{bmatrix}, \;\; \Sigma_3 = \begin{bmatrix} 0.8 & 0.2 \\ 0.2 & 0.8 \end{bmatrix}</script><ul><li>$y$의 사전 확률은 다음과 같이 동일하다.</li></ul><script type="math/tex; mode=display">P(Y=1) = P(Y=2) = P(Y=3) = \dfrac{1}{3}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">N = 100</span><br><span class="line">rv1 = sp.stats.multivariate_normal([ 0, 0], [[0.7, 0.0], [0.0, 0.7]])</span><br><span class="line">rv2 = sp.stats.multivariate_normal([ 1, 1], [[0.8, 0.2], [0.2, 0.8]])</span><br><span class="line">rv3 = sp.stats.multivariate_normal([-1, 1], [[0.8, 0.2], [0.2, 0.8]])</span><br><span class="line">np.random.seed(0)</span><br><span class="line">X1 = rv1.rvs(N)</span><br><span class="line">X2 = rv2.rvs(N)</span><br><span class="line">X3 = rv3.rvs(N)</span><br><span class="line">y1 = np.zeros(N)</span><br><span class="line">y2 = np.ones(N)</span><br><span class="line">y3 = 2 * np.ones(N)</span><br><span class="line">X = np.vstack([X1, X2, X3])</span><br><span class="line">y = np.hstack([y1, y2, y3])</span><br><span class="line"></span><br><span class="line">plt.scatter(X1[:, 0], X1[:, 1], alpha=0.8, s=50, marker=<span class="string">"o"</span>, color=<span class="string">'r'</span>, label=<span class="string">"class 1"</span>)</span><br><span class="line">plt.scatter(X2[:, 0], X2[:, 1], alpha=0.8, s=50, marker=<span class="string">"s"</span>, color=<span class="string">'g'</span>, label=<span class="string">"class 2"</span>)</span><br><span class="line">plt.scatter(X3[:, 0], X3[:, 1], alpha=0.8, s=50, marker=<span class="string">"x"</span>, color=<span class="string">'b'</span>, label=<span class="string">"class 3"</span>)</span><br><span class="line">plt.xlim(-5, 5)</span><br><span class="line">plt.ylim(-4, 5)</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/each_parameter_having_normal_dist.png" alt="클래스별 서로 다른 모수를 갖는 정규분포"></p><ul><li>Scikit-Learn은 이차판별 분석법을 위한 <code>QuadraticDiscriminantAnalysis</code>클래스를 제공한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis</span><br><span class="line"></span><br><span class="line">qda = QuadraticDiscriminantAnalysis(store_covariance=True).fit(X, y)</span><br></pre></td></tr></table></figure><ul><li>학습용 데이터에서 가능도를 추정한 후에는 다음과 같은 속성을 가지게 된다.<ul><li><code>priors_</code> : 각 클래스 $k$의 사전확률</li><li><code>means_</code> : 각 클래스 $k$에서 $x$의 기대값 벡터 $\mu_{k}$의 추정치 벡터</li><li><code>covariance_</code> : 각 클래스 $k$에서 $x$의 공분산 행렬 $\sum_{k}$의 추정치 행렬.(생성자 인수 store_covariance=True인 경우에만 제공)</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qda.priors_</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.33333333, 0.33333333, 0.33333333])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qda.means_</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[-8.01254084e-04,  1.19457204e-01],</span><br><span class="line">       [ 1.16303727e+00,  1.03930605e+00],</span><br><span class="line">       [-8.64060404e-01,  1.02295794e+00]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qda.covariance_[0]</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.73846319, -0.01762041],</span><br><span class="line">       [-0.01762041,  0.72961278]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qda.covariance_[1]</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.66534246, 0.21132313],</span><br><span class="line">       [0.21132313, 0.78806006]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qda.covariance_[2]</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.9351386 , 0.22880955],</span><br><span class="line">       [0.22880955, 0.79142383]])</span><br></pre></td></tr></table></figure><ul><li>이 확률분포를 사용하여 분류를 한 결과는 다음과 같다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x1min, x1max = -5, 5</span><br><span class="line">x2min, x2max = -4, 5</span><br><span class="line">XX1, XX2 = np.meshgrid(np.arange(x1min, x1max, (x1max-x1min)/1000),</span><br><span class="line">                       np.arange(x2min, x2max, (x2max-x2min)/1000))</span><br><span class="line">YY = np.reshape(qda.predict(np.array([XX1.ravel(), XX2.ravel()]).T), XX1.shape)</span><br><span class="line">cmap = mpl.colors.ListedColormap(sns.color_palette([<span class="string">"r"</span>, <span class="string">"g"</span>, <span class="string">"b"</span>]).as_hex())</span><br><span class="line">plt.contourf(XX1, XX2, YY, cmap=cmap, alpha=0.5)</span><br><span class="line">plt.scatter(X1[:, 0], X1[:, 1], alpha=0.8, s=50, marker=<span class="string">"o"</span>, color=<span class="string">'r'</span>, label=<span class="string">"클래스 1"</span>)</span><br><span class="line">plt.scatter(X2[:, 0], X2[:, 1], alpha=0.8, s=50, marker=<span class="string">"s"</span>, color=<span class="string">'g'</span>, label=<span class="string">"클래스 2"</span>)</span><br><span class="line">plt.scatter(X3[:, 0], X3[:, 1], alpha=0.8, s=50, marker=<span class="string">"x"</span>, color=<span class="string">'b'</span>, label=<span class="string">"클래스 3"</span>)</span><br><span class="line">plt.xlim(x1min, x1max)</span><br><span class="line">plt.ylim(x2min, x2max)</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"이차판별분석법 결과"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/QDA_anlaysis_result.png" alt="QDA의 결과"></p><h4 id="QDA를-사용하여-iris-classification문제를-풀고-성능을-confusion-matrix와-classification-report를-출력하라-그리고-각-클래스에-대한-ROC-커브를-그려라"><a href="#QDA를-사용하여-iris-classification문제를-풀고-성능을-confusion-matrix와-classification-report를-출력하라-그리고-각-클래스에-대한-ROC-커브를-그려라" class="headerlink" title="QDA를 사용하여 iris classification문제를 풀고 성능을 confusion_matrix와 classification_report를 출력하라. 그리고 각 클래스에 대한 ROC 커브를 그려라."></a>QDA를 사용하여 iris classification문제를 풀고 성능을 confusion_matrix와 classification_report를 출력하라. 그리고 각 클래스에 대한 ROC 커브를 그려라.</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.metrics import confusion_matrix, classification_report</span><br></pre></td></tr></table></figure><ul><li>각 클래스를 동일하게(stratify) 학습시키기 위해서 train과 test를 나눌때 설정해주었다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1234, stratify=y)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">QDA = QuadraticDiscriminantAnalysis()</span><br><span class="line">QDA.fit(train_X, train_y)</span><br><span class="line">pred_QDA = QDA.predict(test_X)</span><br><span class="line">confusion_matrix(pred_QDA, test_y)</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[15,  0,  0],</span><br><span class="line">       [ 0, 14,  0],</span><br><span class="line">       [ 0,  1, 15]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(pred_QDA, test_y))</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       1.00      1.00      1.00        15</span><br><span class="line">           1       0.93      1.00      0.97        14</span><br><span class="line">           2       1.00      0.94      0.97        16</span><br><span class="line"></span><br><span class="line">    accuracy                           0.98        45</span><br><span class="line">   macro avg       0.98      0.98      0.98        45</span><br><span class="line">weighted avg       0.98      0.98      0.98        45</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">roc_auc_score(label_binarize(test_y, classes=[0,1,2]), QDA.decision_function(test_X))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">roc_auc_score(label_binarize(test_y, classes=[0,1,2]), QDA.predict_proba(test_X))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">label_test_y = label_binarize(test_y, [0, 1, 2])</span><br><span class="line"></span><br><span class="line">fpr = [None] * 3</span><br><span class="line">tpr = [None] * 3</span><br><span class="line">thr = [None] * 3</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(3):</span><br><span class="line">    fpr[i], tpr[i], thr[i] = roc_curve(label_test_y[:, i], QDA.predict_proba(test_X)[:, i])</span><br><span class="line">    plt.plot(fpr[i], tpr[i])</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'위양성률(Fall-Out)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'재현률(Recall)'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/roc_curve_with_QDA_iris_data.png" alt="iris test data에 대한 QDA ROC curve"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/19/machine_learning_09/#disqus_thread</comments>
    </item>
    
    <item>
      <title>K-Nearest Neighbors(KNN)</title>
      <link>https://heung-bae-lee.github.io/2020/04/17/machine_learning_08/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/17/machine_learning_08/</guid>
      <pubDate>Fri, 17 Apr 2020 09:11:42 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;K-Nearest-Neighbors-KNN&quot;&gt;&lt;a href=&quot;#K-Nearest-Neighbors-KNN&quot; class=&quot;headerlink&quot; title=&quot;K-Nearest Neighbors(KNN)&quot;&gt;&lt;/a&gt;K-Nearest Neighb
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="K-Nearest-Neighbors-KNN"><a href="#K-Nearest-Neighbors-KNN" class="headerlink" title="K-Nearest Neighbors(KNN)"></a>K-Nearest Neighbors(KNN)</h1><ul><li><p><code>k의 개수만큼 주변에 있는 sample들의 정보를 이용해서 새로운 관측치의 종속변수를 예측하는 방법</code>이다. 아래 그림에서 기존의 파란색 네모와 빨간색 세모라는 2가지 클래스를 갖는 데이터 집합이 있다고 할 때, 여기서 새로운 관측치인 녹색에 대해 어떻게 예측할지를 생각해 보자.</p><ul><li>k=3인 경우(실선): 빨간색 세모로 분류될 가능성이 높다.</li><li>k=5인 경우(점선): 파란색 네모로 분류될 가능성이 높다.<ul><li>허나, k=5인 경우 빨간색 세모와의 거리가 더 가깝기 때문에 그만큼의 weight를 주어서 갯수를 떠나서 빨간색으로 분류될 가능성도 알고리즘에 따라 존재한다.</li></ul></li></ul></li></ul><p><img src="/image/KNN_algorithm_concept.png" alt="KNN 알고리즘"></p><h2 id="K-Nearest-neighborhood"><a href="#K-Nearest-neighborhood" class="headerlink" title="K-Nearest neighborhood"></a>K-Nearest neighborhood</h2><p><img src="/image/what_kinds_of_case_KNN_with_some_k.png" alt="K에 따른 KNN의 결과"></p><p><img src="/image/KNN_algorithm_how_to_decide_k.png" alt="파라미터 K의 상황 및 voting 방법"></p><p><img src="/image/how_to_decide_target_variable_in_KNN.png" alt="종속변수에 따른 KNN 알고리즘의 결과값"></p><ul><li>그렇다면 voting 방식외에도 관측치들간의 distance에 따른 가중치를 주는데 여기서 거리를 구하는 방법은 아래 그림과 같은 방법들이 존재한다. 물론, 이 밖에도 데이터의 거리를 구하는 방법은 더 많이 존재한다. 대표적인 유클리디안 거리와 맨하탄 거리, 그리고 범주형 변수에 사용되는 Hamming distance를 간단히 보여 줄 것이다. 유클리디안 거리는 우리가 잘 알고있는 피타고라스 정리에 의해 두 지점 사이의 최단 거리를 구하는 공식으로 계산되며, 맨하탄 거리는 맨하탄 같이 블록별로 되어있는 곳에서의 거리를 계산할 경우 사용된다. 마지막으로 Hamming distance를 계산하는 방식은 Indicator함수안의 조건이 참인 경우만을 1로 값을 산출하여 합한 값이다. 즉, 쉽게 말하면 해당 이진값들의 자리에서 다른 곳이 존재하는 개수를 의미한다.</li></ul><p><img src="/image/how_to_calculate_distance_for_each_type_of_variables.png" alt="독립변수에 따른 각 관측치들간 거리 계산 방법"></p><ul><li>먼저 독립 변수와 새로이 예측하려는 관측값과의 거리를 따져 가장 가까운 데이터부터 순서대로 정렬해 놓는다. 통계적으로 순서 통계량이라고 할 수 있다. 그에 따른 순서로 독립변수와 종속 변수의 쌍으로 정렬한다. 여기서의 거리는 위에서 언급했던 방법들을 사용한다.</li></ul><p><img src="/image/KNN_order_statistics_and_distance_cal.png" alt="KNN 알고리즘의 순서"></p><ul><li>종속변수가 범주형이라면 아래와 같이 소프트 보팅방법으로 확률을 구해 확률값이 가장 큰 클래스를 예측값으로 채택한다. 물론, 범주형 변수도 연속형 변수처럼 거리에 의한 가중치를 사용할 수 있다.</li></ul><p><img src="/image/categorical_variable_method_in_KNN.png" alt="종속변수가 범주형인 경우의 KNN"></p><ul><li>종속변수가 연속형인 경우는 평균을 구하는데, 거리에 따른 가중치를 두어 가중평균을 구하는 것이 일반적이다. 가중치는 거리에 반비례하게 하여 거리가 짧을수록 큰 가중치를 갖게 한다.</li></ul><p><img src="/image/numerical_variable_method_in_KNN.png" alt="종속변수가 연속형인 경우의 KNN"></p><h3 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross validation"></a>Cross validation</h3><ul><li><p>교차검증(Cross validation)은 기본적으로 과적합에 대한 방지를 위해서 실행하며, 또 다른 이유로는 sample loss에 관한 측면이 존재한다.</p></li><li><p>과적합(Overfitting)문제는 Training set에만 최적화 되어있어 새로운 데이터가 들어왔을 때 기존의 Training set에 이질적으로 처리하기 때문이다. 이는 모델의 복잡도와도 밀접한 연관이 있다. 모델의 복잡도가 높을수록 Training set은 잘 맞추지만, Test set에 대한 성능은 낮을 가능성이 높다. 그렇다고 너무 간단하게 만들어버려도 Training set과 Test set 모두에 대해 대한 성능이 좋지 않을 것이다.</p></li><li><p>Training error는 error를 과소추정하는 성향이 있다는 말은 아래 그래프를 살펴보면서 설명하겠다. KNN 모델의 경우 K가 작을수록 모델의 복잡도는 높을 것이다. 가장 복잡한 k=1인 경우 Training error는 거의 0에 수렴한다. 허나, Test error는 상당히 높다. 즉, 과적합이 발생되었다는 이야기이다. 여기서 알 수 있는 것은 <code>Training error로 모델을 선택한다면 너무 복잡한 모델을 선택하여 과적합을 발생시켜 Test error가 커지게 될 시킬 수 있다</code>는 점이다.</p></li></ul><p><img src="/image/overfitting_problem_with_KNN.png" alt="과적합 문제"></p><ul><li>또한, Test error를 구하기 위해 처음에 Train set, Test set으로 나누어야 하기에 Test set으로 나눈만큼의 데이터 소실로 인해 Test error는 증가하게 된다. 이렇게 Error를 과소추정하지 않기 위해서는 교차검증을 해야 할 것이다.</li></ul><p><img src="/image/How_to_overcome_overfitting.png" alt="과적합 문제 보완 방안"></p><ul><li>이러한 과적합을 방지하고자 CV를 실행하는데 그 중 K-hold Cross validation은 다음과 같은 방법으로 진행한다.</li></ul><p><img src="/image/K_hold_crosvalidation_with_KNN_01.png" alt="K-hold 교차검증 - 01"></p><ul><li>Loss function은 예로 들어놓은 것인데, 해당 문제의 성능지표를 의미한다.</li></ul><p><img src="/image/K_hold_crosvalidation_with_KNN_02.png" alt="K-hold 교차검증 - 02"></p><h2 id="KNN의-심화적-이해"><a href="#KNN의-심화적-이해" class="headerlink" title="KNN의 심화적 이해"></a>KNN의 심화적 이해</h2><ul><li>데이터에 따라 적절한 K가 다른데 아래 그림에서와 같이 너무 K를 작게 설정하면, Train set은 굉장히 잘 설명하지만 Test set에 대해선 성능이 안좋게 되는 과적합이 발생될 수 있다. 그 다음으로는 K=1로 설정했을 경우 이상치와의 거리가 가장 가까운 데이터에 영향을 줄 것이다. 아래의 예시 그래프 처럼 영역이 불연속적이어서 영역을 나누는 의미가 없어 보이는 경우가 발생 될 수 있다. 반대로 K가 너무 크다면 미세한 경계에 분류가 아쉬울 것이다.</li></ul><p><img src="/image/what_kinds_of_case_KNN_with_some_k.png" alt="모수 K의 결정"></p><ul><li>데이터에 따라 적절한 K가 다르므로 Test error를 작게하는 k를 선택해야 할 것이다. Cross-validation을 이용하여 보통 모형의 모수들을 조절하게 된다. 그 중 검증 데이터에 대한 성능이 좋은 모수를 채택하여 최종적으로 test 데이터를 예측하는 것이다.</li></ul><p><img src="/image/K_decision_on_KNN.png" alt="K의 결정"></p><ul><li>지난 번에 언급했던 차원의 저주와 KNN 알고리즘도 밀접한 관련이 있다. 차원의 저주는 차원이 늘어남에 따라 우리가 설명하고 싶은 공간 대비 설명할 수 있는 공간이 줄어드는 문제인데, 아래 그림에서와 같이 가로축 변수만을 사용한다면 충분히 두 클래스를 나눌 수 있는 기준을 설정할 수 있지만, 오히려 세로축에 의해서는 클래스를 구분하는데 아무런 영향을 주지 않는데 세로축 변수도 고려하게 되면서 다른 예측을 하게 된다.</li></ul><p><img src="/image/demasion_of_curse_on_knn.png" alt="차원의 저주"></p><h2 id="k-Nearest-Neighborhood-Algorithm-실습"><a href="#k-Nearest-Neighborhood-Algorithm-실습" class="headerlink" title="k-Nearest Neighborhood Algorithm 실습"></a>k-Nearest Neighborhood Algorithm 실습</h2><h3 id="1-데이터-모듈-불러오기-및-kNN-피팅-방법"><a href="#1-데이터-모듈-불러오기-및-kNN-피팅-방법" class="headerlink" title="1. 데이터, 모듈 불러오기 및 kNN 피팅 방법"></a>1. 데이터, 모듈 불러오기 및 kNN 피팅 방법</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import modin.pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line">from sklearn import neighbors, datasets</span><br><span class="line">from sklearn.metrics import confusion_matrix</span><br></pre></td></tr></table></figure><ul><li>iris 데이터를 사용할 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line">X = iris.data[:, :2]</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure><ul><li>모델 구축<ul><li>neighbors를 5로 설정</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clf = neighbors.KNeighborsClassifier(n_neighbors=5)</span><br><span class="line">clf.fit(X,y)</span><br><span class="line">y_pred=clf.predict(X)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusion_matrix(y,y_pred)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[49,  1,  0],</span><br><span class="line">       [ 0, 38, 12],</span><br><span class="line">       [ 0, 12, 38]])</span><br></pre></td></tr></table></figure><h3 id="2-Cross-validation을-활용한-최적의-k찾기"><a href="#2-Cross-validation을-활용한-최적의-k찾기" class="headerlink" title="2.Cross-validation을 활용한 최적의 k찾기"></a>2.Cross-validation을 활용한 최적의 k찾기</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line"></span><br><span class="line">k_range = np.arange(1,100)</span><br><span class="line">k_scores = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_range:</span><br><span class="line">    knn=neighbors.KNeighborsClassifier(k)</span><br><span class="line">    scores= cross_val_score(knn, X, y, cv=10, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line">    k_scores.append(scores.mean())</span><br><span class="line"></span><br><span class="line">plt.plot(k_range, k_scores)</span><br><span class="line">plt.xlabel(<span class="string">'Value of K for KNN'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cross-validated accuracy'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li>이렇게 적절한 모수를 찾는 방법은 위에서와 같이 직접 grid search를 하는 방법을 주로 사용한다.</li></ul><p><img src="/image/decision_parameter_through_cross_validation.png" alt="모수 K의 결정"></p><h3 id="2-Weight를-준-kNN"><a href="#2-Weight를-준-kNN" class="headerlink" title="2.Weight를 준 kNN"></a>2.Weight를 준 kNN</h3><ul><li>거리를 가중치로 사용한 경우가 좀 더 decision boundary가 매끄럽게 연결되어 있다. 좀 더 보편적인 모델인 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">n_neighbors = 40</span><br><span class="line"></span><br><span class="line">h = .02  <span class="comment"># step size in the mesh</span></span><br><span class="line"></span><br><span class="line">cmap_light = ListedColormap([<span class="string">'#FFAAAA'</span>, <span class="string">'#AAFFAA'</span>, <span class="string">'#AAAAFF'</span>])</span><br><span class="line">cmap_bold = ListedColormap([<span class="string">'#FF0000'</span>, <span class="string">'#00FF00'</span>, <span class="string">'#0000FF'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> weights <span class="keyword">in</span> [<span class="string">'uniform'</span>, <span class="string">'distance'</span>]:</span><br><span class="line">    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line">    <span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot also the training points</span></span><br><span class="line">    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,</span><br><span class="line">                edgecolor=<span class="string">'k'</span>, s=20)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line">    plt.title(<span class="string">"3-Class classification (k = %i, weights = '%s')"</span></span><br><span class="line">              % (n_neighbors, weights))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/mesh_color_groud_plot_with_KNN.png" alt="거리를 가중치로 사용한 경우와 비교"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(0)</span><br><span class="line">X = np.sort(5 * np.random.rand(40, 1), axis=0)</span><br><span class="line">T = np.linspace(0, 5, 500)[:, np.newaxis]</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line">y[::5] += 1 * (0.5 - np.random.rand(8))</span><br><span class="line"></span><br><span class="line">knn = neighbors.KNeighborsRegressor(n_neighbors)</span><br><span class="line">y_ = knn.fit(X, y).predict(T)</span><br></pre></td></tr></table></figure><ul><li>이웃을 하나만 사용할 때는 훈련 세트의 각 데이터 포인트가 예측에 주는 영향이 커서 예측값이 훈련 데이터 포인트를 모두 지나간다. 이는 매우 불안정한 예측을 만들어 낸다. 이웃을 많이 사용하면 훈련 데이터에는 잘 안 맞을 수 있지만 더 안정된 예측을 얻게 된다. 또한 Uniform하게 모두 동일한 가중치를 사용해서 계산하는 것보다 때론 거리를 통해 가중치를 주는 편이 아래 그림에서와 같이 좀 더 예측하는 데 효과가 있을수도 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">n_neighbors = 5</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, weights <span class="keyword">in</span> enumerate([<span class="string">'uniform'</span>, <span class="string">'distance'</span>]):</span><br><span class="line">    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)</span><br><span class="line">    y_ = knn.fit(X, y).predict(T)</span><br><span class="line"></span><br><span class="line">    plt.subplot(2, 1, i + 1)</span><br><span class="line">    plt.scatter(X, y, c=<span class="string">'k'</span>, label=<span class="string">'data'</span>)</span><br><span class="line">    plt.plot(T, y_, c=<span class="string">'g'</span>, label=<span class="string">'prediction'</span>)</span><br><span class="line">    plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(<span class="string">"KNeighborsRegressor (k = %i, weights = '%s')"</span> % (n_neighbors,</span><br><span class="line">                                                                weights))</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/regression_of_KNN.png" alt="KNN알고리즘을 통한 회귀"></p><h3 id="장단점과-매개변수"><a href="#장단점과-매개변수" class="headerlink" title="장단점과 매개변수"></a>장단점과 매개변수</h3><ul><li>일반적으로 KNeighbors 분류기에 중요한 매개변수는 두 가지이다. 데이터 포인트 사이의 거리를 재는 방법과 이웃의 수이다. 실제로 <code>이웃의 수는 3개나 5개 정도로 적을 때 잘 작동</code>하지만, 이 매개변수는 잘 조정해야 합니다.</li></ul><ul><li>k-NN의 장점은 이해하기 매우 쉬운 모델이라는 점이다. 그리고 많이 조정하지 않아도 자주 좋은 성능을 발휘한다. 더 복잡한 알고리즘을 적용해보기 전에 시도해볼 수 있는 좋은 시작점이다. 보통 최근접 이웃 모델은 매우 빠르게 만들 수 있지만, 훈련 세트가 매우 크면 (특성의 수나 샘플의 수가 클 경우) 예측이 느려진다. k-NN 알고리즘을 사용할 땐 <code>데이터를 전처리하는 과정이 중요</code>하다. 그리고 <code>(수백 개 이상의) 많은 특성을 가진 데이터셋에는 잘 동작하지 않으며, 특성 값 대부분이 0인 (즉 희소한) 데이터셋과는 특히 잘 작동하지 않는다.</code></li></ul><ul><li>k-최근접 이웃 알고리즘이 <code>이해하긴 쉽지만, 예측이 느리고 많은 특성을 처리하는 능력이 부족해 현업에서는 잘 쓰지 않는다.</code></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/17/machine_learning_08/#disqus_thread</comments>
    </item>
    
    <item>
      <title>나이브 베이즈 분류모형</title>
      <link>https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/</guid>
      <pubDate>Tue, 14 Apr 2020 05:59:44 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;분류모형&quot;&gt;&lt;a href=&quot;#분류모형&quot; class=&quot;headerlink&quot; title=&quot;분류모형&quot;&gt;&lt;/a&gt;분류모형&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/image/classification_problem.png&quot; alt=&quot;분류 문제란?&quot;&gt;&lt;/p
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="분류모형"><a href="#분류모형" class="headerlink" title="분류모형"></a>분류모형</h1><p><img src="/image/classification_problem.png" alt="분류 문제란?"></p><ul><li>현실적인 문제로 바꾸어 말하면 어떤 표본에 대한 데이터가 주어졌을 때 그 표본이 어떤 카테고리 혹은 클래스에 속하는지를 알아내는 문제이기도 하다.</li></ul><p><img src="/image/what_kinds_of_clssification_model.png" alt="분류모형의 종류"></p><ul><li>흔히들 많이 사용하는 모형들의 분류모형 종류를 아래의 표로 기재해 놓았다. 이전에 기재했던 로지스틱 회귀 분석도 실질적으론 분류문제에 많이 사용된다.</li></ul><div class="table-container"><table><thead><tr><th>모형</th><th>방법론</th></tr></thead><tbody><tr><td>나이브 베이지안</td><td>확률적 생성모형</td></tr><tr><td>LDA/QDA</td><td>확률적 생성모형</td></tr><tr><td>로지스틱회귀</td><td>확률적 판별모형</td></tr><tr><td>의사결정나무</td><td>확률적 판별모형</td></tr><tr><td>퍼셉트론</td><td>판별함수 모형</td></tr><tr><td>서포트벡터머신</td><td>판별함수 모형</td></tr><tr><td>인공신경망</td><td>판별함수 모형</td></tr></tbody></table></div><p><img src="/image/classification_graph.png" alt="분류모형의 종류 도식화"></p><h1 id="나이브-베이즈-분류-모형"><a href="#나이브-베이즈-분류-모형" class="headerlink" title="나이브 베이즈 분류 모형"></a>나이브 베이즈 분류 모형</h1><ul><li>아래 그림과 같은 질문에 대한 확률을 계산하려면, 아래 결합확률을 계산해야 할 것이다. 독립변수가 2개 즉, Feature의 차원이 2개인 지금은 크게 어렵진 않겠지만, 현실에서는 Feature의 개수가 훨씬 더 많을 것이다. 차원이 커진다면 이에따라 해당 조건부 함수의 가능도함수를 추정하는데 어려움을 겪을 것이다.</li></ul><script type="math/tex; mode=display">\begin{align} P( X = weather state, Humidity | Y = playing Tennis) \\ = \frac{P( X_{1} = weather state, Humidity \cap Y = playing Tennis)}{P(Y = playing Tennis)} \end{align}</script><p><img src="/image/naive_bayesian_model_assumption.png" alt="나이브 베이지안 모형의 가정"></p><ul><li>위의 수식에서 분자부분의 확률인 결합확률(Joint probability)을 구하는 것은 쉬운 문제가 아니다. 결합되어있는 상황을 나누어서 확률을 계산할 순 없을까라는 가정에서 나이브 베이지안 모형은 출발한다. 그렇다면, 어떻게 나눌수 있을까? 통계를 조금이라도 아시는 분들은 독립이라는 개념을 알고계실 것이다. 예를 들어 A: 코로나가 집단 발병한다, B: 2주간 재택근무를 시행한다. 이 두가지 사건은 서로 발현될 확률에 영향을 미치기 때문에 두 사건은 종속(dependent) 관계라고 할 수 있다. 허나, A: 비가온다, B: 대출심사에서 승인을 받는다 라는 두 사건은 서로 둘 중 어떤 사건에도 발현될 확률에 영향을 주지 않는다. 이러한, 경우에 우리는 두 사건이 확률적으로 독립이라고 정의한다. 두 사건 A,B가 독립일 필요충분조건은 아래와 같다.</li></ul><script type="math/tex; mode=display">P(A \cap B) = P(A)P(B)</script><ul><li>이렇게 두 사건이 독립이라면 좀 더 추정하기 쉬울 것이다. 허나, 위의 상황에선 조건부이기 때문에 <code>조건부 독립</code>의 가정이 맞을 것이다. 조건부 독립(conditional independence)은 일반적인 독립과 달리 조건이 되는 별개의 확률변수 C가 존재해야한다. 아래 수식이 성립되면 조건부 독립이라고 한다.</li></ul><script type="math/tex; mode=display">A \text{⫫} B \;\vert\; C \Longrightarrow P(A, B | C) = P(A|C)P(B|C)</script><ul><li>조건부독립과 비교하여 일반적인 독립은 무조건부독립이라고 한다. 무조건부 독립은 다음과 같이 표기하기도 한다.</li></ul><script type="math/tex; mode=display">A \text{⫫} B \;\vert\; \emptyset</script><ul><li>A, B가 C에 대해 조건부 독립이면 다음도 만족한다.</li></ul><script type="math/tex; mode=display">P(A \;\vert\; B, C) = P(A \vert C)</script><script type="math/tex; mode=display">P(B \;\vert\; A, C) = P(B \vert C)</script><script type="math/tex; mode=display">\begin{align} P(A|B,C) &= \frac{P(A,B,C)}{P(B,C)} \\ &= \frac{P(A,B|C)P(C)}{P(B|C)P(C)} \\ &= \frac{P(A|C)P(B|C)}{P(B|C)} \\ &= P(A|C) \end{align}</script><ul><li><code>주의할 점은 조건부 독립과 무조건부 독립은 관계가 없다는 점</code>이다. 즉, 두 확률변수가 독립이라고 항상 조건부 독립이 되는 것도 아니고 조건부 독립이라고 꼭 독립이 되는 것도 아니다.</li></ul><script type="math/tex; mode=display">P(A,B) = P(A)P(B) \;\; \nRightarrow \;\; P(A,B|C) = P(A|C)P(B|C)</script><script type="math/tex; mode=display">P(A,B|C) = P(A|C)P(B|C) \;\; \nRightarrow \;\; P(A,B) = P(A)P(B)</script><h4 id="베이즈-정리-Bayes’-rule"><a href="#베이즈-정리-Bayes’-rule" class="headerlink" title="베이즈 정리(Bayes’ rule)"></a>베이즈 정리(Bayes’ rule)</h4><ul><li>베이즈 정리는 사건 B가 발생함으로써 사건 A의 확률이 어떻게 변화하는지를 표현한 정리이다. 따라서 <code>베이즈 정리는 새로운 정보가 기존의 추론에 어떻게 영향을 미치는지를 나타낸다.</code></li></ul><script type="math/tex; mode=display">P(A|B) = \frac{P(B|A) P(A)}{P(B)}</script><ul><li>$P(A|B)$ : 사후확률(Posterior). 사건 B가 발생한 후 갱신된 사건 A의 확률</li><li>$P(A)$ : 사전확률(Prior). 사건 B가 발생하기 전에 가지고 있던 사건 A의 확률</li><li>$P(B|A)$ : 가능도(likelihood). 사건 A가 발생한 경우 사건 B의 확률</li><li>$P(B)$ : 정규화 상수(normalizing constant) 또는 증거(evidence). 확률의 크기 조정역할을 함.</li></ul><blockquote><p>이렇게 독립변수(Feature)들간에 서로 조건부독립임을 가정하여 조건을 나이브하게 하였고, 베이즈정리를 사용하여 MLE를 통해 가장 큰 확률값을 갖는 모수를 추정해내는 모형이 나이브 베이지안 모형이다.</p></blockquote><script type="math/tex; mode=display">\begin{align} P(y = k \mid x) &= \dfrac{ P(x_1, \ldots, x_D \mid y = k) P(y = k) }{P(x)} \\ &= \dfrac{ \left( \prod_{d=1}^D P(x_{d} \mid y = k) \right) P(y = k) }{P(x)} \end{align}</script><ul><li>제일 처음 예시로 들었던 날씨의 상태와 습도를 독립변수로 하고, 종속변수를 테니스를 치는지에 대한 여부에 대한 문제를 푼다고 가정해보자. 먼저 테니스를 치는 경우에 대한 확률을 계산할 경우 테니스를 치는 사건에 대한 확률을 높게 해주는 두가지 경우의 수를 아래와 같이 생각해 볼 수 있다. 아래 두 그림을 수식으로 정리해보면 다음과 같다는 사실을 쉽게 생각해 볼 수 있다. <code>수식의 마지막에서 분자부분에서 likelihood와 Prior 확률값을 살펴보면 그림에서 언급한 두 가지 경우의 사건들의 확률 값의 곱으로 되어있음</code>을 볼 수 있다. 즉, <code>사전확률에 새롭게 추가되는 likelihood가 추론의 어떤 영향을 미치는지를 나타내는 베이즈 정리의 의미를 생각해 볼 수 있다</code>.</li></ul><script type="math/tex; mode=display">\begin{align} P(weather state, Humidity | playing Tennis) \\ &= \frac{P(weather state)}{P(playing Tennis)} \frac{P(Humidity)}{P(playing Tennis)} \\ &= \frac{P(playing Tennis|weather state)P(weather state)}{P(playing Tennis)} \frac{P(playing Tennis|Humidity)P(Humidity)}{P(playing Tennis)} \end{align}</script><p><img src="/image/naive_bayesian_model_example_01.png" alt="나이브 베이지안 모델의 모수 추정 - 01"></p><p><img src="/image/naive_bayesian_model_example_02.png" alt="나이브 베이지안 모델의 모수 추정 - 02"></p><ul><li>아래 그림과 같이 데이터가 주어졌을 때의 베이즈 정리를 이용해 계산하면 다음과 같다.</li></ul><p><img src="/image/Naive_bayesian_probability.png" alt="베이즈 정리 확률 예시"></p><ul><li>예시로 계산해 본 것과 같이 아래의 그림처럼 독립변수들이 조건부 독립이라는 조건을 통해 곱으로 표현 될 수 있다. 예측값은 마지막 수식에서 보는 것과 같이 해당 가능도함수의 확률값을 최대로 해주는 class를 선택하여 예측값으로 출력해준다.</li></ul><p><a href="https://datascienceschool.net/view-notebook/864a2cc43df44531be32e3fa48769501/" target="_blank" rel="noopener">참조 : MLE 개념</a></p><p><img src="/image/Naive_bayesian_model_estimaion_01.png" alt="나이브 베이지안 모형 모수 추정식"></p><ul><li>이제 하나씩 앞에서 언급했던 순서대로 계산하여 최종적으로 출력을 내는 단계까지 그림을 통해 살펴볼 것이다.</li></ul><p><img src="/image/Naive_bayesian_model_estimaion_02.png" alt="나이브 베이지안 모형 계산 - 01"></p><p><img src="/image/Naive_bayesian_model_estimaion_03.png" alt="나이브 베이지안 모형 계산 - 02"></p><p><img src="/image/Naive_bayesian_model_estimaion_04.png" alt="나이브 베이지안 모형 계산 - 03"></p><p><img src="/image/Naive_bayesian_model_estimaion_05.png" alt="나이브 베이지안 모형 계산 - 04"></p><p><img src="/image/Naive_bayesian_model_estimaion_06.png" alt="나이브 베이지안 모형 계산 - 05"></p><h3 id="나이브-베이지안-모형의-종류"><a href="#나이브-베이지안-모형의-종류" class="headerlink" title="나이브 베이지안 모형의 종류"></a>나이브 베이지안 모형의 종류</h3><p><img src="/image/Naive_bayesian_classifier_type.png" alt="나이브 베이지안 모형의 종류"></p><h4 id="나이브-가정"><a href="#나이브-가정" class="headerlink" title="나이브 가정"></a>나이브 가정</h4><ul><li>독립변수 $x$가 $D$차원이라고 가정하자.</li></ul><script type="math/tex; mode=display">x = (x_1, \ldots, x_D)</script><ul><li>가능도 함수는 $x_{1}, \ldots, x_{D}$의 결합확률이 된다.</li></ul><script type="math/tex; mode=display">P(x \mid y = k) = P(x_1, \ldots, x_D \mid y = k)</script><ul><li>원리상으로는 $y=k$인 데이터만 모아서 이 가능도함수의 모양을 추정할 수 있다. 하지만 차원 $D$가 커지면 가능도함수의 추정이 현실적으로 어려워진다. 따라서 나이브베이즈 분류모형(Naive Bayes classification model)에서는 모든 차원의 개별 독립변수가 서로 조건부 독립(conditional independent)이라는 가정을 사용한다. 이러한 가정을 나이브 가정(naive assumption)이라고 한다.</li></ul><ul><li>나이브 가정으로 사용하면 벡터 $x$의 결합확률분포함수는 개별 스칼라 원소 $x_{d}$의 확률분포함수의 곱이 된다.</li></ul><script type="math/tex; mode=display">P(x_1, \ldots, x_D \mid y = k) = \prod_{d=1}^D P(x_d \mid y = k)</script><ul><li>스칼라 원소 $x_{d}$의 확률분포함수는 결합확률분포함수보다 추정하기 훨씬 쉽다. 가능도함수를 추정한 후에는 베이즈 정리를 사용하며 조건부확률을 계산할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} P(y = k \mid x) &= \dfrac{ P(x_1, \ldots, x_D \mid y = k) P(y = k) }{P(x)} \\ &= \dfrac{ \left(\prod_{d=1}^D P(x_{d} \mid y = k) \right) P(y = k) }{P(x)} \end{align}</script><h4 id="가우시안-분포-정규분포-가능도-모형"><a href="#가우시안-분포-정규분포-가능도-모형" class="headerlink" title="가우시안 분포(정규분포) 가능도 모형"></a>가우시안 분포(정규분포) 가능도 모형</h4><ul><li>$x$벡터의 원소가 모두 실수이고 클래스마다 특정한 값 주변에서 발생한다고 하면 가능도 분포로 정규분포를 사용한다. 각 독립변수 $x_{d}$마다, 그리고 클래스 $k$마다 정규 분포의 기대값 $\mu_{d,k}$, 표준편차 $\sigma^{2}_{d,k}$가 달라진다. QDA모형과는 달리 모든 독립변수들이 서로 조건부독립이라고 가정한다.</li></ul><script type="math/tex; mode=display">P(x_d \mid y = k) = \dfrac{1}{\sqrt{2\pi\sigma_{d,k}^2}} \exp \left(-\dfrac{(x_d-\mu_{d,k})^2}{2\sigma_{d,k}^2}\right)</script><p><img src="/image/gaussian_dist_likelihood_model.png" alt="정규분포 가능도 추정 방법"></p><h4 id="베르누이-분포-가능도-모형"><a href="#베르누이-분포-가능도-모형" class="headerlink" title="베르누이 분포 가능도 모형"></a>베르누이 분포 가능도 모형</h4><ul><li><p>베이누이분포 가능도 모형에서는 각각의 $x = (x_1,\ldots, x_D)$의 각 원소 $x_{d}$가 0 또는 1이라는 값만을 가질 수 있다. 독립변수는 $D$개의 독립적인 베르누이 확률변수, 동전으로 구성된 동전 세트로 표현할 수 있다. 이 동전들의 모수 $\mu_{d}$는 동전 $d$마다 다르다.</p></li><li><p>그런데 class $y=k (k = 1,\ldots, K)$마다도 $x_{d}$가 1이 될 확률이 다르다. 즉, 동전의 모수 $\mu_{d,k}$는 동전 $d$마다 다르고 class $k$마다도 다르다. 즉, 전체 $ D \times K $의 조합의 동전이 존재하며 같은 class에 속하는 D개의 동전이 하나의 동전 세트를 구성하고 이러한 동전 세트가 $K$개 있다고 생각할 수 있다.</p></li></ul><script type="math/tex; mode=display">P(x_d \mid y = k) = \mu_{d,k}^{x_d} (1-\mu_{d,k})^{(1-x_d)}</script><script type="math/tex; mode=display">P(x_1, \ldots, x_D \mid y = k)= \prod_{d=1}^D \mu_{d,k}^{x_d} (1-\mu_{d,k})^{(1-x_d)}</script><ul><li>이러한 동전 세트마다 확률 특성이 다르므로 베르누이분포 가능도 모형을 기반으로 하는 나이브베이즈 모형은 동전 세트를 $N$번 던진 결과로부터 $1, \ldots, K$ 중 어느 동전 세트를 던졌는지를 찾아내는 모형이라고 할 수 있다.</li></ul><h4 id="다항분포-Multinomial-Distribution-가능도-모형"><a href="#다항분포-Multinomial-Distribution-가능도-모형" class="headerlink" title="다항분포(Multinomial Distribution) 가능도 모형"></a>다항분포(Multinomial Distribution) 가능도 모형</h4><ul><li>다항분포 모형에서는 $x$ 벡터가 다항분포의 표본이라고 가정한다. 즉, $D$개의 면을 가지는 주사위를 $\sum_{d=1}^D x_d$ 번 던져서 나온 결과로 본다. 예를 들어 $x$가 다음과 같다면, $x=(1, 4, 0, 5)$ 4면체 주사위를 10번 던져서 1인 면이 1번, 2인 면이 4번, 4인 면이 5번 나온 결과로 해석한다. 각 class 마다 주사위가 다르다고 가정하므로 $K$개의 class를 구분하는 문제에서는 $D$개의 면을 가진 주사위가 $K$개 있다고 본다.</li></ul><script type="math/tex; mode=display">P(x_1, \ldots, x_D \mid y = k) \;\; \propto \;\; \prod_{d=1}^D \mu_{d,k}^{x_{d,k}}</script><script type="math/tex; mode=display">\sum_{d=1}^{D} \mu_{d,k} = 1</script><ul><li>따라서 다항분포 가능도 모형을 기반으로 하는 나이브 베이즈 모형은 주사위를 던진 결과로부터 $1, \ldots, K$ 중 어느 주사위를 던졌는지를 찾아내는 모형이라고 할 수 있다.</li></ul><h2 id="Naive-Bayes-실습"><a href="#Naive-Bayes-실습" class="headerlink" title="Naive Bayes 실습"></a>Naive Bayes 실습</h2><h3 id="1-Gaussian-Naive-Bayes"><a href="#1-Gaussian-Naive-Bayes" class="headerlink" title="1. Gaussian Naive Bayes"></a>1. Gaussian Naive Bayes</h3><ul><li>데이터, 모듈 불러오기<ul><li>iris(붓꽃)데이터를 가지고 실습을 진행할 것이다.</li><li>붓꽃의 종류는 3가지(‘setosa’, ‘versicolor’, ‘virginica’)이며, 각 feature는 sepal length (cm),    sepal width (cm), petal length (cm), petal width (cm) 로 이루어져있다. 모든 피처가 실수의 값을 갖기 때문에 가우시안 나이브 베이즈 모형을 사용할 것이다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">df_X=pd.DataFrame(iris.data, columns=iris.feature_names)</span><br><span class="line">df_Y=pd.DataFrame(iris.target, columns=[<span class="string">"target"</span>])</span><br></pre></td></tr></table></figure><ul><li>예측의 성능은 항상 테스트 데이터를 통해 측정해야 하므로 분리해준다. 지금은 연습삼아 해보는 것이므로 필자는 따로 validation set을 구축하지 않겠다. 허나 실제로 데이터를 분석하고 그에 따른 성능을 측정하려면 꼭 validation set을 따로 두어 베이스라인 모델의 parameter들을 튜닝을 통해 모델의 성능을 높이도록 한 뒤 최종적으로 test set을 예측하여 성능을 검증해야 할 것이다.  </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, test_X, train_Y, test_Y = train_test_split(df_X, df_Y, train_size=0.8, test_size=0.2, random_state=123)</span><br></pre></td></tr></table></figure><ul><li>모델 피팅</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GNB = GaussianNB()</span><br><span class="line">fitted = GNB.fit(train_X, train_Y)</span><br><span class="line">y_pred = fitted.predict(test_X)</span><br><span class="line">y_pred</span><br></pre></td></tr></table></figure><ul><li>확률 구하기<ul><li>먼저 예측한 클래스와 해당 예측 데이터의 클래스별 확률을 살펴 볼 것이다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.predict(test_X)[:1]</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.predict_proba(test_X)[:1]</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[7.24143720e-126, 9.23061979e-001, 7.69380215e-002]])</span><br></pre></td></tr></table></figure><ul><li>위의 확률 직접 구해보기</li></ul><p>1) 위의 확률 값이 나오게 된 중간과정을 살펴보자. 우선 추정된 독립변수의 모수와 정규 분포의 확률밀도 함수를 사용하여 가능도를 구할 수 있다.</p><script type="math/tex; mode=display">p(x_1, x_2, x_3, x_4 | y) \propto p(x_1) p(x_2) p(x_3) p(x_4)</script><pre><code>- 위에서 학습했을 때도 말했던 것 처럼, (class개수 * 변수의 개수)개의 조합의 모수를 갖고 있으므로 아래와 같이 12개의 모수를 갖는다. 아래에서는 각 class별로 정규분포의 모수인 평균과 분산을 보여준다.</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict_data = np.array(test_X.iloc[0])</span><br><span class="line">predict_data</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([6.3, 2.5, 4.9, 1.5])</span><br></pre></td></tr></table></figure><ul><li>추정한 모델의 클래스별 모수(평균과 분산)을 다음과 같이 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.theta_[0], fitted.sigma_[0]</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([5.01621622, 3.43243243, 1.46756757, 0.25945946]),</span><br><span class="line"> array([0.10568298, 0.14975895, 0.02705625, 0.01214025]))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.theta_[1], fitted.sigma_[1]</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([5.95      , 2.78409091, 4.24090909, 1.32272727]),</span><br><span class="line"> array([0.27068182, 0.10042872, 0.22741736, 0.04221075]))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.theta_[2], fitted.sigma_[2]</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([6.58717949, 2.95897436, 5.57948718, 2.02820513]),</span><br><span class="line"> array([0.39752795, 0.11011177, 0.29188692, 0.0774096 ]))</span><br></pre></td></tr></table></figure><ul><li>위의 모수들을 통해 class별 가능도를 구하면 아래와 같을 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">likelihood = [</span><br><span class="line">(sp.stats.norm(fitted.theta_[0][0], np.sqrt(fitted.sigma_[0][0])).pdf(predict_data[0]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[0][1], np.sqrt(fitted.sigma_[0][1])).pdf(predict_data[1]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[0][2], np.sqrt(fitted.sigma_[0][2])).pdf(predict_data[2]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[0][3], np.sqrt(fitted.sigma_[0][3])).pdf(predict_data[3])),\</span><br><span class="line">(sp.stats.norm(fitted.theta_[1][0], np.sqrt(fitted.sigma_[1][0])).pdf(predict_data[0]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[1][1], np.sqrt(fitted.sigma_[1][1])).pdf(predict_data[1]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[1][2], np.sqrt(fitted.sigma_[1][2])).pdf(predict_data[2]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[1][3], np.sqrt(fitted.sigma_[1][3])).pdf(predict_data[3])),\</span><br><span class="line">(sp.stats.norm(fitted.theta_[2][0], np.sqrt(fitted.sigma_[0][0])).pdf(predict_data[0]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[2][1], np.sqrt(fitted.sigma_[0][1])).pdf(predict_data[1]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[2][2], np.sqrt(fitted.sigma_[0][2])).pdf(predict_data[2]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[2][3], np.sqrt(fitted.sigma_[0][3])).pdf(predict_data[3]))    </span><br><span class="line">]</span><br><span class="line">likelihood</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2.0700298536453225e-126, 0.2218869448618605, 7.497361843154609e-09]</span><br></pre></td></tr></table></figure><ul><li>여기에 사전확률을 곱하면 사후 확률에 비례하는 값이 나온다.</li></ul><script type="math/tex; mode=display">p(y|x_1, x_2) \propto p(x_1, x_2|y) p(y)</script><blockquote><p>아직 정규화 상수 $p(x)$로 나누어주지 않았으므로 두 값의 합이 1이 아니다. 즉, 확률이라고 부를 수는 없다. 하지만 크기를 비교하면 이 데이터는 $y=1$일 확률이 가장 높다는 것을 알 수 있다.</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.class_prior_</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.30833333, 0.36666667, 0.325     ])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">posterior = likelihood * fitted.class_prior_</span><br><span class="line">posterior</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([6.38259205e-127, 8.13585464e-002, 2.43664260e-009])</span><br></pre></td></tr></table></figure><ul><li>이 값을 정규화하면 predict_proba 메서드로 구한 것과 같은 값이 나온다. 물론 완벽히 일치하진 않지만 그에 근사하는 값을 추정값으로 계산해내서 얻을 수 있다. 이는 계산시 소수점을 어느정도까지 사용하였는지에 따라 다르기 때문에 이 정도의 오차는 문제가 없다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">posterior / np.sum(posterior, axis=0)</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([7.84501707e-126, 9.99999970e-001, 2.99494353e-008])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.predict_proba(test_X)[:1]</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[7.24143720e-126, 9.23061979e-001, 7.69380215e-002]])</span><br></pre></td></tr></table></figure><ul><li>Confusion matrix 구하기</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line">confusion_matrix(y_pred, test_Y)</span><br></pre></td></tr></table></figure><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[13,  0,  0],</span><br><span class="line">       [ 0,  6,  1],</span><br><span class="line">       [ 0,  0, 10]])</span><br></pre></td></tr></table></figure><ul><li>Prior 설정하기<ul><li>이번에는 class가 발생되는 사전확률을 미리 알고 있었던 경우라고 가정하고 문제를 풀어볼 것이다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GNB2 = GaussianNB(priors=[0.01, 0.01, 0.98])</span><br><span class="line">set_prior_fitted_01 = GNB2.fit(train_X, train_Y)</span><br><span class="line">set_prior_pred_01 = set_prior_fitted_01.predict(test_X)</span><br><span class="line">confusion_matrix(set_prior_pred_01, test_Y)</span><br></pre></td></tr></table></figure><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[13,  0,  0],</span><br><span class="line">       [ 0,  4,  0],</span><br><span class="line">       [ 0,  2, 11]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GNB3 = GaussianNB(priors=[0.01, 0.98, 0.01])</span><br><span class="line">set_prior_fitted_02 = GNB3.fit(train_X, train_Y)</span><br><span class="line">set_prior_pred_02 = set_prior_fitted_02.predict(test_X)</span><br><span class="line">confusion_matrix(set_prior_pred_02, test_Y)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[13,  0,  0],</span><br><span class="line">       [ 0,  6,  4],</span><br><span class="line">       [ 0,  0,  7]])</span><br></pre></td></tr></table></figure><h3 id="2-Bernoulli-naive-bayes"><a href="#2-Bernoulli-naive-bayes" class="headerlink" title="2. Bernoulli naive bayes"></a>2. Bernoulli naive bayes</h3><ul><li>e-mail과 같은 문서 내에 특정한 단어가 포함되어 있는지의 여부는 베르누이 확률변수로 모형화할 수 있다. 이렇게 <code>독립변수가 0 또는 1의 값을 가지면 베르누이 나이브베이즈 모형을 사용</code>한다.</li></ul><ul><li><p>python의 sklearn의 베르누이분포 나이브베이즈 모형 클래스 <code>BernoulliNB</code>는 가능도 추정과 관련하여 다음 속성을 가진다.</p><ul><li><code>feature_count_</code> : 각 class k에 대해 d번째 동전이 앞면이 나온 횟수 $N_{d,k}$</li><li><code>feature_log_prob_</code> : 베르누이분포 모수의 로그값</li></ul></li></ul><script type="math/tex; mode=display">\log \mu_k = (\log \mu_{1,k}, \ldots, \log \mu_{D, k}) = \left( \log \dfrac{N_{1,k}}{N_k}, \ldots, \log \dfrac{N_{D,k}}{N_k} \right)</script><ul><li>여기에서 $N_{k}$은 클래스 k에 대해 동전을 던진 총 횟수이다. <code>표본 데이터의 수가 적은 경우에는 모수에 대해 다음처럼 스무딩(smoothing)을 할 수도 있다.</code></li></ul><h4 id="스무딩-Smoothing"><a href="#스무딩-Smoothing" class="headerlink" title="스무딩(Smoothing)"></a>스무딩(Smoothing)</h4><ul><li>표본 데이터의 수가 적은 경우에는 베르누이 모수가 0 또는 1이라는 극단적인 모수 추정값이 나올 수도 있다. 하지만 현실적으로는 실제 모수값이 이런 극단적인 값이 나올 가능성이 적다. <code>따라서 베르누이 모수가 0.5인 가장 일반적인 경우를 가정하여 0이 나오는 경우와 1이 나오는 경우, 두 개의 가상 표본 데이터를 추가</code>한다. 그러면 0이나 1과 같은 극단적인 추정값이 0.5에 가까운 다음과 같은 값으로 변한다. 이를 <code>라플라스 스무딩(Laplace smoothing)</code> 또는 <code>애드원(Add-One) 스무딩</code>이라고 한다.</li></ul><script type="math/tex; mode=display">\hat{\mu}_{d,k} = \frac{ N_{d,k} + \alpha}{N_k + 2 \alpha}</script><ul><li><p>가중치 $\alpha$를 사용하여 스무딩의 정도를 조절할 수도 있다. 가중치 $\alpha$는 정수가 아니라도 괜찮다. 가중치가 1인 경우는 무정보 사전확률을 사용한 베이즈 모수추정의 결과와 같다.</p></li><li><p>아래의 데이터는 4개의 key word를 사용하여 정상 메일 4개와 spam 메일 6개를 BOW 인코딩한 행렬이다. 예를 들어 첫번째 메일은 정상 메일이고 1번, 4번 key word는 포함하지 않지만 2번,3번 key word를 포함한다고 볼 수 있다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.naive_bayes import BernoulliNB</span><br><span class="line">X = np.array([</span><br><span class="line">    [0, 1, 1, 0],</span><br><span class="line">    [1, 1, 1, 1],</span><br><span class="line">    [1, 1, 1, 0],</span><br><span class="line">    [0, 1, 0, 0],</span><br><span class="line">    [0, 0, 0, 1],</span><br><span class="line">    [0, 1, 1, 0],</span><br><span class="line">    [0, 1, 1, 1],</span><br><span class="line">    [1, 0, 1, 0],</span><br><span class="line">    [1, 0, 1, 1],</span><br><span class="line">    [0, 1, 1, 0]])</span><br><span class="line">y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])</span><br><span class="line"></span><br><span class="line">BNB = BernoulliNB()</span><br><span class="line">fitted = BNB.fit(X, y)</span><br></pre></td></tr></table></figure><ul><li>y 클래스의 종류와 각 클래스에 속하는 표본의 수, 그리고 그 값으로부터 구한 사전 확률의 값은 다음과 같다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.classes_</span><br></pre></td></tr></table></figure><h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0, 1])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.class_count_</span><br></pre></td></tr></table></figure><h5 id="결과-14"><a href="#결과-14" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([4., 6.])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.exp(fitted.class_log_prior_)</span><br></pre></td></tr></table></figure><h5 id="결과-15"><a href="#결과-15" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.4, 0.6])</span><br></pre></td></tr></table></figure><ul><li>각 클래스 k 별로, 그리고 각 독립변수 d 별로, 각각 다른 베르누이 확률변수라고 가정하여 모두 8개의 베르누이 확률변수의 모수를 구하면 다음과 같다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 행은 클래스의 개수, 열은 변수의 개수를 의미</span></span><br><span class="line">count = fitted.feature_count_</span><br><span class="line">count</span><br></pre></td></tr></table></figure><h5 id="결과-16"><a href="#결과-16" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[2., 4., 3., 1.],</span><br><span class="line">       [2., 3., 5., 3.]])</span><br></pre></td></tr></table></figure><ul><li>위의 각 변수의 클래스별로 몇번 나온지에 대한 행렬에 각 클래스의 전체 개수로 나누어 주어야 해당 변수들의 모수인 p값을 알 수 있으므로 class_count의 배열의 모양을 변형시켜주어야 한다. 현재는 1차원의 벡터(2,)이므로 2차원의 (2,1)의 모양을 갖도록 해주어야 나누어 줄 수 있기 때문이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count / fitted.class_count_[:,np.newaxis]</span><br></pre></td></tr></table></figure><h5 id="결과-17"><a href="#결과-17" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.5       , 1.        , 0.75      , 0.25      ],</span><br><span class="line">       [0.33333333, 0.5       , 0.83333333, 0.5       ]])</span><br></pre></td></tr></table></figure><ul><li>위에서는 필자는 Numpy의 brodcasting 연산을 사용하여 구한것인데, 혹시 count 행렬의 모양과 동일하게 만들어 확실하게 연산하고 싶다면, 아래와 같이 실행하면 동일한 결과를 얻을 수 있는 것을 볼 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count / np.repeat(fitted.class_count_[:,np.newaxis], 4, axis=1)</span><br></pre></td></tr></table></figure><h5 id="결과-18"><a href="#결과-18" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.5       , 1.        , 0.75      , 0.25      ],</span><br><span class="line">       [0.33333333, 0.5       , 0.83333333, 0.5       ]])</span><br></pre></td></tr></table></figure><ul><li>그런데 이 값은 모형 내에서 구한 값과 다르다. 모형 내에서 스무딩(smoothing)이 이루어지기 때문이다. 스무딩은 동전의 각 면 즉, 0과 1이 나오는 가상의 데이터를 추가함으로서 추정한 모수의 값이 좀 더 0.5에 가까워지도록 하는 방법이다. 이 때 사용한 스무딩 가중치 값은 다음처럼 확인할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.alpha</span><br></pre></td></tr></table></figure><h5 id="결과-19"><a href="#결과-19" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta = np.exp(fitted.feature_log_prob_)</span><br><span class="line">theta</span><br></pre></td></tr></table></figure><h5 id="결과-20"><a href="#결과-20" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.5       , 0.83333333, 0.66666667, 0.33333333],</span><br><span class="line">       [0.375     , 0.5       , 0.75      , 0.5       ]])</span><br></pre></td></tr></table></figure><ul><li>이에 모형이 완성되었으니 테스트 데이터를 사용하여 예측을 해 본다. 예를 들어 1번, 2번 키워드를 포함한 메일이 정상 메일인지 스팸 메일인지 알아보자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = np.array([0, 1, 1, 1])</span><br><span class="line">fitted.predict_proba([x_new])</span><br></pre></td></tr></table></figure><h5 id="결과-21"><a href="#결과-21" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.34501348, 0.65498652]])</span><br></pre></td></tr></table></figure><ul><li>위 결과에서 정상 메일일 가능성이 약 3배임을 알 수 있다. 이 값은 다음처럼 구할 수도 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># np.prod(axis=1)로 해준 이유는 아래 각 변수별로 독립이므로 가능도를 구하려면 곱을 해주어야 하기 때문</span></span><br><span class="line">p = ((theta ** x_new) * (1 - theta) ** (1 - x_new)).prod(axis=1) * np.exp(fitted.class_log_prior_)</span><br><span class="line">p / p.sum()</span><br></pre></td></tr></table></figure><h5 id="결과-22"><a href="#결과-22" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.34501348, 0.65498652])</span><br></pre></td></tr></table></figure><ul><li>반대로 3번, 4번 keyword가 포함된 메일은 스팸일 가능성이 약 90%이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = np.array([0, 0, 1, 1])</span><br><span class="line">fitted.predict_proba([x_new])</span><br></pre></td></tr></table></figure><h5 id="결과-23"><a href="#결과-23" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.09530901, 0.90469099]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = ((theta ** x_new) * (1 - theta) ** (1 - x_new)).prod(axis=1) * np.exp(fitted.class_log_prior_)</span><br><span class="line">p / p.sum()</span><br></pre></td></tr></table></figure><h5 id="결과-24"><a href="#결과-24" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.09530901, 0.90469099]])</span><br></pre></td></tr></table></figure><ul><li>MNIST 숫자 분류문제에서 sklearn.preprocessing.Binarizer로 x값을 0, 1로 바꾼다(값이 8 이상이면 1, 8 미만이면 0). 즉 흰색과 검은색 픽셀로만 구성된 이미지로 만든다(다음 코드 참조)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">digits = load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line">X = Binarizer(7).fit_transform(X)</span><br></pre></td></tr></table></figure><ul><li><p>이 이미지에 대해 베르누이 나이브베이즈 모형을 적용하자. 분류 결과를 분류보고서 형식으로 나타내라.</p><ul><li><p>(1) BernoulliNB 클래스의 binarize 인수를 사용하여 같은 문제를 풀어본다.</p></li><li><p>(2) 계산된 모형의 모수 벡터 값을 각 클래스별로 8x8 이미지의 형태로 나타내라. 이 이미지는 무엇을 뜻하는가?</p></li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">digits.images[0]</span><br></pre></td></tr></table></figure><h5 id="결과-25"><a href="#결과-25" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],</span><br><span class="line">       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],</span><br><span class="line">       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],</span><br><span class="line">       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],</span><br><span class="line">       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],</span><br><span class="line">       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],</span><br><span class="line">       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape</span><br></pre></td></tr></table></figure><h5 id="결과-26"><a href="#결과-26" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(1797, 64)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BNB =  BernoulliNB()</span><br><span class="line">fitted = BNB.fit(X, y)</span><br><span class="line">theta = np.exp(fitted.feature_log_prob_)</span><br><span class="line">theta = theta.reshape((10, 8, 8))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(2, 5, figsize=(12, 3),</span><br><span class="line">                         subplot_kw=&#123;<span class="string">'xticks'</span>: [], <span class="string">'yticks'</span>: []&#125;)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(5):</span><br><span class="line">    axes[0][i].set_title(<span class="string">"class &#123;&#125;"</span>.format(i))</span><br><span class="line">    axes[0][i].imshow(theta[i], interpolation=<span class="string">'nearest'</span>, cmap=plt.cm.Blues)</span><br><span class="line">    axes[1][i].set_title(<span class="string">"class &#123;&#125;"</span>.format(i+5))</span><br><span class="line">    axes[1][i].imshow(theta[i+5], interpolation=<span class="string">'nearest'</span>, cmap=plt.cm.Blues)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/parameter_vector_of_MNIST_visualization.png" alt="MNIST 베르누이 나이브 베이즈 모형 모수벡터 시각화"></p><ul><li>위의 이미지에서는 모수값이 높은 변수가 진한 파란색을 띄게 된다. sklearn.preprocessing.Binarizer를 통해 x값을 값이 8 이상이면 1, 8 미만이면 0으로 바꾸어주었으므로 <code>8 미만인 데이터보다는 8이상인 데이터가 각 클래스를 구분하는데 좀 더 영향을 주는 공간을 알 수 있게 해준다.</code></li></ul><h3 id="3-Multinomial-naive-bayes"><a href="#3-Multinomial-naive-bayes" class="headerlink" title="3. Multinomial naive bayes"></a>3. Multinomial naive bayes</h3><ul><li>다항분포 나이브베이즈 모형 클래스 <code>MultinomialNB</code>는 가능도 추정과 관련하여 다음 속성을 가진다.<ul><li><code>feature_count_</code> : 각 클래스 $k$에서 $d$번째 면이 나온 횟수 $N_{d,k}$</li><li><code>feature_log_prob_</code> : 다항분포의 모수의 로그</li></ul></li></ul><script type="math/tex; mode=display">\log \mu_k = (\log \mu_{1,k}, \ldots, \log \mu_{D, k}) = \left( \log \dfrac{N_{1,k}}{N_k}, \ldots, \log \dfrac{N_{D,k}}{N_k} \right)</script><ul><li>여기에서 $N_{k}$은 클래스 $k$에 대해 주사위를 던진 총 회수를 뜻한다.</li></ul><ul><li>스무딩공식은 아래와 같다.</li></ul><script type="math/tex; mode=display">\hat{\mu}_{d,k} = \frac{ N_{d,k} + \alpha}{N_k + D \alpha} , \; (D=변수의 개수)</script><ul><li>이번에도 스팸 메일 필터링을 예로 들어보다. 다만 BOW 인코딩을 할 때, 각 키워드가 출현한 빈도를 직접 입력 변수로 사용한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([</span><br><span class="line">    [3, 4, 1, 2],</span><br><span class="line">    [3, 5, 1, 1],</span><br><span class="line">    [3, 3, 0, 4],</span><br><span class="line">    [3, 4, 1, 2],</span><br><span class="line">    [1, 2, 1, 4],</span><br><span class="line">    [0, 0, 5, 3],</span><br><span class="line">    [1, 2, 4, 1],</span><br><span class="line">    [1, 1, 4, 2],</span><br><span class="line">    [0, 1, 2, 5],</span><br><span class="line">    [2, 1, 2, 3]])</span><br><span class="line">y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">MNB = MultinomialNB()</span><br><span class="line">fitted = MNB.fit(X, y)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.classes_</span><br></pre></td></tr></table></figure><h5 id="결과-27"><a href="#결과-27" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0, 1])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.class_count_</span><br></pre></td></tr></table></figure><h5 id="결과-28"><a href="#결과-28" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([4., 6.])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.exp(fitted.class_log_prior_)</span><br></pre></td></tr></table></figure><h5 id="결과-29"><a href="#결과-29" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.4, 0.6])</span><br></pre></td></tr></table></figure><ul><li>다음으로 각 클래스에 대한 가능도 확률분포를 구한다. 다항분포 모형을 사용하므로 각 클래스를 4개의 면을 가진 주사위로 생각할 수 있다. 그리고 각 면이 나올 확률은 각 면이 나온 횟수를 주사위를 던진 전체 횟수로 나누면 된다. 우선 각 클래스 별로 각각의 면이 나온 횟수는 다음과 같다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">count = fitted.feature_count_</span><br><span class="line">count</span><br></pre></td></tr></table></figure><h5 id="결과-30"><a href="#결과-30" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[12., 16.,  3.,  9.],</span><br><span class="line">       [ 5.,  7., 18., 18.]])</span><br></pre></td></tr></table></figure><ul><li>이 데이터에서 클래스 $Y=0$인 주사위를 던진 횟수는 첫번째 행의 값의 합인 40이므로 클래스 $Y=0$인 주사위를 던져 1이라는 면이 나올 확률은 다음처럼 계산할 수 있다.</li></ul><script type="math/tex; mode=display">\mu_{1,Y=0} = \dfrac{12}{40} = 0.3</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count / np.repeat(count.sum(axis=1)[:,np.newaxis], 4, axis=1)</span><br></pre></td></tr></table></figure><h5 id="결과-31"><a href="#결과-31" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.3       , 0.4       , 0.075     , 0.225     ],</span><br><span class="line">       [0.10416667, 0.14583333, 0.375     , 0.375     ]])</span><br></pre></td></tr></table></figure><ul><li>실제로는 극단적인 추정을 피하기 위해 이 값을 가중치 1인 스무딩을 한 추정값을 사용한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.alpha</span><br></pre></td></tr></table></figure><h5 id="결과-32"><a href="#결과-32" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(count + fitted.alpha) / \</span><br><span class="line">(np.repeat(count.sum(axis=1)[:,np.newaxis], 4, axis=1) + fitted.alpha * X.shape[1])</span><br></pre></td></tr></table></figure><h5 id="결과-33"><a href="#결과-33" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.29545455, 0.38636364, 0.09090909, 0.22727273],</span><br><span class="line">       [0.11538462, 0.15384615, 0.36538462, 0.36538462]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta = np.exp(fitted.feature_log_prob_)</span><br><span class="line">theta</span><br></pre></td></tr></table></figure><h5 id="결과-34"><a href="#결과-34" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.29545455, 0.38636364, 0.09090909, 0.22727273],</span><br><span class="line">       [0.11538462, 0.15384615, 0.36538462, 0.36538462]])</span><br></pre></td></tr></table></figure><ul><li>이제 이 값을 사용하여 예측을 해 보자. 만약 어떤 메일에 1번부터 4번까지의 키워드가 각각 10번씩 나왔다면 다음처럼 확률을 구할 수 있다. 구해진 확률로부터 이 메일이 스팸임을 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = np.array([10, 10, 10, 10])</span><br><span class="line">fitted.predict_proba([x_new])</span><br></pre></td></tr></table></figure><h5 id="결과-35"><a href="#결과-35" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.38848858, 0.61151142]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = (theta ** x_new).prod(axis=1) * np.exp(fitted.class_log_prior_)</span><br><span class="line">p / p.sum(axis=0)</span><br></pre></td></tr></table></figure><h5 id="결과-36"><a href="#결과-36" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.38848858, 0.61151142])</span><br></pre></td></tr></table></figure><ul><li>MNIST 숫자 분류문제를 다항분포 나이브베이즈 모형을 사용하여 풀고 이진화(Binarizing)를 하여 베르누이 나이브베이즈 모형을 적용했을 경우와 성능을 비교하라.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line">digits = load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.3, random_state=123)</span><br><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line">binary_train_X = Binarizer(7).fit_transform(train_X)</span><br><span class="line">binary_test_X = Binarizer(7).fit_transform(test_X)</span><br><span class="line">BNB = BernoulliNB().fit(binary_train_X, train_Y)</span><br><span class="line">bnb_pred = BNB.predict(binary_test_X)</span><br><span class="line">MNB = MultinomialNB().fit(train_X, train_Y)</span><br><span class="line">mnb_pred = MNB.predict(test_X)</span><br></pre></td></tr></table></figure><h5 id="이진화-한-베르누이-나이브베이즈-모형-성능"><a href="#이진화-한-베르누이-나이브베이즈-모형-성능" class="headerlink" title="이진화 한 베르누이 나이브베이즈 모형 성능"></a>이진화 한 베르누이 나이브베이즈 모형 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(bnb_pred, test_Y))</span><br></pre></td></tr></table></figure><h5 id="결과-37"><a href="#결과-37" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">            precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.98      0.98      0.98        59</span><br><span class="line">           1       0.82      0.84      0.83        55</span><br><span class="line">           2       0.87      0.90      0.88        51</span><br><span class="line">           3       0.80      0.93      0.86        40</span><br><span class="line">           4       0.95      0.97      0.96        60</span><br><span class="line">           5       0.84      0.94      0.89        51</span><br><span class="line">           6       0.96      1.00      0.98        55</span><br><span class="line">           7       1.00      0.89      0.94        56</span><br><span class="line">           8       0.85      0.80      0.83        51</span><br><span class="line">           9       0.87      0.74      0.80        62</span><br><span class="line"></span><br><span class="line">    accuracy                           0.90       540</span><br><span class="line">   macro avg       0.90      0.90      0.90       540</span><br><span class="line">weighted avg       0.90      0.90      0.90       540</span><br></pre></td></tr></table></figure><h5 id="다항-분포-나이브-베이즈-모형-성능"><a href="#다항-분포-나이브-베이즈-모형-성능" class="headerlink" title="다항 분포 나이브 베이즈 모형 성능"></a>다항 분포 나이브 베이즈 모형 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(mnb_pred, test_Y))</span><br></pre></td></tr></table></figure><h5 id="결과-38"><a href="#결과-38" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.98      1.00      0.99        58</span><br><span class="line">           1       0.77      0.88      0.82        49</span><br><span class="line">           2       0.83      0.88      0.85        50</span><br><span class="line">           3       0.89      1.00      0.94        41</span><br><span class="line">           4       0.97      0.92      0.94        64</span><br><span class="line">           5       0.74      1.00      0.85        42</span><br><span class="line">           6       0.98      0.98      0.98        57</span><br><span class="line">           7       1.00      0.89      0.94        56</span><br><span class="line">           8       0.81      0.74      0.77        53</span><br><span class="line">           9       0.89      0.67      0.76        70</span><br><span class="line"></span><br><span class="line">    accuracy                           0.89       540</span><br><span class="line">   macro avg       0.89      0.90      0.89       540</span><br><span class="line">weighted avg       0.89      0.89      0.89       540</span><br></pre></td></tr></table></figure><ul><li><p>텍스트 분석에서 TF-IDF 인코딩을 하면 단어의 빈도수가 정수가 아닌 실수값이 된다. 이런 경우에도 다항분포 모형을 적용할 수 있는가?</p><ul><li>정수가 아니더라도 해당 적용 가능하다! <code>다항분포의 모수를 추정할 때 해당 관측치의 변수가 갖는 값의 합으로 나누어주어 모수값을 구했는데, TF-idf 행렬은 row가 문서를 의미하고 전체 문서에서의 토큰들이 열을 이루게 되므로 해당 문서에서 어떠한 단어가 몇번 나온것인지에 대해 다항분포를 통해 계산할 수 있기 때문</code>이다.</li></ul></li><li><p><code>MultinomialNB를 사용할 경우 범주형으로 정수이면 사용하는 것이라고 생각하지말고 위의 예시 처럼 데이터 당 각 피처가 유기적으로 하나의 사건에서 파생되어 이루어질 수 있는지에 대해서 먼저 생각해보자. 통계적인 분포를 다항분포로 생각할 수 있는지를 확인해보자는 이야기</code>이다.</p></li><li><p>아래의 뉴스그룹 분류 문제를 통해 검증해보자.</p></li></ul><h4 id="뉴스그룹-분류"><a href="#뉴스그룹-분류" class="headerlink" title="뉴스그룹 분류"></a>뉴스그룹 분류</h4><ul><li>다음은 뉴스그룹 데이터에 대해 나이브베이즈 분류모형을 적용한 결과이다.<ul><li>문서는18846건</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_20newsgroups</span><br><span class="line"></span><br><span class="line">news = fetch_20newsgroups(subset=<span class="string">"all"</span>)</span><br><span class="line">X = news.data</span><br><span class="line">y = news.target</span><br><span class="line"></span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">model1 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, CountVectorizer()),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br><span class="line">model2 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, TfidfVectorizer()),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br><span class="line">model3 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, TfidfVectorizer(stop_words=<span class="string">"english"</span>)),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br><span class="line">model4 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, TfidfVectorizer(stop_words=<span class="string">"english"</span>,</span><br><span class="line">                             token_pattern=r<span class="string">"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b"</span>)),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">from sklearn.model_selection import cross_val_score, KFold</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, model <span class="keyword">in</span> enumerate([model1, model2, model3, model4]):</span><br><span class="line">    scores = cross_val_score(model, X, y, cv=5)</span><br><span class="line">    <span class="built_in">print</span>((<span class="string">"Model&#123;0:d&#125;: Mean score: &#123;1:.3f&#125;"</span>).format(i + 1, np.mean(scores)))</span><br></pre></td></tr></table></figure><h5 id="결과-39"><a href="#결과-39" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Model1: Mean score: 0.855</span><br><span class="line">Model2: Mean score: 0.856</span><br><span class="line">Model3: Mean score: 0.883</span><br><span class="line">Model4: Mean score: 0.888</span><br><span class="line">CPU <span class="built_in">times</span>: user 1min 35s, sys: 4.54 s, total: 1min 40s</span><br><span class="line">Wall time: 1min 53s</span><br></pre></td></tr></table></figure><ul><li>(1) 만약 독립변수로 실수 변수, 0 또는 1 값을 가지는 변수, 자연수 값을 가지는 변수가 섞여있다면 사이킷런에서 제공하는 나이브베이즈 클래스를 사용하여 풀 수 있는가?</li></ul><pre><code>- 위에서 likelihood를 직접 계산했던 것과 같이 `likelihood만을 각각 계산하여 각 변수들은 독립이라는 가정을 전제로하기 때문에 서로 곱한뒤에 베이즈정리식에 따라 최종적으로 확률값을 구해 클래스를 구분`할 수 있다.</code></pre><ul><li>(2) 사이킷런에서 제공하는 분류문제 예제 중 숲의 수종을 예측하는 covtype 분류문제는 연속확률분포 특징과 베르누이확률분포 특징이 섞여있다. 이 문제를 사이킷런에서 제공하는 나이브베이즈 클래스를 사용하여 풀어라.</li></ul><h3 id="대표-수종-데이터-covtype"><a href="#대표-수종-데이터-covtype" class="headerlink" title="대표 수종 데이터(covtype)"></a>대표 수종 데이터(covtype)</h3><ul><li>대표 수종 데이터는 미국 삼림을 30×30m 영역으로 나누어 각 영역의 특징으로부터 대표적인 나무의 종류(species of tree)을 예측하기위한 데이터이다. 수종은 7종류이지만 특징 데이터가 54종류, 표본 데이터의 갯수가 581,012개에 달하는 대규모 데이터이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_covtype</span><br><span class="line">covtype = fetch_covtype()</span><br><span class="line"><span class="comment"># print(covtype.DESCR)</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(covtype.data,</span><br><span class="line">                  columns=[<span class="string">"x&#123;:02d&#125;"</span>.format(i + 1) <span class="keyword">for</span> i <span class="keyword">in</span> range(covtype.data.shape[1])],</span><br><span class="line">                  dtype=int)</span><br><span class="line">sy = pd.Series(covtype.target, dtype=<span class="string">"category"</span>)</span><br><span class="line">df[<span class="string">'covtype'</span>] = sy</span><br></pre></td></tr></table></figure><ul><li>각 특징 데이터가 가지는 값의 종류를 보면 1번부터 10번 특징은 실수값이고 11번부터 54번 특징은 이진 카테고리값이라는 것을 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.iloc[:, 10:54] = df.iloc[:, 10:54].astype(<span class="string">'category'</span>)</span><br></pre></td></tr></table></figure><ul><li>다음 플롯은 카테고리값에 따라 “x14” 특징의 값이 어떻게 변하는지 나타낸 것이다. “x14” 특징이 0인가 1인가를 사용하면 1, 5, 7번 클래스와 4번 클래스는 완벽하게 분류할 수 있다는 것을 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(10,12))</span><br><span class="line">df_count = df.pivot_table(index=<span class="string">"covtype"</span>, columns=<span class="string">"x14"</span>, aggfunc=<span class="string">"size"</span>)</span><br><span class="line">sns.heatmap(df_count, cmap=sns.light_palette(<span class="string">"gray"</span>, as_cmap=True), annot=True, fmt=<span class="string">"0"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/categorical_variable_of_tree_binary.png" alt="x14 피처로 분류할 수 있는 클래스"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Gaussian_df_X = df.iloc[:, :10]</span><br><span class="line">Bern_df_X = df.iloc[:, 10:-1]</span><br><span class="line">df_Y = df.iloc[:, -1]</span><br><span class="line"></span><br><span class="line">GNB = GaussianNB()</span><br><span class="line">fitted_GNB = GNB.fit(Gaussian_df_X, df_Y)</span><br><span class="line"></span><br><span class="line">theta = fitted_GNB.theta_</span><br><span class="line">sigma = fitted_GNB.sigma_</span><br></pre></td></tr></table></figure><ul><li>가능도를 계산하기 위한 함수를 작성하였다.<ul><li>아래 함수는 반복문을 통해 실행하는 방식인데 데이터 수가 많다면 너무 비효율적이다. 그러므로, 데이터(관측치)의 수가 적은 경우에만 이용하는 것을 권장한다.</li><li>그래서 또 직접적으로 for문을 돌리지 않고 사용할 수 있는 방식의 함수를 다시 구현하였다. for문으로 반복문을 작성한것과 비교했을 때는 직관적으로 어떻게 가능도를 구하는 지 알 수 있다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def Gaussian_likelihood_cal(predict_data, theta, sigma, class_count, feature_count):</span><br><span class="line">    likelihood = []</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> np.arange(class_count):</span><br><span class="line">        prod = 1</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> np.arange(feature_count):</span><br><span class="line">            prod = prod * sp.stats.norm(theta[c][f], np.sqrt(sigma[c][f])).pdf(predict_data[f])</span><br><span class="line">        likelihood.append(prod)</span><br><span class="line">    <span class="built_in">return</span> likelihood</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">def Gaussian_likelihood_cal(predict_data, theta, sigma):</span><br><span class="line">    likelihood = [(sp.stats.norm(theta[0][0], np.sqrt(sigma[0][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][1], np.sqrt(sigma[0][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][2], np.sqrt(sigma[0][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][3], np.sqrt(sigma[0][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][4], np.sqrt(sigma[0][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][5], np.sqrt(sigma[0][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][6], np.sqrt(sigma[0][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][7], np.sqrt(sigma[0][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][8], np.sqrt(sigma[0][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][9], np.sqrt(sigma[0][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[1][0], np.sqrt(sigma[1][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][1], np.sqrt(sigma[1][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][2], np.sqrt(sigma[1][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][3], np.sqrt(sigma[1][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][4], np.sqrt(sigma[1][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][5], np.sqrt(sigma[1][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][6], np.sqrt(sigma[1][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][7], np.sqrt(sigma[1][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][8], np.sqrt(sigma[1][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][9], np.sqrt(sigma[1][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[2][0], np.sqrt(sigma[2][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][1], np.sqrt(sigma[2][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][2], np.sqrt(sigma[2][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][3], np.sqrt(sigma[2][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][4], np.sqrt(sigma[2][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][5], np.sqrt(sigma[2][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][6], np.sqrt(sigma[2][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][7], np.sqrt(sigma[2][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][8], np.sqrt(sigma[2][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][9], np.sqrt(sigma[2][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[3][0], np.sqrt(sigma[3][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][1], np.sqrt(sigma[3][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][2], np.sqrt(sigma[3][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][3], np.sqrt(sigma[3][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][4], np.sqrt(sigma[3][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][5], np.sqrt(sigma[3][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][6], np.sqrt(sigma[3][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][7], np.sqrt(sigma[3][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][8], np.sqrt(sigma[3][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][9], np.sqrt(sigma[3][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[4][0], np.sqrt(sigma[4][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][1], np.sqrt(sigma[4][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][2], np.sqrt(sigma[4][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][3], np.sqrt(sigma[4][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][4], np.sqrt(sigma[4][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][5], np.sqrt(sigma[4][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][6], np.sqrt(sigma[4][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][7], np.sqrt(sigma[4][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][8], np.sqrt(sigma[4][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][9], np.sqrt(sigma[4][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[5][0], np.sqrt(sigma[5][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][1], np.sqrt(sigma[5][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][2], np.sqrt(sigma[5][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][3], np.sqrt(sigma[5][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][4], np.sqrt(sigma[5][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][5], np.sqrt(sigma[5][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][6], np.sqrt(sigma[5][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][7], np.sqrt(sigma[5][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][8], np.sqrt(sigma[5][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][9], np.sqrt(sigma[5][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[6][0], np.sqrt(sigma[6][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][1], np.sqrt(sigma[6][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][2], np.sqrt(sigma[6][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][3], np.sqrt(sigma[6][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][4], np.sqrt(sigma[6][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][5], np.sqrt(sigma[6][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][6], np.sqrt(sigma[6][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][7], np.sqrt(sigma[6][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][8], np.sqrt(sigma[6][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][9], np.sqrt(sigma[6][9])).pdf(predict_data[9]))]</span><br><span class="line">    <span class="built_in">return</span> likelihood</span><br></pre></td></tr></table></figure><ul><li>위의 함수를 사용하여 10개의 실수 변수들에 대한 모수를 계산하여 가능도를 구하는 반복문을 작성하였다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">total_num = np.array(Gaussian_df_X).shape[0]</span><br><span class="line">gaussian_likelihood_matrix = []</span><br><span class="line">percentage = 0</span><br><span class="line"><span class="keyword">for</span> num, predict_data <span class="keyword">in</span> enumerate(np.array(Gaussian_df_X)):</span><br><span class="line">    <span class="keyword">if</span> (percentage != int(num / total_num * 100)) and (int(num / total_num * 100) <span class="keyword">in</span> list(np.arange(10,101,10))):</span><br><span class="line">        percentage = int(num / total_num * 100)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"완성도 &#123;&#125; %"</span>.format(percentage))</span><br><span class="line">    likelihood = Gaussian_likelihood_cal(predict_data, theta, sigma)</span><br><span class="line">    gaussian_likelihood_matrix.append(likelihood)</span><br></pre></td></tr></table></figure><ul><li>위에서 가우시안 나이브 베이즈 모형의 가능도를 구했으므로 이젠 베르누이 나이브 베이즈 모형의 가능도를 구할 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BNB = BernoulliNB()</span><br><span class="line">fitted_BNB = BNB.fit(Bern_df_X, df_Y)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta = np.exp(fitted_BNB.feature_log_prob_)</span><br><span class="line">theta.shape</span><br></pre></td></tr></table></figure><ul><li>상대적으로 가우시안 나이브 베이즈 모형의 가능도를 계산하는 것보단 단순 연산으로 이루어져 있어 속도가 훨씬 빠르다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">bern_likelihood_matrix = []</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> np.array(Bern_df_X):</span><br><span class="line">    bern_likelihood_matrix.append(list(((theta ** data) * (1 - theta) ** (1 - data)).prod(axis=1)))</span><br></pre></td></tr></table></figure><h5 id="결과-40"><a href="#결과-40" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CPU <span class="built_in">times</span>: user 49.2 s, sys: 1.28 s, total: 50.5 s</span><br><span class="line">Wall time: 50.4 s</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">likelihood = np.array(gaussian_likelihood_matrix) * np.array(bern_likelihood_matrix)</span><br><span class="line">posterior = likelihood * np.exp(BNB.class_log_prior_)</span><br><span class="line">prob = posterior / np.repeat(posterior.sum(axis=1)[:, np.newaxis], 7, axis=1)</span><br><span class="line">result = np.argmax(prob, axis=1)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result_bern = fitted_BNB.predict(Bern_df_X)</span><br><span class="line">result_gaussian = fitted_GNB.predict(Gaussian_df_X)</span><br></pre></td></tr></table></figure><h5 id="가우시안-나이브-베이즈-모형의-성능"><a href="#가우시안-나이브-베이즈-모형의-성능" class="headerlink" title="가우시안 나이브 베이즈 모형의 성능"></a>가우시안 나이브 베이즈 모형의 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(result_gaussian, df_Y))</span><br></pre></td></tr></table></figure><h5 id="결과-41"><a href="#결과-41" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           1       0.67      0.63      0.65    225973</span><br><span class="line">           2       0.66      0.73      0.69    255934</span><br><span class="line">           3       0.65      0.50      0.56     46785</span><br><span class="line">           4       0.47      0.41      0.44      3099</span><br><span class="line">           5       0.22      0.18      0.20     11425</span><br><span class="line">           6       0.31      0.33      0.32     16424</span><br><span class="line">           7       0.28      0.27      0.28     21372</span><br><span class="line"></span><br><span class="line">    accuracy                           0.63    581012</span><br><span class="line">   macro avg       0.47      0.44      0.45    581012</span><br><span class="line">weighted avg       0.63      0.63      0.63    581012</span><br></pre></td></tr></table></figure><h5 id="베르누이-나이브-베이즈-모형의-성능"><a href="#베르누이-나이브-베이즈-모형의-성능" class="headerlink" title="베르누이 나이브 베이즈 모형의 성능"></a>베르누이 나이브 베이즈 모형의 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(result_bern, df_Y))</span><br></pre></td></tr></table></figure><h5 id="결과-42"><a href="#결과-42" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           1       0.67      0.63      0.65    225973</span><br><span class="line">           2       0.66      0.73      0.69    255934</span><br><span class="line">           3       0.65      0.50      0.56     46785</span><br><span class="line">           4       0.47      0.41      0.44      3099</span><br><span class="line">           5       0.22      0.18      0.20     11425</span><br><span class="line">           6       0.31      0.33      0.32     16424</span><br><span class="line">           7       0.28      0.27      0.28     21372</span><br><span class="line"></span><br><span class="line">    accuracy                           0.63    581012</span><br><span class="line">   macro avg       0.47      0.44      0.45    581012</span><br><span class="line">weighted avg       0.63      0.63      0.63    581012</span><br></pre></td></tr></table></figure><h5 id="가우시안-나이브-베이즈-모형과-베르누이-나이브-베이즈-모형의-가능도를-곱해-확률을-계산한-모형의-성능"><a href="#가우시안-나이브-베이즈-모형과-베르누이-나이브-베이즈-모형의-가능도를-곱해-확률을-계산한-모형의-성능" class="headerlink" title="가우시안 나이브 베이즈 모형과 베르누이 나이브 베이즈 모형의 가능도를 곱해 확률을 계산한 모형의 성능"></a>가우시안 나이브 베이즈 모형과 베르누이 나이브 베이즈 모형의 가능도를 곱해 확률을 계산한 모형의 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(result, df_Y))</span><br></pre></td></tr></table></figure><h5 id="결과-43"><a href="#결과-43" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           1       0.03      0.25      0.06     26481</span><br><span class="line">           2       0.93      0.48      0.63    554531</span><br><span class="line">           3       0.00      0.00      0.00         0</span><br><span class="line">           4       0.00      0.00      0.00         0</span><br><span class="line">           5       0.00      0.00      0.00         0</span><br><span class="line">           6       0.00      0.00      0.00         0</span><br><span class="line">           7       0.00      0.00      0.00         0</span><br><span class="line"></span><br><span class="line">    accuracy                           0.47    581012</span><br><span class="line">   macro avg       0.14      0.10      0.10    581012</span><br><span class="line">weighted avg       0.89      0.47      0.60    581012</span><br></pre></td></tr></table></figure><ul><li>위의 성능 보면 3가지 모형 다 성능이 좋지 않다는 것을 확인 할 수 있다. 이는 적절한 피처의 선택이 이루어지지 않은 모형이기 때문일 것이며, 또한 아래 그림에서와 같이 클래스간의 비율차이가 극심하게 차이가 나는데, 특히 1,2 클래스가 대다수를 이루고 있기 때문에 1, 2클래스에 대한 학습이 많이 된 결과라고 해석 할 수 있을 것이다. 이는 마지막 두 나이브 베이즈 모형의 성능을 보아도 확인 할 수 있다. 마지막 모형의 성능은 다른 클래스로 예측한 데이터는 존재하지 않고 오로지 1과 2로 예측을 했다.</li></ul><p><img src="/image/class_dist_sovya.png" alt="클래스별 분포"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/#disqus_thread</comments>
    </item>
    
    <item>
      <title>PCA를 이해하기 위한 기본적 선형대수</title>
      <link>https://heung-bae-lee.github.io/2020/04/03/machine_learning_06/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/03/machine_learning_06/</guid>
      <pubDate>Fri, 03 Apr 2020 06:40:52 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;차원의-저주&quot;&gt;&lt;a href=&quot;#차원의-저주&quot; class=&quot;headerlink&quot; title=&quot;차원의 저주&quot;&gt;&lt;/a&gt;차원의 저주&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;먼저 PCA를 하는 이유에 대해 설명해 볼 것이다. 전에 언급했던 변수(또는 피처)들이
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="차원의-저주"><a href="#차원의-저주" class="headerlink" title="차원의 저주"></a>차원의 저주</h1><ul><li>먼저 PCA를 하는 이유에 대해 설명해 볼 것이다. 전에 언급했던 변수(또는 피처)들이 많아질수록 변수들이 있는 공간의 차원수 또한 점차적으로 늘어나게 된다. <code>차원의 저주</code>는<code>차원이 늘어남에 따라서 같은 영역의 자료를 갖고 있음에도 전체 영역 대비 모델을 통해 변수로 설명할 수 있는 데이터의 패턴은 줄어들게 되는 것</code>을 의미한다.</li></ul><p><img src="/image/curse_of_dimensionality_01.png" alt="차원의 저주 - 01"></p><ul><li>다른 관점에서 한번 살펴보면, 관측치의 수가 한정되어 있다고 할 경우에 먼저, 1차원 상의 공간(변수가 1개인 경우)에서는 10개의 데이터만 있어도 해당 차원의 절반을 설명할 수 있기에 공간을 설명함에 있어서 부족함이 없다고 판단할 수 있다. 허나 2차원상의 공간으로 살펴보았을 경우 1차원의 경우보다 관측치 간의 간격이 멀게 있어 사이에 빈 공간이 많이 나타남을 알 수 있다. 이렇게 빈 공간이 나타남은 사실상 우리가 모델을 통해 예측은 가능하지만 모형에 따라 변동이 심해지는 어디까지나 예측하는 값을 의미한다. 3차원 상에서는 2차원 공간상에서 보다 훨씬 더 관측치가 떨어져있음을 확인할 수 있다. 그러므로 모델의 변동성 즉, 모델의 분산이 더 커지게 되는 것이다. 간단히 말하자면, 점과 점 사이의 공간이 차원이 늘어날수록 멀어지는데, 빈 공간에 위치한 예측값을 채워넣기 위해서는 임의로 채워넣기 때문에 모델이 어려워진다는 것이다.  </li></ul><p><img src="/image/curse_of_dimensionality_02.png" alt="차원의 저주 - 02"></p><ul><li>예를 들어, 아래와 같이 KNN 알고리즘(가장 가까이 있는 변수가 예측값으로 결과를 갖는 알고리즘)을 통한 비지도학습을 진행한다고 가정해보자. x1 변수의 공간만을 생각하여 1차원적으로 생각하면, 아래 그림에서 검정색 점과 가장 가까운 점은 노란색 테두리안에 있는 2개의 점들 중 하나일 것이다. 허나 x2 변수의 공간도 같이 고려하여 2차원적으로 살펴보면, 파란색 원 안에 놓여있는 1개의 점이 가장 가까운 점이 될 것이다. 여기서 주목해야 할 점은 <code>동일한 데이터를 통해서 2차원으로 살펴보면 최단거리의 길이가 늘어났다는 점</code>이다. <code>KNN 알고리즘의 결과로 1차원으로 살펴본 것 보다 2차원으로 살펴보는 것이 최단거리의 길이가 더 길기 때문에 값이 좀 더 이질적이라고 볼 수 있으며, 결과적으로 과연 2차원적으로 예측한 값을 KNN알고리즘의 결과값으로 사용해도 문제가 없을지에 의문점이 생기게 된다.</code> x2가 Y와 관련된 변수라면 크게 문제가 없겠지만, x2가 전혀 관련없는 변수라면 굉장히 모델이 낭비가되며, 좋지 않은 모델이 될 것이기 때문이다.</li></ul><ul><li>위에서 언급했던 것처럼 차원이 증가하게 되면 좋지 않은 측면이 있고, 이것을 차원의 저주라고 부르며 <code>차원을 될 수 있는 한 축소하는 것이 필요</code>하다.</li></ul><p><img src="/image/necessity_for_reduction_of_dimensionality.png" alt="차원축소의 필요성"></p><ul><li>기본적으로 차원을 축소하는 방법으로 아래와 같은 것들을 예로 들 수 있다. 먼저, 상관계수가 높은 변수들이라면 동일한 움직임의 방향을 갖기 때문에 그들 중 하나의 변수만을 선택해도 큰 문제가 없다고 보는 측면이다. 이 방법에서의 문제점은 예를 들어 두 변수의 상관계수가 0.8이라면, 나머지 0.2에 해당하는 정보는 버려지게 되는 점이다. 이런 부분을 보완한 것이 PCA이다. <code>PCA는 차원을 줄이면서도 정보의 손실을 최소화</code>하는 것이다.</li></ul><p><img src="/image/kinds_of_reduction_of_dimensonality.png" alt="차원축소법의 종류"></p><h1 id="공분산-행렬의-이해"><a href="#공분산-행렬의-이해" class="headerlink" title="공분산 행렬의 이해"></a>공분산 행렬의 이해</h1><ul><li>공분산 행렬은 쉽게 말해 행렬의 내적이라고 할 수 있다. 아래 그림에서 처럼 계산할 수 있다. 대각 요소를 제외한 나머지 비대각요소들은 두 변수간의 움직임의 방향을 알 수 있다. 그러므로 상관관계를 미리 알고 있다면 두 변수간의 공분산의 부호도 알 수 있다.</li></ul><p><img src="/image/covariance_matrix_concept_01.png" alt="공분산 행렬의 이해 - 01"></p><ul><li>공분산 행렬의 활용은 위에서 언급했던 것 처럼 두 변수간의 상관계수의 부호를 알 수 있는 것 말고도 존재한다. 아래 그림과 같이 특정 점들과의 내적연산을 시행하면, 점의 위치를 이동시키는데 이 때 이동된 점들의 분포가 공분산 구조와 비슷한 형태를 가지게 된다. <code>즉 공분산 행렬과 점(벡터 또는 행렬)들의 연산은 공분산 구조로 점들을 분포시키는 기능</code>을 한다.</li></ul><p><img src="/image/covariance_matrix_concept_02.png" alt="공분산 행렬의 이해 - 02"></p><ul><li>공분산 행렬이 대칭행렬이지만, positive definite가 아닌 행렬인 경우를 다음 그림에서 보여주고 있다. 아래 수식이 성립한다면 행렬 A가 positive definite하다고 한다.</li></ul><script type="math/tex; mode=display">x^{T} A  x > 0</script><p><a href="https://datascienceschool.net/view-notebook/d6205659aff0413797c22552947aec83/" target="_blank" rel="noopener">참고 - 행렬의 성질</a></p><p><img src="/image/covariance_matrix_concept_03.png" alt="공분산 행렬의 이해 - 03"></p><ul><li>아래의 공분산 행렬은 행렬식(determinant)가 0인 경우이다. 이렇게 행렬식이 0인 공분산행렬과의 내적은 오른쪽 그림과 같이 원점을 기준으로 일직선의 형태를 이루게 된다.</li></ul><p><img src="/image/covariance_matrix_concept_04.png" alt="공분산 행렬의 이해 - 04"></p><h1 id="Principal-Components의-이해"><a href="#Principal-Components의-이해" class="headerlink" title="Principal Components의 이해"></a>Principal Components의 이해</h1><ul><li>아래 그림을 살펴보면 x1과 x2는 음의 강한 상관관계를 갖는다는 것을 확인 할 수 있다. x1이 증가하면 x2가 감소하는 움직을 갖는다. 결론적으로는 정보가 중복되어있다. 이렇게 중복되어있는 정보를 요약해서 갖고있을수 있을 순 없을까에 대한 방법을 고안한 것이 바로 PCA이다. 아래 그림과 같이 두 변수간의 가장 큰 분산을 나타내는 벡터에 대해 각 데이터를 projection을 취한다면 해당 projection 된 데이터들의 벡터만을 가지고도 기존의 데이터에 대한 분산을 표현할 수 있게 된다.</li></ul><ul><li><code>차원을 줄이면서 정보의 손실을 최소화하는 방법은 자료의 변동을 가장 잘 설명하는 어떤 축을 찾는 것</code>이다.</li></ul><p><img src="/image/PCA_conception_01.png" alt="Principal Components의 개념 - 01"></p><ul><li>Principal Components를 찾아내는 방법을 간단히 말하자면 자료의 분산을 가장 많이 설명하는 축인 장축과 자료의 분산을 가장 적게 설명하는 단축을 찾아내는 것이다.  </li></ul><p><img src="/image/PCA_conception_03.png" alt="Principal Components의 개념 - 02"></p><p><img src="/image/PCA_conception_02.png" alt="Principal Components의 개념 - 03"></p><h1 id="PCA-수학적-개념이해-행렬연산-행렬식-특성방정식"><a href="#PCA-수학적-개념이해-행렬연산-행렬식-특성방정식" class="headerlink" title="PCA 수학적 개념이해 - 행렬연산, 행렬식, 특성방정식"></a>PCA 수학적 개념이해 - 행렬연산, 행렬식, 특성방정식</h1><p><img src="/image/find_determinant.png" alt="행렬식 구하는 방법"></p><ul><li><code>행렬식</code>(determinant)은 쉽게 말해 해당 <code>행렬이 갖는 Volume(2차원인 경우는 면적)</code>을 의미한다. 아래 그림과 같이 2차원 공간상에서 면적이 1을 갖는 좌표들(파란색)을 D라는 행렬로 명명했을 경우, A행렬로 선형변환시켜주면 아래 그림의 빨간색 직사각형으로 변환되며, 부피는 각 행렬식의 곱 형태로 계산 되어 5를 갖는다.</li></ul><ul><li><code>만약 행렬식이 0을 갖는다면</code> 행렬의 곱은 1직선을 이룰 것이고, 선분의 형태로 표현될 것이다.</li></ul><p><img src="/image/deteminant_of_algebra_aspection.png" alt="행렬식의 기하하적인 요소"></p><p><img src="/image/determinant_applications_and_relations.png" alt="행렬식의 활용"></p><h1 id="고유값과-고유벡터"><a href="#고유값과-고유벡터" class="headerlink" title="고유값과 고유벡터"></a>고유값과 고유벡터</h1><ul><li><code>임의의 정방행렬 A로 선형변환하더라도 방향은 유지되면 해당 벡터를 고유벡터라고 하며, 크기(늘어난 정도)를 고유값</code>이라고 한다.</li></ul><p><img src="/image/eigen_value_and_eigen_vector_01.png" alt="고유값과 고유벡터 - 01"></p><ul><li>기하학적으로 설명해보자면, 아래 그림과 같이 고유벡터는 빨간색 선을 제외한 나머지 선형변환을 해도 벡터의 방향이 바뀌지 않는 파란색, 분홍색 벡터들이 고유벡터를 의미한다. 그리고 고유값은 스케일(크기)를 의미한다.</li></ul><p><img src="/image/eigen_value_and_eigen_vector_02.png" alt="고유값과 고유벡터 - 02"></p><ul><li>아래 그림과 같은 식으로 고유값과 고유벡터를 구할 수 있다. 먼저 고유값을 구하는 방법은 $det(A - \lambda I)=0$을 만족하는 고유값을 찾으면 되는데, 이유는 우리가 구하고 싶은 고유값과 고유벡터는 0벡터를 제외한 다른 벡터를 구하고 싶기 때문에 행렬식이 0이아닌 다른 값을 갖는다면 역행렬이 존재하게 되어 0벡터가 해당 선형시스템의 유일한 해가 되지 않도록하려면 non-trivial solution(해가 무수히 많아야)을 갖어야 한다. 그러므로 해당 행렬식이 0의 값을 갖게되어 역행렬이 존재하지 않도록 해야하기 때문에 아래와 같은 수식을 통해 고유값을 구할 수 있다.</li></ul><ul><li>고유값은 크기를 기준으로 내림차순으로(큰 것부터 작은 것 순으로)적어주면, 그에따른 고유벡터도 동일한 인덱스를 매칭해준다. 여기서 주의할 점은 <code>고유벡터는 해당 고유벡터를 normalization을 통해 크기가 1인 벡터를 기준으로 스칼라배를 해도 동일한 고유벡터라고 여긴다는 점</code>이다.</li></ul><p><img src="/image/eigen_value_and_eigen_vector_03.png" alt="고유값과 고유벡터 - 03"></p><h1 id="PCA-수학적-개념이해-Singular-Value-Decomposition-SVD"><a href="#PCA-수학적-개념이해-Singular-Value-Decomposition-SVD" class="headerlink" title="PCA 수학적 개념이해 - Singular Value Decomposition(SVD)"></a>PCA 수학적 개념이해 - Singular Value Decomposition(SVD)</h1><ul><li>PCA는 다르게 행렬식을 계산하기 때문에 정방행렬인 경우에만 계산이 가능하다는 단점이 있다. 이러한 점을 보완하기 위한 Decomposition 방법이 바로 SVD(Singular Value Decomposition)이다.</li></ul><ul><li>SVD는 임의의 행렬 X에 관해 3개의 행렬로 분해가 가능하며, 각각의 행렬의 크기는 아래와 같고, V와 U는 column들이 Orthogonal(직교)한 행렬(서로 직교하는 성분들끼리의 내적은 0)이므로 동일행렬의 내적은 Identity matrix를 갖는다. V와 D가 각각 임의의행렬 X에 공분산행렬에 해당하는 고유벡터 행렬과 고유값 행렬을 의미한다.</li></ul><p><img src="/image/Singular_value_decomposition_01.png" alt="SVD 개념"></p><ul><li>공분산 행렬인 $X^{T}X$를 SVD 한 후, 아래 그림과 같이 eigen vector들의 행렬로 곱해주면 eigen vector들의 행렬과 eigen value들의 내적 값을 구할 수 있다.</li></ul><p><img src="/image/Singular_value_decomposition_02.png" alt="SVD와 eigen vector, eigen value와의 연과성"></p><h1 id="PCA-수행과정-및-수학적-개념-적용"><a href="#PCA-수행과정-및-수학적-개념-적용" class="headerlink" title="PCA 수행과정 및 수학적 개념 적용"></a>PCA 수행과정 및 수학적 개념 적용</h1><ul><li>제일 먼저, 각 데이터에 대해 standardization을 통해 표준정규분포를 따르도록 scaling을 해준다. 이런 normalization은 각 feature에 대한 공간의 범위가 다르다면 비교하는데 어려움이 있기 때문이다. 또한, 이 과정에서 feature 벡터의 크기를 1로 갖게끔 Unit vector로 만들어 주는 것이 추후에 SVD를 계산하는 과정에서 Orthogonal한 eigen vector가 Orthonormal vector가 되어 계산함에 있어서 편리하게 된다.</li></ul><p><img src="/image/PCA_simulation_action_01.png" alt="PCA 수행과정 - 01"></p><ul><li>그 후 SVD는 우선 우리는 프로그램을 사용하여 바로 구할 수 있다는 가정을 해보자. 여기서 <code>eigen vector같은 경우에는 공분산 구조나 공분산 행렬이나 동일한 값을 갖지만, eigen value는 다음 그림에서와 같이 관측치의 개수에서 하나를 빼주어야 한다.</code></li></ul><p><img src="/image/PCA_simulation_action_02.png" alt="PCA 수행과정 - 02"></p><ul><li>PC Score는 위에서와 같이 큰 순서대로 정렬된 eigen value 행렬 D와 앞에 U행렬값의 내적이 곧 행렬 X와 V의 내적을 하여 상수인 $\lambda V$를 만드는 과정과 동일하다.</li></ul><p><img src="/image/PCA_simulation_action_03.png" alt="PCA 수행과정 - 03"></p><ul><li>아래 그림과 같이 PC Score를 구해 중요도를 따져 상위 q개만을 통해 회귀분석을 진행할 수 있다.</li></ul><p><img src="/image/PCA_simulation_action_04.png" alt="PCA 수행과정 - 04"></p><h1 id="PCA의-심화적-이해"><a href="#PCA의-심화적-이해" class="headerlink" title="PCA의 심화적 이해"></a>PCA의 심화적 이해</h1><ul><li><code>한 마디로 PCA는 데이터의 변동을 가장 잘 설명하는 한 축을 찾아서 그 축에 대해 각 관측치들을 projection한 성분을 의미한다. 그 다음에는 이러한 해당 변동축에 직각이 되는 축에 관측치들을 projection하는 과정을 반복하여 진행하게 되는데 직각이 되는 축을 찾는 이유는 이전의 축과 동일한 정보를 갖지않는 새로운 피처를 뽑기 위함이라고 필자는 생각</code>한다.</li></ul><p><img src="/image/what_is_inner_product_of_PC_SCORE.png" alt="PC Score의 본질적인 의미"></p><ul><li>PCA의 <code>단점</code> 중 하나로는 <code>해당 성분이 의미하는 바를 직관적으로 설명할 수 없다</code>는 점이다. 또한 선형회귀에 사용되는 것과 같이 <code>해당 변수들이 비선형관계를 갖는다면 잡아낼 수 없다.</code></li></ul><p><img src="/image/bad_aspection_of_PCA.png" alt="PCA의 단점"></p><ul><li>위에서 언급한 첫번째 단점(직관적으로 PC성분을 통한 피처를 설명하기 어렵다.)을 조금이나마 보완할 수 있는 방법은 grid를 통해 해당하는 곳의 범위들의 데이터를 직접 뽑아 각 성분의 의미를 설명할 수 있다. 아래와 같이 이미지 데이터인 경우는 좀 더 구별하기 쉬울 것이다.</li></ul><p><img src="/image/Complement_the_shortcomings_of_PCA.png" alt="PCA의 단점을 보완하는 방법"></p><h1 id="Kernel-PCA"><a href="#Kernel-PCA" class="headerlink" title="Kernel PCA"></a>Kernel PCA</h1><ul><li>위에서 언급했던 두 번째 단점인 비선형관계를 갖는다면 잡아내기 힘들다는 점을 보완하기 위한 방법으로서, 다음 그림과 같은 상황에서 Kernel PCA를 사용할 것이다. 아래 관측치들의 집합인 행렬 X의 공분산 구조는 0에 가까운 값을 갖을 것이다. 즉, 서로 선형관계가 아닌 상황이다.</li></ul><p><img src="/image/when_we_use_kernel_PCA.png" alt="Kernel PCA를 하는 경우"></p><ul><li>기존의 PCA는 feature들 사이의 패턴이 존재하는 것으로 간주하여 비교(각 feature들간의 움직이는 방향을 나타내는 공분산 구조를 통해 값을 구했기 때문)를 했지만, 이번에는 관측치와 관측치 사이의 패턴을 수치화하여 PC를 구하는 방법이다. 아래 그림에서 K(Kernel matrix)의 예시를 보듯이 여러가지 방법이 존재한다.</li></ul><p><img src="/image/what_is_kernel_PCA.png" alt="Kernel PCA를 계산하는 방법"></p><ul><li>아래 그림과 같이 Kernel PCA를 계산할 수 있다.</li></ul><p><img src="/image/How_to_calculate_Kernel_matrix.png" alt="Kernel matrix를 계산하는 방법"></p><ul><li>아래 그림과 같이 원형의 띠로 구분지어져 있는 형상인데도 불구하구 Kernel PCA를 사용하면 잘 구분되는지에 대한 이유는 다음과 같다. 먼저 각 관측치에대해 pair-wise하게 Kernel matrix를 구해 비슷한 값들을 갖는 관측치들로 정렬하여 보면 비슷한 값을 갖는 관측치들 끼리 모여 block matrix를 형성하는 모습을 볼 수 있다.</li></ul><p><img src="/image/Kernel_matrix_shape_how.png" alt="Kernel matrix의 형태"></p><ul><li>Kernel matrix를 구한 값으로 eigen vector의 역할을 하는 행렬 U와 각 eigen value의 $d_{i}$의 곱인 PC Score를 통해 얻은 결과를 그래프로 그려보면 아래와 같은 결과를 얻을 수 있다. 아래의 경우에서는 첫번째 성분을 선택하여 사용하면 3가지 군집을 잘 판별할 수 있을 것이다.</li></ul><p><img src="/image/result_of_kernel_PCA.png" alt="Kernel matrix의 결과"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/03/machine_learning_06/#disqus_thread</comments>
    </item>
    
    <item>
      <title>로지스틱 회귀분석</title>
      <link>https://heung-bae-lee.github.io/2020/04/03/machine_learning_05/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/03/machine_learning_05/</guid>
      <pubDate>Fri, 03 Apr 2020 04:58:10 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;로지스틱-회귀분석&quot;&gt;&lt;a href=&quot;#로지스틱-회귀분석&quot; class=&quot;headerlink&quot; title=&quot;로지스틱 회귀분석&quot;&gt;&lt;/a&gt;로지스틱 회귀분석&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/image/logistic_regression.png&quot; 
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="로지스틱-회귀분석"><a href="#로지스틱-회귀분석" class="headerlink" title="로지스틱 회귀분석"></a>로지스틱 회귀분석</h1><p><img src="/image/logistic_regression.png" alt="로지스틱 회귀"></p><p><img src="/image/logistic_regression_02.png" alt="로지스틱 함수를 사용하는 이유"></p><ul><li>위의 추정식에서 가장 오른쪽에 있는 로그 오즈비(log odds ratio)를 먼저 설명하자면, 로그 안에 들어간 오즈비(odds ratio)는 베르누이 시도에서 1이 나올 확률 $\theta (x)$와 0이 나올 확률 $1-\theta (x)$의 비율(ratio)을 의미한다.</li></ul><script type="math/tex; mode=display">odds ratio = \frac{\theta (x)}{1 - \theta (x)}</script><p>0부터 1사이의 값만 가지는 확률값인 $\theta (x)$를 승산비로 변환하면 0부터 양의 무한대까지의 값을 가질 수 있다. 승산비를 로그변환한 것이 위에서 언급한 Logit function이다.</p><script type="math/tex; mode=display">y = logit(odds ratio) = log\( \frac{theta (x)}{} \)</script><p>이로써 로지트 함수의 값은 로그변환에 의해 음의 무한대 $(- \infty)$부터 양의 무한대$(\infty)$까지의 값을 가질 수 있다. 로지스틱함수(Logistic function)은 로지트 함수의 역함수이다. 즉 <code>음의 무한대부터 양의 무한대 까지의 값을 가지는 입력변수를 0부터 1사이의 값을 가지는 출력변수로 변환한 것</code>이다.</p><script type="math/tex; mode=display">logistic(z) = \theta (z) = \frac{1}{1 + exp(-z)}</script><ul><li>또한, <code>로지스틱 회귀분석에서는 판별함수 수식으로 선형함수를 사용하므로 판별 경계면 또한 선형이 됨을 유의</code>해야한다.</li></ul><script type="math/tex; mode=display">z = w^{T} x</script><p><img src="/image/Logistic_function_01.png" alt="로지스틱 함수"></p><ul><li>위의 그림에서 알 수 있듯이, X의 범위가 [$-\infty$, $+\infty$]인 경우에 Y의 범위를 [0,1]로 만들어주는 함수이다.</li></ul><p><img src="/image/logistic_regression_example.png" alt="로지스틱 회귀 예제"></p><ul><li><p>위의 그림에서 좌측그림의 적합시킨 회귀선(파란선)을 보면 예측할 확률값이 500미만이면 음수를 갖을 수 있게 되는 것을 확인할 수 있다. 이는 확률값을 예측하는 모델을 만든다는 가정 자체에 위반하는 것이므로 이전에 학습했었던 회귀모형들과는 다른 방법의 회귀모형을 만들어 적합시켜야 할 것임이 분명해졌다.</p></li><li><p>일반적으로 decision boundary(threshold)를 0.5로 하지만 class가 imbalanced 한 경우는 조절하여보면서 적합시킬 수 있다.</p></li></ul><p><img src="/image/logistic_regression_coefficient_estimation_01.png" alt="로지스틱 회귀계수 추정방법 소개"></p><p><img src="/image/logistic_regression_coefficient_estimation_02.png" alt="로지스틱 회귀계수 MLE를 사용한 추정"></p><ul><li>위의 MLE 방식을 통해 parameter의 값을 업데이트하는데, 아래와 같은 방식으로 수치적 최적화를 진행한다. 위의 로그 가능도함수 $log(l(\beta_{0}, \beta_{1}))$을 편의상 LL이라하자. 로그가능도함수 LL을 최대화하는 것은 다음 목적함수를 최소화하는 것과 같다.</li></ul><script type="math/tex; mode=display">J = - LL</script><ul><li>SGD(Steepest Gradient Descent)방식을 사용(Stochastic Gradient Descent아님!!)하여 아래와 같은 그레디언 벡터를 구할 수 있다.</li></ul><script type="math/tex; mode=display">g_{k} = \frac{d}{dw} (-LL)</script><ul><li>위에서 구한 그레디언트 방향으로 step size $(n_{k})$만큼 이동한다.</li></ul><script type="math/tex; mode=display">w_{k+1} = w_{k} - n_{k} g_{k}</script><script type="math/tex; mode=display">= w_{k} + n_{k} \sum_{i=1}^{N} (y_{i} - \theta_{i} (x_{i} ; w)) x_{i}</script><p><img src="/image/multi_logistic_regression_example_01.png" alt="다중 로지스틱 회귀 예제 - 01"></p><ul><li>위의 그림에서 요약된 다중 로지스틱 회귀분석의 결과를 살펴보면, 맨 마지막 설명과 같이 해석할 수 있지만, 좀 더 자연스러운 해석하고 싶을 경우 아래 그림과 같이 Logit 보다 exp를 취해줘 Odds로 변환하여 해석하는 것을 권한다.</li></ul><p><img src="/image/multi_logistic_regression_example_02.png" alt="다중 로지스틱 회귀 예제 - 02"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/03/machine_learning_05/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Python - 00 (Python의 장점 및 자료형)</title>
      <link>https://heung-bae-lee.github.io/2020/03/20/Python_00/</link>
      <guid>https://heung-bae-lee.github.io/2020/03/20/Python_00/</guid>
      <pubDate>Fri, 20 Mar 2020 12:02:48 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Python&quot;&gt;&lt;a href=&quot;#Python&quot; class=&quot;headerlink&quot; title=&quot;Python&quot;&gt;&lt;/a&gt;Python&lt;/h2&gt;&lt;h2 id=&quot;왜-Python을-배워야-할까&quot;&gt;&lt;a href=&quot;#왜-Python을-배워야-할까&quot; cla
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><h2 id="왜-Python을-배워야-할까"><a href="#왜-Python을-배워야-할까" class="headerlink" title="왜 Python을 배워야 할까?"></a>왜 Python을 배워야 할까?</h2><ul><li>프로그래밍 언어를 배우고 싶은데 어떤 언어를 배우면 될까?<ul><li>C, C++, C#, Java, Javascript, Python, Ruby, C#, Go, Rust, Scala Perl, Obj-C, PHP, R, Julia 등</li><li>여러가지 언어가 존재하지만 가장 진입장벽이 낮다.</li></ul></li></ul><p><img src="/image/Python_good_point_for_develop.png" alt="Python의 장점"></p><ul><li>Python 언어의 장점<ul><li>문법이 간결</li><li>다양한 운영체제 지원</li><li>GUI Application 개발(PyQT)</li><li>범용 언어(네트워크, 웹, 데이터 분석, 머신러닝 등)</li><li>방대한 라이브러리 지원<ul><li>data science : Numpy, Pandas, Matplotlib, scikit-learn, statsmodels, tensorflow, keras, Pytorch</li><li>web : Django, Flask 등</li><li>crawling : scrapy, BeautifulSoap, requests 등</li></ul></li></ul></li></ul><h2 id="Python은-어떤-언어인가"><a href="#Python은-어떤-언어인가" class="headerlink" title="Python은 어떤 언어인가?"></a>Python은 어떤 언어인가?</h2><ul><li>Python is an interpreted high-level programming language for general-purpose programming.<a href="https://en.wikipedia.org/wiki/Python_(programming_language" target="_blank" rel="noopener">wikipedia</a>)</li></ul><ul><li><p>Python의 특징</p><ul><li>플랫폼에 독립적</li><li>인터프리터 언어</li><li>객체지향적</li><li>동적타이핑</li></ul></li><li><p>위의 용어에대해 잘 모르겠다면, 먼저 컴퓨터에 대해 알아보자.</p></li></ul><h2 id="컴퓨터를-이해해-보자"><a href="#컴퓨터를-이해해-보자" class="headerlink" title="컴퓨터를 이해해 보자!"></a>컴퓨터를 이해해 보자!</h2><ul><li><p>컴퓨터는 <code>계산을 수행하는 기계</code>이다.</p></li><li><p>컴퓨터를 구성하는 기본 요소</p><ul><li>CPU(Central Processing Unit)</li><li>RAM(Random Access Memory)</li><li>ROM(Read-Only Memory)</li></ul></li><li><p>OS(Operating System : 운영체제)</p><ul><li>Kernel<ul><li>하드웨어를 컨트롤하는 소프트웨어 (운영체제의 핵심)</li></ul></li><li>CPU, RAM, ROM 자원을 사용하는 방법을 정의</li></ul></li><li><p>App(Application : 어플리케이션)</p><ul><li>OS 기반 응용 프로그램</li><li>대부분의 프로그래밍의 영역</li></ul></li></ul><p><img src="/image/computer_structure_image.png" alt="컴퓨터 구조"></p><h2 id="컴퓨터에서-프로그램의-동작원리는"><a href="#컴퓨터에서-프로그램의-동작원리는" class="headerlink" title="컴퓨터에서 프로그램의 동작원리는?"></a>컴퓨터에서 프로그램의 동작원리는?</h2><ul><li><p>CPU, RAM, ROM은 0과 1밖에 모른다는 사실은 상식적으로 다들 알고 있을 것이다.</p></li><li><p>그런데, 프로그램 언어는 숫자와 알파벡과 특수기호를 사용한다. 이는 각 언어의 Compiler가 코드를 0과 1의 이진수로 변환하여 주는 컴파일러 언어이기 때문이다.</p></li><li><p><code>컴파일러 언어</code></p><ul><li>모든 코드를 컴파일링 후에 컴퓨터에서 처리 -&gt; <code>처리속도가 빠르지만 프로그램 실행을 위해 컴파일링 시간을 기다려야한다.</code></li></ul></li><li><p><code>인터프리터 언어</code></p><ul><li>한줄씩 코드를 컴파일링 하면서 컴퓨터에서 처리 -&gt; <code>처리속도가 느리지만 컴파일링 시간 없이 바로바로 프로그램을 실행한다.</code></li></ul></li><li><p>결론적으론, 코드를 잘 만들면 컴퓨터가 효율적으로 일할 수 있다는 것이다!!!</p></li></ul><p><img src="/image/relation_with_between_computer_and_human.png" alt="Computer와 프로그래밍 언어의 상호작용 이미지"></p><h2 id="Python은-어떤-언어인가-1"><a href="#Python은-어떤-언어인가-1" class="headerlink" title="Python은 어떤 언어인가?"></a>Python은 어떤 언어인가?</h2><ul><li><p><code>플랫폼에 독립적</code></p><ul><li>어떠한 종류의 OS에도 같은 문법을 사용할 수 있다.</li><li>즉, Window, Mac OS, Linux등 여러 OS에서 사용가능하다는 의미이다.</li></ul></li><li><p><code>인터프리터 언어</code></p><ul><li>한줄씩 컴파일링 하면서 코드를 수행한다.</li></ul></li><li><p><code>객체지향적</code></p><ul><li>실제 세계를 모델링하여 공통적인 기능을 묶어서 개발하는 방식</li><li><code>추상화(abstraction)</code>, <code>캡슐화(encapsulation)</code>, <code>상속(inheritance)</code>, <code>다형성(polymorphism)</code>의 특징을 갖는다.</li><li>참고로, 반대의 개념은 절차지향이다.</li></ul></li><li><p><code>동적타이핑</code></p><ul><li>변수 선언시 데이터 타입을 지정해 주지 않아도 데이터에 따라서 자동으로 타이핑된다.</li></ul></li></ul><h2 id="Python의-종류는"><a href="#Python의-종류는" class="headerlink" title="Python의 종류는?"></a>Python의 종류는?</h2><ul><li><p>Cpython</p><ul><li>C로 만들어진 파이썬</li><li>우리가 코딩하는 부분은 인터프리터이지만 안에서는 다 컴파일러언어인 C로 동작되어 속도가 빠르다. 그러므로 인터프리터 언어가 갖는 속도가 느리다는 점을 보완할 수 있어 데이터 분석에 용이하다.</li></ul></li><li><p>Jython</p><ul><li>Java로 만들어진 파이썬</li></ul></li><li><p>IronPython</p><ul><li>C#으로 만들어진 파이썬</li></ul></li><li><p>Pypy</p><ul><li>Python으로 만들어진 파이썬</li><li>Cpython 보다 빠르게 수행되는 것을 목표로 한다. <a href="http://pypy.org/" target="_blank" rel="noopener">http://pypy.org/</a></li></ul></li></ul><h2 id="프로그래밍을-한다는-것은"><a href="#프로그래밍을-한다는-것은" class="headerlink" title="프로그래밍을 한다는 것은?"></a>프로그래밍을 한다는 것은?</h2><ul><li><code>컴퓨터와의 효율적인 커뮤니케이션으로 자신이 생각하는 목적을 컴퓨터가 잘 처리할 수 있도록 시스템의 구조를 잘 설계하고 코드를 작성하는 것</code>이다.</li></ul><h2 id="1-PEP-Python-Enhance-Proposal"><a href="#1-PEP-Python-Enhance-Proposal" class="headerlink" title="1. PEP(Python Enhance Proposal)"></a>1. PEP(Python Enhance Proposal)</h2><ul><li>python을 향상시키기 위한 제안</li><li>Zen of Python : PEP20 : <a href="https://www.python.org/dev/peps/pep-0020/" target="_blank" rel="noopener">https://www.python.org/dev/peps/pep-0020/</a>   </li><li>Style Guide for Python Code : PEP8 : <a href="https://www.python.org/dev/peps/pep-0008/" target="_blank" rel="noopener">https://www.python.org/dev/peps/pep-0008/</a></li></ul><h2 id="Python-기본-핵십-이해하기"><a href="#Python-기본-핵십-이해하기" class="headerlink" title="Python 기본 핵십 이해하기"></a>Python 기본 핵십 이해하기</h2><h3 id="값-value-처리"><a href="#값-value-처리" class="headerlink" title="값(value) 처리"></a>값(value) 처리</h3><ul><li><p>Python은 실행되는 모든것을 객체로 관리하므로 객체를 일관성을 가지고 평가할수 있는 규칙을 도입했기 때문에 모든 것을 값(value)으로 처리한다. 값을 재사용을 하기 위해선 변수에 저장해야한다.</p></li><li><p>위의 설명이 아직 까진 잘 이해가 가지 않을 것이다.</p></li></ul><h4 id="literal-리터럴"><a href="#literal-리터럴" class="headerlink" title="literal(리터럴)"></a>literal(리터럴)</h4><ul><li>literal이란 <code>프로그래밍 언어로 작성된 코드에서 값을 대표하는 용어</code>이다. 예를 들면, Python에서는 기본으로 사용되는 정수, 부동 소수점 숫자, 문자열, Boolean 등의 객체로 평가되며 모든 것을 값으로 처리하고 이를 통해 결과를 출력한다.</li></ul><h5 id="정수형-리터럴값-정의"><a href="#정수형-리터럴값-정의" class="headerlink" title="정수형 리터럴값 정의"></a>정수형 리터럴값 정의</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><h5 id="실행결과"><a href="#실행결과" class="headerlink" title="실행결과"></a>실행결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><h4 id="Expression-표현식"><a href="#Expression-표현식" class="headerlink" title="Expression(표현식)"></a>Expression(표현식)</h4><ul><li>literal은 단순히 하나의 값을 처리한다. <code>여러 개의 값을 하나로 묶어서 처리하는 방법</code>을 표현식이라고 한다. 표현식은 프로그래밍 언어가 해석하는 하나 이상의 명시적 값, 상수, 변수, 연산자 조합이고 함수도 실행되면 하나의 값이 되어 이를 조합해도 표현식으로 인식한다. <code>표현식을 평가해서 실행될 때에는 우선순위 및 연관 규칙에 따라 해석하여 실행되며 평가된 결과는 하나의 값인 literal로 표현</code>된다. <code>표현식 처리순서는 좌측부터 우측으로 가며 평가하고 () 연산자를 최우선으로 처리</code>된다.</li></ul><h5 id="표현식-평가"><a href="#표현식-평가" class="headerlink" title="표현식 평가"></a>표현식 평가</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3 + 4</span><br></pre></td></tr></table></figure><h6 id="위의-표현식-결과"><a href="#위의-표현식-결과" class="headerlink" title="위의 표현식 결과"></a>위의 표현식 결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">7</span><br></pre></td></tr></table></figure><h5 id="표현식-평가-1"><a href="#표현식-평가-1" class="headerlink" title="표현식 평가"></a>표현식 평가</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"문자"</span> + <span class="string">"열"</span></span><br></pre></td></tr></table></figure><h6 id="위의-표현식-결과-1"><a href="#위의-표현식-결과-1" class="headerlink" title="위의 표현식 결과"></a>위의 표현식 결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"문자열"</span></span><br></pre></td></tr></table></figure><h4 id="condition-expression-조건식"><a href="#condition-expression-조건식" class="headerlink" title="condition expression(조건식)"></a>condition expression(조건식)</h4><ul><li><code>표현식에서 특정 조건문 등에 제한적으로 사용되는 것</code>을 조건식이라고 한다. 주로 특정 문장인 if문이나 while 문에서 처리된다. 처리된 결과 값은 Boolean 값을 갖는다.</li></ul><h5 id="조건식-평가"><a href="#조건식-평가" class="headerlink" title="조건식 평가"></a>조건식 평가</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bool(<span class="string">""</span>)</span><br></pre></td></tr></table></figure><h5 id="조건식-평가-결과"><a href="#조건식-평가-결과" class="headerlink" title="조건식 평가 결과"></a>조건식 평가 결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure><h3 id="이름-name-처리"><a href="#이름-name-처리" class="headerlink" title="이름(name) 처리"></a>이름(name) 처리</h3><ul><li><p>프로그램을 작성한다는 것은 값들을 저장하고, 다음에 필요할 경우 이를 읽어와서 계산하고 다시 저장해서 처리한다는 뜻이다. 프로그램에서 이런 값들의 변화를 저장해서 관리하는 기준이 필요하고, 파이썬에서는 이를 위해 Name space를 만들어서 관리한다. <code>이름으로 지정할 요소들은 변수(variable), 함수(function), 클래스(class), 모듈(module), 패키지(package)등 이다.</code> 이 중에 <code>변수를 빼면 다 객체로 사용되므로 값으로 관리되는 것을 알 수 있다.</code></p></li><li><p>파이썬은 변수, 함수, 클래스 등에 대한 이름을 구별하지 않으므로 변수, 함수, 클래스 등의 <code>명명 규칙을 명확히 해서 각각을 이름으로 식별할 수 있어야 한다.</code></p></li></ul><h4 id="예약어-keyword"><a href="#예약어-keyword" class="headerlink" title="예약어(keyword)"></a>예약어(keyword)</h4><ul><li><p>파이썬에서 변수, 함수, 클래스 등은 예약어와 동일한 이름으로 정의할 수 없고 <code>파이썬 내부 문법 규칙에서만 사용할 수 있도록 정의한 것</code>을 예약어라고 한다.</p></li><li><p>파이썬에서 모듈(module)은 프로그램을 관리하는 하나의 단위이고 이 내부에 변수, 함수, 클래스 등을 지정해서 관리한다. 파이썬 내부의 예약어를 보여주기 위해 하나의 모듈로 관리하며 그 모듈의 이름이 keyword이다.</p></li></ul><h5 id="Keyword-모듈-알아보기"><a href="#Keyword-모듈-알아보기" class="headerlink" title="Keyword 모듈 알아보기"></a>Keyword 모듈 알아보기</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from pprint import pprint</span><br><span class="line">import keyword</span><br><span class="line"></span><br><span class="line">pprint(keyword.kwlist, width=60, compact=True)</span><br></pre></td></tr></table></figure><h5 id="Keyword-모듈-리스트"><a href="#Keyword-모듈-리스트" class="headerlink" title="Keyword 모듈 리스트"></a>Keyword 모듈 리스트</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'False'</span>, <span class="string">'None'</span>, <span class="string">'True'</span>, <span class="string">'and'</span>, <span class="string">'as'</span>, <span class="string">'assert'</span>, <span class="string">'async'</span>,</span><br><span class="line"> <span class="string">'await'</span>, <span class="string">'break'</span>, <span class="string">'class'</span>, <span class="string">'continue'</span>, <span class="string">'def'</span>, <span class="string">'del'</span>,</span><br><span class="line"> <span class="string">'elif'</span>, <span class="string">'else'</span>, <span class="string">'except'</span>, <span class="string">'finally'</span>, <span class="string">'for'</span>, <span class="string">'from'</span>,</span><br><span class="line"> <span class="string">'global'</span>, <span class="string">'if'</span>, <span class="string">'import'</span>, <span class="string">'in'</span>, <span class="string">'is'</span>, <span class="string">'lambda'</span>, <span class="string">'nonlocal'</span>,</span><br><span class="line"> <span class="string">'not'</span>, <span class="string">'or'</span>, <span class="string">'pass'</span>, <span class="string">'raise'</span>, <span class="string">'return'</span>, <span class="string">'try'</span>, <span class="string">'while'</span>,</span><br><span class="line"> <span class="string">'with'</span>, <span class="string">'yield'</span>]</span><br></pre></td></tr></table></figure><h4 id="명명-규칙-naming-convention"><a href="#명명-규칙-naming-convention" class="headerlink" title="명명 규칙(naming convention)"></a>명명 규칙(naming convention)</h4><ul><li><p>파이썬은 하나의 프로그램을 작성하는 기준이 모듈이므로 <code>모듈 단위로 이름을 관리할 수 있는 하나의 영역이 존재</code>한다. 이렇게 모듈 단위로 관리하는 하나의 이름 관리 영역을 <code>전역 네임스페이스(global name space)</code>라고 한다.</p></li><li><p>변수와 함수 등이 동일한 이름으로 정의될 경우 최종적으로 할당된 값을 갖으므로 <code>최대한 명명 규칙을 준수해야 버전이 변경될 경우에도 일관성 있는 규칙을 준수할 수 있다.</code></p></li></ul><h5 id="식별자-Identifiers"><a href="#식별자-Identifiers" class="headerlink" title="식별자 : Identifiers"></a>식별자 : Identifiers</h5><p>변수, 함수, 클래스, 모듈 등을 구분하기 위해 사용되는 이름을 식별자라고 합니다. 이 식별자는 몇가지의 규칙이 있습니다.</p><ul><li>대소문자를 구분</li><li>소문자(a-z), 대문자(A-Z), 숫자(0-9), 언더스코어( _ ) 사용 가능</li><li>( _ )를 제외한 특수문자는 사용 불가</li><li>가장 앞에 ( __ ) 사용 지양 (reserved global variable)</li><li>가장앞에숫자사용불가</li><li>예약어사용불가</li><li>예약어<ul><li>Fasle, class, finally, is, return, None, continue, for, lambda, try, True, def, from, nonlocal, while, and, del, global, not, with, as, elif, if, or, yield, assert, else, import, pass, break, except, in, raise</li></ul></li></ul><h5 id="파이썬-권장-명명-규칙-모듈-패키지"><a href="#파이썬-권장-명명-규칙-모듈-패키지" class="headerlink" title="파이썬 권장 명명 규칙: 모듈, 패키지"></a>파이썬 권장 명명 규칙: 모듈, 패키지</h5><ul><li><p>모듈 이름은 짧아야 하고, 전부 소문자로 작성하는 것을 권장한다. 가독성을 위해 언더스코어( _ )를 쓸 수 있다.</p></li><li><p>패키지 이름 또한 짧아야 하고, 전부 소문자로 작성하지만, 언더스코어( _ )는 권장하지 않는다.</p></li></ul><h5 id="파이썬-권장-명명-규칙-클래스"><a href="#파이썬-권장-명명-규칙-클래스" class="headerlink" title="파이썬 권장 명명 규칙: 클래스"></a>파이썬 권장 명명 규칙: 클래스</h5><ul><li>클래스 이름은 Capitalized words 형식(단어를 대문자로 시작)을 따르며, <a href="https://dpdpwl.tistory.com/55" target="_blank" rel="noopener">카멜 표기법</a>도 사용이 가능하다.</li></ul><h5 id="파이썬-권장-명명-규칙-상수-변수-함수-메서드"><a href="#파이썬-권장-명명-규칙-상수-변수-함수-메서드" class="headerlink" title="파이썬 권장 명명 규칙: 상수, 변수, 함수, 메서드"></a>파이썬 권장 명명 규칙: 상수, 변수, 함수, 메서드</h5><ul><li>변수, 함수와 메서드의 이름은 원칙적으로 소문자여야하고, 가족성을 위해서 언더스코어( _ )단어로 쓰는 것을 권장한다.<ul><li>보호 속성일 때는 맨앞에 _ 를 추가적으로 붙인다.</li><li>키워드와 동일 변수일 때는 맨 뒤에 _ 를 추가적으로 붙인다.</li><li>비공개 속성일 때는 맨 앞에 __ 하나를 붙인다.</li><li>스페셜 속성일때는 앞과 뒤에 __ 하나씩 붙인다.</li></ul></li></ul><h4 id="Variable-변수"><a href="#Variable-변수" class="headerlink" title="Variable(변수)"></a>Variable(변수)</h4><ul><li><p>파이썬에서 변수는 다른 언어의 변수와 차이가 크다. 동적타이핑의 특성을 지니므로 변수에 특정한 자료형 등을 배정하지 않는다는 점이다. 변수는 단순히 이름만 지정하고 이 변수에 값을 할당해서 사용하므로 다양한 자료형이 할당된다.</p></li><li><p><code>파이썬에서 변수는 단순히 값을 보관하는 장소가 아니라 값들의 임시 저장 장소로만 사용</code>한다. 프로그램 내에서 변수는 단순히 이름으로 구별하는 것이고 변수에는 값인 객체가 어디에 있는지에 대한 주소인 reference만 보관한다. 즉, <code>변수는 단순히 자료형으로 만들어진 객체에 대한 주소만을 관리하는 역할을 수행</code>한다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/03/20/Python_00/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 00 (Node, List, Queue)</title>
      <link>https://heung-bae-lee.github.io/2020/03/19/data_structure_01/</link>
      <guid>https://heung-bae-lee.github.io/2020/03/19/data_structure_01/</guid>
      <pubDate>Thu, 19 Mar 2020 09:46:30 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;목표&quot;&gt;&lt;a href=&quot;#목표&quot; class=&quot;headerlink&quot; title=&quot;목표&quot;&gt;&lt;/a&gt;목표&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;기본 자료 구조/알고리즘 익히기&lt;ul&gt;
&lt;li&gt;알고리즘 풀이를 위해, 기본적으로 알고 있어야 하는 자료구조와 알고리
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="목표"><a href="#목표" class="headerlink" title="목표"></a>목표</h2><ul><li>기본 자료 구조/알고리즘 익히기<ul><li>알고리즘 풀이를 위해, 기본적으로 알고 있어야 하는 자료구조와 알고리즘 정리</li></ul></li></ul><h2 id="자료구조란"><a href="#자료구조란" class="headerlink" title="자료구조란?"></a>자료구조란?</h2><ul><li>용어: 자료구조 = 데이터 구조 = data structure</li><li>대량의 데이터를 효율적으로 관리할 수 있는 데이터의 구조를 의미</li><li>코드상에서 효율적으로 데이터를 처리하기 위해, 데이터 특성에 따라, 체계적으로 데이터를 구조화해야 함.<ul><li><code>어떤 데이터 구조를 사용하느냐에 따라, 코드 효율이 달라진다.</code></li></ul></li></ul><h3 id="효율적으로-데이터를-관리하는-예"><a href="#효율적으로-데이터를-관리하는-예" class="headerlink" title="효율적으로 데이터를 관리하는 예"></a>효율적으로 데이터를 관리하는 예</h3><ul><li><p>우편번호</p><ul><li>5자리 우편번호로 국가의 기초구역을 식별</li><li>5자리 우편번호에서 앞 3자리는 시,군,자치구를 의미, 뒤 2자리는 일련변호로 구성</li></ul></li><li><p>학생관리</p><ul><li>학년, 반, 번호를 학생에게 부여해서, 학생부를 관리</li><li>위같은 기법이 없었다면, 많은 학생 중 특정 학생을 찾기 위해, 전체 학생부를 모두 훑어야 하는 불편함이 생긴다.</li></ul></li></ul><h2 id="대표적인-자료-구조"><a href="#대표적인-자료-구조" class="headerlink" title="대표적인 자료 구조"></a>대표적인 자료 구조</h2><ul><li>array(배열), stack(스택), queue(큐), linked list(링크드 리스트), hash table(해쉬 테이블), heap(힙) 등</li></ul><h2 id="알고리즘이란"><a href="#알고리즘이란" class="headerlink" title="알고리즘이란?"></a>알고리즘이란?</h2><ul><li>용어: 알고리즘(algorithm)</li><li><code>어떤 문제를 풀기 위한 절차/방법</code></li><li>어떤 문제에 대해, 특정한 ‘입력’을 넣으면, 원하는 ‘출력’을 얻을 수 있도록 만드는 프로그래밍</li></ul><h2 id="자료구조와-알고리즘이-중요한-이유"><a href="#자료구조와-알고리즘이-중요한-이유" class="headerlink" title="자료구조와 알고리즘이 중요한 이유"></a>자료구조와 알고리즘이 중요한 이유</h2><ul><li><code>어떤 자료구조와 알고리즘을 쓰느냐에 따라, 성능이 많이 차이나기 때문</code>이다. 결과적으로, 자료구조와 알고리즘을 통해 프로그래밍을 잘 할 수 있는 기술과 역량을 검증할 수 있다.</li></ul><h2 id="자료구조-알고리즘-그리고-Python"><a href="#자료구조-알고리즘-그리고-Python" class="headerlink" title="자료구조/알고리즘, 그리고 Python"></a>자료구조/알고리즘, 그리고 Python</h2><ul><li>어떤 언어로든 자료구조/알고리즘을 익힐 수 있다.<ul><li>이전에는 무조건 C 또는 C++로만 작성하도록 하는 경향이 많았다.</li><li>최근에는 언어로 인한 제약/평가는 없어졌다고 봐도 무방할 것 같다.<ul><li>그러므로, 가장 쉽고 진입장벽이 상대적으로 낮은 Python을 통해 필자는 정리해 볼 것이다.</li></ul></li></ul></li></ul><h2 id="ADT-Abstract-Data-Type"><a href="#ADT-Abstract-Data-Type" class="headerlink" title="ADT(Abstract Data Type)"></a>ADT(Abstract Data Type)</h2><h2 id="추상-자료형"><a href="#추상-자료형" class="headerlink" title="추상 자료형"></a>추상 자료형</h2><p>실제 정의)</p><pre><code>- 내부 구조(object)- 기능(operation) ==&gt; 함수로 구현</code></pre><p>간단한 말로는 내가 사용하고자 하는 <code>자료구조의 함수의 사용법</code>이다.</p><h4 id="필자는-Python을-통해-각-알고리즘을-구현해-볼-것이다-물론-Python에는-이미-만들어져-놓은-library-중-대부분의-알고리즘을-구현해-놓은-것들이-존재하지만-그렇게-사용하다보면-내부적으로-뜯어보지-않는-이상-어떠한-방식으로-작동되는지-알지-못하기-때문에-각-알고리즘의-구조를-설명한-후-ADT를-작성해-보고-class를-만들어-function을-작성하여-구현하는-방식으로-글을-써-갈-것이다-물론-Python-내부에-존재하는-library의-사용법도-같이-소개할-것이다"><a href="#필자는-Python을-통해-각-알고리즘을-구현해-볼-것이다-물론-Python에는-이미-만들어져-놓은-library-중-대부분의-알고리즘을-구현해-놓은-것들이-존재하지만-그렇게-사용하다보면-내부적으로-뜯어보지-않는-이상-어떠한-방식으로-작동되는지-알지-못하기-때문에-각-알고리즘의-구조를-설명한-후-ADT를-작성해-보고-class를-만들어-function을-작성하여-구현하는-방식으로-글을-써-갈-것이다-물론-Python-내부에-존재하는-library의-사용법도-같이-소개할-것이다" class="headerlink" title="필자는 Python을 통해 각 알고리즘을 구현해 볼 것이다. 물론 Python에는 이미 만들어져 놓은 library 중 대부분의 알고리즘을 구현해 놓은 것들이 존재하지만, 그렇게 사용하다보면 내부적으로 뜯어보지 않는 이상 어떠한 방식으로 작동되는지 알지 못하기 때문에 각 알고리즘의 구조를 설명한 후 ADT를 작성해 보고 class를 만들어 function을 작성하여 구현하는 방식으로 글을 써 갈 것이다. 물론, Python 내부에 존재하는 library의 사용법도 같이 소개할 것이다."></a>필자는 Python을 통해 각 알고리즘을 구현해 볼 것이다. 물론 Python에는 이미 만들어져 놓은 library 중 대부분의 알고리즘을 구현해 놓은 것들이 존재하지만, 그렇게 사용하다보면 내부적으로 뜯어보지 않는 이상 어떠한 방식으로 작동되는지 알지 못하기 때문에 각 알고리즘의 구조를 설명한 후 ADT를 작성해 보고 class를 만들어 function을 작성하여 구현하는 방식으로 글을 써 갈 것이다. 물론, Python 내부에 존재하는 library의 사용법도 같이 소개할 것이다.</h4><h3 id="선형구조"><a href="#선형구조" class="headerlink" title="선형구조"></a>선형구조</h3><ul><li>배열</li><li>연결리스트</li><li>스택</li><li>큐</li></ul><h3 id="비선형구조"><a href="#비선형구조" class="headerlink" title="비선형구조"></a>비선형구조</h3><ul><li>트리 (BTS —&gt; B-tree, red-black tree)</li><li>그래프</li><li>테이블</li><li>etc</li></ul><h2 id="Array-배열"><a href="#Array-배열" class="headerlink" title="Array(배열)"></a>Array(배열)</h2><ul><li><code>데이터를 나열하고, 각 데이터를 인덱스에 대응하도록 구성한 데이터 구조</code></li><li>Python에서는 <code>리스트 타입이 배열 기능</code>을 제공한다.</li></ul><h3 id="1-배열은-왜-필요할까"><a href="#1-배열은-왜-필요할까" class="headerlink" title="1. 배열은 왜 필요할까?"></a>1. 배열은 왜 필요할까?</h3><ul><li><code>같은 종류</code>의 데이터를 효율적으로 관리하기 위해 사용</li><li><code>같은 종류</code>의 데이터를 <code>순차적으로 저장</code></li><li>장점:<ul><li><code>빠른 접근 가능</code></li><li>첫 데이터의 위치에서 상대적인 위치로 데이터 접근(인덱스 번호로 접근)</li></ul></li><li>단점:<ul><li><code>데이터 추가/삭제의 어려움</code></li><li>미리 최대 길이를 지정해야 함</li></ul></li></ul><h4 id="C-언어-예-영어-단어-저장"><a href="#C-언어-예-영어-단어-저장" class="headerlink" title="C 언어 예: 영어 단어 저장"></a>C 언어 예: 영어 단어 저장</h4><ul><li>배열을 생성할 때 먼저 길이를 정해주어야 하기 때문에, 아래와 같이 []안에 최대 길이를 지정해준 것을 확인할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#include &lt;stdio.h&gt;</span></span><br><span class="line"></span><br><span class="line">int main(int argc, char * argv[])</span><br><span class="line">&#123;</span><br><span class="line">    char country[3] = <span class="string">"US"</span>;</span><br><span class="line">    <span class="built_in">printf</span> (<span class="string">"%c%c\n"</span>, country[0], country[1]);</span><br><span class="line">    <span class="built_in">printf</span> (<span class="string">"%s\n"</span>, country);    </span><br><span class="line">    <span class="built_in">return</span> 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="파이썬-언어-예-영어-단어-저장"><a href="#파이썬-언어-예-영어-단어-저장" class="headerlink" title="파이썬 언어 예: 영어 단어 저장"></a>파이썬 언어 예: 영어 단어 저장</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">country = <span class="string">'US'</span></span><br><span class="line"><span class="built_in">print</span> (country)</span><br></pre></td></tr></table></figure><h3 id="2-파이썬과-배열"><a href="#2-파이썬과-배열" class="headerlink" title="2. 파이썬과 배열"></a>2. 파이썬과 배열</h3><ul><li>파이썬에서는 리스트로 배열 구현 가능</li></ul><h4 id="1차원-배열-리스트로-구현시"><a href="#1차원-배열-리스트로-구현시" class="headerlink" title="1차원 배열: 리스트로 구현시"></a>1차원 배열: 리스트로 구현시</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_list = [1, 2, 3, 4, 5]</span><br><span class="line">data_list</span><br></pre></td></tr></table></figure><h4 id="2차원-배열-리스트로-구현시"><a href="#2차원-배열-리스트로-구현시" class="headerlink" title="2차원 배열: 리스트로 구현시"></a>2차원 배열: 리스트로 구현시</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]</span><br><span class="line">data_list</span><br></pre></td></tr></table></figure><h4 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (data_list[0])</span><br><span class="line"><span class="comment"># [1, 2, 3]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (data_list[0][0])</span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="built_in">print</span> (data_list[0][1])</span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="built_in">print</span> (data_list[0][2])</span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span> (data_list[1][0])</span><br><span class="line"><span class="comment"># 4</span></span><br><span class="line"><span class="built_in">print</span> (data_list[1][1])</span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"><span class="built_in">print</span> (data_list[1][2])</span><br><span class="line"><span class="comment"># 6</span></span><br></pre></td></tr></table></figure><h3 id="3-프로그래밍-연습"><a href="#3-프로그래밍-연습" class="headerlink" title="3. 프로그래밍 연습"></a>3. 프로그래밍 연습</h3><h4 id="연습1-위의-2차원-배열에서-9-8-7-을-순서대로-출력해보기"><a href="#연습1-위의-2차원-배열에서-9-8-7-을-순서대로-출력해보기" class="headerlink" title="연습1: 위의 2차원 배열에서 9, 8, 7 을 순서대로 출력해보기"></a>연습1: 위의 2차원 배열에서 9, 8, 7 을 순서대로 출력해보기</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(data_list[2][2], data_list[2][1], data_list[2][0])</span><br></pre></td></tr></table></figure><h4 id="연습2-아래의-dataset-리스트에서-전체-이름-안에-M-은-몇-번-나왔는지-빈도수-출력하기"><a href="#연습2-아래의-dataset-리스트에서-전체-이름-안에-M-은-몇-번-나왔는지-빈도수-출력하기" class="headerlink" title="연습2: 아래의 dataset 리스트에서 전체 이름 안에 M 은 몇 번 나왔는지 빈도수 출력하기"></a>연습2: 아래의 dataset 리스트에서 전체 이름 안에 M 은 몇 번 나왔는지 빈도수 출력하기</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">dataset = [<span class="string">'Braund, Mr. Owen Harris'</span>,</span><br><span class="line"><span class="string">'Cumings, Mrs. John Bradley (Florence Briggs Thayer)'</span>,</span><br><span class="line"><span class="string">'Heikkinen, Miss. Laina'</span>,</span><br><span class="line"><span class="string">'Futrelle, Mrs. Jacques Heath (Lily May Peel)'</span>,</span><br><span class="line"><span class="string">'Allen, Mr. William Henry'</span>,</span><br><span class="line"><span class="string">'Moran, Mr. James'</span>,</span><br><span class="line"><span class="string">'McCarthy, Mr. Timothy J'</span>,</span><br><span class="line"><span class="string">'Palsson, Master. Gosta Leonard'</span>,</span><br><span class="line"><span class="string">'Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)'</span>,</span><br><span class="line"><span class="string">'Nasser, Mrs. Nicholas (Adele Achem)'</span>,</span><br><span class="line"><span class="string">'Sandstrom, Miss. Marguerite Rut'</span>,</span><br><span class="line"><span class="string">'Bonnell, Miss. Elizabeth'</span>,</span><br><span class="line"><span class="string">'Saundercock, Mr. William Henry'</span>,</span><br><span class="line"><span class="string">'Andersson, Mr. Anders Johan'</span>,</span><br><span class="line"><span class="string">'Vestrom, Miss. Hulda Amanda Adolfina'</span>,</span><br><span class="line"><span class="string">'Hewlett, Mrs. (Mary D Kingcome) '</span>,</span><br><span class="line"><span class="string">'Rice, Master. Eugene'</span>,</span><br><span class="line"><span class="string">'Williams, Mr. Charles Eugene'</span>,</span><br><span class="line"><span class="string">'Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)'</span>,</span><br><span class="line"><span class="string">'Masselmani, Mrs. Fatima'</span>,</span><br><span class="line"><span class="string">'Fynney, Mr. Joseph J'</span>,</span><br><span class="line"><span class="string">'Beesley, Mr. Lawrence'</span>,</span><br><span class="line"><span class="string">'McGowan, Miss. Anna "Annie"'</span>,</span><br><span class="line"><span class="string">'Sloper, Mr. William Thompson'</span>,</span><br><span class="line"><span class="string">'Palsson, Miss. Torborg Danira'</span>,</span><br><span class="line"><span class="string">'Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)'</span>,</span><br><span class="line"><span class="string">'Emir, Mr. Farred Chehab'</span>,</span><br><span class="line"><span class="string">'Fortune, Mr. Charles Alexander'</span>,</span><br><span class="line"><span class="string">'Dwyer, Miss. Ellen "Nellie"'</span>,</span><br><span class="line"><span class="string">'Todoroff, Mr. Lalio'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">m_count = 0</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        <span class="keyword">if</span> data[index] == <span class="string">'M'</span>:</span><br><span class="line">            m_count += 1</span><br><span class="line"><span class="built_in">print</span> (m_count)</span><br></pre></td></tr></table></figure><ul><li>위에서와 같이 간단하게 이미 만들어져 있는 Python의 자료형인 list를 사용하여 Array를 사용하는 것 말고 Node 개념을 도입하여 Array를 만들어 볼 것이다.</li></ul><h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><ul><li>아래와 같이 하나의 노드는 <code>실제 데이터와 다음데이터를 가리키고 있는 참조로 구성</code>되어있다.</li></ul><p><img src="/image/Node.png" alt="Node"></p><ul><li>Node Class 구현<ul><li>위의 그림과 같이 실제 데이터를 가리키는 부분과 다음 데이터를 참조하는 부분을 나누어서 정의해 준다.</li></ul></li></ul><h3 id="혼자-노드를-만들때-필요한-과정을-정리하면서-만들어보기"><a href="#혼자-노드를-만들때-필요한-과정을-정리하면서-만들어보기" class="headerlink" title="혼자 노드를 만들때 필요한 과정을 정리하면서 만들어보기"></a>혼자 노드를 만들때 필요한 과정을 정리하면서 만들어보기</h3><p>1) Node의 정의를 먼저 살펴보면 노드는 데이터를 포함하고있는 부분과 다음데이터를 가리키는 참조부분으로 이루어져 있다!</p><p>2) 그러므로 Node class에는 먼저, 데이터 부분과 참조부분을 만들어주어야 하는데, 우선 우리의 목표는 어디까지나 Node의 기능을 구현하는 것이다.</p><p>3) Node의 데이터 조회, 데이터 바꾸기와 같은 기능, 그리고 참조를 어디로 하는지 등이필요할 것이다!</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class Node_mine():</span><br><span class="line">    <span class="comment"># 생성자에는 우선적으로 노드의 component들을 필수요소로 넣어줘야할 것이다.</span></span><br><span class="line">    <span class="comment"># 여기서 처음 부터 next를 argument에서 빼 놓은 이유는?</span></span><br><span class="line">    <span class="comment"># 이어져 있기 때문에 다음 노드를 바로 참조하면 되기 때문에 argument로 넣어줄 필요는 없다.</span></span><br><span class="line">    <span class="comment"># 즉, 우리가 마음대로 4번째 노드를 1번째노드로 참조하게끔하는 것은 linked list의 정의를 벗어나므로</span></span><br><span class="line">    <span class="comment"># 참조부분을 Node를 생성하면서부터 지정할 필요는 없다는 것이다.</span></span><br><span class="line"></span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line">        self.__next=None</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 소멸자: 객체가 메모리에서 사라질때 반드시 한번은 호출해 주는 것</span></span><br><span class="line">    def __delete__(self):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"&#123;&#125; is deleted"</span>.format(self.__data))</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def get_data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @get_data.setter</span><br><span class="line">    def get_data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def get_next(self):</span><br><span class="line">        <span class="built_in">return</span> self.__next</span><br><span class="line"></span><br><span class="line">    @get_next.setter</span><br><span class="line">    def get_next(self, nt):</span><br><span class="line">        self.__next=nt</span><br></pre></td></tr></table></figure><ul><li>node 정의 및 생성</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n1=Node_mine(5)</span><br></pre></td></tr></table></figure><ul><li>node의 value 출력</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n1.get_data</span><br><span class="line"></span><br><span class="line"><span class="comment">### 결과</span></span><br><span class="line"><span class="comment">### 5</span></span><br></pre></td></tr></table></figure><ul><li>node에 새로운 값 할당</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n1.get_data=3</span><br></pre></td></tr></table></figure><ul><li>변경된 값을 제대로 할당 받았는지 확인<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n1.get_data</span><br><span class="line">### 결과</span><br><span class="line">### 3</span><br></pre></td></tr></table></figure></li></ul><h4 id="현업에서-사용하는-linked-list라-함은-dummy-duable-linked-list를-의미하고-아래에서의-single-linked-list에서-single이-의미하는-것은-참조가-하나-즉-한개의-방향만을-참조하고-duable은-양방향을-참조한다"><a href="#현업에서-사용하는-linked-list라-함은-dummy-duable-linked-list를-의미하고-아래에서의-single-linked-list에서-single이-의미하는-것은-참조가-하나-즉-한개의-방향만을-참조하고-duable은-양방향을-참조한다" class="headerlink" title="현업에서 사용하는 linked list라 함은 dummy duable linked list를 의미하고, 아래에서의 single linked list에서 single이 의미하는 것은 참조가 하나 즉 한개의 방향만을 참조하고 duable은 양방향을 참조한다."></a>현업에서 사용하는 linked list라 함은 dummy duable linked list를 의미하고, 아래에서의 single linked list에서 single이 의미하는 것은 참조가 하나 즉 한개의 방향만을 참조하고 duable은 양방향을 참조한다.</h4><h4 id="Singel-Linked-list"><a href="#Singel-Linked-list" class="headerlink" title="Singel Linked list"></a>Singel Linked list</h4><h5 id="Instance-Member"><a href="#Instance-Member" class="headerlink" title="Instance Member"></a>Instance Member</h5><ol><li>head<ul><li>리스트의 첫번째 노드를 가리킨다.</li></ul></li><li>d_size<ul><li>리스트의 요소 개수</li></ul></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">class SingleLinkedList:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.head = None</span><br><span class="line">        self.d_size = 0</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.d_size==0:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def size(self):</span><br><span class="line">        <span class="built_in">return</span> self.d_size</span><br><span class="line"></span><br><span class="line">    def add(self, data):</span><br><span class="line">        new_node=Node_mine(data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 아래에서와 같이 단방향으로 이루어져 있고, 기존에 있던 Node를 새로운 Node의 다음으로 참조시켜준 뒤에, SingleLinkedList 클래스의 head값에는 새로운 Node를 추가해준다.</span></span><br><span class="line">        new_node.next=self.head</span><br><span class="line">        self.head=new_node</span><br><span class="line"></span><br><span class="line">        self.d_size+=1</span><br><span class="line"></span><br><span class="line">    def search(self, target):</span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur :</span><br><span class="line">            <span class="keyword">if</span> cur.data == target :</span><br><span class="line">                <span class="built_in">return</span> cur</span><br><span class="line">            <span class="keyword">else</span> :</span><br><span class="line">                cur=cur.next</span><br><span class="line">        <span class="built_in">return</span> cur</span><br><span class="line"></span><br><span class="line">    def delete(self):</span><br><span class="line">        <span class="comment"># head에 있는 값을 지울것이므로 head값을 기존의 head의 다음 값으로 바꾸어주면 garbage collector에 의해서 사라지게 된다.</span></span><br><span class="line">        self.head=self.head.next</span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line">    def traverse(self):</span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            yield cur</span><br><span class="line">            cur=cur.next</span><br></pre></td></tr></table></figure><h2 id="Queue-큐"><a href="#Queue-큐" class="headerlink" title="Queue(큐)"></a>Queue(큐)</h2><ul><li>array와 함께 가장 쉬운 자료구조 중 하나<ul><li>컴퓨터에서 가장 핵심적인 OS(운영체제)에서도 많이 사용되며, 인터넷에서도 네트워크 기능에서도 많이 사용된다.</li></ul></li></ul><h3 id="1-Queue-구조"><a href="#1-Queue-구조" class="headerlink" title="1. Queue 구조"></a>1. Queue 구조</h3><ul><li><code>줄을 서는 행위와 유사</code></li><li>가장 먼저 넣은 데이터를 가장 먼저 꺼낼 수 있는 구조<ul><li>음식점에서 가장 먼저 줄을 선 사람이 제일 먼저 음식점에 입장하는 것과 동일</li><li><code>FIFO(First-In, First-Out)</code> 또는 <code>LILO(Last-In, Last-Out)</code> 방식으로 스택과 꺼내는 순서가 반대</li></ul></li></ul><p><img src="https://www.fun-coding.org/00_Images/queue.png"></p><ul><li>출처: <a href="http://www.stoimen.com/blog/2012/06/05/computer-algorithms-stack-and-queue-data-structure/" target="_blank" rel="noopener">http://www.stoimen.com/blog/2012/06/05/computer-algorithms-stack-and-queue-data-structure/</a></li></ul><h3 id="2-알아둘-용어"><a href="#2-알아둘-용어" class="headerlink" title="2. 알아둘 용어"></a>2. 알아둘 용어</h3><ul><li>Enqueue: 큐에 데이터를 넣는 기능</li><li>Dequeue: 큐에서 데이터를 꺼내는 기능</li><li><font color="#BF360C">Visualgo 사이트에서 시연해보며 이해하기 (enqueue/dequeue 만 클릭해보며): <a href="https://visualgo.net/en/list" target="_blank" rel="noopener">https://visualgo.net/en/list</a></font></li></ul><h3 id="3-파이썬-queue-라이브러리-활용해서-큐-자료-구조-사용하기"><a href="#3-파이썬-queue-라이브러리-활용해서-큐-자료-구조-사용하기" class="headerlink" title="3. 파이썬 queue 라이브러리 활용해서 큐 자료 구조 사용하기"></a>3. 파이썬 queue 라이브러리 활용해서 큐 자료 구조 사용하기</h3><ul><li><strong>queue 라이브러리에는 다양한 큐 구조로 Queue(), LifoQueue(), PriorityQueue() 제공</strong></li><li><font color="#BF360C">프로그램을 작성할 때 프로그램에 따라 적합한 자료 구조를 사용</font><ul><li>Queue(): 가장 일반적인 큐 자료 구조</li><li>LifoQueue(): 나중에 입력된 데이터가 먼저 출력되는 구조 (스택 구조라고 보면 됨)</li><li>PriorityQueue(): 데이터마다 우선순위를 넣어서, 우선순위가 높은 순으로 데이터 출력</li></ul></li></ul><blockquote><p>일반적인 큐 외에 다양한 정책이 적용된 큐들이 있음</p></blockquote><h4 id="3-1-Queue-로-큐-만들기-가장-일반적인-큐-FIFO-First-In-First-Out"><a href="#3-1-Queue-로-큐-만들기-가장-일반적인-큐-FIFO-First-In-First-Out" class="headerlink" title="3.1. Queue()로 큐 만들기 (가장 일반적인 큐, FIFO(First-In, First-Out))"></a>3.1. Queue()로 큐 만들기 (가장 일반적인 큐, FIFO(First-In, First-Out))</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import queue</span><br><span class="line"></span><br><span class="line">data_queue = queue.Queue()</span><br><span class="line">data_queue.put(<span class="string">"funcoding"</span>)</span><br><span class="line">data_queue.put(1)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.qsize()</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.get()</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'funcoding'</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.qsize()</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.get()</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'funcoding'</span></span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.qsize()</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure><h4 id="3-2-LifoQueue-로-큐-만들기-LIFO-Last-In-First-Out"><a href="#3-2-LifoQueue-로-큐-만들기-LIFO-Last-In-First-Out" class="headerlink" title="3.2. LifoQueue()로 큐 만들기 (LIFO(Last-In, First-Out))"></a>3.2. LifoQueue()로 큐 만들기 (LIFO(Last-In, First-Out))</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import queue</span><br><span class="line">data_queue = queue.LifoQueue()</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_queue.put(<span class="string">"funcoding"</span>)</span><br><span class="line">data_queue.put(1)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.qsize()</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure><h5 id="그냥-Queue-구조가-아닌-Lifo-Queue이기-때문에-마지막에-추가해-준-1이-출력으로-나온다"><a href="#그냥-Queue-구조가-아닌-Lifo-Queue이기-때문에-마지막에-추가해-준-1이-출력으로-나온다" class="headerlink" title="그냥 Queue 구조가 아닌 Lifo Queue이기 때문에 마지막에 추가해 준 1이 출력으로 나온다."></a>그냥 Queue 구조가 아닌 Lifo Queue이기 때문에 마지막에 추가해 준 1이 출력으로 나온다.</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.get()</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><h4 id="3-3-PriorityQueue-로-큐-만들기"><a href="#3-3-PriorityQueue-로-큐-만들기" class="headerlink" title="3.3. PriorityQueue()로 큐 만들기"></a>3.3. PriorityQueue()로 큐 만들기</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import queue</span><br><span class="line"></span><br><span class="line">data_queue = queue.PriorityQueue()</span><br></pre></td></tr></table></figure><h5 id="아래에서-보는-것과-같이-우선-순위와-값을-튜플형태로-같이-넣어준다-우선순위는-숫자가-낮을-수록-높은-우선-순위를-지닌다"><a href="#아래에서-보는-것과-같이-우선-순위와-값을-튜플형태로-같이-넣어준다-우선순위는-숫자가-낮을-수록-높은-우선-순위를-지닌다" class="headerlink" title="아래에서 보는 것과 같이 우선 순위와 값을 튜플형태로 같이 넣어준다. 우선순위는 숫자가 낮을 수록 높은 우선 순위를 지닌다."></a>아래에서 보는 것과 같이 우선 순위와 값을 튜플형태로 같이 넣어준다. 우선순위는 숫자가 낮을 수록 높은 우선 순위를 지닌다.</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_queue.put((10, <span class="string">"korea"</span>))</span><br><span class="line">data_queue.put((5, 1))</span><br><span class="line">data_queue.put((15, <span class="string">"china"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.qsize()</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.get()</span><br></pre></td></tr></table></figure><h5 id="우선순위가-제일-높은-제일-낮은-숫자인-5를-지닌-값-1이-출력된다"><a href="#우선순위가-제일-높은-제일-낮은-숫자인-5를-지닌-값-1이-출력된다" class="headerlink" title="우선순위가 제일 높은 제일 낮은 숫자인 5를 지닌 값 1이 출력된다."></a>우선순위가 제일 높은 제일 낮은 숫자인 5를 지닌 값 1이 출력된다.</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(5, 1)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.get()</span><br></pre></td></tr></table></figure><h5 id="그-다음-우선-순위인-10을-지닌-korea가-출력된다"><a href="#그-다음-우선-순위인-10을-지닌-korea가-출력된다" class="headerlink" title="그 다음 우선 순위인 10을 지닌 korea가 출력된다."></a>그 다음 우선 순위인 10을 지닌 korea가 출력된다.</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(10, <span class="string">"korea"</span>)</span><br></pre></td></tr></table></figure><h3 id="참고-어디에-Queue가-많이-쓰일까"><a href="#참고-어디에-Queue가-많이-쓰일까" class="headerlink" title="참고: 어디에 Queue가 많이 쓰일까?"></a>참고: 어디에 Queue가 많이 쓰일까?</h3><ul><li><code>멀티 태스킹을 위한 프로세스 스케쥴링 방식을 구현하기 위해 많이 사용</code>된다. (운영체제 참조)</li></ul><blockquote><p>Queue의 경우에는 장단점 보다는 (특별히 언급되는 장단점이 없음), 큐의 활용 예로 프로세스 스케쥴링 방식을 함께 이해해두는 것이 좋음</p></blockquote><h3 id="4-프로그래밍-연습"><a href="#4-프로그래밍-연습" class="headerlink" title="4. 프로그래밍 연습"></a>4. 프로그래밍 연습</h3><ul><li>연습1: 리스트 변수로 큐를 다루는 enqueue, dequeue 기능 구현해보기</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">queue_list = list()</span><br><span class="line"></span><br><span class="line">def enqueue(data):</span><br><span class="line">    queue_list.append(data)</span><br><span class="line"></span><br><span class="line">def dequeue():</span><br><span class="line">    data = queue_list[0]</span><br><span class="line">    del queue_list[0]</span><br><span class="line">    <span class="built_in">return</span> data</span><br></pre></td></tr></table></figure><ul><li>위에서 만든 queue_list 안에 0~9까지의 수를 넣는다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(10):</span><br><span class="line">    enqueue(index)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(queue_list)</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dequeue()</span><br></pre></td></tr></table></figure><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><ul><li>FIFO 구조이므로 제일 먼저 들어간 0이 출력된다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure><h4 id="파이썬-리스트를-이용한-큐-구현"><a href="#파이썬-리스트를-이용한-큐-구현" class="headerlink" title="파이썬 리스트를 이용한 큐 구현"></a>파이썬 리스트를 이용한 큐 구현</h4><ul><li><p>위에서 만들었던 function들을 이용하여 좀더 많은 기능이 포함되어 있는 하나의 class로 Queue를 구현해 볼 것이다.</p></li><li><p>Python list 자료형의 pop 메서드를 통해 dequeue를 구현할 것이다.</p></li></ul><p><img src="/image/queue_basic_structure.png" alt="Queue 구조"></p><p><img src="/image/enqueue_basic_list.png" alt="Enqueue"></p><p><img src="/image/dequeue_basic_list.png" alt="Dequeue"></p><h5 id="Enqueue"><a href="#Enqueue" class="headerlink" title="Enqueue"></a>Enqueue</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class Queue:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.container=list()</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line"><span class="comment">#         if self.container is None:</span></span><br><span class="line">        <span class="keyword">if</span> not self.container:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def enqueue(self, data):</span><br><span class="line">        self.container.append(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def dequeue(self):</span><br><span class="line">        <span class="built_in">return</span> self.container.pop(0)</span><br><span class="line"></span><br><span class="line">    def peek(self):</span><br><span class="line">        <span class="built_in">return</span> self.container[0]</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">q=Queue()</span><br><span class="line">q.enqueue(1)</span><br><span class="line">q.enqueue(2)</span><br><span class="line">q.enqueue(3)</span><br><span class="line">q.enqueue(4)</span><br><span class="line">q.enqueue(5)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> not q.empty():</span><br><span class="line">    <span class="built_in">print</span>(q.dequeue(), end=<span class="string">'  '</span>)</span><br></pre></td></tr></table></figure><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 3 4 5</span><br></pre></td></tr></table></figure><h3 id="single-linked-list를-이용해-Queue를-구현"><a href="#single-linked-list를-이용해-Queue를-구현" class="headerlink" title="single linked list를 이용해 Queue를 구현"></a>single linked list를 이용해 Queue를 구현</h3><h4 id="ADT"><a href="#ADT" class="headerlink" title="ADT"></a>ADT</h4><ul><li><p>Queue.empty() -&gt; Boolean</p><ul><li>Queue가 비어있으면 True, 아니면 False</li></ul></li><li><p>Queue.enqueue(data) -&gt; None</p><ul><li>Queue의 맨 뒤에 데이터를 쌓는다.</li></ul></li></ul><p><img src="/image/enqueue_Node_01.png" alt="Enqueue - 01"></p><blockquote><p>맨 처음 enqueue 한 데이터는 front에 위치하고 최근에 추가로 삽입해준 데이터는 rear로 위치시킨다. 각각의 Node는 next로 연결을 시켜준다.</p></blockquote><p><img src="/image/enqueue_Node_02.png" alt="Enqueue - 02"></p><blockquote><p>먼저 새로운 Node를 추가해주기 위해서는 현재 self.rear.next를 new_node를 가리키도록하고, self.rear를 new_node로 설정해주면 된다.</p></blockquote><ul><li>Queue.dequeue() -&gt; data<ul><li>Queue 맨 앞의 데이터를 삭제하면서 반환</li></ul></li></ul><p><img src="/image/dequeue_Node_01.png" alt="Dequeue - 01"></p><p><img src="/image/dequeue_Node_02.png" alt="Dequeue - 02"></p><blockquote><p>우선 Dequeue를 하면 제일 먼저 추가해놓았던 데이터의 값을 출력해주면 해당 Node를 제거해 주어야하므로, self.front의 데이터를 cur라는 변수에 따로 저자해 주는데, Node가 하나일때를 생각해 보면 self.front와 self.rear가 동일하므로 먼저 self.rear를 None으로 만들어준다. 그리고 나서 cur에 self.front를 할당해주고, self.front는 self.front.next를 할당해주며, cur.data를 출력해주면 된다.</p></blockquote><ul><li>Queue.peek() -&gt; data<ul><li>Queue 맨 앞 데이터를 반환</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.__data = data</span><br><span class="line">        self.__next = None</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        return self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def next(self):</span><br><span class="line">        return self.__next</span><br><span class="line"></span><br><span class="line">    @next.setter</span><br><span class="line">    def next(self, n):</span><br><span class="line">        self.__next=n</span><br><span class="line"></span><br><span class="line">class LQueue:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.front=None</span><br><span class="line">        self.rear=None</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        if self.front is None:</span><br><span class="line">            return True</span><br><span class="line">        return False</span><br><span class="line"></span><br><span class="line">    def enqueue(self, data):</span><br><span class="line">        new_node=Node(data)</span><br><span class="line">        if self.empty():</span><br><span class="line">            self.front=new_node</span><br><span class="line">            self.rear=new_node</span><br><span class="line">        self.rear.next = new_node</span><br><span class="line">        self.rear = new_node</span><br><span class="line"></span><br><span class="line">    def dequeue(self):</span><br><span class="line">        if self.empty():</span><br><span class="line">            return None</span><br><span class="line"></span><br><span class="line">        if self.front is self.rear:</span><br><span class="line">            self.rear = None</span><br><span class="line">        cur = self.front</span><br><span class="line">        self.front = self.front.next</span><br><span class="line">        return cur.data</span><br><span class="line"></span><br><span class="line">    def peek(self):</span><br><span class="line">        return self.front.data</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">q = LQueue()</span><br><span class="line">q.enqueue(1)</span><br><span class="line">q.enqueue(2)</span><br><span class="line">q.enqueue(3)</span><br><span class="line">q.enqueue(4)</span><br><span class="line">q.enqueue(5)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> not q.empty():</span><br><span class="line">    <span class="built_in">print</span>(q.dequeue(), end=<span class="string">'  '</span>)</span><br></pre></td></tr></table></figure><h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 3 4 5</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/03/19/data_structure_01/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering (데이터 모델링 및 챗봇 만들기)</title>
      <link>https://heung-bae-lee.github.io/2020/03/03/data_engineering_10/</link>
      <guid>https://heung-bae-lee.github.io/2020/03/03/data_engineering_10/</guid>
      <pubDate>Tue, 03 Mar 2020 14:29:20 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Spotify-데이터-유사도-모델링&quot;&gt;&lt;a href=&quot;#Spotify-데이터-유사도-모델링&quot; class=&quot;headerlink&quot; title=&quot;Spotify 데이터 유사도 모델링&quot;&gt;&lt;/a&gt;Spotify 데이터 유사도 모델링&lt;/h1&gt;&lt;ul&gt;

        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Spotify-데이터-유사도-모델링"><a href="#Spotify-데이터-유사도-모델링" class="headerlink" title="Spotify 데이터 유사도 모델링"></a>Spotify 데이터 유사도 모델링</h1><ul><li>모든 track을 다 유클리디안 거리를 계산해서 유사도를 측정하기에는 많은 양이기 때문에 해당 Artist의 track들의 audio feature 데이터에 대해 평균을 낸 값을 사용하여 Artist 끼리의 유사도를 계산할 것이다. 해당 유사도를 계산하기 위해 아래와 같이 먼저 RDS에 접속하여 table을 생성해 준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql -h spotify.cgaj5rvtgf25.ap-northeast-2.rds.amazonaws.com -P 3306 -u hb0619 -p</span><br><span class="line"></span><br><span class="line">CREATE TABLE related_artists (artist_id VARCHAR(255), y_artist VARCHAR(255), distance FLOAT, PRIMARY KEY(artist_id, y_artist)) ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure><ul><li>그 다음은 우리가 미리 만들어놓았던 Athena의 DataBase는 무엇이었는지를 확인하자. 필자는 아래와 같이 따로 Database를 만들지 않고 default로 사용했다. 또한, 입력했었던 날짜를 확인해놓아야 추후에 코드 작성시 Athena로 접속하여 만들어진 테이블들을 참조할 수 있다.</li></ul><p><img src="/image/Athena_similarity_table_create.png" alt="Athena database"></p><ul><li><p>필자는 Athena에 미리 만들어놓았던 두가지 top_tracks와 audio_features 테이블을 이용하여 유사도를 구하고 해당 유사도를 MySQL DB에 insert하는 방식으로 작업을 진행 할 것이다.</p></li><li><p>data_modeling.py</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import logging</span><br><span class="line">import pymysql</span><br><span class="line">import boto3</span><br><span class="line">import time</span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line">host = <span class="string">"end-point url"</span></span><br><span class="line">port = you port number</span><br><span class="line">username = <span class="string">"your MYSQL DB ID"</span></span><br><span class="line">database = <span class="string">"your MYSQL DB Name"</span></span><br><span class="line">password = <span class="string">"your MYSQL DB Password"</span></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        conn = pymysql.connect(host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=<span class="string">'utf8'</span>)</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line">    except:</span><br><span class="line">        logging.error(<span class="string">"could not connect to rds"</span>)</span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    athena = boto3.client(<span class="string">'athena'</span>)</span><br><span class="line"></span><br><span class="line">    query = <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        SELECT</span></span><br><span class="line"><span class="string">         artist_id,</span></span><br><span class="line"><span class="string">         AVG(danceability) AS danceability,</span></span><br><span class="line"><span class="string">         AVG(energy) AS energy,</span></span><br><span class="line"><span class="string">         AVG(loudness) AS loudness,</span></span><br><span class="line"><span class="string">         AVG(speechiness) AS speechiness,</span></span><br><span class="line"><span class="string">         AVG(acousticness) AS acousticness,</span></span><br><span class="line"><span class="string">         AVG(instrumentalness) AS instrumentalness</span></span><br><span class="line"><span class="string">        FROM</span></span><br><span class="line"><span class="string">         top_tracks t1</span></span><br><span class="line"><span class="string">        JOIN</span></span><br><span class="line"><span class="string">         audio_features t2 ON t2.id = t1.id AND CAST(t1.dt AS DATE) = DATE('2020-02-24') AND CAST(t2.dt AS DATE) = DATE('2020-02-24')</span></span><br><span class="line"><span class="string">        GROUP BY t1.artist_id</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    <span class="comment"># 위에서 DATE를 정하는 부분에서 CAST(t1.dt AS DATE) = CURRENT_DATE - INTERVAL '1' DAY 이렇게 현재날짜를 기준으로 차이나는 기간을 통해 정해줄 수 있다.</span></span><br><span class="line">    <span class="comment"># 필자는 여러번 Athena에 실행하지 않았기 때문에 최근에 Athena에 만들어 놓은 위의 두 테이블의 데이터를 직접 보고 날짜를 지정했다.</span></span><br><span class="line"></span><br><span class="line">    r = query_athena(query, athena)</span><br><span class="line">    results = get_query_result(r[<span class="string">'QueryExecutionId'</span>], athena)</span><br><span class="line">    artists = process_data(results)</span><br><span class="line"></span><br><span class="line">    query = <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        SELECT</span></span><br><span class="line"><span class="string">         MIN(danceability) AS danceability_min,</span></span><br><span class="line"><span class="string">         MAX(danceability) AS danceability_max,</span></span><br><span class="line"><span class="string">         MIN(energy) AS energy_min,</span></span><br><span class="line"><span class="string">         MAX(energy) AS energy_max,</span></span><br><span class="line"><span class="string">         MIN(loudness) AS loudness_min,</span></span><br><span class="line"><span class="string">         MAX(loudness) AS loudness_max,</span></span><br><span class="line"><span class="string">         MIN(speechiness) AS speechiness_min,</span></span><br><span class="line"><span class="string">         MAX(speechiness) AS speechiness_max,</span></span><br><span class="line"><span class="string">         ROUND(MIN(acousticness),4) AS acousticness_min,</span></span><br><span class="line"><span class="string">         MAX(acousticness) AS acousticness_max,</span></span><br><span class="line"><span class="string">         MIN(instrumentalness) AS instrumentalness_min,</span></span><br><span class="line"><span class="string">         MAX(instrumentalness) AS instrumentalness_max</span></span><br><span class="line"><span class="string">        FROM</span></span><br><span class="line"><span class="string">         audio_features</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    r = query_athena(query, athena)</span><br><span class="line">    results = get_query_result(r[<span class="string">'QueryExecutionId'</span>], athena)</span><br><span class="line">    avgs = process_data(results)[0]</span><br><span class="line"></span><br><span class="line">    metrics = [<span class="string">'danceability'</span>, <span class="string">'energy'</span>, <span class="string">'loudness'</span>, <span class="string">'speechiness'</span>, <span class="string">'acousticness'</span>, <span class="string">'instrumentalness'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> artists:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> artists:</span><br><span class="line">            dist = 0</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> metrics:</span><br><span class="line">                x = <span class="built_in">float</span>(i[k])</span><br><span class="line">                x_norm = normalize(x, <span class="built_in">float</span>(avgs[k+<span class="string">'_min'</span>]), <span class="built_in">float</span>(avgs[k+<span class="string">'_max'</span>]))</span><br><span class="line">                y = <span class="built_in">float</span>(j[k])</span><br><span class="line">                y_norm = normalize(y, <span class="built_in">float</span>(avgs[k+<span class="string">'_min'</span>]), <span class="built_in">float</span>(avgs[k+<span class="string">'_max'</span>]))</span><br><span class="line">                dist += (x_norm-y_norm)**2</span><br><span class="line"></span><br><span class="line">            dist = math.sqrt(dist) <span class="comment">## euclidean distance</span></span><br><span class="line"></span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'artist_id'</span>: i[<span class="string">'artist_id'</span>],</span><br><span class="line">                <span class="string">'y_artist'</span>: j[<span class="string">'artist_id'</span>],</span><br><span class="line">                <span class="string">'distance'</span>: dist</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            insert_row(cursor, data, <span class="string">'related_artists'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    conn.commit()</span><br><span class="line">    cursor.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def normalize(x, x_min, x_max):</span><br><span class="line"></span><br><span class="line">    normalized = (x-x_min) / (x_max-x_min)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> normalized</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def query_athena(query, athena):</span><br><span class="line">    response = athena.start_query_execution(</span><br><span class="line">        QueryString=query,</span><br><span class="line">        QueryExecutionContext=&#123;</span><br><span class="line">            <span class="string">'Database'</span>: <span class="string">'default'</span></span><br><span class="line">        &#125;,</span><br><span class="line">        ResultConfiguration=&#123;</span><br><span class="line">            <span class="string">'OutputLocation'</span>: <span class="string">"s3://spotify-chatbot-project/athena-panomix-tables/"</span>,</span><br><span class="line">            <span class="string">'EncryptionConfiguration'</span>: &#123;</span><br><span class="line">                <span class="string">'EncryptionOption'</span>: <span class="string">'SSE_S3'</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line"><span class="comment"># 아래와 같이 Athena API는 response를 받았다고 해서 결과를 보여주는 것이 아니라 실행을 시킨 후에</span></span><br><span class="line"><span class="comment"># 해당 query id를 통해 결과를 가져오는 형식으로 이루어져 있다.</span></span><br><span class="line">def get_query_result(query_id, athena):</span><br><span class="line"></span><br><span class="line">    response = athena.get_query_execution(</span><br><span class="line">        QueryExecutionId=str(query_id)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">while</span> response[<span class="string">'QueryExecution'</span>][<span class="string">'Status'</span>][<span class="string">'State'</span>] != <span class="string">'SUCCEEDED'</span>:</span><br><span class="line">        <span class="keyword">if</span> response[<span class="string">'QueryExecution'</span>][<span class="string">'Status'</span>][<span class="string">'State'</span>] == <span class="string">'FAILED'</span>:</span><br><span class="line">            logging.error(<span class="string">'QUERY FAILED'</span>)</span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line">        time.sleep(5)</span><br><span class="line">        response = athena.get_query_execution(</span><br><span class="line">            QueryExecutionId=str(query_id)</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># 중요한 점은 MaxResults가 1000이 Max라는 점이다.</span></span><br><span class="line">    response = athena.get_query_results(</span><br><span class="line">        QueryExecutionId=str(query_id)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def process_data(results):</span><br><span class="line"></span><br><span class="line">    columns = [col[<span class="string">'Label'</span>] <span class="keyword">for</span> col <span class="keyword">in</span> results[<span class="string">'ResultSet'</span>][<span class="string">'ResultSetMetadata'</span>][<span class="string">'ColumnInfo'</span>]]</span><br><span class="line"></span><br><span class="line">    listed_results = []</span><br><span class="line">    <span class="keyword">for</span> res <span class="keyword">in</span> results[<span class="string">'ResultSet'</span>][<span class="string">'Rows'</span>][1:]:</span><br><span class="line">        values = []</span><br><span class="line">        <span class="keyword">for</span> field <span class="keyword">in</span> res[<span class="string">'Data'</span>]:</span><br><span class="line">            try:</span><br><span class="line">                values.append(list(field.values())[0])</span><br><span class="line">            except:</span><br><span class="line">                values.append(list(<span class="string">' '</span>))</span><br><span class="line">        listed_results.append(dict(zip(columns, values)))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> listed_results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def insert_row(cursor, data, table):</span><br><span class="line"></span><br><span class="line">    placeholders = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">    columns = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">    key_placeholders = <span class="string">', '</span>.join([<span class="string">'&#123;0&#125;=%s'</span>.format(k) <span class="keyword">for</span> k <span class="keyword">in</span> data.keys()])</span><br><span class="line">    sql = <span class="string">"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s"</span> % (table, columns, placeholders, key_placeholders)</span><br><span class="line">    cursor.execute(sql, list(data.values())*2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><ul><li>위의 파일을 실행시켜보자. 위의 python script 파일이 존재하는 path로 이동하여 아래 명령문을 실행시키면 실행에 완료될때까지 걸린 시간 또한 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">time python3 data_modeling.py</span><br><span class="line"></span><br><span class="line">real28m11.013s</span><br><span class="line">user1m36.141s</span><br><span class="line">sys0m24.518s</span><br></pre></td></tr></table></figure><ul><li>이제 MySQL에 접속해서 데이터가 제대로 insert 됬는지 확인해 보자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM related_artists LIMIT 20;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line">+------------------------+------------------------+-----------+</span><br><span class="line">| artist_id              | y_artist               | distance  |</span><br><span class="line">+------------------------+------------------------+-----------+</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 00FQb4jTyendYWaN8pK0wa |         0 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 01C9OoXDvCKkGcf735Tcfo |  0.366558 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 02rd0anEWfMtF7iMku9uor |  0.327869 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 02uYdhMhCgdB49hZlYRm9o |  0.595705 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 03r4iKL2g2442PT9n2UKsx |  0.632109 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 03YhcM6fxypfwckPCQV8pQ |  0.812604 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 04gDigrS5kc9YWfZHwBETP |  0.498764 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 04tBaW21jyUfeP5iqiKBVq |  0.322017 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 0543y7yrvny4KymoaneT4W |  0.365608 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 05E3NBxNMdnrPtxF9oraJm |  0.958604 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 06HL4z0CvFAxyc27GXpf02 |  0.483454 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 06nevPmNVfWUXyZkccahL8 | 0.0592581 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 06nsZ3qSOYZ2hPVIMcr1IN |   0.39567 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 085pc2PYOi8bGKj0PNjekA |  0.608243 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 08avsqaGIlK2x3i2Cu7rKH |  0.328059 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 09C0xjtosNAIXP36wTnWxd |  0.210568 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 0BvkDsjIUla7X0k6CSWh1I |  0.606556 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 0bvRYuXRvd14RYEE7c0PRW |  0.670187 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 0C0XlULifJtAgn6ZNCW2eu |   0.70478 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 0cc6vw3VN8YlIcvr1v7tBL |  0.716507 |</span><br><span class="line">+------------------------+------------------------+-----------+</span><br><span class="line">20 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.01 sec)</span><br></pre></td></tr></table></figure><ul><li>JOIN을 통해 각각 이름을 볼수있게 해주면서 가장 distance가 작은 즉 유사성이 큰 데이터 순서로 보여주길 원해 아래와 같은 query를 작성하여 실행시켰다. 그 결과, audio_features로만 모델링을 했음에도 비슷한 장르의 아티스트가 묶여있음을 확인할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">SELECT p1.name, p2.name, p1.url, p2.url, p2.distance FROM artists p1 JOIN (SELECT t1.name, t1.url, t2.y_artist, t2.distance FROM artists t1 JOIN related_artists t2 ON t2.artist_id = t1.id) p2 ON p2.y_artist=p1.id WHERE distance != 0 ORDER BY p2.distance ASC LIMIT 20;</span><br><span class="line"></span><br><span class="line">+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+</span><br><span class="line">| name                          | name                          | url                                                    | url                                                    | distance  |</span><br><span class="line">+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+</span><br><span class="line">| Four Tops                     | Alan Jackson                  | https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4 | https://open.spotify.com/artist/4mxWe1mtYIYfP040G38yvS | 0.0241995 |</span><br><span class="line">| Alan Jackson                  | Four Tops                     | https://open.spotify.com/artist/4mxWe1mtYIYfP040G38yvS | https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4 | 0.0241995 |</span><br><span class="line">| Martha Reeves &amp; The Vandellas | Jimmy Ruffin                  | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | https://open.spotify.com/artist/0hF0PwB04hnXfYMiZWfJzy | 0.0258624 |</span><br><span class="line">| Jimmy Ruffin                  | Martha Reeves &amp; The Vandellas | https://open.spotify.com/artist/0hF0PwB04hnXfYMiZWfJzy | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | 0.0258624 |</span><br><span class="line">| George Harrison               | Martha Reeves &amp; The Vandellas | https://open.spotify.com/artist/7FIoB5PHdrMZVC3q2HE5MS | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | 0.0272287 |</span><br><span class="line">| Martha Reeves &amp; The Vandellas | George Harrison               | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | https://open.spotify.com/artist/7FIoB5PHdrMZVC3q2HE5MS | 0.0272287 |</span><br><span class="line">| Nik Kershaw                   | Elton John                    | https://open.spotify.com/artist/7kCL98rPFsNKjAHDmWrMac | https://open.spotify.com/artist/3PhoLpVuITZKcymswpck5b | 0.0272474 |</span><br><span class="line">| Elton John                    | Nik Kershaw                   | https://open.spotify.com/artist/3PhoLpVuITZKcymswpck5b | https://open.spotify.com/artist/7kCL98rPFsNKjAHDmWrMac | 0.0272474 |</span><br><span class="line">| Tammi Terrell                 | Kim Carnes                    | https://open.spotify.com/artist/75jNCko3SnEMI5gwGqrbb8 | https://open.spotify.com/artist/5PN2aHIvLEM98XIorsPMhE | 0.0279891 |</span><br><span class="line">| Kim Carnes                    | Tammi Terrell                 | https://open.spotify.com/artist/5PN2aHIvLEM98XIorsPMhE | https://open.spotify.com/artist/75jNCko3SnEMI5gwGqrbb8 | 0.0279891 |</span><br><span class="line">| Roger Daltrey                 | Arcade Fire                   | https://open.spotify.com/artist/5odf7hjI7hyvAw66tmxhGF | https://open.spotify.com/artist/3kjuyTCjPG1WMFCiyc5IuB | 0.0291541 |</span><br><span class="line">| Arcade Fire                   | Roger Daltrey                 | https://open.spotify.com/artist/3kjuyTCjPG1WMFCiyc5IuB | https://open.spotify.com/artist/5odf7hjI7hyvAw66tmxhGF | 0.0291541 |</span><br><span class="line">| Billy Fury                    | Otis Redding                  | https://open.spotify.com/artist/7rtLZcKWGV4eaZsBwSKimf | https://open.spotify.com/artist/60df5JBRRPcnSpsIMxxwQm | 0.0292248 |</span><br><span class="line">| Otis Redding                  | Billy Fury                    | https://open.spotify.com/artist/60df5JBRRPcnSpsIMxxwQm | https://open.spotify.com/artist/7rtLZcKWGV4eaZsBwSKimf | 0.0292248 |</span><br><span class="line">| Katy Perry                    | John Fogerty                  | https://open.spotify.com/artist/6jJ0s89eD6GaHleKKya26X | https://open.spotify.com/artist/5ujCegv1BRbEPTCwQqFk6t | 0.0302168 |</span><br><span class="line">| John Fogerty                  | Katy Perry                    | https://open.spotify.com/artist/5ujCegv1BRbEPTCwQqFk6t | https://open.spotify.com/artist/6jJ0s89eD6GaHleKKya26X | 0.0302168 |</span><br><span class="line">| Dierks Bentley                | The Cadillac Three            | https://open.spotify.com/artist/7x8nK0m0cP2ksQf0mjWdPS | https://open.spotify.com/artist/1nivFfWu6oXBFDNyVfFU5x | 0.0313435 |</span><br><span class="line">| The Cadillac Three            | Dierks Bentley                | https://open.spotify.com/artist/1nivFfWu6oXBFDNyVfFU5x | https://open.spotify.com/artist/7x8nK0m0cP2ksQf0mjWdPS | 0.0313435 |</span><br><span class="line">| Sheryl Crow                   | Phil Collins                  | https://open.spotify.com/artist/4TKTii6gnOnUXQHyuo9JaD | https://open.spotify.com/artist/4lxfqrEsLX6N1N4OCSkILp | 0.0317203 |</span><br><span class="line">| Phil Collins                  | Sheryl Crow                   | https://open.spotify.com/artist/4lxfqrEsLX6N1N4OCSkILp | https://open.spotify.com/artist/4TKTii6gnOnUXQHyuo9JaD | 0.0317203 |</span><br><span class="line">+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+</span><br><span class="line">20 rows <span class="keyword">in</span> <span class="built_in">set</span> (1.18 sec)</span><br></pre></td></tr></table></figure><ul><li>이제 Facebook messenger API를 통해 챗봇을 서비스를 만들어 볼 것이다. 아래 그림과 같이 구글에서 Facebook messenger API를 검색하여 페이지로 접속한다.</li></ul><p><img src="/image/search_Facebook_messenger_API.png" alt="Facebook messenger API 검색"></p><ul><li>Facebook messenger API 웹페이지를 접속하면 아래 그림처럼 보이며, 그에 대한 작동원리에 대한 설명은 Introduction의 learn more를 클릭하면 두번째 그림과 같이 작동원리를 보여준다. 간단히 말하자면, User가 Facebook messenger API를 통해 질의하면 그 정보를 Business Server에 보내서 해당 질문에 따른 답변을 가져와 보여주는 형식이다.</li></ul><p><img src="/image/introduction_click_facebook_messenger_api.png" alt="Facebook messenger API 웹페이지"></p><p><img src="/image/Facebook_messenger_plattform_principal.png" alt="Facebook messenger API 작동방식"></p><ul><li>Facebook messenger API를 통해서는 다양한 방식의 답변을 제공할 수 있다. 이미 만들어져있는 UI/UX 템플릿들이 존재하기 때문에 원하는 형식에 맞춰 다양하게 서비스를 제공할 수 있다.</li></ul><p><img src="/image/Quick_answer_Facebook_messenger_api.png" alt="FaceBook messenger API의 Quick Answer"></p><p><img src="/image/Facebook_API_Messenge_template.png" alt="Facebook messenger의 message 템플릿"></p><ul><li><code>Facebook messenger API를 사용하기 위해서는 가장 먼저 페이지가 만들어져 있어야 한다.</code> 아래와 같이 새로운 페이지를 만들거나 이미 만들어져 있는 자신의 페이지를 먼저 등록시킨다.</li></ul><p><img src="/image/create_page_facebook_messenger_api_01.png" alt="페이지 생성 - 01"></p><p><img src="/image/create_page_facebook_messenger_api_02.png" alt="페이지 생성 - 02"></p><p><img src="/image/create_page_facebook_messenger_api_03.png" alt="페이지 생성 - 03"></p><p><img src="/image/create_page_facebook_messenger_api_04.png" alt="페이지 생성 - 04"></p><p><img src="/image/create_page_facebook_messenger_api_05.png" alt="페이지 생성 - 05"></p><ul><li>Lambda를 통해서 AWS와 Facebook messenger API를 연결해 볼 것이다. Lambda를 사용하는 이유는 지난번에 언급했던 것과 같이 EC2와 같이 서버를 항상 띄어놓고 정해진 resource를 통해 서비스를 관리하면 늘어나거나 줄어드는 User에 대해서 유연하게 처리하기 힘들기 때문이다. Lambda는 예를 들어 기하급수적으로 User가 늘더라도 그에따라 병렬적으로 작업하기 때문에 Traffic의 크기에 크게 영향을 받지 않는다. 반대로 EC2의 경우에는 해당 Traffic이 증가함에 따라 여러가지 장치를 구현해 놓아야한다. AWS에 로그인한 후 Lambda를 들어가서, 아래 그림과 같이 새로운 Lambda Function을 생성해 준다.</li></ul><p><img src="/image/new_create_lambda_function_aws_01.png" alt="Facebook messenger API를 사용하기 위한 Lambda Function 생성 - 01"></p><p><img src="/image/new_create_lambda_function_aws_02.png" alt="Facebook messenger API를 사용하기 위한 Lambda Function 생성 - 02"></p><p><img src="/image/new_create_lambda_function_aws_03.png" alt="Facebook messenger API를 사용하기 위한 Lambda Function 생성 - 03"></p><p><img src="/image/new_create_lambda_function_aws_04.png" alt="Facebook messenger API를 사용하기 위한 Lambda Function 생성 - 04"></p><ul><li>이제 위에서 만든 Lambda Function과 Facebook messenger API를 연결하기 위해서 AWS에서 API 관리 및 설정을 담당하는 API Gateway 페이지로 이동해서 새롭게 API Gate를 만들 것이다.</li></ul><p><img src="/image/AWS_api_gate_service.png" alt="AWS API Gateway service"></p><p><img src="/image/Amazon_API_Gateway.png" alt="AWS API Gateway 생성 - 01"></p><ul><li>필자는 REST API 방식으로 생성할 것이기 때문에 아래와 같이 설정하였으며, API이름도 설정해 주었다.</li></ul><p><img src="/image/Amazon_API_Gateway_creation_01.png" alt="AWS API Gateway 생성 - 02"></p><ul><li>그 다음은 Crawling할 때 한번씩 접해봤을 법한 GET, POST Method를 만들어 주는 과정을 거친다. 먼저 GET은 Integration type을 Lambda Function으로 설정해 주고, Lambda Function은 방금 만들어놓은 것을 사용할 것이다. 이렇게 설정한 뒤에 Integration Request 탭으로 이동하여 Mapping Templates의 Request body passthrough 아래와같이 설정하며, <code>mapping Templates에 application/json을 추가</code>해준다. Generate template을 Method Request passthrough로 설정한 후 최종적으로 save를 하여 GET method의 설정을 마친다. 필자가 사용할 서비스에서는 POST method는 Facebook API에 데이터를 주는 역할 밖에 없기 때문에 크게 설정할 것이 없다.</li></ul><p><img src="/image/Amazon_API_Gateway_creation_02.png" alt="AWS API Gateway 생성 - 03"></p><p><img src="/image/Amazon_API_Gateway_creation_03.png" alt="AWS API Gateway 생성 - 04"></p><p><img src="/image/Amazon_API_Gateway_creation_04.png" alt="AWS API Gateway 생성 - 05"></p><p><img src="/image/Amazon_API_Gateway_creation_05.png" alt="AWS API Gateway 생성 - 06"></p><p><img src="/image/Amazon_API_Gateway_creation_06.png" alt="AWS API Gateway 생성 - 07"></p><p><img src="/image/Amazon_API_Gateway_creation_07.png" alt="AWS API Gateway 생성 - 08"></p><p><img src="/image/Amazon_API_Gateway_creation_08.png" alt="AWS API Gateway 생성 - 09"></p><ul><li>GET, POST method를 다 설정했다면, 사용하기 위해서는 배포를 해야 할 것이다. 아래 그림과 같이 action버튼을 눌러 deploy api를 선택하여 stage를 새롭게 만들어주며, 이름을 설정한다.</li></ul><p><img src="/image/Action_deploy_API_AWS.png" alt="API Gateway 배포 - 01"></p><p><img src="/image/deplot_api_stage_name_AWS.png" alt="API Gateway 배포 - 02"></p><ul><li>deploy를 다 완료하게 되면, 아래 그림과 같은 화면이 나타날 것이다. 그 중 아래 빨간색 상자 안에 있는 invoke URL은 우리가 Facebook에 연결해 줄 endpoint 역할을 한다. 추후에 invoke URL 주소를 복사한 후에 아래 그림에서와 같이 Facebook에서 만들어 놓은 app의 콜백 url 추가를 눌러 추가해 줄 것이다.</li></ul><p><img src="/iamge/callback_ural_adding.png" alt="Facebook messenger API webhook url 추가"></p><ul><li>위의 그림 처럼 webhook url을 추가 해주려면 Lambda Function을 만들어 주어야 하는데 먼저 아래 그림에서와 토큰을 생성해서 복사한 후 Lambda Function을 아래 그림과 같이 작성해 준뒤에 webhook url을 추가해 줄 수 있다. 참고로 페이지 토큰은 Facebook app에서 page token을 생성하여 해당 값을 적어주고, verify token은 임으로 지정해주면 된다.</li></ul><p><img src="/image/page_token_access_create.png" alt="페이지 토큰 생성 - 01"></p><p><img src="/image/page_token_creation.png" alt="페이지 토큰 생성 - 02"></p><ul><li>아래와 같이 Lambda Function을 수정하는 이유는 <a href="https://developers.facebook.com/docs/messenger-platform/webhook#setup" target="_blank" rel="noopener">Facebook의 Webhook 사용법</a>을 살펴보면 알 수 있다.</li></ul><p><img src="/image/modify_lambda_Function_for_webhook_url.png" alt="webhook url 추가를 위한 Lambda Function 수정"></p><ul><li>위에서 지정한 verify token과 아래 그림에서 처럼 API Gateway를 클릭하여 이전에 invoke URL의 주소를 복사하여 아래 그림과 같이 webhook url을 추가해준다. 이렇게 하면 connection은 완료한 상태이다.</li></ul><p><img src="/image/API_Gateway_invoke_url.png" alt="invoke URL"></p><p><img src="/image/callback_url_adding_api.png" alt="callback URL 추가"></p><ul><li>이제 본격적으로 챗봇을 구현하기 위해 Lambda Function의 else 밑의 부분을 수정해 볼 것이다. 먼저 이전과 마찬가지로 Lambda Function은 S3에 올려 그 파일을 사용하기 위해 requirements.txt와 shell script를 포함하는 하나의 파일로 만들어 준다. 또한, AWS에서 S3에 새로운 bucket을 생성해준다. 필자는 아래와 같이 spotify-chat-bot이라는 이름으로 새롭게 bucket을 만들어 주었다.</li></ul><p><img src="/image/new_S3_bucket_creation_for_lambda_function.png" alt="새로운 S3 bucket 생성"></p><ul><li><p>전체적인 구조는 아래와 같다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chatbot</span><br><span class="line">├── deploy.sh</span><br><span class="line">├── lambda_handler.py</span><br><span class="line">└── requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>deploy.sh</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">rm -rf ./libs</span><br><span class="line">pip install -r requirements.txt -t ./libs</span><br><span class="line"></span><br><span class="line">rm *.zip</span><br><span class="line">zip spotify.zip -r *</span><br><span class="line"></span><br><span class="line">aws s3 rm s3://spotify-chat-bot/spotify.zip</span><br><span class="line">aws s3 cp ./spotify.zip s3://spotify-chat-bot/spotify.zip</span><br><span class="line">aws lambda update-function-code --<span class="keyword">function</span>-name spotify-lambda --s3-bucket spotify-chat-bot --s3-key spotify.zip</span><br></pre></td></tr></table></figure><ul><li><p>위와 같이 작성했다면 먼저 deploy.sh의 파일 권한을 바꿔준다. 모든 사용자(a)의 실행(x) 권한 추가(+)하여 준다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x deploy.sh</span><br></pre></td></tr></table></figure></li><li><p>requirements.txt</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">requests</span><br><span class="line">pymysql</span><br></pre></td></tr></table></figure><ul><li>lambda_hendler.py</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">import sys</span><br><span class="line">sys.path.append(<span class="string">'./libs'</span>)</span><br><span class="line">import logging</span><br><span class="line">import requests</span><br><span class="line">import pymysql</span><br><span class="line">import fb_bot</span><br><span class="line">import json</span><br><span class="line">import base64</span><br><span class="line">import boto3</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger()</span><br><span class="line">logger.setLevel(logging.INFO)</span><br><span class="line"></span><br><span class="line">PAGE_TOKEN = <span class="string">"your page token"</span></span><br><span class="line">VERIFY_TOKEN = <span class="string">"your verify code"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lambda_handler(event, context):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># event['params'] only exists for HTTPS GET</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'params'</span> <span class="keyword">in</span> event.keys():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> event[<span class="string">'params'</span>][<span class="string">'querystring'</span>][<span class="string">'hub.verify_token'</span>] == VERIFY_TOKEN:</span><br><span class="line">            <span class="built_in">return</span> int(event[<span class="string">'params'</span>][<span class="string">'querystring'</span>][<span class="string">'hub.challenge'</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            logging.error(<span class="string">'wrong validation token'</span>)</span><br><span class="line">            raise SystemExit</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        logger.info(event)</span><br></pre></td></tr></table></figure><ul><li>위와 같이 작성한 상태한 후 해당 파일이 존재하는 path에서 아래 shell script를 작동시킨다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./deploy.sh</span><br></pre></td></tr></table></figure><ul><li>AWS S3에서 해당 bucket을 확인해 보면 아래와 같이 spotify.zip 파일이 존재함을 확인할 수 있다.</li></ul><p><img src="/image/bucket_check_adding_file.png" alt="bucket 업로드 확인"></p><ul><li>다시 AWS Lambda Function으로 돌아가 해당 bucket과 연결을 시켜 볼 것이다.</li></ul><p><img src="/image/bucket_connect_with_Lambda.png" alt="Lambda Function에 bucket 파일 연결하기"></p><ul><li>Facebook App으로 돌아가서 아래 화면과 같이 필드를 추가해 주어야 한다.</li></ul><p><img src="/image/feed_add_facebook_api.png" alt="Facebook messenger API 필드 추가 - 01"></p><p><img src="/image/feed_add_facebook_api_01.png" alt="Facebook messenger API 필드 추가 - 02"></p><ul><li>이제 webhook으로 연결해 놓은 페이지로 접속하여 아래와 같이 버튼을 만들어 놓는다. 그 이유는 Lambda Function의 나머지 부분을 작성하기 위해서는 어떻게 event 구조가 구성되어져 있는지 확인해야 하기 때문이다.</li></ul><p><img src="/image/webhook_page_add_button_01.png" alt="webhook 페이지 버튼 추가 - 01"></p><p><img src="/image/webhook_page_add_button_02.png" alt="webhook 페이지 버튼 추가 - 02"></p><p><img src="/image/webhook_page_add_button_03.png" alt="webhook 페이지 버튼 추가 - 03"></p><ul><li>이제 AWS Lambda Function을 통해 어떻게 message가 들어오는지 확인하기 위해 아래 그림과 같이 page에서 버튼 테스트를 진행하고, 메세지는 간단하게 hello를 입력해보았다.</li></ul><p><img src="/image/button_test_01.png" alt="페이지 버튼 테스트 - 01"></p><p><img src="/image/button_test_02.png" alt="페이지 버튼 테스트 - 02"></p><ul><li>AWS CloudWatch에서 log를 살펴보면, 아래 그림과 같이 받아오는 것을 확인 할 수 있다. 아래 빨간색 상자안의 key 값 중 recipient는 해당 페이지의 id이며, sender의 id는 Facebook User의 id이다. 고유의 값은 아니고 각 페이지에 각 User에 대한 id이므로 동일한 User가 다른 페이지에서 요청을 했다면, 다른 id를 갖는다.</li></ul><p><img src="/image/button_test_03.png" alt="페이지 버튼 테스트 - 03"></p><ul><li>app을 관리할 수 있는 python script 파일을 fb_bot.py라는 이름으로 작성해 주었다. 이 파일 또한 위의 lambda function내에 존재할 수 있도록 path를 잡아주어야 한다. Facebook app은 <a href="https://developers.facebook.com/docs/graph-api/using-graph-api" target="_blank" rel="noopener">graph Facebook API</a>를 통해서 control할 수 있다. 아래 패키지 중 <a href="https://python.flowdas.com/library/enum.html" target="_blank" rel="noopener">Enum</a>은 고유한 이름 집합과 값을 정의하는 데 사용할 수 있는 네 가지 열거형 클래스를 정의하는데 사용되어 진다. 아래함수에서 for문을 통해 NotificationType을 작동시킨다면, NotificationType.REGULAR, NotificationType.SILENT_PUSH, NotificationType.no_push 식으로 값이 프린트 된다. 아래 탬플릿에 맞는 형식은 <a href="https://developers.facebook.com/docs/messenger-platform/send-messages/templates" target="_blank" rel="noopener">Facebook messenger API</a>에서 확인할 수 있다.</li></ul><p><img src="/image/send_API_messenger.png" alt="fb_bot.py에 사용된 템플릿 - 01"></p><ul><li>fb_bot.py</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line">sys.path.append(<span class="string">"./libs"</span>)</span><br><span class="line">import os</span><br><span class="line">import requests</span><br><span class="line">import base64</span><br><span class="line">import json</span><br><span class="line">import logging</span><br><span class="line">from enum import Enum</span><br><span class="line"></span><br><span class="line">DEFAULT_API_VERSION = 6.0</span><br><span class="line"></span><br><span class="line"><span class="comment">## messaging types: "RESPONSE", "UPDATE", "MESSAGE_TAG"</span></span><br><span class="line"></span><br><span class="line">class NotificationType(Enum):</span><br><span class="line">    regular = <span class="string">"REGULAR"</span></span><br><span class="line">    silent_push = <span class="string">"SILENT_PUSH"</span></span><br><span class="line">    no_push = <span class="string">"no_push"</span></span><br><span class="line"></span><br><span class="line">class Bot:</span><br><span class="line"></span><br><span class="line">    def __init__(self, access_token, **kwargs):</span><br><span class="line"></span><br><span class="line">        self.access_token = access_token</span><br><span class="line">        self.api_version = kwargs.get(<span class="string">'api_version'</span>) or DEFAULT_API_VERSION</span><br><span class="line">        self.graph_url = <span class="string">'https://graph.facebook.com/v&#123;0&#125;'</span>.format(self.api_version)</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def auth_args(self):</span><br><span class="line">        <span class="keyword">if</span> not hasattr(self, <span class="string">'_auth_args'</span>):</span><br><span class="line">            auth = &#123;</span><br><span class="line">                <span class="string">'access_token'</span>: self.access_token</span><br><span class="line">            &#125;</span><br><span class="line">            self._auth_args = auth</span><br><span class="line">        <span class="built_in">return</span> self._auth_args</span><br><span class="line"></span><br><span class="line">    def send_message(self, recipient_id, payload, notification_type, messaging_type, tag):</span><br><span class="line"></span><br><span class="line">        payload[<span class="string">'recipient'</span>] = &#123;</span><br><span class="line">            <span class="string">'id'</span>: recipient_id</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#payload['notification_type'] = notification_type</span></span><br><span class="line">        payload[<span class="string">'messaging_type'</span>] = messaging_type</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tag is not None:</span><br><span class="line">            payload[<span class="string">'tag'</span>] = tag</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/messages'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.post(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args,</span><br><span class="line">            json = payload</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        logging.info(payload)</span><br><span class="line">        <span class="built_in">return</span> response.json()</span><br><span class="line"></span><br><span class="line">    def send_text(self, recipient_id, text, notification_type = NotificationType.regular, messaging_type = <span class="string">'RESPONSE'</span>, tag = None):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> self.send_message(</span><br><span class="line">            recipient_id,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"message"</span>: &#123;</span><br><span class="line">                    <span class="string">"text"</span>: text</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            notification_type,</span><br><span class="line">            messaging_type,</span><br><span class="line">            tag</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def send_quick_replies(self, recipient_id, text, quick_replies, notification_type = NotificationType.regular, messaging_type = <span class="string">'RESPONSE'</span>, tag = None):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> self.send_message(</span><br><span class="line">            recipient_id,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"message"</span>:&#123;</span><br><span class="line">                    <span class="string">"text"</span>: text,</span><br><span class="line">                    <span class="string">"quick_replies"</span>: quick_replies</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            notification_type,</span><br><span class="line">            messaging_type,</span><br><span class="line">            tag</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def send_attachment(self, recipient_id, attachment_type, payload, notification_type = NotificationType.regular, messaging_type = <span class="string">'RESPONSE'</span>, tag = None):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> self.send_message(</span><br><span class="line">            recipient_id,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"message"</span>: &#123;</span><br><span class="line">                    <span class="string">"attachment"</span>:&#123;</span><br><span class="line">                        <span class="string">"type"</span>: attachment_type,</span><br><span class="line">                        <span class="string">"payload"</span>: payload</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            notification_type,</span><br><span class="line">            messaging_type,</span><br><span class="line">            tag</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def send_action(self, recipient_id, action, notification_type = NotificationType.regular, messaging_type = <span class="string">'RESPONSE'</span>, tag = None):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> self.send_message(</span><br><span class="line">            recipient_id,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"sender_action"</span>: action</span><br><span class="line">            &#125;,</span><br><span class="line">            notification_type,</span><br><span class="line">            messaging_type,</span><br><span class="line">            tag</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def whitelist_domain(self, domain_list, domain_action_type):</span><br><span class="line"></span><br><span class="line">        payload = &#123;</span><br><span class="line">            <span class="string">"setting_type"</span>: <span class="string">"domain_whitelisting"</span>,</span><br><span class="line">            <span class="string">"whitelisted_domains"</span>: domain_list,</span><br><span class="line">            <span class="string">"domain_action_type"</span>: domain_action_type</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/thread_settings'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.post(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args,</span><br><span class="line">            json = payload</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response.json()</span><br><span class="line"></span><br><span class="line">    def set_greeting(self, template):</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/thread_settings'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.post(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args,</span><br><span class="line">            json = &#123;</span><br><span class="line">                <span class="string">"setting_type"</span>: <span class="string">"greeting"</span>,</span><br><span class="line">                <span class="string">"greeting"</span>: &#123;</span><br><span class="line">                    <span class="string">"text"</span>: template</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line">    def set_get_started(self, text):</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/messenger_profile'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.post(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args,</span><br><span class="line">            json = &#123;</span><br><span class="line">                <span class="string">"get_started"</span>:&#123;</span><br><span class="line">                    <span class="string">"payload"</span>: text</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line">    def get_get_started(self):</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/messenger_profile?fields=get_started'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.get(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line">    def get_messenger_profile(self, field):</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/messenger_profile?fields=&#123;1&#125;'</span>.format(self.graph_url, field)</span><br><span class="line"></span><br><span class="line">        response = requests.get(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def upload_attachment(self, url):</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/message_attachments'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.post(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args,</span><br><span class="line">            json = &#123;</span><br><span class="line">                <span class="string">"message"</span>:&#123;</span><br><span class="line">                    <span class="string">"attachment"</span>:&#123;</span><br><span class="line">                        <span class="string">"type"</span>: <span class="string">"image"</span>,</span><br><span class="line">                        <span class="string">"payload"</span>: &#123;</span><br><span class="line">                            <span class="string">"is_reusable"</span>: True,</span><br><span class="line">                            <span class="string">"url"</span>: url</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response</span><br></pre></td></tr></table></figure><ul><li><p>이제 위의 fb_bot.py를 import하여 lambda_hendler.py 파일을 아래와 같이 수정해 주었다.</p></li><li><p>lambda_hendler.py</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">import sys</span><br><span class="line">sys.path.append(<span class="string">'./libs'</span>)</span><br><span class="line">import logging</span><br><span class="line">import requests</span><br><span class="line">import pymysql</span><br><span class="line">import fb_bot</span><br><span class="line">import json</span><br><span class="line">import base64</span><br><span class="line">import boto3</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger()</span><br><span class="line">logger.setLevel(logging.INFO)</span><br><span class="line"></span><br><span class="line">client_id = <span class="string">"Your Spotify ID"</span></span><br><span class="line">client_secret = <span class="string">"Your Spotify PW"</span></span><br><span class="line"></span><br><span class="line">PAGE_TOKEN = <span class="string">"Your Page Token"</span></span><br><span class="line">VERIFY_TOKEN = <span class="string">"Your verify token"</span></span><br><span class="line"></span><br><span class="line">host = <span class="string">"Your RDS End point"</span></span><br><span class="line">port = 3306</span><br><span class="line">username = <span class="string">"Your RDS ID"</span></span><br><span class="line">database = <span class="string">"Using RDS table name"</span></span><br><span class="line">password = <span class="string">"Your RDS PW"</span></span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    conn = pymysql.connect(host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=<span class="string">'utf8'</span>)</span><br><span class="line">    cursor = conn.cursor()</span><br><span class="line">except:</span><br><span class="line">    logging.error(<span class="string">"could not connect to rds"</span>)</span><br><span class="line">    sys.exit(1)</span><br><span class="line"></span><br><span class="line">bot = fb_bot.Bot(PAGE_TOKEN)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lambda_handler(event, context):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># event['params'] only exists for HTTPS GET</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'params'</span> <span class="keyword">in</span> event.keys():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> event[<span class="string">'params'</span>][<span class="string">'querystring'</span>][<span class="string">'hub.verify_token'</span>] == VERIFY_TOKEN:</span><br><span class="line">            <span class="built_in">return</span> int(event[<span class="string">'params'</span>][<span class="string">'querystring'</span>][<span class="string">'hub.challenge'</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            logging.error(<span class="string">'wrong validation token'</span>)</span><br><span class="line">            raise SystemExit</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        messaging = event[<span class="string">'entry'</span>][0][<span class="string">'messaging'</span>][0]</span><br><span class="line">        user_id = messaging[<span class="string">'sender'</span>][<span class="string">'id'</span>]</span><br><span class="line"></span><br><span class="line">        logger.info(messaging)</span><br><span class="line">        artist_name = messaging[<span class="string">'message'</span>][<span class="string">'text'</span>]</span><br><span class="line"></span><br><span class="line">        query = <span class="string">"SELECT image_url, url FROM artists WHERE name = '&#123;&#125;'"</span>.format(artist_name)</span><br><span class="line">        cursor.execute(query)</span><br><span class="line">        raw = cursor.fetchall()</span><br><span class="line">        <span class="comment"># raw가 0인 경우는 DB안에는 존재하지 않으므로 Spotify API에서 직접 search한다.</span></span><br><span class="line">        <span class="keyword">if</span> len(raw) == 0:</span><br><span class="line">            text = search_artist(cursor, artist_name)</span><br><span class="line">            bot.send_text(user_id, text)</span><br><span class="line">            sys.exit(0)</span><br><span class="line"></span><br><span class="line">        image_url, url = raw[0]</span><br><span class="line"></span><br><span class="line">        payload = &#123;</span><br><span class="line">            <span class="string">'template_type'</span>: <span class="string">'generic'</span>,</span><br><span class="line">            <span class="string">'elements'</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">'title'</span>: <span class="string">"Artist Info: '&#123;&#125;'"</span>.format(artist_name),</span><br><span class="line">                    <span class="string">'image_url'</span>: image_url,</span><br><span class="line">                    <span class="string">'subtitle'</span>: <span class="string">'information'</span>,</span><br><span class="line">                    <span class="string">'default_action'</span>: &#123;</span><br><span class="line">                        <span class="string">'type'</span>: <span class="string">'web_url'</span>,</span><br><span class="line">                        <span class="string">'url'</span>: url,</span><br><span class="line">                        <span class="string">'webview_height_ratio'</span>: <span class="string">'full'</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        bot.send_attachment(user_id, <span class="string">"template"</span>, payload)</span><br><span class="line"></span><br><span class="line">        query = <span class="string">"SELECT t2.genre FROM artists t1 JOIN artist_genres t2 ON t2.artist_id = t1.id WHERE t1.name = '&#123;&#125;'"</span>.format(artist_name)</span><br><span class="line"></span><br><span class="line">        cursor.execute(query)</span><br><span class="line">        genres = []</span><br><span class="line">        <span class="keyword">for</span> (genre, ) <span class="keyword">in</span> cursor.fetchall():</span><br><span class="line">            genres.append(genre)</span><br><span class="line"></span><br><span class="line">        text = <span class="string">"Here are genres of &#123;&#125;"</span>.format(artist_name)</span><br><span class="line">        bot.send_text(user_id, text)</span><br><span class="line">        bot.send_text(user_id, <span class="string">', '</span>.join(genres))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">## 만약에 아티스트가 없을시에는 아티스트 추가</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## Spotify API hit --&gt; Artist Search</span></span><br><span class="line">        <span class="comment">## Database Upload</span></span><br><span class="line">        <span class="comment">## One second</span></span><br><span class="line">        <span class="comment">## 오타 및 아티스트가 아닐 경우</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_headers(client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    endpoint = <span class="string">"https://accounts.spotify.com/api/token"</span></span><br><span class="line">    encoded = base64.b64encode(<span class="string">"&#123;&#125;:&#123;&#125;"</span>.format(client_id, client_secret).encode(<span class="string">'utf-8'</span>)).decode(<span class="string">'ascii'</span>)</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Basic &#123;&#125;"</span>.format(encoded)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">"grant_type"</span>: <span class="string">"client_credentials"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    r = requests.post(endpoint, data=payload, headers=headers)</span><br><span class="line"></span><br><span class="line">    access_token = json.loads(r.text)[<span class="string">'access_token'</span>]</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Bearer &#123;&#125;"</span>.format(access_token)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> headers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def insert_row(cursor, data, table):</span><br><span class="line"></span><br><span class="line">    placeholders = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">    columns = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">    key_placeholders = <span class="string">', '</span>.join([<span class="string">'&#123;0&#125;=%s'</span>.format(k) <span class="keyword">for</span> k <span class="keyword">in</span> data.keys()])</span><br><span class="line">    sql = <span class="string">"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s"</span> % (table, columns, placeholders, key_placeholders)</span><br><span class="line">    cursor.execute(sql, list(data.values())*2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 추가적으로 S3의 top_tracks에도 업데이트해주기 위해서 top-tracks lambda function을 실행시켜주는 함수이다.</span></span><br><span class="line"><span class="comment"># payload 부분은 lambda_handler 함수 안에서 들어오는 event에 관한 부분이다.</span></span><br><span class="line">def invoke_lambda(fxn_name, payload, invocation_type=<span class="string">'Event'</span>):</span><br><span class="line"></span><br><span class="line">    lambda_client = boto3.client(<span class="string">'lambda'</span>)</span><br><span class="line"></span><br><span class="line">    invoke_response = lambda_client.invoke(</span><br><span class="line">        FunctionName = fxn_name,</span><br><span class="line">        InvocationType = invocation_type,</span><br><span class="line">        Payload = json.dumps(payload)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> invoke_response[<span class="string">'StatusCode'</span>] not <span class="keyword">in</span> [200, 202, 204]:</span><br><span class="line">        logging.error(<span class="string">"ERROR: Invoking lmabda function: '&#123;0&#125;' failed"</span>.format(fxn_name))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> invoke_response</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def search_artist(cursor, artist_name):</span><br><span class="line"></span><br><span class="line">    headers = get_headers(client_id, client_secret)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## Spotify Search API</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">"q"</span>: artist_name,</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"artist"</span>,</span><br><span class="line">        <span class="string">"limit"</span>: <span class="string">"1"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    r = requests.get(<span class="string">"https://api.spotify.com/v1/search"</span>, params=params, headers=headers)</span><br><span class="line"></span><br><span class="line">    raw = json.loads(r.text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> raw[<span class="string">'artists'</span>][<span class="string">'items'</span>] == []:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">"Could not find artist. Please Try Again!"</span></span><br><span class="line"></span><br><span class="line">    artist = &#123;&#125;</span><br><span class="line">    artist_raw = raw[<span class="string">'artists'</span>][<span class="string">'items'</span>][0]</span><br><span class="line">    <span class="keyword">if</span> artist_raw[<span class="string">'name'</span>] == params[<span class="string">'q'</span>]:</span><br><span class="line"></span><br><span class="line">        artist.update(</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">'id'</span>: artist_raw[<span class="string">'id'</span>],</span><br><span class="line">                <span class="string">'name'</span>: artist_raw[<span class="string">'name'</span>],</span><br><span class="line">                <span class="string">'followers'</span>: artist_raw[<span class="string">'followers'</span>][<span class="string">'total'</span>],</span><br><span class="line">                <span class="string">'popularity'</span>: artist_raw[<span class="string">'popularity'</span>],</span><br><span class="line">                <span class="string">'url'</span>: artist_raw[<span class="string">'external_urls'</span>][<span class="string">'spotify'</span>],</span><br><span class="line">                <span class="string">'image_url'</span>: artist_raw[<span class="string">'images'</span>][0][<span class="string">'url'</span>]</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> artist_raw[<span class="string">'genres'</span>]:</span><br><span class="line">            <span class="keyword">if</span> len(artist_raw[<span class="string">'genres'</span>]) != 0:</span><br><span class="line">                insert_row(cursor, &#123;<span class="string">'artist_id'</span>: artist_raw[<span class="string">'id'</span>], <span class="string">'genre'</span>: i&#125;, <span class="string">'artist_genres'</span>)</span><br><span class="line"></span><br><span class="line">        insert_row(cursor, artist, <span class="string">'artists'</span>)</span><br><span class="line">        conn.commit()</span><br><span class="line">        r = invoke_lambda(<span class="string">'top-tracks'</span>, payload=&#123;<span class="string">'artist_id'</span>: artist_raw[<span class="string">'id'</span>]&#125;)</span><br><span class="line">        <span class="built_in">print</span>(r)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> <span class="string">"We added artist. Please try again in a second!"</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> <span class="string">"Could not find artist. Please Try Again!"</span></span><br></pre></td></tr></table></figure><ul><li>위에서 다른 trigger lambda function을 참조할 수 있도록 설정해 주었기 때문에 아래와 같이 role을 다른 lambda function을 invoke 할 수 있도록 IAM 페이지에서 permission을 추가해 주어야 정상적으로 작동된다.</li></ul><p><img src="/image/IAM_page_goto.png" alt="IAM 페이지로 이동"></p><p><img src="/image/add_permission_for_invoke.png" alt="IAM 페이지에서 invoke를 위한 permission 추가"></p><ul><li>이제 해당 페이지의 test를 진행해 보면 아래와 같이 위에서 설정한 것 처럼 해당아티스트의 url과 장르를 보내주는 것을 확인 할 수 있다. 그림은 없지만 필자는 2PM도 검색해보았는데, 잘 검색되어 나왔다.</li></ul><p><img src="/image/BTS_find_my_chat_bot.png" alt="BTS 검색 결과"></p><ul><li>이제껏 data engineering에 관한 몇가지 기초적인 부분들을 실습해 보며, data engineer는 상황에 맞춰 resource를 사용할 수 있도록 선택과 집중을 해야 한다고 생각했다. 그러한, 상황에 맞는 선택과 집중을 위해 해당 비즈니스가 처해있는 상황과 단계를 잘 진단하고 깊게 알고있어야 할 것 같다는 생각을 하게 되는 프로젝트 였다.</li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/03/03/data_engineering_10/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering (데이터 파이프라인 자동화)</title>
      <link>https://heung-bae-lee.github.io/2020/03/01/data_engineering_09/</link>
      <guid>https://heung-bae-lee.github.io/2020/03/01/data_engineering_09/</guid>
      <pubDate>Sun, 01 Mar 2020 04:49:06 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;데이터-워크-플로우&quot;&gt;&lt;a href=&quot;#데이터-워크-플로우&quot; class=&quot;headerlink&quot; title=&quot;데이터 워크 플로우&quot;&gt;&lt;/a&gt;데이터 워크 플로우&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이전에도 언급했었듯이 데이터 파이프라인은 아래와 같은 서비
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="데이터-워크-플로우"><a href="#데이터-워크-플로우" class="headerlink" title="데이터 워크 플로우"></a>데이터 워크 플로우</h1><ul><li>이전에도 언급했었듯이 데이터 파이프라인은 아래와 같은 서비스들을 S3에 모아 Athena같은 서비스로 분석해준 뒤 그 결과를 저장해놓은 일련의 데이터 작업의 흐름을 일컫는다.</li></ul><p><img src="/image/S3_data_pipe_line_exaple.png" alt="데이터 파이프 라인 예시"></p><ul><li>하나의 job이 시작되거나 어떠한 event에 trigger가 됬을때, 또 다른 job으로 연결이 되는 이런 정보들을 <code>DAGs(Directed Acyclic Graphs)</code>라고 부른다.</li></ul><p><img src="/image/DAGs.png" alt="DAG"></p><h1 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h1><ul><li>보통은 Extract -&gt; Transform -&gt; Load순으로 작업을 해 왔지만, 최근에는 Extract -&gt; Load -&gt; Transform 순으로 작업을 하기도 한다. 데이터 파이프 라인의 연장선이다. 하나의 예시를 들자면, 하루의 정해진 시간에 의한 스케쥴링인 Amazon CloudWatch Event와 Automation Script를 통해서 machine이 시작하면, AWS안에 AWS Step Functions는 각 과정에서 그 다음과정으로의 연결에 대한 여러가지 경우의 수에 대한 룰을 정해놓는 서비스로 쉽게 말하면, 임의의 단계에서 fail이 일어나면 어떤 event를 발생시켜야 하고, success를 하면 어떤 event를 발생시켜야 하는지를 관리할 수 있도록 도와주는 서비스이다. 이런 Step Function안의 ETL Flow state machine이 시작하고, 이후에는 다양한 job들이 작동하게 된다. 이러한 ETL job들의 log를 CloudWatch에 저장을 하고, 아래와 같은 Flow를 갖게된다.</li></ul><p><img src="/image/Extract_Transform_Load_example.png" alt="ETL 예시"></p><p><img src="/image/ETL_more_examples.png" alt="ETL 예시 - 01"></p><ul><li>AWS의 Step function에 관해 조금 더 말하자면, 아래 그림과 같이 사용할 수 있다. start가 되면 job이 submit이 되고, job이 finish될때 까지 기다려 줄 수 있게끔 Wait Seconds를 사용할 수도 있다. 왜냐하면, 예를 들어 Athena는 어느 정도 빅데이터를 처리하는 시스템이기 때문에 MySQL이나 PostgreSQL보다는 느린 부분이 있다. 이런 경우 위와 같이 time sleep을 통해 python script를 잠깐 멈춰두고 그 다음에 해당 시간이 지났을때 그 query에 대한 결과들을 가져올 수 있다. 이후에는 다시 job status를 받고 job이 끝났는지 아닌지에 따라 작업을 진행하는 flow를 볼 수 있다. 이런 service들이 없었을 때는 하나하나 monitoring을 통해서 수동으로 관리를 해야 했다.</li></ul><p><img src="/image/AWS_step_function_example.png" alt="AWS step function의 예시"></p><ul><li><p>AWS Glue가 가장 좋은 부분은 이전에는 MySQL 같은 경우에는 만들어 놓은 Schema에 맞춰서 data를 insert하였는데, 이제는 data가 너무나 방대해지고 형식도 다 다른데, 이런 것들을 통합하는는 다 Glue한다는 의미의 서비스라는 점이다. 가장 많이 쓰여지는 부분 중에 하나가 Crawler인데 <code>Crawler를 사용하면 자동으로 해당 data를 Crwaling해서 data가 어떤 형식인지에 대해서 지속적으로 Schema 관리가 들어가는 부분</code>이 있다. 그러므로 <code>data 양도 너무나 많고 column도 너무나 많은데 column이 변하는 경우도 있을 경우에 사용하면 좋다.</code></p></li><li><p>AWS Glue 페이지를 보면 아래 그림과 같이 table과 ETL, Trigger등 다양한 작업을 할 수 있다. 한 가지 예시로 S3에 저장해놓은 python Script를 Jobs 탭에서 바로 수정가능하며, Trigger들도 등록해서 관리 할 수 있다.</p></li></ul><p><img src="/iamge/AWS_Glue_example_one.png" alt="AWS Glue"></p><ul><li>해당 job들은 step function이나 Glue를 통해 관리를 하거나, EC2에서 Crontab으로 스케쥴링의 변화를 통해서 관리를 하는 등 다양한 방법으로 관리를 하지만 아래와 같이 서비스들의 지속적인 monitoring을 통해 cost를 효율적으로 사용할 선택과 집중을 해야 할 것이다. 어떤 부분까지 monitoring을 할 것인지에 대해 선택하여 집중하는 것이다.  </li></ul><p><img src="/image/AWS_RDS_monitoring_example.png" alt="RDS monitoring 예시"></p><p><img src="/image/AWS_EC2_monitoring_example.png" alt="EC2 monitoring 예시"></p><h2 id="Crontab이란"><a href="#Crontab이란" class="headerlink" title="Crontab이란?"></a>Crontab이란?</h2><ul><li><p>여러가지 만들어진 data job들을 우리가 작성한 해당 코드를 특정한 시간이나 하루에 한번 일주일에 한번이 됐건 어떠한 스케쥴링 방법을 통해서 지속적으로 작동시키려고 할때 사용한다. <a href="https://www.adminschoice.com/crontab-quick-reference" target="_blank" rel="noopener">Crontab Quick reference</a></p></li><li><p>이러한 스케쥴링을 하려면 규칙에의한 명령이 존재할 것임을 눈치챘을 것이다. 아래 그림과 같이 모두 ‘*‘이면  계속 1분마다 작동하라는 의미이며, 순서대로 분, 시간, 일, 월, 요일 순으로 지정할 수 있다.</p></li></ul><p><img src="/image/Crontab_rules.png" alt="Crontab 규칙"></p><ul><li>한가지 간단한 예로는 아래와 같이 Crontab을 실행하면 매일 오후 6시 30분에 /home/someuser/tmp안의 모든 파일을 제거하라는 job을 스케쥴링할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">30 18 * * * rm /home/someuser/tmp/*</span><br></pre></td></tr></table></figure><ul><li>이제 Crontab을 실습해 보기 위해 EC2 서버를 하나 AWS에서 생성해 볼 것이다. 먼저 service 탭에서 EC2를 클릭하여 아래 그림과 같이 EC2 service 페이지로 이동한 후 다시 instance 탭으로 이동한다. Launch instance 버튼을 클릭하면 생성할 instance의 환경을 설정하는 페이지로 이동하게된다.</li></ul><p><img src="/image/AWS_EC2_create.png" alt="EC2 instance - 01"></p><ul><li>Crontab 실습을 위한 서버이기 때문에 사양이 좋은 것을 고르진 않을 것이다. 필자는 Free tier만 사용가능한 2번째 사양을 고를 것이다. 다만, 실무에서 사용할 경우는 각자의 상황에 맞는 서버의 크기와 성능을 골라서 사용해야 할 것이다.</li></ul><p><img src="/image/AWS_EC2_create_01.png" alt="EC2 instance 생성 - 02"></p><ul><li>필자는 Free Tier만 가능한 t2.micro type을 선택하였다.</li></ul><p><img src="/image/AWS_EC2_create_02.png" alt="EC2 instance 생성 - 03"></p><ul><li>Configure instance 탭에서는 개수를 지정하는 등 instance에 대한 설정을 하는 탭인데, 필자는 default값을 사용하기로 생각해서 아무런 설정도 하지않고 넘어갔다.</li></ul><p><img src="/image/AWS_EC2_create_03.png" alt="EC2 instance 생성 - 04"></p><ul><li>Add storage 탭은 말 그대로 저장 성능을 설정하는 탭으로 필자는 이부분도 별다른 설정없이 기본값으로 하고 넘어갔다.</li></ul><p><img src="/image/AWS_EC2_create_04.png" alt="EC2 instance 생성 - 05"></p><ul><li>tag를 설정하는 탭이며, 필자는 생략하였다.</li></ul><p><img src="/image/AWS_EC2_create_05.png" alt="EC2 instance 생성 - 06"></p><ul><li>Configure Security Group 탭은 Security Group의 설정에 관한 탭이며, 새로 만들수도 있고, 이전에 생성되어있는 그룹을 사용해도 무방하다. 허나, 동일한 inbound 규칙을 사용하지 않는다면 새롭게 생성해서 사용하는 것이 좋다. 필자는 새롭게 만들어서 사용할 것이다. 또한, 접근을 ssh로 할 것이므로 아래와 같이 설정한다.</li></ul><p><img src="/image/AWS_EC2_create_06.png" alt="EC2 instance 생성 - 07"></p><ul><li>마지막으로 Review 탭은 이제까지 설정한 모든 사항을 점검하고 마지막으로 접속시 사용할 key pair를 어떤것으로 할지 정해주고 나면 모든 과정이 끝이 난다.</li></ul><p><img src="/image/AWS_EC2_create_07.png" alt="EC2 instance 생성 - 08"></p><ul><li>이제 다시 EC2의 instance 탭을 살펴보면, 새롭게 EC2 서버가 생성된 것을 확인 할 수 있다. 이제 접속을 해볼 것인데, 아래 그림에서 처럼 자신의 pem 파일이 존재하는 path에서 public DNS 서버 주소를 같이 입력하고 접속하면 된다.</li></ul><p><img src="/image/AWS_EC2_create_08.png" alt="EC2 instance 생성 - 09"></p><ul><li>아래와 같이 command를 실행하면 계속진행할 것이냐는 물음이 나올텐데 yes라고 하면 된다. 이는 앞으로 계속해서 접속하기위해 이 DNS를 추가할 것이냐는 물음이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -i pem_file_name.pem ec2-user@Public_DNS</span><br></pre></td></tr></table></figure><ul><li>이제 Crontab을 실행할 script 파일을 정해야 하는데, 필자는 이전에 만들어 두었던 script 파일 중에 top_tracks와 audio feature들에 대한 데이터를 S3에 parquet화하여 저장하게끔 코드를 작성한 파일을 사용할 것이다. 아래와 같이 필자의 pem 파일은 data_engineering이라는 파일 안에 존재한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">data_engineering</span><br><span class="line">├── Code</span><br><span class="line">│   ├── Chatbot\ Project</span><br><span class="line">│   │   ├── deploy.sh</span><br><span class="line">│   │   ├── fb_bot.py</span><br><span class="line">│   │   ├── lambda_handler.py</span><br><span class="line">│   │   └── requirements.txt</span><br><span class="line">│   ├── artist_list.csv</span><br><span class="line">│   ├── audio_features.parquet</span><br><span class="line">│   ├── create_artist_genres_table.py</span><br><span class="line">│   ├── create_artist_table.py</span><br><span class="line">│   ├── dynamodb_insert.py</span><br><span class="line">│   ├── select_dynamodb.py</span><br><span class="line">│   ├── spotify_S3.py</span><br><span class="line">│   ├── spotify_s3_artist.py</span><br><span class="line">│   ├── spotify_s3_make.py</span><br><span class="line">│   ├── top-tracks.parquet</span><br><span class="line">│   </span><br><span class="line">├── Slide</span><br><span class="line">├── foxyproxy-settings.xml</span><br><span class="line">└── spotift_chatbot.pem</span><br></pre></td></tr></table></figure><ul><li>위와 같은 구조로 되어있으므로 사용할 spotify_s3_make.py을 생성한 EC2 Server로 옮겨 줄 것이다. <code>scp은 서버로 파일을 copy하는 것이라고 생각하면 된다. 단,아래 명령어는 이미 EC2에 접속한 상태가 아닌 로컬에서 진행하여야한다.</code> 아래와 같이 옮겨진것을 볼 수 있고, EC2 서버로 접속하여 확인가능하다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scp -i spotift_chatbot.pem ./Code/spotify_s3_make.py ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.com:~/</span><br><span class="line">spotify_s3_make.py                            100% 5411   536.2KB/s   00:00</span><br><span class="line"></span><br><span class="line">ssh -i spotift_chatbot.pem ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.com</span><br><span class="line">ls</span><br></pre></td></tr></table></figure><ul><li>만일 EC2 서버의 어떤 폴더를 만들어 놓았는데 해당 폴더안으로 이동시키고 싶다면 아래와 같이 하면된다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -i spotift_chatbot.pem ./Code/spotify_s3_make.py ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.com:~/파일이름</span><br></pre></td></tr></table></figure><ul><li>이제 script를 실행하기에 앞서서 작동할 수 있게끔 먼저 환경을 만들어주어야 할 것이다. 가장 먼저 python3가 설치되어있는지를 확인해 보자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">sudo yum list | grep python3</span><br><span class="line"><span class="comment"># 위의 명령문을 실행한 후 python3x에 관한 리스트가 나온다면 python3가 설치되어있는 상태이다.</span></span><br><span class="line"><span class="comment"># 허나 잘 모르겠다면 아래 명령어를 통해 설치하도록하자.</span></span><br><span class="line">sudo yum install python36 -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 그 다음은 pip를 설치해야한다</span></span><br><span class="line">curl -O https://bootstrap.pypa.io/get-pip.py</span><br><span class="line"></span><br><span class="line">sudo python3 get-pip.py</span><br><span class="line"></span><br><span class="line"><span class="comment">#이제 script파일을 run해본 후에 필요한 패키지들을 추가로 설치해준다.</span></span><br><span class="line">python3 spotify_s3_make.py</span><br><span class="line"></span><br><span class="line">pip install boto3 --user</span><br><span class="line">pip install requests --user</span><br><span class="line">pip install pymysql --user</span><br><span class="line">pip install pandas --user</span><br><span class="line">pip install jsonpath --user</span><br></pre></td></tr></table></figure><ul><li><p>이제 본격적으로 crontab을 실습해 볼 것이다. 보통 crontab은 아래 두 가지를 많이 사용한다. crontab 파일은 vim editor를 사용하는데, 해당 스케쥴을 시작할때 email을 보내주는 기능도 있다. 우선 작동시키고 싶은 파일과 파이썬의 위치를 알고있어야 한다.</p></li><li><p><a href="https://ora-sysdba.tistory.com/entry/Cloud-Computing-Amazon-EC2-%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4%EC%9D%98-TIMEZONE-%EB%B3%80%EA%B2%BD" target="_blank" rel="noopener">UTC 시간 변경</a></p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">pwd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line">/home/ec2-user</span><br><span class="line"></span><br><span class="line"><span class="built_in">which</span> python3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line">/usr/bin/python3</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo service crond start</span><br><span class="line"></span><br><span class="line">crontab -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># crontab 파일 쓰는 vim이 켜진 후</span></span><br><span class="line"><span class="comment"># 매일 18시 30분에</span></span><br><span class="line">30 18 * * * /usr/bin/python3 /home/ec2-user/spotify_s3_make.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># vim으로 저장한 후 나오면 아래와 같은 메세지가 나와야 정상적으로 crontab을 설정한 것이다.</span></span><br><span class="line">crontab: installing new crontab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 어떠한 사항을 crontab으로 스케쥴링하고 있는지 확인하기</span></span><br><span class="line">crontab -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 서버는 UTC를 사용하기 때문에 date 명령어를 통해 현재 서버가 몇시인지를 확인하고 우리나라 시간과 맞추도록 crontab을 설정해 주어야 한다.</span></span><br><span class="line">date</span><br></pre></td></tr></table></figure><h2 id="마이크로서비스에-대한-이해"><a href="#마이크로서비스에-대한-이해" class="headerlink" title="마이크로서비스에 대한 이해"></a>마이크로서비스에 대한 이해</h2><ul><li>전체적인 수집 프로세스에 대한 설명을 다시 한번 짚고가자면, Unknown Artist가 챗봇 메세지로 들어왔을 경우 AWS Serverless Lamda 서비스를 통해 Spotify API에 Access를 하고 그리고 해당 데이터가 Top Tracks, Artist Table에 가야되는지 또는 S3에 가야되는지를 관리를 하게 될 것이다. 또한, Ad Hoc Data Job을 통해 하루에 한번이라던지, 직접 로컬에서 command line을 통해 데이터를 가져올 수도 있게된다. <code>Lambda가 필요한 이유는 우리가 Unknown Artist가 챗봇 메세지로 들어왔을때 내용을 업데이트를 해야되는데, 보통 사람들이 기대하는 챗봇은 업데이트를 바로 해주어서 원하는 정보를 얻을 수 있게끔 해주어야 하기에 이렇게 바로 업데이트를 할 수 있게끔 Lambda라는 서비스를 통해서 해결 할 수 있다.</code> 이런 Lambda는 마이크로서비스의 개념인데, monolithic 이라는 개념의 반대이다. monolithic은 하나의 서비스를 만들때 크게 프로젝트 단위로 만들어 놓고 관리를 해주는 개념으로써 관리에 있어서 전체 프로세스가 보이기 때문에 컨트롤하기 쉬운 부분이 있다. 이에 반해 <code>마이크로서비스는 세세한 작업하나씩을 단위로 관리를 하는 것</code>이다.</li></ul><p><img src="/image/data_cralwer_process_related_artist.png" alt="데이터 수집 관련 프로세스"></p><ul><li>챗봇을 Lambda로 구현하는 이유는 Serverless는 하나의 Function이기 때문에 Stateless라고도 하는데 지금 상태가 어떤지 모르겠다는 의미이다. 예를들어, 어떠한 User 어떤 메시지를 보냈다고 가정하면, Lambda Function에는 이전의 어떠한 메시지를 갖고 있었는지를 담고있을 수 없다. 상태가 없는 Function이라고 생각하면 될 것 같다. 그러므로 이런 State를 관리할만한 데이터베이스가 필요할 것이다. 주로 메시지에 특화된 DynamoDB를 사용할 것이다. 또한 Lambda의 경우에는 해당 서비스의 User가 기하급수적으로 늘어났을 때 <code>병렬로 늘어나기 때문에 제한점이 서버로 구현하는것보단 덜하다는 장점</code>도 있다. 서버의 경우에는 메모리나 CPU의 제한된 성능으로 구축된 동일한 서버를 통해 1명에게 서비스하는 것과 백만명에게 서비스하는 것은 완전 다를것이다. <code>또 한가지 좋은 점은 지속적으로 띄워져 있는 것이 아니라 필요할 때 띄워서 사용한 만큼만 비용을 지불한다는 점</code>이다.</li></ul><p><img src="/image/Lambda_Athena_01.png" alt="Lambda, Athena - 01"></p><ul><li>Crontab을 통하여 Lambda를 호출할 수도 있고, Lambda가 Lambda를 호출할 수도 있다.</li></ul><p><img src="/image/Lambda_Athena_02.png" alt="Lambda, Athena - 02"></p><ul><li>본격적으로 Lambda Function을 만들 것이다. 먼저 AWS Lambda Function 페이지로 이동하여 아래와 같이 Create Function 버튼을 클릭한다.</li></ul><p><img src="/image/Click_create_Lambda_Function.png" alt="Lambda Function 생성 - 01"></p><ul><li>이번 <code>Lambda Function은 이전에 DynamoDB에 top track정보를 DynamoDB에 저장했었는데, Artist가 추가된다면 DynamoDB에도 저장되어야하므로 이 작업을 작성해 볼 것</code>이다. 아래 그림에서와 같이 Author from scratch는 기본적인 예제를 통해 시작하는 부분이고, Use a blueprint는 흔하게 사용되는 경우들을 코드로 제공하는 부분이다. 마지막은 App repository에서 바로 연결해서 사용하는 것이다. 필자는 Scratch로 진행할 것이다. Lambda는 하나의 Functiond이므로 제한점도 있을 것이다.</li></ul><p><img src="/image/choose_option_function_Lambda.png" alt="Lambda Function 생성 - 02 옵션 설정"></p><ul><li>아래 그림과 같이 Function의 이름을 정하고 function의 language를 정한다. 또한 가장 중요한 부분인 Permission부분이 남았는데, 이 부분은 현재 필자가 진행할 Function의 목표는 <code>DynamoDB에 새로운 데이터를 추가하는 것이므로 DynamoDB에 대한 permission을 갖고 있어야 오류가 없을 것</code>이다. 그렇기에 2번째 부분인 Use an exsiting role을 클릭하여 사용해야 하는데, 새롭게 규칙을 추가해서 사용하면 error가 어떻게 발생하는 지를 보기위해 우선 첫번째를 선택하였다. 물론 무조건적으로 있던 규칙을 사용하는 것이 아니라 상황에 맞춰 사용해야한다.</li></ul><p><img src="/image/Lambda_function_basic_info_and_permission.png" alt="Lambda Function 생성 - 03 옵션 설정"></p><ul><li>function을 생성하면 아래 그림과 같이 여러가지 설정 및 작업을 할 수 있는 페이지가 나온다.</li></ul><p><img src="/image/finished_function_creation.png" alt="Lambda Function 생성 - 04 생성 완료"></p><ul><li>아래 부분으로 내려보면 다음과 같이 Function의 코드를 작성할 수 있는 부분이 존재한다. 이 곳에서 Function을 정의할 것이다. <code>허나, Edit code inline</code>은 거의 사용하지 못하는 설정이라고 볼 수 있다. 해당 Lambda Function은 Linux 기반의 AMI compute system에 있는데, 함수를 동작할 수 있는 패키지들이 아무런 설치나 설정이 되어있지 않은 상태이기 때문이다. 그래서 zip file을 업로드하거나 S3에서 불러오는 방식을 보통 채택한다. 필자는 S3에서 불러오는 방식을 사용할 것이다.</li></ul><p><img src="/image/Lambda_Function_Code_typeing.png" alt="Lambda Function 생성 - 05 코드 작성"></p><ul><li>위에서 함수에 사용될 변수들을 정의할 수 있다. 예를 들어, Spotify API에 접속하기 위해서는 ID와 Secret Key가 필요했는데 이 부분을 코드에 적기 보다는 보안의 문제로 따로 변수로 처리해 둔 뒤 함수에는 그 값을 받아 사용하는 형식으로 사용된다. 예를 들면 아래 그림에서 환경 변수의 Edit을 클릭하면 변수의 Key와 Value를 입력하여 추가하게끔 되어있는데 추가했다고 가정하면 함수에서는 <code>os.environ.get(key)</code>로 값을 받으면 된다. 또한 <code>Basic settings</code>부분은 Memory(Max : 3GB)와 timeout(Max : 15분)을 설정할 수 있는 부분인데, <code>가장 간단하고 명료하게 병렬적으로 분산처리를 할 수 있도록 코드를 작성하는 것이 좋다는 Lambda의 특징을 잘 보여주는 부분</code>이다. 여기서 Memory는 크게 해 놓아도 사용한 만큼만 비용을 지불하는 것이므로 상관없다는 것에 유의하자.</li></ul><p><img src="/image/ETC_Lambda_Function.png" alt="Lambda Function 생성 - 06 변수 설정"></p><ul><li><p>이미 해당 Artist가 존재하는 것은 확인한 상태여서 해당 Artist ID에 대한 top track만 확보하고 싶은 경우라고 가정할 것이다. 그 ID 값을 이 Lambda Function에 보내 줌으로써 다시 한번 Spotify API에 hit을 하여 top track 정보를 가져오고 다시 DynamoDB에 저장하도록 할 것이다.  </p></li><li><p>본격적으로 Lambda Function을 만들어 볼 것이다. 우선 새로운 폴더를 만들어준다. 그 안에는 Lambda Function에 관한 것들만 담기 위해서이다. 위에서 언급 한 것처럼 우선 아래 패키지들을 shell script를 통해 실행하기 위해서 requirements.txt를 작성할 것이다. 헌데 AWS에는 기본적으로 boto3가 설치되어져 있고 나머지들은 기본 내장 패키지들이므로 requests만 설치해 주면 될 것 같다.</p></li><li><p>requirements.txt</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests</span><br></pre></td></tr></table></figure><ul><li>위에서 만들어 놓은 requirements.txt안의 패키지를 -t옵션의 target 파일에 저장한다는 의미이다. 헌데 이렇게 home이 아닌 파일에 저장하게 되면 pointer issue 때문에 error가 발생하는데 이는 새롭게 <code>setup.cfg</code>라는 파일안에 아래와 같이 작성한 후에 저장해주면 해결된다. 다시 아래의 명령문을 실행하면 error 없이 libs안에 설치가 될 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt -t ./libs</span><br></pre></td></tr></table></figure><ul><li>setup.cfg</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[install]</span><br><span class="line">prefix=</span><br></pre></td></tr></table></figure><ul><li><p>매번 우리가 AWS CLI를 통해 명령문을 치고 실행할 수 없으므로 shell script로 작성해 준다.</p></li><li><p>deploy.sh</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">rm -rf ./libs</span><br><span class="line">pip3 install -r requirements.txt -t ./libs</span><br><span class="line"></span><br><span class="line">rm *.zip</span><br><span class="line">zip top_tracks.zip -r *</span><br><span class="line"></span><br><span class="line">aws s3 rm s3://top-tracks-lambda/top_tracks.zip</span><br><span class="line">aws s3 cp ./top_tracks.zip s3://top-tracks-lambda/top_tracks.zip</span><br><span class="line">aws lambda update-function-code --<span class="keyword">function</span>-name top-tracks --s3-bucket top-tracks-lambda --s3-key top_tracks.zip</span><br></pre></td></tr></table></figure><p>또한, 위에서 lambda function을 update할 s3 bucket을 지정했으므로 새롭게 위의 이름으로 생성한다. 이 단계는 먼저 bucket을 만든뒤에 앞의 shell script를 만들어도 무관하다.</p><p><img src="/image/create_new_bucket_S3_for_lambda_function.png" alt="Lambda Function을 위한 새로운 S3 bucket 생성"></p><ul><li>lambda_function.py</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">sys.path.append(<span class="string">'./libs'</span>)</span><br><span class="line">import os</span><br><span class="line">import boto3</span><br><span class="line">import requests</span><br><span class="line">import base64</span><br><span class="line">import json</span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client_id = <span class="string">"Your Spotify Developer ID"</span></span><br><span class="line">client_secret = <span class="string">"Your Spotify Developer PW"</span></span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    dynamodb = boto3.resource(<span class="string">'dynamodb'</span>, region_name=<span class="string">'ap-northeast-2'</span>, endpoint_url=<span class="string">'http://dynamodb.ap-northeast-2.amazonaws.com'</span>)</span><br><span class="line">except:</span><br><span class="line">    logging.error(<span class="string">'could not connect to dynamodb'</span>)</span><br><span class="line">    sys.exit(1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lambda_handler(event, context):</span><br><span class="line"></span><br><span class="line">    headers = get_headers(client_id, client_secret)</span><br><span class="line"></span><br><span class="line">    table = dynamodb.Table(<span class="string">'top_tracks'</span>)</span><br><span class="line"></span><br><span class="line">    artist_id = event[<span class="string">'artist_id'</span>]</span><br><span class="line"></span><br><span class="line">    URL = <span class="string">"https://api.spotify.com/v1/artists/&#123;&#125;/top-tracks"</span>.format(artist_id)</span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">'country'</span>: <span class="string">'US'</span></span><br><span class="line">    &#125;</span><br><span class="line">    r = requests.get(URL, params=params, headers=headers)</span><br><span class="line"></span><br><span class="line">    raw = json.loads(r.text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> track <span class="keyword">in</span> raw[<span class="string">'tracks'</span>]:</span><br><span class="line"></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'artist_id'</span>: artist_id</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        data.update(track)</span><br><span class="line"></span><br><span class="line">        table.put_item(</span><br><span class="line">            Item=data</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># AWS CloudWatch에서 Log기록을 살펴볼때 확인하기 위해 return 값을 Success로 주었다.</span></span><br><span class="line">    <span class="built_in">return</span> <span class="string">"SUCCESS"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_headers(client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    endpoint = <span class="string">"https://accounts.spotify.com/api/token"</span></span><br><span class="line">    encoded = base64.b64encode(<span class="string">"&#123;&#125;:&#123;&#125;"</span>.format(client_id, client_secret).encode(<span class="string">'utf-8'</span>)).decode(<span class="string">'ascii'</span>)</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Basic &#123;&#125;"</span>.format(encoded)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">"grant_type"</span>: <span class="string">"client_credentials"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    r = requests.post(endpoint, data=payload, headers=headers)</span><br><span class="line"></span><br><span class="line">    access_token = json.loads(r.text)[<span class="string">'access_token'</span>]</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Bearer &#123;&#125;"</span>.format(access_token)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> headers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><ul><li>위와 같이 파일을 모두 작성했다면 아래와 같은 구조로 만들어져 있을 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">top_tracks</span><br><span class="line">├── deploy.sh</span><br><span class="line">├── lambda_function.py</span><br><span class="line">├── requirements.txt</span><br><span class="line">└── setup.cfg</span><br></pre></td></tr></table></figure><ul><li>이제 top_tracks의 path에서 아래와 같이 shell script를 실행시켜준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./deploy.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과는 아래와 같이 permission denied가 발생할 것이다.</span></span><br><span class="line">-bash: ./deploy.sh: Permission denied</span><br></pre></td></tr></table></figure><ul><li><p>Permission 권한을 바꿔주기 위해 다음의 코드를 실행한뒤 다시 shell script를 실행시킨다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chmod +x deploy.sh</span><br><span class="line"></span><br><span class="line">./deploy.sh</span><br></pre></td></tr></table></figure></li><li><p>위의 코드들을 실행한 뒤 다시 AWS Lambda Function의 페이지로 돌아가 보면 아래와 같이 해당 파일들을 사용할 수 있게 설정이 되어져 있는 것을 확인 할 수 있다. 보통은 아래와 같이 sensitive한 정보들(client_id, client_secret) 같은 정보들은 따로 config.py 파일을 만들어서 모아놓는다. 이런 config.py파일은 해당 관리자만 가지고 있도록 하여 보안에 유지하나, 필자는 혼자 사용하므로 따로 만들지 않았다.</p></li></ul><p><img src="/image/change_environment_lambda_funcition.png" alt="Lambda Function S3에서 불러오기"></p><ul><li>만약 위에서 Function Code내에서는 sensitive한 정보들이 안보이도록 하려면 아래 그림과 같이 environment variable을 추가하고, 코드를 작성하면된다. key값과 value값이 문자라도 기호를 사용하지 않고 넣으면 된다.</li></ul><p><img src="/image/environment_variables_adding.png" alt="environment variable 추가"></p><p><img src="/image/get_environment_variable.png" alt="environment 변수 받는 법"></p><ul><li>이제 Lambda Function을 Test해 볼 차례이다. Test 버튼을 클릭해 준 뒤, event의 artist_id값을 주기 위해 RDS에 접속해서 임의의 ID 하나를 필자는 가져왔다.</li></ul><p><img src="/image/test_lambda.png" alt="Lambda Function Test"></p><p><img src="/image/Lambda_Function_Test_example.png" alt="Lambda Function Test 데이터 생성"></p><ul><li>Test 데이터를 저장하면 아래 그림과 같이 테스트할 파일로 바뀌었다는 것을 확인 할 수 있다. 이제 Test 버튼을 클릭한다. Log를 보니 Error가 발생한 것 같으므로 먼저 확인해 볼 것이다. Log는 AWS CloudWatch 서비스에서 확인할 수 있다.</li></ul><p><img src="/image/Lambda_Function_test_result_error.png" alt="Lambda Funtion Test 결과 및 error 발생"></p><p><img src="/image/log_cloudwatch.png" alt="Cloudwatch 페이지"></p><ul><li>아래 로그 중 빨간색 박스 부분을 살펴보면 해당 role에 대한 permission이 없기 때문에 발생한 error임을 확인 할 수 있다.</li></ul><p><img src="/image/cloudwatch_log_error_find.png" alt="로그 중 에러 발생 부분 찾기"></p><ul><li>처음에 Lambda Function을 만들때 말했듯이 permission이 없기 때문에 발생하는 error이므로 excution role을 변경해주기 위해 아래 그림의 빨간색 박스 안의 버튼을 클릭해 준다.</li></ul><p><img src="/image/excution_rule_change_attach.png" alt="excution rule 변경"></p><ul><li>role을 변경하기 위해 IAM 페이지로 이동되며, Attach를 통해 DynamoDB에 access 할 수 있는 권한을 부여할 것이다.</li></ul><p><img src="/image/IAM_change_rule_attach.png" alt="Attach policies - 01"></p><p><img src="/image/DynamoDB_Full_Access.png" alt="Attach policies - 02"></p><ul><li>새롭게 role이 추가되어 기존의 1개에서 2개로 늘어났으며, DynamoDBFullAccess 권한을 부여받음을 확인할 수 있다.</li></ul><p><img src="/image/DynamoDB_attach_polices_result.png" alt="Attach policies - 02"></p><ul><li>이제 다시 Lambda Function 페이지로 돌아와서 실행시켜 보면 제대로 실행됨을 알 수 있다.</li></ul><p><img src="/image/Success_Lambda_Function_result.png" alt="Success Lambda Function"></p><ul><li>Lambda는 Event trigger 뿐만 아니라, Crontab과 같이 스케쥴링에 의한 job을 작동시킬수도 있으며, 적용가능한 여러가지 event들이 존재한다.</li></ul><p><img src="/image/Add_trigger.png" alt="trigger 추가하기"></p><p><img src="/image/type_of_trggers.png" alt="trigger들의 종류"></p><p><img src="/image/Scheduling_trigger_AWS_event_watch.png" alt="Scheduling trigger 방법"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/03/01/data_engineering_09/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
