<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DataLatte&#39;s IT Blog</title>
    <link>https://heung-bae-lee.github.io/</link>
    
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>DataLatte가 Data Science 공부를 하면서 정리해 놓는 블로그</description>
    <pubDate>Mon, 08 Jun 2020 14:39:51 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Linear Independence, Span, and Subspace</title>
      <link>https://heung-bae-lee.github.io/2020/06/08/linear_algebra_04/</link>
      <guid>https://heung-bae-lee.github.io/2020/06/08/linear_algebra_04/</guid>
      <pubDate>Mon, 08 Jun 2020 06:52:22 GMT</pubDate>
      <description>
      
        
        
          &lt;ul&gt;
&lt;li&gt;&lt;p&gt;우선 이전에 언급했듯이 &lt;code&gt;solution가 존재하려면 solution 벡터가 선형시스템이 이루는 Span안에 포함&lt;/code&gt;되어있어야 한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;선형시스템을 이루는 벡터들이 &lt;code&gt;linearly
        
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li><p>우선 이전에 언급했듯이 <code>solution가 존재하려면 solution 벡터가 선형시스템이 이루는 Span안에 포함</code>되어있어야 한다.</p><ul><li>선형시스템을 이루는 벡터들이 <code>linearly Independent</code>하면 <code>Unique한 solution</code>을 갖고</li><li>선형시스템을 이루는 벡터들이 <code>linearly dependent</code>하면 <code>무수히 많은 solution</code>을 갖는다.</li></ul></li></ul><ul><li>기하학적으로 살펴보게 되면, 선형시스템을 이루는 벡터들에 스칼라배한 오직 하나의 평행사변형만을 통해 상수벡터 $ b $ 를 만족시키는 경우에는 Unique Solution을 갖게 되는 것이고, 상수벡터를 여러개의 평행사변형으로 표현할 수 있다면 무수히 많은 Solution을 갖는다.</li></ul><p><img src="/image/Uniqueness_of_Solution_for_Ax_equal_b_if_and_only_if.png" alt="유일한 해를 갖는 조건"></p><ul><li>위에서 언급한 linearly Independent란 아래 그림과 같이 기존의 $ a $ 벡터와 $ b $ 벡터로 이루어진 행렬에서 추가적으로 $ c $ 벡터를 열벡터로 받아 만들어진 행렬의 <code>Span 변화를 생각</code>해보면, 기존의 $ a $ 와 $ b $ 열벡터들간의 조합으로 만들어질 수 있는 Span에서 $ c $ 가 추가 되었음에도 불구하고 변화가 없게 된다. 이러한 경우 linearly dependent하다고 볼 수 있다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([0, 1])</span><br><span class="line">b = np.array([1, 0])</span><br><span class="line">c = np.array([1, 1])</span><br><span class="line">plt.figure(figsize=(12, 8))</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=a, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=b, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=c, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.text(0, 1.2, <span class="string">"<span class="variable">$a</span>$"</span>)</span><br><span class="line">plt.text(1.2, 0, <span class="string">"<span class="variable">$b</span>$"</span>)</span><br><span class="line">plt.text(1.15, 1.15, <span class="string">"<span class="variable">$c</span>$"</span>)</span><br><span class="line">plt.xticks(np.arange(-2, 4))</span><br><span class="line">plt.yticks(np.arange(-1, 4))</span><br><span class="line">plt.xlim(-2.4, 3.4)</span><br><span class="line">plt.ylim(-0.8, 3.4)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/span_linearly_dependent.png" alt="Span의 변화"></p><ul><li>위의 방법처럼 벡터를 하나씩 늘려 나갈 때 마다 기존의 Span에 포함되어있는지를 확인해보면서 linearly dependent한지를 찾는 것도 하나의 방법이다. 만일 기존의 벡터들의 Span에 포함되어있지 않다면 linearly independent가 될 것이다.</li></ul><p><img src="/image/linearly_independence_practical_definition.png" alt="벡터들이 Linearly Independent한지 확인하는 법 - 01"></p><ul><li>행렬의 $ V_{1}, V_{2}, \ldots, V_{n} $개의 열벡터에 대해 homogeneous linear system인 <script type="math/tex">C_{1} V_{1} + C_{2} V_{2} + \cdots + C_{n} V_{n} = 0</script> 을 만족하는 $ C_{1}, C_{2}, \ldots, C_{n} $ 을 찾을 때, 오직 $ C_{1} = C_{2} = \cdots = C_{n} = 0 $ 인 경우 해당 행렬의 열벡터들이 <code>linearly independent</code> 하다고 말한다. 왜냐하면 <code>열벡터들 중 0이아닌 다른 상수로 스칼라배를 하여 영벡터를 만들어 냈다는 의미는 서로 스칼라배로 만들어질 수 있는 관계라는 의미이기 때문</code>이다.</li></ul><ul><li>아래의 $ C_{1} $, $ C_{2} $, $ C_{3} $ 가 모두 $ 0 $ 값을 갖아야 영벡터를 만들 수 있으므로 아래 열벡터들은 서로 <code>linearly Independent</code> 하다고 할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} C_{1} \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} + C_{2} \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} + C_{3} \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix} \end{align}</script><ul><li>아래 행렬도 동일하게 열벡터들간의 선형조합으로 영벡터를 만족시키기 위해선 스칼라들이 모두 다 0이어야 하기 때문에 linearly Independent하다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 3 & 4 & 2 \\ 0 & 1 & 5 \\ 0 & 0 & 5 \end{bmatrix} \Rightarrow \begin{align} C_{1} \begin{bmatrix} 3 \\ 0 \\ 0 \end{bmatrix} + C_{2} \begin{bmatrix} 4 \\ 1 \\ 0 \end{bmatrix} + C_{3} \begin{bmatrix} 2 \\ 5 \\ 5 \end{bmatrix} \end{align}</script><p><img src="/image/linearly_independence_formal_definition.png" alt="벡터들이 Linearly Independent한지 확인하는 법 - 02"></p><ul><li>또 다른 방법으로 마지막으로 추가된 열벡터의 선형조합을 제외하곤 나머지를 우변으로 넘겨 해당 조합들로 마지막 열벡터를 만들 수 있다면 마지막 열벡터는 기존의 열벡터들의 선형조합에 대한 Span에 포함되있는 것이므로 이 또한 linearly dependent 가 될 것이다.</li></ul><p><img src="/image/two_definitions_are_equivalent.png" alt="벡터들이 Linearly Independent한지 확인하는 법 - 03"></p><ul><li>위에서 한 번 언급한 것과 같이 linearly dependent한 경우에는 Span의 변화가 없다. 즉, 열벡터가 하나 추가 되어도 Span이 증가하지 않는다.</li></ul><p><img src="/image/Linearly_dependent_vector_does_not_increase_span.png" alt="Linearly Dependent한 상황에서의 Span의 변화"></p><ul><li><code>linear dependence한 집합은 여러개의 가능한 선형조합을 만들 수 있다는 것</code>이다. 여러 가능한 조합들이 있으므로 <code>solution도 무수히 많다.</code></li></ul><p><img src="/image/Linear_dependence_and_linear_system_solution.png" alt="Linear dependent와 solution의 관계 - 01"></p><p><img src="/image/Linear_dependence_and_linear_system_solution_01.png" alt="Linear dependent와 solution의 관계 - 02"></p><ul><li><code>linear independent한 경우에는 해당 상수벡터(b)를 만들 수 있는 조합이 unique하기 때문에 solution도 unique하다.</code></li></ul><p><img src="/image/Linear_dependence_and_linear_system_solution_02.png" alt="Linear independent와 solution의 관계"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/06/08/linear_algebra_04/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Imbalanced Data</title>
      <link>https://heung-bae-lee.github.io/2020/06/06/machine_learning_20/</link>
      <guid>https://heung-bae-lee.github.io/2020/06/06/machine_learning_20/</guid>
      <pubDate>Fri, 05 Jun 2020 16:52:20 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Imbalanced-Data&quot;&gt;&lt;a href=&quot;#Imbalanced-Data&quot; class=&quot;headerlink&quot; title=&quot;Imbalanced Data&quot;&gt;&lt;/a&gt;Imbalanced Data&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;실제로 도메인에서 적용
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Imbalanced-Data"><a href="#Imbalanced-Data" class="headerlink" title="Imbalanced Data"></a>Imbalanced Data</h2><ul><li>실제로 도메인에서 적용될 때 클래스가 Imbalance한 데이터들이 많을 것이다. 아래와 같이 불균형인 데이터를 그냥 학습시키면 다수의 클래스를 갖는 데이터를 많이 학습하게 되므로 소수 클래스에 대해서는 잘 분류해내지 못한다.</li></ul><ul><li>데이터 클래스 비율이 너무 차이가 나면(highly-Imbalanced data) 단순히 우세한 클래스를 택하는 모형의 정확도가 높아지므로 모형의 성능판별이 어려워진다. 즉, 정확도(Accuracy)가 높아도 데이터 개수가 적은 클래스의 재현율(recall-rate)이 급격히 작아지는 현상이 발생할 수 있다. 이렇게 <code>각 클래스에 속한 데이터의 개수의 차이에 의해 발생하는 문제</code>들을 <code>비대칭 데이터 문제</code>(Imbalanced data problem)이라고 한다.</li></ul><p><img src="/image/what_is_imbalanced_data_problem.png" alt="데이터 불균형 문제 - 01"></p><ul><li>아래 코드와 그림은 SVM을 사용하여 각각 다변량(아래는 이변량) 정규분포를 갖는 비대칭 데이터와 대칭 데이터를 분류한 결과를 비교하는 코드이다. 우선 label마다 확실하게 구분되어질 수 있도록 서로 다른 평균을 갖는 이변량 정규분포에 샘플링하여 사용한다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import *</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import recall_score</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def classification_result(n0, n1, title=<span class="string">""</span>):</span><br><span class="line">    rv1 = sp.stats.multivariate_normal([-1, 0], [[1, 0], [0, 1]])</span><br><span class="line">    rv2 = sp.stats.multivariate_normal([+1, 0], [[1, 0], [0, 1]])</span><br><span class="line">    X0 = rv1.rvs(n0, random_state=0)</span><br><span class="line">    X1 = rv2.rvs(n1, random_state=0)</span><br><span class="line">    X = np.vstack([X0, X1])</span><br><span class="line">    y = np.hstack([np.zeros(n0), np.ones(n1)])</span><br><span class="line"></span><br><span class="line">    x1min = -4; x1max = 4</span><br><span class="line">    x2min = -2; x2max = 2</span><br><span class="line">    xx1 = np.linspace(x1min, x1max, 1000)</span><br><span class="line">    xx2 = np.linspace(x2min, x2max, 1000)</span><br><span class="line">    X1, X2 = np.meshgrid(xx1, xx2)</span><br><span class="line"></span><br><span class="line">    plt.contour(X1, X2, rv1.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=<span class="string">"dashed"</span>)</span><br><span class="line">    plt.contour(X1, X2, rv2.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=<span class="string">"dashed"</span>)</span><br><span class="line"></span><br><span class="line">    model = SVC(kernel=<span class="string">"linear"</span>, C=1e4, random_state=0).fit(X, y)</span><br><span class="line">    Y = np.reshape(model.predict(np.array([X1.ravel(), X2.ravel()]).T), X1.shape)</span><br><span class="line">    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker=<span class="string">'x'</span>, label=<span class="string">"0 클래스"</span>)</span><br><span class="line">    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker=<span class="string">'o'</span>, label=<span class="string">"1 클래스"</span>)</span><br><span class="line">    plt.contour(X1, X2, Y, colors=<span class="string">'k'</span>, levels=[0.5])</span><br><span class="line">    y_pred = model.predict(X)</span><br><span class="line">    plt.xlim(-4, 4)</span><br><span class="line">    plt.ylim(-3, 3)</span><br><span class="line">    plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> model, X, y, y_pred</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))    </span><br><span class="line">plt.subplot(121)</span><br><span class="line">model1, X1, y1, y_pred1 = classification_result(200, 200, <span class="string">"대칭 데이터 (5:5)"</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model2, X2, y2, y_pred2 = classification_result(200, 20, <span class="string">"비대칭 데이터 (9:1)"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">'Imbalanced_data_svc_example'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/Imbalanced_data_svc_example.png" alt="SVC를 이용한 대칭 데이터와 비대칭 데이터의 분류 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classification_report(y1, y_pred1))</span><br><span class="line"><span class="built_in">print</span>(classification_report(y2, y_pred2))</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.86      0.83      0.84       200</span><br><span class="line">         1.0       0.84      0.86      0.85       200</span><br><span class="line"></span><br><span class="line">    accuracy                           0.85       400</span><br><span class="line">   macro avg       0.85      0.85      0.85       400</span><br><span class="line">weighted avg       0.85      0.85      0.85       400</span><br><span class="line"></span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.96      0.98      0.97       200</span><br><span class="line">         1.0       0.75      0.60      0.67        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.95       220</span><br><span class="line">   macro avg       0.86      0.79      0.82       220</span><br><span class="line">weighted avg       0.94      0.95      0.94       220</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import roc_curve, confusion_matrix</span><br><span class="line"></span><br><span class="line">fpr1, tpr1, thresholds1 = roc_curve(y1, model1.decision_function(X1))</span><br><span class="line">fpr2, tpr2, thresholds2 = roc_curve(y2, model2.decision_function(X2))</span><br><span class="line"></span><br><span class="line">c1 = confusion_matrix(y1, y_pred1, labels=[1, 0])</span><br><span class="line">c2 = confusion_matrix(y2, y_pred2, labels=[1, 0])</span><br><span class="line">r1 = c1[0, 0] / (c1[0, 0] + c1[0, 1])</span><br><span class="line">r2 = c2[0, 0] / (c2[0, 0] + c2[0, 1])</span><br><span class="line">f1 = c1[1, 0] / (c1[1, 0] + c1[1, 1])</span><br><span class="line">f2 = c2[1, 0] / (c2[1, 0] + c2[1, 1])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12, 8))</span><br><span class="line">plt.plot(fpr1, tpr1, <span class="string">':'</span>, label=<span class="string">"대칭"</span>)</span><br><span class="line">plt.plot(fpr2, tpr2, <span class="string">'-'</span>, label=<span class="string">"비대칭"</span>)</span><br><span class="line">plt.plot([f1], [r1], <span class="string">'ro'</span>)</span><br><span class="line">plt.plot([f2], [r2], <span class="string">'ro'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'Fall-Out'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.title(<span class="string">'ROC 커브'</span>)</span><br><span class="line"><span class="comment"># plt.savefig("roc_curve_diiferent_result_between_balanced_and_Imbalanced")</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><ul><li>주황색 포인트가 현재 각 모델들의 성능을 의미한다. 대칭 데이터보다 비대칭 데이터를 사용하였을 경우 훨씬 좋지 않다.</li></ul><p><img src="/image/roc_curve_diiferent_result_between_balanced_and_Imbalanced.png" alt="SVC의 결과의 위치 및 ROC 커브"></p><ul><li>모형이 학습을 하면서 위에서 언급한 것 과 같이 소수의 데이터를 잘 학습하지 못하여 소수의 데이터를 잘 분류해 내지 못하는 모습을 아래의 그림에서도 확인할 수 있다. 빨간색 원형의 부분에도 소수 클래스의 데이터라 존재하지만 주변 다수 클래스가 많이 분포되어있어 분류해 내지 못했다.</li></ul><p><img src="/image/data_Imbalanced_examples_01.png" alt="데이터 불균형 문제의 예시 - 01"></p><ul><li>아래 그림은 oversampling을 통해 소수 클래스의 데이터의 비중을 늘려 주어 이전보다 학습을 많이 할 수 있도록 하는 방법을 택했을 경우의 학습 결과를 보여준다. 확실히, 이전에 decision boundary가 생성되어야 할 부분에 생성되어 진 것을 확인 할 수 있다. 또한, <code>이러한 방법은 모형내에 weight 파라미터가 존재할 경우 해당 소수 클래스에 더 많은 가중치를 줌으로써 동일한 결과를 도출해 낼 수 도 있을 것</code>이다.</li></ul><p><img src="/image/data_Imbalanced_examples_02.png" alt="데이터 불균형 문제의 예시 - 02"></p><ul><li><code>소수 클래스의 데이터를 충분히 학습하지 못하는 문제뿐만 아니라 다수 클래스의 데이터에 대한 예측 확률 값은 높을 수록 1로 예측하기 때문에 0에 가까이 수렴할 수 밖에 없다. 그러므로 본래의 threshold인 0.5를 낮춰 설정하여야 좀 더 예측의 성능을 높일 수 있을 것</code>이다. <code>threshold는 validation set을 통해 설정</code>해야 할 것이다.</li></ul><p><img src="/image/what_is_imbalanced_data_problem_02.png" alt="데이터 불균형 문제 - 02"></p><ul><li>또한, 성능지표를 설정하는 것에도 문제가 있다. 기존의 Accuracy만을 생각한다면 다수의 클래스를 잘 예측해내기만 한다면 성능이 높을 수 밖에 없다. 그러므로 <code>소수클래스의 비중이 적은 점을 고려하면서 성능을 비교할 수 있는 지표를 설정</code>해야 할 것이다. 그런 측면에서의 성능지표는 아래와 같이 2 가지를 많이 사용한다.</li></ul><p><img src="/image/considered_imbalanced_data_problem_metrics_01.png" alt="불균형 문제를 갖는 데이터에 대한 성능지표 - 01"></p><ul><li><code>실제 양성이라고 판단한 것 중 양성이라고 예측한 비율을 의미</code>하는 <code>recall(재현율)</code>과 <code>양성이라고 예측한 것 중 실제 양성인 비율을 의미</code>하는 <code>precision(정밀도)</code>의 조화평균이 F1-Score라고 할 수 있다. 두 지표를 모두 고려하는 지표인 것이다. 두 지표는 서로 트레이드 오프 관계를 갖고 있다. 3가지 성능 지표(recall, precision, f1-score) 모두를 구하여 비교해보는 것이 좋다.</li></ul><p><img src="/image/considered_imbalanced_data_problem_metrics_02.png" alt="불균형 문제를 갖는 데이터에 대한 성능지표 - 02"></p><ul><li>threshold가 변동되야 하므로 ROC curve를 그려보거나 AUC를 통한 지표를 설정하는 것도 좋은 방법이다.</li></ul><p><img src="/image/considered_imbalanced_data_problem_metrics_03.png" alt="불균형 문제를 갖는 데이터에 대한 성능지표 - 03"></p><h3 id="해결방법"><a href="#해결방법" class="headerlink" title="해결방법"></a>해결방법</h3><ul><li>위에서 잠깐 언급했듯이 Imbalanced data를 해결하기 위한 방법은 크게 2 가지로 소개 할 수 있다. 첫번째 방법은 리샘플링 방법으로 소수의 데이터를 부풀리는 <code>Over sampling</code> 과 다수의 데이터에서 일부만 사용하는 <code>Under sampling</code>, 그리고 마지막으로 두 가지 방법을 섞어 사용하는 <code>Hybrid sampling</code>이 있다. 두 번째 방법은 모형 자체의 학습하는 가중치를 소수 클래스에 더 주어 학습시키는 방법이다.</li></ul><p><img src="/image/how_to_overcome_Imbalanced_data_problem_with_resampling_01.png" alt="불균형 문제를 해결하기 위한 리샘플링 방법들"></p><ul><li>리샘플링 시 유의할 점은 <code>먼저 데이터세트의 클래스별 비율을 유지한 채 train, validation, test 세트로 나누어야 한다는 점</code>이다. 그리고 나서 <code>학습시킬 데이터에 대해서만 resampling 방법을 적용</code>시킨다는 점이다. 직관적으로 생각해 보면 위의 단계는 당연하지만, 초보자의 입장에선 헷갈릴수 있는 부분이기 때문에 언급하고 넘어가겠다.</li></ul><p><img src="/image/how_to_overcome_Imbalanced_data_problem_with_resampling_01.png" alt="불균형 문제를 해결하기 위한 리샘플링 방법들"></p><p><img src="/image/oversampling_example.png" alt="오버 샘플링을 통한 불균형 문제 해결 예시"></p><h4 id="Imbalanced-learn-패키지"><a href="#Imbalanced-learn-패키지" class="headerlink" title="Imbalanced-learn 패키지"></a>Imbalanced-learn 패키지</h4><ul><li>Imbalanced data 문제를 해결하기 위한 다양한 샘플링 방법을 구현한 Python 패키지</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U imbalanced-learn</span><br></pre></td></tr></table></figure><h3 id="Under-sampling"><a href="#Under-sampling" class="headerlink" title="Under sampling"></a>Under sampling</h3><ul><li><code>RandomUnderSampler</code> : random under-sampling method</li><li><code>TomekLinks</code> : Tomek’s link method</li><li><code>CondensedNearestNeighbour</code> : condensed nearest neighbor method</li><li><code>OneSidedSelection</code> : under-sampling based on one-sided selection method</li><li><code>EditedNearestNeighbours</code> : edited nearest neighbor method</li><li><code>NeighbourhoodCLeaningRule</code> : neighborhood cleaning rule</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">from imblearn.under_sampling import *</span><br><span class="line"></span><br><span class="line">n0 = 200; n1 = 20</span><br><span class="line">rv1 = sp.stats.multivariate_normal([-1, 0], [[1, 0], [0, 1]])</span><br><span class="line">rv2 = sp.stats.multivariate_normal([+1, 0], [[1, 0], [0, 1]])</span><br><span class="line">X0 = rv1.rvs(n0, random_state=0)</span><br><span class="line">X1 = rv2.rvs(n1, random_state=0)</span><br><span class="line">X_imb = np.vstack([X0, X1])</span><br><span class="line">y_imb = np.hstack([np.zeros(n0), np.ones(n1)])</span><br><span class="line"></span><br><span class="line">x1min = -4; x1max = 4</span><br><span class="line">x2min = -2; x2max = 2</span><br><span class="line">xx1 = np.linspace(x1min, x1max, 1000)</span><br><span class="line">xx2 = np.linspace(x2min, x2max, 1000)</span><br><span class="line">X1, X2 = np.meshgrid(xx1, xx2)</span><br><span class="line"></span><br><span class="line">def classification_result2(X, y, title=<span class="string">""</span>):</span><br><span class="line">    plt.contour(X1, X2, rv1.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=<span class="string">"dashed"</span>)</span><br><span class="line">    plt.contour(X1, X2, rv2.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=<span class="string">"dashed"</span>)</span><br><span class="line">    model = SVC(kernel=<span class="string">"linear"</span>, C=1e4, random_state=0).fit(X, y)</span><br><span class="line">    Y = np.reshape(model.predict(np.array([X1.ravel(), X2.ravel()]).T), X1.shape)</span><br><span class="line">    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker=<span class="string">'x'</span>, label=<span class="string">"0 클래스"</span>)</span><br><span class="line">    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker=<span class="string">'o'</span>, label=<span class="string">"1 클래스"</span>)</span><br><span class="line">    plt.contour(X1, X2, Y, colors=<span class="string">'k'</span>, levels=[0.5])</span><br><span class="line">    y_pred = model.predict(X)</span><br><span class="line">    plt.xlim(-4, 4)</span><br><span class="line">    plt.ylim(-3, 3)</span><br><span class="line">    plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="built_in">return</span> model</span><br></pre></td></tr></table></figure><hr><h4 id="Random-Under-Sampler"><a href="#Random-Under-Sampler" class="headerlink" title="Random Under-Sampler"></a>Random Under-Sampler</h4><ul><li>무작위로 데이터를 없애는 단순 샘플링</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = RandomUnderSampler(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = RandomUnderSampler(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'RandomUnderSampler로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('RandomUnderSampler_result_resampling')</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/RandomUnderSampler_result_resampling.png" alt="RandomUnderSampler로 리샘플링한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.92      0.95       200</span><br><span class="line">         1.0       0.51      0.90      0.65        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.91       220</span><br><span class="line">   macro avg       0.75      0.91      0.80       220</span><br><span class="line">weighted avg       0.95      0.91      0.92       220</span><br></pre></td></tr></table></figure><hr><h4 id="Tomek’s-link-method"><a href="#Tomek’s-link-method" class="headerlink" title="Tomek’s link method"></a>Tomek’s link method</h4><ul><li>토멕링크(Tomek’s link)란 서로 다른 클래스에 속하는 한 쌍의 데이터 $ (x_{+}, x_{-}) $ 로 서로에게 더 가까운 다른 데이터 존재하지 않는 상태이다. 클래스가 다른 두 데이터가 아주 가까이 붙어있으면 토멕링크가 된다. 토멕링크 방법은 이러한 토멕링크를 찾은 다음 그 중에서 다수 클래스에 속하는 데이터를 제외하는 방법으로 <code>경계선을 다수 클래스쪽으로 밀어붙이는 효과</code>가 있다.</li></ul><ul><li><code>소수쪽 경계선을 늘리자라는 방법이다.(단 없애지는 것들이 많은 수가 있지는 않다.) 반복가능!</code></li></ul><p><img src="/image/what_is_tomeklinks_under_sampling.png" alt="Tomek&#39;s link 방법"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = TomekLinks(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'Tomeks link method로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('TomekLinks_result_resampling')</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/TomekLinks_result_resampling.png" alt="Tomek&#39;s link method로 리샘플링 한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.97      0.97      0.97       200</span><br><span class="line">         1.0       0.70      0.70      0.70        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.95       220</span><br><span class="line">   macro avg       0.83      0.83      0.83       220</span><br><span class="line">weighted avg       0.95      0.95      0.95       220</span><br></pre></td></tr></table></figure><hr><h4 id="Condensed-Nearest-Neighbor"><a href="#Condensed-Nearest-Neighbor" class="headerlink" title="Condensed Nearest Neighbor"></a>Condensed Nearest Neighbor</h4><ul><li>CNN(Condensed Nearest Neighbor) 방법은 1-NN 모형으로 분류되지 않는 데이터만 남기는 방법이다.<ul><li>원래는 clustering에서 사용하는 방법이지만 응용해서 사용한다.</li><li>랜덤하게 하나를 고른 후, 그 다음에는 1-NN방법을 사용한다. 그 다음 선택된 것과 만약 같은 클래스(다수 클래스)이면 안 뽑고 다른 클래스(소수 클래스)이면 뽑음.</li></ul></li></ul><ul><li><p>선택된 데이터 집합을 $ S $ 라고 하자.</p><ul><li>1) 소수 클래스 데이터를 모두 $ S $ 에 포함시킨다.</li><li>2) 다수 데이터 중에서 하나를 골라서 가장 가까운 데이터가 다수 클래스이면 포함시키지 않고 아니면 $ S $ 에 포함시킨다.</li><li>3) 더 이상 선택되는 데이터가 없을 때 까지 2를 반복한다.</li></ul></li></ul><ul><li><p>이 방법을 사용하면 기존에 선택된 데이터와 가까이 있으면서 같은 클래스인 데이터는 선택되지 않기 때문에 다수 데이터의 경우 선택되는 비율이 적어진다.</p><ul><li><p>허나, <code>가장 가까운 데이터가 소수 클래스인 경우 집합에 포함되겠지만 그만큼 아래 그림처럼 소수 클래스 집단과 거리가 가깝다면 두 클래스를 분류하는데 도움을 주진 못할 것</code>이다.</p></li><li><p>CNN 자체는 <code>경계선을 살리는 역할</code>을 하기 때문에 경계선 보다 멀거나 적은 클래스 주변에 없으면 제거된다. 그러므로 <code>홀로 이걸 사용한다기 보다는 다른 사용에 중간과정에 사용하기도한다.</code></p></li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = CondensedNearestNeighbour(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'CondensedNearestNeighbour로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('CondensedNearestNeighbour_result_resampling')</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/CondensedNearestNeighbour_result_resampling.png" alt="CondensedNearestNeighbour로 리샘플링 한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.96      0.98      0.97       200</span><br><span class="line">         1.0       0.75      0.60      0.67        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.95       220</span><br><span class="line">   macro avg       0.86      0.79      0.82       220</span><br><span class="line">weighted avg       0.94      0.95      0.94       220</span><br></pre></td></tr></table></figure><hr><h4 id="One-Sided-Selection"><a href="#One-Sided-Selection" class="headerlink" title="One Sided Selection"></a>One Sided Selection</h4><ul><li>One Sided Selection은 토맥링크 방법과 Condensed Nearest Neighbour 방법을 섞은 것이다. 토맥링크 중 다수 클래스를 제외하고 나머지 데이터 중에서도 서로 붙어있는 다수 클래스 데이터는 1-NN 방법으로 제외한다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = OneSidedSelection(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'OneSidedSelection로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('OneSidedSelection_result_resampling')</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/OneSidedSelection_result_resampling.png" alt="OneSidedSelection으로 리샘플링 한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.97      0.97      0.97       200</span><br><span class="line">         1.0       0.70      0.70      0.70        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.95       220</span><br><span class="line">   macro avg       0.83      0.83      0.83       220</span><br><span class="line">weighted avg       0.95      0.95      0.95       220</span><br></pre></td></tr></table></figure><hr><h4 id="Edited-Nearest-Neighbours"><a href="#Edited-Nearest-Neighbours" class="headerlink" title="Edited Nearest Neighbours"></a>Edited Nearest Neighbours</h4><ul><li>ENN(Edited Nearest Neighbours) 방법은 다수 클래스 데이터 중 가장 가까운 k(<code>n_neighbors</code>)개의 데이터가 모두(<code>kind_sel=&#39;all&#39;</code>) 또는 다수(<code>kind_sel=&#39;mode&#39;</code>) 다수 클래스가 아니면 삭제하는 방법이다. 소수 클래스 주변의 다수 클래스 데이터는 사라진다.</li></ul><ul><li>그러므로 <code>가까운 k개 중 소수 클래스를 지닌 데이터들은 모두 제거되어 소수 클래스와 다수 클래스 간의 구분이 상대적으로 명확</code>해지게 된다.</li></ul><ul><li>CNN과 비슷하지만, 모든 데이터에 대해서 주변에 제일 가까운 k개를 지정해줘서 주변에 다수 데이터가 많거나 또는 모두 다수 데이터가 아니면 그 데이터를 삭제하여 <code>경계선에 있는 애들 중 다수클래스가 사라져서 위에 말한 것과 동일한 효과</code>를 준다.</li></ul><ul><li><code>knn방법은 데이터간의 모든 거리를 구하기 때문에 데이터 갯수가 많으면 사용하기 힘들다.</code></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = EditedNearestNeighbours(kind_sel=<span class="string">"all"</span>, n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'EditedNearestNeighbours로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('EditedNearestNeighbours_result_resampling')</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/EditedNearestNeighbours_result_resampling.png" alt="EditedNearestNeighbours으로 리샘플링 한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.94      0.96       200</span><br><span class="line">         1.0       0.58      0.90      0.71        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.93       220</span><br><span class="line">   macro avg       0.79      0.92      0.83       220</span><br><span class="line">weighted avg       0.95      0.93      0.94       220</span><br></pre></td></tr></table></figure><hr><h4 id="Neighbourhood-Cleaning-Rule"><a href="#Neighbourhood-Cleaning-Rule" class="headerlink" title="Neighbourhood Cleaning Rule"></a>Neighbourhood Cleaning Rule</h4><ul><li>CNN(Condensed Nearest Neighbour) 방법과 ENN(Edited Nearest Neighbours) 방법을 섞은 것이다.</li></ul><ul><li>가장 가까운 데이터가 소수데이터가 아니거나, 가장 가까운 k개가 다수 클래스이거나 다수가 다수 클래스인 데이터를 제거하는 방법이므로 경계선을 너무 명확히 해주는 것을 방지한 효과를 줄 수 있다고 생각한다. ENN을 통해 소수 클래스 데이터 주변을 너무 많이 제거하는 것을 방지한다고 보면 될 것 같다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = NeighbourhoodCleaningRule(kind_sel=<span class="string">"all"</span>, n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'NeighbourhoodCleaningRule로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('NeighbourhoodCleaningRule_result_resampling')</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/NeighbourhoodCleaningRule_result_resampling.png" alt="NeighbourhoodCleaningRule으로 리샘플링 한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.93      0.96       200</span><br><span class="line">         1.0       0.56      0.95      0.70        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.93       220</span><br><span class="line">   macro avg       0.78      0.94      0.83       220</span><br><span class="line">weighted avg       0.96      0.93      0.94       220</span><br></pre></td></tr></table></figure><hr><h3 id="Over-sampling"><a href="#Over-sampling" class="headerlink" title="Over sampling"></a>Over sampling</h3><ul><li><code>RandomOverSampler</code> : random sampler</li><li><code>ADASYN</code> : Adaptive Synthetic Sampling Approach for Imbalanced Learning</li><li><code>SMOTE</code> : Synthetic Minority Over-sampling Technique</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from imblearn.over_sampling import *</span><br></pre></td></tr></table></figure><hr><h4 id="RandomOverSampler"><a href="#RandomOverSampler" class="headerlink" title="RandomOverSampler"></a>RandomOverSampler</h4><ul><li>Random Over Sampling은 소수 클래스의 데이터를 반복해서 넣는 것(replacement)이다. <code>가중치를 증가시키는 것과 비슷</code>하다.</li></ul><ul><li><code>오른쪽과 왼쪽이 변화가 없는 것 처럼 보이지만 오른쪽은 똑같은 데이터를 복제하는 것이기 때문에 숫자를 늘려 경계선을 밀어버린다.</code></li></ul><p><img src="/image/what_is_Random_over_sampling_conception.png" alt="Random Over Sampling이란"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = RandomOverSampler(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'RandomOverSampler로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('RandomOverSampler_result_resampling')</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/RandomOverSampler_result_resampling.png" alt="RandomOverSampler로 리샘플링 한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.91      0.95       200</span><br><span class="line">         1.0       0.51      0.95      0.67        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.91       220</span><br><span class="line">   macro avg       0.75      0.93      0.81       220</span><br><span class="line">weighted avg       0.95      0.91      0.92       220</span><br></pre></td></tr></table></figure><hr><h4 id="ADASYN"><a href="#ADASYN" class="headerlink" title="ADASYN"></a>ADASYN</h4><ul><li><code>소수 데이터를 랜덤하게 두 포인트를 고른 후 직선으로 이어 그 선 사이의 랜덤한 위치에 데이터를 새로 생성</code>한다.</li></ul><ul><li>ADASYN(Adaptive Synthetic Sampling) 방법은 <code>소수 클래스 데이터와 그 데이터에서 가장 가까운 k개의 소수 클래스 데이터 중 무작위로 선택된 데이터 사이의 직선상에 가상의 소수 클래스 데이터를 만드는 방법</code>이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = ADASYN(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'ADASYN로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('ADASYN_result_resampling')</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/ADASYN_result_resampling.png" alt="ADASYN으로 리샘플링 한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.90      0.94       200</span><br><span class="line">         1.0       0.47      0.95      0.63        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.90       220</span><br><span class="line">   macro avg       0.73      0.92      0.79       220</span><br><span class="line">weighted avg       0.95      0.90      0.91       220</span><br></pre></td></tr></table></figure><hr><h4 id="SMOTE"><a href="#SMOTE" class="headerlink" title="SMOTE"></a>SMOTE</h4><ul><li>SMOTE(Synthetic Minority Over-sampling Technique) 방법도 <code>ADASYN 방법처럼 데이터를 생성하지만 생성된 데이터를 무조건 소수 클래스라고 하지 않고 분류 모형에 따라 분류</code>한다. 따라서 <code>순수하게 소수 클래스만 sampling을 하지는 않는다.</code></li></ul><p><img src="/image/SMOTE_view.png" alt="SMOTE 방법"></p><ul><li>또한, 실행할 때 마다 랜덤하게 데이터가 생성되므로 결과는 매번 다르다.</li></ul><p><img src="/image/what_is_SMOTE_sampling_conception.png" alt="SMOTE 방법의 작동 원리"></p><ul><li><code>oversampling하는 대상이 전체 소수 클래스의 데이터이므로 noise로 판단되어 질 수 있는 밀집되어 나타나지 않는 소수 클래스의 데이터에 대해서도 모두 샘플링하므로 주의</code>하자.</li></ul><p><img src="/image/SMOTE_disadvantages_plot.png" alt="SMOTE를 할 경우 주의할 점"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = SMOTE(random_state=4).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'SMOTE로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('SMOTE_result_resampling')</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/SMOTE_result_resampling.png" alt="SMOTE로 리샘플링 한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.91      0.95       200</span><br><span class="line">         1.0       0.50      0.90      0.64        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.91       220</span><br><span class="line">   macro avg       0.74      0.91      0.80       220</span><br><span class="line">weighted avg       0.94      0.91      0.92       220</span><br></pre></td></tr></table></figure><hr><h4 id="BLSMOTE"><a href="#BLSMOTE" class="headerlink" title="BLSMOTE"></a>BLSMOTE</h4><ul><li><code>Borderline에 있는 데이터는 class Imbalanced problem에 큰 영향을 미친다고 판단하여 해당 dataset에만 SMOTE를 적용하는 방법</code>이다. 애초에 Decision boundary에 영향을 미칠수 있는 데이터를 리샘플링하겠다는 의도이다.</li></ul><p><img src="/image/What_is_BLSMOTE_conception.png" alt="BLSMOTE의 개념 및 작동원리"></p><ul><li>noise같이 판단되어지는 데이터에 대해서도 oversampling을 하는 SMOTE의 단점을 보완하여 noise라는 카테고리로 분류해 새로 리샘플링하는 데이터에 소수 클래스 데이터로 인해 과적합되지 않도록 해준다.</li></ul><p><img src="/image/overcoming_SMOTE_with_BLSMOTE.png" alt="BLSMOTE의 장점"></p><h4 id="DBSMOTE-DBSCAN-SMOTE"><a href="#DBSMOTE-DBSCAN-SMOTE" class="headerlink" title="DBSMOTE(DBSCAN SMOTE)"></a>DBSMOTE(DBSCAN SMOTE)</h4><ul><li>DBSCAN cluster 생성 후, cluster 내에서 SMOTE를 적용하는 방법이다.</li></ul><p><img src="/image/what_is_DBSMOTE_conception.png" alt="DBSMOTE 개념 및 작동원리"></p><ul><li>DBSCAN Cluster 를 진행하기에 군집의 중심과 이어지는 경향이 있으며, 이 또한 <code>BLSMOTE와 비슷하게 원래 DBSCAN의 장점 중 하나인 noise를 제거하여 샘플링해준다.</code></li></ul><p><img src="/image/DBSMOTE_result_plot.png" alt="DBSMOTE의 결과"></p><ul><li>데이터 마다 편차가 크므로, 실험적으로 해본뒤 해당 데이터에 잘 맞는 샘플링 방법을 사용해야 한다.</li></ul><p><img src="/image/Oversampling_disadvantages.png" alt="Oversampling의 단점"></p><h3 id="복합-샘플링"><a href="#복합-샘플링" class="headerlink" title="복합 샘플링"></a>복합 샘플링</h3><ul><li><code>SMOTEENN</code> : SMOTE + ENN</li><li><code>SMOTETomek</code> : SMOTE + Tomek</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from imblearn.combine import *</span><br></pre></td></tr></table></figure><hr><h4 id="SMOTE-ENN"><a href="#SMOTE-ENN" class="headerlink" title="SMOTE + ENN"></a>SMOTE + ENN</h4><ul><li>SMOTE+ENN 방법은 SMOTE(Synthetic Minority Over-sampling Technique) 방법과 ENN(Edited Nearest Neighbours) 방법을 섞은 것이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = SMOTEENN(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'SMOTE +  ENN으로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('SMOTE_and_ENN_result_resampling')</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/SMOTE_and_ENN_result_resampling.png" alt="SMOTE + ENN으로 리샘플링 한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.92      0.96       200</span><br><span class="line">         1.0       0.54      0.95      0.69        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.92       220</span><br><span class="line">   macro avg       0.77      0.94      0.82       220</span><br><span class="line">weighted avg       0.95      0.92      0.93       220</span><br></pre></td></tr></table></figure><hr><h4 id="SMOTE-Tomek"><a href="#SMOTE-Tomek" class="headerlink" title="SMOTE+Tomek"></a>SMOTE+Tomek</h4><ul><li>SMOTE+Tomek 방법은 SMOTE(Synthetic Minority Over-sampling Technique) 방법과 토멕링크 방법을 섞은 것이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = SMOTETomek(random_state=4).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'SMOTE + Tomek으로 리샘플링한 결과'</span>)</span><br><span class="line">plt.savefig(<span class="string">'SMOTETomek_result_resampling'</span>)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/SMOTETomek_result_resampling.png" alt="SMOTE + Tomek으로 리샘플링 한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.92      0.95       200</span><br><span class="line">         1.0       0.51      0.90      0.65        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.91       220</span><br><span class="line">   macro avg       0.75      0.91      0.80       220</span><br><span class="line">weighted avg       0.95      0.91      0.92       220</span><br></pre></td></tr></table></figure><hr>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/06/06/machine_learning_20/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Clustering - Hierarchical, DBSCAN, Affinity Propagation</title>
      <link>https://heung-bae-lee.github.io/2020/06/04/machine_learning_19/</link>
      <guid>https://heung-bae-lee.github.io/2020/06/04/machine_learning_19/</guid>
      <pubDate>Thu, 04 Jun 2020 13:46:15 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Hierarchical-clustering-계층적-군집화&quot;&gt;&lt;a href=&quot;#Hierarchical-clustering-계층적-군집화&quot; class=&quot;headerlink&quot; title=&quot;Hierarchical clustering(계층적 군집
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Hierarchical-clustering-계층적-군집화"><a href="#Hierarchical-clustering-계층적-군집화" class="headerlink" title="Hierarchical clustering(계층적 군집화)"></a>Hierarchical clustering(계층적 군집화)</h2><ul><li><code>계층적 군집화(hierachical clustering)는 여러개의 군집 중에서 가장 유사도가 높은 혹은 거리가 가까운 군집 두개를 선택하여 하나로 합치면서 군집 개수를 줄여 가는 방법</code>을 말한다. 합체 군집화(agglomerative clustering)라고도 한다. 가장 처음에는 모든 군집이 하나의 데이터만을 가진다. 따라서 <code>최초에는 데이터 개수만큼 군집이 존재하지만 군집을 합치면서 최종적으로 하나의 군집만 남게 된다.</code></li></ul><p><img src="/image/Hierachical_distance_conception_01.png" alt="계층적 군집화 개념 - 01"></p><p><img src="/image/Hierachical_distance_conception_02.png" alt="계층적 군집화 개념 - 02"></p><p><img src="/image/Hierachical_distance_conception_03.png" alt="계층적 군집화 개념 - 03"></p><p><img src="/image/Hierachical_distance_conception_04.png" alt="계층적 군집화 개념 - 04"></p><h3 id="군집간의-거리-측정"><a href="#군집간의-거리-측정" class="headerlink" title="군집간의 거리 측정"></a>군집간의 거리 측정</h3><ul><li>계층적 군집화를 하려면 우선 모든 군집 간에 거리를 측정해야 한다. <code>군집 간의 거리를 측정하는 방법에는 계층적 방법에 의존하지 않는 비계층적 방법과 이미 이전 단계에서 계층적 방법으로 군집이 합쳐진 적이 있다는 가정을 하는 계층적 방법 두 가지가 있다.</code></li></ul><h3 id="비계층적-거리-측정법"><a href="#비계층적-거리-측정법" class="headerlink" title="비계층적 거리 측정법"></a>비계층적 거리 측정법</h3><ul><li><code>비계층적 거리측정법</code>은 계층적 군집화가 아니더라도 <code>모든 경우에 사용할 수 있는 거리 측정 방법</code>이다. 중심거리, 단일거리, 완전거리, 평균거리 등이 있다. 다음에 설명할 계층적 거리측정법에 비해 <code>계산량이 많은 단점</code>이 있다.</li></ul><p><img src="/image/non_hierachical_distance_types.png" alt="비계층적 거리 측정 방법들"></p><h4 id="중심-centroid-거리"><a href="#중심-centroid-거리" class="headerlink" title="중심(centroid)거리"></a>중심(centroid)거리</h4><ul><li>두 군집의 중심점(centroid)를 정의한 다음 두 중심점의 거리를 군집간의 거리로 정의한다.</li></ul><script type="math/tex; mode=display">d(u,v) = \|c_u - c_v\|_2</script><ul><li>여기에서 $ d(u, v) $ 는 군집 $ u $ 와 군집  $ v $ 사이의 거리를 뜻한다. 또한 $ c_{u} $ 와 $ c_{v} $ 는 각각 두 군집 $ u $ 와 $ v $ 의 중심점이다. 군집의 중심점은 그 군집에 포함된 모든 데이터의 평균을 사용한다.</li></ul><script type="math/tex; mode=display">c_u = \dfrac{1}{|u|}\sum_i u_i</script><ul><li>이 식에서 $ |\cdot| $ 기호는 군집의 원소의 개수를 말한다.</li></ul><h4 id="단일-single-거리"><a href="#단일-single-거리" class="headerlink" title="단일(single)거리"></a>단일(single)거리</h4><ul><li>군집 $ u $ 의 모든 데이터 $ u_{i} $ 와 군집 $ v $ 의 모든 데이터 $ v_{j} $ 의 모든 조합에 대해 데이터 사이의 거리 $ d( u_{i}, v_{j} ) $ 를 측정해서 가장 작은 값을 구한다. 최소 거리(Nearest Point) 방법이라고도 한다.</li></ul><script type="math/tex; mode=display">d(u,v) = \min(d(u_i,v_j))</script><h4 id="완전-complete-거리"><a href="#완전-complete-거리" class="headerlink" title="완전(complete) 거리"></a>완전(complete) 거리</h4><ul><li>군집 $ u $ 의 모든 데이터 $ u_{i} $ 와 군집 $ v $ 의 모든 데이터 $ v_{j} $ 의 모든 조합에 대해 데이터 사이의 거리 $ d(u_{i}, v_{j}) $ 를 측정해서 가장 큰 값을 구한다. 최장 거리(Farthest Point) 방법이라고도 한다.</li></ul><script type="math/tex; mode=display">d(u,v) = \max(d(u_i,v_j))</script><h4 id="평균-average-거리"><a href="#평균-average-거리" class="headerlink" title="평균(average) 거리"></a>평균(average) 거리</h4><ul><li>군집 $ u $ 의 모든 데이터 $ u_{i} $ 와 군집 $ v $ 의 모든 데이터 $ v_{j} $ 의 모든 조합에 대해 데이터 사이의 거리 $ d(u_{i}, v_{j}) $ 를 측정해서 평균을 구한다. $ |u| $ 와 $ |v| $ 는 각각 두 군집의 원소의 개수를 뜻한다.</li></ul><script type="math/tex; mode=display">d(u,v) = \sum_{i,j} \frac{d(u_i, v_j)}{|u||v|}</script><h3 id="계층적-거리-측정법"><a href="#계층적-거리-측정법" class="headerlink" title="계층적 거리 측정법"></a>계층적 거리 측정법</h3><ul><li>계층적 거리 측정법은 <code>계층적 군집화에서만 사용할 수 있는 방법</code>이다. 즉, 이전 단계에서 이미 어떤 두 개의 군집이 하나로 합쳐진 적이 있다고 가정하여 이 정보를 사용하는 측정법이다. <code>비계층적 거리 측정법에 비해 계산량이 적어 효율적</code>이다.</li></ul><h4 id="중앙값-median-거리"><a href="#중앙값-median-거리" class="headerlink" title="중앙값(median)거리"></a>중앙값(median)거리</h4><ul><li>이 방법은 중심거리 방법의 변형이다. 중심거리 방법처럼 군집의 중심점의 거리를 군집간의 거리라고 한다. 하지만 군집의 중심점을 계산하는 방법이 다르다. 만약 군집 $ u $ 가 군집 $ s $ 와 군집 $ t $ 가 결합하여 생겼다면</li></ul><script type="math/tex; mode=display">u \leftarrow s + t</script><ul><li>군집 $ u $ 의 중심점은 새로 계산하지 않고 원래 군집의 두 군집의 중심정의 평균을 사용한다.</li></ul><script type="math/tex; mode=display">c_u = \dfrac{1}{2}(c_s + s_t)</script><ul><li>따라서 해당 군집의 모든 데이터를 평균하여 중심점을 구하는 것 보다 계산이 빠르다.</li></ul><h4 id="가중-weighted-거리"><a href="#가중-weighted-거리" class="headerlink" title="가중(weighted)거리"></a>가중(weighted)거리</h4><ul><li>가중거리를 사용하려면 원래 어떤 두 개의 군집이 합쳐져서 하나의 군집이 만들어졌는지 알아야 한다. 만약 군집 $ u $ 가 군집 $ s $ 와 군집 $ t $ 가 결합하여 생겼다면</li></ul><script type="math/tex; mode=display">u \leftarrow s + t</script><ul><li>이 군집 $ u $ 와 다른 군집 $ v $ 사이의 거리는 군집 $ u $ 를 구성하는 원래 군집 $ s $, $ t $ 와 $ v $ 사이의 두 거리의 평균을 사용한다.</li></ul><script type="math/tex; mode=display">d(u,v) = \dfrac{1}{2}(d(s,v) + d(t,v))</script><p><img src="/image/weighted_distance_hierachical_distance.png" alt="계층적 거리 - 가중 거리"></p><h4 id="와드-Ward-거리"><a href="#와드-Ward-거리" class="headerlink" title="와드(Ward)거리"></a>와드(Ward)거리</h4><p><img src="/image/hierachical_distance_type_ward_distance.png" alt="계층적 거리 - 와드 거리 개념"></p><ul><li>와드 거리는 <code>가중거리 방법의 변형</code>이다. 만약 군집 $ u $ 가 군집 $ s $ 와 군집 $ t $ 가 결합하여 생겼다면</li></ul><script type="math/tex; mode=display">u \leftarrow s + t</script><ul><li>이 군집 $ u $ 와 다른 군집 $ v $ 사이의 거리를 구하는데 있어서 군집 $ u $ 를 구성하는 원래 군집 $ s $, $ t $ 와  $ v $ 사이의 거리를 사용하는 것은 가중거리 방법과 같지만 원래의 두 군집 $ s $, $ t $ 가 너무 가까우면 $ v $ 와의 거리가 더 먼것으로 인식한다.</li></ul><script type="math/tex; mode=display">d(u,v) = \sqrt{\frac{|v|+|s|}{|v|+|s|+|t|}d(v,s)^2 + \frac{|v|+|t|}{|v|+|s|+|t|}d(v,t)^2 - \frac{|v|}{|v|+|s|+|t|}d(s,t)^2}</script><p><img src="/image/ward_distance_hierachical_distance.png" alt="계층적 거리 - 와드 거리"></p><h4 id="Scipy의-계층적-군집화"><a href="#Scipy의-계층적-군집화" class="headerlink" title="Scipy의 계층적 군집화"></a>Scipy의 계층적 군집화</h4><ul><li>Python으로 계층적 군집화를 하려면 Scipy 패키지의 <code>linkage</code> 명령을 사용하거나 Scikit-Learn 패키지의 <code>AgglomerativeClustering</code> 클래스를 사용한다. 각각 장단점이 있는데 Scipy 패키지는 군집화 결과를 트리 형태로 시각화해주는 <code>dendrogram</code> 명령도 지원한다.</li></ul><ul><li>MNIST digit 이미지 중 20개의 이미지를 무작위로 골라 계층적 군집화를 적용해 볼 것이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line"></span><br><span class="line">digits = load_digits()</span><br><span class="line">n_image = 20</span><br><span class="line">np.random.seed(0)</span><br><span class="line">idx = np.random.choice(range(len(digits.images)), n_image)</span><br><span class="line">X = digits.data[idx]</span><br><span class="line">images = digits.images[idx]</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(20, 15))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_image):</span><br><span class="line">    plt.subplot(1, n_image, i + 1)</span><br><span class="line">    plt.imshow(images[i], cmap=plt.cm.bone)</span><br><span class="line">    plt.grid(False)</span><br><span class="line">    plt.xticks(())</span><br><span class="line">    plt.yticks(())</span><br><span class="line">    plt.title(i)</span><br><span class="line"><span class="comment"># plt.savefig("random_sample_MNIST")</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/random_sample_MNIST.png" alt="임의로 추출한 MNIST 이미지"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from scipy.cluster.hierarchy import linkage, dendrogram</span><br><span class="line"></span><br><span class="line">Z = linkage(X, <span class="string">'ward'</span>)</span><br><span class="line">Z</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">array([[ 3.        , 18.        , 23.51595203,  2.        ],</span><br><span class="line">       [13.        , 19.        , 25.27844932,  2.        ],</span><br><span class="line">       [ 1.        , 14.        , 28.67054237,  2.        ],</span><br><span class="line">       [17.        , 21.        , 31.04298096,  3.        ],</span><br><span class="line">       [ 4.        ,  7.        , 31.51190251,  2.        ],</span><br><span class="line">       [ 6.        ,  8.        , 32.54228019,  2.        ],</span><br><span class="line">       [ 9.        , 10.        , 33.36165464,  2.        ],</span><br><span class="line">       [ 0.        , 24.        , 34.51086785,  3.        ],</span><br><span class="line">       [ 2.        , 22.        , 37.03151811,  3.        ],</span><br><span class="line">       [11.        , 26.        , 43.25505751,  3.        ],</span><br><span class="line">       [12.        , 15.        , 45.31004304,  2.        ],</span><br><span class="line">       [16.        , 20.        , 45.36151085,  3.        ],</span><br><span class="line">       [ 5.        , 27.        , 53.54437412,  4.        ],</span><br><span class="line">       [30.        , 32.        , 56.6892112 ,  6.        ],</span><br><span class="line">       [25.        , 29.        , 60.16809786,  5.        ],</span><br><span class="line">       [28.        , 34.        , 66.61618922,  8.        ],</span><br><span class="line">       [31.        , 33.        , 70.35228813,  9.        ],</span><br><span class="line">       [23.        , 36.        , 80.11172754, 12.        ],</span><br><span class="line">       [35.        , 37.        , 93.57946712, 20.        ]])</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from matplotlib.offsetbox import OffsetImage, AnnotationBbox</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 4))</span><br><span class="line">ax = plt.subplot()</span><br><span class="line"></span><br><span class="line">ddata = dendrogram(Z)</span><br><span class="line"></span><br><span class="line">dcoord = np.array(ddata[<span class="string">"dcoord"</span>])</span><br><span class="line">icoord = np.array(ddata[<span class="string">"icoord"</span>])</span><br><span class="line">leaves = np.array(ddata[<span class="string">"leaves"</span>])</span><br><span class="line">idx = np.argsort(dcoord[:, 2])</span><br><span class="line">dcoord = dcoord[idx, :]</span><br><span class="line">icoord = icoord[idx, :]</span><br><span class="line">idx = np.argsort(Z[:, :2].ravel())</span><br><span class="line">label_pos = icoord[:, 1:3].ravel()[idx][:20]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(20):</span><br><span class="line">    imagebox = OffsetImage(images[i], cmap=plt.cm.bone_r, interpolation=<span class="string">"bilinear"</span>, zoom=3)</span><br><span class="line">    ab = AnnotationBbox(imagebox, (label_pos[i], 0),  box_alignment=(0.5, -0.1),</span><br><span class="line">                        bboxprops=&#123;<span class="string">"edgecolor"</span> : <span class="string">"none"</span>&#125;)</span><br><span class="line">    ax.add_artist(ab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plt.grid()</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/MNIST_hierachical_clustering_random_index_20.png" alt="MNIST 임의 20개 이미지의 계층적 군집화 결과"></p><h3 id="DBSCAN-Clustering"><a href="#DBSCAN-Clustering" class="headerlink" title="DBSCAN Clustering"></a>DBSCAN Clustering</h3><ul><li>K-means 군집화 방법은 단순하고 강력한 방법이지만 군집의 모양이 원형이 아닌 경우에는 잘 동작하지 않으며 군집의 개수를 사용자가 지정해 주어야 한다는 단점이 있다.</li></ul><ul><li><p>DBSCAN(Density-Based Spatial Clustering of Application with Noise) 군집화 방법은 데이터가 밀집한 정도 즉, 밀도를 이용한다. DBSCAN 군집화는 군집의 형태에 구애받지 않으며 군집의 갯수를 사용자가 지정할 필요가 없다. DBSCAN 군집화 방법에서는 초기 데이터로부터 근접한 데이터를 찾아나가는 방법으로 군집을 확장한다. 이 때 다음 사용자 인수를 사용한다.</p><ul><li>최소거리 $ \varepsilon $ : 이웃(neighborhood)을 정의하기 위한 거리</li><li>최소 데이터 개수(minimum points) : 밀집지역을 정의하기 위해 필요한 이웃의 개수</li></ul></li></ul><p><img src="/image/what_is_DBSCAN_cinception.png" alt="DBSCAN 개념"></p><ul><li>만약 $ \varepsilon $ 최소 거리 안의 이웃 영역 안에 최소 데이터 개수 이상의 데이터가 있으면  그 데이터는 핵심(core) 데이터이다. 이렇게 핵심 데이터를 찾아낸 다음에는 이 핵심 데이터의 이웃 영역 안에 있는 데이터를 이 핵심데이터와 연결된 핵심 데이터로 정의한다. 핵심 데이터의 이웃 영역안에 있는 데이터도 마찬가지로 연결된 핵심 데이터가 된다. 만약 고밀도 데이터에 더이상 이웃이 없으면 이 데이터는 경계(border) 데이터라고 한다. 핵심 데이터도 아니고 경계 데이터도 아닌 데이터를 outlier라고 한다.</li></ul><p><img src="/image/how_to_DBSCAN_clustering.png" alt="DBSCAN"></p><p><img src="/image/how_to_make_cluster_in_DBSCAN_Algorithme.png" alt="DBSCAN 알고리즘"></p><p><img src="/image/DBSCAN_examples.png" alt="DBSCAN 알고리즘 예시들"></p><ul><li>Scikit-learn의 cluster 서브패키지는 DBSCAN 클래스를 제공한다. 다음과 같은 인수를 받을 수 있다.<ul><li><code>eps</code> : 이웃을 정의하기 위한 거리. epsilon</li><li><code>min_samples</code> : 핵심 데이터를 정의하기 위해 필요한 이웃영역안의 데이터 개수.</li></ul></li></ul><p><img src="/image/DBSCAN_parameters_setting_tip.png" alt="DBSCAN 파라미터 설정시 유의점"></p><ul><li>군집화가 끝나면 객체는 다음 속성을 갖는다.<ul><li><code>labels_</code> : 군집번호. 아웃라이어는 -1 값을 갖는다.</li><li><code>core_sample_indices_</code> : 핵심 데이터의 인덱스. 여기에 포함되지 않고 outlier도 아닌 데이터는 경계 데이터이다.</li></ul></li></ul><p><img src="/image/DBSCAN_advantages_disadvantages.png" alt="DBSCAN 군집화의 장단점"></p><ul><li>다음은 <code>make_circles</code> 명령과 <code>make_moons</code> 명령으로 만든 동심원, 초승달 데이터를 DBSCAN으로 군집화한 결과를 나타낸 것이다. 마커(marker)의 모양은 군집을 나타내고 마커의 크기가 큰 데이터는 핵심데이터, x 표시된 데이터는 outlier이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_circles, make_moons</span><br><span class="line">from sklearn.cluster import DBSCAN</span><br><span class="line"></span><br><span class="line">n_samples = 1000</span><br><span class="line">np.random.seed(2)</span><br><span class="line">X1, y1 = make_circles(n_samples=n_samples, factor=.5, noise=.09)</span><br><span class="line">X2, y2 = make_moons(n_samples=n_samples, noise=.1)</span><br><span class="line"></span><br><span class="line">def plot_DBSCAN(title, X, eps, xlim, ylim):</span><br><span class="line">    model = DBSCAN(eps=eps)</span><br><span class="line">    y_pred = model.fit_predict(X)</span><br><span class="line">    idx_outlier = model.labels_ == -1</span><br><span class="line">    plt.scatter(X[idx_outlier, 0], X[idx_outlier, 1], marker=<span class="string">'x'</span>, lw=1, s=20)</span><br><span class="line">    plt.scatter(X[model.labels_ == 0, 0], X[model.labels_ == 0, 1], marker=<span class="string">'o'</span>, facecolor=<span class="string">'g'</span>, s=5)</span><br><span class="line">    plt.scatter(X[model.labels_ == 1, 0], X[model.labels_ == 1, 1], marker=<span class="string">'s'</span>, facecolor=<span class="string">'y'</span>, s=5)</span><br><span class="line">    X_core = X[model.core_sample_indices_, :]</span><br><span class="line">    idx_core_0 = np.array(list(<span class="built_in">set</span>(np.where(model.labels_ == 0)[0]).intersection(model.core_sample_indices_)))</span><br><span class="line">    idx_core_1 = np.array(list(<span class="built_in">set</span>(np.where(model.labels_ == 1)[0]).intersection(model.core_sample_indices_)))</span><br><span class="line">    plt.scatter(X[idx_core_0, 0], X[idx_core_0, 1], marker=<span class="string">'o'</span>, facecolor=<span class="string">'g'</span>, s=80, alpha=0.3)</span><br><span class="line">    plt.scatter(X[idx_core_1, 0], X[idx_core_1, 1], marker=<span class="string">'s'</span>, facecolor=<span class="string">'y'</span>, s=80, alpha=0.3)</span><br><span class="line">    plt.grid(False)</span><br><span class="line">    plt.xlim(*xlim)</span><br><span class="line">    plt.ylim(*ylim)</span><br><span class="line">    plt.xticks(())</span><br><span class="line">    plt.yticks(())</span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="built_in">return</span> y_pred</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(10, 5))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">y_pred1 = plot_DBSCAN(<span class="string">"동심원 군집"</span>, X1, 0.1, (-1.2, 1.2), (-1.2, 1.2))</span><br><span class="line">plt.subplot(122)</span><br><span class="line">y_pred2 = plot_DBSCAN(<span class="string">"초승달 군집"</span>, X2, 0.1, (-1.5, 2.5), (-0.8, 1.2))</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/different_result_of_each_clustering_data.png" alt="군집을 이루는 모양에 따른 DBSCAN 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import datasets</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">labels = pd.DataFrame(iris.target)</span><br><span class="line">labels.columns=[<span class="string">'labels'</span>]</span><br><span class="line">data = pd.DataFrame(iris.data)</span><br><span class="line">data.columns=[<span class="string">'Sepal length'</span>,<span class="string">'Sepal width'</span>,<span class="string">'Petal length'</span>,<span class="string">'Petal width'</span>]</span><br><span class="line">data = pd.concat([data,labels],axis=1)</span><br><span class="line"></span><br><span class="line">feature = data[ [<span class="string">'Sepal length'</span>,<span class="string">'Sepal width'</span>,<span class="string">'Petal length'</span>,<span class="string">'Petal width'</span>]]</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import DBSCAN</span><br><span class="line">import matplotlib.pyplot  as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># create model and prediction</span></span><br><span class="line">model = DBSCAN(eps=0.5,min_samples=5)</span><br><span class="line">predict = pd.DataFrame(model.fit_predict(feature))</span><br><span class="line">predict.columns=[<span class="string">'predict'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># concatenate labels to df as a new column</span></span><br><span class="line">r = pd.concat([feature,predict],axis=1)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(r)</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Sepal length  Sepal width  Petal length  Petal width  predict</span><br><span class="line">0             5.1          3.5           1.4          0.2        0</span><br><span class="line">1             4.9          3.0           1.4          0.2        0</span><br><span class="line">2             4.7          3.2           1.3          0.2        0</span><br><span class="line">3             4.6          3.1           1.5          0.2        0</span><br><span class="line">4             5.0          3.6           1.4          0.2        0</span><br><span class="line">..            ...          ...           ...          ...      ...</span><br><span class="line">145           6.7          3.0           5.2          2.3        1</span><br><span class="line">146           6.3          2.5           5.0          1.9        1</span><br><span class="line">147           6.5          3.0           5.2          2.0        1</span><br><span class="line">148           6.2          3.4           5.4          2.3        1</span><br><span class="line">149           5.9          3.0           5.1          1.8        1</span><br></pre></td></tr></table></figure><hr><h4 id="DBSCAN-결과-시각화"><a href="#DBSCAN-결과-시각화" class="headerlink" title="DBSCAN 결과 시각화"></a>DBSCAN 결과 시각화</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pairplot with Seaborn</span></span><br><span class="line">sns.pairplot(r,hue=<span class="string">'predict'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/DBSCAN_with_iris.png" alt="iris 데이터를 DBSCAN 군집한 결과"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pairplot with Seaborn</span></span><br><span class="line">sns.pairplot(data,hue=<span class="string">'labels'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_raw.png" alt="iris raw 데이터의 pair plot"></p><h4 id="Kmeans-결과와-비교"><a href="#Kmeans-결과와-비교" class="headerlink" title="Kmeans 결과와 비교"></a>Kmeans 결과와 비교</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">km = KMeans(n_clusters = 3, n_jobs = 4, random_state=21)</span><br><span class="line">km.fit(feature)</span><br><span class="line">new_labels =pd.DataFrame(km.labels_)</span><br><span class="line">new_labels.columns=[<span class="string">'predict'</span>]</span><br><span class="line">r2 = pd.concat([feature,new_labels],axis=1)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pairplot with Seaborn</span></span><br><span class="line">sns.pairplot(r2,hue=<span class="string">'predict'</span>)</span><br><span class="line">plt.savefig(<span class="string">'k_means_iris_pair_plot'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><ul><li><code>DBSCAN의 결과보다 더 좋게 군집을 나눈것을 볼 수 있다. K-means는 원형으로 군집을 이룬 데이터에 대해 더 예측력이 좋다는 사실을 확인할 수 있다. 또한, K-means는 대체로 균등적으로 군집의 데이터를 나누어 주기 때문에 아래 그림에서 볼 수 있듯이 군집마다 데이터의 개수가 비슷해 보인다.</code></li></ul><p><img src="/image/k_means_iris_pair_plot.png" alt="k-means 군집화 결과의 pair plot"></p><blockquote><p>참고로, DBSCAN의 파라미터에 민감한 단점을 보완한 HDBSCAN이 있다.</p></blockquote><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install hdbscan</span><br></pre></td></tr></table></figure><hr><p><a href="https://godongyoung.github.io/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/2019/07/15/HDBSCAN-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-(with-python" target="_blank" rel="noopener">참조</a>.html)</p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/06/04/machine_learning_19/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Clustering - K-means, K-medoid</title>
      <link>https://heung-bae-lee.github.io/2020/05/30/machine_learning_18/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/30/machine_learning_18/</guid>
      <pubDate>Fri, 29 May 2020 16:01:30 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;K-means-Clusterig&quot;&gt;&lt;a href=&quot;#K-means-Clusterig&quot; class=&quot;headerlink&quot; title=&quot;K-means Clusterig&quot;&gt;&lt;/a&gt;K-means Clusterig&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;cod
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="K-means-Clusterig"><a href="#K-means-Clusterig" class="headerlink" title="K-means Clusterig"></a>K-means Clusterig</h2><ul><li><code>각 군집에 할당된 포인트들의 평균 좌표를 이용해 중심점을 반복적으로 업데이트</code>하면서 군집을 분류해 나가는 방법<ul><li>가장 단순하고 빠른 군집화 방법</li></ul></li></ul><ul><li>초기에 제일 처음 랜덤하게 포인트를 하나 잡아서 그 포인트에 가까운 데이터들을 같은 군집으로 할당해준다. 그 다음 아래와 같은 방법으로 반복한다.<ul><li>다음과 같은 목저함수 값이 최소화될 때까지 군집의 중심위치와 각 데이터가 소속될 군집을 반복해서 찾는다. 이 값을 관성(inertia)이라 한다.</li></ul></li></ul><script type="math/tex; mode=display">J = \sum_{k=1}^K \sum_{i \in C_k} d(x_i, \mu_k)</script><ul><li>이 식에서 $ K $ 는 군집의 갯수이고 $ C_{k} $ 는 $ k $ 번쨰 군집에 속하는 데이터의 집합, $ \mu_{k} $ 는 $ k $ 번째 군집의 중심위치(centroid), $ d $ 는 $ x_{i}, \mu_{k} $ 두 데이터 사이의 거리 혹은 비유사도(dissimilarity)로 정의한다. 만약 유클리드 거리를 사용한다면 다음과 같다.</li></ul><script type="math/tex; mode=display">d(x_i, \mu_k) = || x_i - \mu_k ||^2</script><ul><li>위 식은 다음처럼 표현할 수 도 있다.</li></ul><script type="math/tex; mode=display">J = \sum_{i=1}^{N}\min_{\mu_j \in C}(||x_i - \mu_j||^2)</script><ul><li>세부 알고리즘은 다음과 같다.<ul><li>1) 임의의 중심위치 $ \mu_{k} (k=1, \ldots , K) $ 를 고른다. 보통 데이터 표본 중에서 $ K $ 개를 선택한다.</li><li>2) 모든 데이터 $ x_{i} (i = 1, \ldots , N) $ 에서 각각의 중심위치 $ \mu_{k} $ 까지의 거리를 계산한다.</li><li>3) 각 데이터에서 가장 가까운 중심위치를 선택하여 각 데이터가 속하는 군집을 정한다.</li><li>4) 각 군집에 대해 중심위치 $ \mu_{k} $ 를 다시 계산한다.</li><li>5) 2~4를 반복한다.</li></ul></li></ul><ul><li>K-means 군집화는 <code>항상 수렴하지만 최종 군집화 결과가 전역 최적점이라는 보장은 없다. 군집화 결과는 초기 중심위치에 따라 달라질 수 있다.</code></li></ul><ul><li><code>K-means clusetering은 유클리드 거리를 사용하므로 너무 차원이 높을 때는 군집화 성능이 떨어질 수 있다.</code></li></ul><p><img src="/image/how_to_decide_mean_point_in_K_means_clustering_01.png" alt="K-means clustering의 작동 원리"></p><ul><li><p>Scikit-Learn의 cluster 서브패키지는 K-means 군집화를 위한 <code>KMeans</code> 클래스를 제공한다. 다음과 같은 인수를 받을 수 있다.</p><ul><li><code>n_clusters</code> : 군집의 개수</li><li><code>init</code> : 초기화 방법 <code>random</code>이면 무작위, <code>k-means++</code> 이면 K-means++ 방법. 또는 각 데이터의 군집 label</li><li><code>n_init</code> : 초기 중심위치 시도 횟수. default 10이고 10개의 무작위 중심위치 목록 중 가장 좋은 값을 선택한다.</li><li><code>max_iter</code> : 최대 반복 횟수</li><li><code>random_state</code> : 시드값</li></ul></li><li><p>다음은 <code>make_blobs</code> 커맨드를 통해 만든 데이터를 2개로 K-means 군집화하는 과정을 나타낸 것이다. 각각의 그림은 군집을 정하는 단계 3에서 멈춘 것이다. 마커(marker)의 모양은 소속된 군집을 나타내고 크기가 큰 마커가 해당 군집의 중심위치이다. 각 단계에서 중심위치는 전단계의 군집의 평균으로 다시 계산되는 것을 확인할 수 있다.</p></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line"></span><br><span class="line">X, _ = make_blobs(n_samples=20, random_state=4)</span><br><span class="line"></span><br><span class="line">def plot_KMeans(n):</span><br><span class="line">    model = KMeans(n_clusters=2, init=<span class="string">"random"</span>, n_init=1, max_iter=n, random_state=6).fit(X)</span><br><span class="line">    c0, c1 = model.cluster_centers_</span><br><span class="line">    plt.scatter(X[model.labels_ == 0, 0], X[model.labels_ == 0, 1], marker=<span class="string">'v'</span>, facecolor=<span class="string">'r'</span>, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    plt.scatter(X[model.labels_ == 1, 0], X[model.labels_ == 1, 1], marker=<span class="string">'^'</span>, facecolor=<span class="string">'y'</span>, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    plt.scatter(c0[0], c0[1], marker=<span class="string">'v'</span>, c=<span class="string">"r"</span>, s=200)</span><br><span class="line">    plt.scatter(c1[0], c1[1], marker=<span class="string">'^'</span>, c=<span class="string">"y"</span>, s=200)</span><br><span class="line">    plt.grid(False)</span><br><span class="line">    plt.title(<span class="string">"반복횟수=&#123;&#125;, 관성=&#123;:5.2f&#125;"</span>.format(n, -model.score(X)))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12, 10))</span><br><span class="line">plt.subplot(321)</span><br><span class="line">plot_KMeans(1)</span><br><span class="line">plt.subplot(322)</span><br><span class="line">plot_KMeans(2)</span><br><span class="line">plt.subplot(323)</span><br><span class="line">plot_KMeans(3)</span><br><span class="line">plt.subplot(324)</span><br><span class="line">plot_KMeans(4)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/k_means_center_moving_each_iter_step.png" alt="k-means 반복횟수 증가에 따른 중심 위치의 변화"></p><h3 id="K-means-알고리즘"><a href="#K-means-알고리즘" class="headerlink" title="K-means++ 알고리즘"></a>K-means++ 알고리즘</h3><ul><li><p>K-means++ 알고리즘은 <code>초기 중심위치를 설정하기 위한 알고리즘</code>이다. 다음과 같은 방법을 통해 <code>되도록 멀리 떨어진 중심위치 집합을 찾아낸다.</code></p><ul><li><ol><li>중심위치를 저장할 집합 $ M $ 준비</li></ol></li><li><ol><li>우선 하나의 중심위치 $ \mu_{0} $ 를 랜덤하게 선택하여 $ M $ 에 넣는다.</li></ol></li><li><ol><li>$ M $ 에 속하지 않는 모든 표본 $ x_{i} $ 에 대해 거리 $ d(M, x_{i}) $ 를 계산. $ d(M, x_{i}) $ 는 $ M $ 안의 모든 샘플 $ \mu_{k} $ 에 대해 $ d(\mu_{k}, x_{i}) $ 를 계산하여 가장 작은 값 선택</li></ol></li><li><ol><li>$ d(M, x_{i}) $ 에 비례한 확률로 다음 중심위치 $ \mu $를 선택</li></ol></li><li><ol><li>K 개의 중심위치를 선택할 때 까지 반복</li></ol></li><li><ol><li>K-means 방법 사용</li></ol></li></ul></li></ul><ul><li>중심을 구할 경우에는 각 데이터(포인트)들간의 거리의 합이 최소화하는 되는 지점이 중심이 될 것이므로 포인트간의 거리를 계산해야 한다. 여기서는 가장 간단하고 많이들 알고있는 유클리디안 거리와 맨하탄 거리를 소개할 것이다. 이외에도 마할라노비스의 거리등 <code>거리를 구하는 방법</code>은 여러가지가 있다. 이를 결정하는 것은 <code>해당 데이터에 성질(예를 들어 위도 경도 데이터라고 해서 무조건적으로 Haversine distance를 사용하는 것은아니지만 위도와 경도의 데이터를 통해 항공기들간의 시간대 별 위치의 중심점을 찾아야 하는 경우 Haversine distance를 사용하듯)에 따라 분석자가 결정해야 할 몫</code>이다.</li></ul><p><a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank" rel="noopener">참조 Distance type</a></p><p><img src="/image/how_to_decide_mean_point_in_K_means_clustering_02.png" alt="K-means clustering의 작동 원리"></p><h3 id="최적의-K를-찾는-방법"><a href="#최적의-K를-찾는-방법" class="headerlink" title="최적의 K를 찾는 방법"></a>최적의 K를 찾는 방법</h3><ul><li>K값을 설정하는 방법 중 가장 좋은 것은 군집의 개수를 미리 알고있는 경우이다. 예를 들어 뉴스기사를 각 카테고리별로 묶는다고 가정하면 크롤링해온 뉴스 기사의 카테고리 수를 미리 알고 있기 때문에 정확히 군집의 수를 결정할 수 있다. 허나, 대부분의 경우 우리는 데이터의 군집을 정확히 알고 있지 않은 상황에서 분석을 진행한다. 그러므로 다음과 같이 대체로 <code>Elbow method</code> 또는 <code>Silhouette method</code></li></ul><p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_01.png" alt="최적의 k를 찾는 방법 - 01"></p><ul><li><code>Elbow method</code>는 $ \frac{군집간 분산}{전체 분산} $인 비율을 보고 k를 결정한다. <code>WSS(군집 내 분산)은 작을 수록 군집의 중심에 많이 모여있는 것이므로 WSS(군집 내 분산)이 작을 수록 좋다는 말은 달리 말해 BSS(군집 간 분산)이 클수록 좋다는 의미</code>이기도 하다. 전체 데이터의 중심과 군집의 중심들간의 거리가 떨어져 있을 수록 군집이 잘 분류 되었다고 판단할 수 있기 때문이다. <code>군집의 개수가 늘어날 수록 BSS는 높은 값을 갖아 데이터의 개수만큼 K를 설정하면 비율의 값은 100%가 될 것</code>이다.</li></ul><p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_02.png" alt="최적의 k를 찾는 방법 - 02"></p><ul><li>위의 방법에서는 $ \frac{군집간 분산}{전체 분산} $의 비율 증가분을 보고 k를 결정하였다면 TSS(전체 분산)=BSS(군집 간 분산)+WSS(군집 내 분산)이므로 WSS가 가장 낮아지는 cluster의 개수를 찾는 방법도 동일한 결과를 얻을 수 있다는 사실을 알 수 있다.</li></ul><p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_03.png" alt="최적의 k를 찾는 방법 - 03"></p><ul><li>다른 한가지 방법인 <code>Silhouette method</code>도 거의 유사한 원리로 k를 결정하는 데 군집 내의 임의의 데이터와 다른 데이터들 간의 거리로 유사성을 파악하고, 다른 군집 중 제일 가까운 데이터와의 거리로 군집간의 유사성을 대표하는 지표로 삼아 제일 가까운 군집의 데이터의 거리와 객체내의 데이터들과의 거리의 차이가 크면 클수록 잘 군집이 형성되어졌다고 판단한다.</li></ul><p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_04.png" alt="최적의 k를 찾는 방법 - 04"></p><ul><li>최적의 k로 군집이 형성되었을 경우 가까운 군집과의 거리가 군집내 다른 데이터들의 거리보다 크기 때문에 분모에 해당하는 값은 b(i)로 되며, 분자의 가까운 군집과의 거리는 멀어질 것이며 군집내 데이터들과의 거리 차이는 0에 가까울 수록 최적의 k임을 의미하는 것이기 때문에 전체적인 값을 최대 1을 갖게되고 반대로 생각해보면 -1의 값을 최소로 갖게된다. <code>s(i)값이 클수록 적절한 k가 될 확률이 높다.</code></li></ul><p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_05.png" alt="최적의 k를 찾는 방법 - 05"></p><ul><li>실루엣 메서드의 간단한 예시를 보면 아래와 같으며, 실루엣 값의 평균값인 <code>실루엣 계수가 높은 값을 갖는 k를 최적의 k로 결정</code>한다.</li></ul><p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_06.png" alt="최적의 k를 찾는 방법 - 06"></p><blockquote><p>Elbow method를 사용하는 것이 일반적인데, 사실 k는 분석자가 미리 알고있을 경우가 가장 좋을 것이다. 허나, 모를 경우 최적의 k를 찾아도 고차원의 데이터에 대해서는 정확히 맞아 떨어지기 힘들다. <code>고차원 상에서도 데이터간의 거리를 계산할 순 있지만, 차원이 높아질수록 거리의 개념이 실질적으로 가까운지에 대한 감각이 무뎌지기 때문</code>이다. <code>그런 이유로 거리를 기반으로 하는 군집 분석 같은 경우에는 차원이 높아질수록 최적의 k로 결정한 군집이 실제로 맞지 않을 가능성이 커진다.</code></p></blockquote><ul><li>이러한 단점을 보완하여 나온 방법이 <code>K-medoid clustering</code>이다.</li></ul><h2 id="K-medoid-Clustering"><a href="#K-medoid-Clustering" class="headerlink" title="K-medoid Clustering"></a>K-medoid Clustering</h2><ul><li><code>Elbow method와 Silhouette method도 k를 찾을 수 있는 하나의 방법인 것이지 찾은 k가 절대적으로 최적이라는 것은 아니다.</code> 그래서 군집의 갯수를 찾는 것이 상당히 어려운 문제이다. k-means clustering은 아래와 같은 단점이 존재한다.</li></ul><p><img src="/image/disadvantages_of_k_means_clustering.png" alt="k-means clustering의 단점"></p><ul><li>위에서 언급한 k-means의 단점을 보완하기 위한 방법으로 가장 간단히 <code>평균대신 중간점을 사용</code>하는 방벙이다.</li></ul><p><img src="/image/what_is_k_medoid_clustering.png" alt="k-medoid clustering이란?"></p><ul><li>k-medoid clustering도 다른 clustering과 동일하게 찾은 k가 절대적으로 최적인 k를 의미하진 않는다. 예를 들어 아래 그림에서와 같이 <code>초승달 모양의 데이터 군집 분포는 거리에 기반한 모델인 k-means와 k-medoid clustering은 명확히 2군집으로 분류하기 어렵다.</code></li></ul><p><img src="/image/k_means_versus_k_medoid.png" alt="k-means와 k-medoid 비교"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import datasets</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.cluster import KMeans</span><br></pre></td></tr></table></figure><hr><h4 id="Iris-데이터를-활용하여-Kmeans-clustering"><a href="#Iris-데이터를-활용하여-Kmeans-clustering" class="headerlink" title="Iris 데이터를 활용하여 Kmeans clustering"></a>Iris 데이터를 활용하여 Kmeans clustering</h4><ul><li>iris data에서 2가지 feature를 가지고만 군집 분석을 실행할 것이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, :2]</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X[:,0], X[:,1], c=y, cmap=<span class="string">'gist_rainbow'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Spea1 Length'</span>, fontsize=18)</span><br><span class="line">plt.ylabel(<span class="string">'Sepal Width'</span>, fontsize=18)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_scatter_plot_for_k_means_clustering.png" alt="iris 데이터 scatter plot"></p><ul><li>k=3으로 설정하고 k-means 군집분석을 진행해 보았다. 물론 이미 class label이 3개라는 사실을 알고 있으므로 여기선 3으로 설정하였으므로, 추후에 역으로 3으로 설정한 것이 타당한지에 대한 검증을 해 볼 것이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">km = KMeans(n_clusters = 3, n_jobs = 4, random_state=21)</span><br><span class="line">km.fit(X)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">KMeans(algorithm=<span class="string">'auto'</span>, copy_x=True, init=<span class="string">'k-means++'</span>, max_iter=300,</span><br><span class="line">       n_clusters=3, n_init=10, n_jobs=4, precompute_distances=<span class="string">'auto'</span>,</span><br><span class="line">       random_state=21, tol=0.0001, verbose=0)</span><br></pre></td></tr></table></figure><hr><ul><li>k=3으로 설정한 k-means 군집 분석의 결과의 군집 중심은 다음과 같다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">centers = km.cluster_centers_</span><br><span class="line"><span class="built_in">print</span>(centers)</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[5.77358491 2.69245283]</span><br><span class="line"> [5.006      3.428     ]</span><br><span class="line"> [6.81276596 3.07446809]]</span><br></pre></td></tr></table></figure><hr><ul><li>해당 k-means 군집 분석의 결과로 예측한 label을 토대로 동일한 scatter plot을 그려 실제 데이터와 비교해 볼 것이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">new_labels = km.labels_</span><br><span class="line">fig, axes = plt.subplots(1, 2, figsize=(16,8))</span><br><span class="line">axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap=<span class="string">'gist_rainbow'</span>,</span><br><span class="line">edgecolor=<span class="string">'k'</span>, s=150)</span><br><span class="line">axes[1].scatter(X[:, 0], X[:, 1], c=new_labels, cmap=<span class="string">'jet'</span>,</span><br><span class="line">edgecolor=<span class="string">'k'</span>, s=150)</span><br><span class="line">axes[0].set_xlabel(<span class="string">'Sepal length'</span>, fontsize=18)</span><br><span class="line">axes[0].set_ylabel(<span class="string">'Sepal width'</span>, fontsize=18)</span><br><span class="line">axes[1].set_xlabel(<span class="string">'Sepal length'</span>, fontsize=18)</span><br><span class="line">axes[1].set_ylabel(<span class="string">'Sepal width'</span>, fontsize=18)</span><br><span class="line">axes[0].tick_params(direction=<span class="string">'in'</span>, length=10, width=5, colors=<span class="string">'k'</span>, labelsize=20)</span><br><span class="line">axes[1].tick_params(direction=<span class="string">'in'</span>, length=10, width=5, colors=<span class="string">'k'</span>, labelsize=20)</span><br><span class="line">axes[0].set_title(<span class="string">'Actual'</span>, fontsize=18)</span><br><span class="line">axes[1].set_title(<span class="string">'Predicted'</span>, fontsize=18)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/difference_between_result_of_clustering_and_true_label.png" alt="clustering 결과와 실제 데이터 비교"></p><h4 id="2차원의-가상-데이터에-Kmeans-clustering"><a href="#2차원의-가상-데이터에-Kmeans-clustering" class="headerlink" title="2차원의 가상 데이터에 Kmeans clustering"></a>2차원의 가상 데이터에 Kmeans clustering</h4><ul><li>새롭게 군집분석을 위한 데이터를 가상으로 만들어 진행해 볼 것이다. feature의 개수는 2개이고 데이터의 수는 150개로 하고 군집의 표준편차는 0.5로 하여 3개의 군집을 띄게 데이터를 생성해 주었다. 이는 위의 iris 데이터와 거의 동일한 분포를 띄는 데이터를 얻을 수 있다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line"><span class="comment"># create dataset</span></span><br><span class="line">X, y = make_blobs(</span><br><span class="line">   n_samples=150, n_features=2,</span><br><span class="line">   centers=3, cluster_std=0.5,</span><br><span class="line">   shuffle=True, random_state=0</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">plt.scatter(</span><br><span class="line">   X[:, 0], X[:, 1],</span><br><span class="line">   c=<span class="string">'white'</span>, marker=<span class="string">'o'</span>,</span><br><span class="line">   edgecolor=<span class="string">'black'</span>, s=50</span><br><span class="line">)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/make_classification_blob_and_scatter_plot.png" alt="새롭게 생성한 2차원 데이터"></p><ul><li>새롭게 만든 데이터도 k=3으로 하여 군집분석을 진행한다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">km = KMeans(</span><br><span class="line">    n_clusters=3, init=<span class="string">'random'</span>,</span><br><span class="line">    n_init=10, max_iter=300,</span><br><span class="line">    tol=1e-04, random_state=0</span><br><span class="line">)</span><br><span class="line">y_km = km.fit_predict(X)</span><br></pre></td></tr></table></figure><hr><ul><li>새롭게 만든 데이터를 군집 분석 결과로 예측된 label의 값에 따라 marker와 색을 다르게 주어 2차원 plot을 그려 보았다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(10,6))</span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 0, 0], X[y_km == 0, 1],</span><br><span class="line">    s=50, c=<span class="string">'lightgreen'</span>,</span><br><span class="line">    marker=<span class="string">'s'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 1'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 1, 0], X[y_km == 1, 1],</span><br><span class="line">    s=50, c=<span class="string">'orange'</span>,</span><br><span class="line">    marker=<span class="string">'o'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 2'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 2, 0], X[y_km == 2, 1],</span><br><span class="line">    s=50, c=<span class="string">'lightblue'</span>,</span><br><span class="line">    marker=<span class="string">'v'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 3'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the centroids</span></span><br><span class="line">plt.scatter(</span><br><span class="line">    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],</span><br><span class="line">    s=250, marker=<span class="string">'*'</span>,</span><br><span class="line">    c=<span class="string">'red'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'centroids'</span></span><br><span class="line">)</span><br><span class="line">plt.legend(scatterpoints=1)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/make_classification_blob_and_scatter_plot_predicted_value.png" alt="가상의 생성 데이터에 대한 군집 분석 결과"></p><h4 id="k-를-4로-할경우"><a href="#k-를-4로-할경우" class="headerlink" title="k 를 4로 할경우"></a>k 를 4로 할경우</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">km = KMeans(</span><br><span class="line">    n_clusters=4, init=<span class="string">'random'</span>,</span><br><span class="line">    n_init=10, max_iter=300,</span><br><span class="line">    tol=1e-04, random_state=0</span><br><span class="line">)</span><br><span class="line">y_km = km.fit_predict(X)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(10,6))</span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 0, 0], X[y_km == 0, 1],</span><br><span class="line">    s=50, c=<span class="string">'lightgreen'</span>,</span><br><span class="line">    marker=<span class="string">'s'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 1'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 1, 0], X[y_km == 1, 1],</span><br><span class="line">    s=50, c=<span class="string">'orange'</span>,</span><br><span class="line">    marker=<span class="string">'o'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 2'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 2, 0], X[y_km == 2, 1],</span><br><span class="line">    s=50, c=<span class="string">'lightblue'</span>,</span><br><span class="line">    marker=<span class="string">'v'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 3'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 3, 0], X[y_km == 3, 1],</span><br><span class="line">    s=50, c=<span class="string">'lightblue'</span>,</span><br><span class="line">    marker=<span class="string">'d'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 4'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the centroids</span></span><br><span class="line">plt.scatter(</span><br><span class="line">    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],</span><br><span class="line">    s=250, marker=<span class="string">'*'</span>,</span><br><span class="line">    c=<span class="string">'red'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'centroids'</span></span><br><span class="line">)</span><br><span class="line">plt.legend(scatterpoints=1)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/make_classification_blob_and_scatter_plot_predicted_value_k_is_4.png" alt="k=4로 한 경우의 scatter plot"></p><ul><li>Elbow method를 통해 최적의 k를 찾기 위해 각 cluster의 개수 별로 WSS(군집 내 분산)을 계산하여 그래프로 시각화한다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">distortions = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(1, 11):</span><br><span class="line">    km = KMeans(</span><br><span class="line">        n_clusters=i, init=<span class="string">'random'</span>,</span><br><span class="line">        n_init=10, max_iter=300,</span><br><span class="line">        tol=1e-04, random_state=0</span><br><span class="line">    )</span><br><span class="line">    km.fit(X)</span><br><span class="line">    <span class="comment">#inertia가 군집 내의 분산을 의미</span></span><br><span class="line">    distortions.append(km.inertia_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">plt.figure(figsize=(10,6))</span><br><span class="line">plt.plot(range(1, 11), distortions, marker=<span class="string">'o'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Number of clusters'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Distortion'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/decide_to_optimal_k_using_Elbow_method.png" alt="Elbow method를 사용한 최적의 k 추정"></p><h4 id="비정형-데이터에-대한-K-means-군집-분석"><a href="#비정형-데이터에-대한-K-means-군집-분석" class="headerlink" title="비정형 데이터에 대한 K-means 군집 분석"></a>비정형 데이터에 대한 K-means 군집 분석</h4><ul><li>정형 데이터에 관한 군집 분석 말고도 비정형 데이터인 문자열 데이터에 관한 군집 분석도 실행해 볼 것이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">from sklearn.metrics import adjusted_rand_score</span><br><span class="line"></span><br><span class="line">documents = [<span class="string">"This little kitty came to play when I was eating at a restaurant."</span>,<span class="string">"hello kitty is my favorite character"</span>,</span><br><span class="line">             <span class="string">"Merley has the best squooshy kitten belly."</span>,<span class="string">"Is Google translator so good?"</span>,<span class="string">"google google"</span></span><br><span class="line">             <span class="string">"google Translate app is incredible."</span>,<span class="string">"My dog s name is Kong"</span>,<span class="string">"dog dog dog"</span>,<span class="string">"cat cat"</span></span><br><span class="line">             <span class="string">"If you open 100 tab in google you get a smiley face."</span>,<span class="string">"Kong is a very cute and lovely dog"</span>,</span><br><span class="line">             <span class="string">"Best cat photo I've ever taken."</span>,<span class="string">"This is a cat house"</span></span><br><span class="line">             <span class="string">"Climbing ninja cat kitty."</span>,<span class="string">"What's your dog's name?"</span>,<span class="string">"Cat s paws look like jelly"</span>,</span><br><span class="line">             <span class="string">"Impressed with google map feedback."</span>,<span class="string">"I want to join google"</span>,<span class="string">"You have to wear a collar when you walk the dog"</span>,</span><br><span class="line">             <span class="string">"Key promoter extension for google Chrome."</span>,<span class="string">"Google is the best company"</span>,<span class="string">"Google researcher"</span>]</span><br><span class="line"></span><br><span class="line">vectorizer = TfidfVectorizer(stop_words=<span class="string">'english'</span>)</span><br><span class="line">X = vectorizer.fit_transform(documents)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">true_k = 3</span><br><span class="line">model = KMeans(n_clusters=true_k, init=<span class="string">'k-means++'</span>, max_iter=100, n_init=1)</span><br><span class="line">model.fit(X)</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">KMeans(algorithm=<span class="string">'auto'</span>, copy_x=True, init=<span class="string">'k-means++'</span>, max_iter=100,</span><br><span class="line">       n_clusters=3, n_init=1, n_jobs=None, precompute_distances=<span class="string">'auto'</span>,</span><br><span class="line">       random_state=None, tol=0.0001, verbose=0)</span><br></pre></td></tr></table></figure><hr><ul><li><code>아래의 label은 위의 군집분석을 실행 할 때마다 다르기에 군집의 결과를 살펴보면서 어떤 데이터들이 같은 군집으로 묶였는지 확인해 본다.</code>그러므로 위의 코드를 실행시 필자와 다른 결과를 볼 수 있을 것이다. 허나 군집으로 묶여있는 데이터들은 동일 할 것이다.</li></ul><h4 id="예측-결과-군집-0인-데이터들의-문서"><a href="#예측-결과-군집-0인-데이터들의-문서" class="headerlink" title="예측 결과 군집 0인 데이터들의 문서"></a>예측 결과 군집 0인 데이터들의 문서</h4><ul><li>주로 고양이를 의미하는 kitty와 cat을 갖는 문서를 하나의 군집으로 묶어 주었다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(documents, model.labels_) <span class="keyword">if</span>  y == 0]</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This little kitty came to play when I was eating at a restaurant.'</span>,</span><br><span class="line"> <span class="string">'hello kitty is my favorite character'</span>,</span><br><span class="line"> <span class="string">'Merley has the best squooshy kitten belly.'</span>,</span><br><span class="line"> <span class="string">'cat catIf you open 100 tab in google you get a smiley face.'</span>,</span><br><span class="line"> <span class="string">"Best cat photo I've ever taken."</span>,</span><br><span class="line"> <span class="string">'This is a cat houseClimbing ninja cat kitty.'</span>,</span><br><span class="line"> <span class="string">'Cat s paws look like jelly'</span>]</span><br></pre></td></tr></table></figure><hr><h4 id="예측-결과-군집-1인-데이터들의-문서"><a href="#예측-결과-군집-1인-데이터들의-문서" class="headerlink" title="예측 결과 군집 1인 데이터들의 문서"></a>예측 결과 군집 1인 데이터들의 문서</h4><ul><li>주로 dog를 갖는 문서를 하나의 군집으로 묶어주었다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(documents, model.labels_) <span class="keyword">if</span>  y == 1]</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'My dog s name is Kong'</span>,</span><br><span class="line"> <span class="string">'dog dog dog'</span>,</span><br><span class="line"> <span class="string">'Kong is a very cute and lovely dog'</span>,</span><br><span class="line"> <span class="string">"What's your dog's name?"</span>,</span><br><span class="line"> <span class="string">'You have to wear a collar when you walk the dog'</span>]</span><br></pre></td></tr></table></figure><hr><h4 id="예측-결과-군집-2인-데이터들의-문서"><a href="#예측-결과-군집-2인-데이터들의-문서" class="headerlink" title="예측 결과 군집 2인 데이터들의 문서"></a>예측 결과 군집 2인 데이터들의 문서</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(documents, model.labels_) <span class="keyword">if</span>  y == 2]</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'Is Google translator so good?'</span>,</span><br><span class="line"> <span class="string">'google googlegoogle Translate app is incredible.'</span>,</span><br><span class="line"> <span class="string">'Impressed with google map feedback.'</span>,</span><br><span class="line"> <span class="string">'I want to join google'</span>,</span><br><span class="line"> <span class="string">'Key promoter extension for google Chrome.'</span>,</span><br><span class="line"> <span class="string">'Google is the best company'</span>,</span><br><span class="line"> <span class="string">'Google researcher'</span>]</span><br></pre></td></tr></table></figure><hr><ul><li>아래와 같은 새로운 문자열의 예측한 경우 위에서 살펴본 키워드를 갖으면 해당 군집으로 예측하는 것을 볼 수 있다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Y = vectorizer.transform([<span class="string">"chrome browser to open."</span>])</span><br><span class="line">prediction = model.predict(Y)</span><br><span class="line"><span class="built_in">print</span>(prediction)</span><br><span class="line"></span><br><span class="line">Y = vectorizer.transform([<span class="string">"I want to have a dog"</span>])</span><br><span class="line">prediction = model.predict(Y)</span><br><span class="line"><span class="built_in">print</span>(prediction)</span><br><span class="line"></span><br><span class="line">Y = vectorizer.transform([<span class="string">"My cat is hungry."</span>])</span><br><span class="line">prediction = model.predict(Y)</span><br><span class="line"><span class="built_in">print</span>(prediction)</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[0]</span><br><span class="line">[1]</span><br><span class="line">[0]</span><br></pre></td></tr></table></figure><hr><ul><li>다음은 K-means++ 방법을 사용하여 MNIST Digit 이미지 데이터를 군집화한 결과이다. 각 군집에서 10개씩의 데이터만 표시하였다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line"></span><br><span class="line">digits = load_digits()</span><br><span class="line"></span><br><span class="line">model = KMeans(init=<span class="string">"k-means++"</span>, n_clusters=10, random_state=0)</span><br><span class="line">model.fit(digits.data)</span><br><span class="line">y_pred = model.labels_</span><br><span class="line"></span><br><span class="line">def show_digits(images, labels):</span><br><span class="line">    f = plt.figure(figsize=(8, 2))</span><br><span class="line">    i = 0</span><br><span class="line">    <span class="keyword">while</span> (i &lt; 10 and i &lt; images.shape[0]):</span><br><span class="line">        ax = f.add_subplot(1, 10, i + 1)</span><br><span class="line">        ax.imshow(images[i], cmap=plt.cm.bone)</span><br><span class="line">        ax.grid(False)</span><br><span class="line">        ax.set_title(labels[i])</span><br><span class="line">        ax.xaxis.set_ticks([])</span><br><span class="line">        ax.yaxis.set_ticks([])</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">        i += 1</span><br><span class="line"></span><br><span class="line">def show_cluster(images, y_pred, cluster_number):</span><br><span class="line">    images = images[y_pred == cluster_number]</span><br><span class="line">    y_pred = y_pred[y_pred == cluster_number]</span><br><span class="line">    show_digits(images, y_pred)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(10):</span><br><span class="line">    show_cluster(digits.images, y_pred, i)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/mnist_k_means_cluster_result.png" alt="MNIST 데이터 K-means 군집화한 결과"></p><ul><li><code>이미지의 제목에 있는 숫자는 군집 번호에 지나지 않으므로 원래 숫자의 번호와 일치하지 않는다.</code> 하지만 이를 예측문제라고 가정하고 분류 결과 행렬을 만들면 아래와 같다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line"></span><br><span class="line">confusion_matrix(digits.target, y_pred)</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[  1,   0,   0,   0,   0, 177,   0,   0,   0,   0],</span><br><span class="line">       [  0,   1,  55,  24,   0,   0,   0,   2,   1,  99],</span><br><span class="line">       [  0,  13,   2, 148,   2,   1,   3,   0,   0,   8],</span><br><span class="line">       [  0, 155,   0,   1,  11,   0,   7,   0,   2,   7],</span><br><span class="line">       [163,   0,   7,   0,   0,   0,   7,   0,   0,   4],</span><br><span class="line">       [  2,   1,   0,   0,  42,   0,   0,   1, 136,   0],</span><br><span class="line">       [  0,   0,   1,   0,   0,   1,   0, 177,   0,   2],</span><br><span class="line">       [  0,   0,   0,   0,   0,   0, 177,   0,   0,   2],</span><br><span class="line">       [  0,   4,   6,   3,  48,   0,   5,   2,   4, 102],</span><br><span class="line">       [  0,   6,  20,   0, 139,   0,   7,   0,   6,   2]])</span><br></pre></td></tr></table></figure><hr><ul><li>이 군집화 결과의 ARI, AMI, 실루엣 계수 값은 다음과 같다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"ARI:"</span>, adjusted_rand_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"AMI:"</span>, adjusted_mutual_info_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Silhouette Score:"</span>, silhouette_score(digits.data, y_pred))</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ARI: 0.6703800183468681</span><br><span class="line">AMI: 0.7417664506416767</span><br><span class="line">Silhouette Score: 0.18249069204151275</span><br></pre></td></tr></table></figure><hr><ul><li>군집화 결과를 주성분 분석을 통해 2차원에 투영하면 다음과 같다. 겹쳐져 있는 부분은 고차원상에서는 떨어져 있을 수 있다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import PCA</span><br><span class="line"></span><br><span class="line">pca = PCA(n_components=2)</span><br><span class="line">X = pca.fit_transform(digits.data)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=plt.cm.Set1)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/MNIST_K_means_clustering_PCA_2_ponents.png" alt="MNIST 데이터를 PCA를 통해 2개의 주성분 벡터를 갖도록 차원을 축소한 그래프"></p><ul><li><code>K-means clusetering은 유클리드 거리를 사용하므로 너무 차원이 높을 때는 군집화 성능이 떨어질 수 있다. 이 때는 차원 축소를 한 후 군집화를 하는 것이 도움이 될 수도 있다.</code></li></ul><ul><li>MNIST Digit 데이터를 10차원으로 차원 축소하여 K-means 군집화하고 ARI, AMI, 실루엣 계수를 각각 계산하여 차원축소를 하지 않았을 때와 비교하라.</li></ul><h2 id="차원을-축소하여도-실루엣계수를-포함한-모든-지표들이-살펴보았을-때-오히려-낮아졌다-이를-통해-복잡한-데이터에-대해서는-적합하지-않은-실루엣계수의-단점을-확인-할-수-있으며-무조건-적으로-PCA를-하더라도-높아지지-않는다는-점을-확인할-수-있다"><a href="#차원을-축소하여도-실루엣계수를-포함한-모든-지표들이-살펴보았을-때-오히려-낮아졌다-이를-통해-복잡한-데이터에-대해서는-적합하지-않은-실루엣계수의-단점을-확인-할-수-있으며-무조건-적으로-PCA를-하더라도-높아지지-않는다는-점을-확인할-수-있다" class="headerlink" title="-  차원을 축소하여도 실루엣계수를 포함한 모든 지표들이 살펴보았을 때 오히려 낮아졌다. 이를 통해 복잡한 데이터에 대해서는 적합하지 않은 실루엣계수의 단점을 확인 할 수 있으며, 무조건 적으로 PCA를 하더라도 높아지지 않는다는 점을 확인할 수 있다."></a>-  차원을 축소하여도 실루엣계수를 포함한 모든 지표들이 살펴보았을 때 오히려 낮아졌다. 이를 통해 복잡한 데이터에 대해서는 적합하지 않은 실루엣계수의 단점을 확인 할 수 있으며, 무조건 적으로 PCA를 하더라도 높아지지 않는다는 점을 확인할 수 있다.</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score</span><br><span class="line"></span><br><span class="line">model = KMeans(init=<span class="string">"k-means++"</span>, n_clusters=10, random_state=0)</span><br><span class="line">model.fit(digits.data)</span><br><span class="line">y_pred = model.labels_</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"ARI:"</span>, adjusted_rand_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"AMI:"</span>, adjusted_mutual_info_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Silhouette Score:"</span>, silhouette_score(digits.data, y_pred))</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ARI: 0.6686991223627669</span><br><span class="line">AMI: 0.7397973157276612</span><br><span class="line">Silhouette Score: 0.18251916424600556</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA(n_components=10)</span><br><span class="line">X = pca.fit_transform(digits.data)</span><br><span class="line"></span><br><span class="line">model = KMeans(init=<span class="string">"k-means++"</span>, n_clusters=10, random_state=0)</span><br><span class="line">model.fit(X)</span><br><span class="line">y_pred = model.labels_</span><br><span class="line"></span><br><span class="line">from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"ARI:"</span>, adjusted_rand_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"AMI:"</span>, adjusted_mutual_info_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Silhouette Score:"</span>, silhouette_score(digits.data, y_pred))</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ARI: 0.6492572540256956</span><br><span class="line">AMI: 0.7183058275931469</span><br><span class="line">Silhouette Score: 0.18116247153029966</span><br></pre></td></tr></table></figure><hr><h3 id="미니배치-K-means-군집화"><a href="#미니배치-K-means-군집화" class="headerlink" title="미니배치 K-means 군집화"></a>미니배치 K-means 군집화</h3><ul><li>K-means 방법에서는 중심위치와 모든 데이터 사이의 거리를 계산해야 하기 때문에 데이터의 갯수가 많아지면 계산량도 늘어난다. <code>데이터의 수가 너무 많을 때는 미니 배치 K-means 군집화 방법을 사용하면 계산량을 줄일 수 있다.</code> 미니배치 K-means 군집화는 데이터를 미니배치 크기만큼 무작위로 분리하여 K-means 군집화를 한다. 모든 데이터를 한꺼번에 썼을 때와 결과가 다를 수는 있지만 큰 차이가 없다.</li></ul><ul><li>Scikit-Learn의 cluster 서브패키지는 미니배치 K-means 군집화를 위한 <code>MiniBatchKMeans</code> 클래스를 제공한다. 미니배치 크기 <code>batch_size</code>인수를 추가로 받는다.</li></ul><ul><li>150,000개의 데이터를 사용하여 실행 시간을 비교할 것이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import MiniBatchKMeans</span><br><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(n_samples=150000, cluster_std=[1.0, 2.5, 0.5], random_state=170)</span><br></pre></td></tr></table></figure><hr><ul><li>미니배치 군집화의 속도가 훨씬 빠른 것을 알 수 있다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line">model1 = KMeans(n_clusters=3).fit(X)</span><br></pre></td></tr></table></figure><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CPU <span class="built_in">times</span>: user 1.01 s, sys: 66.8 ms, total: 1.08 s</span><br><span class="line">Wall time: 604 ms</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line">model2 = MiniBatchKMeans(n_clusters=3, batch_size=1000, compute_labels=True).fit(X)</span><br></pre></td></tr></table></figure><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CPU <span class="built_in">times</span>: user 191 ms, sys: 7.71 ms, total: 199 ms</span><br><span class="line">Wall time: 158 ms</span><br></pre></td></tr></table></figure><hr><ul><li>군집화 결과는 그다지 차이가 없다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">idx = np.random.randint(150000, size=300)</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.scatter(X[idx, 0], X[idx, 1], c=model1.labels_[idx])</span><br><span class="line">plt.title(<span class="string">"K-평균 군집화"</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plt.scatter(X[idx, 0], X[idx, 1], c=model2.labels_[idx])</span><br><span class="line">plt.title(<span class="string">"미니배치 K-평균 군집화"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/mini_batch_k_means_vs_k_means_clustering_result.png" alt="미니배치와 일반적인 K-means 군집화 결과"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/30/machine_learning_18/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Clustering</title>
      <link>https://heung-bae-lee.github.io/2020/05/29/machine_learning_17/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/29/machine_learning_17/</guid>
      <pubDate>Thu, 28 May 2020 18:17:21 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Clustering-군집화-이란&quot;&gt;&lt;a href=&quot;#Clustering-군집화-이란&quot; class=&quot;headerlink&quot; title=&quot;Clustering(군집화)이란?&quot;&gt;&lt;/a&gt;Clustering(군집화)이란?&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;군집
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Clustering-군집화-이란"><a href="#Clustering-군집화-이란" class="headerlink" title="Clustering(군집화)이란?"></a>Clustering(군집화)이란?</h2><ul><li>군집 분석이라고도 불리며 클러스터링 알고리즘은 비지도 학습(Unsupervised Learning)에 해당한다. <code>X(입력변수)간의 관계를 규명하거나 살펴보는 것에 많이 사용</code>된다.</li></ul><p><img src="/image/what_is_clustering_01.png" alt="Clustering이란? - 01"></p><ul><li>주어딘 데이터 집합을 유사한 데이터들의 그룹으로 나누는 것을 군집화(clustering)이라 하고 이렇게 나누어진 유사한 데이터의 그룹을 군집(cluster)이라고 한다.</li></ul><ul><li>군집 분석이란 <code>유사성을 측정하여 높은 대상 집단을 분류하고, 군집 간에 상이성을 규명하는 방법</code>이다. <code>군집 분석을 통해 분류된 군집에 대한 해석은 분석자가 직접해야한다.</code> 단지 군집을 나누는 행위만을 생가했을 땐 classification과 별 다를것이 없을 것이라고 생각이 들겠지만, Classification은 Supervised Learning으로써 Y(label)값이 존재하여 같이 학습시킨다. clustering은 학습에 Y(label)이 필요하지 않다. 그러므로 전반적인 성능은 Y(label)값을 같이 학습시키는 Supervised Learning이 더 좋은 것이 일반적이다. 허나, 무조건적으로 예측을 할 때 사용하는 것보다 <code>X(입력변수)간의 관계를 규명하거나 살펴보기위한 시각화를 위해 많이 사용</code>되므로 label값이 존재한다고 해서 분석 시에 Unsupervised Learning을 고려하지 않아도 된다는 것은 아니다.</li></ul><p><img src="/image/what_is_clustering_02.png" alt="Clustering이란? - 02"></p><h3 id="군집화-방법"><a href="#군집화-방법" class="headerlink" title="군집화 방법"></a>군집화 방법</h3><ul><li>군집화 방법에는 목적과 방법에 따라 다양한 모형이 존재한다.<ul><li>K-means Clustering</li><li>DBSCAN Clustering</li><li>Affinity Propagation Clustering(유사도 전파 군집화)</li><li>Hierarchical Clustering(계층적 군집화)</li><li>Spectral Clustering(스펙트럴 군집화)</li></ul></li></ul><ul><li>군집화 방법은 사용법과 모수 등이 서로 다르다. 예를 들어 K-means clustering이나 Spectral Clustering 등은 군집의 개수를 미리 지정해 주어야 하지만 DBSCAN Clustering이나 Affinity Propagation clustering 등은 군집의 개수를 지정할 필요가 없다. 다만 이 경우에는 모형에 따라 특별한 모수를 지정해 주어야 하는데 이 모수의 값에 따라 군집화 개수가 달라질 수 있다.</li></ul><ul><li>다음은 몇가지 예제 데이터에 대해 위에서 나열한 군집화 방법을 적용한 결과이다. 같은 색상의 데이터는 같은 군집으로 분류된 것이다. 그림에서 볼 수 있듯 <code>각 군집화 방법마다 특성이 다르므로 원하는 목적과 데이터 유형에 맞게 사용</code>해야 한다. 또한 지정된 모수의 값에 따라 성능이 달라질 수 있다. 이 결과는 최적화된 모수를 사용한 결과는 아니라는 점에 유의하자.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import *</span><br><span class="line">from sklearn.cluster import *</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line">from sklearn.utils.testing import ignore_warnings</span><br><span class="line">import matplotlib.font_manager as fm</span><br><span class="line"></span><br><span class="line">path =[(f.name, f.fname) <span class="keyword">for</span> f <span class="keyword">in</span> fm.fontManager.ttflist <span class="keyword">if</span> <span class="string">'Nanum'</span> <span class="keyword">in</span> f.name][1][1]</span><br><span class="line">font_name = fm.FontProperties(fname=path, size=50).get_name()</span><br><span class="line">plt.rc(<span class="string">'font'</span>, family=font_name)</span><br><span class="line"></span><br><span class="line">np.random.seed(0)</span><br><span class="line">n_samples = 1500</span><br><span class="line">blobs = make_blobs(n_samples=n_samples, random_state=8)</span><br><span class="line">X, y = make_blobs(n_samples=n_samples, random_state=170)</span><br><span class="line">anisotropic = (np.dot(X, [[0.6, -0.6], [-0.4, 0.8]]), y)</span><br><span class="line">varied = make_blobs(n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=170)</span><br><span class="line">noisy_circles = make_circles(n_samples=n_samples, factor=.5, noise=.05)</span><br><span class="line">noisy_moons = make_moons(n_samples=n_samples, noise=.05)</span><br><span class="line">no_structure = np.random.rand(n_samples, 2), None</span><br><span class="line">datasets = &#123;</span><br><span class="line">    <span class="string">"같은 크기의 원형"</span>: blobs,</span><br><span class="line">    <span class="string">"같은 크기의 타원형"</span>: anisotropic,</span><br><span class="line">    <span class="string">"다른 크기의 원형"</span>: varied,</span><br><span class="line">    <span class="string">"초승달"</span>: noisy_moons,</span><br><span class="line">    <span class="string">"동심원"</span>: noisy_circles,</span><br><span class="line">    <span class="string">"비구조화"</span>: no_structure</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(11, 11))</span><br><span class="line">plot_num = 1</span><br><span class="line"><span class="keyword">for</span> i, (data_name, (X, y)) <span class="keyword">in</span> enumerate(datasets.items()):</span><br><span class="line">    <span class="keyword">if</span> data_name <span class="keyword">in</span> [<span class="string">"초승달"</span>, <span class="string">"동심원"</span>]:</span><br><span class="line">        n_clusters = 2</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        n_clusters = 3</span><br><span class="line"></span><br><span class="line">    X = StandardScaler().fit_transform(X)</span><br><span class="line"></span><br><span class="line">    two_means = MiniBatchKMeans(n_clusters=n_clusters)</span><br><span class="line">    dbscan = DBSCAN(eps=0.15)</span><br><span class="line">    spectral = SpectralClustering(n_clusters=n_clusters, affinity=<span class="string">"nearest_neighbors"</span>)</span><br><span class="line">    ward = AgglomerativeClustering(n_clusters=n_clusters)</span><br><span class="line">    affinity_propagation = AffinityPropagation(damping=0.9, preference=-200)</span><br><span class="line">    clustering_algorithms = (</span><br><span class="line">        (<span class="string">'K-Means'</span>, two_means),</span><br><span class="line">        (<span class="string">'DBSCAN'</span>, dbscan),</span><br><span class="line">        (<span class="string">'Hierarchical Clustering'</span>, ward),</span><br><span class="line">        (<span class="string">'Affinity Propagation'</span>, affinity_propagation),</span><br><span class="line">        (<span class="string">'Spectral Clustering'</span>, spectral),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j, (name, algorithm) <span class="keyword">in</span> enumerate(clustering_algorithms):</span><br><span class="line">        with ignore_warnings(category=UserWarning):</span><br><span class="line">            algorithm.fit(X)</span><br><span class="line">        <span class="keyword">if</span> hasattr(algorithm, <span class="string">'labels_'</span>):</span><br><span class="line">            y_pred = algorithm.labels_.astype(np.int)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_pred = algorithm.predict(X)</span><br><span class="line">        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)</span><br><span class="line">        <span class="keyword">if</span> i == 0:</span><br><span class="line">            plt.title(name)</span><br><span class="line">        <span class="keyword">if</span> j == 0:</span><br><span class="line">            plt.ylabel(data_name)</span><br><span class="line">        colors = plt.cm.tab10(np.arange(20, dtype=int))</span><br><span class="line">        plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[y_pred])</span><br><span class="line">        plt.xlim(-2.5, 2.5)</span><br><span class="line">        plt.ylim(-2.5, 2.5)</span><br><span class="line">        plt.xticks(())</span><br><span class="line">        plt.yticks(())</span><br><span class="line">        plot_num += 1</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/clustering_method_difference_many.png" alt="clustering 방법에 따른 특성"></p><h3 id="군집화-성능-기준"><a href="#군집화-성능-기준" class="headerlink" title="군집화 성능 기준"></a>군집화 성능 기준</h3><ul><li>군집화의 경우에는 Classification 문제와 달리 성능 기준을 만들기 어렵다. 심지어는 원래 데이터가 어떻게 군집화 되어 있었는지를 보여주는 정답(ground truth)이 있는 경우도 마찬가지이다. 따라서 다양한 성능 기준이 사용되고 있다.</li></ul><ul><li><p>다음은 군집화 성능 기준의 예이다.</p><ul><li>Adjusted Rand Index (조정 랜드지수)</li><li>Adjusted Mutual Information (조정 상호정보량)</li><li>Silhouette Coefficient (실루엣 계수)</li></ul></li></ul><h3 id="일치-행렬"><a href="#일치-행렬" class="headerlink" title="일치 행렬"></a>일치 행렬</h3><ul><li><code>랜드 지수를 구하려면 데이터가 원래 어떻게 군집화 되어 있어야 하는지를 알려주는 정답(ground truth)이 있어야 한다.</code> $ N $개의 데이터 집합에서 $ i $, $ j $ 두 개의 데이터를 선택하였을 때 그 두 데이터가 같은 군집에 속하면 1 다른 군집에 속하면 0이라고 하자. 이 값을 $ N \prod N $ 행렬 $ T $ 로 나타내면 다음과 같다.</li></ul><script type="math/tex; mode=display">T_{ij} = \begin{cases} 1 & \text{ $ i $와 $ j $가 같은 군집} \\ 0 & \text{ $ i $와 $ j $가 다른 군집} \\ \end{cases}</script><ul><li>예를 들어 $ \{ 0, 1, 2, 3, 4 \} $ 라는 5개의 데이터 집합에서 $ \{ 0, 1, 2 \} $와 $ \{ 3, 4 \} $ 가 각각 같은 군집이라면 행렬 T는 다음과 같다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">groundtruth = np.array([</span><br><span class="line">    [1, 1, 1, 0, 0],</span><br><span class="line">    [1, 1, 1, 0, 0],</span><br><span class="line">    [1, 1, 1, 0, 0],</span><br><span class="line">    [0, 0, 0, 1, 1],</span><br><span class="line">    [0, 0, 0, 1, 1],</span><br><span class="line">])</span><br></pre></td></tr></table></figure><hr><ul><li>이제 군집화 결과를 같은 방법으로 행렬 $ C $ 로 표시하자. 만약 군집화가 정확하다면 이 행렬은 정답을 이용해서 만든 행렬과 거의 같은 값을 가져야 한다. 만약 군집화 결과 $ \{ 0, 1 \} $ 와 $ \{ 2, 3, 4 \} $ 가 같은 군집이라면 행렬 C 는 다음과 같다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">clustering = np.array([</span><br><span class="line">    [1, 1, 0, 0, 0],</span><br><span class="line">    [1, 1, 0, 0, 0],</span><br><span class="line">    [0, 0, 1, 1, 1],</span><br><span class="line">    [0, 0, 1, 1, 1],</span><br><span class="line">    [0, 0, 1, 1, 1],</span><br><span class="line">])</span><br></pre></td></tr></table></figure><hr><ul><li>이 두 행렬의 모든 원소에 대해 값이 같으면 1 다르면 0으로 계산한 행렬을 일치행렬(incidence matrix)라고 한다. 즉, 데이터 집합에서 만들 수 있는 모든 데이터 쌍에 대해 정답과 군집화 결과에서 동일값을 나타내면 1, 다르면 0이 된다.</li></ul><script type="math/tex; mode=display">R_{ij} = \begin{cases} 1 & \text{ if } T_{ij} = C_{ij} \\ 0 & \text{ if } T_{ij} \neq C_{ij} \\ \end{cases}</script><ul><li>즉, 원래 정답에서 1번 데이터와 2번 데이터가 같은(다른) 군집인데 군집화 결과에서도 같은(다른) 군집이라고 하면 $ R_{12} = 1$ 이다. 위 예제에서 일치행렬을 구하면 다음과 같다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">incidence = 1 * (groundtruth == clustering)  <span class="comment"># 1*는 True/False를 숫자 0/1로 바꾸기 위한 계산</span></span><br><span class="line">incidence</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">array([[1, 1, 0, 1, 1],</span><br><span class="line">       [1, 1, 0, 1, 1],</span><br><span class="line">       [0, 0, 1, 0, 0],</span><br><span class="line">       [1, 1, 0, 1, 1],</span><br><span class="line">       [1, 1, 0, 1, 1]])</span><br></pre></td></tr></table></figure><hr><ul><li>이 일치 행렬은 두 데이터의 순서를 포함하므로 대칭행렬이다. 만약 데이터의 순서를 무시한다면 위 행렬에서 대각성분과 아래쪽 비대각 성분은 제외한 위쪽 비대각 성분만을 고려해야 한다. 위쪽 비대각 성분에서 1의 개수는 다음과 같아 진다.</li></ul><script type="math/tex; mode=display">a = \text{T에서 같은 군집에 있고 C에서도 같은 군집에 있는 데이터 쌍의 수}</script><script type="math/tex; mode=display">b = \text{T에서 다른 군집에 있고 C에서도 다른 군집에 있는 데이터 쌍의 수}</script><script type="math/tex; mode=display">\text{일치행렬 위쪽 비대각 성분에서 1의 개수} = a + b</script><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.fill_diagonal(incidence, 0) <span class="comment"># 대각성분 제외</span></span><br><span class="line">a_plus_b = np.sum(incidence) / 2 <span class="comment"># 대칭행렬이므로 절반만 센다.</span></span><br><span class="line">a_plus_b</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6.0</span><br></pre></td></tr></table></figure><hr><h3 id="랜드-지수"><a href="#랜드-지수" class="headerlink" title="랜드 지수"></a>랜드 지수</h3><ul><li>랜드 지수(Rand Index, RI)는 가능한 모든 데이터 쌍의 개수에 대해 정답인 데이터 쌍의 개수의 비율로 정의한다.</li></ul><script type="math/tex; mode=display">\text{Rand Index} = \frac{a+b}{ {}_N C_2 }</script><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from scipy.special import comb</span><br><span class="line">rand_index = a_plus_b / comb(incidence.shape[0], 2)</span><br><span class="line">rand_index</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.6</span><br></pre></td></tr></table></figure><hr><h3 id="조정-랜드-지수"><a href="#조정-랜드-지수" class="headerlink" title="조정 랜드 지수"></a>조정 랜드 지수</h3><ul><li>랜드지수는 0부터 1까지의 값을 가지고 1이 가장 좋은 성능을 뜻한다. <code>랜드지수의 문제점은 무작위로 군집화를 한 경우에도 어느 정도 좋은 값이 나올 가능성이 높다는 점</code>이다. 즉 무작위 군집화에서 생기는 랜드지수의 기대값이 너무 크다. 이를 해결하기 위해 무작위 군집화에서 생기는 랜드지수의 기대값을 원래의 값에서 빼서 기댓값과 분산을 재조정하는 것이 조정 랜드지수(adjusted Rand Index, ARI)이다.</li></ul><script type="math/tex; mode=display">\text{ARI} = \dfrac{\text{RI} - \text{E}[\text{RI}]}{\max(\text{RI}) - \text{E}[\text{RI}]}</script><ul><li>adjusted Rand Index는 성능이 완벽한 경우 1이 된다. 반대로 가장 나쁜 경우로서 무작위 군집화를 하면 0에 가까운 값이 나온다. 경우에 따라서는 음수가 나올 수도 있다.</li></ul><ul><li>위에서 예로 들었던 타원형 데이터 예제에 대해 여러가지 군집화 방법을 적용하였을 때 조정 랜드지수를 계산해보면 DBSCAN과 Spectral Clustering의 값이 높게 나오는 것을 확인할 수 있다. Scikit-Learn 패키지의 metrics.cluster 서브패키지는 조정 랜드지수를 계산하는 <code>adjusted_rand_score</code>명령을 제공한다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics.cluster import adjusted_rand_score</span><br><span class="line"></span><br><span class="line">X, y_true = anisotropic</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(20, 5))</span><br><span class="line">plot_num = 1</span><br><span class="line">X = StandardScaler().fit_transform(X)</span><br><span class="line"><span class="keyword">for</span> name, algorithm <span class="keyword">in</span> clustering_algorithms:</span><br><span class="line">    with ignore_warnings(category=UserWarning):</span><br><span class="line">        algorithm.fit(X)</span><br><span class="line">    <span class="keyword">if</span> hasattr(algorithm, <span class="string">'labels_'</span>):</span><br><span class="line">        y_pred = algorithm.labels_.astype(np.int)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y_pred = algorithm.predict(X)</span><br><span class="line">    title = <span class="string">"ARI=&#123;:5.3f&#125;"</span>.format(adjusted_rand_score(y_true, y_pred))</span><br><span class="line">    plt.subplot(1, len(datasets), plot_num)</span><br><span class="line">    plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[y_pred])</span><br><span class="line">    plt.xlim(-2.5, 2.5)</span><br><span class="line">    plt.ylim(-2.5, 2.5)</span><br><span class="line">    plt.xlabel(name)</span><br><span class="line">    plt.xticks(())</span><br><span class="line">    plt.yticks(())</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plot_num += 1</span><br></pre></td></tr></table></figure><hr><p><img src="/image/adjusted_rand_score.png" alt="타원형 군집의 데이터에 대한 Adjusted Rand Index"></p><h3 id="조정-상호정보량"><a href="#조정-상호정보량" class="headerlink" title="조정 상호정보량"></a>조정 상호정보량</h3><ul><li><p>상호정보량(mutual information)은 두 확률변수간의 상호 의존성을 측정한 값이다. 군집화 결과를 <code>이산확률변수라고 가정</code>한다.</p></li><li><p>정답은 아래와 같이 $ r $ 개의 값을 가질 수 있는 이산확률변수이고</p></li></ul><script type="math/tex; mode=display">T = \{T_1, T_2,\ldots, T_r\}</script><ul><li>군집화 결과는 아래와 같이 $ s $ 개의 값을 가질 수 있는 이산확률 변수라고 하자.</li></ul><script type="math/tex; mode=display">C = \{C_1, C_2,\ldots, C_s\}</script><ul><li>전체 데이터의 개수를 $ N $ 이라고 하면 이산확률변수 $ T $ 의 분포는 아래와 같이 추정할 수 있다. 이 식에서 $ |T_i| $는 군집 $ T_{i} $ 에 속하는 데이터의 개수를 나타낸다.</li></ul><script type="math/tex; mode=display">P(i)=\frac{|T_i|}{N}</script><ul><li>비슷하게 이산확률 변수 $ C $ 의 분포는 아래와 같이 추정할 수 있다.</li></ul><script type="math/tex; mode=display">P'(j)=\frac{|C_i|}{N}</script><ul><li><p>$ T $ 와 $ C $ 의 결합확률분포는 아래와 같이 추정할 수 있다. 여기에서 $ |T_i \cap C_j| $ 는 군집 $ T_{i} $ 에도 속하고 군집 $ C_{j} $ 에도 속하는 데이터의 개수를 나타낸다.</p></li><li><p>확률변수 $ T, C $ 의 상호정보량은 아래와 같이 정의한다.</p></li></ul><script type="math/tex; mode=display">MI(T,C)=\sum_{i=1}^r \sum_{j=1}^s P(i,j)\log \frac{P(i,j)}{P(i)P'(j)}</script><ul><li><p><code>만약 두 확률변수가 서로 독립이면 상호정보량의 값은 로그값이 1이 되어 0이며 이 값이 상호정보량이 가질 수 있는 최소값</code>이다. <code>두 확률변수가 의존성이 강할수록 상호정보량은 증가</code>한다. <code>또한 군집의 개수가 많아져도 상호정보량이 증가하므로 올바른 비교가 어렵다.</code> 따라서 조정 랜드지수의 경우와 마찬가지로 각 경우에 따른 상호정보량의 기댓값을 빼서 재조정한 것이 조정 상호정보량이다.</p></li><li><p>다음은 위에서 예로 들었던 타원형 데이터 예제에 대해 여러가지 군집화 방법을 적용하였을때 조정 상호 정보량값을 계산한 결과이다. Scikit-Learn 패키지의 met rics.cluster 서브패키지는 조정 상호정보량을 계산하는 <code>adjusted_mutual_info_score</code> 명령을 제공한다.</p></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics.cluster import adjusted_mutual_info_score</span><br><span class="line"></span><br><span class="line">X, y_true = anisotropic</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(20, 5))</span><br><span class="line">plot_num = 1</span><br><span class="line">X = StandardScaler().fit_transform(X)</span><br><span class="line"><span class="keyword">for</span> name, algorithm <span class="keyword">in</span> clustering_algorithms:</span><br><span class="line">    with ignore_warnings(category=UserWarning):</span><br><span class="line">        algorithm.fit(X)</span><br><span class="line">    <span class="keyword">if</span> hasattr(algorithm, <span class="string">'labels_'</span>):</span><br><span class="line">        y_pred = algorithm.labels_.astype(np.int)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        y_pred = algorithm.predict(X)</span><br><span class="line">    title = <span class="string">"AMI=&#123;:5.3f&#125;"</span>.format(adjusted_mutual_info_score(y_true, y_pred))</span><br><span class="line">    plt.subplot(1, len(datasets), plot_num)</span><br><span class="line">    plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[y_pred])</span><br><span class="line">    plt.xlim(-2.5, 2.5)</span><br><span class="line">    plt.ylim(-2.5, 2.5)</span><br><span class="line">    plt.xlabel(name)</span><br><span class="line">    plt.xticks(())</span><br><span class="line">    plt.yticks(())</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plot_num += 1</span><br></pre></td></tr></table></figure><hr><p><img src="/image/adjusted_mutual_info_score.png" alt="알고리즘별 조정 상호정보량"></p><h3 id="실루엣-계수"><a href="#실루엣-계수" class="headerlink" title="실루엣 계수"></a>실루엣 계수</h3><ul><li>지금까지는 각각의 데이터가 원래 어떤 군집에 속해있었는지 정답(groundtruth)를 알고있는 경우를 다루었다. 하지만 <code>정답 정보가 없다면 어떻게 군집화가 잘 되어었는지 판단할 수 있을까?</code> 실루엣 계수(Silhouette Coefficient)는 이러한 경우에 군집화의 성능을 판단하기 위한 기준의 하나이다.</li></ul><ul><li><p>우선 모든 데이터 쌍 $ (i, j) $ 에 대해 거리 혹은 비유사도(dissimilarity)을 구한다. 이 결과를 이용하여 모든 데이터 $ i $ 에 대해 다음 값을 구한다.</p><ul><li>$ a_{i} $ : $ i $ 와 같은 군집에 속한 원소들의 평균 거리</li><li>$ b_{i} $ : $ i $ 와 다른 군집 중 가장 가까운 군집까지의 평균 거리</li></ul></li></ul><ul><li>이 때 데이터 $ i $에 대한 실루엣 계수는 아래와 같이 정의하며 <code>전체 데이터의 실루엣계수를 평균된 값을 평균 실루엣 계수</code>라고 한다.</li></ul><script type="math/tex; mode=display">s_i = \dfrac{b_i - a_i}{\max{(a_i, b_i)}}</script><ul><li>만약 데이터 $ i $ 에 대해 같은 군집의 데이터가 다른 군집의 데이터 보다 더 가깝다면 그 데이터의 실루엣 계수는 양수가 된다. 하지만 만약 다른 군집의 데이터가 같은 군집의 데이터 보다 더 가깝다면 군집화가 잘못된 경우라고 볼 수 있는데 이 때는 그 데이터의 실루엣 계수가 음수가 된다. <code>잘못된 군집화에서는 실루엣 계수가 음수인 데이터가 많아지므로 평균 실루엣 계수가 작아진다.</code> 따라서 <code>실루엣 계수가 클수록 좋은 군집화</code>라고 할 수 있다.</li></ul><ul><li><code>군집화 방법 중에는 군집의 개수를 사용자가 정해주어야 하는 것들이 있는데 실루엣 계수는 이 경우 군집의 개수를 정하는데 큰 도움이 된다.</code></li></ul><ul><li>앞에서 예로 들었던 3개의 원형 데이터에 대해 Spectral Clustering 방법으로 군집 개수를 바꾸어 가면서 군집화 결과를 살펴보자.<ul><li>참고로, Scikit-Learn 패키지의 metrics 서브패키지는 실루엣계수를 계산하는 <code>silhouette_samples</code> 명령을 제공한다.</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">silhouette_samples(X, y_pred)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import silhouette_samples</span><br><span class="line"></span><br><span class="line">def plot_silhouette(data):</span><br><span class="line">    X = StandardScaler().fit_transform(data[0])</span><br><span class="line">    colors = plt.cm.tab10(np.arange(20, dtype=int))</span><br><span class="line">    plt.figure(figsize=(15, 12))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(4):</span><br><span class="line">        model = SpectralClustering(n_clusters=i + 2, affinity=<span class="string">"nearest_neighbors"</span>)</span><br><span class="line">        cluster_labels = model.fit_predict(X)</span><br><span class="line">        sample_silhouette_values = silhouette_samples(X, cluster_labels)</span><br><span class="line">        silhouette_avg = sample_silhouette_values.mean()</span><br><span class="line"></span><br><span class="line">        plt.subplot(4, 2, 2 * i + 1)</span><br><span class="line">        y_lower = 10</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i + 2):</span><br><span class="line">            jth_cluster_silhouette_values = sample_silhouette_values[cluster_labels == j]</span><br><span class="line">            jth_cluster_silhouette_values.sort()</span><br><span class="line">            size_cluster_j = jth_cluster_silhouette_values.shape[0]</span><br><span class="line">            y_upper = y_lower + size_cluster_j</span><br><span class="line">            plt.fill_betweenx(np.arange(y_lower, y_upper),</span><br><span class="line">                              0, jth_cluster_silhouette_values,</span><br><span class="line">                              facecolor=colors[j], edgecolor=colors[j])</span><br><span class="line">            plt.text(-0.05, y_lower + 0.5 * size_cluster_j, str(j + 1))</span><br><span class="line">            plt.axvline(x=silhouette_avg, color=<span class="string">"red"</span>, linestyle=<span class="string">"--"</span>)</span><br><span class="line">            plt.xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])</span><br><span class="line">            plt.yticks([])</span><br><span class="line">            plt.title(<span class="string">"실루엣계수 평균: &#123;:5.2f&#125;"</span>.format(silhouette_avg))</span><br><span class="line">            y_lower = y_upper + 10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        plt.subplot(4, 2, 2 * i + 2)</span><br><span class="line">        plt.scatter(X[:, 0], X[:, 1], s=5, color=colors[cluster_labels])</span><br><span class="line">        plt.xlim(-2.5, 2.5)</span><br><span class="line">        plt.ylim(-2.5, 2.5)</span><br><span class="line">        plt.xticks(())</span><br><span class="line">        plt.yticks(())</span><br><span class="line">        plt.title(<span class="string">"군집 수: &#123;&#125;"</span>.format(i + 2))</span><br><span class="line"></span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    <span class="comment"># plt.savefig("silhouette_coefficient")</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">plot_silhouette(blobs)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/silhouette_coefficient.png" alt="타원형 데이터를 Spectral Clustering한 결과와 실루엣 계수값"></p><ul><li><code>다만 실루엣 계수는 군집의 형상이 복잡하거나 크기의 차이가 많이 나면 정상적인 비교가 불가능</code>하다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_silhouette(noisy_circles)</span><br></pre></td></tr></table></figure><p><img src="/image/silhouette_coefficient_01.png" alt="원형 데이터를 Spectral Clustering한 결과와 실루엣 계수값"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/29/machine_learning_17/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Ensemble Learning - Ensemble의 Ensemble</title>
      <link>https://heung-bae-lee.github.io/2020/05/27/machine_learning_16/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/27/machine_learning_16/</guid>
      <pubDate>Tue, 26 May 2020 17:04:18 GMT</pubDate>
      <description>
      
        
        
          &lt;h3 id=&quot;Ensemble의-Ensemble&quot;&gt;&lt;a href=&quot;#Ensemble의-Ensemble&quot; class=&quot;headerlink&quot; title=&quot;Ensemble의 Ensemble&quot;&gt;&lt;/a&gt;Ensemble의 Ensemble&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h3 id="Ensemble의-Ensemble"><a href="#Ensemble의-Ensemble" class="headerlink" title="Ensemble의 Ensemble"></a>Ensemble의 Ensemble</h3><ul><li>Ensemble의 개념 자체가 여러 개의 기본 모델을 활용하여 하나의 새로운 모델을 만들어 내는 개념이다.</li></ul><p><img src="/image/Ensemble_and_Ensemble_conception_01.png" alt="Ensemble의 개념"></p><ul><li>그러므로 Ensemble 모델을 하나의 weak learner로 설정하면 Ensemble의 Ensemble 모델을 만들 수 있다.</li></ul><p><img src="/image/Ensemble_and_Ensemble_conception_02.png" alt="Ensemble의 Ensemble 모델 개념 - 01"></p><ul><li>Ensemble의 Ensemble 모델은 다양한 모델을 사용하므로 Boosting 계열 알고리즘이 갖는 <code>hyper parameter에 민감한 경향을 완화</code>시켜 줄 수 있다. 또한, <code>패키지로 되어있지 않기 때문에 셔플을 통한 데이터의 추룰로 Bagging과 같은 효과를 줄 수 있고, 추가적으로 feature들에도 마치 deep learning의 dropout 같은 효과를 통해 Randomforest 알고리즘과 같은 효과를 기대 할 수도 있다.</code></li></ul><p><img src="/image/Ensemble_and_Ensemble_conception_03.png" alt="Ensemble의 Ensemble 모델 개념 - 02"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터 불러오기</span></span><br><span class="line">data = pd.read_csv(<span class="string">"../data/kc_house_data.csv"</span>)</span><br><span class="line">data.head() <span class="comment"># 데이터 확인</span></span><br></pre></td></tr></table></figure><hr><h4 id="불필요한-데이터-제거-및-train-test-set-분리"><a href="#불필요한-데이터-제거-및-train-test-set-분리" class="headerlink" title="불필요한 데이터 제거 및 train, test set 분리"></a>불필요한 데이터 제거 및 train, test set 분리</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data = data.drop([<span class="string">'id'</span>, <span class="string">'date'</span>, <span class="string">'zipcode'</span>, <span class="string">'lat'</span>, <span class="string">'long'</span>], axis = 1) <span class="comment"># id, date, zipcode, lat, long  제거</span></span><br><span class="line">feature_columns = list(data.columns.difference([<span class="string">'price'</span>])) <span class="comment"># Price를 제외한 모든 행</span></span><br><span class="line">X = data[feature_columns]</span><br><span class="line">y = data[<span class="string">'price'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 42) <span class="comment"># 학습데이터와 평가데이터의 비율을 7:3</span></span><br><span class="line"><span class="built_in">print</span>(train_x.shape, test_x.shape, train_y.shape, test_y.shape) <span class="comment"># 데이터 개수 확인</span></span><br></pre></td></tr></table></figure><hr><h4 id="LightGBM-학습"><a href="#LightGBM-학습" class="headerlink" title="LightGBM 학습"></a>LightGBM 학습</h4><ul><li>Scikit-learn과 호환되는 LightGBMClassifier API를 이용하는 것이 더 편하지만, 필자는 python wrapper package를 사용해 예시를 들어 볼 것이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install lightgbm</span></span><br><span class="line">import lightgbm as lgb</span><br><span class="line">start = time.time() <span class="comment"># 시작 시간 지정</span></span><br><span class="line">lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) <span class="comment"># 학습 데이터를 LightGBM 모델에 맞게 변환</span></span><br><span class="line">lgb_param = &#123;<span class="string">'max_depth'</span>: 10, <span class="comment"># 트리 깊이</span></span><br><span class="line">            <span class="string">'learning_rate'</span>: 0.01, <span class="comment"># Step Size</span></span><br><span class="line">            <span class="string">'n_estimators'</span>: 500, <span class="comment"># Number of trees, 트리 생성 개수</span></span><br><span class="line">            <span class="string">'objective'</span>: <span class="string">'regression'</span>&#125; <span class="comment"># 파라미터 추가, Label must be in [0, num_class) -&gt; num_class보다 1 커야한다.</span></span><br><span class="line">lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) <span class="comment"># 학습 진행</span></span><br></pre></td></tr></table></figure><ul><li>위에서 적합 시킨 Boosting계열의 Ensemble 모델인 LightGBM 모델의 성능을 먼저 측정해 보면 다음과 같다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import mean_squared_error, r2_score</span><br><span class="line">from math import sqrt</span><br><span class="line"></span><br><span class="line">sqrt(mean_squared_error(lgb_model.predict(test_x),test_y))</span><br></pre></td></tr></table></figure><hr><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">210904.17249451784</span><br></pre></td></tr></table></figure><h3 id="Ensemble의-Ensemble-1"><a href="#Ensemble의-Ensemble-1" class="headerlink" title="Ensemble의 Ensemble"></a>Ensemble의 Ensemble</h3><ul><li>Ensemble의 Ensemble을 함으로써, hyper paramter tuning에 덜 민감해 질 수 있다.</li></ul><ul><li>아래 코드를 실해하고나면 random하게 추출된 데이터로 학습된 30번의 LightGBM 모델의 예측 결과의 배열이 존재하는 리스트의 값을 얻을 수 있다.<ul><li>random하게 추출하였기에 Bagging의 효과를 얻을 수 있으며, 필자는 데이터만 임의복원추출하였지만, feature도 임의복원추출 한다면 RandomForest와 같은 효과를 기대할 수 있을 것이다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">bagging_predict_result = [] <span class="comment"># 빈 리스트 생성</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(30):</span><br><span class="line">    data_index = [data_index <span class="keyword">for</span> data_index <span class="keyword">in</span> range(train_x.shape[0])] <span class="comment"># 학습 데이터의 인덱스를 리스트로 변환</span></span><br><span class="line">    random_data_index = np.random.choice(data_index, train_x.shape[0])</span><br><span class="line">    <span class="built_in">print</span>(len(<span class="built_in">set</span>(random_data_index)))</span><br><span class="line">    lgb_dtrain = lgb.Dataset(data = train_x.iloc[random_data_index,], label = train_y.iloc[random_data_index]) <span class="comment"># 학습 데이터를 LightGBM 모델에 맞게 변환</span></span><br><span class="line">    lgb_param = &#123;<span class="string">'max_depth'</span>: 10, <span class="comment"># 트리 깊이</span></span><br><span class="line">                <span class="string">'learning_rate'</span>: 0.01, <span class="comment"># Step Size</span></span><br><span class="line">                <span class="string">'n_estimators'</span>: 500, <span class="comment"># Number of trees, 트리 생성 개수</span></span><br><span class="line">                <span class="string">'objective'</span>: <span class="string">'regression'</span>&#125; <span class="comment"># regression이므로 목적(비용)함수를 regression으로 바꾸어준다.</span></span><br><span class="line">    lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) <span class="comment"># 학습 진행</span></span><br><span class="line"></span><br><span class="line">    predict1 = lgb_model.predict(test_x) <span class="comment"># 테스트 데이터 예측</span></span><br><span class="line">    bagging_predict_result.append(predict1) <span class="comment"># 반복문이 실행되기 전 빈 리스트에 결과 값 저장</span></span><br><span class="line">    <span class="built_in">print</span>(sqrt(mean_squared_error(lgb_model.predict(test_x),test_y)))</span><br></pre></td></tr></table></figure><hr><ul><li>위의 결과를 토대로 ensemble 기법을 사용하기 위해선 다음과 같이 각 모델별 동일한 인덱스의 예측값에 대한 평균을 구해 최종적인 예측값으로 사용해야 할 것이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Bagging을 바탕으로 예측한 결과값에 대한 평균을 계산</span></span><br><span class="line">bagging_predict = [] <span class="comment"># 빈 리스트 생성</span></span><br><span class="line"><span class="keyword">for</span> lst2_index <span class="keyword">in</span> range(test_x.shape[0]): <span class="comment"># 테스트 데이터 개수만큼의 반복</span></span><br><span class="line">    temp_predict = [] <span class="comment"># 임시 빈 리스트 생성 (반복문 내 결과값 저장)</span></span><br><span class="line">    <span class="keyword">for</span> lst_index <span class="keyword">in</span> range(len(bagging_predict_result)): <span class="comment"># Bagging 결과 리스트 반복</span></span><br><span class="line">        temp_predict.append(bagging_predict_result[lst_index][lst2_index]) <span class="comment"># 각 Bagging 결과 예측한 값 중 같은 인덱스를 리스트에 저장</span></span><br><span class="line">    bagging_predict.append(np.mean(temp_predict)) <span class="comment"># 해당 인덱스의 30개의 결과값에 대한 평균을 최종 리스트에 추가</span></span><br></pre></td></tr></table></figure><hr><ul><li>최종적인 성능값은 다음과 같다. 단일 Ensemble을 사용한 경우보다 성능이 좋아졌다는 것을 확인 할 수 있다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqrt(mean_squared_error(bagging_predict,test_y))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">209736.90720982864</span><br></pre></td></tr></table></figure><h3 id="중요변수-추출-방법"><a href="#중요변수-추출-방법" class="headerlink" title="중요변수 추출 방법"></a>중요변수 추출 방법</h3><ul><li>모델의 성능이 좋아야하는 것고 물론 중요하지만, Y(target)에 대한 중요한 변수가 무엇인지 어떻게 영향을 끼치는지를 해석하고 활용하는 부분도 굉장히 중요하다.</li></ul><ul><li>아래와 같이 단일 모형들은 상대적으로 변수에 대한 해석이 용이하다. 예를 들어, 선형회귀는 각 변수들의 계수를 통해 해당 변수가 반응변수에 미치는 영향력의 크기와 방향을 알 수 있으며, 그에 따른 신뢰도도 확인할 수 있다. 또한 Decision Tree의 경우에는 가장 information gain이 큰 변수가 제일 상위 노드에 위치할 것이며, 이런 방식을 통해 어떤 변수들이 반응 변수에 영향력을 주는지 확인 할 수 있다.</li></ul><p><img src="/image/how_to_translate_variable_with_model.png" alt="변수 추출 방법"></p><ul><li>이에 반해, Ensemble Model들과 Deep Learning의 경우 변수에 대한 해석이 어렵다. 각각의 장단점은 존재한다. 아래 그림에서 볼 수 있듯이 Accuracy(성능)와 변수 설명의 용이함에 따른 트레이드 오프가 존재한다고 볼 수 있다.</li></ul><p><img src="/image/how_to_translate_variable_with_Ensemble_model_and_deep_dlearning.png" alt="Ensemble Model과 deep learning의 변수 해석의 어려움"></p><ul><li>물론 Ensemble Model들도 feature importance를 구할 수는 있다. 허나 해당 수치를 바탕으로 합리적으로 변수가 반응변수에 미치는 영향을 합리적으로 판단하여 생각할 뿐이지, 영향력의 방향은 알 수 없다. 예를 들어 아래 그림에서와 같이 은행 이용 고객 데이터에 대해 수입이 50만 달러가 넘는지를 예측하는 Ensemble Model의 상위 12개의 feature에 대한 importance를 그래프로 표현하면 오른쪽 막대 그래프와 같다고 하자. 이 경우 회귀 분석과는 다르게 수치만 보았을 경우 명확히 나이가 반응 변수에 미치는 영향력의 크기는 상대적으로 가늠할 순 있겠지만 정확한 크기나 방향은 전혀 알 수 없다. 단지 우리가 나이가 많을수록 수입이 높아진다는 경험적사고를 바탕으로 합리적 생각을 하는 것 뿐이다.</li></ul><p><img src="/image/how_to_measure_feature_importance_in_Ensemble_model_01.png" alt="Ensemble Model의 Feature Importance 해석"></p><ul><li>위와 같이 Ensemble Model의 Feature Importance를 측정하는 기준이 존재한다.<ul><li><code>Weight</code>는 단순히 변수 별 데이터를 분리하는 데 사용된 횟수인데 반해, <code>Cover</code>는 분리된 데이터의 수를 가중치로 사용하기 때문에 트리의 구조상 가장 상위에 높은 분별력을 지닌 노드가 가중치가 가장 높기에 Weight에 비해 <code>합리적</code>이다. 마지막으로 <code>Gain</code>은 커버와 비슷한 개념이다.</li></ul></li></ul><p><img src="/image/feature_importance_measurity_in_Ensemble_Model.png" alt="Ensemble Model Feature Importance를 측정하는 기준 - 01"></p><ul><li>아래 그림과 같이 <code>각 Feature Importance를 측정하는 기준에 따라 변수의 중요도가 달라진다.</code> 또한, 위에서 언급한 것과 같이 해석이 용이하지 않다. 단지, <code>해당 변수가 중요하다는 정도만을 알 수 있을 뿐 양의방향으로 영향을 미치는지 음의 방향으로 영향을 미치는지 또는 얼마나 영향을 미치는지에 대해서는 알 수 없다.</code></li></ul><p><img src="/image/feature_importance_measurity_in_Ensemble_Model_01.png" alt="Ensemble Model Feature Importance를 측정하는 기준 - 02"></p><ul><li><code>Ensemble Model의 Feature에 대한 해석에 어려움은 대부분의 Feature importance 지표가 Inconsistency하기 때문</code>이기도 하다. 예를 들면, Bagging을 할 경우 부트스트랩 방식으로 train data를 임의복원추출하여 매번 진행하기 때문에 각 모델을 적합 시킨 데이터가 다르므로 Feature Importance를 계산해보아도 모든 모델에 대해 변수가 동일한 Feature Importance를 갖기 어렵다 그러므로 Feature Importance를 통해 변수를 비교하는 것은 어렵다.</li></ul><p><img src="/image/why_feature_importance_on_ensemble_model.png" alt="Ensemble Model의 Feature 해석의 어려움 - 02"></p><ul><li>아래 그림에서 Decision Tree Model들은 각각 열이나 기침에 의해서 어떤 질병이 걸릴 score에 대한 것이다. 허나 학습에 사용되어진 데이터에 의해 각 변수의 중요도는 모델에 따라 상이함을 확인할 수 있다.</li></ul><p><img src="/image/why_feature_importance_on_ensemble_model_example.png" alt="Ensemble Model의 Feature 해석의 어려움 예시"></p><h3 id="Shap-value"><a href="#Shap-value" class="headerlink" title="Shap value"></a>Shap value</h3><ul><li>위와 같은 문제점에 대해 조금 다르게 바라보고, 정말 중요한 변수가 무엇이고 어떻게 영향을 미치는지를 보기위한 지표이다.  </li></ul><p><img src="/image/what_is_shap_value.png" alt="Shap value의 개념"></p><ul><li>다음과 같이 평균 아파트 값이 310,000 유로 라고 할 때, 4가지 변수(park-nearby, cat-forbidden, area-50$ m^2 $, floor-2nd)를 사용하여 모델을 학습시킨 결과 예측값이 300,000 유로라고 가정해보자.</li></ul><p><img src="/image/shap_value_example_01.png" alt="Shap value 예시 - 01"></p><ul><li>shap value의 목적은 <code>평균 예측값과 실제 예측값의 차이에 대해 변수들에 대한 기여도를 계산하는 것</code>이다.</li></ul><p><img src="/image/shap_value_example_02.png" alt="Shap value 예시 - 02"></p><ul><li>임의의 변수에 대해서 기여도를 측정하고자 한다면 측정하고자하는 변수들을 제외한 나머지 다른 변수들은 무작위로 임의복원추출을 하여 학습시킨 후 예측치와 측정하고자 하는 변수들 중 특정 하나의 변수와 측정하지 않는 변수들을 임의복원추출하고 나머지 측정 대상 변수들은 고정을 시켜 학습한 모델의 예측치의 차이를 여러번 계산한 값들의 평균으로 기여도를 계산한다.</li></ul><p><img src="/image/shap_value_example_03.png" alt="Shap value 예시 - 03"></p><ul><li>아래 그림처럼 <code>Shap value를 통해 변수를 살펴보면 수치는 어느 정도 차이가 있지만 어떤 변수가 더 중요한지에 대한 일치성은 갖게 된다.</code></li></ul><p><img src="/image/shap_value_example_04.png" alt="Shap value 예시 - 04"></p><p><img src="/image/shap_value_example_05.png" alt="Shap value 예시 - 05"></p><ul><li><code>평균적으로 어떤 방향을 갖는다는 것을 의미하는 것이지, 회귀모형의 계수 해석시에 인과관계로 해석하면 안되는 것과 같이 Shap value 또한 인과관계로 해석하면 안된다.</code></li></ul><p><img src="/image/shap_value_example_06.png" alt="Shap value 해석시 유의점"></p><ul><li>아래 그림을 살펴보면 하나의 point는 하나의 관측치를 의미한다. $ x $축은 shap value를 의미하고, $ y $축은 각각의 변수들을 나타낸다. 점들이 많이 몰려있으면 색이 진해지므로 색이 진할수록 각 방향으로 영향력이 높은 것이다.</li></ul><p><img src="/image/shap_value_example_07.png" alt="Shap value 해석 예시 - 01"></p><ul><li>아래 그래프에서 20대 근방과 60대 이후에는 수직적으로 넓게 분포하고 30대 부터는 shap value가 양의 값을 갖는 경향을 보인다. 이를 바탕으로 30대 부터 60대 이전까지는 대체로 양의 영향을 주며, 20 근방에서는 다른 변수가 Age의 Importance에 영향을 준다고 해석할 수 있다.</li></ul><p><img src="/image/shap_value_example_08.png" alt="Shap value 해석 예시 - 02"></p><p><img src="/image/shap_value_example_09.png" alt="Shap value 해석 예시 - 03"></p><p><img src="/image/shap_value_example_10.png" alt="Shap value 해석 예시 - 04"></p><h3 id="Shap-value-실습"><a href="#Shap-value-실습" class="headerlink" title="Shap value 실습"></a>Shap value 실습</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br></pre></td></tr></table></figure><hr><h4 id="데이터-불러오기"><a href="#데이터-불러오기" class="headerlink" title="데이터 불러오기"></a>데이터 불러오기</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">"../data/kc_house_data.csv"</span>)</span><br><span class="line">data.head() <span class="comment"># 데이터 확인</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/shap_value_example_data_head.png" alt="데이터 상위 5개"></p><h4 id="데이터-info"><a href="#데이터-info" class="headerlink" title="데이터 info"></a>데이터 info</h4><ul><li>id: 집 고유아이디</li><li>date: 집이 팔린 날짜</li><li>price: 집 가격 (타겟변수)</li><li>bedrooms: 주택 당 침실 개수</li><li>bathrooms: 주택 당 화장실 개수</li><li>floors: 전체 층 개수</li><li>waterfront: 해변이 보이는지 (0, 1)</li><li>condition: 집 청소상태 (1~5)</li><li>grade: King County grading system 으로 인한 평점 (1~13)</li><li>yr_built: 집이 지어진 년도</li><li>yr_renovated: 집이 리모델링 된 년도</li><li>zipcode: 우편번호</li><li>lat: 위도</li><li>long: 경도</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nCar = data.shape[0] <span class="comment"># 데이터 개수</span></span><br><span class="line">nVar = data.shape[1] <span class="comment"># 변수 개수</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'nCar: %d'</span> % nCar, <span class="string">'nVar: %d'</span> % nVar )</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nCar: 21613 nVar: 14</span><br></pre></td></tr></table></figure><hr><h4 id="의미가-없다고-판단되는-변수-제거"><a href="#의미가-없다고-판단되는-변수-제거" class="headerlink" title="의미가 없다고 판단되는 변수 제거"></a>의미가 없다고 판단되는 변수 제거</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = data.drop([<span class="string">'id'</span>, <span class="string">'date'</span>, <span class="string">'zipcode'</span>, <span class="string">'lat'</span>, <span class="string">'long'</span>], axis = 1) <span class="comment"># id, date, zipcode, lat, long  제거</span></span><br></pre></td></tr></table></figure><hr><h4 id="범주형-변수를-이진형-변수로-변환"><a href="#범주형-변수를-이진형-변수로-변환" class="headerlink" title="범주형 변수를 이진형 변수로 변환"></a>범주형 변수를 이진형 변수로 변환</h4><ul><li>범주형 변수는 waterfront 컬럼 뿐이며, 이진 분류이기 때문에 0, 1로 표현한다.</li><li>데이터에서 0, 1로 표현되어 있으므로 과정 생략</li></ul><h4 id="설명변수와-타겟변수를-분리-학습데이터와-평가데이터-분리"><a href="#설명변수와-타겟변수를-분리-학습데이터와-평가데이터-분리" class="headerlink" title="설명변수와 타겟변수를 분리, 학습데이터와 평가데이터 분리"></a>설명변수와 타겟변수를 분리, 학습데이터와 평가데이터 분리</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">feature_columns = list(data.columns.difference([<span class="string">'price'</span>])) <span class="comment"># Price를 제외한 모든 행</span></span><br><span class="line">X = data[feature_columns]</span><br><span class="line">y = data[<span class="string">'price'</span>]</span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 42) <span class="comment"># 학습데이터와 평가데이터의 비율을 7:3</span></span><br><span class="line"><span class="built_in">print</span>(train_x.shape, test_x.shape, train_y.shape, test_y.shape) <span class="comment"># 데이터 개수 확인</span></span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(15129, 8) (6484, 8) (15129,) (6484,)</span><br></pre></td></tr></table></figure><hr><h4 id="LightGBM-을-이용하여-아파트-가격-예측-회귀"><a href="#LightGBM-을-이용하여-아파트-가격-예측-회귀" class="headerlink" title="LightGBM 을 이용하여 아파트 가격 예측 (회귀)"></a>LightGBM 을 이용하여 아파트 가격 예측 (회귀)</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import lightgbm as lgb</span><br><span class="line">from math import sqrt</span><br><span class="line">from sklearn.metrics import mean_squared_error</span><br><span class="line">lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) <span class="comment"># 학습 데이터를 LightGBM 모델에 맞게 변환</span></span><br><span class="line">lgb_param = &#123;<span class="string">'max_depth'</span>: 10, <span class="comment"># 트리 깊이</span></span><br><span class="line">            <span class="string">'learning_rate'</span>: 0.01, <span class="comment"># Step Size</span></span><br><span class="line">            <span class="string">'n_estimators'</span>: 1000, <span class="comment"># Number of trees, 트리 생성 개수</span></span><br><span class="line">            <span class="string">'objective'</span>: <span class="string">'regression'</span>&#125; <span class="comment"># 목적 함수 (L2 Loss)</span></span><br><span class="line">lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) <span class="comment"># 학습 진행</span></span><br><span class="line">lgb_model_predict = lgb_model.predict(test_x) <span class="comment"># 평가 데이터 예측</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"RMSE: &#123;&#125;"</span>.format(sqrt(mean_squared_error(lgb_model_predict, test_y)))) <span class="comment"># RMSE</span></span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RMSE: 212217.42594653403</span><br></pre></td></tr></table></figure><hr><h4 id="Shap-Value를-이용하여-변수-별-영향도-파악"><a href="#Shap-Value를-이용하여-변수-별-영향도-파악" class="headerlink" title="Shap Value를 이용하여 변수 별 영향도 파악"></a>Shap Value를 이용하여 변수 별 영향도 파악</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install shap (에러 발생시, skimage version 확인 (0.14.2 버젼보다 최신 버젼을 권장))</span></span><br><span class="line"><span class="comment"># import skimage -&gt; skimage.__version__ (skimage version 확인 방법)</span></span><br><span class="line"><span class="comment"># skimage version upgrade -&gt; !pip install --upgrade scikit-image</span></span><br><span class="line">import shap</span><br><span class="line">import skimage</span><br><span class="line"><span class="comment"># skimage.__version__</span></span><br><span class="line">explainer = shap.TreeExplainer(lgb_model) <span class="comment"># 트리 모델 Shap Value 계산 객체 지정</span></span><br><span class="line">shap_values = explainer.shap_values(test_x) <span class="comment"># Shap Values 계산</span></span><br></pre></td></tr></table></figure><hr><ul><li>해당 데이터의 변수 중 ‘yr_built=1986’와 ‘waterfront=0’이라는 것이 음의 영향을 미친다는 것을 확인 할 수 있다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shap.initjs() <span class="comment"># 자바스크립트 초기화 (그래프 초기화)</span></span><br><span class="line">shap.force_plot(explainer.expected_value, shap_values[0,:], test_x.iloc[0,:]) <span class="comment"># 첫 번째 검증 데이터 인스턴스에 대해 Shap Value를 적용하여 시각화</span></span><br><span class="line"><span class="comment"># 빨간색이 영향도가 높으며, 파란색이 영향도가 낮음</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/force_plot_first_data_only.png" alt="첫번째 관측치에 대한 shap value 그래프"></p><ul><li>전체 검증 데이터에 대한 그래프는 각 변수마다 볼 수 있다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap.force_plot(explainer.expected_value, shap_values, test_x) <span class="comment"># 전체 검증 데이터 셋에 대해서 적용</span></span><br></pre></td></tr></table></figure><hr><p><img src="/image/force_plot_full_data.png" alt="전체 데이터에 대한 shap value 그래프"></p><ul><li>floor 변수처럼 값이 섞여있는 경향이 크면 그만큼 반응변수에 미치는 영향이 거의 없다는 것으로 해석할 수 있다.<ul><li>grade : 변수의 값이 높을 수록, 예상 가격이 높은 경향성이 있다.</li><li>yr_built : 변수의 값이 낮을 수록(예전에 지어진 집일수록), 예상 가격이 높은 경향성이 있다.</li><li>bathrooms : 변수의 값이 높을 수록, 예상 가격이 높은 경향성이 있다.</li><li>bedrooms : 변수의 값이 높을 수록, 예상 가격이 높은 경향성이 있다.</li><li>condition : 변수의 값이 높을 수록, 예상 가격이 높은 경향성이 있다</li><li>waterfront : 변수의 값이 높을 수록, 예상 가격이 높은 경향성이 있다.</li><li>floors : 해석 모호성 (Feature Value에 따른 Shap Values의 상관성 파악 모호)</li><li>yr_renovated : 해석 모호성 (Feature Value에 따른 Shap Values의 상관성 파악 모호)</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap.summary_plot(shap_values, test_x)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/summary_plot_of_shap_value.png" alt="전체 변수에 대한 shap plot"></p><ul><li>각 변수에 대한 Shap Values의 절대값으로 중요도 파악</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap.summary_plot(shap_values, test_x, plot_type = <span class="string">"bar"</span>)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/summary_plot_of__absolute_shap_value.png" alt="Shap value의 절대값 그래프"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap.dependence_plot(<span class="string">"grade"</span>, shap_values, test_x)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/shap_dependence_plot_grade.png" alt="grade에 대한 Shap value와 의존적인 변수의 관계 그래프"></p><ul><li>확실히 완공시점이 낮을수록 집값이 높다는 사실을 확인할 수 있다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shap.dependence_plot(<span class="string">"yr_built"</span>, shap_values, test_x)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/shap_dependence_plot_yr_built.png" alt="yr_built에 대한 Shap value와 의존적인 변수의 관계 그래프"></p><h3 id="DS분야에서-Tree-기반-모델이-쓰이는-이유"><a href="#DS분야에서-Tree-기반-모델이-쓰이는-이유" class="headerlink" title="DS분야에서 Tree 기반 모델이 쓰이는 이유"></a>DS분야에서 Tree 기반 모델이 쓰이는 이유</h3><ul><li>NN은 각 층을 연결하는 노드의 가중치를 업데이트하면서 학습하기에 overffiting이 심하게 일어나고 학습시간이 상대적으로 머신러닝 알고리즘보다 오래 걸렸기 때문에 이전에는 머신러닝 기반의 알고리즘들이 훨씬 많이 사용되어졌다.</li></ul><p><img src="/image/why_based_tree_model_is_using_in_DS_part.png" alt="DS 분야에서 Tree 기반 모델이 쓰이는 이유 - 01"></p><ul><li>허나, 알고리즘 및 GPU의 발전이 이전의 NN보다 훨씬 깊은 층을 학습할 수 있도록 연산의 속도를 높였으며, overfitting을 완화시킬 수 있는 Dropout과 같은 기법들을 통해 보완하며 복잡한 데이터의 학습이 가능토록 하였다.</li></ul><p><img src="/image/why_based_tree_model_is_using_in_DS_part_01.png" alt="DS 분야에서 Tree 기반 모델이 쓰이는 이유 - 02"></p><ul><li>이미지 분류에서의 기존 머신러닝 모델은 각각의 픽셀 값을 독립변수로 사용하였는데, 이때 이 <code>피처들 사이의 관계를 독립이라고 가정한 채로 문제를 푸는 형식</code>이었다. 허나, 이미지의 픽셀값은 주변의 픽셀값과의 관계가 없는 것이 아니라는 점은 직관적으로 생각할 수 있다. <code>이렇게 각 변수들이 독립적인 가정을 이미지의 공간적인 특성을 잡아내는데 적합하지 않다.</code></li></ul><p><img src="/image/why_based_tree_model_is_using_in_DS_part_02.png" alt="DS 분야에서 Tree 기반 모델이 쓰이는 이유 - 03"></p><ul><li>그에 반해, <code>Deep Learning에서 CNN은 Convolution Layer를 통과해 feature map을 구성하며 이미지의 지역적인 특성부터 최종적으로 전체적인 이미지의 공간적인 특성을 잡아낼 수 있다.</code></li></ul><p><img src="/image/why_based_tree_model_is_using_in_DS_part_03.png" alt="DS 분야에서 Tree 기반 모델이 쓰이는 이유 - 04"></p><p><img src="/image/why_based_tree_model_is_using_in_DS_part_04.png" alt="DS 분야에서 Tree 기반 모델이 쓰이는 이유 - 05"></p><p><img src="/image/why_based_tree_model_is_using_in_DS_part_05.png" alt="DS 분야에서 Tree 기반 모델이 쓰이는 이유 - 06"></p><ul><li>이미지가 아닌 정형 데이터인 게임 User data를 통해 User의 이탈을 예측하는 문제를 푼다고 생각해보자. 각 변수들은 모두 연속적인 관계를 갖는가? 변수들간의 관계를 독립적이라고 가정해도 무방할 것이다. 물론 그 중 몇가지 Feature들은 연관성이 있겠지만 모든 Feature가 그렇진 않을 것이다.</li></ul><p><img src="/image/why_based_tree_model_is_using_in_DS_part_07.png" alt="DS 분야에서 Tree 기반 모델이 쓰이는 이유 - 07"></p><blockquote><p>최근 10년간 Deep Learning의 부흥기라고 해도 과언이 아닐 것이다. 그럼에도 불구 하고 Kaggle 대회를 비롯한 각종 머신러닝 대회에서 boosting 계열 알고리즘이 우승하는 가장 큰 이유는 대회의 데이터의 성질 때문이라고 생각한다. 물론 필자의 개인적인 생각이지만 머신러닝 알고리즘은 위에서 언급한 것과 같이 개별적인 피처들간의 관계를 독립이라고 가정하고 푸는 알고리즘들이 대부분이다. 예를 들어 회귀분석에서는 최대한 다중공선성을 제거하기 위해 전처리를 하는 것만 생각해봐도 알 수 있을 것이다. 이 처럼 피처간의 관계가 독립이라고 가정하고 문제를 풀어도 무관하거나 실제로 독립적인 피처를 갖춘 데이터로 문제를 풀 경우는 Deep Learning 보단 Machine Learning 알고리즘이 더 강세라고 생각한다.</p><p>또한 Machine Learning과 Deep Learning의 가장 큰 차이점은 Machine Learning의 경우 사람이 입력해주는 Feature를 잘 처리해주어야 기본적으로 잘 작돌하고, Deep Learning의 경우 입력을 넣어주면 Layer를 통과하면서 입력 데이터의 특징을 잘 잡아 마지막에 잡아낸 Feature를 Fully connected layer(NN)을 통해 학습하는 방식이므로 Feature에 대한 사람의 노력이 상대적으로 덜 소요될 순 있을 것이다.</p></blockquote><p><img src="/image/why_based_tree_model_is_using_in_DS_part_06.png" alt="DS 분야에서 Tree 기반 모델이 쓰이는 이유 - 08"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/27/machine_learning_16/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Ensemble Learning - Boosting, Stacking</title>
      <link>https://heung-bae-lee.github.io/2020/05/27/machine_learning_15/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/27/machine_learning_15/</guid>
      <pubDate>Tue, 26 May 2020 17:00:49 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Boosting&quot;&gt;&lt;a href=&quot;#Boosting&quot; class=&quot;headerlink&quot; title=&quot;Boosting&quot;&gt;&lt;/a&gt;Boosting&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;앞에서 언급했던 Bagging이나 Random Forests는 부트스트랩
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><ul><li>앞에서 언급했던 Bagging이나 Random Forests는 부트스트랩 방식으로 데이터를 뽑긴해도 각 모델에 대해 독립적이라고 가정하지만, <code>Boosting은 resampling을 할 때 오분류된 데이터에 더 가중치를 주어서 오분류된 데이터가 뽑힐 확률이 높도록 하여 복원 추출을 하고 다시 학습하기 때문에 모델들이 Sequential한 것이다.</code></li></ul><p><img src="/image/Boosting_conception.png" alt="Boosting 개념"></p><ul><li>부스트(boost) 방법은 미리 정해진 갯수의 모형 집합을 사용하는 것이 아니라 하나의 모형에서 시작하여 모형 집합에 포함할 개별 모형을 하나씩 추가한다. 모형의 집합은 위원회(commit) $C$라고 하고 $m$개의 모형을 포함하는 위원회를 $C_{m}$으로 표시한다. 위원회에 들어가는 개별 모형을 약 분류기(weak classifier)라고 하며 $k$로 표시한다.</li></ul><ul><li><code>부스트 방법의 특징은 한번에 하나씩 모형을 추가한다는 것</code>이다.</li></ul><script type="math/tex; mode=display">C_1 = \{ k_1 \}</script><script type="math/tex; mode=display">C_2 = C_1 \cup k_2 = \{ k_1, k_2 \}</script><script type="math/tex; mode=display">C_3 = C_2 \cup k_3 = \{ k_1, k_2, k_3 \}</script><script type="math/tex; mode=display">\vdots</script><script type="math/tex; mode=display">C_m = C_{m-1} \cup k_m = \{ k_1, k_2, \ldots, k_m \}</script><ul><li>그리고 m번째로 위원회에 추가할 개별 모형 $k_{m}$의 선택 기준은 그 전단계의 위원회 $C_{m-1}$의 성능을 보완하는 것이다. <code>위원회 $C_{m}$의 최종 결정은 다수결 방법을 사용하지 않고 각각의 개별 모형의 출력을 가중치 $\alpha$로 가중 선형조합한 값을 판별함수로 사용</code>한다. 또한 부스트 방법은 이진 분류에만 사용할 수 있으며 $y$값은 1 또는 -1의 값을 가진다.</li></ul><script type="math/tex; mode=display">y = -1 \text{ or } 1</script><script type="math/tex; mode=display">C_{m}(x_i) =  \text{sign} \left( \alpha_1k_1(x_i) + \cdots + \alpha_{m}k_{m}(x_i) \right)</script><h2 id="AdaBoost-에이다부스트"><a href="#AdaBoost-에이다부스트" class="headerlink" title="AdaBoost(에이다부스트)"></a>AdaBoost(에이다부스트)</h2><p><img src="/image/Adaboosting_conception.png" alt="AdaBoost 개념"></p><ul><li>에이다 부스트(adaboost)라는 이름은 적응 부스트(adaptive boost)라는 용어에서 나왔다. 에이다부스트는 위원회에 넣을 개별 모형 $k_{m}$을 선별하는 방법으로학습데이터 집합의 $i$번째 데이터에 가중치 $w_{i}$를 주고 분류 모형이 틀리게 예측한 데이터의 가중치를 합한 값을 손실함수 $L$로 사용한다. 이 손실함수를 최소화하는 모형이 k_{m}으로 선택된다.</li></ul><script type="math/tex; mode=display">L_m = \sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)</script><ul><li>위 식에서 $I$는 $k(x_{i}) \neq y_{i}$라는 조건이 만족되면 1, 아니면 0을 갖는 indicator function이다. 즉 예측을 틀리게한 데이터들에 대한 가중치의 합이다. 위원회 $C_{m}$에 포함될 개별 모형 $k_{m}$이 선택된 후에는 가중치 $\alpha_{m}$을 결정해야 한다. 이 값은 다음처럼 계산한다.</li></ul><script type="math/tex; mode=display">\epsilon_m = \dfrac{\sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)}{\sum_{i=1}^N w_{m,i}}</script><script type="math/tex; mode=display">\alpha_m = \frac{1}{2}\log\left( \frac{1 - \epsilon_m}{\epsilon_m}\right)</script><ul><li>데이터에 대한 가중치 $w_{m, i}$는 최초에는$(m=1)$ 모든 데이터에 대해 동일한 값을 갖지만, 위원회가 증가하면서 값이 바뀐다. 가중치의 값은 지시함수를 사용하여 위원회 $C_{m-1}$이 맞춘 문제는 작게, 틀린 문제는 크게 확대(boosting)된다.</li></ul><script type="math/tex; mode=display">w_{m,i} = w_{m-1,i}  \exp (-y_iC_{m-1}) = \begin{cases} w_{m-1,i}e^{-1}  & \text{ if } C_{m-1} = y_i\\ w_{m-1,i}e & \text{ if } C_{m-1} \neq y_i \end{cases}</script><ul><li>$m$번째 멤버의 모든 후보에 대해 위 손실함수를 적용하여 가장 값이 작은 후보를 $m$번째 멤버로 선정한다.</li></ul><ul><li>에이다 부스팅은 사실 다음과 같은 손실함수를 최소화하는 $C_{m}$을 찾아가는 방법이라는 것을 증명할 수 있다.</li></ul><script type="math/tex; mode=display">L_m = \sum_{i=1}^N \exp(−y_i C_m(x_i))</script><ul><li>개별 멤버 $k_{m}$과 위원회 관계는</li></ul><script type="math/tex; mode=display">C_m(x_i) = \sum_{j=1}^m \alpha_j k_j(x_i) = C_{m-1}(x_i) + \alpha_m k_m(x_i)</script><ul><li>이고 이 식을 대입하면</li></ul><script type="math/tex; mode=display">\begin{eqnarray} L_m &=& \sum_{i=1}^N \exp(−y_i C_m(x_i)) \\ &=& \sum_{i=1}^N \exp\left(−y_iC_{m-1}(x_i) - \alpha_m y_i k_m(x_i) \right) \\ &=& \sum_{i=1}^N \exp(−y_iC_{m-1}(x_i)) \exp\left(-\alpha_m y_i k_m(x_i)\right) \\ &=& \sum_{i=1}^N w_{m,i} \exp\left(-\alpha_m y_i k_m(x_i)\right) \\ \end{eqnarray}</script><ul><li>$y_{i}$와 $k_{M}(x_{i})$ 1 또는 -1값만 가질 수 있다는 점을 이용하면,</li></ul><script type="math/tex; mode=display">\begin{eqnarray} L_m &=& e^{-\alpha_m}\sum_{k_m(x_i) = y_i} w_{m,i} + e^{\alpha_m}\sum_{k_m(x_i) \neq y_i} w_{m,i} \\ &=& \left(e^{\alpha_m}-e^{-\alpha_m}\right) \sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right) + e^{-\alpha_m}\sum_{i=1}^N w_{m,i} \end{eqnarray}</script><ul><li>$L_{m}$을 최소화하려면 $\sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)$을 최소화하는 $k_{m}$ 함수를 찾은 다음 $L_{m}$을 최소화하는 $\alpha_{m}$을 찾아야 한다.</li></ul><script type="math/tex; mode=display">\dfrac{d L_m}{d \alpha_m} = 0</script><ul><li>이 조건으로부터 $\alpha_{m}$ 공식을 유도할 수 있다.</li></ul><p><img src="/image/cost_function_of_adaboost.png" alt="Adaboost 비용함수"></p><ul><li>다음은 Scikit-Learn의 ensemble 서브패키지가 제공하는 <code>AdaBoostClassifier</code> 클래스를 사용하여 분류 예측을 하는 예이다. 약분류기로는 깊이가 1인 단순한 의사결정나무를 채택하였다. 여기에서는 각 표본 데이터의 가중치 값을 알아보기 위해 기존의 <code>AdaBoostClassifier</code> 클래스를 서브 클래싱하여 가중치를 속성으로 저장하도록 수정한 모형을 사용하였다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_gaussian_quantiles</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.ensemble import AdaBoostClassifier</span><br><span class="line"></span><br><span class="line">X1, y1 = make_gaussian_quantiles(cov=2.,</span><br><span class="line">                                 n_samples=100, n_features=2,</span><br><span class="line">                                 n_classes=2, random_state=1)</span><br><span class="line">X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,</span><br><span class="line">                                 n_samples=200, n_features=2,</span><br><span class="line">                                 n_classes=2, random_state=1)</span><br><span class="line">X = np.concatenate((X1, X2))</span><br><span class="line">y = np.concatenate((y1, - y2 + 1))</span><br><span class="line"></span><br><span class="line">class MyAdaBoostClassifier(AdaBoostClassifier):</span><br><span class="line"></span><br><span class="line">    def __init__(self,</span><br><span class="line">                 base_estimator=None,</span><br><span class="line">                 n_estimators=50,</span><br><span class="line">                 learning_rate=1.,</span><br><span class="line">                 algorithm=<span class="string">'SAMME.R'</span>,</span><br><span class="line">                 random_state=None):</span><br><span class="line"></span><br><span class="line">        super(MyAdaBoostClassifier, self).__init__(</span><br><span class="line">            base_estimator=base_estimator,</span><br><span class="line">            n_estimators=n_estimators,</span><br><span class="line">            learning_rate=learning_rate,</span><br><span class="line">            random_state=random_state)</span><br><span class="line">        self.sample_weight = [None] * n_estimators</span><br><span class="line"></span><br><span class="line">    def _boost(self, iboost, X, y, sample_weight, random_state):</span><br><span class="line">        sample_weight, estimator_weight, estimator_error = \</span><br><span class="line">        super(MyAdaBoostClassifier, self)._boost(iboost, X, y, sample_weight, random_state)</span><br><span class="line">        self.sample_weight[iboost] = sample_weight.copy()</span><br><span class="line">        <span class="built_in">return</span> sample_weight, estimator_weight, estimator_error</span><br><span class="line"></span><br><span class="line">model_ada = MyAdaBoostClassifier(DecisionTreeClassifier(max_depth=1, random_state=0), n_estimators=20)</span><br><span class="line">model_ada.fit(X, y)</span><br><span class="line"></span><br><span class="line">def plot_result(model, title=<span class="string">"분류결과"</span>, legend=False, s=50):</span><br><span class="line">    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02), np.arange(x2_min, x2_max, 0.02))</span><br><span class="line">    <span class="keyword">if</span> isinstance(model, list):</span><br><span class="line">        Y = model[0].predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(model) - 1):</span><br><span class="line">            Y += model[i + 1].predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Y = model.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)</span><br><span class="line">    cs = plt.contourf(xx1, xx2, Y, cmap=plt.cm.Paired, alpha=0.5)</span><br><span class="line">    <span class="keyword">for</span> i, n, c <span class="keyword">in</span> zip(range(2), <span class="string">"01"</span>, <span class="string">"br"</span>):</span><br><span class="line">        idx = np.where(y == i)</span><br><span class="line">        plt.scatter(X[idx, 0], X[idx, 1], c=c, s=s, alpha=0.5, label=<span class="string">"Class %s"</span> % n)</span><br><span class="line">    plt.xlim(x1_min, x1_max)</span><br><span class="line">    plt.ylim(x2_min, x2_max)</span><br><span class="line">    plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'x2'</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.colorbar(cs)</span><br><span class="line">    <span class="keyword">if</span> legend:</span><br><span class="line">        plt.legend()</span><br><span class="line">    plt.grid(False)</span><br><span class="line"></span><br><span class="line">plot_result(model_ada, <span class="string">"에이다부스트(m=20) 분류 결과"</span>)</span><br></pre></td></tr></table></figure><p><img src="/image/Adaboost_m_20_result.png" alt="Adaboost 20번째 모델까지의 결과"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(10, 15))</span><br><span class="line">plt.subplot(421);</span><br><span class="line">plot_result(model_ada.estimators_[0], <span class="string">"1번 분류모형의 분류 결과"</span>, s=10)</span><br><span class="line">plt.subplot(422);</span><br><span class="line">plot_result(model_ada.estimators_[1], <span class="string">"2번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[0]).astype(int))</span><br><span class="line">plt.subplot(423);</span><br><span class="line">plot_result(model_ada.estimators_[2], <span class="string">"3번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[1]).astype(int))</span><br><span class="line">plt.subplot(424);</span><br><span class="line">plot_result(model_ada.estimators_[3], <span class="string">"4번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[2]).astype(int))</span><br><span class="line">plt.subplot(425);</span><br><span class="line">plot_result(model_ada.estimators_[4], <span class="string">"5번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[3]).astype(int))</span><br><span class="line">plt.subplot(426);</span><br><span class="line">plot_result(model_ada.estimators_[5], <span class="string">"6번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[4]).astype(int))</span><br><span class="line">plt.subplot(427);</span><br><span class="line">plot_result(model_ada.estimators_[6], <span class="string">"7번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[5]).astype(int))</span><br><span class="line">plt.subplot(428);</span><br><span class="line">plot_result(model_ada.estimators_[7], <span class="string">"8번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[6]).astype(int))</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="/image/Adaboost_m_20_result_01.png" alt="Adaboost 모델의 가중치의 변화에 따른 decision boundary의 변화 - 01"></p><p><img src="/image/Adaboost_m_20_result_02.png" alt="Adaboost 모델의 가중치의 변화에 따른 decision boundary의 변화 - 02"></p><h2 id="Adaboost-모형의-정규화"><a href="#Adaboost-모형의-정규화" class="headerlink" title="Adaboost 모형의 정규화"></a>Adaboost 모형의 정규화</h2><ul><li>Adaboost 모형이 과최적화(overfitting)가 되는 경우에는 학습 속도(learning rate)를 조정하여 정규화를 할 수 있다. 이는 <code>필요한 멤버의 수를 강제로 증가시켜서 과최적화를 막는 역할을 한다.</code> 즉, 새롭게 적용되는 모형에 대한 가중치를 줄여서 동일한 모형의 횟수를 거치더라도 가중치가 크게 영향을 받지 않도록 하여 과최적화를 없애는 방법이다.</li></ul><script type="math/tex; mode=display">C_m = C_{m-1} + \mu \alpha_m k_m</script><ul><li><code>AdaBoostClassifier</code> 클래스에서는  <code>learning_rate</code>인수를 1보다 적게 주면 새로운 멤버의 가중치를 강제로 낮춘다.</li></ul><h2 id="그레디언트-부스팅-Gradient-boosting"><a href="#그레디언트-부스팅-Gradient-boosting" class="headerlink" title="그레디언트 부스팅 (Gradient boosting)"></a>그레디언트 부스팅 (Gradient boosting)</h2><ul><li>기본적으로 부스팅은 다음 Round에서 이전에 잘못 예측한 데이터들에 대한 처리를 어떻게 하느냐에 따라 종류별로 차이가 존재한다. <code>Gradient Boosting은 이전 Round의 분류기로 예측한 error를 다음 Round의 분류기가 예측할 수 있도록 학습하면서 진행</code>한다.</li></ul><p><img src="/image/what_is_gradient_boosting.png" alt="그레디언트 부스팅의 개념"></p><ul><li>이전 모델의 error를 다음 모델이 예측할 수 있게끔 학습시켜 해당 분류기들의 학습된 결과를 계속해서 합해 나가면 마지막에는 최소한의 error만 남으므로, error를 최대한 줄일 수 있게 된다.</li></ul><p><img src="/image/gradient_boosting_method_principal.png" alt="그레디언트 부스팅의 원리"></p><ul><li>위에서 언급했던 것과 같이 error를 예측하게 하므로 이해하기 쉽게 regression을 통한 예시로 설명하겠다. 처음 모델의 error를 다음 모델은 예측하도록 학습하므로 이전 모델보다 오차가 더 줄어들 것이다. 그 다음 모델도 이전 모델의 오차를 학습하게 되므로 더 오차가 줄어들 것이다. 이렇게 최종적으로는 error가 최대한 0에 가까워질 때 까지 학습하여 train set에 대해서는 과최적화가 이루어 질 것이다.</li></ul><p><img src="/image/gradient_boosting_steps.png" alt="그레디언트 부스팅의 이해"></p><ul><li>최종적으로는 학습 데이터에 대한 error를 작게 하는 것이므로 아래 그림에서와 같이 negative gradient를 최소화시키면서 학습 될 것이다.</li></ul><p><img src="/image/cost_function_with_gradient_boosting.png" alt="그레디언트 부스팅의 cost function"></p><ul><li>위의 그림에서 볼 수 있듯이 <code>그레디언트 부스트 모형은 변분법(calculus of variations)을 사용한 모형</code>이다. 학습 $f(x)$를 최소화하는 $x$는 다음과 같이 gradient descent 방법으로 찾을 수 있다.</li></ul><script type="math/tex; mode=display">x_{m} = x_{m-1} - \alpha_m \dfrac{df}{dx}</script><ul><li>그레디언트 부스트 모형에서는 손실 범함수(loss functional) $L(y, C_{m-1})$을 최소화하는 개별 분류함수 $k_{m}$를 찾는다. 이론적으로 가장 최적의 함수는 범함수의 미분이다.</li></ul><script type="math/tex; mode=display">C_{m} = C_{m-1} - \alpha_m \dfrac{\delta L(y, C_{m-1})}{\delta C_{m-1}} = C_{m-1} + \alpha_m k_m</script><ul><li><code>따라서 그레디언트 부스트 모형은 분류/회귀 문제에 상관없이 개별 멤버 모형으로 회귀분석 모형을 사용</code>한다. 가장 많이 사용되는 회귀분석 모형은 의사결정 회귀나무(decision tree regression model)모형이다.</li></ul><ul><li><p>그레디언트 부스트 모형에서는 다음과 같은 과정을 반복하여 멤버와 그 가중치를 계산한다.</p><ul><li><ol><li>$-\tfrac{\delta L(y, C_m)}{\delta C_m}$를 목표값으로 개별 멤버 모형 $k_{m}$을 찾는다.</li></ol></li><li><ol><li>$ \left( y - (C_{m-1} + \alpha_m k_m) \right)^2 $ 를 최소화하는 스텝사이즈 $\alpha_{m}$을 찾는다.</li></ol></li><li><ol><li>$ C_m = C_{m-1} + \alpha_m k_m $ 최종 모형을 갱신한다.</li></ol></li></ul></li><li><p>만약 손실 범함수가 오차 제곱 형태라면</p></li></ul><script type="math/tex; mode=display">L(y, C_{m-1}) = \dfrac{1}{2}(y - C_{m-1})^2</script><ul><li>범함수의 미분은 실제 목표값 $y$와 $C_{m-1}$과의 차이 즉, 잔차(residual)가 된다.</li></ul><script type="math/tex; mode=display">-\dfrac{dL(y, C_m)}{dC_m} = y - C_{m-1}</script><ul><li><p>Scikit-Learn의 GradientBoostingClassifier는 약한 학습기의 순차적인 예측 오류 보정을 통해 학습을 수행하므로 멀티 CPU 코어 시스템을 사용하더라도 병렬처리가 지원되지 않아서 대용량 데이터의 경우 학습에 매우 많은 시간이 필요하다. 또한 일반적으로 GBM이 랜덤 포레스트보다는 예측 성능이 조금 뛰어난 경우가 많다. 그러나 수행시간이 오래 걸리고, 하이퍼 파라미터 튜닝 노력도 더 필요하다.</p></li><li><p><code>loss</code>: 경사 하강법에서 사용할 비용 함수를 저장한다. 특별한 이유가 없으면 default인 ‘deviance’를 그대로 적용한다.</p></li></ul><ul><li><code>learning_rate</code>: GBM이 학습을 진행할 때마다 적용하는 학습률이다. Weak learner가 순차적으로 오류 값을 보정해 나가는 데 적용하는 계수이다. 0~1 사이의 값을 지정할 수 있으며 default=0.1이다. 너무 작은 값을 적용하면 업데이트 되는 값이 작아져서 최소 오류 값을 찾아 예측 성능이 높아질 가능성이 높다. 하지만 많은 weak learner는 순차적인 반복이 필요해서 수행 시간이 오래 걸리고, 또 너무 작게 설정하면 모든 weak learner의 반복이 완료돼도 최소 오류 값을 찾지 못할 수 있다. 반대로 큰 값을 적용하면 최소 오류 값을 찾지 못하고 그냥 지나챠 버려 예측 성능이 떨어질 가능성이 높아지지만, 빠른 수행이 가능하다. <code>이러한 특성 때문에 learning_rate는 n_estimators와 상호 보완적으로 조합해 사용한다. learning_rate를 작게하고 n_estimators를 크게 하면 더 이상 성능이 좋아지지 않는 한계점까지는 예측 성능이 조금씩 좋아질 수 있다.</code></li></ul><ul><li><code>subsample</code>: weak learner가 학습에 사용하는 데이터의 샘플링 비율이다. default=1이며, 이는 전체 학습 데이터를 기반으로 학습한다는 의미이다.(0.5이면 학습데이터의 50%를 의미) 과적합이 염려되는 경우 subsample을 1보다 작은 값으로 설정한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line">model_grad = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=0)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">model_grad.fit(X, y)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CPU <span class="built_in">times</span>: user 50 ms, sys: 0 ns, total: 50 ms</span><br><span class="line">Wall time: 50.4 ms</span><br><span class="line"></span><br><span class="line">GradientBoostingClassifier(criterion=<span class="string">'friedman_mse'</span>, init=None,</span><br><span class="line">                           learning_rate=0.1, loss=<span class="string">'deviance'</span>, max_depth=2,</span><br><span class="line">                           max_features=None, max_leaf_nodes=None,</span><br><span class="line">                           min_impurity_decrease=0.0, min_impurity_split=None,</span><br><span class="line">                           min_samples_leaf=1, min_samples_split=2,</span><br><span class="line">                           min_weight_fraction_leaf=0.0, n_estimators=100,</span><br><span class="line">                           n_iter_no_change=None, presort=<span class="string">'auto'</span>,</span><br><span class="line">                           random_state=0, subsample=1.0, tol=0.0001,</span><br><span class="line">                           validation_fraction=0.1, verbose=0,</span><br><span class="line">                           warm_start=False)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_result(model_grad)</span><br></pre></td></tr></table></figure><p><img src="/image/result_of_gradient_boost_plot_decision_boundary.png" alt="그레디언트 부스트의 decision boundary"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(121)</span><br><span class="line">plot_result(model_grad.estimators_[3][0])</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_result([model_grad.estimators_[0][0],</span><br><span class="line">             model_grad.estimators_[1][0],</span><br><span class="line">             model_grad.estimators_[2][0],</span><br><span class="line">             model_grad.estimators_[3][0]])</span><br></pre></td></tr></table></figure><p><img src="/image/result_of_gradient_boost_plot_decision_boundary_01.png" alt="그레디언트 부스트에 사용된 모형들의 4번째 까지의 각각의 decision decision boundary"></p><h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><p><img src="/image/XGBoost_conception.png" alt="XGBoost 개념"></p><ul><li>XGboost는 GBM에 기반하고 있지만, GBM의 단점인 느린 수행 시간 및 과적합 규제(Regularization) 부재 등의 문제를 해결해서 매우 각광을 받고 있다. 특히 XGBoost는 병렬 CPU 환경에서 병렬 학습이 가능해 기존 GBM보다 빠르게 학습을 완료할 수 있다. 다음은 XGboost의 장점이다.</li></ul><div class="table-container"><table><thead><tr><th>항목</th><th>설명</th></tr></thead><tbody><tr><td>뛰어난 예측성능</td><td>일반적으로 분류와 회귀 영역에서 뛰어난 예측 성능을 발휘한다.</td></tr><tr><td>GBM 대비 빠른 수행 시간</td><td>일반적인 GBM은 순차적으로 Weak Learner가 가중치를 증감하는 방법으로 학습하기 때문에 전반적으로 속도가 느리다. 하지만 XGBoost는 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행 성능을 보장한다. 아쉽게도 XGBoost가 일반적인 GBM에 비해 수행 시간이 빠르다는 것이지, 다른 머신러닝 알고리즘 (예를 들어 랜덤 포레스트)에 비해서 빠르다는 의미는 아니다.</td></tr><tr><td>과적합 규제 (Regularization)</td><td>표준 GBM의 경우 과적합 규제 기능이 없으나 XGBoost는 자체에 과적합 규제 기능으로 과적합에 좀 더 강한 내구성을 가질 수 있다.</td></tr><tr><td>Tree pruning (나무 가지치기)</td><td>일반적으로 GBM은 분할 시 부정 손실이 발생하면 분할을 더 이상 수행하지 않지만, 이러한 방식도 자칫 지나치게 많은 분할을 발생할 수 있다. 다른 GBM과 마찬가지로 XGBoost도 max_depth 파라미터로  분할 깊이를 조정하기도 하지만, tree pruning으로 더 이상 긍정 이득이 없는 분할을 가지치기 해서 분할 수를 더 줄이는 추가적인 장점을 가지고 있다.</td></tr><tr><td>자체 내장된 교차 검증</td><td>XGBoost는 반복 수행 시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차 검증을 수행해 최적화된 반복 수행 횟수를 가질 수 있다. 지정된 반복 횟수가 아니라 교차 검증을 통해 평가 데이터 세트의 평가값이 최적화 되면 반복을 중간에 멈출 수 있는 조기 중단 기능이 있다.</td></tr><tr><td>결손값 자체 처리</td><td>XGBoost는 결손값을 자체 처리할 수 있는 기능을 가지고 있다.</td></tr></tbody></table></div><p><img src="/image/XGboost_better_than_gbm.png" alt="XGBoost의 장점"></p><ul><li>XGBoost의 핵심 라이브러리는 C/C++로 작성돼 있다. XGBoost 개발 그룹은 파이썬에서도 XGBoost를 구동할 수 있도록 파이썬 패키지를 제공한다. 이 파이썬 패키지의 역할은 대부분 C/C++ 핵심 라이브러리를 호출하는 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># window용</span></span><br><span class="line"><span class="comment"># conda install -c anaconda py-xgboost</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Linux용</span></span><br><span class="line">conda install -c conda-forge xgboost</span><br></pre></td></tr></table></figure><ul><li><p><code>python 래퍼 모듈과 Scikit-Learn 래퍼 XGBoost 모듈의 일부 hyper-parameter는 약간 다르므로 이에 대한 주의가 필요</code>하다.</p></li><li><p>python 래퍼 XGBoost 모듈</p><ul><li><p>XGboost 고유의 프레임워크를 python 언어 기반에서 구현한 것으로 별도의 API기반을 갖고 있어 Scikit-Learn 프레임워크를 기반으로 한 것이 아니기에 Scikit-Learn의 fit(), predict() 메서드 같은 Scikit-Learn 고유의 아키텍처와 다른 다양한 유틸리티(cross_val_score, GridSearchCV, Pipeline 등)와 함께 사용될 수 없다.</p></li><li><p>일반 parameter</p><ul><li>일반적으로 실행 시 thread의 개수나 silent 모드 등의 선택을 위한 parameter로서 default parameter 값을 바꾸는 경우는 거의 없다.</li><li><code>booster</code> : gbtree(tree based model) 또는 gblinear(linear model)선택 default=gbtree</li><li><code>silent</code> : default=0이며, 출력 메시지를 나타내고 싶지 않을 경우 1로 설정한다.</li><li><code>nthread</code> : CPU의 실행 thread 개수를 조정하며, <code>default는 CPU의 전체 thread를 다 사용하는 것</code>이다. Multi Core/thread CPU 시스템에서 전체 CPU를 사용하지 않고 일부 CPU만 사용해 ML 애플리케이션을 구동하는 경우에 변경한다.</li></ul></li><li><p>Booster parameter</p><ul><li><p>tree 최적화, Boosting, Regularization 등과 관련 parameter 등을 지칭한다.</p></li><li><p><code>eta [default=0.3, alias:learning_rate]</code> : GBM의 학습률(learning rate)과 같은 parameter이다. 0~1 사이의 값을 지정하며 Boosting step을 반복적으로 수행할 때 업데이트되는 학습률 값. python 래퍼 기반의 xgboost를 이용할 경우 default=0.3 scikit-learn 래퍼를 이용할 경우 eta는 learning_rate로 대체되며, default=0.1이다. <code>보통은 0.01~0.2 사이의 값을 선호</code>한다.</p></li><li><p><code>num_boost_rounds</code> : GBM의 n_estimators와 같은 parameter이다.</p></li><li><code>min_child_weight[default=1]</code> : GBM의 min_child_leaf와 유사함(똑같지는 않음). 과적합을 조절하기 위해 사용된다.</li><li><code>gamma [default=0, alias: min_split_loss]</code> : tree의 leaf 노드를 추가적으로 나눌지를 결정할 최소 손실 감소 값이다. <code>해당 값보다 큰 손실(loss)이 감소된 경우에 leaf 노드를 분리</code>한다. 값이 클수록 과적합 감소 효과가 있다.</li><li><code>max_depth [default=6]</code> : tree 기반 알고리즘의 max_depth와 같다. 0을 지정하면 깊이에 제한이 없다. Max_depth가 높으면 특정 feature 조건에 특화되어 룰 조건이 만들어지므로 과적합 가능성이 높아지며 <code>보통은 3~10</code>사이의 값을 적용한다.</li><li><code>sub_sample [default=1]</code> : GBM의 subsample과 동일하다. tree가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율을 지정한다. sub_sample=0.5로 지정하면 전체 데이터의 절반을 tree를 생성하는 데 사용한다. <code>0에서 1사이의 값이 가능하나 일반적으로 0.5~1사이의 값을 사용한다.</code></li><li><code>colsample_bytree [default=1]</code> : GBM의 max_features와 유사하다. tree 생성에 필요한 feature(column)를 임의로 샘플링 하는 데 사용된다. 매우 많은 feature가 있는 경우 과적합을 조정하는 데 적용한다.</li><li><code>lambda [default=1, alias:reg_lambda]</code> : <code>L2 Regularization 적용 값</code>이다. feature 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있다.</li><li><code>alpha</code> : <code>L1 Regularization 적용값</code>이다. feature 개수가 많을 경우 적용을 검토하며 값이 클수록 감소 효과가 있다.</li><li><code>scale_pos_weight [default=1]</code> : 특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지하기 위한 paramter이다.</li></ul></li><li><p>학습 task parameter</p><ul><li>학습 수행 시의 객체 함수, 평가를 위한 지표 등을 설정하는 parameter이다.</li><li><code>objective</code> : 최솟값을 가져야할 손실 함수(loss function)을 정의한다. XGBoost는 많은 유형의 손실함수를 사용할 수 있다. 주로 사용되는 손실함수는 이진 분류인지 다중 분류인지에 따라 달라진다.<ul><li><code>binary:logistic</code> : 이진 분류일 때 적용한다.</li><li><code>multi:softmax</code> : 다중 분류일 때 적용한다. 손실 함수가 multi:softmax 일 경우에는 label class의 개수인 num_class parameter를 지정해야 한다.</li><li><code>multi:softprob</code> : multi:softmax와 유사하나 개별 label class의 해당되는 예측 확률을 반환한다.</li></ul></li><li><code>eval_metric</code> : 검증에 사용되는 함수를 정의한다. default는 회귀인 경우 rmse, 분류인 경우 error이다. 다음은 eval_metric의 값 유형이다.<ul><li>rmse : Root Mean Square Error</li><li>mae : Mean Absolute Error</li><li>logloss : Negative log-likelihood</li><li>error : Binary classification error rate (0.5 threshold)</li><li>merror : Multiclass classification error rate</li><li>mlogloss : Multiclass logloss</li><li>auc : Area under the curve</li></ul></li></ul></li><li><p><code>대부분의 hyper parameter는 Booster paramter에 속한다.</code></p></li></ul></li><li><p>Scikit-Learn 래퍼 XGBoost 모듈</p><ul><li>XGboost 패키지의 Scikit-Learn 래퍼 클래스는 <code>XGBClassifier</code>, <code>XGBRegressor</code>이다. 이를 이용하면 Scikit-Learn estimator가 학습을 위해 사용하는 fit(), predict() 와 같은 표준 Scikit-Learn 개발 프로세스 및 다양한 유틸리티를 활용할 수 있다.</li></ul></li><li><p>과적합(overfitting) 문제가 심각하다면 다음과 같이 적용할 것을 고려할 수 있다.</p><ul><li>eta 값을 낮춘다.(0.01~0.1)<ul><li>eta 값을 낮출 경우 num_round(또는 n_estimators)는 반대로 높여줘야 한다.</li></ul></li><li>max_depth 값을 낮춘다.</li><li>min_child_weight 값을 높인다.</li><li>gamma 값을 높인다.</li><li>또한 subsample과 colsample_bytree를 조정하는 것도 tree가 너무 복잡하게 생성되는 것을 막아 과적합 문제에 도움이 될 수 있다.</li></ul></li><li><p>XGBoost 자체적으로 교차 검증, 성능 평가, feature 중요도 등의 시각화 기능을 가지고 있다. 또한 XGBoost는 기본 GBM에서 부족한 다른 여러 가지 성능 향상 기능이 있다. 그 중에 수행 속도를 향상시키기 위한 대표적인 기능으로 <code>Early Stopping</code> 기능이 있다. <code>기본 GBM의 경우 지정된 횟수를 다 완료해야 한다. 허나, XGBoost와 LightGBM은 모두 early Stopping 기능이 있어서 n_estimators에 지정한 Boosting 반복 횟수에 도달하지 않더라도 예측 오류가 더 이상 개선되지 않으면 반복을 끝까지 수행하지 않고 중지해 수행 시간을 개선 할 수 있다.</code></p></li><li><p>예를 들어 n_estimators=200, early Stopping 파라미터 값을 50으로 설정하면, 1부터 200회까지 Boosting을 반복하다가 50회를 반복하는 동안 학습 오류가 감소하지 않으면 더 이상 Boosting을 진행하지 않고 종료한다.(가령 100회에서 학습 오류 값이 0.8인데, 101회~150회 반복하는 동안 예측 오류가 0.8보다 작은 값이 하나도 없으면 Boosting을 종료한다.)</p></li><li><p>아래는 python 래퍼의 Xgboost 사용법을 간단히 정리해 놓은 것이다. <code>일반적으로 XGBoost는 GBM과는 다르게 병렬처리와 early Stopping 등으로 빠른 수행시간 처리가 가능하지만, CPU 코어가 많지 않은 개인용 PC에서는 수행시간 향상을 경험하기 어려울 수도 있다.</code></p></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import xgboost as xgb</span><br><span class="line">from xgboost import plot_importance</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">'ignore'</span>)</span><br><span class="line"></span><br><span class="line">dataset = load_breast_cancer()</span><br><span class="line">X_features = dataset.data</span><br><span class="line">y_label = dataset.target</span><br><span class="line"></span><br><span class="line">cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names)</span><br><span class="line">cancer_df[<span class="string">'target'</span>] = y_label</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(dataset.target_names)</span><br><span class="line"><span class="built_in">print</span>(cancer_df[<span class="string">'target'</span>].value_counts())</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'malignant'</span>, <span class="string">'benign'</span>]</span><br><span class="line">1    357</span><br><span class="line">0    212</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=156)</span><br><span class="line"><span class="built_in">print</span>(X_train.shape, X_test.shape)</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(455, 30) (114, 30)</span><br></pre></td></tr></table></figure><hr><ul><li><p>python 래퍼 XGboost가 Scikit-Learn 래퍼 XGboost와 차이점은 여러가지가 있지만, 가장 큰 차이는 학습용 데이터와 테스트용 데이터 세트를 위해 별도의 객체인 DMatrix를 생성한다는 점이다.</p><ul><li>Dmatrix는 주로 numpy 입력 parameter를 받아서 만들어지는 XGBoost만의 전용 데이터 세트이지만 numpy이외에 libsvm txt 포맷 파일, xgboost 이진 버퍼 파일을 parameter로 입력받아 변환할 수 있다.</li></ul></li><li><p>data는 피처 데이터 세트이며, label은 classification의 경우에는 label 데이터 세트, regression의 경우에는 숫자형인 종속값 데이터 세트이다.</p></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dtrain = xgb.DMatrix(data=X_train, label=y_train)</span><br><span class="line">dtest = xgb.DMatrix(data=X_test, label=y_test)</span><br></pre></td></tr></table></figure><hr><ul><li>early_stopping_rounds 파라미터를 설정해 조기 중단을 수행하기 위해서는 <code>반드시 eval_set과 eval_metric이 함께 설정되야 한다.</code> XGboost는 반복마다 eval_set으로 지정된 데이터 세트에서 eval_metric의 지정된 평가 지표로 예측 오류를 측정한다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;<span class="string">'max_depth'</span>:3,</span><br><span class="line">          <span class="string">'eta'</span>:0.1,</span><br><span class="line">          <span class="string">'objective'</span>:<span class="string">'binary:logistic'</span>,</span><br><span class="line">          <span class="string">'eval_metric'</span>:<span class="string">'logloss'</span>,</span><br><span class="line">          <span class="string">'early_stoppings'</span>:100</span><br><span class="line">      &#125;</span><br><span class="line">num_rounds = 400</span><br><span class="line"></span><br><span class="line"><span class="comment"># train 데이터 세트는 'train', evaluation(test) 데이터 세트는 'eval'로 명시한다.</span></span><br><span class="line">wlist = [(dtrain, <span class="string">'train'</span>), (dtest, <span class="string">'eval'</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 하이퍼 파라미터와 early stopping 파라미터를 train() 함수의 파라미터로 전달</span></span><br><span class="line">xgb_model = xgb.train(params=params, dtrain=dtrain, num_boost_round=num_rounds, evals=wlist)</span><br></pre></td></tr></table></figure><hr><ul><li>python 래퍼 xgboost는 predict() 메서드가 예측 결과값이 아닌 예측 결과를 추정할 수 있는 확률 값을 반환한다는 것이다.<ul><li>예측 확률이 0.5보다 크면 1, 그렇지 않으면 0으로 예측하는 로직을 추가</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pred_probs = xgb_model.predict(dtest)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'predict() 수행 결과값을 10개만 표시, 예측 화귤값으로 표시된'</span>)</span><br><span class="line"><span class="built_in">print</span>(np.round(pred_probs[:10], 3))</span><br><span class="line"></span><br><span class="line">preds = [1 <span class="keyword">if</span> prob &gt; 0.5 <span class="keyword">else</span> 0 <span class="keyword">for</span> prob <span class="keyword">in</span> pred_probs]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'예측값 10개만 표시:'</span>, preds[:10])</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predict() 수행 결과값을 10개만 표시, 예측 화귤값으로 표시된</span><br><span class="line">[0.95  0.003 0.9   0.086 0.993 1.    1.    0.999 0.998 0.   ]</span><br><span class="line">예측값 10개만 표시: [1, 0, 1, 0, 1, 1, 1, 1, 1, 0]</span><br></pre></td></tr></table></figure><hr><ul><li>xgboost 패키지에 내장된 시각화 기능 중 <code>plot_importance() API</code>는 feature의 중요도를 막대그래프 형식으로 나타낸다. <code>기본 평가 지표로 f1-score를 기반으로 해 각 feature의 중요도를 나타낸다.</code> Scikit-Learn은 Estimator 객체의 feature_importances_ 속성을 이용해 직접 시각화 코드를 작성해야 하지만, xgboost 패키지는 plot_importance()를 이용해 바로 피처 중요도를 시각화할 수 있다. plot_importance() 호출 시 파라미터로 앞에서 학습이 완료된 모델 객체 및 Matplotlib의 ax 객체를 입력하기만 하면 된다.</li></ul><ul><li>내장된 plot_importance() 이용 시 유의할 점은 xgboost numpy 기반의 feature 데이터터로 학습시에 피처명을 제대로 알 수 가 없으므로 f0, f1와 같이 feature 순서별로 f자 뒤에 순서를 붙여서 X 축에 feature들로 나열한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from xgboost import plot_importance</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(10, 12))</span><br><span class="line">plot_importance(xgb_model, ax=ax)</span><br></pre></td></tr></table></figure><p><img src="/image/python_wrapper_xgboost_feature_importance_bar_plot.png" alt="python 래퍼의 xgboost feature 중요도"></p><ul><li><p>또한, Decision Tree에서 보여준 tree 기반 규칙 구조도 xgboost에서 시각화할 수 있다. xgboost 모듈의 to_graphviz() API를 이용하면 jupyter notebook에 바로 규칙 tree 구조를 그릴 수 있다. xgboost.cv() API를 통해 GridSearchCV와 유사한 기증을 수행할 수 있다.</p></li><li><p>아래는 Scikit-Learn 래퍼의 xgboost의 사용법을 정리해 놓은 것이다.</p><ul><li>앞의 python 래퍼와 동일한 결과를 보여준다.</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from xgboost import XGBClassifier</span><br><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line"></span><br><span class="line">xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3, random_state=156)</span><br><span class="line">xgb_wrapper.fit(X_train, y_train)</span><br><span class="line">w_preds = xgb_wrapper.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, w_preds))</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[35  2]</span><br><span class="line"> [ 1 76]]</span><br></pre></td></tr></table></figure><hr><ul><li><p>early stopping 기능을 사용하는 방법은 아래와 같다. 성능 평가를 수행할 데이터 세트는 학습 데이터가 아니라 별도의 데이터 세트이어야 한다. 허나, 아래 데이터 자체의 크기가 작기 때문에 평가용으로 사용해 보았다. 허나, 절대 아래와 같이 evals에 test 데이터를 사용하면 안된다. 만일 test data를 사용했다면 predict하는 경우에는 학습에 사용되지 않은 또 다른 데이터를 사용해야한다.</p></li><li><p><code>또한, early stopping을 너무 적게 잡는다면 전역 최적화가 이루어지지 않을 수도 있으므로 주의</code>하자</p><ul><li>GridSearchCV와 같이 hyper parameter를 tuning할 경우에는 XGBoost가 GBM보다는 빠르지만 아무래도 GBM을 기반으로 하고 있기 때문에 수행 시간이 상당히 더 많이 요구된다. 앙상블 계열 알고리즘은 overfitting이나 noise에 기본적으로 뛰어난 알고리즘이므로 hyper parameter tuning으로 성능 수치 개선이 급격하게 좋아지는 경우는 그리 많지 않다. 일반 PC가 아닌 적어도 8-Core이상의 병렬 CPU Core 시스템을 가진 컴퓨터가 있다면 더 다양하게 hyper parameter 변경해 가면서 성능 향상을 적극저으로 시도해 보면 좋을 것이다.</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3, random_state=156)</span><br><span class="line">evals = [(X_test, y_test)]</span><br><span class="line">xgb_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=<span class="string">'logloss'</span>, eval_set=evals, verbose=True)</span><br><span class="line">w_preds = xgb_wrapper.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y_test, w_preds))</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[34  3]</span><br><span class="line"> [ 1 76]]</span><br></pre></td></tr></table></figure><hr><h2 id="LightGBM"><a href="#LightGBM" class="headerlink" title="LightGBM"></a>LightGBM</h2><ul><li>LightGBM은 XGBoost와 함께 부스팅 계열 알고리즘에서 가장 각광 받고 있다. XGBoost는 매우 뛰어난 부스팅 알고리즘이지만, XGBoost에서 GridSearchCV로 hyper parameter 튜닝을 수행하다 보면 여전히 학습시간이 오래 걸린다. 물론 GBM 보다는 빠르지만, <code>대용량 데이터의 경우 만족할 만한 학습 성능을 기대하려면 많은 CPU Core를 가진 시스템에서 높은 병렬도로 학습을 진행해야 한다.</code></li></ul><ul><li>LightGBM의 가장 큰 장점은 XGBoost보다 학습에 걸리는 시간이 훨씬 적다는 점이다. 또한 메모리 사용량도 상대적으로 적다. LightGBM이 XGBoost보다 2년 후에 만들어지다보니 XGBoost의 장점은 계승하고 단점은 보완하는 방식으로 개발되었기 때문에 예측 성능에서의 차이는 거의 없지만, 기능상의 다양성이 더 높다.</li></ul><ul><li><code>LightGBM의 한 가지 단점으로 알려진 것은 적은 데이터 세트에 적용할 경우 과적합이 발생하기 쉽다는 것</code>이다. 적은 데이터 세트의 기준은 애매하지만, 일반적으로 10,000건 이하의 데이터 세트 정도라고 LightGBM 공식 문서에서 기술하고 있다.</li></ul><ul><li>LightGBM은 일반 GBM 계열의 트리 분할 방법과 다르게 leaf 중심 트리 분할(Leaf Wise) 방식을 사용한다. 기존의 대부분 트리 기반 알고리즘은 트리의 깊이를 효과적으로 줄이기 위한 균형 트리 분할(Level wise)방식을 사용한다. 즉, <code>최대한 균형 잡힌 트리를 유지하면서 분할하기 때문에 트리의 깊이가 최소화될 수 있다.</code> 이렇게 균형잡힌 트리를 생성하는 이유는 과적합(overfitting)에 보다 더 강한 구조를 가질 수 있다고 알려져 있기 때문이다. 반대로 균형을 맞추기 위한 시간이 필요하다는 상대적인 단점이 있다. <code>하지만, LightGBM의 leaf 중심 트리 분할 방식은 트리의 균형을 맞추지 않고, 최대 손실 값(max delta loss)을 가지는 leaf 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적인 규칙 트리가 생성된다. 하지만 이렇게 최대 손실값을 가지는 leaf 노드를 지속적으로 분할해 생성된 규칙 트리는 학습을 반복할수록 결국은 균형 트리 분할 방식보다 예측 오류 손실을 최소화 할 수 있다는 것이 LightGBM의 구현 사상</code>이다.</li></ul><p><img src="/image/what_is_lightGBM.png" alt="LightGBM의 개념"></p><ul><li>LightGBM 설치 방법<ul><li>Window에 설치할 경우에는 Visual Studio Build tool 2015 이상이 설치돼있어야 한다.</li><li>아나콘다 프롬프트를 관리자 권한으로 실행한 다음 아래 명려어 실행</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge lightgbm</span><br></pre></td></tr></table></figure><ul><li><code>LightGBM 하이퍼 파라미터는 XGBoost와 많은 부분이 유사하지만 트리의 분할 방식이 다르므로 예를 들어 max_depth를 매우 크게 가져가는 것과 같이 트리 특성에 맞게 설정해 주어야 할 것</code>이다.</li></ul><h4 id="주요-파라미터"><a href="#주요-파라미터" class="headerlink" title="주요 파라미터"></a>주요 파라미터</h4><ul><li><p><code>num_iterations [default = 100]</code> : 반복 수행하려는 트리의 개수를 지정한다. 크게 지정할수록 예측 성능이 높아질수 있으나, 너무 크게 지정하면 오히려 과적합으로 성능이 저하 될 수 있다. Scikit-Learn GBM과 XGBoost의 Scikit-Learn 호환 클래스의 n_estimators와 같은 파라미터이므로 LightGBM의 Scikit-Learn 호환 클래스에서는 n_estimators로 이름이 변경된다.</p></li><li><p><code>learning_rate [default = 0.1]</code> : 0에서 1사이의 값을 지정하며 Boosting 스텝을 반복적으로 수행할 때 업데이트되는 학습롤값이다. 일반적으로 n_estimators를 크게하고 learning_rate를 작게해서 예측 성능을 향상시킬 수 있으나, 마찬가지로 과적합(overfitting) 이슈와 학습 시간이 길어지는 부정적인 영향도 고려해야한다. GBM, XGBoost의 learning_rate와 같은 파라미터이다.</p></li><li><p><code>max_depth [default=1]</code> : 트리 기반 알고리즘의 max_depth와 같다. 0보다 작은 값을 지정하면 깊이에 제한이 없다. <code>지금까지 소개한 Depth Wise 방식의 트리와 다르게 LightGBM은 Leaf wise 기반이므로 깊이가 상대적으로 더 깊다.</code></p></li><li><p><code>min_data_in_leaf [default=20]</code> : Decision Tree의 min_samples_leaf와 같은 파라미터이다. 하지만 Scikit-Learn 래퍼 LightGBM 클래스인 LightGBMClassifier에서는 min_child_samples 파라미터로 이름이 변경된다. <code>최종 결정 클래스인 Leaf 노드가 되기 위해서 최소한으로 필요한 레코드(데이터) 수이며, 과적합을 제어하기 위한 파라미터이다.</code></p></li><li><p><code>num_leaves [default=31]</code> : 하나의 트리가 가질 수 있는 최대 Leaf 개수이다.</p></li><li><p><code>boosting [default=gbdt]</code> : Boosting 트리를 생성하는 알고리즘을 기술한다.</p><ul><li>gbdt : 일반적인 그레디언트 부스팅 결정트리</li><li>rf : 랜덤포레스트</li></ul></li><li><p><code>bagging_fraction [default=1.0]</code> : 트리가 커져서 과적합되는 것을 제어하기 위해서 데이터 샘플링하는 비율을 지정한다. Scikit-Learn의 GBM과 XGBoost의 sub_sample 파라미터와 동일하기에 Scikit-Learn 래퍼 LightGBM인 LightGBMClassifier에서는 sub_sample로 동일하게 파라미터 이름이 변경된다.</p></li><li><p><code>feature_fraction [default=1.0]</code> : 개별 트리를 학습할 때마다 무작위로 선택하는 feature의 비율이다. 과적합을 막기 위해 사용된다. GBM의 max_features와 유사하며, XGBClassifier의 colsample_bytree와 똑같으므로 LightGBMClassifier에서는 동일하게 colsample_bytree로 변경된다.</p></li><li><p><code>lambda_l2 [default=0.0]</code> : L2 Regulation 제어를 위한 값이다. feature 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과가 있다. XGBClassifier의 reg_lambda와 동일하므로 LightGBMClassifier에서는 reg_lambda로 변경된다.</p></li><li><p><code>lambda_l1 [default=0.0]</code> : L1 Regulation 제어를 위한 갑이다. L2와 마찬가지로 과적합 제어를 위한 것이며, XGBClassifier의 reg_alpha와 동일하므로 LightGBMClassifier에서는 reg_alpha로 변경된다.</p></li></ul><h4 id="Learning-Task-파라미터"><a href="#Learning-Task-파라미터" class="headerlink" title="Learning Task 파라미터"></a>Learning Task 파라미터</h4><ul><li><code>objective</code> : 최솟값을 가져야 할 손실함수(loss function)을 정의한다. XGBoost의 objective 파라미터와 동일하다. 애플리케이션 유형, 즉 regression, multiclass classification, binary classificationdl인지에 따라 objective인 손실함수가 지정된다.</li></ul><h4 id="하이퍼-파라미터-튜닝-방안"><a href="#하이퍼-파라미터-튜닝-방안" class="headerlink" title="하이퍼 파라미터 튜닝 방안"></a>하이퍼 파라미터 튜닝 방안</h4><ul><li><p><code>num_leaves의 개수를 중심으로 min_child_samples(min_data_in_leaf), max_depth를 함께 조정하면서 모델의 복잡도를 줄이는 것이 기본 튜닝 방안</code>이다.</p><ul><li><p>num_leaves는 개별 트리가 가질 수 있는 최대 Leaf의 개수이고 LightGBM 모델의 복잡도를 제어하는 주요 파라미터이다. 일반적으로 num_leaves의 개수를 높이면 정확도가 높아지지만, 반대로 트리의 깊이가 깊어지고 모델의 복잡도가 커져서 과적합 영향도가 커진다.</p></li><li><p>min_data_in_leaf는 Scikit-Learn 래퍼 클래스에서는 min_child_samples로 이름이 바뀐다. 과적합을 개선하기 위한 중요한 파라미터이다. num_leaves와 학습 데이터의 크기에 따라 달라지지만, 보통 큰 값으로 설정하면 트리가 깊어지는 것을 방지한다.</p></li><li><p>max_depth는 명시적으로 깊이의 크기를 제한한다. num_leaves, min_data_in_leaf와 결합해 과적합을 개선하는데 사용한다.</p></li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from lightgbm import LGBMClassifier</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">dataset = load_breast_cancer()</span><br><span class="line">X_data = dataset.data</span><br><span class="line">y = dataset.target</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.2 ,random_state=1234)</span><br><span class="line"></span><br><span class="line">lgbm_wrapper = LGBMClassifier(n_estimators=400)</span><br><span class="line"></span><br><span class="line">evals = [(X_test, y_test)]</span><br><span class="line">lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=<span class="string">"logloss"</span>, eval_set=evals, verbose=True)</span><br><span class="line">preds = lgbm_wrapper.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line">confusion_matrix(y_test, preds)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[36,  9],</span><br><span class="line">       [ 2, 67]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def confusion_matrix_mine(y_test, pred):</span><br><span class="line">    class_types=list(<span class="built_in">set</span>(y_test))</span><br><span class="line">    confusion_matrix=np.zeros((len(class_types),len(class_types)))</span><br><span class="line">    <span class="keyword">for</span> y, pred <span class="keyword">in</span> zip(y_test, preds):</span><br><span class="line">        <span class="keyword">if</span> y==pred:</span><br><span class="line">            confusion_matrix[y,pred]=confusion_matrix[y,pred]+1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            confusion_matrix[y,pred]=confusion_matrix[y,pred]+1</span><br><span class="line">    <span class="built_in">return</span> confusion_matrix</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusion_matrix_mine(y_test, pred)</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[36.,  9.],</span><br><span class="line">       [ 2., 67.]])</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import classification_report</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, preds))</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.95      0.80      0.87        45</span><br><span class="line">           1       0.88      0.97      0.92        69</span><br><span class="line"></span><br><span class="line">    accuracy                           0.90       114</span><br><span class="line">   macro avg       0.91      0.89      0.90       114</span><br><span class="line">weighted avg       0.91      0.90      0.90       114</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from lightgbm import plot_importance</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(figsize=(10,12))</span><br><span class="line">plot_importance(lgbm_wrapper, ax=ax)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/python_wrapper_xgboost_feature_importance_bar_plot.png" alt="lightgbm 피처 중요도 그래프"></p><h2 id="CatBoost"><a href="#CatBoost" class="headerlink" title="CatBoost"></a>CatBoost</h2><ul><li>최근 machine learning 알고리즘은 아래 그림에서 볼 수 있듯이 lightgbm과 catboost를 사용하는 유저들이 많아졌다는 것을 확인할 수 있다. 그렇다면 과연 CatBoost과 무엇이길래 많은 사람들이 사용하는지 한번 알아보자.</li></ul><p><img src="/image/catboost_having__many_user.png" alt="CatBoost의 성장"></p><ul><li>categorical feature에 잘 맞는다고 알려져있다. 일반적으로 머신러닝의 모델은 분산(Variance)과 편향(bias)의 trade-off관계를 조절하며 어떤 것에 더 중점을 둘지를 결정하여 만들게 된다. 그러나 이 CatBoost는 <code>잔차 추정의 분산을 최소로 하면서 bias를 피하는 boosting</code>기법이다. 즉, validation set을 제외한 data set에서 train에 사용되지 않은 data들을 통해 잔차의 분산을 최소화시키도록 학습을 시키는 방법이다.</li></ul><p><img src="/image/what_is_cat_boost.png" alt="Catboost 개념"></p><p><a href="https://gentlej90.tistory.com/100" target="_blank" rel="noopener">CatBoost 참조</a></p><h3 id="기존의-부스팅-방법"><a href="#기존의-부스팅-방법" class="headerlink" title="기존의 부스팅 방법"></a>기존의 부스팅 방법</h3><ul><li><p>기존의 부스팅 기법을 간략히 설명하면 다음과 같다.</p><ul><li>1) 실제 값들의 평균과 차이인 잔차(Residual)를 구한다.</li><li>2) 데이터로 이 잔차들을 학습하는 모델을 만든다.</li><li>3) 모델을 통해 학습한 파라미터 (평균 + 잔차예측 값 * learning_rate)를 업데이트한다.</li><li>4) 위의 과정을 loss값이 일정 round 동안 수렴할때까지 반복한다.</li></ul></li></ul><h3 id="기존의-부스팅의-문제점"><a href="#기존의-부스팅의-문제점" class="headerlink" title="기존의 부스팅의 문제점"></a>기존의 부스팅의 문제점</h3><ul><li><p>1) <code>느린 학습 속도</code></p><ul><li>부스팅 모델이 아닌 배깅과 비교했을 때, 훨씬 느린 속도를 보인다. 배깅의 경우 여러 트리들이 병렬적으로 모델 학습을 수행하고 부스팅의 경우 순차적으로 모델학습을 수행하기 때문에 느린 속도를 갖을 수 밖에 없다. 이런 문제점을 보완한 것이 XGBoost, LightGBM, CatBoost들이다.</li></ul></li><li><p>2) overfitting</p><ul><li>속도문제를 샘플링이나 알고리즘 최적화로 어느정도 개선이 되었다면, 남아있는 문제는 overfitting이다. 이는 부스팅이라는 개념 자체가 가지고 있는 문제인데, 부스팅 자체가 <code>오차(error)를 줄여나가기 위해 학습하는 모델이기 때문에 굉장히 High Variance한 모델이기 때문이다.</code></li></ul></li></ul><h3 id="CatBoost의-특징"><a href="#CatBoost의-특징" class="headerlink" title="CatBoost의 특징"></a>CatBoost의 특징</h3><h4 id="Level-wise-Tree"><a href="#Level-wise-Tree" class="headerlink" title="Level-wise Tree"></a>Level-wise Tree</h4><ul><li>LightGBM은 Leaf-wise 방식으로 트리를 만들었지만, XGBoost와 동일하게 Level-wise 방식으로 트리를 만들어 나간다. 직관적으로 표현하자면 Level-wise는 BFS같이 트리를 만들어나가는 방식이고, Leaf-wise는 DFS 같이 트리를 만들어나가는 형태인 것이다.</li></ul><h4 id="Ordered-Boosting"><a href="#Ordered-Boosting" class="headerlink" title="Ordered Boosting"></a>Ordered Boosting</h4><ul><li><p>CatBoost는 기존의 부스팅 과정과 전체적인 양상은 비슷하되, 조금 다르다. <code>기존의 부스팅 모델이 일괄적으로 모든 훈련 데이터를 대상으로 잔차를 계산 했다면, CatBoost는 일부 즉, 훈련에 사용되지 않은 나머지 데이터에 대해 error를 추정한 뒤, 이것을 통해 모델을 만들고, 그 뒤에 데이터의 잔차는 만들어진 모델을 통해 예측한 값을 사용한다.</code></p></li><li><p>예를 들면, 아래와 같은 데이터가 있다고 가정해보자.</p></li></ul><div class="table-container"><table><thead><tr><th>time</th><th>datapoint</th><th>class label</th></tr></thead><tbody><tr><td>12:00</td><td>$x_1$</td><td>10</td></tr><tr><td>12:01</td><td>$x_2$</td><td>12</td></tr><tr><td>12:02</td><td>$x_3$</td><td>9</td></tr><tr><td>12:03</td><td>$x_4$</td><td>4</td></tr><tr><td>12:04</td><td>$x_5$</td><td>52</td></tr><tr><td>12:05</td><td>$x_6$</td><td>22</td></tr></tbody></table></div><ul><li><p>기존의 부스팅 기법은 모든 학습 데이터(x1~x10)까지의 잔차를 일괄 계산한다. 반면, CatBoost의 과정은 다음과 같다.</p><ul><li>1) 먼저 $x_{1}$의 잔차만 계산하고, 이를 기반으로 모델을 만든다. 그리고 $x_{2}$의 잔차를 이 모델로 예측한다.</li><li>2) $x_{1}, x_{2}$의 잔차를 가지고 모델을 만든다. 이를 기반으로 $x_{3}, x_{4}$의 잔차를 모델로 예측한다.</li><li>3) $x_{1}, x_{2}, x_{3}, x_{4}$를 가지고 모델을 만든다. 이를 기반으로 $x_{5}, x_{6}, x_{7}, x_{8}$의 잔차를 모델로 예측한다.</li><li>4) 반복</li></ul></li><li><p><code>위와 같이 순서에 따라 모델을 만들고 예측하는 방식을 Ordered Boosting</code>이라고 부른다.</p></li></ul><h4 id="Random-Permutation"><a href="#Random-Permutation" class="headerlink" title="Random Permutation"></a>Random Permutation</h4><ul><li>위에서 Ordered Boosting을 할 때, 데이터 순서를 섞어주지 않으면 매번 같은 순서대로 잔차를 예측하는 모델을 만들 가능성이 있다. <code>그러므로 이 순서를 위사 임의로 랜덤하게 섞어주어야 한다.</code> CatBoost는 이러한 점을 감안해서 데이터를 셔플링하여 뽑아낸다. 뽑아낼 때도 역시 모든 데이터를 뽑는게 아니라, 그 중 일부만 가져오게 할 수 있다. 이 모든 기법이 overffiting을 방지하기 위해 tree를 다각적으로 만들려는 시도인 것이다.</li></ul><h4 id="Ordered-Target-Encoding"><a href="#Ordered-Target-Encoding" class="headerlink" title="Ordered Target Encoding"></a>Ordered Target Encoding</h4><ul><li>Target Encoding, Mean Encoding, Response Encoding이라고 불리는 데 모두 다 같은 개념을 지칭하는 용어이다.</li><li>범주형 변수를 수로 인코딩 시키는 방법 중, 비교적 가장 최근에 나온 기법인데, 간단한 설명을 하면 다음과 같다.</li></ul><div class="table-container"><table><thead><tr><th>time</th><th>feature</th><th>class_labels(max_temperature on that day)</th></tr></thead><tbody><tr><td>sunday</td><td>sunny</td><td>35</td></tr><tr><td>monday</td><td>sunny</td><td>32</td></tr><tr><td>tuesday</td><td>cloudy</td><td>15</td></tr><tr><td>wednesday</td><td>cloudy</td><td>14</td></tr><tr><td>thursday</td><td>mostly_cloudy</td><td>10</td></tr><tr><td>friday</td><td>cloudy</td><td>20</td></tr><tr><td>saturday</td><td>cloudy</td><td>25</td></tr></tbody></table></div><ul><li>위 데이터에서 time, feature로 class_label을 예측한다고 가정해보자. feature의 cloudy는 다음과 같이 인코딩 할 수 있다.<ul><li>즉, cloudy를 cloudy를 가진 데이터들의 class_label의 값의 평균으로 인코딩하는 것이다. 이러한 이유로 Mean encoding이라 불리기도 한다.</li></ul></li></ul><script type="math/tex; mode=display">cloudy = \frac{(15+14+20+25)}{4} = 18.5</script><ul><li><p>그런데, 위에서 우리가 예측하는 값이 Train set feature에 들어가버리는 문제, 즉 Data Leakage 문제를 일으킨다. 이는 overfitting을 발생시키는 주 원인이자, Mean encoding 방법 자체의 문제이기도 하다. <code>그래서 CatBoost는 이에 대한 해결책으로, 현재 데이터의 인코딩을 위해 이전 데이터들의 인코딩된 값을 사용한다.</code></p></li><li><p>즉, 현재 데이터의 Target값을 사용하지 않고, 이전 데이터들의 Target값 만을 사용하니, Data Leakage가 일어나지 않는 것이다. <code>물론 data 중 이미 평균값과 동일한 값이 존재 한다면 사용할 수 없을 것이다.</code></p></li></ul><script type="math/tex; mode=display">Friday: cloudy = \frac{(15+14)}{2} = 15.5</script><script type="math/tex; mode=display">Saturday: cloudy = \frac{(15+14+20)}{3} = 16.3</script><h4 id="Categorical-Feature-Combinations"><a href="#Categorical-Feature-Combinations" class="headerlink" title="Categorical Feature Combinations"></a>Categorical Feature Combinations</h4><ul><li>아래 데이터의 경우에는 country만 보아도 hair_color feature를 알 수 있기 때문에, class_label을 예측하는데 있어, 두 feature 다 필요 없이 이 중 하나의 feature만 있으면 된다. CatBoost는 이렇게 information gain이 동일한 두 feature를 하나의 feature로 묶어버린다. <code>결과적으로, 데이터 전처리에 있어 feature selection에 대해 부담이 줄어들 수 있다고 볼 수 있다.</code></li></ul><div class="table-container"><table><thead><tr><th>country</th><th>hair clolor</th><th>class_label</th></tr></thead><tbody><tr><td>India</td><td>Black</td><td>1</td></tr><tr><td>India</td><td>Black</td><td>1</td></tr><tr><td>India</td><td>Black</td><td>1</td></tr><tr><td>India</td><td>Black</td><td>1</td></tr><tr><td>russia</td><td>white</td><td>0</td></tr><tr><td>russia</td><td>white</td><td>0</td></tr><tr><td>russia</td><td>white</td><td>0</td></tr></tbody></table></div><h4 id="One-hot-Encoding"><a href="#One-hot-Encoding" class="headerlink" title="One-hot Encoding"></a>One-hot Encoding</h4><ul><li>범주형 변수를 항상 Target Encoding하는 것은 아니다. Catboost는 낮은 Cardinality를 가지는 범주형 변수에 한해서, 기본적으로 One-hot encoding을 시행한다. Cardinality 기준은 <code>one_hot_max_size</code> 파라미터로 설정할 수 있다.</li></ul><ul><li>예를 들어, <code>one_hot_max_size = 3</code>으로 설정한 경우, Cardinality가 3이하인 범주형 변수들은 Target Encoding이 아니라 One-hot Encoding으로 변환한다. 낮은 개수를 갖는 범주형 변수의 경우 One-hot Encoding이 더 효율적이라 그런 것이라 생각든다.</li></ul><h4 id="Optimized-Parameter-tuning"><a href="#Optimized-Parameter-tuning" class="headerlink" title="Optimized Parameter tuning"></a>Optimized Parameter tuning</h4><ul><li>CatBoost는 파라미터들의 default 값이 기본적으로 최적화가 잘 되어서, 파라미터 튜닝에 크게 신경쓰지 않아도 된다고한다. 물론 데이터 마다 다르기 때문에 튜닝을 해보는 것이 좋긴하다.</li></ul><h3 id="CatBoost의-한계"><a href="#CatBoost의-한계" class="headerlink" title="CatBoost의 한계"></a>CatBoost의 한계</h3><ul><li><code>Sparse한 Matrix는 처리하지 못한다.</code></li></ul><ul><li><code>데이터 대부분이 수치형 변수인 경우, LightGBM보다 학습 속도가 느리다.</code>그러므로 대부분이 범주형 변수인 경우에만 사용하는 것을 추천한다.</li></ul><h3 id="Ensemble-실습"><a href="#Ensemble-실습" class="headerlink" title="Ensemble 실습"></a>Ensemble 실습</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br></pre></td></tr></table></figure><hr><h4 id="데이터-info"><a href="#데이터-info" class="headerlink" title="데이터 info"></a>데이터 info</h4><ul><li>id: 고유 아이디</li><li>feat_1 ~ feat_93: 설명변수</li><li>target: 타겟변수 (1~9)</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터 불러오기</span></span><br><span class="line">data = pd.read_csv(<span class="string">"../data/otto_train.csv"</span>) <span class="comment"># Product Category</span></span><br><span class="line">data.head() <span class="comment"># 데이터 확인</span></span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nCar = data.shape[0] <span class="comment"># 데이터 개수</span></span><br><span class="line">nVar = data.shape[1] <span class="comment"># 변수 개수</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'nCar: %d'</span> % nCar, <span class="string">'nVar: %d'</span> % nVar )</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nCar: 61878 nVar: 95</span><br></pre></td></tr></table></figure><hr><h4 id="의미가-없다고-판단되는-변수-제거"><a href="#의미가-없다고-판단되는-변수-제거" class="headerlink" title="의미가 없다고 판단되는 변수 제거"></a>의미가 없다고 판단되는 변수 제거</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = data.drop([<span class="string">'id'</span>], axis = 1) <span class="comment"># id 제거</span></span><br></pre></td></tr></table></figure><hr><h4 id="타겟-변수의-문자열을-숫자로-변환"><a href="#타겟-변수의-문자열을-숫자로-변환" class="headerlink" title="타겟 변수의 문자열을 숫자로 변환"></a>타겟 변수의 문자열을 숫자로 변환</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mapping_dict = &#123;<span class="string">"Class_1"</span>: 1,</span><br><span class="line">                <span class="string">"Class_2"</span>: 2,</span><br><span class="line">                <span class="string">"Class_3"</span>: 3,</span><br><span class="line">                <span class="string">"Class_4"</span>: 4,</span><br><span class="line">                <span class="string">"Class_5"</span>: 5,</span><br><span class="line">                <span class="string">"Class_6"</span>: 6,</span><br><span class="line">                <span class="string">"Class_7"</span>: 7,</span><br><span class="line">                <span class="string">"Class_8"</span>: 8,</span><br><span class="line">                <span class="string">"Class_9"</span>: 9&#125;</span><br><span class="line">after_mapping_target = data[<span class="string">'target'</span>].apply(lambda x: mapping_dict[x])</span><br></pre></td></tr></table></figure><hr><h4 id="설명변수와-타겟변수를-분리-학습데이터와-평가데이터-분리"><a href="#설명변수와-타겟변수를-분리-학습데이터와-평가데이터-분리" class="headerlink" title="설명변수와 타겟변수를 분리, 학습데이터와 평가데이터 분리"></a>설명변수와 타겟변수를 분리, 학습데이터와 평가데이터 분리</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">feature_columns = list(data.columns.difference([<span class="string">'target'</span>])) <span class="comment"># target을 제외한 모든 행</span></span><br><span class="line">X = data[feature_columns] <span class="comment"># 설명변수</span></span><br><span class="line">y = after_mapping_target <span class="comment"># 타겟변수</span></span><br><span class="line">train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 42) <span class="comment"># 학습데이터와 평가데이터의 비율을 8:2 로 분할|</span></span><br><span class="line"><span class="built_in">print</span>(train_x.shape, test_x.shape, train_y.shape, test_y.shape) <span class="comment"># 데이터 개수 확인</span></span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(49502, 93) (12376, 93) (49502,) (12376,)</span><br></pre></td></tr></table></figure><hr><h4 id="1-XGBoost"><a href="#1-XGBoost" class="headerlink" title="1. XGBoost"></a>1. XGBoost</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install xgboost</span></span><br><span class="line">import xgboost as xgb</span><br><span class="line">import time</span><br><span class="line">start = time.time() <span class="comment"># 시작 시간 지정</span></span><br><span class="line">xgb_dtrain = xgb.DMatrix(data = train_x, label = train_y) <span class="comment"># 학습 데이터를 XGBoost 모델에 맞게 변환</span></span><br><span class="line">xgb_dtest = xgb.DMatrix(data = test_x) <span class="comment"># 평가 데이터를 XGBoost 모델에 맞게 변환</span></span><br><span class="line">xgb_param = &#123;<span class="string">'max_depth'</span>: 10, <span class="comment"># 트리 깊이</span></span><br><span class="line">         <span class="string">'learning_rate'</span>: 0.01, <span class="comment"># Step Size</span></span><br><span class="line">         <span class="string">'n_estimators'</span>: 100, <span class="comment"># Number of trees, 트리 생성 개수</span></span><br><span class="line">         <span class="string">'objective'</span>: <span class="string">'multi:softmax'</span>, <span class="comment"># 목적 함수</span></span><br><span class="line">        <span class="string">'num_class'</span>: len(<span class="built_in">set</span>(train_y)) + 1&#125; <span class="comment"># 파라미터 추가, Label must be in [0, num_class) -&gt; num_class보다 1 커야한다.</span></span><br><span class="line">xgb_model = xgb.train(params = xgb_param, dtrain = xgb_dtrain) <span class="comment"># 학습 진행</span></span><br><span class="line">xgb_model_predict = xgb_model.predict(xgb_dtest) <span class="comment"># 평가 데이터 예측</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Accuracy: %.2f"</span> % (accuracy_score(test_y, xgb_model_predict) * 100), <span class="string">"%"</span>) <span class="comment"># 정확도 % 계산</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Time: %.2f"</span> % (time.time() - start), <span class="string">"seconds"</span>) <span class="comment"># 코드 실행 시간 계산</span></span><br></pre></td></tr></table></figure><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 76.67 %</span><br><span class="line">Time: 20.48 seconds</span><br></pre></td></tr></table></figure><hr><h4 id="2-LightGBM"><a href="#2-LightGBM" class="headerlink" title="2. LightGBM"></a>2. LightGBM</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install lightgbm</span></span><br><span class="line">import lightgbm as lgb</span><br><span class="line">start = time.time() <span class="comment"># 시작 시간 지정</span></span><br><span class="line">lgb_dtrain = lgb.Dataset(data = train_x, label = train_y) <span class="comment"># 학습 데이터를 LightGBM 모델에 맞게 변환</span></span><br><span class="line">lgb_param = &#123;<span class="string">'max_depth'</span>: 10, <span class="comment"># 트리 깊이</span></span><br><span class="line">            <span class="string">'learning_rate'</span>: 0.01, <span class="comment"># Step Size</span></span><br><span class="line">            <span class="string">'n_estimators'</span>: 100, <span class="comment"># Number of trees, 트리 생성 개수</span></span><br><span class="line">            <span class="string">'objective'</span>: <span class="string">'multiclass'</span>, <span class="comment"># 목적 함수</span></span><br><span class="line">            <span class="string">'num_class'</span>: len(<span class="built_in">set</span>(train_y)) + 1&#125; <span class="comment"># 파라미터 추가, Label must be in [0, num_class) -&gt; num_class보다 1 커야한다.</span></span><br><span class="line">lgb_model = lgb.train(params = lgb_param, train_set = lgb_dtrain) <span class="comment"># 학습 진행</span></span><br><span class="line">lgb_model_predict = np.argmax(lgb_model.predict(test_x), axis = 1) <span class="comment"># 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Accuracy: %.2f"</span> % (accuracy_score(test_y, lgb_model_predict) * 100), <span class="string">"%"</span>) <span class="comment"># 정확도 % 계산</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Time: %.2f"</span> % (time.time() - start), <span class="string">"seconds"</span>) <span class="comment"># 코드 실행 시간 계산</span></span><br></pre></td></tr></table></figure><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Accuracy: 73.57 %</span><br><span class="line">Time: 8.46 seconds</span><br></pre></td></tr></table></figure><hr><h4 id="3-Catboost"><a href="#3-Catboost" class="headerlink" title="3. Catboost"></a>3. Catboost</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !pip install catboost</span></span><br><span class="line">import catboost as cb</span><br><span class="line">start = time.time() <span class="comment"># 시작 시간 지정</span></span><br><span class="line">cb_dtrain = cb.Pool(data = train_x, label = train_y) <span class="comment"># 학습 데이터를 Catboost 모델에 맞게 변환</span></span><br><span class="line">cb_param = &#123;<span class="string">'max_depth'</span>: 10, <span class="comment"># 트리 깊이</span></span><br><span class="line">            <span class="string">'learning_rate'</span>: 0.01, <span class="comment"># Step Size</span></span><br><span class="line">            <span class="string">'n_estimators'</span>: 100, <span class="comment"># Number of trees, 트리 생성 개수</span></span><br><span class="line">            <span class="string">'eval_metric'</span>: <span class="string">'Accuracy'</span>, <span class="comment"># 평가 척도</span></span><br><span class="line">            <span class="string">'loss_function'</span>: <span class="string">'MultiClass'</span>&#125; <span class="comment"># 손실 함수, 목적 함수</span></span><br><span class="line">cb_model = cb.train(pool = cb_dtrain, params = cb_param) <span class="comment"># 학습 진행</span></span><br><span class="line">cb_model_predict = np.argmax(cb_model.predict(test_x), axis = 1) + 1 <span class="comment"># 평가 데이터 예측, Softmax의 결과값 중 가장 큰 값의 Label로 예측, 인덱스의 순서를 맞추기 위해 +1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Accuracy: %.2f"</span> % (accuracy_score(test_y, cb_model_predict) * 100), <span class="string">"%"</span>) <span class="comment"># 정확도 % 계산</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Time: %.2f"</span> % (time.time() - start), <span class="string">"seconds"</span>) <span class="comment"># 코드 실행 시간 계산</span></span><br></pre></td></tr></table></figure><h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">90:learn: 0.6928407total: 53sremaining: 5.24s</span><br><span class="line">91:learn: 0.6930427total: 53.5sremaining: 4.66s</span><br><span class="line">92:learn: 0.6935073total: 54.1sremaining: 4.07s</span><br><span class="line">93:learn: 0.6940932total: 54.6sremaining: 3.49s</span><br><span class="line">94:learn: 0.6944972total: 55.2sremaining: 2.9s</span><br><span class="line">95:learn: 0.6948810total: 55.7sremaining: 2.32s</span><br><span class="line">96:learn: 0.6951840total: 56.3sremaining: 1.74s</span><br><span class="line">97:learn: 0.6954264total: 56.8sremaining: 1.16s</span><br><span class="line">98:learn: 0.6955881total: 57.4sremaining: 580ms</span><br><span class="line">99:learn: 0.6956285total: 57.9sremaining: 0us</span><br><span class="line">Accuracy: 69.64 %</span><br><span class="line">Time: 58.31 seconds</span><br></pre></td></tr></table></figure><hr><h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><ul><li><p>스태킹(Stacking)은 개별적인 여러 알고리즘을 서로 결합해 예측 결과를 도출한다는 점에서 앞서 소개한 Bagging 및 Boosting과 공통정을 가지고 있다. 하지만 <code>가장 큰 차이점은 개별 알고리즘으로 예측한 데이터를 기반으로 다시 예측을 수행한다는 점</code>이다. 즉, <code>개별 알고리즘의 예측 결과 데이터 세트를 최종적인 메타 데이터 세트로 만들어 별도의 ML 알고리즘으로 최종 학습을 수행하고 테스트 데이터를 기반으로 다시 최종 예측을 수행하는 방식</code>이다. 이렇게 개별 모델의 예측된 데이터 세트를 기반으로 학습하고 예측하는 방식을 Meta Model이라고 한다.</p></li><li><p>스태킹을 현실 모델에 적용하는 경우는 그리 많지 않다. 그러나 캐글과 같은 대회에서 높은 순위를 차지하기 위해 조금이라도 성능 수치를 높여야 할 경우 자주 사용된다. 스태킹을 적용할 때는 많은 개별 모델이 필요하다. 2~3개의 개별 모델만을 결합해서는 쉽게 예측 성능을 향상 시킬 수 없으며, 스태킹을 적용한다고 해서 반드시 성능이 향상 되리라는 보장도 없다.</p></li><li><p>위의 Ensemble 방법들과 다르게 python의 내장 모듈로 되어있지는 않다. 그러므로 직접 코딩을 해서 사용해야 한다. 학습 데이터에 대해 단일 모델만 사용하는 것이 아니라 여러 모델을 사용한다. 이렇게 추정한 학습 모델을 통해 학습데이터를 예측한다. <code>이렇게 얻게 된 각 모델의 예측결과를 다시 독립 변수로 사용</code>한다. 해당 독립 변수들을 통해 검증 데이터를 예측하여 성능을 측정한다.</p></li></ul><p><img src="/image/what_is_stacking.png" alt="Stacking이란?"></p><ul><li>Stacking을 할 때 k-fold(보통 k=5 or 10)를 통해 학습 데이터와 검증데이터를 나누어 주어야 한다. 또한 <code>학습하는데 굉장히 오랜 시간이 소요되므로 비효율적인 모델이긴 하지만 성능은 좋다.</code> 그러므로 <code>캐글 같이 성능을 올려야만 성적이 좋아지는 상황이 아닌 현업에서 사용하기에는 굉장히 무리가 있다.</code> 그러므로 성능이 중요한 상황에서 최후의 보루로 생각하고 있는 것이 좋다.</li></ul><ul><li>아래의 그림에서 예를 들어 설명하자면 다음과 같다. 먼저 train과 test 데이터를 각각 5-fold로 나누어 준다. 모델은 총 4가지로 SVM, KNN, RF, GBM을 사용할 것이다. 4가지 모델에 대해 5번 학습을 하므로 총 20번을 학습해야 한다. 그러므로 그에 따른 소요시간은 엄청날 것이다. 이렇게 학습한 모델을 통해 train 데이터 중 학습에 사용되지 않은 데이터를 예측하여 새로운 feature들을 얻는다. 본래 데이터의 feature는 2개 였지만 각 모델의 예측값을 통해 새로운 feature 4개를 얻게 된다. Test 데이터에 대해서도 학습한 모델을 통해 예측한 결과를 새로운 feature로 사용한다.</li></ul><p><img src="/image/what_is_stacking_01.png" alt="Stacking이란? - 01"></p><p><img src="/image/what_is_stacking_02.png" alt="Stacking이란? - 02"></p><p><img src="/image/what_is_stacking_03.png" alt="Stacking이란? - 03"></p><ul><li>아래와 같이 학습한 모델들의 예측값들을 새로운 feature로 사용하여 기존의 train 데이터와 test 데이터에 merge 시켜준다.</li></ul><p><img src="/image/new_feature_merge_train_data_stacking.png" alt="Stacking이란? - 04"></p><p><img src="/image/new_feature_merge_test_data_stacking.png" alt="Stacking이란? - 05"></p><ul><li>새롭게 얻은 train 데이터를 모델에 적합시키고 test 데이터를 예측하여 성능을 계산한다.</li></ul><p><img src="/image/final_step_in_stacking.png" alt="Stacking이란? - 06"></p><ul><li>다음과 같이 <code>각 모델별 prediction을 통해 얻은 feature들만을 사용하기도 한다.</code></li></ul><p><img src="/image/Stacking_method_new_prediciton_feature.png" alt="Stacking이란? - 07"></p><h3 id="기본-스태킹-모델-실습"><a href="#기본-스태킹-모델-실습" class="headerlink" title="기본 스태킹 모델 실습"></a>기본 스태킹 모델 실습</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line">from sklearn.ensemble import AdaBoostClassifier</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">from sklearn.datasets import load_breast_cancer</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import accuracy_score</span><br><span class="line"></span><br><span class="line">cancer_data = load_breast_cancer()</span><br><span class="line">X_data = cancer_data.data</span><br><span class="line">y_label = cancer_data.target</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_data, y_label, test_size=0.2, random_state=0)</span><br></pre></td></tr></table></figure><hr><h5 id="개별-ML-모델-생성-및-메타-모델-생성"><a href="#개별-ML-모델-생성-및-메타-모델-생성" class="headerlink" title="개별 ML 모델 생성 및 메타 모델 생성"></a>개별 ML 모델 생성 및 메타 모델 생성</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># weak_learners</span></span><br><span class="line">knn_clf = KNeighborsClassifier(n_neighbors=4)</span><br><span class="line">rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)</span><br><span class="line">dt_clf = DecisionTreeClassifier()</span><br><span class="line">ada_clf = AdaBoostClassifier(n_estimators=100)</span><br><span class="line"></span><br><span class="line"><span class="comment"># meta Model</span></span><br><span class="line">lr_final = LogisticRegression(C=10)</span><br></pre></td></tr></table></figure><hr><h5 id="개별-모델-학습"><a href="#개별-모델-학습" class="headerlink" title="개별 모델 학습"></a>개별 모델 학습</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">knn_clf.fit(X_train, y_train)</span><br><span class="line">rf_clf.fit(X_train, y_train)</span><br><span class="line">dt_clf.fit(X_train, y_train)</span><br><span class="line">ada_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><hr><h5 id="개별-모델들이-반환하는-예측-데이터와-각-모델-별-정확도-측정"><a href="#개별-모델들이-반환하는-예측-데이터와-각-모델-별-정확도-측정" class="headerlink" title="개별 모델들이 반환하는 예측 데이터와 각 모델 별 정확도 측정"></a>개별 모델들이 반환하는 예측 데이터와 각 모델 별 정확도 측정</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">knn_pred = knn_clf.predict(X_test)</span><br><span class="line">rf_pred = rf_clf.predict(X_test)</span><br><span class="line">dt_pred = dt_clf.predict(X_test)</span><br><span class="line">ada_pred = ada_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'KNN 정확도: &#123;0:.4f&#125;'</span>.format(accuracy_score(y_test, knn_pred)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Random Forest 정확도: &#123;0:.4f&#125;'</span>.format(accuracy_score(y_test, rf_pred)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Decision Tree 정확도: &#123;0:.4f&#125;'</span>.format(accuracy_score(y_test, dt_pred)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'AdaBoost 정확도: &#123;0:.4f&#125;'</span>.format(accuracy_score(y_test, ada_pred)))</span><br></pre></td></tr></table></figure><h5 id="결과-14"><a href="#결과-14" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">KNN 정확도: 0.9211</span><br><span class="line">Random Forest 정확도: 0.9649</span><br><span class="line">Decision Tree 정확도: 0.9123</span><br><span class="line">AdaBoost 정확도: 0.9561</span><br></pre></td></tr></table></figure><hr><ul><li>개별 알고리즘으로부터 예측된 예측값을 column level로 옆으로 붙여서 피처 값으로 만들어, 최종 메타 모델인 로지스틱 회귀에서 학습 데이터로 사용할 것이다. 반환된 예측 데이터 세트는 1차원 형태의 ndarray이므로 먼저 반환된 예측 결과를 행 형태로 붙인 뒤, numpy의 transpose()를 이용해 행과 열 위치를 바꾼 ndarray로 변환하면 된다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pred=np.array([knn_pred, rf_pred, dt_pred, ada_pred]).T</span><br><span class="line">lr_final.fit(pred, y_test)</span><br><span class="line">final=lr_final.predict(pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"최종 메타 모델의 예측 정확도 : &#123;:.4f&#125;"</span>.format(accuracy_score(y_test, final)))</span><br></pre></td></tr></table></figure><h5 id="결과-15"><a href="#결과-15" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">최종 메타 모델의 예측 정확도 : 0.9737</span><br></pre></td></tr></table></figure><hr><h3 id="CV-기반의-스태킹"><a href="#CV-기반의-스태킹" class="headerlink" title="CV 기반의 스태킹"></a>CV 기반의 스태킹</h3><ul><li><p><code>과적합(overfitting) 방지를 위한 CV세트 기반의 스태킹</code>모델을 살펴보겠다. 앞의 마지막 메타 모델인 로지스틱 회귀 모델을 학습할 때 label 데이터 세트로 학습데이터가 아닌 테스트용 label 데이터 세트를 기반으로 학습했기 때문에 과적합 문제가 발생할 수 있다.</p></li><li><p>이는 다음과 같이 2단계의 step으로 구분될 수 있다.</p><ul><li>step 1) 각 모델별로 원본 train/test 데이터를 예측한 결과 값을 기반으로 메타 모델을 위한 train/test용 데이터를 생성한다.</li><li>step 2) step 1에서 개별 모델들이 예측한 train용 데이터를 모두 스태킹 형태로 합쳐서 메타 모델이 학습할 최종 train 데이터 세트를 생성한다. 마찬가지로 각 모델들이 예측한 test용 데이터를 모두 스태킹 형태로 합쳐서 메타 모델이 예측할 최종 테스트 데이터 세트를 생성한다. 메타 모델은 최종적으로 생성된 train 데이터 세트와 원본 train 데이터의 label 데이터를 기반으로 학습한 뒤, 최종적으로 생성된 test용 데이터 세트를 예측하고, 원본 test 데이터의 label 데이터를 기반으로 평가한다.</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold</span><br><span class="line">from sklearn.model_selection import StratifiedKFold</span><br><span class="line"></span><br><span class="line">def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):</span><br><span class="line">    <span class="comment"># 지정된 n_folds값으로 KFold 생성</span></span><br><span class="line">    kf=KFold(n_splits=n_folds, shuffle=False, random_state=0)</span><br><span class="line">    <span class="comment"># 추후에 meta model이 사용할 학습 데이터 반환을 위한 numpy array 초기화</span></span><br><span class="line">    train_fold_pred = np.zeros((X_train_n.shape[0], 1))</span><br><span class="line">    test_pred = np.zeros((X_test_n.shape[0], n_folds))</span><br><span class="line">    <span class="built_in">print</span>(model.__class__.__name__, <span class="string">'model 시작'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> folder_counter, (train_index, valid_index) <span class="keyword">in</span> enumerate(kf.split(X_train_n)):</span><br><span class="line">        <span class="comment"># 입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 세트 추출</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'\t 폴드 세트:'</span>, folder_counter, <span class="string">'시작'</span>)</span><br><span class="line">        X_tr = X_train_n[train_index]</span><br><span class="line">        y_tr = y_train_n[train_index]</span><br><span class="line">        X_te = X_train_n[valid_index]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행</span></span><br><span class="line">        model.fit(X_tr, y_tr)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장</span></span><br><span class="line">        train_fold_pred[valid_index, :]=model.predict(X_te).reshape(-1,1)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장</span></span><br><span class="line">        test_pred[:, folder_counter] = model.predict(X_test_n)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터 생성</span></span><br><span class="line">    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1, 1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터</span></span><br><span class="line">    <span class="built_in">return</span> train_fold_pred, test_pred_mean</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)</span><br><span class="line">rf_train, rf_test = get_stacking_base_datasets(rf_clf, X_train, y_train, X_test, 7)</span><br><span class="line">dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)</span><br><span class="line">ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)</span><br></pre></td></tr></table></figure><h5 id="결과-16"><a href="#결과-16" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">KNeighborsClassifier model 시작</span><br><span class="line"> 폴드 세트: 0 시작</span><br><span class="line"> 폴드 세트: 1 시작</span><br><span class="line"> 폴드 세트: 2 시작</span><br><span class="line"> 폴드 세트: 3 시작</span><br><span class="line"> 폴드 세트: 4 시작</span><br><span class="line"> 폴드 세트: 5 시작</span><br><span class="line"> 폴드 세트: 6 시작</span><br><span class="line">RandomForestClassifier model 시작</span><br><span class="line"> 폴드 세트: 0 시작</span><br><span class="line"> 폴드 세트: 1 시작</span><br><span class="line"> 폴드 세트: 2 시작</span><br><span class="line"> 폴드 세트: 3 시작</span><br><span class="line"> 폴드 세트: 4 시작</span><br><span class="line"> 폴드 세트: 5 시작</span><br><span class="line"> 폴드 세트: 6 시작</span><br><span class="line">DecisionTreeClassifier model 시작</span><br><span class="line"> 폴드 세트: 0 시작</span><br><span class="line"> 폴드 세트: 1 시작</span><br><span class="line"> 폴드 세트: 2 시작</span><br><span class="line"> 폴드 세트: 3 시작</span><br><span class="line"> 폴드 세트: 4 시작</span><br><span class="line"> 폴드 세트: 5 시작</span><br><span class="line"> 폴드 세트: 6 시작</span><br><span class="line">AdaBoostClassifier model 시작</span><br><span class="line"> 폴드 세트: 0 시작</span><br><span class="line"> 폴드 세트: 1 시작</span><br><span class="line"> 폴드 세트: 2 시작</span><br><span class="line"> 폴드 세트: 3 시작</span><br><span class="line"> 폴드 세트: 4 시작</span><br><span class="line"> 폴드 세트: 5 시작</span><br><span class="line"> 폴드 세트: 6 시작</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)</span><br><span class="line">Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'원본 학습 피처 데이터 Shape:'</span>, X_train.shape, <span class="string">'원본 테스트 피처 Shape:'</span>, X_test.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'스태킹 학습 피처 데이터 Shape:'</span>, Stack_final_X_train.shape, <span class="string">'스태킹 테스트 피처 데이터 Shape:'</span>, Stack_final_X_test.shape)</span><br></pre></td></tr></table></figure><h5 id="결과-17"><a href="#결과-17" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">원본 학습 피처 데이터 Shape: (455, 30) 원본 테스트 피처 Shape: (114, 30)</span><br><span class="line">스태킹 학습 피처 데이터 Shape: (455, 4) 스태킹 테스트 피처 데이터 Shape: (114, 4)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr_final.fit(Stack_final_X_train, y_train)</span><br><span class="line">stack_final = lr_final.predict(Stack_final_X_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'최종 메타 모델의 예측 정확도:&#123;0:.4f&#125;'</span>.format(accuracy_score(y_test, stack_final)))</span><br></pre></td></tr></table></figure><h5 id="결과-18"><a href="#결과-18" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">최종 메타 모델의 예측 정확도:0.9737</span><br></pre></td></tr></table></figure><hr>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/27/machine_learning_15/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 06 - 힙(heap)</title>
      <link>https://heung-bae-lee.github.io/2020/05/17/data_structure_07/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/17/data_structure_07/</guid>
      <pubDate>Sun, 17 May 2020 14:39:31 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;대표적인-데이터-구조8-힙&quot;&gt;&lt;a href=&quot;#대표적인-데이터-구조8-힙&quot; class=&quot;headerlink&quot; title=&quot;대표적인 데이터 구조8: 힙&quot;&gt;&lt;/a&gt;대표적인 데이터 구조8: 힙&lt;/h2&gt;&lt;h3 id=&quot;1-힙-Heap-이란&quot;&gt;&lt;a
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="대표적인-데이터-구조8-힙"><a href="#대표적인-데이터-구조8-힙" class="headerlink" title="대표적인 데이터 구조8: 힙"></a>대표적인 데이터 구조8: 힙</h2><h3 id="1-힙-Heap-이란"><a href="#1-힙-Heap-이란" class="headerlink" title="1. 힙 (Heap) 이란?"></a>1. 힙 (Heap) 이란?</h3><ul><li>힙: 데이터에서 <code>최대값과 최소값을 빠르게 찾기 위해 고안된 완전 이진 트리</code>(Complete Binary Tree)<ul><li>완전 이진 트리: 노드를 삽입할 때 최하단 왼쪽 노드부터 차례대로 삽입하는 트리</li></ul></li></ul><p><img src="https://www.fun-coding.org/00_Images/completebinarytree.png" width="300"></p><ul><li>힙을 사용하는 이유<ul><li>배열에 데이터를 넣고, 최대값과 최소값을 찾으려면 O(n) 이 걸림</li><li>이에 반해, 힙에 데이터를 넣고, 최대값과 최소값을 찾으면, $ O(log n) $ 이 걸림</li><li><code>우선순위 큐와 같이 최대값 또는 최소값을 빠르게 찾아야 하는 자료구조 및 알고리즘 구현 등에 활용됨</code></li></ul></li></ul><h3 id="2-힙-Heap-구조"><a href="#2-힙-Heap-구조" class="headerlink" title="2. 힙 (Heap) 구조"></a>2. 힙 (Heap) 구조</h3><ul><li>힙은 최대값을 구하기 위한 구조 (최대 힙, Max Heap) 와, 최소값을 구하기 위한 구조 (최소 힙, Min Heap) 로 분류할 수 있음</li><li>힙은 다음과 같이 두 가지 조건을 가지고 있는 자료구조임</li></ul><ol><li><code>각 노드의 값은 해당 노드의 자식 노드가 가진 값보다 크거나 같다. (최대 힙의 경우)</code><ul><li><code>최소 힙의 경우는 각 노드의 값은 해당 노드의 자식 노드가 가진 값보다 크거나 작음</code></li></ul></li><li><code>완전 이진 트리 형태를 가짐</code></li></ol><h3 id="힙과-이진-탐색-트리의-공통점과-차이점"><a href="#힙과-이진-탐색-트리의-공통점과-차이점" class="headerlink" title="힙과 이진 탐색 트리의 공통점과 차이점"></a>힙과 이진 탐색 트리의 공통점과 차이점</h3><ul><li>공통점: 힙과 이진 탐색 트리는 모두 이진 트리임</li><li>차이점:</li><li>힙은 각 노드의 값이 자식 노드보다 크거나 같음(Max Heap의 경우)</li><li>이진 탐색 트리는 왼쪽 자식 노드의 값이 가장 작고, 그 다음 부모 노드, 그 다음 오른쪽 자식 노드 값이 가장 큼</li><li><code>힙은 이진 탐색 트리의 조건인 자식 노드에서 작은 값은 왼쪽, 큰 값은 오른쪽이라는 조건은 없음</code><ul><li>힙의 왼쪽 및 오른쪽 자식 노드의 값은 오른쪽이 클 수도 있고, 왼쪽이 클 수도 있음</li></ul></li><li>이진 탐색 트리는 탐색을 위한 구조, 힙은 최대/최소값 검색을 위한 구조 중 하나로 이해하면 됨<br><img src="https://www.fun-coding.org/00_Images/completebinarytree_bst.png" width="800"></li></ul><h3 id="3-힙-Heap-동작"><a href="#3-힙-Heap-동작" class="headerlink" title="3. 힙 (Heap) 동작"></a>3. 힙 (Heap) 동작</h3><ul><li>데이터를 힙 구조에 삽입, 삭제하는 과정을 그림을 통해 선명하게 이해하기</li></ul><h3 id="힙에-데이터-삽입하기-기본-동작"><a href="#힙에-데이터-삽입하기-기본-동작" class="headerlink" title="힙에 데이터 삽입하기 - 기본 동작"></a>힙에 데이터 삽입하기 - 기본 동작</h3><ul><li>힙은 완전 이진 트리이므로, 삽입할 노드는 기본적으로 왼쪽 최하단부 노드부터 채워지는 형태로 삽입<br><img src="https://www.fun-coding.org/00_Images/heap_ordinary.png"></li></ul><h3 id="힙의-데이터-삭제하기-Max-Heap-의-예"><a href="#힙의-데이터-삭제하기-Max-Heap-의-예" class="headerlink" title="힙의 데이터 삭제하기 (Max Heap 의 예)"></a>힙의 데이터 삭제하기 (Max Heap 의 예)</h3><ul><li><code>보통 삭제는 최상단 노드 (root 노드)를 삭제하는 것이 일반적임</code><ul><li>힙의 용도는 최대값 또는 최소값을 root 노드에 놓아서, 최대값과 최소값을 바로 꺼내 쓸 수 있도록 하는 것임</li></ul></li><li>상단의 데이터 삭제시, 가장 최하단부 왼쪽에 위치한 노드 (일반적으로 가장 마지막에 추가한 노드) 를 root 노드로 이동</li><li>root 노드의 값이 child node 보다 작을 경우, root 노드의 child node 중 가장 큰 값을 가진 노드와 root 노드 위치를 바꿔주는 작업을 반복함 (swap)</li></ul><p><img src="https://www.fun-coding.org/00_Images/heap_remove.png"></p><h3 id="4-힙-구현"><a href="#4-힙-구현" class="headerlink" title="4. 힙 구현"></a>4. 힙 구현</h3><h3 id="힙과-배열"><a href="#힙과-배열" class="headerlink" title="힙과 배열"></a>힙과 배열</h3><ul><li>일반적으로 <code>힙 구현시 배열 자료구조를 활용</code>함</li><li>배열은 인덱스가 0번부터 시작하지만, 힙 구현의 편의를 위해, root 노드 인덱스 번호를 1로 지정하면, 구현이 좀더 수월함<ul><li>부모 노드 인덱스 번호 (parent node’s index) = 자식 노드 인덱스 번호 (child node’s index) // 2</li><li>왼쪽 자식 노드 인덱스 번호 (left child node’s index) = 부모 노드 인덱스 번호 (parent node’s index) * 2</li><li>오른쪽 자식 노드 인덱스 번호 (right child node’s index) = 부모 노드 인덱스 번호 (parent node’s index) * 2 + 1<br><img src="https://www.fun-coding.org/00_Images/heap_array.png" width="400"></li></ul></li></ul><h5 id="예1-10-노드의-부모-노드-인덱스"><a href="#예1-10-노드의-부모-노드-인덱스" class="headerlink" title="예1 - 10 노드의 부모 노드 인덱스"></a>예1 - 10 노드의 부모 노드 인덱스</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2 // 2</span><br></pre></td></tr></table></figure><hr><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><hr><h5 id="예1-15-노드의-왼쪽-자식-노드-인덱스-번호"><a href="#예1-15-노드의-왼쪽-자식-노드-인덱스-번호" class="headerlink" title="예1 - 15 노드의 왼쪽 자식 노드 인덱스 번호"></a>예1 - 15 노드의 왼쪽 자식 노드 인덱스 번호</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 * 2</span><br></pre></td></tr></table></figure><hr><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure><hr><h5 id="예1-15-노드의-오른쪽-자식-노드-인덱스-번호"><a href="#예1-15-노드의-오른쪽-자식-노드-인덱스-번호" class="headerlink" title="예1 - 15 노드의 오른쪽 자식 노드 인덱스 번호"></a>예1 - 15 노드의 오른쪽 자식 노드 인덱스 번호</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2 * 2 + 1</span><br></pre></td></tr></table></figure><hr><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5</span><br></pre></td></tr></table></figure><hr><h3 id="힙에-데이터-삽입-구현-Max-Heap-예"><a href="#힙에-데이터-삽입-구현-Max-Heap-예" class="headerlink" title="힙에 데이터 삽입 구현 (Max Heap 예)"></a>힙에 데이터 삽입 구현 (Max Heap 예)</h3><ul><li>힙 클래스 구현1</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class Heap:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        <span class="comment"># 배열 구조로 주로 하기 때문에</span></span><br><span class="line">        self.heap_array = list()</span><br><span class="line">        <span class="comment"># 완전 이진 트리의 성질을 이용하여 부모, 자식노드를 쉽게 찾기 위해</span></span><br><span class="line">        <span class="comment"># 인덱싱을 활용하기 위해 인덱스를 1부터 가져간다.</span></span><br><span class="line">        self.heap_array.append(None)</span><br><span class="line">        self.heap_array.append(data)</span><br></pre></td></tr></table></figure><hr><h5 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">heap = Heap(1)</span><br><span class="line">heap.heap_array</span><br></pre></td></tr></table></figure><hr><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[None, 1]</span><br></pre></td></tr></table></figure><hr><ul><li>힙 클래스 구현2 - insert1<ul><li>인덱스 번호는 1번부터 시작하도록 변경</li></ul></li></ul><p><img src="https://www.fun-coding.org/00_Images/heap_ordinary.png"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class Heap:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.heap_array = list()</span><br><span class="line">        self.heap_array.append(None)</span><br><span class="line">        self.heap_array.append(data)</span><br><span class="line"></span><br><span class="line">    def insert(self, data):</span><br><span class="line">        <span class="keyword">if</span> len(self.heap_array) == 0:</span><br><span class="line">            self.heap_array.append(None)</span><br><span class="line">            self.heap_array.append(data)</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line"></span><br><span class="line">        self.heap_array.append(data)</span><br><span class="line">        <span class="built_in">return</span> True</span><br></pre></td></tr></table></figure><hr><ul><li>힙 클래스 구현3 - insert2<ul><li>삽입한 노드가 부모 노드의 값보다 클 경우, 부모 노드와 삽입한 노드 위치를 바꿈</li><li>삽입한 노드가 루트 노드가 되거나, 부모 노드보다 값이 작거나 같을 경우까지 반복</li></ul></li></ul><hr><ul><li>특정 노드의 관련 노드 위치 알아내기<ul><li>부모 노드 인덱스 번호 (parent node’s index) = 자식 노드 인덱스 번호 (child node’s index) // 2</li><li>왼쪽 자식 노드 인덱스 번호 (left child node’s index) = 부모 노드 인덱스 번호 (parent node’s index) * 2</li><li>오른쪽 자식 노드 인덱스 번호 (right child node’s index) = 부모 노드 인덱스 번호 (parent node’s index) * 2 + 1</li></ul></li></ul><p><img src="https://www.fun-coding.org/00_Images/heap_insert.png"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">heap = Heap(15)</span><br><span class="line">heap.insert(10)</span><br><span class="line">heap.insert(8)</span><br><span class="line">heap.insert(5)</span><br><span class="line">heap.insert(4)</span><br><span class="line">heap.insert(20)</span><br><span class="line">heap.heap_array</span><br></pre></td></tr></table></figure><hr><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[None, 20, 10, 15, 5, 4, 8]</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">class Heap:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.heap_array = list()</span><br><span class="line">        self.heap_array.append(None)</span><br><span class="line">        self.heap_array.append(data)</span><br><span class="line"></span><br><span class="line">    def move_up(self, inserted_idx):</span><br><span class="line">        <span class="keyword">if</span> inserted_idx &lt;= 1:</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">        parent_idx = inserted_idx // 2</span><br><span class="line">        <span class="keyword">if</span> self.heap_array[inserted_idx] &gt; self.heap_array[parent_idx]:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def insert(self, data):</span><br><span class="line">        <span class="comment"># 배열에 데이터가 존재하지 않는다면 reset시켜줌.</span></span><br><span class="line">        <span class="keyword">if</span> len(self.heap_array) == 0:</span><br><span class="line">            self.heap_array.append(None)</span><br><span class="line">            self.heap_array.append(data)</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line"></span><br><span class="line">        self.heap_array.append(data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># None이 0번째 index에 존재하므로</span></span><br><span class="line">        <span class="comment"># index를 통해 부모,자식노드를 분별하기 쉽게 -1을 해줌</span></span><br><span class="line">        inserted_idx = len(self.heap_array) - 1</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 이제 index를 통해 heap 구조를 띄도록 추가한 노드의 값이</span></span><br><span class="line">        <span class="comment"># Max heap인 경우 부모노드보다 작은지를 확인한 후에 계속해서</span></span><br><span class="line">        <span class="comment"># 바꿔주게끔 함수 하나를 만들어 바꿔줘야하는 노드이면 True를 반환하여</span></span><br><span class="line">        <span class="comment"># while문을 반복할 수 있게끔 한다.</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> self.move_up(inserted_idx):</span><br><span class="line">            parent_idx = inserted_idx // 2</span><br><span class="line">            self.heap_array[inserted_idx], self.heap_array[parent_idx] = self.heap_array[parent_idx], self.heap_array[inserted_idx]</span><br><span class="line">            inserted_idx = parent_idx</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> True</span><br></pre></td></tr></table></figure><hr><h3 id="힙에-데이터-삭제-구현-Max-Heap-예"><a href="#힙에-데이터-삭제-구현-Max-Heap-예" class="headerlink" title="힙에 데이터 삭제 구현 (Max Heap 예)"></a>힙에 데이터 삭제 구현 (Max Heap 예)</h3><ul><li>힙 클래스 구현4 - delete1<ul><li><code>보통 삭제는 최상단 노드 (root 노드)를 삭제하는 것이 일반적임</code></li><li>힙의 용도는 최대값 또는 최소값을 root 노드에 놓아서, 최대값과 최소값을 바로 꺼내 쓸 수 있도록 하는 것임</li></ul></li></ul><ul><li>힙 클래스 구현4 - delete2<ul><li>상단의 데이터 삭제시, 가장 최하단부 왼쪽에 위치한 노드 (일반적으로 가장 마지막에 추가한 노드) 를 root 노드로 이동</li><li>root 노드의 값이 child node 보다 작을 경우, root 노드의 child node 중 가장 큰 값을 가진 노드와 root 노드 위치를 바꿔주는 작업을 반복함 (swap)</li></ul></li></ul><ul><li>특정 노드의 관련 노드 위치 알아내기<ul><li>부모 노드 인덱스 번호 (parent node’s index) = 자식 노드 인덱스 번호 (child node’s index) // 2</li><li>왼쪽 자식 노드 인덱스 번호 (left child node’s index) = 부모 노드 인덱스 번호 (parent node’s index) * 2</li><li>오른쪽 자식 노드 인덱스 번호 (right child node’s index) = 부모 노드 인덱스 번호 (parent node’s index) * 2 + 1</li></ul></li></ul><p><img src="https://www.fun-coding.org/00_Images/heap_remove.png"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">class Heap:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.heap_array=list()</span><br><span class="line">        self.heap_array.append(None)</span><br><span class="line">        self.heap_array.append(data)</span><br><span class="line"></span><br><span class="line">    def move_down(self, popped_idx):</span><br><span class="line">        left_child_popped_idx = popped_idx * 2</span><br><span class="line">        right_child_popped_idx = popped_idx * 2 + 1</span><br><span class="line"></span><br><span class="line">        <span class="comment"># case1: 왼쪽 자식 노드도 없을 때</span></span><br><span class="line">        <span class="keyword">if</span> left_child_popped_idx &gt;= len(self.heap_array):</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line">        <span class="comment"># case2: 오른쪽 자식 노드만 없을 때</span></span><br><span class="line">        <span class="keyword">elif</span> right_child_popped_idx &gt;= len(self.heap_array):</span><br><span class="line">            <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[left_child_popped_idx]:</span><br><span class="line">                <span class="built_in">return</span> True</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">return</span> False</span><br><span class="line">        <span class="comment"># case3: 왼쪽, 오른쪽 자식 노드 모두 있을 때</span></span><br><span class="line">        <span class="comment"># 자식노드가 둘다있다면, 먼저 자식노드끼리 비교한다.</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.heap_array[left_child_popped_idx] &gt; self.heap_array[right_child_popped_idx]:</span><br><span class="line">                <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[left_child_popped_idx]:</span><br><span class="line">                    <span class="built_in">return</span> True</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">return</span> False</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[right_child_popped_idx]:</span><br><span class="line">                    <span class="built_in">return</span> True</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def pop(self):</span><br><span class="line">        <span class="keyword">if</span> len(self.heap_array) &lt;= 1:</span><br><span class="line">            <span class="built_in">return</span> None</span><br><span class="line"></span><br><span class="line">        returned_data = self.heap_array[1]</span><br><span class="line">        self.heap_array[1] = self.heap_array[-1]</span><br><span class="line">        del self.heap_array[-1]</span><br><span class="line">        popped_idx = 1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> self.move_down(popped_idx):</span><br><span class="line">            left_child_popped_idx = popped_idx * 2</span><br><span class="line">            right_child_popped_idx = popped_idx * 2 + 1</span><br><span class="line"></span><br><span class="line">            <span class="comment"># case2: 오른쪽 자식 노드만 없을 때</span></span><br><span class="line">            <span class="keyword">if</span> right_child_popped_idx &gt;= len(self.heap_array):</span><br><span class="line">                <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[left_child_popped_idx]:</span><br><span class="line">                    self.heap_array[popped_idx], self.heap_array[left_child_popped_idx] = self.heap_array[left_child_popped_idx], self.heap_array[popped_idx]</span><br><span class="line">                    popped_idx = left_child_popped_idx</span><br><span class="line">            <span class="comment"># case3: 왼쪽, 오른쪽 자식 노드 모두 있을 때</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> self.heap_array[left_child_popped_idx] &gt; self.heap_array[right_child_popped_idx]:</span><br><span class="line">                    <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[left_child_popped_idx]:</span><br><span class="line">                        self.heap_array[popped_idx], self.heap_array[left_child_popped_idx] = self.heap_array[left_child_popped_idx], self.heap_array[popped_idx]</span><br><span class="line">                        popped_idx = left_child_popped_idx</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[right_child_popped_idx]:</span><br><span class="line">                        self.heap_array[popped_idx], self.heap_array[right_child_popped_idx] = self.heap_array[right_child_popped_idx], self.heap_array[popped_idx]</span><br><span class="line">                        popped_idx = right_child_popped_idx</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> returned_data</span><br></pre></td></tr></table></figure><hr><h3 id="Heap-구현"><a href="#Heap-구현" class="headerlink" title="Heap 구현"></a>Heap 구현</h3><ul><li>위의 모든 function들을 모아 하나의 class로 만들어 놓은 파일이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">class Heap:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.heap_array = list()</span><br><span class="line">        self.heap_array.append(None)</span><br><span class="line">        self.heap_array.append(data)</span><br><span class="line"></span><br><span class="line">    def move_down(self, popped_idx):</span><br><span class="line">        left_child_popped_idx = popped_idx * 2</span><br><span class="line">        right_child_popped_idx = popped_idx * 2 + 1</span><br><span class="line"></span><br><span class="line">        <span class="comment"># case1: 왼쪽 자식 노드도 없을 때</span></span><br><span class="line">        <span class="keyword">if</span> left_child_popped_idx &gt;= len(self.heap_array):</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line">        <span class="comment"># case2: 오른쪽 자식 노드만 없을 때</span></span><br><span class="line">        <span class="keyword">elif</span> right_child_popped_idx &gt;= len(self.heap_array):</span><br><span class="line">            <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[left_child_popped_idx]:</span><br><span class="line">                <span class="built_in">return</span> True</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">return</span> False</span><br><span class="line">        <span class="comment"># case3: 왼쪽, 오른쪽 자식 노드 모두 있을 때</span></span><br><span class="line">        <span class="comment"># 자식노드가 둘다있다면, 먼저 자식노드끼리 비교한다.</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.heap_array[left_child_popped_idx] &gt; self.heap_array[right_child_popped_idx]:</span><br><span class="line">                <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[left_child_popped_idx]:</span><br><span class="line">                    <span class="built_in">return</span> True</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">return</span> False</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[right_child_popped_idx]:</span><br><span class="line">                    <span class="built_in">return</span> True</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def pop(self):</span><br><span class="line">        <span class="keyword">if</span> len(self.heap_array) &lt;= 1:</span><br><span class="line">            <span class="built_in">return</span> None</span><br><span class="line"></span><br><span class="line">        returned_data = self.heap_array[1]</span><br><span class="line">        self.heap_array[1] = self.heap_array[-1]</span><br><span class="line">        del self.heap_array[-1]</span><br><span class="line">        popped_idx = 1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> self.move_down(popped_idx):</span><br><span class="line">            left_child_popped_idx = popped_idx * 2</span><br><span class="line">            right_child_popped_idx = popped_idx * 2 + 1</span><br><span class="line"></span><br><span class="line">            <span class="comment"># case2: 오른쪽 자식 노드만 없을 때</span></span><br><span class="line">            <span class="keyword">if</span> right_child_popped_idx &gt;= len(self.heap_array):</span><br><span class="line">                <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[left_child_popped_idx]:</span><br><span class="line">                    self.heap_array[popped_idx], self.heap_array[left_child_popped_idx] = self.heap_array[left_child_popped_idx], self.heap_array[popped_idx]</span><br><span class="line">                    popped_idx = left_child_popped_idx</span><br><span class="line">            <span class="comment"># case3: 왼쪽, 오른쪽 자식 노드 모두 있을 때</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> self.heap_array[left_child_popped_idx] &gt; self.heap_array[right_child_popped_idx]:</span><br><span class="line">                    <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[left_child_popped_idx]:</span><br><span class="line">                        self.heap_array[popped_idx], self.heap_array[left_child_popped_idx] = self.heap_array[left_child_popped_idx], self.heap_array[popped_idx]</span><br><span class="line">                        popped_idx = left_child_popped_idx</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> self.heap_array[popped_idx] &lt; self.heap_array[right_child_popped_idx]:</span><br><span class="line">                        self.heap_array[popped_idx], self.heap_array[right_child_popped_idx] = self.heap_array[right_child_popped_idx], self.heap_array[popped_idx]</span><br><span class="line">                        popped_idx = right_child_popped_idx</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> returned_data</span><br><span class="line"></span><br><span class="line">    def move_up(self, inserted_idx):</span><br><span class="line">        <span class="keyword">if</span> inserted_idx &lt;= 1:</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line">        parent_idx = inserted_idx // 2</span><br><span class="line">        <span class="keyword">if</span> self.heap_array[inserted_idx] &gt; self.heap_array[parent_idx]:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def insert(self, data):</span><br><span class="line">        <span class="keyword">if</span> len(self.heap_array) == 1:</span><br><span class="line">            self.heap_array.append(data)</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line"></span><br><span class="line">        self.heap_array.append(data)</span><br><span class="line">        inserted_idx = len(self.heap_array) - 1</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> self.move_up(inserted_idx):</span><br><span class="line">            parent_idx = inserted_idx // 2</span><br><span class="line">            self.heap_array[inserted_idx], self.heap_array[parent_idx] = self.heap_array[parent_idx], self.heap_array[inserted_idx]</span><br><span class="line">            inserted_idx = parent_idx</span><br><span class="line">        <span class="built_in">return</span> True</span><br></pre></td></tr></table></figure><hr><h4 id="Test-1"><a href="#Test-1" class="headerlink" title="Test"></a>Test</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">heap = Heap(15)</span><br><span class="line">heap.insert(10)</span><br><span class="line">heap.insert(8)</span><br><span class="line">heap.insert(5)</span><br><span class="line">heap.insert(4)</span><br><span class="line">heap.insert(20)</span><br><span class="line">heap.heap_array</span><br></pre></td></tr></table></figure><hr><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[None, 20, 10, 15, 5, 4, 8]</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heap.pop()</span><br></pre></td></tr></table></figure><hr><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">20</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">heap.heap_array</span><br></pre></td></tr></table></figure><hr><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[None, 15, 10, 8, 5, 4]</span><br></pre></td></tr></table></figure><hr><h3 id="5-힙-Heap-시간-복잡도"><a href="#5-힙-Heap-시간-복잡도" class="headerlink" title="5. 힙 (Heap) 시간 복잡도"></a>5. 힙 (Heap) 시간 복잡도</h3><ul><li>depth (트리의 높이) 를 h라고 표기한다면,</li><li>n개의 노드를 가지는 heap 에 데이터 삽입 또는 삭제시, 최악의 경우 root 노드에서 leaf 노드까지 비교해야 하므로 $h = log_2{n} $ 에 가까우므로, 시간 복잡도는 $ O(log{n}) $<ul><li>참고: 빅오 표기법에서 $log{n}$ 에서의 log의 밑은 10이 아니라, 2입니다.</li><li>한번 실행시마다, 50%의 실행할 수도 있는 명령을 제거한다는 의미. 즉 50%의 실행시간을 단축시킬 수 있다는 것을 의미함</li></ul></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/17/data_structure_07/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Linear combination, vector equation, Four views of matrix multiplication</title>
      <link>https://heung-bae-lee.github.io/2020/05/14/linear_algebra_03/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/14/linear_algebra_03/</guid>
      <pubDate>Thu, 14 May 2020 06:36:03 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Linear-combination-vector-equation-Four-views-of-matrix-multiplication&quot;&gt;&lt;a href=&quot;#Linear-combination-vector-equation-Four-views-of-m
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Linear-combination-vector-equation-Four-views-of-matrix-multiplication"><a href="#Linear-combination-vector-equation-Four-views-of-matrix-multiplication" class="headerlink" title="Linear combination, vector equation, Four views of matrix multiplication"></a>Linear combination, vector equation, Four views of matrix multiplication</h1><ul><li><p>아래 내용은 <a href="https://datascienceschool.net/view-notebook/04358acdcf3347fc989c4cfc0ef6121c/" target="_blank" rel="noopener">김도형 박사님의 선형대수 강의안</a> <a href="https://www.edwith.org/linearalgebra4ai/joinLectures/14072" target="_blank" rel="noopener">edwith의 인공지능을 위한 선형대수</a> 강의와 <a href="http://www.kocw.net/home/search/kemView.do?kemId=977757" target="_blank" rel="noopener">KOCW의 한양대학교 이상화 교수님의 선형대수학</a> 강의를 보고 정리한 내용이다.</p></li><li><p>앞서 언급했던 것과 같이 벡터에 스칼라를 곱하여 더한 조합이 선형 조합이다.</p></li></ul><p><img src="/image/what_is_linear_combination.png" alt="선형조합의 의미"></p><ul><li>아래 그림처럼 일반적으로 연립방정식으로 생각하기 보다는 column form으로 나타내 linear combination으로 나타내는 것이 solution을 찾는데 기하학적으로나 직관적으로 해석하기 더 쉽다. 이제 부터 그 이유에 대해서 살펴 볼 것이다.</li></ul><p><img src="/image/linear_combination_example_matrix_multiplication_01.png" alt="선형조합의 예시 - 01"></p><ul><li>이전에는 아래 수식에서 Unique한 Solution이 존재하려면 행렬 A에 대한 역행렬이 존재해야 한다고 설명했다. 수식적으로 이해를 했다면 이제는 <code>기하학적</code>으로 살펴보자.</li></ul><p><img src="/image/linear_combination_example_matrix_multiplication_02.png" alt="선형조합의 예시 - 02"></p><ul><li>Solution의 집합이 어떤 형태인지 어디에 속해있는지를 다루기 위해서는 먼저 <code>Vector Space에 대해 알고 있어야한다.</code></li></ul><h2 id="Vector-Space"><a href="#Vector-Space" class="headerlink" title="Vector Space"></a>Vector Space</h2><ul><li><p>space : set(집합) closed under addition and scalar multiplication</p><ul><li>즉, <code>덧셈과 곱셈에 대하여 닫혀있는 집합</code>을 Space라고 한다. 위에서의 <code>벡터의 선형조합을 보면 scalar와의 곱셈에의해 닫혀있고 벡터간의 덧셈에 대해 닫혀 있으므로 하나의 space를 갖는다고 볼 수 있다.</code></li></ul></li><li><p>좀 더 엄밀히 말하자면 다음을 만족해야 한다.</p></li></ul><ul><li><p>for any vectors $x, y \in R^{n}$, any scalar $c \in R$</p><ul><li>1) $x+y = y+x$</li><li>2) $x+(y+z) = (x+y)+z$</li><li>3) There is a zero-vector such that $x+o = o+x = x $<ul><li>여기서 0(zero-vector)는 벡터의 덧셈연산에 대한 항등원이다. 이 처럼 항등원이 있는지를 확인할 경우에는 항상, 교환법칙이 성립하는 지도 확인 해야한다.</li><li>위의 사실로 부터 vector Space가 되려면 원점을 포함해야한다는 사실을 알 수 있다.</li><li>참고로 원점을 포함하지 않는 공간을 아핀 공간(Affine Space)라고 한다.</li></ul></li><li>4) For each vectore $x, x+(-x) = (-x) + x = o$<ul><li>$-x$ 벡터의 덧셈 연산에 대한 역원이다. 또한 -x는 unique하다.</li></ul></li><li>5) $1 \cdot x=x$</li><li>6) $c(x+y) = cx+cy$</li><li>7) $(c_{1}+c_{2})x=c_{1}x+c_{2}x$</li></ul></li><li><p>이해가 잘 가지 않을 수 있기 때문에 예시를 들자면 2차 다항함수가 아래와 같이 존재할 때의 vector Space는 3차원이다.</p></li></ul><script type="math/tex; mode=display">\begin{align} a x^{2}+ b x +c \Rightarrow \begin{bmatrix} a \\ b \\ c \end{bmatrix} \in R^{3} \end{align}</script><h2 id="Subspace"><a href="#Subspace" class="headerlink" title="Subspace"></a>Subspace</h2><ul><li>임의의 벡터들에 대한 선형조합의 결과도 여전히 그 set에 대해 닫혀 있으면서도 전체 Vector Space에 부분집합을 Subspace라고 한다.</li></ul><p><img src="/image/vector_space_and_subspace_related_in_point.png" alt="Vector Space, Subspace 그리고 원점의 관계"></p><ul><li><p>임의의 벡터들을 가지고 온갖 종류의 linear combination을 해서 vector space를 만들어 내는 것을 Spanning이라고 하고, 공간을 만든 벡터들은 특정한 vector space를 span한다라고 표현한다.</p></li><li><p>all linear combination of vectors $\{ v_{1}, v_{2}, \cdots ,v_{n} \}$ construct a vector space</p><ul><li>$= \{v_{1}, v_{2}, \cdots ,v_{n} \}$ span vector space</li></ul></li></ul><p><img src="/image/what_is_span_on_vector_space.png" alt="Span의 개념"></p><ul><li><p>Span의 개념에 대한 예시는 다음과 같다. 아래와 같은 두 개의 vector를 갖고 어떤 형태로 linear combination을 하던지 span할 수 있는 vector 공간은 x, y 평면이라는 2차원 공간밖에 안된다. 각 벡터의 차원은 3차원이지만 전체 vector space의 차원은 2이다.</p></li><li><p>아래의 벡터들의 linear combination은 2차원 공간(x,y 평면)을 span한다. 똑같은 <code>vector space라고 하더라고 span하는 vector들의 조합은 무한</code>하다.</p></li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix} \end{align}</script><h3 id="Column-Space-of-A-C-A"><a href="#Column-Space-of-A-C-A" class="headerlink" title="Column Space of A (C(A))"></a>Column Space of A (C(A))</h3><ul><li><code>임의의 행렬 A의 열 벡터의 모든 선형조합(Linear combination)의 집합</code>을 <code>Column Space</code>라고 한다.</li></ul><script type="math/tex; mode=display">\begin{align} A = \begin{bmatrix} a_{1} & a_{2} & \cdots & a_{N} \end{bmatrix} \Rightarrow \sum^{n}_{i=1} c_{i} a_{i} \end{align}</script><script type="math/tex; mode=display">\begin{align} Ax= \begin{bmatrix} a_{1} & a_{2} & \cdots & a_{M} \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{M} \end{bmatrix} = x_1 a_1 + x_2 a_2 + \cdots + x_M a_M  = b \end{align}</script><ul><li>if $b \in C(A)$, there is at least one solution. else if $b \notin C(A)$<ul><li>벡터 $x$라는 해가 존재한다는 것은 모든 Combination 중 하나가 벡터 $b$이라는 이야기이므로 벡터 $b$가 column space에 속하면 해가 존재하는 것이고 속하지 않으면 해가 존재하지 않는다.</li></ul></li></ul><ul><li><code>span한 결과가 모든 space를 만드러 내느냐 그렇지 않는냐는 행렬 A의 inverse가 존재하느냐 안하는냐에 달려 있다.</code></li></ul><blockquote><p>$Ax=b$ 에서 unique한 solution이 존재할 조건은 임의의 행렬 $A$에 대하여 역행렬이 존재하는 것이다. 역행렬($A^{-1}$)이 존재한다면 항상 $b \in C(A)$이다.</p></blockquote><ul><li>left Null space와 상호 보완관계<ul><li>전체 Space의 차원 = Column space의 차원  + left Null space의 차원</li></ul></li></ul><p>Example) 아래와 같은 행렬 $A$는 역행렬(A^{-1})이 존재하지 않기에 모든 3차원 공간의 벡터를 표현할 수 없다. 그러므로 solution은 존재하지 않거나 무수히 많을 것이다.</p><script type="math/tex; mode=display">\begin{align} A= \begin{bmatrix} 1 & -2 & 1 \\ 2 & 1 & 1 \\ 0 & 0 & 0 \end{bmatrix} \end{align}</script><ul><li>아래 그림에서와 같이 non-zero 벡터이며 서로 상수배로 만들어질 수 없는 3차원 크기의 $v_{1}, v_{2}$ 벡터들은 3차원 상에서 평면을 span한다.</li></ul><p><img src="/image/column_space_of_v_01.png" alt="span 개념으로 보는 linear combination"></p><ul><li>만약 벡터 $b$가 column 벡터들의 linear combination으로 표현되어질 수 있다면, column space안에 벡터 $b$가 포함되어있다는 이야기가 된다.</li></ul><p><img src="/image/column_space_of_v_02.png" alt="span 개념으로 보는 해의 관계"></p><h3 id="Null-Space-of-A-N-A"><a href="#Null-Space-of-A-N-A" class="headerlink" title="Null Space of A (N(A))"></a>Null Space of A (N(A))</h3><ul><li><p>Set of vectors such that $Ax = 0$,</p><ul><li>$N(A)=\{ x| A x = 0 \}$</li></ul></li><li><p>space는 덧셈과 곱셈에 대하여 닫혀있어야 하므로 다음과 같이 증명해 보일 수 있다.</p></li><li><p>1) closed under addition</p><ul><li>for $Ax_{1}=0, Ax_{2}=0$ $x_{1}+x_{2} \in N(A)$이려면, $A(x_{1}+x_{2})=0$인지 확인해주면 된다.</li><li>$Ax_{1} +Ax_{2} = 0 + 0 = 0$ 이므로 $x_{1}+x_{2} \in N(A)$이다.</li></ul></li><li><p>2) closed under scalar multiplication</p><ul><li>for $Ax=0, for any c cx \in N(A)?$</li><li>$A(cx)=cAx=0$이므로 $cx \in N(A)$이다.</li></ul></li><li><p>Row space와 상호 보완관계</p><ul><li>전체 Space의 차원 = Row space의 차원  + Null space의 차원</li></ul></li></ul><ul><li>비대칭 구조인 rectangular system에서는 미지수하고 방정식의 개수가 똑같지 않은 비대칭적인 구조이기 때문에 해가 없을 때도 있겠지만 해가 존재하는 경우에는 무수히 많다. 무수히 많은 해 집합인 경우 $Ax=b$에 대한 해는 $Ax=0$인 방정식의 해집합과 상당히 연관이 높은데 그 이유는 $Ax=b$에 대한 해집합은 $Ax=0$인 경우에 대한 해인 Null space와 나머지로 나누어지기 때문이다.  </li></ul><script type="math/tex; mode=display">\begin{align} \boxed{\begin{matrix} \phantom{} & \phantom{} & \phantom{} & \phantom{} & \phantom{} \\ & & M & &\\ \phantom{} & \phantom{} & \phantom{} & \phantom{} & \phantom{} \\ \end{matrix}} \, \boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ v \\ \phantom{\LARGE\mathstrut} \end{matrix}} = \boxed{\begin{matrix} \phantom{} \\ Mv \\ \phantom{} \end{matrix}} \end{align}</script><script type="math/tex; mode=display">Rectangular system에서의 해집합 = Null space + particular solution</script><ul><li>지금부터 그에 대한 예시를 풀면서 이해해 볼 것이다. 아래 수식에 대한 해집합을 구해보자.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 3 & 2 \\ 2 & 6 & 9 & 7 \\ -1 & -3 & 3 & 4 \end{bmatrix} \begin{bmatrix} u \\ v \\ w \\ z \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0  \end{bmatrix} \end{align}</script><ul><li>가우스 소거법을 사용하여 행렬을 풀어준다.<ul><li>첫 번째 행에 2를 곱하여 두 번째 행에 빼준다.</li><li>첫 번째 행을 세 번째 행에 더해준다.</li><li>위의 2단계를 거치면 아래와 같은 행렬을 얻게 될 것이다.<ul><li>아래 행렬처럼 $ A_{2,2}=0 $이 되어 pivot이 되지 못하므로 그 다음 0이 아닌 수가 있는 $ A_{2,3} $이 pivot의 자리를 갖는다.</li></ul></li></ul></li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 3 & 2 \\ 0 & 0 & 3 & 3 \\ 0 & 0 & 6 & 6 \end{bmatrix} \end{align}</script><ul><li>두 번째 행에서 2를 곱해 세 번째 행에서 빼준다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 3 & 2 \\ 0 & 0 & 3 & 3 \\ 0 & 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} u \\ v \\ w \\ z \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0  \end{bmatrix} \end{align}</script><ul><li>모든 pivot의 자리를 1로 만들어 주어야 하므로 두 번째 행을 3으로 나누어 준다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 3 & 2 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 \end{bmatrix} \end{align}</script><ul><li>마지막으로 pivot이 존재하는 column에 pivot이 존재하는 자리를 제외하고는 0으로 만들어 주기 위해 위에서 소거해준 방법을 사용하여 정리해준다.<ul><li>소거해주기 위해 두 번째 행에 3을 곱한뒤 첫 번째 행에서 빼주면 아래와 같은 행렬을 얻을 수 있다.</li></ul></li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 0 & -1 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 \end{bmatrix} \end{align}</script><ul><li>최종적으로 얻은 행렬을 다시 처음 방정식에 맞춰서 정리하면 다음과 같다.<ul><li>pivot variable : u, w</li><li>free variable : v, z</li></ul></li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 0 & -1 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 \end{bmatrix} \begin{bmatrix} u \\ v \\ w \\ z \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0  \end{bmatrix} \end{align}</script><ul><li>위의 방정식을 풀어 써서 row form으로 바꿔 보면 아래와 같다. 또한 pivot variable을 free variable의 형태로 표현하면 행렬의 Null space를 구할 수 있다. 또한 pivot variable의 개수가 해당 행렬의 Null space의 차원이다. 예시의 행렬은 $Dim(N(A))=2$이다.</li></ul><script type="math/tex; mode=display">u + 3v -z =0 \\ w+z = 0</script><script type="math/tex; mode=display">u = =3v + z \\ w = -z</script><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} u \\ v \\ w \\ z \end{bmatrix} = \begin{bmatrix} -3v+z \\ v \\ -z \\ z \end{bmatrix} \end{align}</script><ul><li>위와 같이 방정식을 모두 풀지 않아도 그 전 Row Reduced form 행렬에서 Null space를 구할 수 있다.<ul><li>Row Reduced form에서 free variable들의 column에 계수들을 pivot variable인 경우에는 부호를 바꿔주고 자기 자신의 순서인 자리에는 1을 나머지 free variable 자리에는 0을 넣어주면 된다.</li></ul></li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 0 & -1 \\ 0 & 0 & 1 & 1 \\ 0 & 0 & 0 & 0 \end{bmatrix} \end{align}</script><ul><li>자세하게 설명하자면 <code>pivot variable을 free variable의 형태로 표현</code>하는 것이므로 먼저 free variable의 column인 두 번째 열과 네 번째 열만 가져와 위의 방법 처럼 진행하면 아래와 같은 두 free variable 벡터의 조합을 구할 수 있으며 이 조합이 Null space이다.</li></ul><script type="math/tex; mode=display">\begin{align} v \begin{bmatrix} -3 \\ 1 \\ 0 \\ 0 \end{bmatrix} + z \begin{bmatrix} 1 \\ 0 \\ -1 \\ 1 \end{bmatrix} \in N(A) \end{align}</script><ul><li>만약 위의 예시 문제가 $Ax=b$를 푸는 문제라고 가정해보자.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 3 & 2 \\ 2 & 6 & 9 & 7 \\ -1 & -3 & 3 & 4 \end{bmatrix} \begin{bmatrix} u \\ v \\ w \\ z \end{bmatrix} = \begin{bmatrix} b_{1} \\ b_{2} \\ b_{3}  \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 3 & 2 | b_{1} \\ 0 & 0 & 3 & 3 | b_{2} - 2b_{1} \\ 0 & 0 & 6 & 6 | b_{3}+b_{1} \end{bmatrix} \end{align}</script><ul><li>행렬 A의 column vector들의 linear combination이 $b$이어야 하므로 해가 존재한다면 $b \in C(A)$일 것이다. 그런데 여기서는 그 $b$라는 벡터가 $5b_{1} - 2b_{2} + b_{3}=0$임을 만족해야 해가 존재한다. 그 이유는 아래 행렬을 살펴보면 마지막 행은 미지수들과 곱을 해도 0이 되기 때문이다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 3 & 2 | b_{1} \\ 0 & 0 & 3 & 3 | b_{2} - 2b_{1} \\ 0 & 0 & 0 & 0 | 5b_{1} - 2b_{2} + b_{3} \end{bmatrix} \end{align}</script><ul><li>$5b_{1} - 2b_{2} + b_{3}=0$를 만족하는 벡터 $b$는 해를 찾을 수 있으므로 조건을 만족시키는 조합만 찾는다면 해를 구할 수 있다. 그러므로 해는 무한하다. 아래의 예시에서는 벡터 $b$를 다음과 같이 정의 할 것이다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1  \\ 5 \\ 5 \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 3 & 2 | 1 \\ 0 & 0 & 3 & 3 | 3 \\ 0 & 0 & 0 & 0 | 0 \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 3 & 2 | 1 \\ 0 & 0 & 1 & 1 | 1 \\ 0 & 0 & 0 & 0 | 0 \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 1 & 3 & 0 & -1 | -2 \\ 0 & 0 & 1 & 1 | 1 \\ 0 & 0 & 0 & 0 | 0 \end{bmatrix} \end{align}</script><blockquote><p>$ X =  Null space + particular solution$으로 이루어져있음을 확인 할 수 있다. 원래 원점을 지나는 평면인 Null space이었는데 particular solution만큼 평생이동된 평면이 해 집합이 되었다는 것으로 해석 할 수 있다.</p></blockquote><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} u \\ v \\ w \\ z \end{bmatrix} = v \begin{bmatrix} -3 \\ 1 \\ 0 \\ 0 \end{bmatrix} + z \begin{bmatrix} 1 \\ 0 \\ -1 \\ 1 \end{bmatrix} + \begin{bmatrix} -2 \\ 0 \\ 1 \\ 0 \end{bmatrix} \in N(A) \end{align}</script><ul><li>행렬의 곱을 이제는 linear combination의 관점으로 계산하는 방법에 대해 살펴보자. 아래 그림에서 보듯이 행렬을 column vector로 나누어 linear combination으로 행렬의 곱을 볼 수 있다.</li></ul><p><img src="/image/matrix_multiplication_using_column_vectors_linear combination_01.png" alt="행렬의 곱을 column vector linear combination으로 생각하기 - 01"></p><p><img src="/image/matrix_multiplication_using_column_vectors_linear combination_02.png" alt="행렬의 곱을 column vector linear combination으로 생각하기 - 02"></p><ul><li>아래와 같이 column vector말고 row vector를 사용한 곱의 방법을 사용할 수도 있다. 위의 경우와의 차이는 <code>왼쪽의 행렬에 관한 관점으로 바라볼때는 column vector들의 linear combination 오른쪽의 행렬의 관점으로 곱을 계산할 경우는 row vector들의 linear combination으로 계산 할 수 있다.</code></li></ul><p><img src="/image/matrix_multiplication_using_row_vectors_linear_combination.png" alt="행렬의 곱을 row vector linear combination으로 생각하기"></p><ul><li>위의 개념들을 생각하여 외적을 계산할 수 있다. 아래 그림에서와 같이 두 행렬의 외적을 2부분으로 나누어 계산하면 손쉽게 계산할 수 있다.</li></ul><p><img src="/image/how_to_outer_product_matrix_multiplication.png" alt="외적을 계산하는 방법"></p><p><img src="/image/sum_of_outer_products.png" alt="머신러닝에서 외적이 사용되는 개념들"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/14/linear_algebra_03/#disqus_thread</comments>
    </item>
    
    <item>
      <title>선형 시스템(Linear system)</title>
      <link>https://heung-bae-lee.github.io/2020/05/13/linear_algebra_02/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/13/linear_algebra_02/</guid>
      <pubDate>Wed, 13 May 2020 07:50:33 GMT</pubDate>
      <description>
      
        
        
          &lt;ul&gt;
&lt;li&gt;아래 내용은 &lt;a href=&quot;https://datascienceschool.net/view-notebook/04358acdcf3347fc989c4cfc0ef6121c/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;김도형 박
        
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>아래 내용은 <a href="https://datascienceschool.net/view-notebook/04358acdcf3347fc989c4cfc0ef6121c/" target="_blank" rel="noopener">김도형 박사님의 선형대수 강의안</a> <a href="https://www.edwith.org/linearalgebra4ai/joinLectures/14072" target="_blank" rel="noopener">edwith의 인공지능을 위한 선형대수</a> 강의와 <a href="http://www.kocw.net/home/search/kemView.do?kemId=977757" target="_blank" rel="noopener">KOCW의 한양대학교 이상화 교수님의 선형대수학</a> 강의를 보고 정리한 내용이다.</li></ul><h2 id="선형-조합"><a href="#선형-조합" class="headerlink" title="선형 조합"></a>선형 조합</h2><p><img src="/image/linear_equation.png" alt="선형 조합의 개념 - 01"></p><p><img src="/image/linear_equation_01.png" alt="선형 조합의 개념 - 02"></p><ul><li>벡터/행렬에 다음처럼 스칼라값을 곱한 후 더하거나 뺀 것을 벡터/행렬의 선형조합(linear combination)이라고 한다. 벡터나 행렬을 선형조합해도 크기는 변하지 않는다.</li></ul><script type="math/tex; mode=display">\begin{align} c_1x_1 + c_2x_2 + c_3x_3 + \cdots + c_Lx_L = x \end{align}</script><script type="math/tex; mode=display">\begin{align} c_1A_1 + c_2A_2 + c_3A_3 + \cdots + c_LA_L = A \end{align}</script><script type="math/tex; mode=display">\begin{align} c_1, c_2, \ldots, c_L \in \mathbf{R} \end{align}</script><script type="math/tex; mode=display">\begin{align} x_1, x_2, \ldots, x_L, x \in \mathbf{R}^M \end{align}</script><script type="math/tex; mode=display">\begin{align} A_1, A_2, \ldots, A_L, A \in \mathbf{R}^{M \times N} \end{align}</script><ul><li>벡터나 행렬의 크기를 직사각형으로 표시하면 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{matrix} c_1\,\boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ x_1 \\ \phantom{\LARGE\mathstrut} \end{matrix}} & + & c_2\,\boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ x_2 \\ \phantom{\LARGE\mathstrut} \end{matrix}} & + & \cdots \!\!\!\!& + & c_L\,\boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ x_L \\ \phantom{\LARGE\mathstrut} \end{matrix}} \end{matrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} \begin{matrix} c_1\,\boxed{\begin{matrix} \phantom{} & \phantom{} & \phantom{} \\ & A_1 & \\ \phantom{} & \phantom{} & \phantom{} \end{matrix}} & + & c_2\,\boxed{\begin{matrix} \phantom{} & \phantom{} & \phantom{} \\ & A_2 & \\ \phantom{} & \phantom{} & \phantom{} \end{matrix}} & + & \cdots & + & c_L\,\boxed{\begin{matrix} \phantom{} & \phantom{} & \phantom{} \\ & A_L & \\ \phantom{} & \phantom{} & \phantom{} \end{matrix}} \end{matrix} \end{align}</script><h3 id="행렬과-벡터의-곱"><a href="#행렬과-벡터의-곱" class="headerlink" title="행렬과 벡터의 곱"></a>행렬과 벡터의 곱</h3><ul><li>행렬의 곱셈 중 가장 많이 사용되는 것은 아래와 같은 형태의 행렬 $M$과 벡터 $v$의 곱이다.</li></ul><script type="math/tex; mode=display">\begin{align} \boxed{\begin{matrix} \phantom{} & \phantom{} & \phantom{} & \phantom{} & \phantom{} \\ & & M & &\\ \phantom{} & \phantom{} & \phantom{} & \phantom{} & \phantom{} \\ \end{matrix}} \, \boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ v \\ \phantom{\LARGE\mathstrut} \end{matrix}} = \boxed{\begin{matrix} \phantom{} \\ Mv \\ \phantom{} \end{matrix}} \end{align}</script><h3 id="열-벡터의-선형조합"><a href="#열-벡터의-선형조합" class="headerlink" title="열 벡터의 선형조합"></a>열 벡터의 선형조합</h3><ul><li>행렬 X와 벡터 $w$의 곱은 행렬 $X$를 이루는 열벡터 $c_{1}, c_{2}, \ldots ,c_{M}$을 뒤에 곱해지는 벡터 $w$의 각 성분 $w_{1}, w_{1}, \ldots, w_{M}$으로 선형조합(linear combination)을 한 결과 벡터와 같다.</li></ul><script type="math/tex; mode=display">\begin{align} Xw= \begin{bmatrix} c_1 & c_2 & \cdots & c_M \end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_M \end{bmatrix} = w_1 c_1 + w_2 c_2 + \cdots + w_M c_M \end{align}</script><ul><li>벡터의 크기를 직사각형으로 표시하면 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} \boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ c_{1_{\phantom{1}}} \\ \phantom{\LARGE\mathstrut} \end{matrix}} & \boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ c_{2_{\phantom{1}}} \\ \phantom{\LARGE\mathstrut} \end{matrix}} & \cdots & \boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ c_M \\ \phantom{\LARGE\mathstrut} \end{matrix}} \end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_M \end{bmatrix} \\ &=& \begin{matrix} w_1\,\boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ c_{1_{\phantom{1}}} \\ \phantom{\LARGE\mathstrut} \end{matrix}} & + & w_2\,\boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ c_{2_{\phantom{1}}} \\ \phantom{\LARGE\mathstrut} \end{matrix}}& + & \cdots \!\!\!\!& + & w_M\,\boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ c_M \\ \phantom{\LARGE\mathstrut} \end{matrix}} \end{matrix} \end{align}</script><ul><li>벡터의 선형조합은 다양한 분야에 응용된다. 예를 들어 두 이미지 벡터의 선형조합은 두 이미지를 섞어놓은 모핑(morphing)효과를 얻는 데 사용할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_olivetti_faces</span><br><span class="line"></span><br><span class="line">faces = fetch_olivetti_faces()</span><br><span class="line"></span><br><span class="line">f, ax = plt.subplots(1, 3)</span><br><span class="line"></span><br><span class="line">ax[0].imshow(faces.images[6], cmap=plt.cm.bone)</span><br><span class="line">ax[0].grid(False)</span><br><span class="line">ax[0].set_xticks([])</span><br><span class="line">ax[0].set_yticks([])</span><br><span class="line">ax[0].set_title(<span class="string">"image 1: <span class="variable">$x_1</span>$"</span>)</span><br><span class="line"></span><br><span class="line">ax[1].imshow(faces.images[10], cmap=plt.cm.bone)</span><br><span class="line">ax[1].grid(False)</span><br><span class="line">ax[1].set_xticks([])</span><br><span class="line">ax[1].set_yticks([])</span><br><span class="line">ax[1].set_title(<span class="string">"image 2: <span class="variable">$x_2</span>$"</span>)</span><br><span class="line"></span><br><span class="line">new_face = 0.7 * faces.images[6] + 0.3 * faces.images[10]</span><br><span class="line">ax[2].imshow(new_face, cmap=plt.cm.bone)</span><br><span class="line">ax[2].grid(False)</span><br><span class="line">ax[2].set_xticks([])</span><br><span class="line">ax[2].set_yticks([])</span><br><span class="line">ax[2].set_title(<span class="string">"image 3: <span class="variable">$0</span>.7x_1 + 0.3x_2$"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/olivetti_faces_linear_combinations.png" alt="올리베티 이미지 벡터의 선형조합"></p><h3 id="선형-시스템-예시"><a href="#선형-시스템-예시" class="headerlink" title="선형 시스템 예시"></a>선형 시스템 예시</h3><h4 id="여러-개의-벡터에-대한-가중합-동시-계산"><a href="#여러-개의-벡터에-대한-가중합-동시-계산" class="headerlink" title="여러 개의 벡터에 대한 가중합 동시 계산"></a>여러 개의 벡터에 대한 가중합 동시 계산</h4><ul><li>벡터 하나의 가중합은 $w^{T} x$ 또는 $x^{T}w$로 표시할 수 있다는 것을 배웠다. 그런데 만약 이렇게 $w$가중치를 사용한 가중합을 하나의 벡터 $x$가 아니라 여러 벡터 $x_{1}, x_{2}, \ldots, x_{M}$개에 대해서 모두 계산해야 한다면 어떻게 해야 할까? 예를 들어 선형회귀 모형을 사용하여 여러 데이터 $x_{1}, x_{2}, \ldots, x_{N}$개의 데이터 모두에 대해 예측값 $y_{1}, y_{2}, \ldots, y_{M}$을 한꺼번에 계산하고 싶다면 다음처럼 데이터 행렬 $X$를 사용하여 $\hat{y} = Xw$라는 수식으로 간단하게 표현할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \hat{y} = \begin{bmatrix} \hat{y}_1 \\ \hat{y}_2 \\ \vdots \\ \hat{y}_M \\ \end{bmatrix} &= \begin{bmatrix} w_1 x_{1,1} + w_2 x_{1,2} + \cdots + w_N x_{1,N} \\ w_1 x_{2,1} + w_2 x_{2,2} + \cdots + w_N x_{2,N} \\ \vdots  \\ w_1 x_{M,1} + w_2 x_{M,2} + \cdots + w_N x_{M,N} \\ \end{bmatrix} \\ &= \begin{bmatrix} x_{1,1} & x_{1,2} & \cdots & x_{1,N} \\ x_{2,1} & x_{2,2} & \cdots & x_{2,N} \\ \vdots  & \vdots  & \vdots & \vdots \\ x_{M,1} & x_{M,2} & \cdots & x_{M,N} \\ \end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_N \end{bmatrix} \\ &= \begin{bmatrix} x_1^T \\ x_2^T \\ \vdots \\ x_M^T \\ \end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \\ \vdots \\ w_N \end{bmatrix} \\ &= X w \end{aligned} \end{align}</script><ul><li>아래 그림과 같이 몸무게, 키 그리고 흡연여부를 통해 기대 수명을 예측하고자 한다고 가정하자. 그렇다면 각 feature마다 갖는 가중치들을 구해야 할 것이다. 3개의 연립방정식을 푸는 것과 동일한 문제이다.</li></ul><p><img src="/image/Linear_system_example_01.png" alt="선형 시스템의 예시 - 01"></p><ul><li>이 문제를 행렬을 통해 나타내면 아래와 같다.</li></ul><p><img src="/image/Linear_system_example_02.png" alt="선형 시스템의 예시 - 02"></p><ul><li>결국에는 <code>행렬과 벡터의 곱을 통해 예측</code>하는 방법이다. 즉, 회귀(regression)를 하는 것이다. 마지막 질문처럼 어떻게 벡터 $x$를 구할 수 있을까?</li></ul><p><img src="/image/Linear_system_example_03.png" alt="선형 시스템의 예시 - 03"></p><ul><li>역행렬을 통해 구할 수 있다. 간단히 2x2의 행렬에 대한 역행렬은 다음과 같다. 여기서 <code>주의할 점</code>은 $AA^{-1} = A^{-1}A$가 성립되어야 하는 전제조건이라는 점이다. 모든 $A_{nxn}$ 행렬이 역행렬을 갖지는 않는다. $det(A_{nxn}) \neq 0$일 때만 역행렬이 존재하게 된다.</li></ul><p><img src="/image/Inverse_matrix_conception.png" alt="역행렬의 개념"></p><ul><li><p>참고로 <code>대각행렬의 역행렬은 각 대각원소의 역수가 되어 역행렬을 구하기 쉽기 때문에 후에 행렬의 분해를 할때 대각 행렬을 자주 사용</code>한다.</p></li><li><p>역행렬이 존재하는 경우에는 아래와 같이 역행렬을 계산하여 간단히 문제를 해결 할 수 있다.</p></li></ul><p><img src="/image/solving_with_inverse_matrix.png" alt="역행렬이 존재하여 solution이 unique한 경우"></p><p><img src="/image/Solving_Linear_System_via_Inverse_Matrix_01.png" alt="역행렬을 통한 해 찾기 - 01"></p><p><img src="/image/Solving_Linear_System_via_Inverse_Matrix_02.png" alt="역행렬을 통한 해 찾기 - 02"></p><ul><li>역행렬이 존재한다면의 가정아래 설명하였기에 그렇다면 역행렬이 존재하지 않는 경우에는 어떻게 해결하여야 할 지 의문이 들 것이다. 먼저, 선형 시스템에서의 해를 구할 경우에는 크게 3가지가 존재한다.<ul><li>1) Unique solution</li><li>2) No Solution</li><li>3) Infinity Solutions</li></ul></li></ul><ul><li>위에서 언급한 문제의 경우에는 미지수의 개수(벡터 $x$의 차원)와 방정식의 개수가 동일하며, 정방행렬에 대한 determinant가 존재하여 역행렬이 계산될 수 있어 Unique한 Solution을 갖는다. 여기서 주의할 것은 <code>역행렬을 갖는다면 Unique한 Solution을 갖는다</code>는 점이다.</li></ul><p><img src="/image/Inverse_matrix_realted_in_solution.png" alt="역행렬의 존재 여부와 해의 관계"></p><ul><li><code>determinant가 0이 아니어야 역행렬이 존재한다.</code> 아래 주소를 통해 길버트 스트랭 교수님의 강의에서 determinant의 개념을 좀 더 자세히 알 수 있다. 혹시 영어 강의라 어려움이 있으신 분들은 <a href="https://datascienceschool.net/view-notebook/d6205659aff0413797c22552947aec83/" target="_blank" rel="noopener">김도형 박사님 강의안 - 행렬의 성질</a>을 참고하길 권한다.</li></ul><p><img src="/image/inverse_matrix_determinant_conception.png" alt="역행렬이 존재하기 위한 조건 - determinant"></p><ul><li><code>determinant=0이기 때문에 역행렬이 존재하지 않는 경우에는 해가 무수히 많거나, 해가 존재하지 않는 경우</code>이다.</li></ul><p><img src="/image/Non_invertible_matrix_A_for_Ax_equal_b_01.png" alt="역행렬이 존재하지 않는 경우의 해"></p><ul><li>우리가 접하는 데이터의 shape은 대부분 정방행렬로 이루어져 있지 않다. 그렇다면 <code>정방행렬이 아닌 경우에는 역행렬을 구할 수 없으므로 어떻게 해를 구할 수 있을까라는 의문이 들 것이다.</code>우선 간단히 예전에 배웠던 경험을 토대로 생각해 보자.</li></ul><p><img src="/image/Non_invertible_matrix_A_for_Ax_equal_b_02.png" alt="직사각행렬에서의 해"></p><ul><li>미지수의 수와 방정식의 수를 고려해 볼 때 연립방정식에는 다음과 같은 세 종류가 있을 수 있다.<ul><li><ol><li>방정식의 수가 미지수의 수와 같다. (m=n)</li></ol></li><li><ol><li>방정식의 수가 미지수의 수보다 적다. (m &lt; n)</li></ol></li><li><ol><li>방정식의 수가 미지수의 수보다 많다. (m &gt; n)</li></ol></li></ul></li></ul><ul><li>1번의 경우, 즉 방정식의 수가 미지수의 수와 같은 경우는 앞에서 다루었다. 2번의 경우, 방정식의 개수가 미지수의 개수보다 적다면 풀려는 미지수의 개수가 더 많기 때문에 해가 무수히 많이 존재할 것이다. 아래의 수식에서 미지수는 3개지만 방정식은 2개 뿐이다. 이 때는 $x_{2}$가 어떤 값이 되더라도 $x_{1}=x_{3}=2-x_{2}$만 만족하면 되므로 무한히 많은 해가 존재한다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{matrix} x_1 & + & x_2 &   &     & = & 2  \\ &   & x_2 & + & x_3 & = & 2  \\ \end{matrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} x = \begin{bmatrix} 2 \\ 0 \\ 2 \end{bmatrix} ,\;\; x = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix} ,\;\; x = \begin{bmatrix} 0 \\ 2 \\ 0 \end{bmatrix} ,\;\; \cdots \end{align}</script><ul><li>3번의 경우, 반대로 방정식의 개수가 미지수의 개수보다 많다면 풀려는 미지수의 개수보다 적기 때문에 해가 없을 것이다. 예를 들어 다음 선형 연립방정식을 생각해 보자. 미지수는 3개지만 방정식은 4개다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{matrix} x_1 & + & x_2 &   &     & = & 2  \\ &   & x_2 & + & x_3 & = & 2  \\ x_1 & + & x_2 & + & x_3 & = & 3  \\ x_1 & + & x_2 & + & 2x_3 & = & 5  \\ \end{matrix} \end{align}</script><ul><li>위의 3개 방정식을 동시에 만족하는 해는 $x_{1}=x_{2}=x_{3}=1$인데 이 값은 4번째 방정식을 만족하지 못한다. 따라서 4개의 방정식을 모두 만족하는 해는 존재하지 않는다.</li></ul><script type="math/tex; mode=display">\begin{align} x_1 + x_2 + 2x_3 = 4 \end{align}</script><ul><li>선형 예측모형을 구하는 문제는 계수행렬이 특징행렬 $X$, 미지수 벡터가 가중치 벡터 $w$인 선형 연립방정식 문제이다. 그런데 보통 데이터의 수는 입력차원보다 큰 경우가 많다. 예를 들어 면적, 층수, 한강이 보이는지의 여부로 집값을 결정하는 모형을 만들기 위해서 딱 3가구의 아파트 가격만 조사하는 경우는 없을 것이다. 보통은 10가구 혹은 100가구의 아파트 가격을 수집하여 이용하는 것이 일반적이다. 다시 말해 선형 예측 모형을 구할 때는 3번과 같은 경우가 많다는 것을 알 수 있다. 이떄는 <code>선형 연립방정식의 해가 존재하지 않으므로 선형 연립방정식을 푸는 방식으로는 선형 예측모형의 가중치 벡터를 구할 수 없다.</code></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/13/linear_algebra_02/#disqus_thread</comments>
    </item>
    
    <item>
      <title>선형대수 요소(Elements in linear algebra)</title>
      <link>https://heung-bae-lee.github.io/2020/05/12/linear_algebra_01/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/12/linear_algebra_01/</guid>
      <pubDate>Tue, 12 May 2020 13:41:59 GMT</pubDate>
      <description>
      
        
        
          &lt;ul&gt;
&lt;li&gt;아래 내용은 &lt;a href=&quot;https://datascienceschool.net/view-notebook/04358acdcf3347fc989c4cfc0ef6121c/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;김도형 박
        
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>아래 내용은 <a href="https://datascienceschool.net/view-notebook/04358acdcf3347fc989c4cfc0ef6121c/" target="_blank" rel="noopener">김도형 박사님의 선형대수 강의안</a> <a href="https://www.edwith.org/linearalgebra4ai/joinLectures/14072" target="_blank" rel="noopener">edwith의 인공지능을 위한 선형대수</a> 강의와 <a href="http://www.kocw.net/home/search/kemView.do?kemId=977757" target="_blank" rel="noopener">KOCW의 한양대학교 이상화 교수님의 선형대수학</a> 강의를 보고 정리한 내용이다.</li></ul><h2 id="Linearity-선형성"><a href="#Linearity-선형성" class="headerlink" title="Linearity(선형성)"></a>Linearity(선형성)</h2><ul><li><p>행렬로 표현할 수 있는 것들은 기본적으로 다 선형성이라는 것을 만족해야 한다. 선형성은 아래 두 가지 조건을 만족할 때 선형성을 갖는다고 할 수 있다.</p></li><li><p>1) Superposition(중첩의 원리)</p></li></ul><script type="math/tex; mode=display">f(x_{1} + x_{2}) = f(x_{1}) + f(x_{2})</script><ul><li>2) Homogeniety</li></ul><script type="math/tex; mode=display">f(ax) = af(x), a는 constant(상수)</script><blockquote><p>예시 1)$y=mx=f(x)$; 원점을 지나는 직선 -&gt; Linearity를 결정하는 중요한 조건으로 원점을 지나는지를 확인해야 한다. 직선이라고 해서 다 Linearity를 만족하는 것은 아니다.</p></blockquote><script type="math/tex; mode=display">m(a_{1}x_{1} + a_{2}x_{2}) = a_{1}mx_{1} + a_{2}mx_{2} = a_{1}f(x_{1}) +  a_{2}f(x_{2})</script><blockquote><p>예시 2) $y = mx+n = f(x), n \neq 0$</p></blockquote><script type="math/tex; mode=display">m(a_{1}x_{1} + a_{2}x_{2}) + n \neq a_{1}(m x_{1} + n) + a_{2}(m x_{2} + n)</script><ul><li>위와 같으므로 선형성을 갖지 않는다는 것을 알 수 있다.</li></ul><h1 id="Elements-of-Linear-Algebra"><a href="#Elements-of-Linear-Algebra" class="headerlink" title="Elements of Linear Algebra"></a>Elements of Linear Algebra</h1><p><img src="/image/linear_algebra_elements.png" alt="선형대수의 요소"></p><ul><li>선형대수에서 다루는 데이터는 개수나 형태에 따라 크게 스칼라(scalar), 벡터(vector), 행렬(matrix), 텐서(tensor) 유형으로 나뉜다. 스칼라는 숫자 하나로 이루어진 데이터이고, 벡터는 여러 숫자로 이루어진 데이터 레코드(data record)이며, 행렬은 이러한 벡터, 즉 데이터 레코드가 여럿인 데이터 집합이라고 볼 수 있다. 텐서는 같은 크기의 행렬이 여러 개 있는 것이라고 생각하면 된다.</li></ul><h3 id="스칼라-Scalar"><a href="#스칼라-Scalar" class="headerlink" title="스칼라(Scalar)"></a>스칼라(Scalar)</h3><ul><li>스칼라는 하나의 숫자만으로 이루어진 데이터를 말한다. 스칼라는 보통 $x$와 같이 알파벳 소문자로 표기하며 실수(real number)인 숫자 중의 하나이므로 실수 집합  $𝐑$의 원소라는 의미에서 다음처럼 표기한다.</li></ul><script type="math/tex; mode=display">\begin{align} x \in \mathbf{R} \end{align}</script><h3 id="벡터-Vector"><a href="#벡터-Vector" class="headerlink" title="벡터(Vector)"></a>벡터(Vector)</h3><ul><li><p>벡터는 여러 개의 숫자가 특정한 순서대로 모여 있는 것을 말한다. 사실 대부분의 데이터 레코드는 여러 개의 숫자로 이루어진 경우가 많다. 하나의 묶음(tuple)으로 묶어놓는 것이 좋다. 이때 숫자의 순서가 바뀌면 어떤 숫자가 어떤 피처에 매핑되는 값인지알 수 없으므로 <code>숫자의 순서를 유지하는 것이 중요</code>하다. 이런 데이터 묶음을 선형대수에서는 벡터라고 부른다.</p></li><li><p>이때 벡터는 복수의 가로줄, 즉 행(row)을 가지고 하나의 세로줄, 즉 열(column)을 가지는 형태로 위에서 아래로 내려써서 표기해야 한다. 하나의 벡터를 이루는 데이터의 개수가  $𝑛$개이면 이 벡터를 $n$-차원 벡터($n$-dimensional vector)라고 하며 다음처럼 표기한다.</p></li></ul><script type="math/tex; mode=display">\begin{align} x = \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{N} \\ \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} x \in \mathbf{R}^N \end{align}</script><ul><li>순서는 차원을 의미한다. 즉, 위에서 벡터 $x$는 1차원에 $x_1$ 2차원에 $x_2$, N차원에 $x_N$이라는 값을 갖는다는 의미이다. 참고로 순서가 정해져 있지 않은 배열(array)는 집합(set)이다. 또한 벡터라고 하는 것은 한방향으로만 있는 1-dimensional로 존재하는 배열 또는 하나의 숫자가 된다.</li></ul><h3 id="행렬-Matrix"><a href="#행렬-Matrix" class="headerlink" title="행렬(Matrix)"></a>행렬(Matrix)</h3><ul><li>행렬(Matrix)는 기본적으로 2-Dimensional 배열(array)를 의미한다. 행렬은 복수의 차원을 가지는 데이터 레코드가 다시 여러 개 있는 경우의 데이터를 합쳐서 표기한 것이다. 행렬은 보통  $𝑋$와 같이 알파벳 대문자로 표기한다.</li></ul><script type="math/tex; mode=display">\begin{align} X = \begin{bmatrix} \boxed{\begin{matrix} x_{1, 1} & x_{1, 2} & x_{1, 3} & x_{1, 4}\end{matrix}}  \\ \begin{matrix} x_{2, 1} & x_{2, 2} & x_{2, 3} & x_{2, 4}\end{matrix} \\ \begin{matrix} x_{3, 1} & x_{3, 2} & x_{3, 3} & x_{3, 4}\end{matrix} \\ \begin{matrix} x_{4, 1} & x_{4, 2} & x_{4, 3} & x_{4, 4}\end{matrix} \\ \begin{matrix} x_{5, 1} & x_{5, 2} & x_{5, 3} & x_{5, 4}\end{matrix} \\ \begin{matrix} x_{6, 1} & x_{6, 2} & x_{6, 3} & x_{6, 4}\end{matrix} \\ \end{bmatrix} \end{align}</script><ul><li>스칼라와 벡터도 수학적으로는 행렬에 속한다. 스칼라는 열과 행의 수가 각각 1인 행렬이고 벡터는 열의 수가 1인 행렬이다. 그래서 스칼라는 $ \begin{align} a \in \mathbf{R}^{1\times 1} \end{align}$ 벡터는 $\begin{align} x \in \mathbf{R}^{n\times 1} \end{align}$</li></ul><h3 id="텐서-Tensor"><a href="#텐서-Tensor" class="headerlink" title="텐서(Tensor)"></a>텐서(Tensor)</h3><ul><li>텐서는 같은 크기의 행렬이 여러 개 같이 묶여 있는 것을 말한다. 엄격한 수학적 정의로는 텐서는 다차원 배열로 표현되는 사상(mapping)으로 다차원 배열 자체를 뜻하지 않는다. 하지만 데이터 사이언스 분야에서는 흔히 다차원 배열을 텐서라고 부르므로 여기에서는 이러한 정의를 따르도록 한다.</li></ul><ul><li>예를 들어 다음 컬러 이미지는 2차원의 행렬처럼 보이지만 사실 빨강, 초록, 파랑의 밝기를 나타내는 3가지의 이미지가 겹친 것이다. 컬러 이미지에서는 각각의 색을 나타내는 행렬을 채널(channel)이라고 한다. 예제 이미지는 크기가 768 x 1024이고 3개의 채널이 있으므로 768 x 1024 x 3 크기의 3차원 텐서다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from scipy import misc  <span class="comment"># 패키지 임포트</span></span><br><span class="line"></span><br><span class="line">img_rgb = misc.face()  <span class="comment"># 컬러 이미지 로드</span></span><br><span class="line">img_rgb.shape  <span class="comment"># 데이터의 모양</span></span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(768, 1024, 3)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(221)</span><br><span class="line">plt.imshow(img_rgb, cmap=plt.cm.gray)  <span class="comment"># 컬러 이미지 출력</span></span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"RGB 컬러 이미지"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(222)</span><br><span class="line">plt.imshow(img_rgb[:, :, 0], cmap=plt.cm.gray)  <span class="comment"># red 채널 출력</span></span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Red 채널"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(223)</span><br><span class="line">plt.imshow(img_rgb[:, :, 1], cmap=plt.cm.gray)  <span class="comment"># green 채널 출력</span></span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Green 채널"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(224)</span><br><span class="line">plt.imshow(img_rgb[:, :, 2], cmap=plt.cm.gray)  <span class="comment"># blue 채널 출력</span></span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">"Blue 채널"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/image_tensor_using.png" alt="tensor로 살펴보는 이미지 파일"></p><h3 id="전치"><a href="#전치" class="headerlink" title="전치"></a>전치</h3><ul><li>전치(transpose)연산은 행렬에서 가장 기본이 되는 연산으로 행렬의 <code>행과 열을 바꾸는 연산</code>을 의미한다.</li></ul><p><img src="/image/vector_expression_01.png" alt="벡터의 표현"></p><ul><li>아래 그림과 같이 <code>기본적으로 벡터는 열벡터(column vector)</code>를 사용하며, 행 벡터(row vector)를 표기할때는 열벡터의 전치행렬로 표기해준다. 전치 연산과 행 벡터, 열 벡터를 이용하면 다음처럼 행렬을 복수의 열 벡터 $c_{i}$, 또는 복수의 행 벡터 $r^{T}_{j}$을 합친(concatenated)형태로 표기할 수도 있다.</li></ul><script type="math/tex; mode=display">\begin{align} X = \begin{bmatrix} c_1 & c_2 & \cdots & c_M \end{bmatrix} = \begin{bmatrix} r_1^T  \\ r_2^T  \\  \vdots \\ r_N^T  \\  \end{bmatrix} \end{align}</script><ul><li>위 식에서 행렬과 벡터의 크기는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} X \in \mathbf{R}^{N\times M} \end{align}</script><script type="math/tex; mode=display">\begin{align} c_i \in \mathbf{R}^{N \times 1} \; (i=1,\cdots,M) \end{align}</script><script type="math/tex; mode=display">\begin{align} r_j^T \in \mathbf{R}^{1 \times M} \; (j=1,\cdots,N) \end{align}</script><ul><li>벡터의 모양을 직사각형으로 표시하면 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} X = \begin{bmatrix} \boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ c_1 \\ \phantom{\LARGE\mathstrut} \end{matrix}} & \boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ c_2 \\ \phantom{\LARGE\mathstrut} \end{matrix}} &  \cdots & \boxed{\begin{matrix} \phantom{\LARGE\mathstrut} \\ c_M \\ \phantom{\LARGE\mathstrut} \end{matrix}} \end{bmatrix} = \begin{bmatrix} \boxed{\begin{matrix} \phantom{} & \phantom{} & r_1^T & \phantom{} & \phantom{} \end{matrix}} \\ \boxed{\begin{matrix} \phantom{} & \phantom{} & r_2^T & \phantom{} & \phantom{} \end{matrix}} \\ \vdots \\ \boxed{\begin{matrix} \phantom{} & \phantom{} & r_N^T & \phantom{} & \phantom{} \end{matrix}} \\ \end{bmatrix} \end{align}</script><h2 id="특수한-벡터와-행렬"><a href="#특수한-벡터와-행렬" class="headerlink" title="특수한 벡터와 행렬"></a>특수한 벡터와 행렬</h2><ul><li>몇 가지 특수한 벡터와 행렬은 별도의 기호나 이름이 붙는다.</li></ul><p><img src="/image/various_type_of_matrix.png" alt="행렬의 종류"></p><h3 id="영벡터"><a href="#영벡터" class="headerlink" title="영벡터"></a>영벡터</h3><ul><li>모든 원소가 0인 $N$차원 벡터는 영벡터(zeros-vector)라고 하며 다음처럼 표기한다. 문맥으로 벡터의 크기를 알 수 있을 때는 크기를 나타내는 아래 첨자 $N$을 생략할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} \mathbf{0}_N = \mathbf{0} = 0 = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \\ \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} 0 \in \mathbf{R}^{N \times 1} \end{align}</script><h3 id="일벡터"><a href="#일벡터" class="headerlink" title="일벡터"></a>일벡터</h3><ul><li>모든 원소가 1인  $N$차원 벡터는 일벡터(ones-vector)라고 하며 다음처럼 표기한다. 마찬가지로 문맥으로 벡터의 크기를 알 수 있을 때는 크기를 나타내는 아래 첨자  $N$을 생략할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} \mathbf{1}_N = \mathbf{1}  = 1 = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \\ \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} 1 \in \mathbf{R}^{N \times 1} \end{align}</script><h3 id="정방행렬"><a href="#정방행렬" class="headerlink" title="정방행렬"></a>정방행렬</h3><ul><li>행의 개수와 열의 개수가 같은 행렬을 정방행렬(square matrix)라고 한다.</li></ul><h3 id="대각행렬"><a href="#대각행렬" class="headerlink" title="대각행렬"></a>대각행렬</h3><ul><li>행렬에서 행과 열이 같은 위치를 주 대각(main diagonal) 또는 간단히 대각(diagonal)이라고 한다. 대각 위치에 있지 않은 것들은 비대각(off-diagonal)이라고 한다. 모든 비대각 요소가 0인 행렬을 대각행렬(diagonal matrix)이라고 한다.</li></ul><script type="math/tex; mode=display">\begin{align} D = \begin{bmatrix} d_{1} & 0 & \cdots & 0 \\ 0 & d_{2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & d_{N} \\ \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} D \in \mathbf{R}^{N \times N} \end{align}</script><ul><li><code>대각행렬이 되려면 비대각 성분이 0이기만 하면 되고 대각성분은 0이든 아니든 상관없다. 또한 반드시 정방행렬일 필요도 없다.</code> 예를 들어 다음 행렬도 대각행렬이라고 할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} D = \begin{bmatrix} d_{1} & 0 & \cdots & 0 \\ 0 & d_{2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & d_{M} \\ 0 & 0 & \cdots & 0 \\ 0 & 0 & \cdots & 0 \\ 0 & 0 & \cdots & 0 \\ \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} D \in \mathbf{R}^{N \times M} \end{align}</script><h3 id="항등행렬"><a href="#항등행렬" class="headerlink" title="항등행렬"></a>항등행렬</h3><ul><li>대각행렬 중에서도 모든 대각성분의 값이 1인 대각행렬을 항등행렬(identity matrix)이라고 한다. 항등행렬은 보통 알파벳 대문자 $I$로 표기한다. (E로 표기하는 방식도 있다.)</li></ul><ul><li>임의의 행렬 A에 대해 행렬곱을 해도 임의의 행렬 A가 나오도록 하는 항등원 개념과 동일하다.</li></ul><script type="math/tex; mode=display">A I = I A = A</script><script type="math/tex; mode=display">\begin{align} I = \begin{bmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \\ \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} I \in \mathbf{R}^{N \times N} \end{align}</script><ul><li>참고로 추후에 말하겠지만, 항등원의 개념이 있다면 역원의 개념에 대해 생각해 볼 수 있을 것이다. 역원의 정의에 맞게 임의의 행렬에 대한 곱셈에대한 역원을 찾는다면 아래와 같이 구할 수 있다. 즉 행렬에서 임의의 행렬 A에 곱셈에 대한 역원은 역행렬을 의미한다.</li></ul><script type="math/tex; mode=display">A A^{-1} = A^{-1} A = I</script><h3 id="대칭행렬"><a href="#대칭행렬" class="headerlink" title="대칭행렬"></a>대칭행렬</h3><ul><li>만약 전치연산을 통해서 얻은 전치행렬과 원래의 행렬이 같으면 대칭행렬(symmetric matrix)라고 한다. 정방행렬만이 대칭행렬이 될 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} S^{T} = S \end{align}</script><script type="math/tex; mode=display">\begin{align} S \in \mathbf{R}^{N \times N} \end{align}</script><h2 id="벡터의-행렬과-연산"><a href="#벡터의-행렬과-연산" class="headerlink" title="벡터의 행렬과 연산"></a>벡터의 행렬과 연산</h2><ul><li>벡터와 행렬도 숫자처럼 덧셈, 뺄셈, 곱셈 등의 연산을 할 수 있다. <code>데이터 사이언스에 있어서 선형대수를 알고있어야 하는 이유는 여러가지가 있지만 그 중 한가지로 벡터와 행렬의 연산을 이용하면 대량의 데이터에 대한 계산을 간단한 수식으로 나타낼 수 있기 때문</code>이다. 물론 벡터와 행렬에 대한 연산은 숫자의 사칙 연산과는 몇 가지 다른 점이 있으므로 이러한 차이를 잘 알아야 한다.</li></ul><h3 id="벡터-행렬의-덧셈과-뺄셈"><a href="#벡터-행렬의-덧셈과-뺄셈" class="headerlink" title="벡터/행렬의 덧셈과 뺄셈"></a>벡터/행렬의 덧셈과 뺄셈</h3><p><img src="/image/matrix_operation_01.png" alt="행렬의 덧셈과 곱셈"></p><ul><li>같은 크기를 가진 두 개의 벡터나 행렬은 덧셈과 뺄셈을 할 수 있다. 두 벡터와 행렬에서 같은 위치에 있는 원소끼리 덧셈과 뺄셈을 하면 된다. 이러한 연산을 요소별(element-wise)연산이라고 한다.</li></ul><ul><li>예를 들어 벡터 $x$와 $y$가 다음과 같으면,</li></ul><script type="math/tex; mode=display">\begin{align} x= \begin{bmatrix} 10 \\ 11 \\ 12 \\ \end{bmatrix} ,\;\; y= \begin{bmatrix} 0 \\ 1 \\ 2 \\ \end{bmatrix} \end{align}</script><ul><li>벡터  $𝑥$ 와  $𝑦$ 의 덧셈  $𝑥+𝑦$와 뺄셈  $𝑥−𝑦$는 다음처럼 계산한다.</li></ul><script type="math/tex; mode=display">\begin{align} x + y = \begin{bmatrix} 10 \\ 11 \\ 12 \\ \end{bmatrix} + \begin{bmatrix} 0 \\ 1 \\ 2 \\ \end{bmatrix} = \begin{bmatrix} 10 + 0 \\ 11 + 1 \\ 12 + 2 \\ \end{bmatrix} = \begin{bmatrix} 10 \\ 12 \\ 14 \\ \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} x - y = \begin{bmatrix} 10 \\ 11 \\ 12 \\ \end{bmatrix} - \begin{bmatrix} 0 \\ 1 \\ 2 \\ \end{bmatrix} = \begin{bmatrix} 10 - 0 \\ 11 - 1 \\ 12 - 2 \\ \end{bmatrix} = \begin{bmatrix} 10 \\ 10 \\ 10 \\ \end{bmatrix} \end{align}</script><ul><li>행렬도 같은 방법으로 덧셈과 뺄셈을 할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 5 & 6 \\ 7 & 8 \end{bmatrix} + \begin{bmatrix} 10 & 20 \\ 30 & 40 \\ \end{bmatrix} -\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix} = \begin{bmatrix} 14 & 24 \\ 34 & 44 \end{bmatrix} \end{align}</script><h3 id="스칼라와-벡터-행렬의-곱셈"><a href="#스칼라와-벡터-행렬의-곱셈" class="headerlink" title="스칼라와 벡터/행렬의 곱셈"></a>스칼라와 벡터/행렬의 곱셈</h3><ul><li>벡터 $x$또는 행렬 $A$에 스칼라값 $c$를 곱하는 것은 벡터 $x$ 또는 행렬 $A$의 모든 원소에 스칼라값 $c$를 곱하는 것과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} c \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} cx_1 \\ cx_2 \end{bmatrix} \end{align}</script><script type="math/tex; mode=display">\begin{align} c \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} = \begin{bmatrix} ca_{11} & ca_{12} \\ ca_{21} & ca_{22} \end{bmatrix} \end{align}</script><h3 id="브로드캐스팅"><a href="#브로드캐스팅" class="headerlink" title="브로드캐스팅"></a>브로드캐스팅</h3><ul><li>원래 덧셈과 뺄셈은 크기(차원)가 같은 두 벡터에 대해서만 할 수 있다. 하지만 Numpy에서 벡터와 스칼라의 경우에는 관례적으로 다음처럼 1-벡터를 사용하여 스칼라를 벡터로 연산을 허용한다. 이를 브로드캐스팅(broadcasting)이라고 한다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{bmatrix} 10 \\ 11 \\ 12 \\ \end{bmatrix} - 10 = \begin{bmatrix} 10 \\ 11 \\ 12 \\ \end{bmatrix} - 10\cdot \mathbf{1} = \begin{bmatrix} 10 \\ 11 \\ 12 \\ \end{bmatrix} - \begin{bmatrix} 10 \\ 10 \\ 10 \\ \end{bmatrix} \end{align}</script><ul><li>데이터 분석에서는 원래의 데이터 벡터 $x$가 아니라 그 데이터 벡터의 각 원소의 평균값을 뺀 평균제거(mean removed) 벡터 혹은 0-평균(zero-mean)벡터를 사용하는 경우가 많다.</li></ul><script type="math/tex; mode=display">\begin{align} x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \end{bmatrix} \;\; \rightarrow \;\; x - m = \begin{bmatrix} x_1 - m\\ x_2 - m \\ \vdots \\ x_N - m \end{bmatrix} \end{align}</script><ul><li>아래 그림에서 볼 수 있듯이, 행렬의 곱셈에 대한 교환법칙은 성립하지 않는다. 물론, $AB=BA$인 경우도 존재하지만, 항상 존재하진 않기 때문이다.</li></ul><h3 id="벡터와-벡터의-곱셈"><a href="#벡터와-벡터의-곱셈" class="headerlink" title="벡터와 벡터의 곱셈"></a>벡터와 벡터의 곱셈</h3><ul><li>벡터 $x$와 벡터 $y$의 내적은 다음처럼 표기 한다.</li></ul><script type="math/tex; mode=display">\begin{align} x^T y \end{align}</script><ul><li>내적은 다음처럼 점(dot)으로 표기하는 경우도 있어서 닷 프로덕트(dot product)라고도 부르고 &lt; x, y &gt; 기호로 나타낼 수도 있다.</li></ul><script type="math/tex; mode=display">\begin{align} x \cdot y = \, < x, y > \, = x^T y \end{align}</script><ul><li><p>두 벡터를 내적하려면 다음과 같은 조건이 만족되어야 한다.</p><ul><li><ol><li>우선 두 벡터의 차원(길이)이 같아야 한다.</li></ol></li><li><ol><li>앞의 벡터가 행 벡터이고 뒤의 벡터가 열 벡터여야 한다.</li></ol></li></ul></li><li><p>이때 내적의 결과는 스칼라값이 되며 다음처럼 계산한다. 우선 같은 위치에 있는 원소들을 요소별 곱셈처럼 곱한 다음, 그 값들을 다시 모두 더해서 하나의 스칼라값으로 만든다.</p></li></ul><script type="math/tex; mode=display">\begin{align} x^T y = \begin{bmatrix} x_{1} & x_{2} & \cdots & x_{N} \end{bmatrix} \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{N} \\ \end{bmatrix} = x_1 y_1 + \cdots + x_N y_N = \sum_{i=1}^N x_i y_i \end{align}</script><script type="math/tex; mode=display">\begin{align} x \in \mathbf{R}^{N \times 1}, y \in \mathbf{R}^{N \times 1}, x^T y \in \mathbf{R} \end{align}</script><h3 id="행렬과-행렬의-곱셈"><a href="#행렬과-행렬의-곱셈" class="headerlink" title="행렬과 행렬의 곱셈"></a>행렬과 행렬의 곱셈</h3><ul><li>벡터의 곱셈을 정의한 후에는 이를 이용하여 행렬의 곱셈을 정의할 수 있다. 행렬과 행렬을 곱하면 행렬이 된다. 방법은 다음과 같다.</li></ul><ul><li>$A$ 행렬과 $B$ 행렬을 곱한 결과가 $C$ 행렬이 된다고 하자. $C$의 $i$번째 행, $j$번째 열의 원소 $c_{ij}$의 값은 $A$ 행렬의 $i$번째 행 벡터 $a^{T}_{i}$와 $B$ 행렬의 $j$번째 열 벡터 $b_{j}$의 곱이다.</li></ul><script type="math/tex; mode=display">\begin{align} C = AB \; \rightarrow \; c_{ij} = a_i^T b_j \end{align}</script><ul><li>이 정의가 성립하려면 앞의 행렬 $A$의 열의수가 뒤의 행렬 $B$의 행의 수와 일치해야만 한다.</li></ul><script type="math/tex; mode=display">\begin{align} A \in \mathbf{R}^{N \times L} , \; B \in \mathbf{R}^{L \times M} \;  \rightarrow \; AB \in \mathbf{R}^{N \times M} \end{align}</script><h3 id="교환-법칙과-분배법칙"><a href="#교환-법칙과-분배법칙" class="headerlink" title="교환 법칙과 분배법칙"></a>교환 법칙과 분배법칙</h3><ul><li><code>행렬의 곱셈</code>은 곱하는 행렬의 순서를 바꾸는 <code>교환 법칙이 성립하지 않는다.</code> 그러나 덧셈에 대한 분배 법칙은 성립한다.</li></ul><script type="math/tex; mode=display">\begin{align} AB \neq BA \end{align}</script><script type="math/tex; mode=display">\begin{align} A(B + C) = AB + AC \end{align}</script><script type="math/tex; mode=display">\begin{align} (A + B)C = AC + BC \end{align}</script><ul><li>전치 연산도 마찬가지로 마찬가지로 덧셈/뺄셈에 대해 분배 법칙이 성립한다.</li></ul><script type="math/tex; mode=display">\begin{align} (A + B)^T = A^T + B^T \end{align}</script><ul><li>전치 연산 곱셈의 경우에는 분배 법칙이 성립하기는 하지만 <code>전치 연산이 분배되면서 곱셈의 순서가 바뀐다.</code></li></ul><script type="math/tex; mode=display">\begin{align} (AB)^T = B^T A^T \end{align}</script><script type="math/tex; mode=display">\begin{align} (ABC)^T = C^T B^T A^T \end{align}</script><p><img src="/image/matrix_operation_02.png" alt="행렬의 곱셈에 대한 교환법칙은 성립하지 않는다"></p><h3 id="곱셈의-연결"><a href="#곱셈의-연결" class="headerlink" title="곱셈의 연결"></a>곱셈의 연결</h3><ul><li>연속된 행렬의 곱셈은 계산 순서를 임의의 순서로 해도 상관없다.</li></ul><script type="math/tex; mode=display">\begin{align} ABC = (AB)C = A(BC) \end{align}</script><script type="math/tex; mode=display">\begin{align} ABCD = ((AB)C)D = (AB)(CD) = A(BCD) = A(BC)D \end{align}</script><p><img src="/image/matrix_operation_03.png" alt="행렬의 연산"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/12/linear_algebra_01/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Ensemble Learning - Bagging, RandomForest</title>
      <link>https://heung-bae-lee.github.io/2020/05/02/machine_learning_14/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/02/machine_learning_14/</guid>
      <pubDate>Sat, 02 May 2020 12:00:10 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Ensemble-Learning이란&quot;&gt;&lt;a href=&quot;#Ensemble-Learning이란&quot; class=&quot;headerlink&quot; title=&quot;Ensemble Learning이란?&quot;&gt;&lt;/a&gt;Ensemble Learning이란?&lt;/h2&gt;&lt;ul
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Ensemble-Learning이란"><a href="#Ensemble-Learning이란" class="headerlink" title="Ensemble Learning이란?"></a>Ensemble Learning이란?</h2><ul><li>모형 결합(model combining)방법은 앙상블 방법론(ensemble methods)라고도 한다. 이는 특정한 하나의 예측 방법이 아니라 복수의 예측모형을 결합하여 더 나은 성능의 예측을 하려는 시도이다.</li></ul><ul><li><p>모형 결합 방법을 사용하면 <code>일반적으로 계산량은 증가하지만 다음과 같은 효과가 있다.</code></p><ul><li>단일 모형을 사용할 때 보다 성능 분산이 감소하기에 <code>과최적화(overfitting)을 방지</code>한다.</li><li><code>개별 모형이 성능이 안좋을 경우에는 결합 모형의 성능이 더 향상</code>된다.</li></ul></li><li><p>Ensemble Learning은 캐글이나 다른 대회에서 높은 성능을 자랑하며 여러 차례 우승을 차지한 알고리즘으로 그만큼 강력하지만, 현업에서는 Ensemble Learning을 사용하지 않을 가능성이 매우 높다. 왜냐하면 굉장히 강력하지만 다른 모델들과의 성능차이가 엄청나게 차이나는 것이 아니며, 실제 Domain에서 중요한 변수가 무엇인지와 같은 원인을 찾는 feature selection 부분이 더 중요할 수 있기 때문이다. 물론, 성능측면이 중요한 Domain분야에서는 Ensemble Learning이 중요할 것이다.</p></li><li><p>Ensemble이라는 의미의 사전적 정의는 ‘합창단’, ‘조화’라는 의미를 지닌다. 즉, 머신러닝에서의 개념은 여러개의 모델을 조합을 시킨다라는 의미로 받아들일 수 있다.</p></li></ul><p><img src="/image/Ensemble_dictionary_mean.png" alt="Ensemble의 의미"></p><ul><li>통계학에서의 대수의 법칙이라는 개념이 있는데, 큰 모집단에서 무작위로 뽑은 표본의 수가 많아 질수록(보통은 30개이상의 관측이) 모집단의 평균에 가까울 확률이 높아진다는 개념이다. <code>많은 시행의 결과가 수학적으로 합리적인 결과를 보여준다</code>는 것을 의미하는데, Ensemble learning에 적용하여 생각해보면 다수의 모델이 더 합리적인 성능을 가져올 수 있다는 것으로 해석할 수도 있다.</li></ul><p><img src="/image/the_law_of_large_number.png" alt="대수의 법칙과 Ensemble"></p><ul><li>하지만 아래에서 <code>합치는 모델의 성능 자체가 떨어지는 모델을 가지고 Ensemble learning을 진행한다고 해도 성능을 올릴 수는 없다.</code></li></ul><p><img src="/image/Ensemble_learning_conception_01.png" alt="Ensemble learning의 개념 - 01"></p><p><img src="/image/Ensemble_learning_conception_02.png" alt="Ensemble learning의 개념 - 02"></p><ul><li>아래에서는 이진분류에 대해서만 언급했지만, 대부분의 classification 문제에서는 One VS Rest 방식으로 문제를 풀기에 이진 분류 뿐만아니라, class가 여러개인 multi class 문제에서도 적용되는 내용이다.</li></ul><p><img src="/image/why_performence_prob_higher_than_half.png" alt="Ensemble learning에서 base model의 성능의 전제조건"></p><ul><li>아래에서와 같이 각각의 성능이 0.5인 분류기들을 voting을 통해 결과를 내게 되는데, 각각의 weak한 분류기들의 조합을 통해 최종적으로는 0.625라는 성능을 내게 된다.</li></ul><p><img src="/image/how_to_decision_predicted_value_on_ensemble.png" alt="Ensemble을 통한 결과 도출"></p><ul><li>다수결 모형이 개별 모형보다 더 나은 성능을 보이는 이유는 다음 실험에서도 확인 할 수 있다. 만약 개별 모형이 정답을 출력할 확률이 $p$인 경우에 <code>서로 다르고 독립적인 모형</code> $N$개를 모아서 다수결 모형을 만들면 정답을 출력할 확률이 다음과 같아진다.</li></ul><script type="math/tex; mode=display">\sum_{k>\frac{N}{2}}^N \binom N k p^k (1-p)^{N-k}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def total_error(p, N):</span><br><span class="line">    te = 0.0</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(int(np.ceil(N/2)), N + 1):</span><br><span class="line">        te += sp.misc.comb(N, k) * p**k * (1-p)**(N-k)</span><br><span class="line">    <span class="built_in">return</span> te</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.linspace(0, 1, 100)</span><br><span class="line">plt.plot(x, x, <span class="string">'g:'</span>, lw=3, label=<span class="string">"개별 모형"</span>)</span><br><span class="line">plt.plot(x, total_error(x, 10), <span class="string">'b-'</span>, label=<span class="string">"다수결 모형 (N=10)"</span>)</span><br><span class="line">plt.plot(x, total_error(x, 100), <span class="string">'r-'</span>, label=<span class="string">"다수결 모형 (N=100)"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"개별 모형의 성능"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"다수결 모형의 성능"</span>)</span><br><span class="line">plt.legend(loc=0)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/each_model_and_voting_medel_performance_differ.png" alt="개별모델과 앙상블 모델의 성능 비교"></p><ul><li>각각의 분류기(모델)를 통해 최종적으로는 해당 데이터들의 decision boundary의 평균을 사용하는 것과 동일한 결과를 얻을 수 있다.</li></ul><p><img src="/image/Ensemble_learning_classifier_decision_boundary_visualization.png" alt="Ensemble을 통한 예측의 decision boundary"></p><ul><li>아래에서와 같이 <code>Test data에 대해 일정부분 bias되는 부분을 줄이기 위해 overfitting이 잘 되는 트리기반의 모형을 주로 베이스 모델로 사용</code>한다.</li></ul><p><img src="/image/base_line_model_why_is_based_on_tree.png" alt="Ensemble 모델에서 트리기반을 주로 사용하는 이유"></p><ul><li>Ensemble Learning의 종류는 아래 그림과 같이 나눌 수 있다. 쉽게 말하면 <code>Bagging</code>은 여러 개의 모델을 만들기 위해서는 Tree기반이나 선형회귀 분석 같은 경우는 동일한 feature와 동일한 data를 사용했을 경우 동일한 결과를 내주기 때문에, <code>모델을 여러 개 만들기 위해서 데이터를 나누어서 각 모델에 fitting시키는 것을 의미</code>한다. <code>Random Forest</code>는 데이터 뿐만 아니라 feature들의 선택도 각 모델별로 달리하여 fitting하는 것이며, <code>Boosting</code>은 분류기가 틀리게 예측한 데이터들에 대해 그 다음 학습기는 좀 더 학습을 잘 할 수 있도록 가중치를 주는 개념이다. 마지막으로 <code>Stacking</code>은 성능순으로 점수를 매기는 캐글에서는 0.1%라도 올리는 것이 중요하기 때문에 사용되어 지는데, 다른 Ensemble 기법들 보다 많은 성능을 높이지는 못하여 잘 사용되지는 않는다. 굉장히 많은 학습 연산량을 필요로 하기 때문에 실제 Domain에서 사용되어지기에는 쉽지 않다.</li></ul><p><img src="/image/Ensmble_learning_types.png" alt="Ensemble learning의 종류"></p><ul><li>위에서 언급한 것과 같이 모형 결합 방법은 크게 나누어 취합(aggregation) 방법론과 부스팅(boosting)방법론으로 나눌 수 있다.<ul><li>취합 방법론은 사용할 모형의 집합이 이미 결정되어있다.</li><li>부스팅 방법론은 사용할 모형을 점진적으로 늘려간다.</li></ul></li></ul><ul><li><p>각 방법론의 대표적인 방법들은 아래와 같다.</p><ul><li><p>취합 방법론</p><ul><li>다수결(Majority Voting)</li><li>배깅(Bagging)</li><li>랜덤 포레스트(Random Forests)</li></ul></li><li><p>부스팅 방법론</p><ul><li>에이다부스트(AdaBoost)</li><li>그레디언트 부스트(Gradient Boost)</li></ul></li></ul></li></ul><h2 id="다수결-방법-Voting"><a href="#다수결-방법-Voting" class="headerlink" title="다수결 방법(Voting)"></a>다수결 방법(Voting)</h2><ul><li>다수결 방법은 가장 단순한 모형 결합 방법으로 전혀 다른 모형도 결합할 수 있다. 다수결 방법은 Hard Voting과 Soft Voting 두 가지로 나뉘어진다.<ul><li>hard voting: 단순 투표. 개별 모형의 결과 기준</li><li>soft voting: 가중치 투표. 개별 모형의 <code>조건부 확률의 합</code> 기준</li><li><code>일반적으로 hard voting보다는 soft voting이 예측 성능이 좋아서 더 많이 사용된다.</code></li></ul></li></ul><ul><li><p>Scikit-Learn의 ensemble 서브 패키지는 다수결 방법을 위한 <code>VotingClassifier</code>클래스를 제공한다. 입력인수는 다음과 같다.</p><ul><li><code>estimators</code>: 개별 모형 목록, 리스트나 named parameter 형식으로 입력</li><li><code>voting</code>: 문자열 {hard, soft} hard voting과 soft voting 선택. 디폴트는 hard</li><li><code>weights</code>: 사용자 가중치 리스트</li></ul></li><li><p>다음과 같은 예제 데이터를 가지는 이진 분류 문제를 생각해보자.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([[0, -0.5], [-1.5, -1.5], [1, 0.5], [-3.5, -2.5], [0, 1], [1, 1.5], [-2, -0.5]])</span><br><span class="line">y = np.array([1, 1, 1, 2, 2, 2, 2])</span><br><span class="line">x_new = [0, -1.5]</span><br><span class="line">plt.scatter(X[y == 1, 0], X[y == 1, 1], s=100, marker=<span class="string">'o'</span>, c=<span class="string">'r'</span>, label=<span class="string">"클래스 1"</span>)</span><br><span class="line">plt.scatter(X[y == 2, 0], X[y == 2, 1], s=100, marker=<span class="string">'x'</span>, c=<span class="string">'b'</span>, label=<span class="string">"클래스 2"</span>)</span><br><span class="line">plt.scatter(x_new[0], x_new[1], s=100, marker=<span class="string">'^'</span>, c=<span class="string">'g'</span>, label=<span class="string">"테스트 데이터"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.title(<span class="string">"이진 분류 예제 데이터"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/Ensemble_voting_method_data.png" alt="Ensemble 방법 중 Voting 방법을 사용할 데이터"></p><ul><li>먼저, 이 문제를 3가지 다른 방법으로 풀어볼 것이다.<ul><li>로지스틱 회귀모형</li><li>QDA 모형</li><li>가우시안 나이브베이즈 모형</li></ul></li></ul><ul><li>마지막으로 3가지 모형을 다수결로 합친 모형을 <code>VotingClassifier</code>클래스로 만들었다. 다만 3가지 모형의 가중치가 각각 1,1,2로 가우시안 나이브베이즈 모형의 가중치를 높였다.</li></ul><ul><li>결과는 다음과 같이, 로지스틱 회귀모형과 가우시안 나이브베이즈 모형은 클래스 1이라는 결과를 보이지만 QDA모형은 클래스 2라는 결과를 보였다. 소프트 방식의 다수결 모형은 클래스 2라는 결론을 보인다. 만약 하드 방식의 다수결 모형이었다면 예측 결과는 클래스 1이 될 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis</span><br><span class="line">from sklearn.ensemble import VotingClassifier</span><br><span class="line"></span><br><span class="line">model1 = LogisticRegression(random_state=1)</span><br><span class="line">model2 = QuadraticDiscriminantAnalysis()</span><br><span class="line">model3 = GaussianNB()</span><br><span class="line">ensemble = VotingClassifier(estimators=[(<span class="string">'lr'</span>, model1), (<span class="string">'qda'</span>, model2), (<span class="string">'gnb'</span>, model3)], voting=<span class="string">'soft'</span>)</span><br><span class="line"></span><br><span class="line">probas = [c.fit(X, y).predict_proba([x_new]) <span class="keyword">for</span> c <span class="keyword">in</span> (model1, model2, model3, ensemble)]</span><br><span class="line">class1_1 = [pr[0, 0] <span class="keyword">for</span> pr <span class="keyword">in</span> probas]</span><br><span class="line">class2_1 = [pr[0, 1] <span class="keyword">for</span> pr <span class="keyword">in</span> probas]</span><br><span class="line"></span><br><span class="line">ind = np.arange(4)</span><br><span class="line">width = 0.35  <span class="comment"># bar width</span></span><br><span class="line">p1 = plt.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color=<span class="string">'green'</span>)</span><br><span class="line">p2 = plt.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width, color=<span class="string">'lightgreen'</span>)</span><br><span class="line">p3 = plt.bar(ind, [0, 0, 0, class1_1[-1]], width, color=<span class="string">'blue'</span>)</span><br><span class="line">p4 = plt.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color=<span class="string">'steelblue'</span>)</span><br><span class="line"></span><br><span class="line">plt.xticks(ind + 0.5 * width, [<span class="string">'로지스틱 회귀 모형'</span>, <span class="string">'QDA 모형'</span>, <span class="string">'가우시안 나이브베이즈'</span>, <span class="string">'소프트 다수결 모형'</span>])</span><br><span class="line">plt.ylim([0, 1.1])</span><br><span class="line">plt.title(<span class="string">'세가지 다른 분류 모형과 소프트 다수결 모형의 분류 결과'</span>)</span><br><span class="line">plt.legend([p1[0], p2[0]], [<span class="string">'클래스 1'</span>, <span class="string">'클래스 2'</span>], loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/soft_voting_method_on_data_test.png" alt="소프트 보팅을 사용한 결과와 개별모델 예측 결과와의 비교"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from itertools import product</span><br><span class="line"></span><br><span class="line">x_min, x_max = -4, 2</span><br><span class="line">y_min, y_max = -3, 2</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.005),</span><br><span class="line">                     np.arange(y_min, y_max, 0.005))</span><br><span class="line">f, axarr = plt.subplots(2, 2)</span><br><span class="line"><span class="keyword">for</span> idx, clf, tt <span class="keyword">in</span> zip(product([0, 1], [0, 1]),</span><br><span class="line">                        [model1, model2, model3, ensemble],</span><br><span class="line">                        [<span class="string">'로지스틱 회귀'</span>, <span class="string">'QDA'</span>, <span class="string">'가우시안 나이브베이즈'</span>, <span class="string">'다수결 모형'</span>]):</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.2, cmap=mpl.cm.jet)</span><br><span class="line">    axarr[idx[0], idx[1]].scatter(</span><br><span class="line">        X[:, 0], X[:, 1], c=y, alpha=0.5, s=50, cmap=mpl.cm.jet)</span><br><span class="line">    axarr[idx[0], idx[1]].scatter(x_new[0], x_new[1], marker=<span class="string">'x'</span>)</span><br><span class="line">    axarr[idx[0], idx[1]].set_title(tt)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li>아래 각 모형별로 decision boundary를 살펴 보았을 경우 어떠한 생각이 드는가? 필자의 생각엔 물론 전제조건이 아래 train 데이터가 모집단의 분포를 대표할 수 있는 데이터들이라는 가정하에 soft 방식으로 한 결과는 옳지 못한 결과라고 생각한다. 2클래스로 분류되기엔 1 클래스가 많은 영역에 존재하기 때문이다. 이렇게 시각화를 통해 살펴보는 방법도 결과에 대한 검증을 위해 필요할 것이다. 허나, 다변량인 경우는 몇가지 중요한 변수들에 대해서만 시각화를 해 본다던가 아니면 해당 조합들에 대해 모두 그려보는 것도 때론 좋은 방법일 수 있을 것이다.</li></ul><p><img src="/image/each_model_decision_boundary_differ_result.png" alt="모델 별 decision boundary"></p><ul><li>앞서 모형 결합에서 사용하는 독립적인 모형의 수가 많을 수록 성능 향상이 일어날 가능성이 높다는 것을 알았다. <code>각각 다른 확률 모형을 사용하는데에는 한계가 있으므로 보통은 배깅 방법을 사용하여 같은 확률 모형을 쓰지만 서로 다른 결과를 출력하는 다수의 모형을 만든다.</code></li></ul><h2 id="배깅-Bagging"><a href="#배깅-Bagging" class="headerlink" title="배깅(Bagging)"></a>배깅(Bagging)</h2><ul><li><p>Bagging은 Bootstrap Aggregating의 약자로 Sampling을 하는 방식이 Bootstrap방식을 사용하기 때문이다. 아래 그림에서 볼 수 있듯이 복원추출의 방식이라고 생각하면된다. 하나의 모델에 대하여 데이터를 추출할 경우 해당 모델에 들어가있는 데이터는 중복된 데이터가 있을 수 있다.(오른쪽 첫번째 데이터세트에서와 같이)</p><ul><li>같은 데이터 샘플을 중복사용(replacement)하지 않으면: Pasting</li><li>같은 데이터 샘플을 중복사용(replacement)하면: Bagging</li><li>데이터가 아니라 다차원 독립 변수 중 일부 차원을 선택하는 경우에는: Random Subspaces</li><li>데이터 샘플과 독립 변수 차원 모두 일부만 랜덤하게 사용하면: Random Patches</li></ul></li></ul><p><img src="/image/What_is_bagging_01.png" alt="Bagging의 개념 - 01"></p><ul><li>이렇게 추출하는 데이터는 전체 데이터 중 약 63%정도만 추출을 하게 된다. 아래 첫 번째 그림에서는 밑줄이 그러진 원의 데이터는 추출되지 않는 데이터들이다. 2번째 그림은 Bootstrap size가 5라면 5개씩 12개의 데이터 set를 복원추출을 통하여 뽑는 것이다. 여기서의 $k$는 임의로 정할 수 있다.</li></ul><p><img src="/image/What_is_bagging_02.png" alt="Bagging의 개념 - 02"></p><p><img src="/image/What_is_bagging_03.png" alt="Bagging의 개념 - 03"></p><ul><li>위에서 언급했듯이 추출되지 않은 데이터 set이 있는 것은 학습에 활용되지 않았으므로 그대로 두면 데이터를 낭비하는 것과 동일하다. 물론 Test set을 미리 나누어 놓고 해당 Test set을 prediction한 결과를 voting하여 성능을 측정하지만, 사용되지 않은 데이터(Out-of_Bag data)에 대해서도 모델별 성능을 계산한다.</li></ul><p><img src="/image/What_is_bagging_04.png" alt="Bagging의 개념 - 04"></p><ul><li>트리(Tree)와 배깅(Bagging)을 비교하자면 깊이 성장한 트리는 overfitting이 굉장히 심해지기 때문에 분산이 증가하기 때문에 편향은 줄어들 것이다. 그러나 배깅은 이러한 트리들을 결합시키므로 <code>편향이 유지되며, 분사은 감소하는 모델</code>이 될 것이다. <code>학습데이터의 noise에 robust</code>하다. 그러나 모형해석이 어려워지는 단점이 있다. 이러한 단점이 실제 Domain에서 사용되지 못하는 이유가 될 수 있다.</li></ul><p><img src="/image/What_is_bagging_05.png" alt="Bagging의 개념 - 05"></p><ul><li>Scikit-Learn의 ensemble 서브 패키지는 배깅 모형 결합을 위한 <code>BaggingClassifier</code> 클래스를 제공한다. 사용법은 아래와 같다. 참고로 <code>BaggingRegressor</code>도 존재하며 사용법은 동일하다.<ul><li><code>base_estimator</code>: 기본모형</li><li><code>n_estimators</code>: 모형 갯수. default=10</li><li><code>bootstrap</code>: 데이터 중복 사용 여부. default=True</li><li><code>max_samples</code>: 데이터 샘플 중 선택할 샘플의 수 혹은 비율. default=1.0</li><li><code>bootstrap_features</code>: feature의 중복 사용 여부. default=False</li><li><code>max_features</code>: 다차원 독립 변수 중 선택할 차원의 수 혹은 비율. default=1.0</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.ensemble import BaggingClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data[:, [0, 2]], iris.target</span><br><span class="line"></span><br><span class="line">model1 = DecisionTreeClassifier(max_depth=10, random_state=0).fit(X, y)</span><br><span class="line">model2 = BaggingClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100, random_state=0).fit(X, y)</span><br><span class="line"></span><br><span class="line">x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">Z1 = model1.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z1, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"개별 모형"</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">Z2 = model2.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z2, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"배깅 모형"</span>)</span><br><span class="line">plt.suptitle(<span class="string">"붓꽃 데이터의 분류 결과"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li>왼쪽은 단일 모형으로 max_depth=10으로 설정한 Decision tree의 decision boundary의 모습이다. 트리의 깊이가 깊으므로 과최적화(overfitting)이 발생되었다. 오른쪽의 그림은 max_depth=2로 설정한 Decision tree모형을 100개 결합한 배깅 모형의 decision boundary 모습이다. 물론 depth를 작게하여 기본적인 모형자체도 과최적화(overfitting)를 방지하였지만, 배깅을 함으로써 모형의 분산이 줄어들며 더 train data에 robust하게 decision boundary가 그려진것을 확인할 수 있다.</li></ul><p><img src="/image/bagging_and_single_model_performance_differ_result.png" alt="배깅과 단일모형간의 decision boundary의 차이"></p><h2 id="랜덤-포레스트-Random-Forest"><a href="#랜덤-포레스트-Random-Forest" class="headerlink" title="랜덤 포레스트(Random Forest)"></a>랜덤 포레스트(Random Forest)</h2><ul><li>아래 그림과 같이 배깅은 여러 모델들을 결합하지만 부트스트랩 방식을 사용하여 모델들간의 사용되어지는 데이터가 동일한 집합들이 있을 수 있다. 그러므로 <code>Ensemble Learning의 개념에서 언급했었던 각 모델별 독립이라는 가정에 크게 위반</code>되어진다. 결국 비슷한 트리가 만들어지게 되어 모델들간의 공분산이 크게 되어 모델이 많아짐에 따라 점점 전체 Ensemble 모델의 분산은 커진다는 것이다. 분산이 커진다면 편향이 감소되어 더 좋은것이 아닌가라고 생각이 들수도 있겠지만, 모델의 예측 성능의 변동폭이 너무 크게 되면(분산이 크게 되어) 그만큼 불확신성도 높아지기 때문이다. 게다가, 애초에 다양한 모델에 대한 결합을 한 Ensemble 모델을 만들려고 한 의도조차 변질되어진다.</li></ul><p><img src="/image/why_is_randomforest_better_than_bagging.png" alt="배깅의 유의점 보완을 위한 방안"></p><ul><li>랜덤포레스트(Random Forest)는 의사 결정 나무(Decision Tree)를 개별 모형으로 사용하는 모형 결합 방법을 말한다. 랜덤 포레스트는 데이터 특징차원의 일부만 선택하여 사용한다. 하지만 <code>노드 분리시 모든 독립 변수들을 비교하여 최선의 독립 변수를 선택하는 것이 아니라 독립 변수 차원을 랜덤하게 감소시킨 다음 그 중에서 독립 변수를 선택</code>한다. 이렇게 하면 <code>개별 모형들 사이의 상관관계가 줄어들기 때문에 모형 성능의 변동이 감소하는 효과</code>가 있다. 이러한 방법을 극단적으로 적용한 것이 Extremely Randomized Trees 모형으로 이 경우에는 각 노드에서 랜덤하게 독립 변수를 선택한다.</li></ul><p><img src="/image/random_forest_conception.png" alt="랜덤포레스트의 개념"></p><p><img src="/image/random_forest_how_to_decide_parameter.png" alt="랜덤 포레스트의 개념 및 특징"></p><ul><li>랜덤 포레스트와 Extremely Randomized Trees 모형은 각각 <code>RandomForestClassifier</code> 클래스와 <code>ExtraTreesClassifier</code> 클래스로 구현되어 있다.</li></ul><ul><li>랜덤 포레스트는 CPU 병렬 처리도 효과적으로 수행되어 빠른 학습이 가능하기 때문에 뒤에 소개할 그레디언트 부스팅보다 예측 성능이 약간 떨어지더라도 랜덤 포레스트로 일단 기반 모델을 먼저 구축하는 경우가 많다. 멀티 코어 환경에서는 RandomForestClassifier 생성자와 GridSearchCV 생성 시 n_jobs = -1 파라미터를 추가하면 모든 CPU 코어을 이용해 학습할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data[:, [0, 2]], iris.target</span><br><span class="line"></span><br><span class="line">model1 = DecisionTreeClassifier(max_depth=10, random_state=0).fit(X, y)</span><br><span class="line">model2 = RandomForestClassifier(max_depth=2, n_estimators=100, random_state=0).fit(X, y)</span><br><span class="line"></span><br><span class="line">x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">Z1 = model1.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z1, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"개별 모형"</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">Z2 = model2.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z2, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"배깅 모형"</span>)</span><br><span class="line">plt.suptitle(<span class="string">"붓꽃 데이터의 분류 결과"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li>아래 그림에서 오른쪽은 max_depth=2로 설정하고 모형의 수를 100개로 한 RandomForest 모델의 decision boundary의 시각화한 것이다.</li></ul><p><img src="/image/randomforest_decision_boundary_reuslt_viz.png" alt="랜덤포레스트와 단일 Decision Tree 모형의 decision boundary 비교"></p><ul><li>랜덤 포레스트의 장점 중 하나는 각 독립 변수의 중요도(feature importance)를 계산할 수 있다는 점이다. 포레스트 안에서 사용된 모든 노드에 대해 어떤 독립 변수를 사용하였고 그 노드에서 얻은 information gain을 구할 수 있으므로 각각의 독립 변수들이 얻어낸 information gain의 평균을 비교하면 어떤 독립 변수가 중요한지를 비교할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_classification</span><br><span class="line">from sklearn.ensemble import ExtraTreesClassifier</span><br><span class="line"></span><br><span class="line">X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0,</span><br><span class="line">                           n_classes=2, random_state=0, shuffle=False)</span><br><span class="line"></span><br><span class="line">forest = ExtraTreesClassifier(n_estimators=250, random_state=0)</span><br><span class="line">forest.fit(X, y)</span><br><span class="line"></span><br><span class="line">importances = forest.feature_importances_</span><br><span class="line"></span><br><span class="line">std = np.std([tree.feature_importances_ <span class="keyword">for</span> tree <span class="keyword">in</span> forest.estimators_], axis=0)</span><br><span class="line">indices = np.argsort(importances)[::-1]</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"특성 중요도"</span>)</span><br><span class="line">plt.bar(range(X.shape[1]), importances[indices],</span><br><span class="line">        color=<span class="string">"r"</span>, yerr=std[indices], align=<span class="string">"center"</span>)</span><br><span class="line">plt.xticks(range(X.shape[1]), indices)</span><br><span class="line">plt.xlim([-1, X.shape[1]])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/feature_importance_using_random_forests.png" alt="랜덤포레스트를 통한 특성 중요도"></p><ul><li>다음은 올리베티 얼굴 사진을 Extreme 랜덤 포레스트로 구한 뒤 특징(pixel) 중요도를 이미지로 나타낸 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_olivetti_faces</span><br><span class="line">from sklearn.ensemble import ExtraTreesClassifier</span><br><span class="line"></span><br><span class="line">data = fetch_olivetti_faces()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line"></span><br><span class="line">forest = ExtraTreesClassifier(n_estimators=1000, random_state=0)</span><br><span class="line">forest.fit(X, y)</span><br><span class="line"></span><br><span class="line">importances = forest.feature_importances_</span><br><span class="line">importances = importances.reshape(data.images[0].shape)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8, 8))</span><br><span class="line">plt.imshow(importances, cmap=plt.cm.bone_r)</span><br><span class="line">plt.grid(False)</span><br><span class="line">plt.title(<span class="string">"픽셀 중요도(pixel importance)"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/pixel_importance_using_random_forests.png" alt="랜덤포레스트를 이용해 시각화한 픽셀 중요도"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/02/machine_learning_14/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 05 - 트리(Tree)</title>
      <link>https://heung-bae-lee.github.io/2020/05/02/data_structure_06/</link>
      <guid>https://heung-bae-lee.github.io/2020/05/02/data_structure_06/</guid>
      <pubDate>Sat, 02 May 2020 10:27:21 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;대표적인-데이터-구조7-트리&quot;&gt;&lt;a href=&quot;#대표적인-데이터-구조7-트리&quot; class=&quot;headerlink&quot; title=&quot;대표적인 데이터 구조7: 트리&quot;&gt;&lt;/a&gt;대표적인 데이터 구조7: 트리&lt;/h2&gt;&lt;h3 id=&quot;1-트리-Tree-구
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="대표적인-데이터-구조7-트리"><a href="#대표적인-데이터-구조7-트리" class="headerlink" title="대표적인 데이터 구조7: 트리"></a>대표적인 데이터 구조7: 트리</h2><h3 id="1-트리-Tree-구조"><a href="#1-트리-Tree-구조" class="headerlink" title="1. 트리 (Tree) 구조"></a>1. 트리 (Tree) 구조</h3><ul><li>트리: <code>Node와 Branch를 이용해서, 사이클을 이루지 않도록 구성한 데이터 구조</code><ul><li><code>트리는 connected acyclic graph구조로 즉, 1개 이상의 노드로 이루어진 유한 집합이다.</code></li><li>루트 노드(root)를 반드시 가진다.</li><li>트리를 구성하는 노드 간에 단순 경로가 존재 또는 루트노드를 제외하고 나머지 노드들은 <code>서로 연결 될 수 없는 분리집합</code> $T_{1}, T_{2}, \ldots, T_{n}$으로 분할 가능하다. $T_{1}, T_{2}, \ldots, T_{n}$등은 각각 하나의 트리(서브 트리)로 재귀적 정의로서 표현 할 수도 있다.</li></ul></li><li>실제로 어디에 많이 사용되나?<ul><li>트리 중 이진 트리 (Binary Tree) 형태의 구조로, <code>탐색(검색) 알고리즘 구현을 위해 많이 사용</code>됨</li></ul></li></ul><h3 id="2-알아둘-용어"><a href="#2-알아둘-용어" class="headerlink" title="2. 알아둘 용어"></a>2. 알아둘 용어</h3><ul><li>Node: 트리에서 데이터를 저장하는 기본 요소 (데이터와 다른 연결된 노드에 대한 Branch 정보 포함)</li><li>Degree : 어떤 노드의 자식 노드의 개수</li><li>Degree of a tree : 트리에 있는 노드의 최대 차수</li><li>Root Node: 트리 맨 위에 있는 노드</li><li>Level: 최상위 노드를 Level 0으로 하였을 때, 하위 Branch로 연결된 노드의 깊이를 나타냄</li><li>Parent Node: 어떤 노드의 다음 레벨에 연결된 노드</li><li>Child Node: 어떤 노드의 상위 레벨에 연결된 노드</li><li>Leaf Node (Terminal Node): Child Node가 하나도 없는 노드, 즉 Degree가 0인 노드</li><li>Sibling (Brother Node): 동일한 Parent Node를 가진 노드</li><li>Depth(Height): 트리에서 Node가 가질 수 있는 최대 Level</li><li>forest: 루트 노드를 없앤 후 얻은 서브 트리의 집합<br><img src="http://www.fun-coding.org/00_Images/tree.png" width="600"></li></ul><p><img src="/image/tree_structure_conception.png" alt="트리의 구조"></p><p><img src="/image/what_is_binary_tree_conception.png" alt="이진 트리(binary tree)"></p><p><img src="/image/relationship_between_node_and_edge.png" alt="노드와 엣지의 관계"></p><ul><li><p>이진 트리(binary tree)의 특징</p><ul><li>1) level i에서 최대 노드 수 : $2^{i-1}$</li><li>2) 높이가 h인 이진 트리의 최대 노드 수 : $2^{h}-1$</li><li>3) 높이가 h인 이진 트리의 최소 노드의 개수 : h</li></ul></li></ul><p><img src="/image/binary_tree_relationship_example.png" alt="이진 트리의 예시"></p><ul><li>이진 트리(binary tree)의 종류</li></ul><p><img src="/image/full_binary_tree.png" alt="이진 트리의 종류 - 포화 이진 트리"></p><p><img src="/image/complete_binary_tree.png" alt="이진 트리의 종류 - 완전 이진 트리"></p><p><img src="/image/skewed_binary_tree.png" alt="이진 트리의 종류 - 편향 이진 트리"></p><ul><li><p>이진 트리는 말 그대로 부모노드가 가질수 있는 자식노드가 최대 2개라는 의미이다. 그러한 이진 트리의 데이터를 탐색하는 방법을 설명하면 다음과 같은 방법들이 있다.</p></li><li><p>전위 순회(preorder traversal)</p><ul><li><code>현재 노드</code>가 처음 그리고 <code>왼쪽 자식 노드</code> 다음 <code>오른쪽 자식노드</code>를 탐색하게 하는 방식으로 <code>노드에 값이 존재하지 않을 때까지 순회(재귀)하도록</code>하는 탐색방식이다.</li></ul></li></ul><p><img src="/image/preorder_traversal.png" alt="전위 순회(preorder traversal)"></p><ul><li>중위 순회(inorder traversal)<ul><li>현재 노드의 <code>왼쪽 자식노드</code>부터 시작해서 <code>현재 노드</code> 그리고 마지막으로 <code>오른쪽 자식 노드</code>를 순회하는 방법이다.</li></ul></li></ul><p><img src="/image/inorder_traversal.png" alt="중위 순회(inorder traversal)"></p><ul><li>후위 순회(postorder traversal)<ul><li>현재 노드의 <code>왼쪽 자식노드</code>부터 시작해서 현재 노드의 <code>오른쪽 자식노드</code> 그리고 마지막으로 <code>현재노드</code>를 순회하는 방법이다.</li></ul></li></ul><p><img src="/image/postorder_traversal.png" alt="후위 순회(postorder traversal)"></p><ul><li>레벨 순회(level traversal)<ul><li><code>level(depth)순으로 위에서 아래로 그리고 왼쪽부터 오른쪽으로 순회하는 방법</code>이다.</li></ul></li></ul><p><img src="/image/level_traversal.png" alt="레벨 순회(level traversal)"></p><ul><li>위의 이진 트리의 순회를 반복적으로 하기 위해서 이전에 배웠던 자료구조인 스택과 큐를 활용할 것이다. 활용하기 위해서 다시 한번 기억을 떠올리며 만들어 본다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data=None):</span><br><span class="line">        self.__data=data</span><br><span class="line">        self.__next=None</span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def next(self):</span><br><span class="line">        <span class="built_in">return</span> self.__next</span><br><span class="line"></span><br><span class="line">    @next.setter</span><br><span class="line">    def next(self, target):</span><br><span class="line">        self.__next=target</span><br><span class="line"></span><br><span class="line">class My_Queue:</span><br><span class="line">    def __init__(self, head=None, tail=None):</span><br><span class="line">        self.head=head</span><br><span class="line">        self.tail=tail</span><br><span class="line"></span><br><span class="line">    def is_empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.head == None:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line"></span><br><span class="line">    def enqueue(self, data):</span><br><span class="line">        new_node=Node(data)</span><br><span class="line">        <span class="keyword">if</span> self.is_empty():</span><br><span class="line">            self.head=new_node</span><br><span class="line">            self.tail=new_node</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.tail.next=new_node</span><br><span class="line">            self.tail=new_node</span><br><span class="line"></span><br><span class="line">    def dequeue(self):</span><br><span class="line">        <span class="keyword">if</span> self.is_empty():</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"현재 Queue에 노드가 존재하지 않습니다."</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pop_data=self.head</span><br><span class="line">            self.head=self.head.next</span><br><span class="line">            <span class="built_in">return</span> pop_data.data</span><br><span class="line"></span><br><span class="line">class My_stack:</span><br><span class="line">    def __init__(self, head=None, tail=None):</span><br><span class="line">        self.head=head</span><br><span class="line">        self.tail=tail</span><br><span class="line"></span><br><span class="line">    def is_empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.head:</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line"></span><br><span class="line">    def push(self, data):</span><br><span class="line">        new_node=Node(data)</span><br><span class="line">        <span class="keyword">if</span> self.is_empty():</span><br><span class="line">            self.head=new_node</span><br><span class="line">            self.tail=new_node</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new_node.next=self.head</span><br><span class="line">            self.head=new_node</span><br><span class="line"></span><br><span class="line">    def pop(self):</span><br><span class="line">        <span class="keyword">if</span> self.is_empty():</span><br><span class="line">            <span class="built_in">return</span> None</span><br><span class="line">        pop_node=self.head</span><br><span class="line">        self.head=self.head.next</span><br><span class="line"><span class="comment">#         print(pop_node.next)</span></span><br><span class="line">        pop_node.next=None</span><br><span class="line">        <span class="built_in">return</span> pop_node.data</span><br></pre></td></tr></table></figure><hr><ul><li><p>정상적으로 작동하는지 확인하기 위해 테스트를 해본다.</p></li><li><p>Queue 테스트</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">q = My_Queue()</span><br><span class="line">q.enqueue(1)</span><br><span class="line">q.enqueue(2)</span><br><span class="line">q.enqueue(3)</span><br><span class="line">q.enqueue(4)</span><br><span class="line">q.enqueue(5)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> not q.is_empty():</span><br><span class="line">    <span class="built_in">print</span>(q.dequeue(), end=<span class="string">'  '</span>)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1  2  3  4  5</span><br></pre></td></tr></table></figure><ul><li>Stack 테스트</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">s = My_stack()</span><br><span class="line"></span><br><span class="line">s.push(1)</span><br><span class="line">s.push(2)</span><br><span class="line">s.push(3)</span><br><span class="line">s.push(4)</span><br><span class="line">s.push(5)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> not s.is_empty():</span><br><span class="line">    <span class="built_in">print</span>(s.pop(), end=<span class="string">"  "</span>)</span><br></pre></td></tr></table></figure><h4 id><a href="#" class="headerlink" title="#"></a>#</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5  4  3  2  1</span><br></pre></td></tr></table></figure><h3 id="스택-큐는-순회를-구현할때-사용"><a href="#스택-큐는-순회를-구현할때-사용" class="headerlink" title="스택, 큐는 순회를 구현할때 사용"></a>스택, 큐는 순회를 구현할때 사용</h3><ul><li>위의 파일을 스택과 큐를 나누어서 각각 queue1, stack1이라는 python 파일로 저장하였다.</li></ul><h4 id="큐-레벨-순서-순회시-필요"><a href="#큐-레벨-순서-순회시-필요" class="headerlink" title="큐 : 레벨 순서 순회시 필요"></a>큐 : 레벨 순서 순회시 필요</h4><h4 id="스택-전위-중위-후위-순회를-반복문으로-구현할때-필요"><a href="#스택-전위-중위-후위-순회를-반복문으로-구현할때-필요" class="headerlink" title="스택 : 전위, 중위, 후위 순회를 반복문으로 구현할때 필요"></a>스택 : 전위, 중위, 후위 순회를 반복문으로 구현할때 필요</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from queue1 import My_Queue</span><br><span class="line">from stack1 import My_Stack</span><br></pre></td></tr></table></figure><ul><li>기본적으로 Tree의 노드를 먼저 만들어 본다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">class TreeNode:</span><br><span class="line">    def __init__(self, data=None):</span><br><span class="line">        <span class="comment"># 데이터 저장</span></span><br><span class="line">        self.__data=data</span><br><span class="line">        <span class="comment"># 왼쪽 자식 노드</span></span><br><span class="line">        self.__left=None</span><br><span class="line">        <span class="comment"># 오른쪽 자식 노드</span></span><br><span class="line">        self.__right=None</span><br><span class="line"></span><br><span class="line">    def __del__(self):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'data &#123;&#125; is deleted'</span>.format(self.__data))</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def left(self):</span><br><span class="line">        <span class="built_in">return</span> self.__left</span><br><span class="line"></span><br><span class="line">    @left.setter</span><br><span class="line">    def left(self, left):</span><br><span class="line">        self.__left=left</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def right(self):</span><br><span class="line">        <span class="built_in">return</span> self.__right</span><br><span class="line"></span><br><span class="line">    @right.setter</span><br><span class="line">    def right(self, right):</span><br><span class="line">        self.__right=right</span><br></pre></td></tr></table></figure><h3 id="순회-traversal"><a href="#순회-traversal" class="headerlink" title="순회(traversal)"></a>순회(traversal)</h3><ul><li><p>순회(traversal)를 하는 코드를 작성하는 방법은 크게 2가지를 생각해 볼 수 있다.</p><ul><li><p>재귀 : 성능은 떨어지지만, 가독성이 높다.</p></li><li><p>반복문 : 성능은 높지만, 가독성이 떨어진다.(실제로는 성능이 최우선이므로 반복문을 사용할 수 있다면, 반복문을 사용하는 것이 좋다.)</p></li></ul></li></ul><h4 id="재귀함수를-통한-순회-코드-작성"><a href="#재귀함수를-통한-순회-코드-작성" class="headerlink" title="재귀함수를 통한 순회 코드 작성"></a>재귀함수를 통한 순회 코드 작성</h4><ul><li>전위, 순위, 후위는 현재 노드의 데이터를 프린트해주는 순서에 의해서 결정될 것이다. 각 함수는공통된 코드를 갖으며, 우선 현재 노드의 왼쪽노드, 오른쪽 노드 순으로 각 함수를 재귀적으로 동작한다.</li></ul><h3 id="preorder-전위"><a href="#preorder-전위" class="headerlink" title="preorder(전위)"></a>preorder(전위)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def preorder(cur):</span><br><span class="line">    <span class="comment"># base case</span></span><br><span class="line">    <span class="keyword">if</span> not cur: <span class="comment"># cur가 empty node라면</span></span><br><span class="line">        <span class="built_in">return</span></span><br><span class="line">    <span class="comment"># 방문 코드</span></span><br><span class="line">    <span class="built_in">print</span>(cur.data, end = <span class="string">" "</span>)</span><br><span class="line">    <span class="comment"># 재귀</span></span><br><span class="line">    preorder(cur.left) <span class="comment">#여기서 실행이 끝났다는 얘기는 왼쪽은 다 순회를 했으므로 오른쪽으로 이동</span></span><br><span class="line">    <span class="comment"># 재귀</span></span><br><span class="line">    preorder(cur.right)</span><br></pre></td></tr></table></figure><h3 id="inorder-중위"><a href="#inorder-중위" class="headerlink" title="inorder(중위)"></a>inorder(중위)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def inorder(cur):</span><br><span class="line">    <span class="comment"># base case</span></span><br><span class="line">    <span class="keyword">if</span> not cur: <span class="comment"># cur가 empty node라면</span></span><br><span class="line">        <span class="built_in">return</span></span><br><span class="line">    <span class="comment"># 재귀</span></span><br><span class="line">    inorder(cur.left) <span class="comment">#여기서 실행이 끝났다는 얘기는 다 순회를 했으므로 오른쪽으로 이동</span></span><br><span class="line">    <span class="comment"># 방문 코드</span></span><br><span class="line">    <span class="built_in">print</span>(cur.data, end = <span class="string">" "</span>)</span><br><span class="line">    <span class="comment"># 재귀</span></span><br><span class="line">    inorder(cur.right)</span><br></pre></td></tr></table></figure><h3 id="postorder-후위"><a href="#postorder-후위" class="headerlink" title="postorder(후위)"></a>postorder(후위)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def postorder(cur):</span><br><span class="line">    <span class="comment"># base case</span></span><br><span class="line">    <span class="keyword">if</span> not cur: <span class="comment"># cur가 empty node라면</span></span><br><span class="line">        <span class="built_in">return</span></span><br><span class="line">    <span class="comment"># 재귀</span></span><br><span class="line">    postorder(cur.left) <span class="comment">#여기서 실행이 끝났다는 얘기는 다 순회를 했으므로 오른쪽으로 이동</span></span><br><span class="line">    <span class="comment"># 재귀</span></span><br><span class="line">    postorder(cur.right)</span><br><span class="line">    <span class="comment"># 방문 코드</span></span><br><span class="line">    <span class="built_in">print</span>(cur.data, end = <span class="string">" "</span>)</span><br></pre></td></tr></table></figure><h4 id="반복문을-통한-순회-코드-작성"><a href="#반복문을-통한-순회-코드-작성" class="headerlink" title="반복문을 통한 순회 코드 작성"></a>반복문을 통한 순회 코드 작성</h4><ul><li>1) 스택(Stack) : 재귀함수로 전위, 중위, 후위를 구현한다는 것은 각 함수가 계속쌓이는 것과 동일하므로 스택을 사용한 것이라고 할 수 있음.</li><li>2) 큐(Queue) :</li></ul><h4 id="iter-preorder"><a href="#iter-preorder" class="headerlink" title="iter_preorder"></a>iter_preorder</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def iter_preorder(cur):</span><br><span class="line">    s=My_stack()</span><br><span class="line">    <span class="keyword">while</span> True:</span><br><span class="line">        <span class="keyword">while</span> cur: <span class="comment">#cur가 있다면 왼쪽으로 계속 내려감.</span></span><br><span class="line">            <span class="comment"># 방문</span></span><br><span class="line">            <span class="built_in">print</span>(cur.data, end = <span class="string">"  "</span>)</span><br><span class="line">            s.push(cur)</span><br><span class="line">            cur = cur.left</span><br><span class="line">        cur=s.pop()</span><br><span class="line">        <span class="keyword">if</span> not cur:</span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line">        cur=cur.right</span><br></pre></td></tr></table></figure><h4 id="iter-inorder"><a href="#iter-inorder" class="headerlink" title="iter_inorder"></a>iter_inorder</h4><ul><li>2가지 방식으로 구현해보았다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def iter_inorder(cur):</span><br><span class="line">    s=My_stack()</span><br><span class="line">    <span class="keyword">while</span> True:</span><br><span class="line">        <span class="keyword">while</span> cur: <span class="comment">#cur가 있다면 왼쪽으로 계속 내려감.</span></span><br><span class="line">            s.push(cur)</span><br><span class="line">            cur = cur.left</span><br><span class="line">        cur=s.pop()</span><br><span class="line">        <span class="keyword">if</span> not cur:</span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line">        <span class="comment"># 방문</span></span><br><span class="line">        <span class="built_in">print</span>(cur.data, end = <span class="string">"  "</span>)</span><br><span class="line">        cur=cur.right</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">def iter_inorder(cur):</span><br><span class="line">    stack_ls=My_stack()</span><br><span class="line">    <span class="keyword">while</span> True:</span><br><span class="line">        <span class="keyword">while</span> cur.left:</span><br><span class="line">            stack_ls.push(cur)</span><br><span class="line">            cur=cur.left</span><br><span class="line">        <span class="built_in">print</span>(cur.data, end = <span class="string">"  "</span>)</span><br><span class="line">        <span class="keyword">if</span> stack_ls.is_empty():</span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line">        cur=stack_ls.pop()</span><br><span class="line">        <span class="built_in">print</span>(cur.data, end = <span class="string">"  "</span>)</span><br><span class="line">        cur=cur.right</span><br></pre></td></tr></table></figure><h4 id="iter-postorder"><a href="#iter-postorder" class="headerlink" title="iter_postorder"></a>iter_postorder</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def iter_postorder(cur):</span><br><span class="line">    s1=My_stack()</span><br><span class="line">    s2=My_stack()</span><br><span class="line">    s1.push(cur)</span><br><span class="line">    <span class="keyword">while</span> s1.is_empty():</span><br><span class="line">        cur=s1.pop()</span><br><span class="line">        <span class="keyword">if</span> not cur.left:</span><br><span class="line">            s1.push(cur.left)</span><br><span class="line">        <span class="keyword">if</span> not cur.right:</span><br><span class="line">            s1.push(cur.right)</span><br><span class="line">        s2.push(cur)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> not s2.is_empty():</span><br><span class="line">        cur=s2.pop()</span><br><span class="line">        <span class="built_in">print</span>(cur.data, end=<span class="string">"  "</span>)</span><br></pre></td></tr></table></figure><h4 id="levelorder"><a href="#levelorder" class="headerlink" title="levelorder"></a>levelorder</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def levelorder(cur):</span><br><span class="line">    q = My_Queue()</span><br><span class="line">    q.enqueue(cur)</span><br><span class="line">    <span class="keyword">while</span> not q.is_empty():</span><br><span class="line">        cur = q.dequeue()</span><br><span class="line">        <span class="comment">#방문</span></span><br><span class="line">        <span class="built_in">print</span>(cur.data, end= <span class="string">"  "</span>)</span><br><span class="line">        <span class="keyword">if</span> cur.left :</span><br><span class="line">            q.enqueue(cur.left)</span><br><span class="line">        <span class="keyword">if</span> cur.right :</span><br><span class="line">            q.enqueue(cur.right)</span><br></pre></td></tr></table></figure><h5 id="순회-테스트"><a href="#순회-테스트" class="headerlink" title="순회 테스트"></a>순회 테스트</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">n1=TreeNode(1)</span><br><span class="line">n2=TreeNode(2)</span><br><span class="line">n3=TreeNode(3)</span><br><span class="line">n4=TreeNode(4)</span><br><span class="line">n5=TreeNode(5)</span><br><span class="line">n6=TreeNode(6)</span><br><span class="line">n7=TreeNode(7)</span><br><span class="line"></span><br><span class="line">n1.left=n2; n1.right=n3</span><br><span class="line">n2.left=n4; n2.right=n5</span><br><span class="line">n3.left=n6; n3.right=n7</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">preorder(n1)</span><br></pre></td></tr></table></figure><h6 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 4 5 3 6 7</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inorder(n1)</span><br></pre></td></tr></table></figure><h6 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4  2  5  1  6  3  7</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">postorder(n1)</span><br></pre></td></tr></table></figure><h6 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4  5  2  6  7  3  1</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iter_preorder(n1)</span><br></pre></td></tr></table></figure><h6 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1  2  4  5  3  6  7</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iter_inorder(n1)</span><br></pre></td></tr></table></figure><h6 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4  2  5  1  6  3  7</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iter_postorder(n1)</span><br></pre></td></tr></table></figure><h6 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4  5  2  6  7  3  1</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">levelorder(n1)</span><br></pre></td></tr></table></figure><h6 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1  2  3  4  5  6  7</span><br></pre></td></tr></table></figure><hr><h3 id="4-자료-구조-이진-탐색-트리의-장점과-주요-용도"><a href="#4-자료-구조-이진-탐색-트리의-장점과-주요-용도" class="headerlink" title="4. 자료 구조 이진 탐색 트리의 장점과 주요 용도"></a>4. 자료 구조 이진 탐색 트리의 장점과 주요 용도</h3><ul><li>주요 용도: 데이터 검색(탐색)</li><li>장점: 탐색 속도를 개선할 수 있음</li></ul><blockquote><p>단점은 이진 탐색 트리 알고리즘 이해 후에 살펴보기로 함</p><ul><li>아래 이미에서 볼 수 있듯이 이진 탐색 트리 알고리즘은 3 step안에 27이라는 숫자를 찾아내지만, 배열(array)에서는 하나씩 비교하면 찾기 때문에 훨씬 많은 step을 거쳐야 한다.</li></ul></blockquote><ul><li>이와 같이 검색하기 위한 데이터를 저장해 놓을 때 이진 탐색 트리를 많이 사용한다.</li></ul><h3 id="이진트리와-정렬된-배열간의-탐색-비교"><a href="#이진트리와-정렬된-배열간의-탐색-비교" class="headerlink" title="이진트리와 정렬된 배열간의 탐색 비교"></a>이진트리와 정렬된 배열간의 탐색 비교</h3><p><img src="https://www.mathwarehouse.com/programming/images/binary-search-tree/binary-search-tree-sorted-array-animation.gif"></p><p>(출처: <a href="https://www.mathwarehouse.com/programming/gifs/binary-search-tree.php#binary-search-tree-insertion-node" target="_blank" rel="noopener">https://www.mathwarehouse.com/programming/gifs/binary-search-tree.php#binary-search-tree-insertion-node</a>)</p><h3 id="5-파이썬-객체지향-프로그래밍으로-링크드-리스트-구현하기"><a href="#5-파이썬-객체지향-프로그래밍으로-링크드-리스트-구현하기" class="headerlink" title="5. 파이썬 객체지향 프로그래밍으로 링크드 리스트 구현하기"></a>5. 파이썬 객체지향 프로그래밍으로 링크드 리스트 구현하기</h3><h4 id="5-1-노드-클래스-만들기"><a href="#5-1-노드-클래스-만들기" class="headerlink" title="5.1. 노드 클래스 만들기"></a>5.1. 노드 클래스 만들기</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, value):</span><br><span class="line">        self.value = value</span><br><span class="line">        self.left = None</span><br><span class="line">        self.right = None</span><br></pre></td></tr></table></figure><hr><h4 id="5-2-이진-탐색-트리에-데이터-넣기"><a href="#5-2-이진-탐색-트리에-데이터-넣기" class="headerlink" title="5.2. 이진 탐색 트리에 데이터 넣기"></a>5.2. 이진 탐색 트리에 데이터 넣기</h4><ul><li>이진 탐색 트리 조건에 부합하게 데이터를 넣어야 함<ul><li>값이 같거나 크면 오른쪽에 위치하도록하고, 작으면 왼쪽에 위치하도록 코드를 작성함.</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class NodeMgmt:</span><br><span class="line">    def __init__(self, head):</span><br><span class="line">        self.head = head</span><br><span class="line"></span><br><span class="line">    def insert(self, value):</span><br><span class="line">        self.current_node = self.head</span><br><span class="line">        <span class="keyword">while</span> True:</span><br><span class="line">            <span class="keyword">if</span> value &lt; self.current_node.value:</span><br><span class="line">                <span class="keyword">if</span> self.current_node.left != None:</span><br><span class="line">                    self.current_node = self.current_node.left</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.current_node.left = Node(value)</span><br><span class="line">                    <span class="built_in">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> self.current_node.right != None:</span><br><span class="line">                    self.current_node = self.current_node.right</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.current_node.right = Node(value)</span><br><span class="line">                    <span class="built_in">break</span></span><br><span class="line"></span><br><span class="line">    def search(self, target):</span><br><span class="line">        self.current_node = self.head</span><br><span class="line">        <span class="keyword">while</span> self.current_node:</span><br><span class="line">            <span class="keyword">if</span> self.currnet_node.value == target:</span><br><span class="line">                <span class="built_in">return</span> self.current_node</span><br><span class="line">            <span class="keyword">elif</span> self.current_node.value &gt; target:</span><br><span class="line">                self.current_node=self.current_node.left</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.current_node=self.current_node.right</span><br><span class="line">        <span class="built_in">return</span> False</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">head = Node(1)</span><br><span class="line">BST = NodeMgmt(head)</span><br><span class="line">BST.insert(2)</span><br><span class="line">BST.insert(3)</span><br><span class="line">BST.insert(0)</span><br><span class="line">BST.insert(4)</span><br><span class="line">BST.insert(8)</span><br><span class="line">BST.search(-1)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure><hr><h4 id="5-4-이진-탐색-트리-삭제"><a href="#5-4-이진-탐색-트리-삭제" class="headerlink" title="5.4. 이진 탐색 트리 삭제"></a>5.4. 이진 탐색 트리 삭제</h4><ul><li>매우 복잡함. <strong>경우를 나누어서 이해하는 것이 좋음</strong></li></ul><h4 id="5-4-1-Leaf-Node-삭제"><a href="#5-4-1-Leaf-Node-삭제" class="headerlink" title="5.4.1. Leaf Node 삭제"></a>5.4.1. Leaf Node 삭제</h4><ul><li>Leaf Node: Child Node 가 없는 Node</li><li>삭제할 Node의 Parent Node가 삭제할 Node를 가리키지 않도록 한다.<br><img src="http://www.fun-coding.org/00_Images/tree_remove_leaf.png" width="800"></li></ul><h4 id="5-4-2-Child-Node-가-하나인-Node-삭제"><a href="#5-4-2-Child-Node-가-하나인-Node-삭제" class="headerlink" title="5.4.2. Child Node 가 하나인 Node 삭제"></a>5.4.2. Child Node 가 하나인 Node 삭제</h4><ul><li>삭제할 Node의 Parent Node가 삭제할 Node의 Child Node를 가리키도록 한다.<br><img src="http://www.fun-coding.org/00_Images/tree_remove_1child.png" width="800"></li></ul><h4 id="5-4-3-Child-Node-가-두-개인-Node-삭제"><a href="#5-4-3-Child-Node-가-두-개인-Node-삭제" class="headerlink" title="5.4.3. Child Node 가 두 개인 Node 삭제"></a>5.4.3. Child Node 가 두 개인 Node 삭제</h4><ol><li><strong>삭제할 Node의 오른쪽 자식 중, 가장 작은 값을 삭제할 Node의 Parent Node가 가리키도록 한다.</strong></li><li>삭제할 Node의 왼쪽 자식 중, 가장 큰 값을 삭제할 Node의 Parent Node가 가리키도록 한다.<br><img src="http://www.fun-coding.org/00_Images/tree_remove_2child.png" width="800"></li></ol><h5 id="5-4-3-1-삭제할-Node의-오른쪽-자식중-가장-작은-값을-삭제할-Node의-Parent-Node가-가리키게-할-경우"><a href="#5-4-3-1-삭제할-Node의-오른쪽-자식중-가장-작은-값을-삭제할-Node의-Parent-Node가-가리키게-할-경우" class="headerlink" title="5.4.3.1. 삭제할 Node의 오른쪽 자식중, 가장 작은 값을 삭제할 Node의 Parent Node가 가리키게 할 경우"></a>5.4.3.1. 삭제할 Node의 오른쪽 자식중, 가장 작은 값을 삭제할 Node의 Parent Node가 가리키게 할 경우</h5><ul><li>삭제할 Node의 오른쪽 자식 선택</li><li>오른쪽 자식의 가장 왼쪽에 있는 Node를 선택</li><li>해당 Node를 삭제할 Node의 Parent Node의 왼쪽 Branch가 가리키게 함</li><li>해당 Node의 왼쪽 Branch가 삭제할 Node의 왼쪽 Child Node를 가리키게 함</li><li>해당 Node의 오른쪽 Branch가 삭제할 Node의 오른쪽 Child Node를 가리키게 함</li><li>만약 해당 Node가 오른쪽 Child Node를 가지고 있었을 경우에는, 해당 Node의 본래 Parent Node의 왼쪽 Branch가 해당 오른쪽 Child Node를 가리키게 함</li></ul><h3 id="5-5-이진-탐색-트리-삭제-코드-구현과-분석"><a href="#5-5-이진-탐색-트리-삭제-코드-구현과-분석" class="headerlink" title="5.5. 이진 탐색 트리 삭제 코드 구현과 분석"></a>5.5. 이진 탐색 트리 삭제 코드 구현과 분석</h3><h4 id="5-5-1-삭제할-Node-탐색"><a href="#5-5-1-삭제할-Node-탐색" class="headerlink" title="5.5.1 삭제할 Node 탐색"></a>5.5.1 삭제할 Node 탐색</h4><ul><li>삭제할 Node가 없는 경우도 처리해야 함<ul><li>이를 위해 삭제할 Node가 없는 경우는 False를 리턴하고, 함수를 종료 시킴</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def delete(self, value):</span><br><span class="line">    searched = False</span><br><span class="line">    self.current_node = self.head</span><br><span class="line">    self.parent = self.head</span><br><span class="line">    <span class="keyword">while</span> self.current_node:</span><br><span class="line">        <span class="keyword">if</span> self.current_node.value == value:</span><br><span class="line">            searched = True</span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line">        <span class="keyword">elif</span> value &lt; self.current_node.value:</span><br><span class="line">            self.parent = self.current_node</span><br><span class="line">            self.current_node = self.current_node.left</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.parent = self.current_node</span><br><span class="line">            self.current_node = self.current_node.right</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> searched == False:</span><br><span class="line">        <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line"><span class="comment">### 이후부터 Case들을 분리해서, 코드 작성</span></span><br></pre></td></tr></table></figure><hr><h4 id="5-5-2-Case1-삭제할-Node가-Leaf-Node인-경우"><a href="#5-5-2-Case1-삭제할-Node가-Leaf-Node인-경우" class="headerlink" title="5.5.2. Case1: 삭제할 Node가 Leaf Node인 경우"></a>5.5.2. Case1: 삭제할 Node가 Leaf Node인 경우</h4><p><img src="http://www.fun-coding.org/00_Images/tree_remove_leaf_code.png" width="600"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># self.current_node 가 삭제할 Node, self.parent는 삭제할 Node의 Parent Node인 상태</span></span><br><span class="line">    <span class="keyword">if</span>  self.current_node.left == None and self.current_node.right == None:</span><br><span class="line">        <span class="keyword">if</span> value &lt; self.parent.value:</span><br><span class="line">            self.parent.left = None</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.parent.right = None</span><br><span class="line">        del self.current_node</span><br></pre></td></tr></table></figure><hr><h4 id="5-5-2-Case2-삭제할-Node가-Child-Node를-한-개-가지고-있을-경우"><a href="#5-5-2-Case2-삭제할-Node가-Child-Node를-한-개-가지고-있을-경우" class="headerlink" title="5.5.2. Case2: 삭제할 Node가 Child Node를 한 개 가지고 있을 경우"></a>5.5.2. Case2: 삭제할 Node가 Child Node를 한 개 가지고 있을 경우</h4><p><img src="http://www.fun-coding.org/00_Images/tree_remove_1child_code.png" width="400"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.current_node.left != None and self.current_node.right == None:</span><br><span class="line">    <span class="keyword">if</span> value &lt; self.parent.value:</span><br><span class="line">        self.parent.left = self.current_node.left</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.parent.right = self.current_node.left</span><br><span class="line"><span class="keyword">elif</span> self.current_node.left == None and self.current_node.right != None:</span><br><span class="line">    <span class="keyword">if</span> value &lt; self.parent.value:</span><br><span class="line">        self.parent.left = self.current_node.right</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.parent.right = self.current_node.right</span><br></pre></td></tr></table></figure><hr><h4 id="5-5-3-Case3-1-삭제할-Node가-Child-Node를-두-개-가지고-있을-경우-삭제할-Node가-Parent-Node-왼쪽에-있을-때"><a href="#5-5-3-Case3-1-삭제할-Node가-Child-Node를-두-개-가지고-있을-경우-삭제할-Node가-Parent-Node-왼쪽에-있을-때" class="headerlink" title="5.5.3. Case3-1: 삭제할 Node가 Child Node를 두 개 가지고 있을 경우 (삭제할 Node가 Parent Node 왼쪽에 있을 때)"></a>5.5.3. Case3-1: 삭제할 Node가 Child Node를 두 개 가지고 있을 경우 (삭제할 Node가 Parent Node 왼쪽에 있을 때)</h4><ul><li>기본 사용 가능 전략<ol><li><strong>삭제할 Node의 오른쪽 자식 중, 가장 작은 값을 삭제할 Node의 Parent Node가 가리키도록 한다.</strong></li><li>삭제할 Node의 왼쪽 자식 중, 가장 큰 값을 삭제할 Node의 Parent Node가 가리키도록 한다.</li></ol></li><li>기본 사용 가능 전략 중, 1번 전략을 사용하여 코드를 구현하기로 함<ul><li>경우의 수가 또다시 두가지가 있음<ul><li><strong>Case3-1-1:</strong> 삭제할 Node가 Parent Node의 왼쪽에 있고, 삭제할 Node의 오른쪽 자식 중, 가장 작은 값을 가진 Node의 Child Node가 없을 때</li><li><strong>Case3-1-2:</strong> 삭제할 Node가 Parent Node의 왼쪽에 있고, 삭제할 Node의 오른쪽 자식 중, 가장 작은 값을 가진 Node의 오른쪽에 Child Node가 있을 때<ul><li>가장 작은 값을 가진 Node의 Child Node가 왼쪽에 있을 경우는 없음, 왜냐하면 왼쪽 Node가 있다는 것은 해당 Node보다 더 작은 값을 가진 Node가 있다는 뜻이기 때문임</li></ul></li></ul></li></ul></li></ul><p><img src="http://www.fun-coding.org/00_Images/tree_remove_2child_code_left.png" width="600"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.current_node.left != None and self.current_node.right != None: <span class="comment"># case3</span></span><br><span class="line">    <span class="keyword">if</span> value &lt; self.parent.value: <span class="comment"># case3-1</span></span><br><span class="line">        self.change_node = self.current_node.right</span><br><span class="line">        self.change_node_parent = self.current_node.right</span><br><span class="line">        <span class="keyword">while</span> self.change_node.left != None:</span><br><span class="line">            self.change_node_parent = self.change_node</span><br><span class="line">            self.change_node = self.change_node.left</span><br><span class="line">        <span class="keyword">if</span> self.change_node.right != None:</span><br><span class="line">            self.change_node_parent.left = self.change_node.right</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.change_node_parent.left = None</span><br><span class="line">        self.parent.left = self.change_node</span><br><span class="line">        self.change_node.right = self.current_node.right</span><br><span class="line">        self.change_node.left = self.current_node.left</span><br></pre></td></tr></table></figure><hr><h4 id="5-5-4-Case3-2-삭제할-Node가-Child-Node를-두-개-가지고-있을-경우-삭제할-Node가-Parent-Node-오른쪽에-있을-때"><a href="#5-5-4-Case3-2-삭제할-Node가-Child-Node를-두-개-가지고-있을-경우-삭제할-Node가-Parent-Node-오른쪽에-있을-때" class="headerlink" title="5.5.4. Case3-2: 삭제할 Node가 Child Node를 두 개 가지고 있을 경우 (삭제할 Node가 Parent Node 오른쪽에 있을 때)"></a>5.5.4. Case3-2: 삭제할 Node가 Child Node를 두 개 가지고 있을 경우 (삭제할 Node가 Parent Node 오른쪽에 있을 때)</h4><ul><li>기본 사용 가능 전략<ol><li><strong>삭제할 Node의 오른쪽 자식 중, 가장 작은 값을 삭제할 Node의 Parent Node가 가리키도록 한다.</strong></li><li>삭제할 Node의 왼쪽 자식 중, 가장 큰 값을 삭제할 Node의 Parent Node가 가리키도록 한다.</li></ol></li><li>기본 사용 가능 전략 중, 1번 전략을 사용하여 코드를 구현하기로 함<ul><li>경우의 수가 또다시 두가지가 있음<ul><li><strong>Case3-2-1:</strong> 삭제할 Node가 Parent Node의 오른쪽에 있고, 삭제할 Node의 오른쪽 자식 중, 가장 작은 값을 가진 Node의 Child Node가 없을 때</li><li><strong>Case3-2-2:</strong> 삭제할 Node가 Parent Node의 오른쪽에 있고, 삭제할 Node의 오른쪽 자식 중, 가장 작은 값을 가진 Node의 오른쪽에 Child Node가 있을 때<ul><li>가장 작은 값을 가진 Node의 Child Node가 왼쪽에 있을 경우는 없음, 왜냐하면 왼쪽 Node가 있다는 것은 해당 Node보다 더 작은 값을 가진 Node가 있다는 뜻이기 때문임</li></ul></li></ul></li></ul></li></ul><p><img src="http://www.fun-coding.org/00_Images/tree_remove_2child_code_right.png" width="600"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    self.change_node = self.current_node.right</span><br><span class="line">    self.change_node_parent = self.current_node.right</span><br><span class="line">    <span class="keyword">while</span> self.change_node.left != None:</span><br><span class="line">        self.change_node_parent = self.change_node</span><br><span class="line">        self.change_node = self.change_node.left</span><br><span class="line">    <span class="keyword">if</span> self.change_node.right != None:</span><br><span class="line">        self.change_node_parent.left = self.change_node.right</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.change_node_parent.left = None</span><br><span class="line">    self.parent.right = self.change_node</span><br><span class="line">    self.change_node.left = self.current_node.left</span><br><span class="line">    self.change_node.right = self.current_node.right</span><br></pre></td></tr></table></figure><hr><h4 id="5-5-5-파이썬-전체-코드-구현"><a href="#5-5-5-파이썬-전체-코드-구현" class="headerlink" title="5.5.5. 파이썬 전체 코드 구현"></a>5.5.5. 파이썬 전체 코드 구현</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, value):</span><br><span class="line">        self.value = value</span><br><span class="line">        self.left = None</span><br><span class="line">        self.right = None</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class NodeMgmt:</span><br><span class="line">    def __init__(self, head):</span><br><span class="line">        self.head = head</span><br><span class="line"></span><br><span class="line">    def insert(self, value):</span><br><span class="line">        self.current_node = self.head</span><br><span class="line">        <span class="keyword">while</span> True:</span><br><span class="line">            <span class="keyword">if</span> value &lt; self.current_node.value:</span><br><span class="line">                <span class="keyword">if</span> self.current_node.left != None:</span><br><span class="line">                    self.current_node = self.current_node.left</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.current_node.left = Node(value)</span><br><span class="line">                    <span class="built_in">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> self.current_node.right != None:</span><br><span class="line">                    self.current_node = self.current_node.right</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.current_node.right = Node(value)</span><br><span class="line">                    <span class="built_in">break</span></span><br><span class="line"></span><br><span class="line">    def search(self, value):</span><br><span class="line">        self.current_node = self.head</span><br><span class="line">        <span class="keyword">while</span> self.current_node:</span><br><span class="line">            <span class="keyword">if</span> self.current_node.value == value:</span><br><span class="line">                <span class="built_in">return</span> True</span><br><span class="line">            <span class="keyword">elif</span> value &lt; self.current_node.value:</span><br><span class="line">                self.current_node = self.current_node.left</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.current_node = self.current_node.right</span><br><span class="line">        <span class="built_in">return</span> False        </span><br><span class="line"></span><br><span class="line">    def delete(self, value):</span><br><span class="line">        <span class="comment"># 삭제할 노드 탐색</span></span><br><span class="line">        searched = False</span><br><span class="line">        self.current_node = self.head</span><br><span class="line">        self.parent = self.head</span><br><span class="line">        <span class="keyword">while</span> self.current_node:</span><br><span class="line">            <span class="keyword">if</span> self.current_node.value == value:</span><br><span class="line">                searched = True</span><br><span class="line">                <span class="built_in">break</span></span><br><span class="line">            <span class="keyword">elif</span> value &lt; self.current_node.value:</span><br><span class="line">                self.parent = self.current_node</span><br><span class="line">                self.current_node = self.current_node.left</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.parent = self.current_node</span><br><span class="line">                self.current_node = self.current_node.right</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> searched == False:</span><br><span class="line">            <span class="built_in">return</span> False    </span><br><span class="line"></span><br><span class="line">        <span class="comment"># case1</span></span><br><span class="line">        <span class="keyword">if</span>  self.current_node.left == None and self.current_node.right == None:</span><br><span class="line">            <span class="keyword">if</span> value &lt; self.parent.value:</span><br><span class="line">                self.parent.left = None</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.parent.right = None</span><br><span class="line"></span><br><span class="line">        <span class="comment"># case2</span></span><br><span class="line">        <span class="keyword">elif</span> self.current_node.left != None and self.current_node.right == None:</span><br><span class="line">            <span class="keyword">if</span> value &lt; self.parent.value:</span><br><span class="line">                self.parent.left = self.current_node.left</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.parent.right = self.current_node.left</span><br><span class="line">        <span class="keyword">elif</span> self.current_node.left == None and self.current_node.right != None:</span><br><span class="line">            <span class="keyword">if</span> value &lt; self.parent.value:</span><br><span class="line">                self.parent.left = self.current_node.right</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.parent.right = self.current_node.right        </span><br><span class="line"></span><br><span class="line">        <span class="comment"># case 3</span></span><br><span class="line">        <span class="keyword">elif</span> self.current_node.left != None and self.current_node.right != None:</span><br><span class="line">            <span class="comment"># case3-1</span></span><br><span class="line">            <span class="keyword">if</span> value &lt; self.parent.value:</span><br><span class="line">                self.change_node = self.current_node.right</span><br><span class="line">                self.change_node_parent = self.current_node.right</span><br><span class="line">                <span class="keyword">while</span> self.change_node.left != None:</span><br><span class="line">                    self.change_node_parent = self.change_node</span><br><span class="line">                    self.change_node = self.change_node.left</span><br><span class="line">                <span class="keyword">if</span> self.change_node.right != None:</span><br><span class="line">                    self.change_node_parent.left = self.change_node.right</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.change_node_parent.left = None</span><br><span class="line">                self.parent.left = self.change_node</span><br><span class="line">                self.change_node.right = self.current_node.right</span><br><span class="line">                self.change_node.left = self.current_node.left</span><br><span class="line">            <span class="comment"># case 3-2</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.change_node = self.current_node.right</span><br><span class="line">                self.change_node_parent = self.current_node.right</span><br><span class="line">                <span class="keyword">while</span> self.change_node.left != None:</span><br><span class="line">                    self.change_node_parent = self.change_node</span><br><span class="line">                    self.change_node = self.change_node.left</span><br><span class="line">                <span class="keyword">if</span> self.change_node.right != None:</span><br><span class="line">                    self.change_node_parent.left = self.change_node.right</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    self.change_node_parent.left = None</span><br><span class="line">                self.parent.right = self.change_node</span><br><span class="line">                self.change_node.right = self.current_node.right</span><br><span class="line">                self.change_node.left = self.current_node.left</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> True</span><br></pre></td></tr></table></figure><hr><p>참고: <a href="http://ejklike.github.io/2018/01/09/traversing-a-binary-tree-1.html" target="_blank" rel="noopener">http://ejklike.github.io/2018/01/09/traversing-a-binary-tree-1.html</a></p><h4 id="5-5-6-파이썬-전체-코드-테스트"><a href="#5-5-6-파이썬-전체-코드-테스트" class="headerlink" title="5.5.6. 파이썬 전체 코드 테스트"></a>5.5.6. 파이썬 전체 코드 테스트</h4><ul><li>random 라이브러리 활용<ul><li>random.randint(첫번째 숫자, 마지막 숫자): 첫번째 숫자부터 마지막 숫자 사이에 있는 숫자를 랜덤하게 선택해서 리턴<ul><li>예: random.randint(0, 99): 0에서 99까지 숫자중 특정 숫자를 랜덤하게 선택해서 리턴해줌</li></ul></li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0 ~ 999 숫자 중에서 임의로 100개를 추출해서, 이진 탐색 트리에 입력, 검색, 삭제</span></span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0 ~ 999 중, 100 개의 숫자 랜덤 선택</span></span><br><span class="line"><span class="comment"># set 자료형을 사용하는 이유는 이진 탐색 트리(BST)에서 입력, 검색, 삭제할 경우 중복이있는 숫자의 경우</span></span><br><span class="line"><span class="comment"># 문제가 있을 수 있으므로 중복이 없도록 해서 100개를 뽑기 위해 아래와 같이 코드를 작성하였다.</span></span><br><span class="line">bst_nums = <span class="built_in">set</span>()</span><br><span class="line"><span class="keyword">while</span> len(bst_nums) != 100:</span><br><span class="line">    bst_nums.add(random.randint(0, 999))</span><br><span class="line"><span class="comment"># print (bst_nums)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 선택된 100개의 숫자를 이진 탐색 트리에 입력, 임의로 루트노드는 500을 넣기로 함</span></span><br><span class="line">head = Node(500)</span><br><span class="line">binary_tree = NodeMgmt(head)</span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> bst_nums:</span><br><span class="line">    binary_tree.insert(num)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력한 100개의 숫자 검색 (검색 기능 확인)</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> bst_nums:</span><br><span class="line">    <span class="keyword">if</span> binary_tree.search(num) == False:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">'search failed'</span>, num)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력한 100개의 숫자 중 10개의 숫자를 랜덤 선택</span></span><br><span class="line">delete_nums = <span class="built_in">set</span>()</span><br><span class="line">bst_nums = list(bst_nums)</span><br><span class="line"><span class="keyword">while</span> len(delete_nums) != 10:</span><br><span class="line">    delete_nums.add(bst_nums[random.randint(0, 99)])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 선택한 10개의 숫자를 삭제 (삭제 기능 확인)</span></span><br><span class="line"><span class="keyword">for</span> del_num <span class="keyword">in</span> delete_nums:</span><br><span class="line">    <span class="keyword">if</span> binary_tree.delete(del_num) == False:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'delete failed'</span>, del_num)</span><br></pre></td></tr></table></figure><hr><ul><li>위와 같은 방법과 다르게 아래 그림과 같은 BST를 구현해보고자 한다.</li></ul><p><img src="/image/BST_exmple_mine.png" alt="BST 구성"></p><h3 id="Binary-Search-Tree"><a href="#Binary-Search-Tree" class="headerlink" title="Binary Search Tree"></a>Binary Search Tree</h3><ul><li><ol><li>모든 원소는 서로 다은 key를 가진다.</li></ol></li><li><ol><li>왼쪽 서브 트리에 있는 모든 키값들은 루트의 키값보다 작다.</li></ol></li><li><ol><li>오른쪽 서브 트리에 있는 모든 키값들은 루트의 키값보다 크다.</li></ol></li></ul><p><img src="/image/BST_conception_mine_01.png" alt="BST - 개념 01"></p><ul><li><ol><li>왼쪽 서브 트리와 오른쪽 서브 트리도 이진 탐색 트리이다.</li></ol></li></ul><p><img src="/image/BST_conception_mine_02.png" alt="BST - 개념 02"></p><h2 id="binary-tree-py-파일"><a href="#binary-tree-py-파일" class="headerlink" title="- binary_tree.py 파일"></a>- binary_tree.py 파일</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line">from queue1 import My_Queue</span><br><span class="line">from stack1 import MY_stack</span><br><span class="line"></span><br><span class="line">class TreeNode:</span><br><span class="line">    def __init__(self, data=None):</span><br><span class="line">        self.__data=data</span><br><span class="line">        self.__left=None</span><br><span class="line">        self.__right=None</span><br><span class="line"></span><br><span class="line">    def __del__(self):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'data &#123;&#125; is deleted'</span>.format(self.__data))</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def left(self):</span><br><span class="line">        <span class="built_in">return</span> self.__left</span><br><span class="line"></span><br><span class="line">    @left.setter</span><br><span class="line">    def left(self, left):</span><br><span class="line">        self.__left=left</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def right(self):</span><br><span class="line">        <span class="built_in">return</span> self.__right</span><br><span class="line"></span><br><span class="line">    @right.setter</span><br><span class="line">    def right(self, right):</span><br><span class="line">        self.__right=right</span><br><span class="line"></span><br><span class="line">def preorder(cur):</span><br><span class="line">    <span class="keyword">if</span> not cur:</span><br><span class="line">        <span class="built_in">return</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(cur.data, end=<span class="string">'  '</span>)</span><br><span class="line">    preorder(cur.left)</span><br><span class="line">    preorder(cur.right)</span><br><span class="line"></span><br><span class="line">def inorder(cur):</span><br><span class="line">    <span class="keyword">if</span> not cur:</span><br><span class="line">        <span class="built_in">return</span></span><br><span class="line"></span><br><span class="line">    inorder(cur.left)</span><br><span class="line">    <span class="built_in">print</span>(cur.data, end=<span class="string">'  '</span>)</span><br><span class="line">    inorder(cur.right)</span><br><span class="line"></span><br><span class="line">def postorder(cur):</span><br><span class="line">    <span class="keyword">if</span> not cur:</span><br><span class="line">        <span class="built_in">return</span></span><br><span class="line"></span><br><span class="line">    postorder(cur.left)</span><br><span class="line">    postorder(cur.right)</span><br><span class="line">    <span class="built_in">print</span>(cur.data, end=<span class="string">'  '</span>)</span><br><span class="line"></span><br><span class="line">def iter_preorder(cur):</span><br><span class="line">    s=Stack()</span><br><span class="line">    <span class="keyword">while</span> True:</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="built_in">print</span>(cur.data, end=<span class="string">'  '</span>)</span><br><span class="line">            s.push(cur)</span><br><span class="line">            cur=cur.left</span><br><span class="line">        cur=s.pop()</span><br><span class="line">        <span class="keyword">if</span> not cur:</span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line">        cur=cur.right</span><br><span class="line"></span><br><span class="line">def iter_inorder(cur):</span><br><span class="line">    s=Stack()</span><br><span class="line">    <span class="keyword">while</span> True:</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            s.push(cur)</span><br><span class="line">            cur=cur.left</span><br><span class="line">        cur=s.pop()</span><br><span class="line">        <span class="keyword">if</span> not cur:</span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line">        <span class="built_in">print</span>(cur.data, end=<span class="string">'  '</span>)</span><br><span class="line">        cur=cur.right</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def iter_postorder(cur):</span><br><span class="line">    s1=Stack()</span><br><span class="line">    s2=Stack()</span><br><span class="line"></span><br><span class="line">    s1.push(cur)</span><br><span class="line">    <span class="keyword">while</span> not s1.empty():</span><br><span class="line">        cur=s1.pop()</span><br><span class="line">        s2.push(cur)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur.left:</span><br><span class="line">            s1.push(cur.left)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur.right:</span><br><span class="line">            s1.push(cur.right)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> not s2.empty():</span><br><span class="line">        cur=s2.pop()</span><br><span class="line">        <span class="built_in">print</span>(cur.data, end=<span class="string">'  '</span>)</span><br><span class="line"></span><br><span class="line">def levelorder(cur):</span><br><span class="line">    q=Queue()</span><br><span class="line"></span><br><span class="line">    q.enqueue(cur)</span><br><span class="line">    <span class="keyword">while</span> not q.empty():</span><br><span class="line">        cur=q.dequeue()</span><br><span class="line">        <span class="built_in">print</span>(cur.data, end=<span class="string">'  '</span>)</span><br><span class="line">        <span class="keyword">if</span> cur.left:</span><br><span class="line">            q.enqueue(cur.left)</span><br><span class="line">        <span class="keyword">if</span> cur.right:</span><br><span class="line">            q.enqueue(cur.right)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    n1=TreeNode(1)</span><br><span class="line">    n2=TreeNode(2)</span><br><span class="line">    n3=TreeNode(3)</span><br><span class="line">    n4=TreeNode(4)</span><br><span class="line">    n5=TreeNode(5)</span><br><span class="line">    n6=TreeNode(6)</span><br><span class="line">    n7=TreeNode(7)</span><br><span class="line"></span><br><span class="line">    n1.left=n2; n1.right=n3</span><br><span class="line">    n2.left=n4; n2.right=n5</span><br><span class="line">    n3.left=n6; n3.right=n7</span><br><span class="line"></span><br><span class="line">    <span class="comment">#preorder(n1)</span></span><br><span class="line">    iter_preorder(n1)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#inorder(n1)</span></span><br><span class="line">    iter_inorder(n1)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#postorder(n1)</span></span><br><span class="line">    iter_postorder(n1)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br><span class="line">    levelorder(n1)</span><br><span class="line">    <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure><ul><li>앞의 파일의 class를 활용하여 BST를 구현해 볼 것이다. 우선 각 기능을 분할하여 설명할 것이다.<ul><li>앞서 배웠던 순회의 개념 중 preorder 방식으로 노드가 갖고 있는 키값을 확인할 수 있도록 재귀(recursion)를 활용한 함수를 만든다.</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from binary_tree import TreeNode</span><br><span class="line"></span><br><span class="line">class BST:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.root=None</span><br><span class="line"></span><br><span class="line">    def get_root(self):</span><br><span class="line">        <span class="built_in">return</span> self.root</span><br><span class="line"></span><br><span class="line">    def preorder_traverse(self, cur, func, *args, **kwargs):</span><br><span class="line">        <span class="keyword">if</span> not cur:</span><br><span class="line">            <span class="built_in">return</span></span><br><span class="line"></span><br><span class="line">        func(cur, *args, **kwargs) <span class="comment">#앞에서 했던 preorder 재귀함수의 print()대신 방문하는 것을 확인 하는 코드</span></span><br><span class="line">        self.preorder_traverse(cur.left, func, *args, **kwargs)</span><br><span class="line">        self.preorder_traverse(cur.right, func, *args, **kwargs)</span><br></pre></td></tr></table></figure><ul><li>insert 함수는 새로운 키값을 추가해주기 위한 함수로서 현재노드와 크기를 비교하여 작은 값은 왼쪽으로 큰 값은 오른쪽으로 이동하면서 빈 자리에 노드를 추가해 주면된다.</li></ul><p><img src="/image/BST_insert_function.png" alt="BST - insert 함수"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def insert(self, data):</span><br><span class="line">    new_node=TreeNode(data)</span><br><span class="line"></span><br><span class="line">    cur=self.root</span><br><span class="line">    <span class="comment"># 아직 데이터가 하나도 없으면</span></span><br><span class="line">    <span class="comment"># 새로운 노드를 루트로 가져온다.</span></span><br><span class="line">    <span class="keyword">if</span> not cur:</span><br><span class="line">        self.root=new_node</span><br><span class="line">        <span class="built_in">return</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> True:</span><br><span class="line">        parent=cur</span><br><span class="line">        <span class="keyword">if</span> data &lt; cur.data:</span><br><span class="line">            cur=cur.left</span><br><span class="line">            <span class="keyword">if</span> not cur:</span><br><span class="line">                parent.left=new_node</span><br><span class="line">                <span class="built_in">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur=cur.right</span><br><span class="line">            <span class="keyword">if</span> not cur:</span><br><span class="line">                parent.right=new_node</span><br><span class="line">                <span class="built_in">return</span></span><br></pre></td></tr></table></figure><ul><li>키값을 받아 현재 BST에 있는 노드들의 값과 일치하는 노드를 return한다.</li></ul><p><img src="/image/BST_search_function.png" alt="BST - search"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def search(self, target):</span><br><span class="line">    cur=self.root</span><br><span class="line">    while cur:</span><br><span class="line">        if cur.data==target:</span><br><span class="line">            return cur</span><br><span class="line">        elif cur.data &gt; target:</span><br><span class="line">            cur=cur.left</span><br><span class="line">        elif cur.data &lt; target:</span><br><span class="line">            cur=cur.right</span><br><span class="line">    return cur</span><br></pre></td></tr></table></figure><ul><li>target값과 동일한 키값을 갖는 노드를 현재 BST 노드에서 제거하는 함수이다. 위에서 if-else 구문을 통해서 만든 BST와 동일한 기능을 하며, 위에서 만든 방식은 제거할 노드의 자식 노드가 2개인 경우 제거할 노드의 오른쪽 노드 서브트리에서 제일 작은 값으로 바꿔주었지만, 아래의 코드는 <code>제거할 노드의 왼쪽 노드 서브트리 중에 가장 큰 값으로 바꿔주는 방법</code>만 다르다.</li></ul><p><img src="/image/BST_remove_function_case.png" alt="BST - remove 함수"></p><p><img src="/image/BST_remove_function_case01_01.png" alt="BST - remove 함수 리프노드제거하는 경우 - 01"></p><p><img src="/image/BST_remove_function_case01_02.png" alt="BST - remove 함수 리프노드제거하는 경우 - 02"></p><p><img src="/image/BST_remove_function_case01_03.png" alt="BST - remove 함수 리프노드제거하는 경우 - 03"></p><p><img src="/image/BST_remove_function_case01_04.png" alt="BST - remove 함수 리프노드제거하는 경우 - 04"></p><p><img src="/image/BST_remove_function_case02.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 1개인 경우"></p><p><img src="/image/BST_remove_function_case02_01.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 1개인 경우 - 01"></p><p><img src="/image/BST_remove_function_case02_02.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 1개인 경우 - 02"></p><p><img src="/image/BST_remove_function_case02_03.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 1개인 경우 - 03"></p><p><img src="/image/BST_remove_function_case03.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 2개인 경우"></p><p><img src="/image/BST_remove_function_case03_01.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 2개인 경우 - 01"></p><p><img src="/image/BST_remove_function_case03_02.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 2개인 경우 - 02"></p><p><img src="/image/BST_remove_function_case03_03.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 2개인 경우 - 03"></p><p><img src="/image/BST_remove_function_case03_04.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 2개인 경우 - 04"></p><p><img src="/image/BST_remove_function_case03_05.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 2개인 경우 - 05"></p><p><img src="/image/BST_remove_function_case03_06.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 2개인 경우 - 06"></p><p><img src="/image/BST_remove_function_case03_07.png" alt="BST - remove 함수 제거하는 노드의 자식노드가 2개인 경우 - 07"></p><p><img src="/image/why_update_root_nodein_remove_function_01.png" alt="BST - remove 함수에서 root node를 업데이트 하는 이유 - 01"></p><p><img src="/image/why_update_root_nodein_remove_function_02.png" alt="BST - remove 함수에서 root node를 업데이트 하는 이유 - 02"></p><p><img src="/image/why_update_root_nodein_remove_function_03.png" alt="BST - remove 함수에서 root node를 업데이트 하는 이유 - 03"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def __remove_recursion(self, cur, target):</span><br><span class="line">    <span class="keyword">if</span> not cur:</span><br><span class="line">        <span class="built_in">return</span> None, None <span class="comment">#recursion은 점화식이랑 기저조건이 필요한데, 기저조건은 탈출할때 필요하므로 기저조건1</span></span><br><span class="line">    <span class="keyword">elif</span> target &lt; cur.data:</span><br><span class="line">        cur.left, rem_node=self.__remove_recursion(cur.left, target)</span><br><span class="line">    <span class="keyword">elif</span> target &gt; cur.data:</span><br><span class="line">        cur.right, rem_node=self.__remove_recursion(cur.right, target)</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment">#기저조건(base case) 2</span></span><br><span class="line">        <span class="keyword">if</span> not cur.left and not cur.right:</span><br><span class="line">            rem_node=cur</span><br><span class="line">            cur=None</span><br><span class="line">        <span class="keyword">elif</span> not cur.right:</span><br><span class="line">            rem_node=cur</span><br><span class="line">            cur=cur.left</span><br><span class="line">        <span class="keyword">elif</span> not cur.left:</span><br><span class="line">            rem_node=cur</span><br><span class="line">            cur=cur.right</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            replace=cur.left</span><br><span class="line">            <span class="keyword">while</span> replace.right:</span><br><span class="line">                replace=replace.right</span><br><span class="line">            cur.data, replace.data=replace.data, cur.data</span><br><span class="line">            cur.left, rem_node=self.__remove_recursion(cur.left, replace.data)</span><br><span class="line">    <span class="built_in">return</span> cur, rem_node</span><br><span class="line"></span><br><span class="line">def remove(self, target):</span><br><span class="line">    self.root, removed_node=self.__remove_recursion(self.root, target)</span><br><span class="line">    <span class="comment"># self.root를 출력해주는 이유 루트를 삭제시 기존의 root에 계속 있어 꼬이므로!</span></span><br><span class="line">    <span class="keyword">if</span> removed_node:</span><br><span class="line">        removed_node.left=removed_node.right=None</span><br><span class="line">    <span class="built_in">return</span> removed_node</span><br></pre></td></tr></table></figure><hr><h4 id="테스트"><a href="#테스트" class="headerlink" title="테스트"></a>테스트</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bst=BST()</span><br></pre></td></tr></table></figure><hr><h5 id="insert"><a href="#insert" class="headerlink" title="insert"></a>insert</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bst.insert(6)</span><br><span class="line">bst.insert(3)</span><br><span class="line">bst.insert(2)</span><br><span class="line">bst.insert(4)</span><br><span class="line">bst.insert(5)</span><br><span class="line">bst.insert(8)</span><br><span class="line">bst.insert(10)</span><br><span class="line">bst.insert(9)</span><br><span class="line">bst.insert(11)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f=lambda x: <span class="built_in">print</span>(x.data, end=<span class="string">'  '</span>)</span><br><span class="line">bst.preorder_traverse(bst.get_root(), f)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">6  3  2  4  5  8  10  9  11</span><br></pre></td></tr></table></figure><hr><h5 id="search"><a href="#search" class="headerlink" title="search"></a>search</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">res=bst.search(8)</span><br><span class="line"><span class="keyword">if</span> res:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'searched data : &#123;&#125;'</span>.format(res.data))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'search failed'</span>)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">searched data : 8</span><br></pre></td></tr></table></figure><hr><h5 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#지울 노드가 리프 노드일 때</span></span><br><span class="line"><span class="comment">#bst.remove(9)</span></span><br><span class="line"><span class="comment">#자식 노드가 하나일 때</span></span><br><span class="line"><span class="comment">#bst.remove(8)</span></span><br><span class="line"><span class="comment">#자식 노드가 둘일 때</span></span><br><span class="line">bst.remove(6)</span><br><span class="line">bst.preorder_traverse(bst.get_root(), f)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data 6 is deleted</span><br><span class="line">5  3  2  4  8  10  9  11</span><br></pre></td></tr></table></figure><hr><h3 id="6-이진-탐색-트리의-시간-복잡도와-단점"><a href="#6-이진-탐색-트리의-시간-복잡도와-단점" class="headerlink" title="6. 이진 탐색 트리의 시간 복잡도와 단점"></a>6. 이진 탐색 트리의 시간 복잡도와 단점</h3><h4 id="6-1-시간-복잡도-탐색시"><a href="#6-1-시간-복잡도-탐색시" class="headerlink" title="6.1. 시간 복잡도 (탐색시)"></a>6.1. 시간 복잡도 (탐색시)</h4><ul><li>depth (트리의 높이) 를 h라고 표기한다면, O(h)</li><li>n개의 노드를 가진다면, $h = log_2{n} $ 에 가까우므로, 시간 복잡도는 $ O(log{n}) $<ul><li>참고: 빅오 표기법에서 $log{n}$ 에서의 log의 밑은 10이 아니라, 2입니다.<ul><li>한번 실행시마다, 50%의 실행할 수도 있는 명령을 제거한다는 의미. 즉 50%의 실행시간을 단축시킬 수 있다는 것을 의미함<br><img src="https://www.mathwarehouse.com/programming/images/binary-search-tree/binary-search-tree-sorted-array-animation.gif"></li></ul></li></ul></li></ul><p>(출처: <a href="https://www.mathwarehouse.com/programming/gifs/binary-search-tree.php#binary-search-tree-insertion-node" target="_blank" rel="noopener">https://www.mathwarehouse.com/programming/gifs/binary-search-tree.php#binary-search-tree-insertion-node</a>)</p><h4 id="6-2-이진-탐색-트리-단점"><a href="#6-2-이진-탐색-트리-단점" class="headerlink" title="6.2. 이진 탐색 트리 단점"></a>6.2. 이진 탐색 트리 단점</h4><ul><li>평균 시간 복잡도는 $ O(log{n}) $ 이지만,<ul><li>이는 트리가 균형잡혀 있을 때의 평균 시간복잡도이며,</li></ul></li><li>다음 예와 같이 구성되어 있을 경우, <code>최악의 경우는 링크드 리스트등과 동일한 성능을 보여줌</code> ( $O(n)$ )<br><img src="http://www.fun-coding.org/00_Images/worstcase_bst.png" width="300"></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/05/02/data_structure_06/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 04 - 해쉬 테이블</title>
      <link>https://heung-bae-lee.github.io/2020/04/30/data_structure_05/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/30/data_structure_05/</guid>
      <pubDate>Thu, 30 Apr 2020 14:32:54 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;대표적인-데이터-구조6-해쉬-테이블-Hash-Table&quot;&gt;&lt;a href=&quot;#대표적인-데이터-구조6-해쉬-테이블-Hash-Table&quot; class=&quot;headerlink&quot; title=&quot;대표적인 데이터 구조6: 해쉬 테이블 (Hash Table
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="대표적인-데이터-구조6-해쉬-테이블-Hash-Table"><a href="#대표적인-데이터-구조6-해쉬-테이블-Hash-Table" class="headerlink" title="대표적인 데이터 구조6: 해쉬 테이블 (Hash Table)"></a>대표적인 데이터 구조6: 해쉬 테이블 (Hash Table)</h1><h2 id="1-해쉬-구조"><a href="#1-해쉬-구조" class="headerlink" title="1. 해쉬 구조"></a>1. 해쉬 구조</h2><ul><li>Hash Table: <code>키(Key)에 데이터(Value)를 저장하는 데이터 구조</code><ul><li>Key를 통해 바로 데이터를 받아올 수 있으므로, 속도가 획기적으로 빨라짐</li><li><code>파이썬 딕셔너리(Dictionary) 타입이 해쉬 테이블의 예</code>: Key를 가지고 바로 데이터(Value)를 꺼냄</li><li>보통 배열로 미리 Hash Table 사이즈만큼 생성 후에 사용 (공간과 탐색 시간을 맞바꾸는 기법)<ul><li>배열을 Hash Table을 만드는데 사용하지만, 여러 키에 해당하는 주소가 동일할 경우 충돌을 해결하기 위해 해쉬테이블의 공간을 늘림으로인해서 배열보단 많은 저장공간이 필요할 수 있기 때문</li></ul></li><li><font color="#BF360C">단, 파이썬에서는 해쉬를 별도 구현할 이유가 없음 - 딕셔너리 타입을 사용하면 됨</font></li></ul></li></ul><h2 id="2-알아둘-용어"><a href="#2-알아둘-용어" class="headerlink" title="2. 알아둘 용어"></a>2. 알아둘 용어</h2><ul><li>해쉬(Hash): 임의 값(데이터)을 고정 길이로 변환하는 것</li><li>해쉬 테이블(Hash Table): 키 값의 연산에 의해 직접 접근이 가능한 데이터 구조</li><li>해싱 함수(Hashing Function): Key에 대해 산술 연산을 이용해 데이터 위치를 찾을 수 있는 함수</li><li>해쉬 값(Hash Value) 또는 해쉬 주소(Hash Address): Key를 해싱 함수로 연산해서, 해쉬 값을 알아내고, 이를 기반으로 해쉬 테이블에서 해당 Key에 대한 데이터 위치를 일관성있게 찾을 수 있음</li><li>슬롯(Slot): 한 개의 데이터를 저장할 수 있는 공간</li><li>저장할 데이터에 대해 Key를 추출할 수 있는 별도 함수도 존재할 수 있음<br><img src="https://www.fun-coding.org/00_Images/hash.png" width="400"></li></ul><h3 id="3-간단한-해쉬-예"><a href="#3-간단한-해쉬-예" class="headerlink" title="3. 간단한 해쉬 예"></a>3. 간단한 해쉬 예</h3><h4 id="3-1-hash-table-만들기"><a href="#3-1-hash-table-만들기" class="headerlink" title="3.1. hash table 만들기"></a>3.1. hash table 만들기</h4><ul><li>참고: 파이썬 list comprehension - <a href="https://www.fun-coding.org/PL&amp;OOP5-2.html" target="_blank" rel="noopener">https://www.fun-coding.org/PL&amp;OOP5-2.html</a></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([i <span class="keyword">for</span> i <span class="keyword">in</span> range(10)])</span><br><span class="line">hash_table</span><br></pre></td></tr></table></figure><hr><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</span><br></pre></td></tr></table></figure><hr><h4 id="3-2-이번엔-초간단-해쉬-함수를-만들어보자"><a href="#3-2-이번엔-초간단-해쉬-함수를-만들어보자" class="headerlink" title="3.2. 이번엔 초간단 해쉬 함수를 만들어보자."></a>3.2. 이번엔 초간단 해쉬 함수를 만들어보자.</h4><ul><li>다양한 해쉬 함수 고안 기법이 있으며, 가장 간단한 방식이 Division 법 (나누기를 통한 나머지 값을 사용하는 기법)이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def hash_func(key):</span><br><span class="line">    <span class="built_in">return</span> key % 5</span><br></pre></td></tr></table></figure><hr><h4 id="3-3-해쉬-테이블에-저장해보겠다"><a href="#3-3-해쉬-테이블에-저장해보겠다" class="headerlink" title="3.3. 해쉬 테이블에 저장해보겠다."></a>3.3. 해쉬 테이블에 저장해보겠다.</h4><h2 id="데이터에-따라-필요시-key-생성-방법-정의가-필요함"><a href="#데이터에-따라-필요시-key-생성-방법-정의가-필요함" class="headerlink" title="- 데이터에 따라 필요시 key 생성 방법 정의가 필요함"></a>- 데이터에 따라 필요시 key 생성 방법 정의가 필요함</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">data1 = <span class="string">'Andy'</span></span><br><span class="line">data2 = <span class="string">'Dave'</span></span><br><span class="line">data3 = <span class="string">'Trump'</span></span><br><span class="line">data4 = <span class="string">'Anthor'</span></span><br><span class="line"><span class="comment">## ord(): 문자의 ASCII(아스키)코드 리턴</span></span><br><span class="line"><span class="built_in">print</span> (ord(data1[0]), ord(data2[0]), ord(data3[0]))</span><br><span class="line"><span class="built_in">print</span> (ord(data1[0]), hash_func(ord(data1[0])))</span><br><span class="line"><span class="built_in">print</span> (ord(data1[0]), ord(data4[0]))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">65 68 84</span><br><span class="line">65 0</span><br><span class="line">65 65</span><br></pre></td></tr></table></figure><hr><ul><li>3.3.2. 해쉬 테이블에 값 저장 예<ul><li>data:value 와 같이 data 와 value를 넣으면, 해당 data에 대한 key를 찾아서, 해당 key에 대응하는 해쉬주소에 value를 저장하는 예</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def storage_data(data, value):</span><br><span class="line">    key = ord(data[0])</span><br><span class="line">    hash_address = hash_func(key)</span><br><span class="line">    hash_table[hash_address] = value</span><br></pre></td></tr></table></figure><hr><h4 id="3-4-해쉬-테이블에서-특정-주소의-데이터를-가져오는-함수도-만들어보자"><a href="#3-4-해쉬-테이블에서-특정-주소의-데이터를-가져오는-함수도-만들어보자" class="headerlink" title="3.4. 해쉬 테이블에서 특정 주소의 데이터를 가져오는 함수도 만들어보자."></a>3.4. 해쉬 테이블에서 특정 주소의 데이터를 가져오는 함수도 만들어보자.</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">storage_data(<span class="string">'Andy'</span>, <span class="string">'01055553333'</span>)</span><br><span class="line">storage_data(<span class="string">'Dave'</span>, <span class="string">'01044443333'</span>)</span><br><span class="line">storage_data(<span class="string">'Trump'</span>, <span class="string">'01022223333'</span>)</span><br><span class="line"><span class="comment"># storage_data('Anthor', '01046723456')</span></span><br></pre></td></tr></table></figure><hr><h4 id="3-5-실제-데이터를-저장하고-읽어보겠다"><a href="#3-5-실제-데이터를-저장하고-읽어보겠다" class="headerlink" title="3.5. 실제 데이터를 저장하고, 읽어보겠다."></a>3.5. 실제 데이터를 저장하고, 읽어보겠다.</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def get_data(data):</span><br><span class="line">    key = ord(data[0])</span><br><span class="line">    hash_address = hash_func(key)</span><br><span class="line">    <span class="built_in">return</span> hash_table[hash_address]</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_data(<span class="string">'Andy'</span>)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'01046723456'</span></span><br></pre></td></tr></table></figure><hr><h3 id="4-자료-구조-해쉬-테이블의-장단점과-주요-용도"><a href="#4-자료-구조-해쉬-테이블의-장단점과-주요-용도" class="headerlink" title="4. 자료 구조 해쉬 테이블의 장단점과 주요 용도"></a>4. 자료 구조 해쉬 테이블의 장단점과 주요 용도</h3><ul><li>장점<ul><li><code>데이터 저장/읽기 속도가 빠르다. (검색 속도가 빠르다.)</code></li><li>해쉬는 키에 대한 데이터가 있는지(중복) 확인이 쉬움</li></ul></li><li>단점<ul><li><code>일반적으로 저장공간이 좀더 많이 필요</code>하다.</li><li><strong><code>여러 키에 해당하는 주소가 동일할 경우 충돌을 해결하기 위한 별도 자료구조가 필요함</code></strong></li></ul></li><li>주요 용도<ul><li>검색이 많이 필요한 경우</li><li>저장, 삭제, 읽기가 빈번한 경우</li><li>캐쉬 구현시 (중복 확인이 쉽기 때문)</li></ul></li></ul><h3 id="5-프로그래밍-연습"><a href="#5-프로그래밍-연습" class="headerlink" title="5. 프로그래밍 연습"></a>5. 프로그래밍 연습</h3><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">연습1: 리스트 변수를 활용해서 해쉬 테이블 구현해보기</font></strong><br>1. 해쉬 함수: key % 8<br>2. 해쉬 키 생성: hash(data)</div><ul><li>밑에서 사용되는 hash함수는 python을 새로 시작할 때마다 출력되는 값이 변화되므로 주의하자.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([0 <span class="keyword">for</span> i <span class="keyword">in</span> range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">hash</span>(data)</span><br><span class="line"></span><br><span class="line">def hash_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    hash_address = hash_function(get_key(data))</span><br><span class="line">    hash_table[hash_address] = value</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    hash_address = hash_function(get_key(data))</span><br><span class="line">    <span class="built_in">return</span> hash_table[hash_address]</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_data(<span class="string">'Dave'</span>, <span class="string">'0102030200'</span>)</span><br><span class="line">save_data(<span class="string">'Andy'</span>, <span class="string">'01033232200'</span>)</span><br><span class="line">read_data(<span class="string">'Dave'</span>)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'0102030200'</span></span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hash_table</span><br></pre></td></tr></table></figure><hr><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0, 0, <span class="string">'01033232200'</span>, 0, 0, 0, <span class="string">'0102030200'</span>, 0]</span><br></pre></td></tr></table></figure><hr><h3 id="6-충돌-Collision-해결-알고리즘-좋은-해쉬-함수-사용하기"><a href="#6-충돌-Collision-해결-알고리즘-좋은-해쉬-함수-사용하기" class="headerlink" title="6. 충돌(Collision) 해결 알고리즘 (좋은 해쉬 함수 사용하기)"></a>6. 충돌(Collision) 해결 알고리즘 (좋은 해쉬 함수 사용하기)</h3><blockquote><p><code>해쉬 테이블의 가장 큰 문제</code>는 <code>충돌(Collision)의 경우</code>이다.<br>이 문제를 충돌(Collision) 또는 해쉬 충돌(Hash Collision)이라고 부릅니다.</p></blockquote><h4 id="6-1-Chaining-기법"><a href="#6-1-Chaining-기법" class="headerlink" title="6.1. Chaining 기법"></a>6.1. Chaining 기법</h4><ul><li><strong>개방 해슁 또는 Open Hashing 기법</strong> 중 하나: <code>해쉬 테이블 저장공간 외의 공간을 활용하는 기법</code></li><li>충돌이 일어나면, 링크드 리스트라는 자료 구조를 사용해서, 링크드 리스트로 데이터를 추가로 뒤에 연결시켜서 저장하는 기법</li></ul><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">연습2: 연습1의 해쉬 테이블 코드에 Chaining 기법으로 충돌해결 코드를 추가해보기</font></strong><br>1. 해쉬 함수: key % 8<br>2. 해쉬 키 생성: hash(data)</div><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([0 <span class="keyword">for</span> i range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">hash</span>(data)</span><br><span class="line"></span><br><span class="line">def hashing_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    key=get(data)</span><br><span class="line">    hash_address=hashing_function(key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        hash_table[hash_address]=value</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        prev_value = hash_table[hash_address]</span><br><span class="line">        hash_table[hash_address] = [prev_value, value]</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    key=get(data)</span><br><span class="line">    hash_address=hashing_function(key)</span><br><span class="line">    <span class="built_in">return</span> hash_table[hash_address]</span><br></pre></td></tr></table></figure><hr><ul><li>내가 만든 함수는 동일한 키값에 여러 데이터를 가지고 있으므로 Chaining 기법으로 충동해결을 방지한 코드가 아니다. 리스트를 사용하는 것은 좋았으나, 아래와 같이 <code>해당 key값을 갖고있어야 동일한 키값이 아닌 각각 고유의 키값을 가져 데이터를 읽어들일 경우에도 정확하게 특정 키값에 해당하는 데이터만을 불러올 수 있기 때문</code>이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([0 <span class="keyword">for</span> i <span class="keyword">in</span> range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">hash</span>(data)</span><br><span class="line"></span><br><span class="line">def hash_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="comment"># 해당 hash_address값에 이미 데이터가 존재하는 경우 키값과 동일한 것이라면</span></span><br><span class="line">        <span class="comment"># 값을 저장하지만 그렇지 않다면 리스트로 [index, value]형식으로 저장</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(hash_table[hash_address])):</span><br><span class="line">            <span class="keyword">if</span> hash_table[hash_address][index][0] == index_key:</span><br><span class="line">                hash_table[hash_address][index][1] = value</span><br><span class="line">                <span class="built_in">return</span></span><br><span class="line">        hash_table[hash_address].append([index_key, value])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        hash_table[hash_address] = [[index_key, value]]</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(len(hash_table[hash_address])):</span><br><span class="line">            <span class="keyword">if</span> hash_table[hash_address][index][0] == index_key:</span><br><span class="line">                <span class="built_in">return</span> hash_table[hash_address][index][1]</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">return</span> None</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'Dave'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'Da'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'Data'</span>) % 8)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">6</span><br><span class="line">6</span><br><span class="line">3</span><br></pre></td></tr></table></figure><hr><ul><li>동일한 hash함수 값을 갖지만 동일 주소에서 저장을하지만 키값을 다르게 하여 저장한다. 또한, 현재는 Chaning 기법을 사용했으므 링크드리스트로 동일한 주소안에 저장되어 연결되어있음을 확인하자.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">save_data(<span class="string">'Da'</span>, <span class="string">'1201023010'</span>)</span><br><span class="line">save_data(<span class="string">'Dave'</span>, <span class="string">'3301023010'</span>)</span><br><span class="line"><span class="built_in">print</span>(read_data(<span class="string">'Da'</span>))</span><br><span class="line"><span class="built_in">print</span>(read_data(<span class="string">'Dave'</span>))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1201023010</span><br><span class="line">3301023010</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hash_table</span><br></pre></td></tr></table></figure><hr><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[0,</span><br><span class="line"> 0,</span><br><span class="line"> 0,</span><br><span class="line"> 0,</span><br><span class="line"> 0,</span><br><span class="line"> 0,</span><br><span class="line"> [[-8338113502661437674, <span class="string">'1201023010'</span>], [909867193312922558, <span class="string">'3301023010'</span>]],</span><br><span class="line"> 0]</span><br></pre></td></tr></table></figure><hr><h4 id="6-2-Linear-Probing-기법"><a href="#6-2-Linear-Probing-기법" class="headerlink" title="6.2. Linear Probing 기법"></a>6.2. Linear Probing 기법</h4><ul><li><strong>폐쇄 해슁 또는 Close Hashing 기법</strong> 중 하나: <code>해쉬 테이블 저장공간 안에서 충돌 문제를 해결하는 기법</code></li><li>충돌이 일어나면, 해당 hash address의 다음 address부터 맨 처음 나오는 빈공간에 저장하는 기법<ul><li><code>저장공간 활용도를 높이기 위한 기법</code></li></ul></li></ul><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">연습3: 연습1의 해쉬 테이블 코드에 Linear Probling 기법으로 충돌해결 코드를 추가해보기</font></strong><br>1. 해쉬 함수: key % 8<br>2. 해쉬 키 생성: hash(data)</div><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([0 <span class="keyword">for</span> i <span class="keyword">in</span> range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">hash</span>(data)</span><br><span class="line"></span><br><span class="line">def hash_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    index_key=get_key(data)</span><br><span class="line">    hash_address=hash_function(index_key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] !=0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                hash_table[index] = [index_key, value]</span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                hash_table[index][1] = value</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        hash_table[hash_address] = [index_key, value]</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    index_key=get_key(data)</span><br><span class="line">    hash_address=hash_function(index_key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] == 0:</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                <span class="built_in">return</span> None</span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                <span class="built_in">return</span> hash_table[index][1]</span><br></pre></td></tr></table></figure><hr><ul><li><code>read_data</code>함수에서 linear probling 기법은 해당 hash_address에 값이 저장되어있다면(동일한 키값은 업데이트하지만) 다음 주소에 값을 저장하므로 만약 다음 주소 중 0값이 있다면 해당 데이터를 저장한적이 없다는 것을 의미하므로 그에 해당하는 코드도 작성해주어야 함을 유의하자!</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([0 <span class="keyword">for</span> i <span class="keyword">in</span> range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">    <span class="built_in">return</span> <span class="built_in">hash</span>(data)</span><br><span class="line"></span><br><span class="line">def hash_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                hash_table[index] = [index_key, value]</span><br><span class="line">                <span class="built_in">return</span></span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                hash_table[index][1] = value</span><br><span class="line">                <span class="built_in">return</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        hash_table[hash_address] = [index_key, value]</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                <span class="built_in">return</span> None</span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                <span class="built_in">return</span> hash_table[index][1]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">return</span> None</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'dk'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'dw'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (<span class="built_in">hash</span>(<span class="string">'dc'</span>) % 8)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">2</span><br><span class="line">2</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">save_data(<span class="string">'dk'</span>, <span class="string">'01200123123'</span>)</span><br><span class="line">save_data(<span class="string">'dw'</span>, <span class="string">'3333333333'</span>)</span><br><span class="line">save_data(<span class="string">'dc'</span>, <span class="string">'23456781234'</span>)</span><br><span class="line">read_data(<span class="string">'dc'</span>)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'23456781234'</span></span><br></pre></td></tr></table></figure><hr><h4 id="6-3-빈번한-충돌을-개선하는-기법"><a href="#6-3-빈번한-충돌을-개선하는-기법" class="headerlink" title="6.3. 빈번한 충돌을 개선하는 기법"></a>6.3. 빈번한 충돌을 개선하는 기법</h4><ul><li>해쉬 함수을 재정의 및 해쉬 테이블 저장공간을 확대<ul><li>예를 들어 저장하고자하는 데이터가 8개라면 이에 2배에 해당하는 공간을 해쉬 테이블 구조로 저장공간을 만들어 두는 것이 일반적이다.</li></ul></li><li>예:</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hash_table = list([<span class="literal">None</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">16</span>)])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hash_function</span><span class="params">(key)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> key % <span class="number">16</span></span><br></pre></td></tr></table></figure><h3 id="참고-해쉬-함수와-키-생성-함수"><a href="#참고-해쉬-함수와-키-생성-함수" class="headerlink" title="참고: 해쉬 함수와 키 생성 함수"></a>참고: 해쉬 함수와 키 생성 함수</h3><ul><li><code>파이썬의 hash() 함수는 실행할 때마다, 값이 달라질 수 있음</code></li><li>유명한 해쉬 함수들이 있음: SHA(Secure Hash Algorithm, 안전한 해시 알고리즘)<ul><li>어떤 데이터도 유일한 고정된 크기의 고정값을 리턴해주므로, 해쉬 함수로 유용하게 활용 가능</li></ul></li></ul><h4 id="SHA-1"><a href="#SHA-1" class="headerlink" title="SHA-1"></a>SHA-1</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import hashlib</span><br><span class="line"></span><br><span class="line">data = <span class="string">'test'</span>.encode()</span><br><span class="line">hash_object = hashlib.sha1()</span><br><span class="line">hash_object.update(data)</span><br><span class="line">hex_dig = hash_object.hexdigest()</span><br><span class="line"><span class="built_in">print</span> (hex_dig)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a94a8fe5ccb19ba61c4c0873d391e987982fbbd3</span><br></pre></td></tr></table></figure><hr><h4 id="SHA-256"><a href="#SHA-256" class="headerlink" title="SHA-256"></a>SHA-256</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import hashlib</span><br><span class="line"></span><br><span class="line">data = <span class="string">'test'</span>.encode()</span><br><span class="line">hash_object = hashlib.sha256()</span><br><span class="line">hash_object.update(data)</span><br><span class="line">hex_dig = hash_object.hexdigest()</span><br><span class="line"><span class="built_in">print</span> (hex_dig)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9f86d081884c7d659a2feaa0c55ad015a3bf4f1b2b0b822cd15d6c15b0f00a08</span><br></pre></td></tr></table></figure><hr><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">연습4: 연습2의 Chaining 기법을 적용한 해쉬 테이블 코드에 키 생성 함수를 sha256 해쉬 알고리즘을 사용하도록 변경해보기</font></strong><br>1. 해쉬 함수: key % 8<br>2. 해쉬 키 생성: hash(data)</div><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">import hashlib</span><br><span class="line"></span><br><span class="line">hash_table = list([0 <span class="keyword">for</span> i <span class="keyword">in</span> range(8)])</span><br><span class="line"></span><br><span class="line">def get_key(data):</span><br><span class="line">        hash_object = hashlib.sha256()</span><br><span class="line">        hash_object.update(data.encode())</span><br><span class="line">        hex_dig = hash_object.hexdigest()</span><br><span class="line">        <span class="built_in">return</span> int(hex_dig, 16)</span><br><span class="line"></span><br><span class="line">def hash_function(key):</span><br><span class="line">    <span class="built_in">return</span> key % 8</span><br><span class="line"></span><br><span class="line">def save_data(data, value):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                hash_table[index] = [index_key, value]</span><br><span class="line">                <span class="built_in">return</span></span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                hash_table[index][1] = value</span><br><span class="line">                <span class="built_in">return</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        hash_table[hash_address] = [index_key, value]</span><br><span class="line"></span><br><span class="line">def read_data(data):</span><br><span class="line">    index_key = get_key(data)</span><br><span class="line">    hash_address = hash_function(index_key)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> hash_table[hash_address] != 0:</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hash_address, len(hash_table)):</span><br><span class="line">            <span class="keyword">if</span> hash_table[index] == 0:</span><br><span class="line">                <span class="built_in">return</span> None</span><br><span class="line">            <span class="keyword">elif</span> hash_table[index][0] == index_key:</span><br><span class="line">                <span class="built_in">return</span> hash_table[index][1]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">return</span> None</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (get_key(<span class="string">'dw'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (get_key(<span class="string">'deo'</span>) % 8)</span><br><span class="line"><span class="built_in">print</span> (get_key(<span class="string">'dh'</span>) % 8)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2</span><br><span class="line">0</span><br><span class="line">0</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_data(<span class="string">'deo'</span>, <span class="string">'01200123123'</span>)</span><br><span class="line">save_data(<span class="string">'dh'</span>, <span class="string">'3333333333'</span>)</span><br><span class="line">read_data(<span class="string">'dh'</span>)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'3333333333'</span></span><br></pre></td></tr></table></figure><hr><h3 id="7-시간-복잡도"><a href="#7-시간-복잡도" class="headerlink" title="7. 시간 복잡도"></a>7. 시간 복잡도</h3><ul><li><code>일반적인 경우(Collision이 없는 경우)는 O(1)</code></li><li>최악의 경우(Collision이 모두 발생하는 경우)는 O(n)</li></ul><blockquote><p>해쉬 테이블의 경우, 일반적인 경우를 기대하고 만들기 때문에, 시간 복잡도는 O(1) 이라고 말할 수 있음</p></blockquote><h3 id="검색에서-해쉬-테이블의-사용-예"><a href="#검색에서-해쉬-테이블의-사용-예" class="headerlink" title="검색에서 해쉬 테이블의 사용 예"></a>검색에서 해쉬 테이블의 사용 예</h3><ul><li>16개의 배열에 데이터를 저장하고, 검색할 때 O(n)</li><li>16개의 데이터 저장공간을 가진 위의 해쉬 테이블에 데이터를 저장하고, 검색할 때 O(1)</li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/30/data_structure_05/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 03 - 시간복잡도</title>
      <link>https://heung-bae-lee.github.io/2020/04/30/data_structure_04/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/30/data_structure_04/</guid>
      <pubDate>Thu, 30 Apr 2020 09:12:50 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;알고리즘-복잡도-표현-방법&quot;&gt;&lt;a href=&quot;#알고리즘-복잡도-표현-방법&quot; class=&quot;headerlink&quot; title=&quot;알고리즘 복잡도 표현 방법&quot;&gt;&lt;/a&gt;알고리즘 복잡도 표현 방법&lt;/h2&gt;&lt;h3 id=&quot;1-알고리즘-복잡도-계산이-필요
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="알고리즘-복잡도-표현-방법"><a href="#알고리즘-복잡도-표현-방법" class="headerlink" title="알고리즘 복잡도 표현 방법"></a>알고리즘 복잡도 표현 방법</h2><h3 id="1-알고리즘-복잡도-계산이-필요한-이유"><a href="#1-알고리즘-복잡도-계산이-필요한-이유" class="headerlink" title="1. 알고리즘 복잡도 계산이 필요한 이유"></a>1. 알고리즘 복잡도 계산이 필요한 이유</h3><h4 id="하나의-문제를-푸는-알고리즘은-다양할-수-있음"><a href="#하나의-문제를-푸는-알고리즘은-다양할-수-있음" class="headerlink" title="하나의 문제를 푸는 알고리즘은 다양할 수 있음"></a>하나의 문제를 푸는 알고리즘은 다양할 수 있음</h4><ul><li>정수의 절대값 구하기<ul><li>1, -1 -&gt;&gt; 1</li><li>방법1: 정수값을 제곱한 값에 다시 루트를 씌우기</li><li>방법2: 정수가 음수인지 확인해서, 음수일 때만, -1을 곱하기</li></ul></li></ul><blockquote><p>다양한 알고리즘 중 어느 알고리즘이 더 좋은지를 분석하기 위해, 복잡도를 정의하고 계산함</p></blockquote><h3 id="2-알고리즘-복잡도-계산-항목"><a href="#2-알고리즘-복잡도-계산-항목" class="headerlink" title="2. 알고리즘 복잡도 계산 항목"></a>2. 알고리즘 복잡도 계산 항목</h3><ol><li><strong>시간 복잡도</strong>: 알고리즘 실행 속도</li><li><strong>공간 복잡도</strong>: 알고리즘이 사용하는 메모리 사이즈</li></ol><blockquote><p>가장 중요한 시간 복잡도를 꼭 이해하고 계산할 수 있어야 함</p></blockquote><h3 id="알고리즘-시간-복잡도의-주요-요소"><a href="#알고리즘-시간-복잡도의-주요-요소" class="headerlink" title="알고리즘 시간 복잡도의 주요 요소"></a>알고리즘 시간 복잡도의 주요 요소</h3><blockquote><p><code>반복문이 지배</code>한다.</p></blockquote><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">생각해보기: 자동차로 서울에서 부산을 가기 위해, 다음과 같이 항목을 나누었을 때, 가장 총 시간에 영향을 많이 미칠 것 같은 요소는?</font></strong><br> 5번!!!* 예:  - 자동차로 서울에서 부산가기    1. 자동차 문열기    2. 자동차 문닫기    3. 자동차 운전석 등받이 조정하기    4. 자동차 시동걸기    5. `자동차로 서울에서 부산가기`    6. 자동차 시동끄기    7. 자동차 문열기    8. 자동차 문닫기</div><h3 id="마찬가지로-프로그래밍에서-시간-복잡도에-가장-영향을-많이-미치는-요소는-반복문"><a href="#마찬가지로-프로그래밍에서-시간-복잡도에-가장-영향을-많이-미치는-요소는-반복문" class="headerlink" title="마찬가지로, 프로그래밍에서 시간 복잡도에 가장 영향을 많이 미치는 요소는 반복문"></a>마찬가지로, 프로그래밍에서 시간 복잡도에 가장 영향을 많이 미치는 요소는 반복문</h3><ul><li>입력의 크기가 커지면 커질수록 반복문이 알고리즘 수행 시간을 지배함</li></ul><h3 id="알고리즘-성능-표기법"><a href="#알고리즘-성능-표기법" class="headerlink" title="알고리즘 성능 표기법"></a>알고리즘 성능 표기법</h3><ul><li><p><code>Big O (빅-오) 표기법</code>: O(N)</p><ul><li>알고리즘 <code>최악의 실행 시간을 표기</code></li><li><strong>가장 많이/일반적으로 사용함</strong></li><li><strong>아무리 최악의 상황이라도, <code>이정도의 성능은 보장한다는 의미이기 때문</code></strong></li></ul></li><li><p>Ω (오메가) 표기법:  Ω(N)</p><ul><li>오메가 표기법은 알고리즘 최상의 실행 시간을 표기</li></ul></li><li><p>Θ (세타) 표기법: Θ(N)</p><ul><li>오메가 표기법은 알고리즘 평균 실행 시간을 표기</li></ul></li></ul><blockquote><p>시간 복잡도 계산은 반복문이 핵심 요소임을 인지하고, 계산 표기는 최상, 평균, 최악 중, 최악의 시간인 Big-O 표기법을 중심으로 익히면 됨</p></blockquote><h3 id="3-대문자-O-표기법"><a href="#3-대문자-O-표기법" class="headerlink" title="3. 대문자 O 표기법"></a>3. 대문자 O 표기법</h3><ul><li>빅 오 표기법, Big-O 표기법 이라고도 부름</li><li><p>O(입력)</p><ul><li>입력 n 에 따라 결정되는 시간 복잡도 함수</li><li>O(1), O($log n$), O(n), O(n$log n$), O($n^2$), O($2^n$), O(n!)등으로 표기함</li><li>입력 n 의 크기에 따라 기하급수적으로 시간 복잡도가 늘어날 수 있음<ul><li><strong>O(1) &lt; O($log n$) &lt; O(n) &lt; O(n$log n$) &lt; O($n^2$) &lt; O($2^n$) &lt; O(n!)</strong><ul><li>참고: log n 의 베이스는 2 : $log_2 n$</li></ul></li></ul></li></ul></li><li><p>단순하게 입력 n에 따라, 몇번 실행이 되는지를 계산하면 됩니다.</p><ul><li><strong>표현식에 가장 큰 영향을 미치는 n 의 단위로 표기합니다.</strong></li><li><p>n이 1이든 100이든, 1000이든, 10000이든 실행을</p><ul><li><p>무조건 2회(상수회) 실행한다: O(1)</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> n &gt; <span class="number">10</span>:</span><br><span class="line">     print(n)</span><br></pre></td></tr></table></figure></li><li><p>n에 따라, n번, n + 10 번, 또는 3n + 10 번등 실행한다: O(n)</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">variable = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(n):</span><br><span class="line">         print(index)</span><br></pre></td></tr></table></figure></li><li><p>n에 따라, $n^2$번, $n^2$ + 1000 번, 100$n^2$ - 100, 또는 300$n^2$ + 1번등 실행한다: O($n^2$)</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">            variable = <span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">                <span class="keyword">for</span> num <span class="keyword">in</span> range(n):</span><br><span class="line">                    <span class="keyword">for</span> index <span class="keyword">in</span> range(n):</span><br><span class="line">                         print(index)</span><br><span class="line">       ```    </span><br><span class="line"></span><br><span class="line">&lt;img src=<span class="string">"http://www.fun-coding.org/00_Images/bigo.png"</span> width=<span class="number">400</span>/&gt;</span><br><span class="line"></span><br><span class="line">* 빅 오 입력값 표기 방법</span><br><span class="line">  - 예:</span><br><span class="line">    - 만약 시간 복잡도 함수가 <span class="number">2</span>$n^<span class="number">2</span>$ + <span class="number">3</span>n 이라면</span><br><span class="line">      - 가장 높은 차수는 <span class="number">2</span>$n^<span class="number">2</span>$</span><br><span class="line">      - 상수는 실제 큰 영향이 없음</span><br><span class="line">      - 결국 빅 오 표기법으로는 O($n^<span class="number">2</span>$) (서울부터 부산까지 가는 자동차의 예를 상기)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 4. 실제 알고리즘을 예로 각 알고리즘의 시간 복잡도와 빅 오 표기법 알아보기</span></span><br><span class="line"></span><br><span class="line">&lt;div class="alert alert-block alert-warning"&gt;</span><br><span class="line">&lt;strong&gt;&lt;font color="blue" size="3em"&gt;연습1:  1부터 n까지의 합을 구하는 알고리즘 작성해보기&lt;/font&gt;&lt;/strong&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">### 알고리즘1: 1부터 n까지의 합을 구하는 알고리즘1</span></span><br><span class="line">* 합을 기록할 변수를 만들고 <span class="number">0</span>을 저장</span><br><span class="line">* n을 <span class="number">1</span>부터 <span class="number">1</span>씩 증가하면서 반복</span><br><span class="line">* 반복문 안에서 합을 기록할 변수에 <span class="number">1</span>씩 증가된 값을 더함</span><br><span class="line">* 반복이 끝나면 합을 출력</span><br><span class="line"></span><br><span class="line">--------</span><br><span class="line">``` bash</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sum_all</span><span class="params">(n)</span>:</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> range(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        total += num</span><br><span class="line">    <span class="keyword">return</span> total</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">sum_all(100)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">4.62 µs ± 217 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)</span><br></pre></td></tr></table></figure><h3 id="시간-복잡도-구하기"><a href="#시간-복잡도-구하기" class="headerlink" title="시간 복잡도 구하기"></a>시간 복잡도 구하기</h3><ul><li>1부터 n까지의 합을 구하는 알고리즘1<ul><li>입력 n에 따라 덧셈을 n 번 해야 함 (반복문!)</li><li>시간 복잡도: n, 빅 오 표기법으로는 <strong>O(n)</strong></li></ul></li></ul><ul><li>위의 함수의 시간복잡도는 O(n)이다. 그렇다면, 이보다 더 빠르게 함수를 작성할 순 없을까 우리는 이미 어렸을때 n번째 까지 수의 합을 구하는 공식을 알고있다.</li></ul><h3 id="알고리즘2-1부터-n까지의-합을-구하는-알고리즘2"><a href="#알고리즘2-1부터-n까지의-합을-구하는-알고리즘2" class="headerlink" title="알고리즘2: 1부터 n까지의 합을 구하는 알고리즘2"></a>알고리즘2: 1부터 n까지의 합을 구하는 알고리즘2</h3><ul><li><font size="5em">$\frac { n (n + 1) }{ 2 }$</font></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">def sum_all(n):</span><br><span class="line">    <span class="built_in">return</span> int(n * (n + 1) / 2)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">sum_all(100)</span><br></pre></td></tr></table></figure><hr><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">248 ns ± 3.71 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)</span><br></pre></td></tr></table></figure><hr><h3 id="시간-복잡도-구하기-1"><a href="#시간-복잡도-구하기-1" class="headerlink" title="시간 복잡도 구하기"></a>시간 복잡도 구하기</h3><ul><li>1부터 n까지의 합을 구하는 알고리즘2<ul><li>입력 n이 어떻든 간에, 곱셈/덧셈/나눗셈 하면 됨 (반복문이 없음!)</li><li>시간 복잡도: 1, 빅 오 표기법으로는 <strong>O(1)</strong></li></ul></li></ul><h3 id="어느-알고리즘이-성능이-좋은가요"><a href="#어느-알고리즘이-성능이-좋은가요" class="headerlink" title="어느 알고리즘이 성능이 좋은가요?"></a>어느 알고리즘이 성능이 좋은가요?</h3><ul><li>알고리즘1 vs 알고리즘2</li><li>O(n) vs O(1)</li></ul><blockquote><p>이와 같이, 동일한 문제를 푸는 알고리즘은 다양할 수 있음<br>어느 알고리즘이 보다 좋은지를 객관적으로 비교하기 위해, 빅 오 표기법등의 시간복잡도 계산법을 사용함</p></blockquote><h4 id="이후-자료구조-알고리즘부터는-빅-오-표기법으로-성능을-계산해보면서-빅-오-표기법과-계산방법에-익숙해지기로-하자"><a href="#이후-자료구조-알고리즘부터는-빅-오-표기법으로-성능을-계산해보면서-빅-오-표기법과-계산방법에-익숙해지기로-하자" class="headerlink" title="이후 자료구조, 알고리즘부터는 빅 오 표기법으로 성능을 계산해보면서, 빅 오 표기법과 계산방법에 익숙해지기로 하자."></a>이후 자료구조, 알고리즘부터는 빅 오 표기법으로 성능을 계산해보면서, 빅 오 표기법과 계산방법에 익숙해지기로 하자.</h4>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/30/data_structure_04/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 02 Linked List</title>
      <link>https://heung-bae-lee.github.io/2020/04/28/data_structure_03/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/28/data_structure_03/</guid>
      <pubDate>Mon, 27 Apr 2020 19:41:46 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;대표적인-데이터-구조-링크드-리스트-Linked-List&quot;&gt;&lt;a href=&quot;#대표적인-데이터-구조-링크드-리스트-Linked-List&quot; class=&quot;headerlink&quot; title=&quot;대표적인 데이터 구조: 링크드 리스트 (Linked L
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="대표적인-데이터-구조-링크드-리스트-Linked-List"><a href="#대표적인-데이터-구조-링크드-리스트-Linked-List" class="headerlink" title="대표적인 데이터 구조: 링크드 리스트 (Linked List)"></a>대표적인 데이터 구조: 링크드 리스트 (Linked List)</h1><h2 id="1-링크드-리스트-Linked-List-구조"><a href="#1-링크드-리스트-Linked-List-구조" class="headerlink" title="1. 링크드 리스트 (Linked List) 구조"></a>1. 링크드 리스트 (Linked List) 구조</h2><ul><li>연결 리스트라고도 함</li><li>배열은 순차적으로 연결된 공간에 데이터를 나열하는 데이터 구조<ul><li>그렇기 때문에 미리 연결된 공간을 예약을 해놓아야 한다는 것이 단점!</li></ul></li><li>링크드 리스트는 위와 같은 배열의 단점을 보완하고자 떨어진 곳에 존재하는 데이터를 화살표로 연결해서 관리하는 데이터 구조</li><li><font color="#BF360C">본래 C언어에서는 주요한 데이터 구조이지만, 파이썬은 리스트 타입이 링크드 리스트의 기능을 모두 지원</font></li><li><p>링크드 리스트 기본 구조와 용어</p><ul><li>노드(Node): 데이터 저장 단위 (데이터값, 포인터) 로 구성</li><li>포인터(pointer): 각 노드 안에서, 다음이나 이전의 노드와의 연결 정보를 가지고 있는 공간</li></ul></li></ul><p><br></p><ul><li>일반적인 링크드 리스트 형태<br><img src="https://www.fun-coding.org/00_Images/linkedlist.png"><br>(출처: wikipedia, <a href="https://en.wikipedia.org/wiki/Linked_list" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Linked_list</a>)</li></ul><h2 id="2-간단한-링크드-리스트-예"><a href="#2-간단한-링크드-리스트-예" class="headerlink" title="2. 간단한 링크드 리스트 예"></a>2. 간단한 링크드 리스트 예</h2><h3 id="Node-구현"><a href="#Node-구현" class="headerlink" title="Node 구현"></a>Node 구현</h3><ul><li>보통 파이썬에서 링크드 리스트 구현시, 파이썬 클래스를 활용함<ul><li>파이썬 객체지향 문법 이해 필요</li><li>참고: <a href="https://www.fun-coding.org/PL&amp;OOP1-3.html" target="_blank" rel="noopener">https://www.fun-coding.org/PL&amp;OOP1-3.html</a></li></ul></li></ul><h5 id="간단한-노드-1"><a href="#간단한-노드-1" class="headerlink" title="간단한 노드 1"></a>간단한 노드 1</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.next = None</span><br></pre></td></tr></table></figure><hr><h5 id="간단한-노드-2"><a href="#간단한-노드-2" class="headerlink" title="간단한 노드 2"></a>간단한 노드 2</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data, next=None):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.next = next</span><br></pre></td></tr></table></figure><hr><h5 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line">        self.__next=None</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def next(self):</span><br><span class="line">        <span class="built_in">return</span> self.__next</span><br><span class="line"></span><br><span class="line">    @next.setter</span><br><span class="line">    def next(self, n):</span><br><span class="line">        self.__next=n</span><br></pre></td></tr></table></figure><hr><h3 id="Node와-Node-연결하기-포인터-활용"><a href="#Node와-Node-연결하기-포인터-활용" class="headerlink" title="Node와 Node 연결하기 (포인터 활용)"></a>Node와 Node 연결하기 (포인터 활용)</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">node1 = Node(1)</span><br><span class="line">node2 = Node(2)</span><br><span class="line"><span class="built_in">print</span>(node1.data)</span><br><span class="line">node1.data=2</span><br><span class="line"><span class="built_in">print</span>(node1.data)</span><br><span class="line"></span><br><span class="line">node1.next = node2</span><br><span class="line"><span class="built_in">print</span>(node1.next.data)</span><br></pre></td></tr></table></figure><hr><h6 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">2</span><br></pre></td></tr></table></figure><hr><h3 id="3-링크드-리스트의-장단점-전통적인-C언어에서의-배열과-링크드-리스트"><a href="#3-링크드-리스트의-장단점-전통적인-C언어에서의-배열과-링크드-리스트" class="headerlink" title="3. 링크드 리스트의 장단점 (전통적인 C언어에서의 배열과 링크드 리스트)"></a>3. 링크드 리스트의 장단점 (전통적인 C언어에서의 배열과 링크드 리스트)</h3><ul><li>장점<ul><li>미리 데이터 공간을 미리 할당하지 않아도 됨<ul><li>배열은 <strong>미리 데이터 공간을 할당</strong> 해야 함</li></ul></li></ul></li><li>단점<ul><li>연결을 위한 별도 데이터 공간이 필요하므로, <code>저장공간 효율이 높지 않음</code></li><li>연결 정보를 찾는 시간이 필요하므로 <code>접근 속도가 느림</code> <strong>그에 반해 배열은 인덱싱을 통해 빠르게 접근할 수 있음</strong></li><li><code>중간 데이터 삭제시, 앞뒤 데이터의 연결을 재구성해야 하는 부가적인 작업 필요</code></li></ul></li></ul><h3 id="4-링크드-리스트의-복잡한-기능1-링크드-리스트-데이터-사이에-데이터를-추가"><a href="#4-링크드-리스트의-복잡한-기능1-링크드-리스트-데이터-사이에-데이터를-추가" class="headerlink" title="4. 링크드 리스트의 복잡한 기능1 (링크드 리스트 데이터 사이에 데이터를 추가)"></a>4. 링크드 리스트의 복잡한 기능1 (링크드 리스트 데이터 사이에 데이터를 추가)</h3><ul><li>링크드 리스트는 유지 관리에 부가적인 구현이 필요함</li></ul><p><img src="https://www.fun-coding.org/00_Images/linkedlistadd.png"><br>(출처: wikipedia, <a href="https://en.wikipedia.org/wiki/Linked_list" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Linked_list</a>)</p><h4 id="링크드-리스트의-사이에-데이터를-추가하기"><a href="#링크드-리스트의-사이에-데이터를-추가하기" class="headerlink" title="링크드 리스트의 사이에 데이터를 추가하기"></a>링크드 리스트의 사이에 데이터를 추가하기</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data, next=None):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.next = next</span><br><span class="line"></span><br><span class="line">def add(data):</span><br><span class="line">    node = head</span><br><span class="line">    <span class="keyword">while</span> node.next:</span><br><span class="line">        node = node.next</span><br><span class="line">    node.next = Node(data)</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">node1 = Node(1)</span><br><span class="line">head = node1</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(2, 10):</span><br><span class="line">    add(index)</span><br></pre></td></tr></table></figure><hr><h4 id="링크드-리스트-데이터-출력하기-검색하기"><a href="#링크드-리스트-데이터-출력하기-검색하기" class="headerlink" title="링크드 리스트 데이터 출력하기(검색하기)"></a>링크드 리스트 데이터 출력하기(검색하기)</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">node = head</span><br><span class="line"><span class="keyword">while</span> node.next:</span><br><span class="line">    <span class="built_in">print</span>(node.data)</span><br><span class="line">    node = node.next</span><br><span class="line"><span class="built_in">print</span> (node.data)</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td></tr></table></figure><hr><ul><li>node3이 들어갈 자리를 아래와 같이 1의 데이터값을 갖는 노드 다음에 위치시키려면 1의 값을 갖는 노드의 next값을 먼저 저장해준뒤, 다음에 node3을 연결해준다. 그리고 다시 node3의 next가 이전 1의 값을 갖는 node의 next를 가리키게 하면된다.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">node3 = Node(1.5)</span><br><span class="line"></span><br><span class="line">node = head</span><br><span class="line">search = True</span><br><span class="line"><span class="keyword">while</span> search:</span><br><span class="line">    <span class="keyword">if</span> node.data == 1:</span><br><span class="line">        search = False</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        node = node.next</span><br><span class="line"></span><br><span class="line">node_next = node.next</span><br><span class="line">node.next = node3</span><br><span class="line">node3.next = node_next</span><br></pre></td></tr></table></figure></li></ul><hr><ul><li>이제 원하는대로 node가 삽입 되었는지 살펴보자.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">node = head</span><br><span class="line"><span class="keyword">while</span> node.next:</span><br><span class="line">    <span class="built_in">print</span>(node.data)</span><br><span class="line">    node = node.next</span><br><span class="line"><span class="built_in">print</span> (node.data)</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">1.5</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td></tr></table></figure><hr><h3 id="Pseudo-Code-for-Single-Linked-List"><a href="#Pseudo-Code-for-Single-Linked-List" class="headerlink" title="Pseudo Code for Single Linked List"></a>Pseudo Code for Single Linked List</h3><ul><li><p>위에서 언급한 기능들을 포함한 Single linked List를 객체로 만들어 본다.</p></li><li><ol><li>먼저, Single linked List에 필요한 기능 중 하나인 next를 찾으려면 시작점을 정해주어야 할 것이다. 그러므로 <code>head</code>와 리스트의 전체 사이즈를 나타내는 <code>n_size</code>를 만들어준다.</li></ol></li><li><ol><li>리스트를 추가해주는 <code>add</code>와 해당 데이터 값을 갖고있는 노드의 위치와 데이터값을 출력해주는 <code>search</code>를 만든다. <code>add</code>를 만들 때 주의할 점은head가 새롭게 추가되는 데이터가 되도록만들어 준다는 것을 생각해주어야 한다. next로 연결해주는 순서를 통해 만들어준다. <code>search</code>를 만들때 주의점은 while문을 돌면서 head 부터 시작하여 차례대로 찾는값과 비교해 주는데 찾는값이 해당 Linked list에 꼭 있다는 보장이 없기 때문에 data.next = None (마지막 노드)인 경우에는 cur.data가 없어 error를 발생시키게 된다. 그러므로 우선 cur의 데이터가 존재하는지 부터 생각하자. 또한, Node 자체를 반환해주는 식으로 작성해야 할 것이다. Linked list는 노드의 연결체이기 때문이다.</li></ol></li><li><ol><li><code>delete</code>함수는 python의 garbage collection을 통해 head였던 node의 연결을 끊어 줌으로써 구현할 수 있다.</li></ol></li><li><ol><li><code>traverse</code>함수는 제너레이터를 사용하여 전체적인 노드값을 보여주는 제너레이터는 제너레이터 객체에서 <strong>next</strong> 메서드를 호출할 때마다 함수 안의 yield까지 코드를 실행하며 yield에서 값을 발생시킨다.(generate) <code>yield를 사용하면 값을 함수 바깥으로 전달하면서 코드 실행을 함수 바깥에 양보하게된다. 따라서 yield는 현재 함수를 잠시 중단하고 함수 바깥의 코드가 실행되도록 만든다.</code></li></ol></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line">        self.__next=None</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def next(self):</span><br><span class="line">        <span class="built_in">return</span> self.__next</span><br><span class="line"></span><br><span class="line">    @next.setter</span><br><span class="line">    def next(self, n):</span><br><span class="line">        self.__next=n</span><br><span class="line"></span><br><span class="line">class Single_Linked_List:</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.head=None</span><br><span class="line">        self.n_size=0</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.n_size==0:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def size(self):</span><br><span class="line">        <span class="built_in">return</span> self.n_size</span><br><span class="line"></span><br><span class="line">    def add(self, data):</span><br><span class="line">        node=Node(data)</span><br><span class="line">        <span class="keyword">if</span> self.head==None:</span><br><span class="line">            self.head=node</span><br><span class="line">            self.n_size+=1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur=self.head</span><br><span class="line">            <span class="keyword">while</span> cur:</span><br><span class="line">                <span class="keyword">if</span> cur.next == None:</span><br><span class="line">                    cur.next=node</span><br><span class="line">                    self.n_size += 1</span><br><span class="line">                    cur=node.next</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    cur = cur.next</span><br><span class="line"></span><br><span class="line">    def add_after(self, data, target):</span><br><span class="line">        node=Node(data)</span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data == target:</span><br><span class="line">                cur_next=cur.next</span><br><span class="line">                cur.next=node</span><br><span class="line">                node.next=cur_next</span><br><span class="line">                <span class="built_in">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur=cur.next</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def search(self, data):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        이 함수는 리스트의 search 명령과 같이 동일한 값을 갖는 처음 만나는 노드를 반환한다.</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data == data:</span><br><span class="line">                <span class="built_in">return</span> cur</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur=cur.next</span><br><span class="line"></span><br><span class="line">    def delete(self, data):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        이 함수는 python의 garbage collection의 개념을 통해 연결을 끊어 제거해주는 방식으로 구현하였다.</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        cur = self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data == data:</span><br><span class="line">                previous.next =cur.next</span><br><span class="line">                <span class="built_in">break</span></span><br><span class="line">            previous = cur</span><br><span class="line">            cur = cur.next</span><br><span class="line">        self.n_size -= 1</span><br><span class="line"></span><br><span class="line">    def traverse(self):</span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            yield cur</span><br><span class="line">            cur=cur.next</span><br></pre></td></tr></table></figure><hr><h4 id="print-the-list"><a href="#print-the-list" class="headerlink" title="print the list"></a>print the list</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def show_list(slist):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'data size : &#123;&#125;'</span>.format(slist.size()))</span><br><span class="line">    g=slist.traverse()</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> g:</span><br><span class="line">        <span class="built_in">print</span>(node.data, end= <span class="string">'  '</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure><hr><h4 id="리스트에-삽입-및-데이터-확인"><a href="#리스트에-삽입-및-데이터-확인" class="headerlink" title="리스트에 삽입 및 데이터 확인"></a>리스트에 삽입 및 데이터 확인</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">slist=Single_Linked_List()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삽입'</span>)</span><br><span class="line">slist.add(3)</span><br><span class="line">slist.add(1)</span><br><span class="line">slist.add(5)</span><br><span class="line">slist.add(2)</span><br><span class="line">slist.add(7)</span><br><span class="line">slist.add(8)</span><br><span class="line">slist.add(3)</span><br><span class="line">show_list(slist)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">데이터 삽입</span><br><span class="line">data size : 7</span><br><span class="line">3  1  5  2  7  8  3</span><br></pre></td></tr></table></figure><hr><h4 id="리스트-중간에-삽입"><a href="#리스트-중간에-삽입" class="headerlink" title="리스트 중간에 삽입"></a>리스트 중간에 삽입</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slist.add_after(7, 5)</span><br><span class="line">show_list(slist)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data size : 7</span><br><span class="line">3  1  5  7  2  7  8  3</span><br></pre></td></tr></table></figure><hr><h4 id="search"><a href="#search" class="headerlink" title="search"></a>search</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 탐색'</span>)</span><br><span class="line">target=7</span><br><span class="line">res=slist.search(target)</span><br><span class="line"><span class="keyword">if</span> res:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'데이터 &#123;&#125; 검색 성공'</span>.format(res.data))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'데이터 &#123;&#125; 탐색 실패'</span>.format(target))</span><br><span class="line">res=None</span><br><span class="line"><span class="built_in">print</span>()</span><br></pre></td></tr></table></figure><hr><h6 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">데이터 탐색</span><br><span class="line">데이터 7 검색 성공</span><br></pre></td></tr></table></figure><hr><h4 id="delete"><a href="#delete" class="headerlink" title="delete"></a>delete</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삭제'</span>)</span><br><span class="line">slist.delete(5)</span><br><span class="line">slist.delete(7)</span><br><span class="line">slist.delete(8)</span><br><span class="line">show_list(slist)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">데이터 삭제</span><br><span class="line">data size : 4</span><br><span class="line">3  1  2  7  3</span><br></pre></td></tr></table></figure><hr><h3 id="7-다양한-링크드-리스트-구조"><a href="#7-다양한-링크드-리스트-구조" class="headerlink" title="7. 다양한 링크드 리스트 구조"></a>7. 다양한 링크드 리스트 구조</h3><ul><li>더블 링크드 리스트(Doubly linked list) 기본 구조<ul><li>이중 연결 리스트라고도 함</li><li>장점: 양방향으로 연결되어 있어서 노드 탐색이 양쪽으로 모두 가능<br><br><br><img src="https://www.fun-coding.org/00_Images/doublelinkedlist.png"><br>(출처: wikipedia, <a href="https://en.wikipedia.org/wiki/Linked_list" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Linked_list</a>)</li></ul></li></ul><ul><li>위에서 언급한 것과 같이 양방향에서 모두 탐색이 가능하다는 점을 주의하자.<ul><li>양방향을 사용하기 위해선 위에서 정의했던 노드의 방향을 추가하여 재정의해주어야한다.</li></ul></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self,data=None):</span><br><span class="line">        self.__data = data</span><br><span class="line">        self. __next = None</span><br><span class="line">        self.__before = None</span><br><span class="line">   <span class="comment"># 소멸자 : 객체가 메모리에서 사라질 때 반드시 한번 호출하는 것을 보장</span></span><br><span class="line">    def __del__(self):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'&#123;&#125; is deleted'</span>.format(self.__data))</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data = data</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def next(self):</span><br><span class="line">        <span class="built_in">return</span> self.__next</span><br><span class="line"></span><br><span class="line">    @next.setter</span><br><span class="line">    def next(self, next):</span><br><span class="line">        self.__next = next</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def before(self):</span><br><span class="line">        <span class="built_in">return</span> self.__before</span><br><span class="line"></span><br><span class="line">    @before.setter</span><br><span class="line">    def before(self, before):</span><br><span class="line">        self.__before = before</span><br></pre></td></tr></table></figure><hr><h3 id="Pseudo-Code-for-Single-Linked-List-1"><a href="#Pseudo-Code-for-Single-Linked-List-1" class="headerlink" title="Pseudo Code for Single Linked List"></a>Pseudo Code for Single Linked List</h3><ul><li><p>가장 먼저 single linked list와 다르게 head만 존재하는 것이 아니라 <code>tail</code>도 존재한다는 것을 유의하자.</p></li><li><p>필요한 기능을 나열해보자. 대략적으로 나누어서 생각해보면 다음과 같은 기능들이 필요할 것이다.</p><ul><li>size</li><li>empty</li><li>add<ul><li>head after</li><li>tail before</li><li>mid</li></ul></li><li>search<ul><li>head -&gt; tail</li><li>tail -&gt; head</li></ul></li><li>delete<ul><li>head</li><li>tail</li><li>mid</li></ul></li></ul></li><li><p><code>insert_after</code>, <code>insert_before</code>는 if문의 순서를 바꾸어 주어 링크드리스트내에 노드가 1개짜리인 경우에는 각각 <code>add_first</code>와 <code>add_last</code>를 하는 것과 동일하므로 함수로 계산하는 것으로 대체해주었다.</p></li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line">class DoubleLinkedList:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.head = None</span><br><span class="line">        self.tail = None</span><br><span class="line">        self.d_size = 0</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.d_size==0:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def size(self):</span><br><span class="line">        <span class="built_in">return</span> self.d_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def add_first(self, data):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            new=Node(data)</span><br><span class="line">            self.head=new</span><br><span class="line">            self.tail=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new=Node(data)</span><br><span class="line">            new.next=self.head</span><br><span class="line">            self.head.before=new</span><br><span class="line">            self.head=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def add_last(self, data):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            new=Node(data)</span><br><span class="line">            self.head=new</span><br><span class="line">            self.tail=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new=Node(data)</span><br><span class="line">            self.tail.next=new</span><br><span class="line">            new.before=self.tail</span><br><span class="line">            self.tail=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def insert_after(self, data, node):</span><br><span class="line">        <span class="keyword">if</span> node == self.tail:</span><br><span class="line">            self.add_last(data)</span><br><span class="line">        <span class="keyword">elif</span> node == self.head:</span><br><span class="line">            new=Node(data)</span><br><span class="line">            new.next=self.head.next</span><br><span class="line">            new.before=self.head</span><br><span class="line">            self.head.next.before=new</span><br><span class="line">            self.head.next=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            new=Node(data)</span><br><span class="line">            new.next=node.next</span><br><span class="line">            new.before=node</span><br><span class="line">            node.next.before=new</span><br><span class="line">            node.next=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line"></span><br><span class="line">    def insert_before(self, data, node):</span><br><span class="line">        <span class="keyword">if</span> node == self.head:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"this"</span>)</span><br><span class="line">            self.add_first(data)</span><br><span class="line">        <span class="keyword">elif</span> node == self.tail:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"is"</span>)</span><br><span class="line">            new=Node(data)</span><br><span class="line">            new.next=self.tail</span><br><span class="line">            new.before=self.tail.before</span><br><span class="line">            self.tail.before.next=new</span><br><span class="line">            self.tail.before=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"shit"</span>)</span><br><span class="line">            new=Node(data)</span><br><span class="line">            new.next=node</span><br><span class="line">            new.before=node.before</span><br><span class="line">            node.before.next=new</span><br><span class="line">            node.before=new</span><br><span class="line">            self.d_size+=1</span><br><span class="line"></span><br><span class="line">    def search_forward(self, data):</span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data==data:</span><br><span class="line">                <span class="built_in">return</span> cur</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur=cur.next</span><br><span class="line">        <span class="keyword">if</span> cur == None:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"리스트 안에 찾는 데이터가 존재하지 않습니다."</span>)</span><br><span class="line"></span><br><span class="line">    def search_backward(self, data):</span><br><span class="line">        cur=self.tail</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data == data:</span><br><span class="line">                <span class="built_in">return</span> cur</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                cur=cur.before</span><br><span class="line">        <span class="keyword">if</span> cur == None:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"리스트 안에 찾는 데이터가 존재하지 않습니다."</span>)</span><br><span class="line"></span><br><span class="line">    def delete_first(self):</span><br><span class="line">        new_head=self.head.next</span><br><span class="line">        self.head.next=None</span><br><span class="line">        self.head=new_head</span><br><span class="line">        new_head.before=None</span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line">    def delete_last(self):</span><br><span class="line">        new_tail=self.tail.before</span><br><span class="line">        self.tail.before=None</span><br><span class="line">        self.tail=new_tail</span><br><span class="line">        self.tail.next=None</span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def delete_node(self, node):</span><br><span class="line">        <span class="keyword">if</span> node == self.head:</span><br><span class="line">            self.delete_first()</span><br><span class="line">        <span class="keyword">elif</span> node == self.tail:</span><br><span class="line">            self.delete_last()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node.before.next=node.next</span><br><span class="line">            node.next.before=node.before</span><br><span class="line"></span><br><span class="line">    def traverse(self, start=True):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        start=True --&gt; from head</span></span><br><span class="line"><span class="string">        start=False --&gt; from tail</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="keyword">if</span> start:</span><br><span class="line">            cur=self.head</span><br><span class="line">            <span class="keyword">while</span> cur:</span><br><span class="line">                yield cur</span><br><span class="line">                cur=cur.next</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur=self.tail</span><br><span class="line">            <span class="keyword">while</span> cur:</span><br><span class="line">                yield cur</span><br><span class="line">                cur=cur.before</span><br></pre></td></tr></table></figure><hr><h4 id="print-the-linked-list"><a href="#print-the-linked-list" class="headerlink" title="print the linked list"></a>print the linked list</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def show_list(dlist, start=True):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'data size : &#123;&#125;'</span>.format(dlist.size()))</span><br><span class="line">    g=dlist.traverse(start)</span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> g:</span><br><span class="line">        <span class="built_in">print</span>(node.data, end=<span class="string">'  '</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br></pre></td></tr></table></figure><hr><h2 id="make-a-double-linked-list"><a href="#make-a-double-linked-list" class="headerlink" title="make a double linked list"></a>make a double linked list</h2><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dlist=DoubleLinkedList()</span><br></pre></td></tr></table></figure><hr><h4 id="insert-1-add-first"><a href="#insert-1-add-first" class="headerlink" title="insert 1 - add_first"></a>insert 1 - add_first</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삽입 -add_first'</span>)</span><br><span class="line">dlist.add_first(1)</span><br><span class="line">dlist.add_first(2)</span><br><span class="line">dlist.add_first(3)</span><br><span class="line">dlist.add_first(5)</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">데이터 삽입 -add_first</span><br><span class="line">data size : 4</span><br><span class="line">5  3  2  1</span><br></pre></td></tr></table></figure><hr><h4 id="insert-2-add-last"><a href="#insert-2-add-last" class="headerlink" title="insert 2 - add_last"></a>insert 2 - add_last</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삽입 -add_last'</span>)</span><br><span class="line">dlist.add_last(1)</span><br><span class="line">dlist.add_last(2)</span><br><span class="line">dlist.add_last(3)</span><br><span class="line">dlist.add_last(5)</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">데이터 삽입 -add_last</span><br><span class="line">data size : 8</span><br><span class="line">5  3  2  1  1  2  3  5</span><br></pre></td></tr></table></figure><hr><h4 id="insert-3-insert-after"><a href="#insert-3-insert-after" class="headerlink" title="insert 3 - insert_after"></a>insert 3 - insert_after</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삽입 - insert_after'</span>)</span><br><span class="line">dlist.insert_after(4, dlist.search_forward(5))</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">데이터 삽입 - insert_after</span><br><span class="line">data size : 9</span><br><span class="line">5  4  3  2  1  1  2  3  5</span><br></pre></td></tr></table></figure><hr><h4 id="insert-4-insert-before"><a href="#insert-4-insert-before" class="headerlink" title="insert 4 - insert_before"></a>insert 4 - insert_before</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 삽입 - insert_before'</span>)</span><br><span class="line">dlist.insert_before(4, node=dlist.search_forward(5))</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">데이터 삽입 - insert_before</span><br><span class="line">this</span><br><span class="line">data size : 10</span><br><span class="line">4  5  4  3  2  1  1  2  3  5</span><br></pre></td></tr></table></figure><hr><h4 id="search-1"><a href="#search-1" class="headerlink" title="search"></a>search</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'데이터 탐색'</span>)</span><br><span class="line">target=2</span><br><span class="line"><span class="comment">#res=dlist.search_forward(target)</span></span><br><span class="line">res=dlist.search_backward(target)</span><br><span class="line"><span class="keyword">if</span> res:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'데이터 &#123;&#125; 탐색 성공'</span>.format(res.data))</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'데이터 &#123;&#125; 탐색 실패'</span>.format(target))</span><br><span class="line">res=None</span><br></pre></td></tr></table></figure><hr><h6 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">데이터 탐색</span><br><span class="line">데이터 2 탐색 성공</span><br></pre></td></tr></table></figure><hr><h4 id="delete-first"><a href="#delete-first" class="headerlink" title="delete_first"></a>delete_first</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dlist.delete_first()</span><br><span class="line">dlist.delete_first()</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4 is deleted</span><br><span class="line">5 is deleted</span><br><span class="line">data size : 8</span><br><span class="line">4  3  2  1  1  2  3  5</span><br></pre></td></tr></table></figure><hr><h4 id="delete-last"><a href="#delete-last" class="headerlink" title="delete_last"></a>delete_last</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dlist.delete_last()</span><br><span class="line">dlist.delete_last()</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">5 is deleted</span><br><span class="line">3 is deleted</span><br><span class="line">data size : 6</span><br><span class="line">4  3  2  1  1  2</span><br></pre></td></tr></table></figure><hr><h4 id="delete-node"><a href="#delete-node" class="headerlink" title="delete_node"></a>delete_node</h4><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dlist.delete_node(dlist.search_backward(1))</span><br><span class="line">show_list(dlist, start=True)</span><br></pre></td></tr></table></figure><hr><h6 id="결과-14"><a href="#결과-14" class="headerlink" title="결과"></a>결과</h6><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 is deleted</span><br><span class="line">data size : 6</span><br><span class="line">4  3  2  1  2</span><br></pre></td></tr></table></figure><hr><ul><li>위의 테스트와 동일한 결과를 출력하나 head와 tail을 Node로 만들어 head와 tail에 관해 좀더 직관적으로 확인할 수 있고, Node 추가시 조금의 이점이 더 있게끔 구현해본 또 다른 코드이다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">class DoubleLinkedList:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.head = Node()</span><br><span class="line">        self.tail = Node()</span><br><span class="line">        self.head.next = self.tail</span><br><span class="line">        self.tail.before = self.head</span><br><span class="line">        self.d_size = 0</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.d_size==0:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def size(self):</span><br><span class="line">        <span class="built_in">return</span> self.d_size</span><br><span class="line"></span><br><span class="line">    def add_first(self, data):</span><br><span class="line">        <span class="comment">#새 노드를 생성</span></span><br><span class="line">        new_node = Node(data)</span><br><span class="line"></span><br><span class="line">        new_node.next = self.head.next</span><br><span class="line">        new_node.before = self.head</span><br><span class="line">        <span class="comment">#위의 두가지는 순서가 바뀌어도 됨!</span></span><br><span class="line">        self.head.next.before = new_node</span><br><span class="line">        self.head.next = new_node</span><br><span class="line"></span><br><span class="line">        self.d_size+=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def add_last(self, data):</span><br><span class="line">        new_node = Node(data)</span><br><span class="line"></span><br><span class="line">        new_node.next = self.tail</span><br><span class="line">        new_node.before = self.tail.before</span><br><span class="line"></span><br><span class="line">        self.tail.before.next = new_node</span><br><span class="line">        self.tail.before = new_node</span><br><span class="line"></span><br><span class="line">        self.d_size+=1</span><br><span class="line"></span><br><span class="line">    def insert_after(self, data, node):</span><br><span class="line">        new_node = Node(data)</span><br><span class="line"></span><br><span class="line">        new_node.next = node.next</span><br><span class="line">        new_node.before = node</span><br><span class="line">        <span class="comment">#new_node.before = node.next.before</span></span><br><span class="line">        node.next.before = new_node</span><br><span class="line">        node.next = new_node</span><br><span class="line"></span><br><span class="line">        self.d_size+=1</span><br><span class="line"></span><br><span class="line">    def insert_before(self, data, node):</span><br><span class="line">        new_node = Node(data)</span><br><span class="line"></span><br><span class="line">        new_node.next = node</span><br><span class="line">        new_node.before = node.before</span><br><span class="line">        node.before.next = new_node</span><br><span class="line">        node.before = new_node</span><br><span class="line"></span><br><span class="line">        self.d_size+=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def search_forward(self, data):</span><br><span class="line">        current = self.head.next</span><br><span class="line">        <span class="keyword">while</span>(current is not self.tail):</span><br><span class="line">            <span class="keyword">if</span>(current.data == data):</span><br><span class="line">                <span class="built_in">return</span> current</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                current = current.next</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line"></span><br><span class="line">    def search_backward(self, data):</span><br><span class="line">        current = self.tail.before</span><br><span class="line">        <span class="keyword">while</span>(current is not self.head): <span class="comment"># 주소값을 비교하기 때문에 !=이 아닌 is not으로 하는 것이 더 좋음!!</span></span><br><span class="line">            <span class="keyword">if</span>(current.data == data):</span><br><span class="line">                <span class="built_in">return</span> current</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                current = current.before</span><br><span class="line">        <span class="built_in">return</span> None</span><br><span class="line"></span><br><span class="line">    def delete_first(self):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            <span class="built_in">return</span></span><br><span class="line">        self.head.next = self.head.next.next</span><br><span class="line">        self.head.next.before = self.head  </span><br><span class="line"></span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line">    def delete_last(self):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            <span class="built_in">return</span></span><br><span class="line">        self.tail.before=self.tail.before.before</span><br><span class="line">        self.tail.before.next=self.tail</span><br><span class="line"></span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line">    def delete_node(self, node):</span><br><span class="line">        node.before.next=node.next <span class="comment">#reference count가 0이되어 사라짐!</span></span><br><span class="line">        node.next.before=node.before</span><br><span class="line"></span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line">    def traverse(self, start=True):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        start=True --&gt; from head</span></span><br><span class="line"><span class="string">        start=False --&gt; from tail</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line">        <span class="keyword">if</span> start:</span><br><span class="line">            cur=self.head</span><br><span class="line">            <span class="keyword">while</span> cur is not self.tail:</span><br><span class="line">                yield cur</span><br><span class="line">                cur=cur.next</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cur=self.tail.before</span><br><span class="line">            <span class="keyword">while</span> cur is not self.head:</span><br><span class="line">                yield cur</span><br><span class="line">                cur=cur.before</span><br></pre></td></tr></table></figure><hr><blockquote><p>위의 클래스를 사용하여 Python의 리스트를 구현해보았다.</p></blockquote><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line">from double_linked_list import DoubleLinkedList</span><br><span class="line"></span><br><span class="line">class PseudoList(DoubleLinkedList):</span><br><span class="line">    <span class="comment">#pos는 파이썬 리스트의 인덱스와 비슷하게</span></span><br><span class="line">    <span class="comment">#데이터의 위치를 나타냄.</span></span><br><span class="line">    <span class="comment">#인덱스처럼 0이 첫번째 위치를 의미</span></span><br><span class="line">    def __init__(self, *args):</span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="keyword">for</span> elem <span class="keyword">in</span> args:</span><br><span class="line">            self.add_last(elem)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#전역함수 len(list)을 호출할 때 이 함수가 호출</span></span><br><span class="line">    def __len__(self):</span><br><span class="line">        <span class="built_in">return</span> self.size()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#파이썬 리스트의 append는 맨 뒤에 데이터를 추가</span></span><br><span class="line">    <span class="comment">#더블 링크드 리스트에 있는 add_last 함수를 사용</span></span><br><span class="line">    def append(self, data): <span class="comment">#래핑 함수</span></span><br><span class="line">        self.add_last(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">#인자로 pos를 받으면 pos에 위치한 노드를 반환한다.</span></span><br><span class="line">    def __find_position(self, pos):</span><br><span class="line">        <span class="keyword">if</span> pos &gt;=self.size():</span><br><span class="line">            raise IndexError(<span class="string">'list index out of range'</span>)</span><br><span class="line">        cur = self.head.next</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(pos):</span><br><span class="line">            cur = cur.next</span><br><span class="line">        <span class="built_in">return</span> cur</span><br><span class="line"></span><br><span class="line">    <span class="comment">#pos에 위치한 노드를 구한 뒤 --&gt; __find_position()</span></span><br><span class="line">    <span class="comment">#그 노드의 앞에 데이터를 삽입 --&gt; insert_before()</span></span><br><span class="line">    def insert(self, pos, data):    </span><br><span class="line">        node = self.__find_position(pos)</span><br><span class="line">        self.insert_before(data, node)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#리스트에 있는 data의 개수를 카운트</span></span><br><span class="line">    <span class="comment">#리스트를 순회하면서 데이터가 있으면 cnt 변수를 1씩 증가시킴</span></span><br><span class="line">    def count(self, data):</span><br><span class="line">        cnt = 0</span><br><span class="line">        cur = self.head.next</span><br><span class="line">        <span class="keyword">while</span> cur is not self.tail:</span><br><span class="line">            <span class="keyword">if</span> cur.data == data:</span><br><span class="line">                cnt += 1</span><br><span class="line">            cur = cur.next</span><br><span class="line">        <span class="built_in">return</span> cnt</span><br><span class="line"></span><br><span class="line">    <span class="comment">#인자 data가 위치한 인덱스를(여기에서는 pos) 반환한다</span></span><br><span class="line">    <span class="comment">#start는 데이터를 찾기 시작하는 위치</span></span><br><span class="line">    def index(self, data, start=0):</span><br><span class="line">        cur = self.__find_position(start)</span><br><span class="line">        index = start</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            <span class="keyword">if</span> cur.data == data:</span><br><span class="line">                <span class="built_in">return</span> index</span><br><span class="line">            cur = cur.next</span><br><span class="line">            index += 1</span><br><span class="line"></span><br><span class="line">        raise ValueError(<span class="string">'&#123;&#125; is not in the list'</span>.format(data))</span><br><span class="line"></span><br><span class="line">    <span class="comment">#print(li[3]) 처럼 [] 연산자를 통해 값을 가져올 때 내부에서 호출</span></span><br><span class="line">    <span class="comment">#index는 pos를 의미</span></span><br><span class="line">    <span class="comment">#index에 위치한 노드의 데이터를 반환</span></span><br><span class="line">    def __getitem__(self, index):<span class="comment">#연산자 오버로딩</span></span><br><span class="line">        node = self.__find_position(index)</span><br><span class="line">        <span class="built_in">return</span> node.data</span><br><span class="line"></span><br><span class="line">    <span class="comment">#li[3]=10 처럼 [] 연산자로 값을 대입할 때 내부에서 호출</span></span><br><span class="line">    <span class="comment">#index는 pos를 의미</span></span><br><span class="line">    <span class="comment">#index에 위치한 노드의 값을 data로 바꿈</span></span><br><span class="line">    def __setitem__(self, index, data):<span class="comment">#연산자 오버로딩</span></span><br><span class="line">        node = self.__find_position(index)</span><br><span class="line">        node.data = data</span><br><span class="line"></span><br><span class="line">    <span class="comment">#pos에 있는 데이터를 삭제하면서 반환</span></span><br><span class="line">    <span class="comment">#파이썬 리스트처럼 인자를 주지 않으면</span></span><br><span class="line">    <span class="comment">#리스트의 맨 마지막 데이터를 삭제하면서 반환</span></span><br><span class="line">    def pop(self, pos=None):</span><br><span class="line">        <span class="comment">#인자 pos가 있는 경우</span></span><br><span class="line">        <span class="keyword">if</span> pos:</span><br><span class="line">            node = self.__find_position(pos)</span><br><span class="line">        <span class="comment">#pos가 비어 있는 경우</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node = self.tail.before</span><br><span class="line">        <span class="comment">#delete_node 함수를 이용해 노드 삭제</span></span><br><span class="line">        cur = node</span><br><span class="line">        self.delete_node(node)</span><br><span class="line">        <span class="built_in">return</span> cur.data</span><br><span class="line"></span><br><span class="line">    <span class="comment">#리스트에 있는 데이터를 삭제</span></span><br><span class="line">    <span class="comment">#만약 같은 데이터가 여러개라면</span></span><br><span class="line">    <span class="comment">#리스트에서 위치상 첫번째 데이터가 삭제된다</span></span><br><span class="line">    <span class="comment">#반환은 하지 않는다</span></span><br><span class="line">    def remove(self, data):</span><br><span class="line">        <span class="comment">#delete_node와 search_forward 함수 이용</span></span><br><span class="line">        self.delete_node(self.search_forward(data))</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        string = <span class="string">'['</span></span><br><span class="line">        cur = self.head.next</span><br><span class="line">        <span class="keyword">while</span> cur is not self.tail:</span><br><span class="line">            string+=str(cur.data)</span><br><span class="line">            <span class="keyword">if</span> cur.next is not self.tail:</span><br><span class="line">                string+=<span class="string">', '</span></span><br><span class="line">            cur= cur.next</span><br><span class="line"></span><br><span class="line">        string+=<span class="string">']'</span></span><br><span class="line">        <span class="built_in">return</span> string</span><br></pre></td></tr></table></figure><hr><h3 id="객체-생성"><a href="#객체-생성" class="headerlink" title="객체 생성"></a>객체 생성</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">initial = [1, 2, 3, 4]</span><br><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li = PseudoList(*initial)</span><br><span class="line"><span class="comment">#python list</span></span><br><span class="line">py_li = initial</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-15"><a href="#결과-15" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pseudo list : [1, 2, 3, 4]</span><br><span class="line">python list : [1, 2, 3, 4]</span><br></pre></td></tr></table></figure><hr><h3 id="append"><a href="#append" class="headerlink" title="append"></a>append</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li.append(2)</span><br><span class="line">li.append(1)</span><br><span class="line">li.append(2)</span><br><span class="line">li.append(7)</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li.append(2)</span><br><span class="line">py_li.append(1)</span><br><span class="line">py_li.append(2)</span><br><span class="line">py_li.append(7)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-16"><a href="#결과-16" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pseudo list : [1, 2, 3, 4, 2, 1, 2, 7]</span><br><span class="line">python list : [1, 2, 3, 4, 2, 1, 2, 7]</span><br></pre></td></tr></table></figure><hr><h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">target = 2</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'count of &#123;&#125; : &#123;&#125; in pseudo_list'</span>.format(target, li.count(target)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'count of &#123;&#125; : &#123;&#125; in python_list'</span>.format(target, py_li.count(target)))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-17"><a href="#결과-17" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">count of 2 : 3 <span class="keyword">in</span> pseudo_list</span><br><span class="line">count of 2 : 3 <span class="keyword">in</span> python_list</span><br></pre></td></tr></table></figure><hr><h3 id="pop"><a href="#pop" class="headerlink" title="pop"></a>pop</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li.pop(2)</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li.pop(2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-18"><a href="#결과-18" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data of 7 is deleted</span><br><span class="line">data of 2 is deleted</span><br><span class="line">pseudo list : [1, 2, 3, 4, 2, 1]</span><br><span class="line">python list : [1, 2, 3, 4, 2, 1]</span><br></pre></td></tr></table></figure><hr><h3 id="pop-index"><a href="#pop-index" class="headerlink" title="pop(index)"></a>pop(index)</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li.pop(2)</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li.pop(2)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-19"><a href="#결과-19" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data of 3 is deleted</span><br><span class="line">pseudo list : [1, 2, 4, 2, 1]</span><br><span class="line">python list : [1, 2, 4, 2, 1]</span><br></pre></td></tr></table></figure><hr><h3 id="insert"><a href="#insert" class="headerlink" title="insert"></a>insert</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li.insert(3, 9)</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li.insert(3, 9)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-20"><a href="#결과-20" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pseudo list : [1, 2, 4, 9, 2, 1]</span><br><span class="line">python list : [1, 2, 4, 9, 2, 1]</span><br></pre></td></tr></table></figure><hr><h3 id="index"><a href="#index" class="headerlink" title="index"></a>index</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">target = 9</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"index of &#123;&#125; : &#123;&#125; in pseudo_list"</span>.format(target, li.index(target)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"index of &#123;&#125; : &#123;&#125; in python_list"</span>.format(target, py_li.index(target)))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-21"><a href="#결과-21" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index of 9 : 3 <span class="keyword">in</span> pseudo_list</span><br><span class="line">index of 9 : 3 <span class="keyword">in</span> python_list</span><br></pre></td></tr></table></figure><hr><h3 id="indexing"><a href="#indexing" class="headerlink" title="indexing"></a>indexing</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li[3]=7</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li[3]=7</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-22"><a href="#결과-22" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pseudo list : [1, 2, 4, 7, 2, 1]</span><br><span class="line">python list : [1, 2, 4, 7, 2, 1]</span><br></pre></td></tr></table></figure><hr><h3 id="remove"><a href="#remove" class="headerlink" title="remove"></a>remove</h3><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pseudo_list</span></span><br><span class="line">li.remove(9)</span><br><span class="line"><span class="comment">#python_list</span></span><br><span class="line">py_li.remove(9)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"pseudo list : "</span>+ str(li))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"python list : "</span> + str(py_li))</span><br></pre></td></tr></table></figure><hr><h5 id="결과-23"><a href="#결과-23" class="headerlink" title="결과"></a>결과</h5><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data of 9 is deleted</span><br><span class="line">pseudo list : [1, 2, 4, 2, 1]</span><br><span class="line">python list : [1, 2, 4, 2, 1]</span><br></pre></td></tr></table></figure><hr>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/28/data_structure_03/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 01 Stack</title>
      <link>https://heung-bae-lee.github.io/2020/04/27/data_structure_02/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/27/data_structure_02/</guid>
      <pubDate>Mon, 27 Apr 2020 12:38:39 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Stack&quot;&gt;&lt;a href=&quot;#Stack&quot; class=&quot;headerlink&quot; title=&quot;Stack&quot;&gt;&lt;/a&gt;Stack&lt;/h1&gt;&lt;h2 id=&quot;꼭-알아둬야-할-자료-구조-스택-Stack&quot;&gt;&lt;a href=&quot;#꼭-알아둬야-할-자료-구조-스택-
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Stack"><a href="#Stack" class="headerlink" title="Stack"></a>Stack</h1><h2 id="꼭-알아둬야-할-자료-구조-스택-Stack"><a href="#꼭-알아둬야-할-자료-구조-스택-Stack" class="headerlink" title="꼭 알아둬야 할 자료 구조: 스택 (Stack)"></a>꼭 알아둬야 할 자료 구조: 스택 (Stack)</h2><ul><li>데이터를 제한적으로 접근할 수 있는 구조<ul><li>한쪽 끝에서만 자료를 넣거나 뺄 수 있는 구조</li></ul></li><li><p>가장 나중에 쌓은 데이터를 가장 먼저 빼낼 수 있는 데이터 구조</p><ul><li>큐: FIFO 정책 -&gt; 줄 세우기</li><li>스택: <code>LIFO 정책</code> -&gt; 책 쌓기</li></ul><h3 id="1-스택-구조"><a href="#1-스택-구조" class="headerlink" title="1. 스택 구조"></a>1. 스택 구조</h3><ul><li><p>스택은 LIFO(Last In, Fisrt Out) 또는 FILO(First In, Last Out) 데이터 관리 방식을 따름</p><ul><li>LIFO: 마지막에 넣은 데이터를 가장 먼저 추출하는 데이터 관리 정책</li><li>FILO: 처음에 넣은 데이터를 가장 마지막에 추출하는 데이터 관리 정책</li><li>참고로 Queue는 FIFO라고 많이 얘기하지만, Stack은 그냥 Stack이라고 한다.</li></ul></li><li><p>대표적인 스택의 활용</p><ul><li><code>컴퓨터 내부의 프로세스 구조의 함수 동작 방식</code></li></ul></li><li><p>주요 기능</p><ul><li>push(): 데이터를 스택에 넣기</li><li>pop(): 데이터를 스택에서 꺼내기</li></ul></li><li><p><font color="#BF360C">Visualgo 사이트에서 시연해보며 이해하기 (push/pop 만 클릭해보며): <a href="https://visualgo.net/en/list" target="_blank" rel="noopener">https://visualgo.net/en/list</a><br><br><br><img src="http://www.fun-coding.org/00_Images/stack.png"></font></p></li></ul><blockquote><p>그림으로 이해해보기</p></blockquote></li></ul><h2 id="2-스택-구조와-프로세스-스택"><a href="#2-스택-구조와-프로세스-스택" class="headerlink" title="2. 스택 구조와 프로세스 스택"></a>2. 스택 구조와 프로세스 스택</h2><ul><li>스택 구조는 <code>프로세스 실행 구조의 가장 기본</code><ul><li>함수 호출시 프로세스 실행 구조를 스택과 비교해서 이해 필요</li><li>stack이랑 queue는 일시적으로 자료를 보관할때 사용!!!</li></ul></li></ul><h1 id="재귀-함수"><a href="#재귀-함수" class="headerlink" title="재귀 함수"></a>재귀 함수</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def recursive(data):</span><br><span class="line">    <span class="keyword">if</span> data &lt; 0:</span><br><span class="line">        <span class="built_in">print</span> (<span class="string">"ended"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">print</span>(data)</span><br><span class="line">        recursive(data - 1)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"returned"</span>, data)</span><br></pre></td></tr></table></figure><h5 id="실행"><a href="#실행" class="headerlink" title="실행"></a>실행</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">recursive(4)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">3</span><br><span class="line">2</span><br><span class="line">1</span><br><span class="line">0</span><br><span class="line">ended</span><br><span class="line">returned 0</span><br><span class="line">returned 1</span><br><span class="line">returned 2</span><br><span class="line">returned 3</span><br><span class="line">returned 4</span><br></pre></td></tr></table></figure><ul><li>Process에서 함수가 어떻게 동작하는지 그리고 그것이 어떻게 Stack이라는 자료구조와 연결이 되는지 중점적으로 설명하면 다음과 같다. Program이 실행되는 상태를 Process라고 하는데 그 Process안에서 함수가 호출이 된 것이므로 Process Stack에 아래 그림과 같이 쌓이게 된다. recursive 함수가 최종적으로 끝나게 되면, Stack 구조와 같이 제일 마직막 실행된 함수부터 실행을 마치게 되어 위와 같은 결과가 출력이 되는 것이다.</li></ul><p><img src="/image/Stack_process.png" alt="Stack Process"></p><h2 id="3-자료-구조-스택의-장단점"><a href="#3-자료-구조-스택의-장단점" class="headerlink" title="3. 자료 구조 스택의 장단점"></a>3. 자료 구조 스택의 장단점</h2><ul><li>장점<ul><li>구조가 단순해서, 구현이 쉽다.</li><li>데이터 저장/읽기 속도가 빠르다.</li></ul></li><li>단점 (일반적인 스택 구현시)<ul><li><code>데이터 최대 갯수를 미리 정해야 한다.</code><ul><li>파이썬의 경우 재귀 함수는 1000번까지만 호출이 가능함</li></ul></li><li><code>저장 공간의 낭비가 발생할 수 있음</code><ul><li>미리 최대 갯수만큼 저장 공간을 확보해야 함</li></ul></li></ul></li></ul><blockquote><p>스택은 단순하고 빠른 성능을 위해 사용되므로, 보통 배열 구조를 활용해서 구현하는 것이 일반적임.<br>이 경우, 위에서 열거한 단점이 있을 수 있음</p></blockquote><h2 id="4-파이썬-리스트-기능에서-제공하는-메서드로-스택-사용해보기"><a href="#4-파이썬-리스트-기능에서-제공하는-메서드로-스택-사용해보기" class="headerlink" title="4. 파이썬 리스트 기능에서 제공하는 메서드로 스택 사용해보기"></a>4. 파이썬 리스트 기능에서 제공하는 메서드로 스택 사용해보기</h2><ul><li>append(push), pop 메서드 제공</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_stack = list()</span><br><span class="line"></span><br><span class="line">data_stack.append(1)</span><br><span class="line">data_stack.append(2)</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_stack</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[1, 2]</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_stack.pop()</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure><h2 id="5-프로그래밍-연습"><a href="#5-프로그래밍-연습" class="headerlink" title="5. 프로그래밍 연습"></a>5. 프로그래밍 연습</h2><div class="alert alert-block alert-warning"><strong><font color="blue" size="3em">연습1: 리스트 변수로 스택을 다루는 pop, push 기능 구현해보기 (pop, push 함수 사용하지 않고 직접 구현해보기)</font></strong><br></div><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">stack_list = list()</span><br><span class="line"></span><br><span class="line">def push(data):</span><br><span class="line">    stack_list.append(data)</span><br><span class="line"></span><br><span class="line">def pop():</span><br><span class="line">    data = stack_list[-1]</span><br><span class="line">    del stack_list[-1]</span><br><span class="line">    <span class="built_in">return</span> data</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(10):</span><br><span class="line">    push(index)</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pop()</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9</span><br></pre></td></tr></table></figure><h2 id="또-다른-방식으로-구현하는-Stack-구조"><a href="#또-다른-방식으로-구현하는-Stack-구조" class="headerlink" title="또 다른 방식으로 구현하는 Stack 구조"></a>또 다른 방식으로 구현하는 Stack 구조</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 많은 언어들이 실제 구현되어 있는 것을 보면 실제 자료는 container라는 stack안의 공간에 있고, stack은 인터페이스만 제공</span></span><br><span class="line">class Stack:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="comment"># 실제 데이터를 가지고 있는 자료구조</span></span><br><span class="line">        self.container=list() <span class="comment"># 빈 list 객체가 만들어져서 container에 저장.</span></span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> not self.container: <span class="comment"># 비어있으면 true가되니까</span></span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def push(self, data):</span><br><span class="line">        self.container.append(data)</span><br><span class="line"></span><br><span class="line">    def pop(self):</span><br><span class="line">        <span class="built_in">return</span> self.container.pop()</span><br><span class="line"></span><br><span class="line">    def peek(self):</span><br><span class="line">        <span class="built_in">return</span> self.container[-1]</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">s=Stack()</span><br><span class="line"></span><br><span class="line">s.push(1)</span><br><span class="line">s.push(2)</span><br><span class="line">s.push(3)</span><br><span class="line">s.push(4)</span><br><span class="line">s.push(5)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> not s.empty():</span><br><span class="line">    <span class="built_in">print</span>(s.pop(), end=<span class="string">'  '</span>)</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5  4  3  2  1</span><br></pre></td></tr></table></figure><h2 id="Node를-활용한-Stack-구조-구현하기"><a href="#Node를-활용한-Stack-구조-구현하기" class="headerlink" title="Node를 활용한 Stack 구조 구현하기"></a>Node를 활용한 Stack 구조 구현하기</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data=None):</span><br><span class="line">        self.__data=data</span><br><span class="line">        self.__next=None</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def next(self):</span><br><span class="line">        <span class="built_in">return</span> self.__next</span><br><span class="line"></span><br><span class="line">    @next.setter</span><br><span class="line">    def next(self, n):</span><br><span class="line">        self.__next=n</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">class LStack:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="comment">#인스턴스 멤버</span></span><br><span class="line">        self.top=None</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.top is None:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def push(self, data):</span><br><span class="line">        new_node = Node(data)</span><br><span class="line"><span class="comment">#         new_node.__data=data</span></span><br><span class="line"><span class="comment">#         if self.empty():</span></span><br><span class="line"><span class="comment">#             self.top = new_node</span></span><br><span class="line"><span class="comment">#             return</span></span><br><span class="line"></span><br><span class="line">        new_node.next = self.top</span><br><span class="line">        self.top = new_node</span><br><span class="line"></span><br><span class="line">    def pop(self):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            <span class="built_in">return</span> None</span><br><span class="line">        cur = self.top</span><br><span class="line">        self.top = self.top.next</span><br><span class="line">        <span class="built_in">return</span> cur.data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def peek(self):</span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            <span class="built_in">return</span> None</span><br><span class="line">        cur = self.top</span><br><span class="line">        <span class="built_in">return</span> cur.data</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">s = LStack()</span><br><span class="line"></span><br><span class="line">s.push(1)</span><br><span class="line">s.push(2)</span><br><span class="line">s.push(3)</span><br><span class="line">s.push(4)</span><br><span class="line">s.push(5)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> not s.empty():</span><br><span class="line">    <span class="built_in">print</span>(s.pop(), end=<span class="string">"  "</span>)</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5  4  3  2  1</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">s = LStack()</span><br><span class="line"></span><br><span class="line">s.push(1)</span><br><span class="line">s.push(2)</span><br><span class="line">s.empty()</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/27/data_structure_02/#disqus_thread</comments>
    </item>
    
    <item>
      <title>의사결정나무</title>
      <link>https://heung-bae-lee.github.io/2020/04/26/machine_learning_13/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/26/machine_learning_13/</guid>
      <pubDate>Sat, 25 Apr 2020 18:27:27 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Decision-tree-배경&quot;&gt;&lt;a href=&quot;#Decision-tree-배경&quot; class=&quot;headerlink&quot; title=&quot;Decision tree 배경&quot;&gt;&lt;/a&gt;Decision tree 배경&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;의사결정나무의 
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Decision-tree-배경"><a href="#Decision-tree-배경" class="headerlink" title="Decision tree 배경"></a>Decision tree 배경</h1><ul><li>의사결정나무의 장점은 해석력이 좋다. 우리가 모델을 만들때 성능이 좋은 것도 중요하지만, 어떻게 사람들한테 메세지를 줄 수 있는가처럼 어떻게 활용할 수 있는가가 더 중요한 경우도 있다. 예측력이 조금 떨어지더라도 이야기로 풀어서 어떠한 근거로 인해 Y는 이렇게 된다는 식으로 풀어서 설명할 수 있다는 의미이다.</li></ul><ul><li>결정트리는 매우 쉽고 유연하게 적용될 수 있는 알고리즘이다. 또한 <code>데이터의 Scaling이나 정규화(normalize) 등의 사전 가공의 영향이 매우 적다. 하지만, 예측 성능을 향상시키기 위해 복잡한 규칙 구조를 거쳐야 하며, 이로 인한 과적합(overfitting)이 발생해 반대로 예측 성능이 저하될 수도 있다는 단점이있다.</code> 이러한 단점이 앙상블 기법에서는 오히려 장점으로 작용한다. 앙상블은 매우 많은 여러개의 예측 성능이 상대적으로 떨어지는 학습 알고리즘을 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트하면서 예측 성능을 향상시키는데, 결정트리가 좋은 약한 학습기가 되기 때문이다.</li></ul><ul><li>결정 트리(Decision Tree)는 ML 알고리즘 중 직관적으로 이해하기 쉬운 알고리즘이다. <code>데이터에 있는 규칙을 학습을 통해 자동으로 찾아내는 트리(Tree) 기반의 분류 규칙을 만드는 것</code>이다. 따라서, <code>데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인각가 알고리즘의 성능을 크게 좌우</code>한다. 의사결정나무(decision tree)는 여러 가지 규칙을 순차적으로 적용하면서 독립 변수 공간을 분할하는 분류 모형이다. 분류(classification)와 회귀 분석(regression)에 모두 사용될 수 있기 때문에 CART(Classification And Regression Tree)라고도 한다.</li></ul><p><img src="/image/decision_tree_concept.png" alt="결정 트리"></p><ul><li>아래 그림은 결정 트리의 구조를 간략하게 나타낸 것이다. 데이터 세트에 feature가 있고 이러한 feature가 결합해 규칙 조건을 만들 때마다 규칙 노드가 만들어지며 새로운 규칙 조검마다 서브 트리(Sub tree)가 생성된다. 하지만 <code>많은 규칙이 있다는 것은 곧 분류를 결정하는 방식이 더욱 복잡해진다는 얘기이고, 이는 곧 과적합(overfitting)으로 이어지기 쉽다. 즉, 트리의 깊이(Depth)가 깊어질수록 결정 트리의 예측 성능이 저할될 가능성이 높아진다는 의미</code>이다.</li></ul><p><img src="/image/consist_of_decision_tree_node.png" alt="결정 트리 용어 - 01"></p><p><img src="/image/consist_of_decision_tree_node_01.png" alt="결정 트리 용어 - 02"></p><ul><li>결정트리는 다음과 같이 종속변수(반응 변수, target 값)의 자료형에 의해서 다음과 같이 분류될 수 있다. 아래 그림에서 오른쪽 그림이 분류트리이고 왼쪽 그림이 회귀 트리이다.</li></ul><p><img src="/image/what_kinds_of_decision_tree_for_dependent_variable.png" alt="결정 트리 종류"></p><h3 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h3><ul><li><p>그렇다면, 가능한 적은 결정 노드로 높은 예측 정확도를 가지려면 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙이 정해져야 한다. 이를 위해서는 어떻게 트리를 분할(Split)할 것인가가 중요한데 최대한 균일한 데이터 세트를 구성할 수 있도록 분할하는 것이 필요하다.</p></li><li><p>엔트로피는 섞여있는 상태를 의미한다고 생각하면 이해하기 쉽다. 섞여있는 상태면 엔트로피가 높은 것이고 물리적인 힘을 써서 분리는 해놓은 경우는 엔트로피가 낮은 상태이다. 아래 그림에서 $x$축의 $P+$가 의미하는 것이 노란 곡물이 나올 확률이라고 가정해 보자. $P+$가 0인 상황은 노란 곡물이 없는 상태를 의미하고, 1인 경우는 노란 곡물만 있는 상태일 것이다. 이 때의 엔트로피는 잘 분리되어있기 때문에 0의 값을 갖게된다. 허나 $P+$가 0.5일 경우는 노란곡물이 존재하거나 하지 않을 확률이 각각 절반이기 때문에 엔트로피가 가장 높게 된다.</p></li></ul><p><img src="/image/what_is_entropy.png" alt="Entropy"></p><ul><li><p>확률론에서의 <code>엔트로피</code> 개념은 <code>확률분포의 모양을 설명하는 특징값이며 확률분포가 가지고 있는 정보의 양을 나타내는 값</code>이기도 하다. 엔트로피는 두 확률분포의 모양이 어떤 관계를 가지는지 혹은 유사한지를 표현하는 데도 쓰인다. 조건부엔트로피는 한 확률분포에 의해 다른 확률분포가 받는 영향을 설명한다. 교차엔트로피와 쿨백-라이블러 발산은 두 확률분포가 얼마나 닮았는지를 나타낸다. 마지막으로 두 확률분포의 독립 및 상관관계를 나타내는 상호정보량에 대해서 설명할 것이다.</p></li><li><p>$Y\;=\;0$ 또는 $Y\;=\;1$인 두 가지 값을 가지는 확률변수의 확률분포가 다음과 같이 세 종류가 있다고 하자.</p><ul><li>확률분포 $Y_{1}$ : $P(Y=0)=0.5, P(Y=1)=0.5$</li><li>확률분포 $Y_{2}$ : $P(Y=0)=0.8, P(Y=1)=0.2$</li><li>확률분포 $Y_{3}$ : $P(Y=0)=1.0, P(Y=0)=0.0$</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(9, 3))</span><br><span class="line">plt.subplot(131)</span><br><span class="line">plt.bar([0, 1], [0.5, 0.5])</span><br><span class="line">plt.xticks([0, 1], [<span class="string">"Y=0"</span>, <span class="string">"Y=1"</span>])</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">plt.title(<span class="string">"<span class="variable">$Y_1</span>$"</span>)</span><br><span class="line">plt.subplot(132)</span><br><span class="line">plt.bar([0, 1], [0.8, 0.2])</span><br><span class="line">plt.xticks([0, 1], [<span class="string">"Y=0"</span>, <span class="string">"Y=1"</span>])</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">plt.title(<span class="string">"<span class="variable">$Y_2</span>$"</span>)</span><br><span class="line">plt.subplot(133)</span><br><span class="line">plt.bar([0, 1], [1.0, 0.0])</span><br><span class="line">plt.xticks([0, 1], [<span class="string">"Y=0"</span>, <span class="string">"Y=1"</span>])</span><br><span class="line">plt.ylim(0, 1.1)</span><br><span class="line">plt.title(<span class="string">"<span class="variable">$Y_3</span>$"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/probability_distribution_ploting.png" alt="확률분포 그래프"></p><ul><li><p>베이지안 관점에서 위 확률분포는 다음과 같은 정보를 나타낸다.</p><ul><li>확률분포 $Y_{1}$은 $y$값에 대해 아무것도 모르는 상태</li><li>확률분포 $Y_{2}$은 $y$값이 0이라고 믿지만 아닐 가능성도 있다는 것을 아는 상태</li><li>확률분포 $Y_{2}$은 $y$값이 0이라고 100% 확신하는 상태</li></ul></li><li><p>확률 분포가 가지는 이러한 차이를 하나의 숫자로 나타낸 것이 바로 엔트로피이다.</p></li></ul><h3 id="Entropy-정의"><a href="#Entropy-정의" class="headerlink" title="Entropy 정의"></a>Entropy 정의</h3><ul><li><p>엔트로피(Entropy)는 <code>확률분포가 가지는 정보의 확신도 혹은 정보량을 수치로 표현한 것</code>이다. 확률분포에서 특정한 값이 나올 확률이 높아지고 나머지 값의 확률은 낮아진다면 엔트로피가 작아진다. 반대로 여러가지 값이 나올 확률이 대부분 비슷한 경우에는 엔트로피가 높아진다. 엔트로피는 확률분포의 모양이 어떤지를 나타내는 특성값 중 하나로 볼 수도 있다. <code>확률 또는 확률밀도가 특정값에 몰려있으면 엔트로피가 작다고 하고 반대로 여러가지 값에 골고루 퍼져 있다면 엔트로피가 크다고 한다.</code> 확률분포의 엔트로피는 물리학의 엔트로피 용어를 빌려온 것이다. 물리학에서는 물질의 상태가 분산되는 정도를 엔트로피로 정의한다. 물체의 상태가 여러가지로 고루 분산되어 있으면 엔트로피가 높고 특정한 하나의 상태로 몰려있으면 엔트로피가 낮다. 수학적으로 엔트로피는 확률분포함수를 입력으로 받아 숫자를 출력하는 범함수(functional)로 정의한다.</p></li><li><p>확률변수 $Y$가 카테고리분포와 같은 이산확률변수이면 다음처럼 정의한다. 이 식에서 $K$는 $X$가 가질 수 있는 클래스의 수이고 p(y)는 확률질량함수이다. 확률의 로그값이 항상 음수이므로 음수 기호를 붙여서 양수로 만들었다.</p></li></ul><script type="math/tex; mode=display">\begin{align} H[Y] = -\sum_{k=1}^K p(y_k) \log_2 p(y_k) \end{align}</script><ul><li>확률변수 $Y$가 연속확률변수이면 다음처럼 정의한다. 아래 수식에서 $p(y)$는 pdf이다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y] = -\int_{-\infty}^{\infty} p(y) \log_2 p(y) \; dy \end{align}</script><ul><li>로그의 밑(base)이 2로 정의된 것은 정보통신과 관련을 가지는 역사적인 이유 때문이다. 엔트로피 계산에서 $p(y)\;=\;0$인 경우에는 로그값이 정의되지 않으므로 다음과 같은 극한값을 사용한다.</li></ul><script type="math/tex; mode=display">\begin{align} \lim_{p\rightarrow 0} \; p\log_2{p} = 0 \end{align}</script><ul><li>이 값은 로피탈의 정리에서 구할 수 있다. 위에서 예를 든 $Y_{1}, Y_{2}, Y_{3}$ 3개의 이산확률분포에 대해 엔트로피를 구하면 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y_1] = -\dfrac{1}{2} \log_2 \dfrac{1}{2} -\dfrac{1}{2} \log_2 \dfrac{1}{2} = 1 \end{align}</script><script type="math/tex; mode=display">\begin{align} H[Y_2] = -\dfrac{8}{10} \log_2 \dfrac{8}{10} -\dfrac{2}{10} \log_2 \dfrac{2}{10} \approx 0.72 \end{align}</script><script type="math/tex; mode=display">\begin{align} H[Y_3] = -1 \log_2 1 -0 \log_2 0 = 0 \end{align}</script><ul><li>다음은 Numpy로 엔트로피를 계산한 결과다. 확률값이 0일 때는 가장 작은 값인 <code>eps</code>를 대신 사용한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-0.5 * np.log2(0.5) - 0.5 * np.log2(0.5)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-0.8 * np.log2(0.8) - 0.2 * np.log2(0.2)</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7219280948873623</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">eps = np.finfo(<span class="built_in">float</span>).eps</span><br><span class="line">-1 * np.log2(1) - eps * np.log2(eps)</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.1546319456101628e-14</span><br></pre></td></tr></table></figure><blockquote><p>연습문제) 베르누이분포에서 확률값 $P(Y=1)$은 0부터 1까지의 값을 가질 수 있다. 각각의 값에 대해 엔트로피를 계산하여 가로축이 P(Y=1)이고 세로축이 H(Y)인 그래프를 그려라.</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.font_manager as fm</span><br><span class="line"></span><br><span class="line">path = <span class="string">'/Library/Fonts/NanumGothic.ttf'</span></span><br><span class="line">font_name = fm.FontProperties(fname=path, size=50).get_name()</span><br><span class="line">plt.rc(<span class="string">'font'</span>, family=font_name)</span><br><span class="line"></span><br><span class="line">P_Y = np.linspace(0, 1, 100)</span><br><span class="line">ls=[]</span><br><span class="line"><span class="keyword">for</span> p_y <span class="keyword">in</span> P_Y:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (p_y != 0) and (1-p_y != 0):</span><br><span class="line">        ls.append(- p_y * np.log2(p_y) -(1 - p_y) * np.log2(1-p_y))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (p_y == 0) and (1 - p_y == 1):</span><br><span class="line">        p_y = np.finfo(<span class="built_in">float</span>).eps</span><br><span class="line">        ls.append(- p_y * np.log2(p_y) -(1 - p_y) * np.log2(1-p_y))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (p_y == 1) and (1 - p_y == 0):</span><br><span class="line">        ls.append(- p_y * np.log2(p_y) -(np.finfo(<span class="built_in">float</span>).eps) * np.log2(np.finfo(<span class="built_in">float</span>).eps))</span><br><span class="line"></span><br><span class="line">plt.plot(P_Y, ls, <span class="string">"-"</span>, label=<span class="string">"엔트로피"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"엔트로피"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"베르누이 분포의 모수"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/bernoulli_distribution_parameters_entropy.png" alt="베르누이 분포의 모수에 따른 엔트로피"></p><p><img src="/image/ENTROPY_dost_plot.png" alt="엔트로피의 직관적 해석"></p><blockquote><p>다음 확률분포의 엔트로피를 계산하라.</p></blockquote><script type="math/tex; mode=display">\begin{align} H[Y] = -\sum_{k=1}^K p(y_k) \log_2 p(y_k) \end{align}</script><ul><li>엔트로피의 정의에따라 풀면 다음과 같다.</li></ul><script type="math/tex; mode=display">(1) P(Y=0)=\dfrac{1}{8}, P(Y=1)=\dfrac{1}{8}, P(Y=2)=\dfrac{1}{4}, P(Y=3)=\dfrac{1}{2}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- 1/8 * np.log2(1/8) -(1/8) * np.log2(1/8) -(2/8) * np.log2(2/8) -(4/8) * np.log2(4/8)</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.75</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">(2) P(Y=0)=1, P(Y=1)=0, P(Y=2)=0, P(Y=3)=0</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- 1 * np.log2(1) -(np.finfo(<span class="built_in">float</span>).eps) * np.log2(np.finfo(<span class="built_in">float</span>).eps) -(np.finfo(<span class="built_in">float</span>).eps) * np.log2(np.finfo(<span class="built_in">float</span>).eps) -(np.finfo(<span class="built_in">float</span>).eps) * np.log2(np.finfo(<span class="built_in">float</span>).eps)</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3.4638958368304884e-14</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">(3) P(Y=0)=\dfrac{1}{4}, P(Y=1)=\dfrac{1}{4}, P(Y=2)=\dfrac{1}{4}, P(Y=3)=\dfrac{1}{4}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- 1/4 * np.log2(1/4) -(1/4) * np.log2(1/4) -(1/4) * np.log2(1/4) -(1/4) * np.log2(1/4)</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2.0</span><br></pre></td></tr></table></figure><h3 id="엔트로피의-성질"><a href="#엔트로피의-성질" class="headerlink" title="엔트로피의 성질"></a>엔트로피의 성질</h3><ul><li>확률변수가 결정론적이면 확률분포에서 특정한 하나의 값이 나올 확률이 1이다. 이 때 엔트로피는 0이 되고 이 값은 엔트로피가 가질 수 있는 최솟값이다. 반대로 엔트로피의 최대값은 이산 확률변수의 클래스의 갯수에 따라 달라진다. 만약 이산확률분포가 가질 수 있는 값이 $2^{K}$개면 엔트로피의 최대값은 각 값에 대한 확률이 모두 같은 값인 $\frac{1}/{2^{K}}$이다. 엔트로피의 값은 아래의 수식과 같을 것이다.</li></ul><script type="math/tex; mode=display">\begin{align} H = -2^K \cdot \frac{1}{2^K}\log_2\dfrac{1}{2^K} = K \end{align}</script><h3 id="엔트로피의-추정"><a href="#엔트로피의-추정" class="headerlink" title="엔트로피의 추정"></a>엔트로피의 추정</h3><ul><li>이론적인 확률밀도함수가 없고 실제 데이터가 주어진 경우에는 데이터에서 확률질량함수를 추정한 후, 이를 기반으로 엔트로피를 계산한다. 예를 들어 데이터가 모두 80개가 있고 그 중 $Y=0$인 데이터가 40개, $Y=1$ 데이터가 40개 있는 경우는 엔트로피가 1이다.</li></ul><script type="math/tex; mode=display">\begin{align} P(y=0) = \dfrac{40}{80} = \dfrac{1}{2} \end{align}</script><script type="math/tex; mode=display">\begin{align} P(y=1) = \dfrac{40}{80} = \dfrac{1}{2} \end{align}</script><script type="math/tex; mode=display">\begin{align} H[Y] = -\dfrac{1}{2}\log_2\left(\dfrac{1}{2}\right) -\dfrac{1}{2}\log_2\left(\dfrac{1}{2}\right) = \dfrac{1}{2} + \dfrac{1}{2}  = 1 \end{align}</script><ul><li>Scipy의 stats 서브패키즈는 엔트로피를 구하는 <code>entropy</code>함수를 제공한다. <code>base</code>인수값은 2가 되어야 한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = [0.5, 0.5]</span><br><span class="line">sp.stats.entropy(p, base=2)</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure><blockquote><p>연습 문제)</p></blockquote><ul><li>(1) 데이터가 모두 60개가 있고 그 중 $Y=0$인 데이터가 20개, $Y=1$인 데이터가 40개 있는 경우의 엔트로피를 계산하라.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = [1/3, 2/3]</span><br><span class="line">sp.stats.entropy(p, base=2)</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9182958340544894</span><br></pre></td></tr></table></figure><ul><li>(2) 데이터가 모두 40개가 있고 그 중 $Y=0$인 데이터가 30개, $Y=1$인 데이터가 10개 있는 경우의 엔트로피를 계산하라.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = [3/4, 1/4]</span><br><span class="line">sp.stats.entropy(p, base=2)</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.8112781244591328</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = [1, 0]</span><br><span class="line">sp.stats.entropy(p, base=2)</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.0</span><br></pre></td></tr></table></figure><h3 id="가변길이-인코딩"><a href="#가변길이-인코딩" class="headerlink" title="가변길이 인코딩"></a>가변길이 인코딩</h3><ul><li>엔트로피는 원래 통신 분야에서 데이터가 가지고 있는 정보량을 계산하기 위해 고안되었다. 예를 들어 4개의 글자 A, B, C, D로 씌여진 다음과 같은 문서가 있다고 하자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">N = 200</span><br><span class="line">p = [1/2, 1/4, 1/8, 1/8]</span><br><span class="line">doc0 = list(<span class="string">""</span>.join([int(N * p[i]) * c <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(<span class="string">"ABCD"</span>)]))</span><br><span class="line">np.random.shuffle(doc0)</span><br><span class="line">doc = <span class="string">""</span>.join(doc0)</span><br><span class="line">doc</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'BDABABACBABBAACADAADAAADAAAAAABBAABADAAAAABBACAABACBBACDBAAACBCABBAABAAAAADDBABCBDBBDDBAABBBADCACAADAADCABADCAAAAACADBAABABCBAACAAABCDAADDCCCAAABABBDACACAAAAAABABBADABBABDBADBACAABDCAAABAAABACCDABAABA'</span></span><br></pre></td></tr></table></figure><ul><li><p>이 문서를 0과 1로 이루어진 이진수로 변환해야 하면 보통 다음처럼 인코딩한다.</p><ul><li>A=”00”</li><li>A=”01”</li><li>A=”10”</li><li>A=”11”</li></ul></li><li><p>이렇게 인코딩을 하면 200글자로 이루어진 문서는 이진수 400개가 된다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">encoder = &#123;<span class="string">"A"</span>: <span class="string">"00"</span>, <span class="string">"B"</span>: <span class="string">"01"</span>, <span class="string">"C"</span>: <span class="string">"10"</span>, <span class="string">"D"</span>: <span class="string">"11"</span>&#125;</span><br><span class="line">encoded_doc = <span class="string">""</span>.join([encoder[c] <span class="keyword">for</span> c <span class="keyword">in</span> doc])</span><br><span class="line">encoded_doc</span><br></pre></td></tr></table></figure><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'0111000100010010010001010000100011000011000000110000000000000101000001001100000000000101001000000100100101001011010000001001100001010000010000000000111101000110011101011111010000010101001110001000001100001110000100111000000000001000110100000100011001000010000000011011000011111010100000000100010111001000100000000000000100010100110001010001110100110100100000011110000000010000000100101011000100000100'</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(encoded_doc)</span><br></pre></td></tr></table></figure><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">400</span><br></pre></td></tr></table></figure><ul><li>그런데 이진수로 변환할 때 더 글자수를 줄일 수 있는 방법이 있다. 우선 위 글자의 분포를 조사하자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sns.countplot(list(doc), order=<span class="string">"ABCD"</span>)</span><br><span class="line">plt.title(<span class="string">"글자수의 분포"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/count_plot_with_characters_count.png" alt="글자수의 분포"></p><ul><li>글자수의 분포가 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} P(Y=A)=\dfrac{1}{2}, P(Y=B)=\dfrac{1}{4}, P(Y=C)=\dfrac{1}{8}, P(Y=D)=\dfrac{1}{8} \end{align}</script><ul><li><p>지프의 법칙(Zipf’s law)에 따르면 이러한 분포는 현실의 글자 빈도수에서도 흔히 나타난다. 확률분포가 위와 같을 때는 다음처럼 인코딩하면 인코딩된 후의 이진수 수를 줄일 수 있다.</p><ul><li>A=”0”</li><li>B=”10”</li><li>C=”110”</li><li>D=”111”</li></ul></li><li><p>이 방법은 글자마다 인코딩하는 이진수의 숫자가 다르기 때문에 가변길이 인코딩(variable length encoding)이라고 한다. 가장 많이 출현하는 ‘A’는 두 글자가 아닌 한 글자이므로 인코딩 후의 이진수 수가 감소한다. 반대로 ‘C’, ‘D’는 이진수의 수가 3개로 많지만 글자의 빈도가 적어서 영향이 적다.</p></li></ul><ul><li>만약 문서의 분포가 위에서 가정한 분포와 정확하게 같다면 인코딩된 이진수의 숫자는 다음 계산에서 350개가 됨을 알 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} \left(200 \times \dfrac{1}{2}\right) \cdot 1 + \left(200 \times \dfrac{1}{4}\right) \cdot 2 + \left(200 \times \dfrac{1}{8}\right) \cdot 3 + \left(200 \times \dfrac{1}{8}\right) \cdot 3 = 350 \end{align}</script><ul><li>따라서 알파벳 한 글자를 인코딩하는데 필요한 평균 비트(bit)수는 $350 \div 200 = 1.75$이고 이 값은 확률변수의 엔트로피 값과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} H = -\dfrac{1}{2}\log_2\dfrac{1}{2} -\dfrac{1}{4}\log_2\dfrac{1}{4} -\dfrac{2}{8}\log_2\dfrac{1}{8} = 1.75 \end{align}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vl_encoder = &#123;<span class="string">"A"</span>: <span class="string">"0"</span>, <span class="string">"B"</span>: <span class="string">"10"</span>, <span class="string">"C"</span>: <span class="string">"110"</span>, <span class="string">"D"</span>: <span class="string">"111"</span>&#125;</span><br><span class="line">vl_encoded_doc = <span class="string">""</span>.join([vl_encoder[c] <span class="keyword">for</span> c <span class="keyword">in</span> doc])</span><br><span class="line">vl_encoded_doc</span><br></pre></td></tr></table></figure><h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'10111010010011010010100011001110011100011100000010100010011100000101001100010011010100110111100001101011001010001000000111111100101101011110101111111000101010011111001100011100111110010011111000000110011110001001011010001100001011011100111111110110110000100101011101100110000000100101001110101001011110011110011000101111100001000010011011011101000100'</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(vl_encoded_doc)</span><br></pre></td></tr></table></figure><h5 id="결과-14"><a href="#결과-14" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">350</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sp.stats.entropy([1/2, 1/4, 1/8, 1/8], base=2)</span><br></pre></td></tr></table></figure><h5 id="결과-15"><a href="#결과-15" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.75</span><br></pre></td></tr></table></figure><h3 id="지니-불순도"><a href="#지니-불순도" class="headerlink" title="지니 불순도"></a>지니 불순도</h3><ul><li>엔트로피와 유사한 개념으로 지니불순도(Gini impurity)라는 것이 있다. 지니불순도는 엔트로피처럼 확률분포가 어느쪽에 치우쳐져있는가를 재는 척도지만 로그를 사용하지 않으므로 계산량이 더 적어 엔트로피 대용으로 많이 사용된다. 경젠학에서도 사용되지만 지니계수(Gini coefficient)와는 다른 개념이라는 점에 주의해야 한다.</li></ul><script type="math/tex; mode=display">\begin{align} G[Y] = \sum_{k=1}^K P(y_k) (1 - P(y_k)) \end{align}</script><ul><li>다음 그림은 값이 두 개인 이산확률분포에서 지니불순도와 엔트로피를 비교한 결과이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">P0 = np.linspace(0.001, 1 - 0.001, 1000)</span><br><span class="line">P1 = 1 - P0</span><br><span class="line">H = - P0 * np.log2(P0) - P1 * np.log2(P1)</span><br><span class="line">G = 2 * (P0 * (1 - P0) + P1 * (1 - P1))</span><br><span class="line"></span><br><span class="line">plt.plot(P1, H, <span class="string">"-"</span>, label=<span class="string">"엔트로피"</span>)</span><br><span class="line">plt.plot(P1, G, <span class="string">"--"</span>, label=<span class="string">"지니불순도"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">"P(Y=1)"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/difference_with_Gini_and_entropy.png" alt="엔트로피와 지니 불순도 비교"></p><h3 id="엔트로피-최대화"><a href="#엔트로피-최대화" class="headerlink" title="엔트로피 최대화"></a>엔트로피 최대화</h3><ul><li><p>기대값 $0$, 분산 $\sigma^{2}$이 주어졌을 때 엔트로피 $\text{H}[p(x)]$를 가장 크게 만드는 확률밀도함수 $p(x)$는 정규분포가 된다. 이는 아래와 같이 증명한다. 우선 확률 밀도함수가 지켜야 할 제한조건은 다음과 같다.</p></li><li><p>(1) 확률밀도함수의 총면적은 1</p></li></ul><script type="math/tex; mode=display">\begin{align} \int_{-\infty}^{\infty} p(x) dx = 1 \end{align}</script><ul><li>(2) 기댓값(평균)은 0</li></ul><script type="math/tex; mode=display">\begin{align} \int_{-\infty}^{\infty} xp(x) dx = 0 \end{align}</script><ul><li>(3) 분산은 $\sigma^{2}$</li></ul><script type="math/tex; mode=display">\begin{align} \int_{-\infty}^{\infty} x^2 p(x) dx = \sigma^2 \end{align}</script><ul><li>최대화할 목적범함수(objective functional)은 엔트로피이다.</li></ul><script type="math/tex; mode=display">\begin{align} \text{H}[p(x)] = -\int_{-\infty}^{\infty} p(x)\log p(x) dx \end{align}</script><ul><li>라그랑주 승수법으로 제한조건을 추가하면 다음과 같아진다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \text{H}[p(x)] &= -\int_{-\infty}^{\infty} p(x)\log p(x) dx + \lambda_1 \left( \int_{-\infty}^{\infty} p(x) dx - 1 \right) \\ & + \lambda_2 \left( \int_{-\infty}^{\infty} xp(x) dx\right) + \lambda_3 \left( \int_{-\infty}^{\infty} x^2 p(x) dx - \sigma^2 \right) \\ &= \int_{-\infty}^{\infty} \left(-p(x)\log p(x) + \lambda_1 p(x) + \lambda_2 xp(x) + \lambda_3 x^2p(x) - \lambda_1 - \lambda_3 \sigma^2 \right) dx \end{aligned} \end{align}</script><ul><li>변분법에서 도함수는 다음과 같이 계산된다.</li></ul><p><a href="https://datascienceschool.net/view-notebook/006a077ee4964eb58c6b08b213126f8f/" target="_blank" rel="noopener">참고 : 변분법</a></p><script type="math/tex; mode=display">\begin{align} \dfrac{\delta H}{\delta p(x)} = -\log p(x) - 1 + \lambda_1 + \lambda_2 x + \lambda_3 x^2 = 0 \end{align}</script><ul><li>따라서 확률밀도함수의 형태는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} p(x) = \exp \left( - 1 + \lambda_1 + \lambda_2 x + \lambda_3 x^2 \right) \end{align}</script><ul><li>적분을 통해 위 형태의 확률밀도함수의 면적, 기대값, 분산을 계산하고 주어진 제한조건을 만족하도록 연립방정식을 풀면 라그랑주 승수를 다음처럼 구할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \lambda_1 &= 1-\dfrac{1}{2} \log{2\pi\sigma^2} \\ \lambda_2 &= 0 \\ \lambda_3 &= -\dfrac{1}{2\sigma^2} \\ \end{aligned} \end{align}</script><ul><li>이 값을 대입하면 정규분포라는 것을 알 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} p(x) = \dfrac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\dfrac{x^2}{2\sigma^2} \right) \end{align}</script><ul><li>따라서 정규분포는 기댓값과 표준편차를 알고있는 확률분포 중에서 가장 엔트로피가 크고 따라서 가장 정보가 적은 확률분포이다. <code>정규분포는 베이즈 추정에 있어서 사실상의 무정보 사전확률분포로 사용되는 경우가 많다.</code></li></ul><h3 id="의사결정나무를-사용한-분류예측"><a href="#의사결정나무를-사용한-분류예측" class="headerlink" title="의사결정나무를 사용한 분류예측"></a>의사결정나무를 사용한 분류예측</h3><ul><li>의사결정나무에 전체 트레이닝 데이터를 모두 적용해 보면 각 데이터는 특정한 노드를 타고 내려가게 된다. 각 노드는 그 노드를 선택한 데이터 집합을 갖는다. 이 때 노드에 속한 데이터의 클래스의 비율을 구하여 이를 그 노드의 조건부 확률분포 $P(Y\;=\;k|X)_{node}$라고 정의한다.</li></ul><script type="math/tex; mode=display">P(Y=k|X)_{\text{node}} \approx \dfrac{N_{\text{node},k}}{N_{\text{node}}}</script><ul><li>테스트 데이터 $X_{test}$의 클래스를 예측할 때는 가장 상위의 노드부터 분류 규칙을 차례대로 적용하여 마지막에 도달하는 노드의 조건부 확률 분포를 이용하여 클래스를 예측한다.</li></ul><script type="math/tex; mode=display">\hat{Y} = \text{arg}\max_k P(Y=k|X_{\text{test}})_{\text{last node}}</script><h3 id="분류-규칙을-정하는-방법"><a href="#분류-규칙을-정하는-방법" class="headerlink" title="분류 규칙을 정하는 방법"></a>분류 규칙을 정하는 방법</h3><ul><li>분류 규칙을 정하는 방법은 부모 노드와 자식 노드간의 엔트로피를 가장 낮게 만드는 최상의 독립 변수와 기준값을 찾는 것이다. 이러한 기준을 정량화한 것이 정보획득량(Information Gain)이다. 기본적으로 모든 독립변수와 모든 가능한 기준값에 대해 정보획득량을 구하여 가장 정보획들량이 큰 독립 변수와 기준값을 선택한다.</li></ul><h3 id="Information-Gain"><a href="#Information-Gain" class="headerlink" title="Information Gain"></a>Information Gain</h3><ul><li><p>데이터 세트의 균일도는 데이터를 구분하는 데 필요한 정보의 양에 영향을 미친다. 결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만든다. 즉, 정보 균일도가 데이터 세트로 쪼개질 수 있도록 조건을 찾아 서브 데이터 세트를 만들고, 다시 이 서브 데이터 세트에서 균일도가 높은 자식 데이터 세트로 쪼개는 방식을 자식 트로 내려가면서 반복하는 방식으로 데이터 값을 예측하게 된다. 이러한 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한  Information Gain과 Gini 계수가 있다. 즉, <code>(이전 엔트로피 - 이후 엔트로피)의 차가 많이 나는 규칙부터 실행하여 균일도를 높이는 방식</code>이다. 규칙노드를 통해 이전 엔트로피와의 차이가 클수록 유의미한 규칙이 될 것이다.</p></li><li><p>정보획득량(Information Gain)는 $X$라는 조건에 의해 확률 변수 $Y$의 엔트로피가 얼마나 감소하였는가를 나타내는 값이다. 다음처럼 $Y$의 엔트로피에서 $X$에 대한 $Y$의 조건부 엔트로피를 뺀 값으로 정의된다.</p></li></ul><script type="math/tex; mode=display">IG[Y,X] = H[Y] - H[Y|X]</script><p><img src="/image/information_Gain_concept.png" alt="Information Gain의 개념"></p><h3 id="결합-엔트로피"><a href="#결합-엔트로피" class="headerlink" title="결합 엔트로피"></a>결합 엔트로피</h3><ul><li>결합엔트로피(joint entropy)는 결합확률분포를 사용하여 정의한 엔트로피를 말한다. 이산 확률변수 $X,\;Y$에 대해 결합엔트로피는 다음 처럼 정의한다. 아래 식에서 $K_{X}, K_{Y}$는 각각 $X$와 $Y$가 가질 수 있는 값의 개수이고, $p$는 결합 확률질량함수이다.</li></ul><script type="math/tex; mode=display">\begin{align} H[X, Y] = - \sum_{i=1}^{K_X} \sum_{j=1}^{K_Y} \,p(x_i, y_j) \log_2 p(x_i, y_j) \end{align}</script><ul><li>연속확률변수 $X,\;Y$에 대한 결합엔트로피는 다음처럼 정의한다. 아래 식에서 $p$는 결합 확률밀도함수이다.</li></ul><script type="math/tex; mode=display">\begin{align} H[X, Y] = - \int_{x} \int_{y} \,p(x, y) \log_2 p(x, y)  \; dxdy \end{align}</script><ul><li>결합엔트로피도 결합확률분포라는 점만 제외하면 일반적인 엔트로피와 같다. 모든 경우에 대해 골고루 확률이 분포되어 있으면 엔트로피값이 커지고 특정한 한 가지 경우에 대해 확률이 모여있으면 엔트로피가 0에 가까워진다.</li></ul><h3 id="조건부-엔트로피"><a href="#조건부-엔트로피" class="headerlink" title="조건부 엔트로피"></a>조건부 엔트로피</h3><ul><li>조건부 엔트로피(conditional entropy)는 어떤 확률 변수 $X$가 다른 확률변수 $Y$의 값을 예측하는데 도움이 되는지를 측정하는 방법 중의 하나이다. 만약 확률변수 X의 값이 어떤 특정한 하나의 값을 가질 때 확률변수 $Y$도 마찬가지로 특정한 값이 된다면 $X$로 $Y$를 예측할 수 있다. 반대로 확률변수 $X$의 값이 어떤 특정한 하나의 값을 가져도 확률변수 $Y$가 여러 값으로 골고루 분포되어 있다면 $X$는 $Y$의 값을 예측하는데 도움이 안된다.</li></ul><ul><li>조건부 엔트로피의 정의는 다음과 같이 유도한다. 확률변수 $X,\;Y$가 모두 이산확률변수라고 가정하고 $X$가 특정한 값 $x_{i]}$를 가질 떄의 $Y$의 엔트로피 $H[Y \mid X=x_i]$는 다음처럼 조건부확률분포의 엔트로피로 정의한다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y \mid X=x_i] = - \sum_{j=1}^{K_Y} p(y_j \mid x_i)  \log_2 p(y_j \mid x_i) \end{align}</script><ul><li>조건부 엔트로피는 확률변수 $X$가 가질 수 있는 모든 경우에 대해 $H[Y \mid X=x_i]$를 가중평균한 값으로 정의한다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y \mid X] &= \sum_{i=1}^{K_X} \,p(x_i)\,H[Y \mid X=x_i]  \\ &= - \sum_{i=1}^{K_X} \sum_{j=1}^{K_Y} p(y_j \mid x_i)p(x_i)  \log_2 p(y_j \mid x_i)  \\ &= - \sum_{i=1}^{K_X} \sum_{j=1}^{K_Y} p(x_i, y_j)  \log_2 p(y_j \mid x_i)  \\ \end{align}</script><ul><li>연속확률변수의 경우에는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y \mid X=x] = - \int_{y} p(y \mid x)  \log_2 p(y \mid x)\; dy \end{align}</script><script type="math/tex; mode=display">\begin{align} H[Y \mid X] &= - \int_{x} \,p(x) \,H[Y \mid X=x] \; dx \\ &= - \int_{x} p(x) \left( \int_{y} p(y \mid x)  \log_2 p(y \mid x)\; dy \right) \; dx   \\ &= - \int_{x} \int_{y} p(y \mid x_i) p(x) \log_2 p(y \mid x) \; dxdy \\ &= - \int_{x} \int_{y} \,p(x, y) \log_2 p(y \mid x) \; dxdy \\ \end{align}</script><ul><li>따라서 조건부엔트로피의 최종적인 수학적 정의는 다음과 같다.</li></ul><ul><li>이산확률변수의 경우에는 다음과 같이 정의한다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y \mid X] = - \sum_{i=1}^{K_X} \sum_{j=1}^{K_Y} \,p(x_i, y_j) \log_2 p(y_j \mid x_i) \end{align}</script><ul><li>연속확률변수의 경우에는 다음과 같이 정의한다.</li></ul><script type="math/tex; mode=display">\begin{align} H[Y \mid X] = - \int_{x} \int_{y} \,p(x, y) \log_2 p(y \mid x)  \; dxdy \end{align}</script><ul><li>다시 돌아와 Decision Tree를 이야기하자면 Decision Tree의 일반적인 알고리즘은 데이터 세트를 분할하는 데 가장 좋은 조건, 즉 Information Gain이나 Gini coefficient가 높은 조건을 찾아서 자식 트리 노등에 걸쳐 반복적으로 분할한 뒤, 데이터가 모두 특정 분류에 속하게 되면 분할을 멈추고 분류를 결정한다.</li></ul><p><img src="/image/how_to_calculate_information_gain_01.png" alt="Information gain 계산 방법"></p><p><img src="/image/how_to_calculate_information_gain.png" alt="Information gatin 계산 예시"></p><p><img src="/image/Decision_tree_using_information_gain.png" alt="Inforamtion gain을 이용한 Decision Tree 분류"></p><p><img src="/image/how_to_split_node_next_step_using_information_gain.png" alt="Decision Tree의 분할과정"></p><p><img src="/image/decision_tree_process_cal.png" alt="Decision Tree의 분할"></p><h3 id="Information-Gain-예시"><a href="#Information-Gain-예시" class="headerlink" title="Information Gain 예시"></a>Information Gain 예시</h3><p><img src="/image/conditional_entropy_calculate_example.png" alt="정보획득량 예시"></p><ul><li>예를 들어 A,B 두 가지의 다른 분류 규칙을 적용했더니 위에서 처럼 서로 다르게 데이터가 나뉘어 졌다고 가정하자.</li></ul><ul><li>A방법과 B방법 모두 노드 분리전에는 Y=0인 데이터의 수와 Y=1인 데이터의 수가 모두 40개였다.</li></ul><ul><li>A방법으로 노드를 분리하면 다음과 같은 두 개의 자식 노드가 생긴다.<ul><li>자식 노드 A1은 Y=0인 데이터가 30개, Y=1인 데이터가 10개</li><li>자식 노드 A2은 Y=0인 데이터가 10개, Y=1인 데이터가 30개</li></ul></li></ul><ul><li>B방법으로 노드를 분리하면 다음과 같은 두 개의 자식 노드가 생긴다.<ul><li>자식 노드 B1은 Y=0인 데이터가 20개, Y=1인 데이터가 40개</li><li>자식 노드 B2은 Y=0인 데이터가 20개, Y=1인 데이터가 30개</li></ul></li></ul><ul><li>우선 부모 노드의 엔트로피를 계산하면 다음과 같다.</li></ul><script type="math/tex; mode=display">H[Y] = -\dfrac{1}{2}\log_2\left(\dfrac{1}{2}\right) -\dfrac{1}{2}\log_2\left(\dfrac{1}{2}\right) = \dfrac{1}{2} + \dfrac{1}{2}  = 1</script><ul><li>A 방법에 대해 IG를 계산하면 다음과 같다.</li></ul><script type="math/tex; mode=display">H[Y|X=X_1] = -\dfrac{3}{4}\log_2\left(\dfrac{3}{4}\right) -\dfrac{1}{4}\log_2\left(\dfrac{1}{4}\right) = 0.81</script><script type="math/tex; mode=display">H[Y|X=X_2] = -\dfrac{1}{4}\log_2\left(\dfrac{1}{4}\right)  -\dfrac{3}{4}\log_2\left(\dfrac{3}{4}\right) = 0.81</script><script type="math/tex; mode=display">H[Y|X] = \dfrac{1}{2} H[Y|X=X_1] + \dfrac{1}{2} H[Y|X=X_2] = 0.81</script><script type="math/tex; mode=display">IG = H[Y] - H[Y|X] = 0.19</script><ul><li>B 방법에 대해 IG를 계산하면 다음과 같다.</li></ul><script type="math/tex; mode=display">H[Y|X=X_1] = -\dfrac{1}{3}\log_2\left(\dfrac{1}{3}\right) - \dfrac{2}{3}\log_2\left(\dfrac{2}{3}\right) = 0.92</script><script type="math/tex; mode=display">H[Y|X=X_2] = 0</script><script type="math/tex; mode=display">H[Y|X] = \dfrac{3}{4} H[Y|X=X_1] + \dfrac{1}{4} H[Y|X=X_2] = 0.69</script><script type="math/tex; mode=display">IG = H[D] - H[Y|X] = 0.31</script><ul><li><code>따라서 B 방법이 더 나은 방법임을 알 수 있다.</code></li></ul><ul><li>위와 같이 model을 fitting하였으면, 이제 어떻게 해당 영역에 예측할 데이터가 포함된다면 어떻게 클래스를 정하는지에 대해서 알아볼 것이다. 간단히 말하자면 해당 영역에 포함된 클래스의 개수가 많은 것으로 예측한다.</li></ul><p><img src="/image/how_to_precidct_data_class_01.png" alt="Decision tree의 predict - 01"></p><p><img src="/image/how_to_precidct_data_class_02.png" alt="Decision tree의 predict - 01"></p><p><img src="/image/how_to_precidct_data_class_03.png" alt="Decision tree의 predict - 01"></p><h3 id="Decision-Tree의-특징"><a href="#Decision-Tree의-특징" class="headerlink" title="Decision Tree의 특징"></a>Decision Tree의 특징</h3><ul><li>결정 트리의 가장 큰 장점은 정보의 균일도라는 률을 기반으로 하고 있어서 <code>알고리즘이 쉽고 직관적</code>이라는 점이다. 결정 트리가 룰이 매우 명확하고, 이에 기반해 어떻게 규칙 노드와 리프 노드가 만들어지는지 알 수 있고, 시각화로 표현까지 할 수 있다. <code>또한 정보의 균일도만 신경쓰면 되므로 특별한 경우를 제외하고는 각 feature의 스케일링과 normalization과 같은 작업이 크게 영향을 미치지 않는다.</code> 반면에 결정 트리 모델의 가장 큰 단점은 <code>과적합(overfitting)으로 정확도가 떨어진다는 점</code>이다. 복잡한 학습 모델은 결국에는 실제 상황(테스트 데이터)에 유연하게 대처할 수 없어서 예측 성능이 떨어질 수 밖에 없다. <code>트리의 크기를 사전에 제한하는 것이 오히려 성능 튜닝에 더 도움이 될 것</code>이다.</li></ul><h3 id="Scikit-Learn의-의사결정나무-클래스"><a href="#Scikit-Learn의-의사결정나무-클래스" class="headerlink" title="Scikit-Learn의 의사결정나무 클래스"></a>Scikit-Learn의 의사결정나무 클래스</h3><h2 id="Scikit-Learn에서-의사결정나무는-DecisionTreeClassifier클래스로-구현되어있다-여기에서는-붓꽃-분류-문제를-예를-들어-의사결정나무를-설명한다-이-예제에서는-독립변수-공간을-공간상에-표시하기-위해-꽃의-길이와-폭만을-독립변수로-사용하였다"><a href="#Scikit-Learn에서-의사결정나무는-DecisionTreeClassifier클래스로-구현되어있다-여기에서는-붓꽃-분류-문제를-예를-들어-의사결정나무를-설명한다-이-예제에서는-독립변수-공간을-공간상에-표시하기-위해-꽃의-길이와-폭만을-독립변수로-사용하였다" class="headerlink" title="- Scikit-Learn에서 의사결정나무는 DecisionTreeClassifier클래스로 구현되어있다. 여기에서는 붓꽃 분류 문제를 예를 들어 의사결정나무를 설명한다. 이 예제에서는 독립변수 공간을 공간상에 표시하기 위해 꽃의 길이와 폭만을 독립변수로 사용하였다."></a>- Scikit-Learn에서 의사결정나무는 <code>DecisionTreeClassifier</code>클래스로 구현되어있다. 여기에서는 붓꽃 분류 문제를 예를 들어 의사결정나무를 설명한다. 이 예제에서는 독립변수 공간을 공간상에 표시하기 위해 꽃의 길이와 폭만을 독립변수로 사용하였다.</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line"></span><br><span class="line">data = load_iris()</span><br><span class="line">y = data.target</span><br><span class="line">X = data.data[:, 2:]</span><br><span class="line">feature_names = data.feature_names[2:]</span><br><span class="line"></span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">tree1 = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, max_depth=1, random_state=0).fit(X, y)</span><br></pre></td></tr></table></figure><hr><ul><li>다음은 의사결정나무를 시각화하기 위한 코드이다. <code>draw_decision_tree</code>함수는 의사결정나무의 의사 결정 과정의 세부적인 내역을 다이어그램으로 보여주고 <code>plot_decision_regions</code>함수는 이러한 의사 결정에 의해 데이터의 영역이 어떻게 나뉘어졌는지를 시각화하여 보여준다.</li></ul><ul><li>아래 방법으로 draw_decision_tree함수를 동일하게 출력할 수 있다.<ul><li><code>filled</code>: 그래프에 각 클래스별로 색상을 입힘.</li><li><code>rounded</code>: 반올림 시켜주는 역할</li><li><code>special_characters</code>: 특수문자가 있을 경우 제외시켜줌.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dot_data=tree.export_graphviz(out_file=None, decision_tree=tree1,feature_names=iris.feature_names,</span><br><span class="line">                             class_names=iris.target_names,</span><br><span class="line">                             filled=True, rounded=True, special_characters=True)</span><br><span class="line">graphviz.Source(dot_data)</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">import io</span><br><span class="line">import pydot</span><br><span class="line">from IPython.core.display import Image</span><br><span class="line">from sklearn.tree import export_graphviz</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_decision_tree(model):</span><br><span class="line">    dot_buf = io.StringIO()</span><br><span class="line">    export_graphviz(model, out_file=dot_buf, feature_names=feature_names)</span><br><span class="line">    graph = pydot.graph_from_dot_data(dot_buf.getvalue())[0]</span><br><span class="line">    image = graph.create_png()</span><br><span class="line">    <span class="built_in">return</span> Image(image)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot_decision_regions(X, y, model, title):</span><br><span class="line">    resolution = 0.01</span><br><span class="line">    markers = (<span class="string">'s'</span>, <span class="string">'^'</span>, <span class="string">'o'</span>)</span><br><span class="line">    colors = (<span class="string">'red'</span>, <span class="string">'blue'</span>, <span class="string">'lightgreen'</span>)</span><br><span class="line">    cmap = mpl.colors.ListedColormap(colors)</span><br><span class="line"></span><br><span class="line">    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),</span><br><span class="line">                           np.arange(x2_min, x2_max, resolution))</span><br><span class="line">    Z = model.predict(</span><br><span class="line">        np.array([xx1.ravel(), xx2.ravel()]).T).reshape(xx1.shape)</span><br><span class="line"></span><br><span class="line">    plt.contour(xx1, xx2, Z, cmap=mpl.colors.ListedColormap([<span class="string">'k'</span>]))</span><br><span class="line">    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)</span><br><span class="line">    plt.xlim(xx1.min(), xx1.max())</span><br><span class="line">    plt.ylim(xx2.min(), xx2.max())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> idx, cl <span class="keyword">in</span> enumerate(np.unique(y)):</span><br><span class="line">        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], alpha=0.8,</span><br><span class="line">                    c=[cmap(idx)], marker=markers[idx], s=80, label=cl)</span><br><span class="line"></span><br><span class="line">    plt.xlabel(data.feature_names[2])</span><br><span class="line">    plt.ylabel(data.feature_names[3])</span><br><span class="line">    plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> Z</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_decision_tree(tree1)</span><br></pre></td></tr></table></figure><hr><h2 id><a href="#" class="headerlink" title></a><img src="/image/iris_data_first_divide_on_plot_01.png" alt="max_depth=1인 경우의 decision tree의 분할 - 01"></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_regions(X, y, tree1, <span class="string">"Depth 1"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><h2 id="-1"><a href="#-1" class="headerlink" title></a><img src="/image/iris_data_first_divide_on_plot_02.png" alt="max_depth=1인 경우의 decision tree의 분할 - 02"></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line"></span><br><span class="line">confusion_matrix(y, tree1.predict(X))</span><br></pre></td></tr></table></figure><h5 id="결과-16"><a href="#결과-16" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[50,  0,  0],</span><br><span class="line">       [ 0, 50,  0],</span><br><span class="line">       [ 0, 50,  0]])</span><br></pre></td></tr></table></figure><hr><h2 id="depth-2로-변경한-후의-결과"><a href="#depth-2로-변경한-후의-결과" class="headerlink" title="- depth=2로 변경한 후의 결과"></a>- depth=2로 변경한 후의 결과</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tree2 = DecisionTreeClassifier(</span><br><span class="line">    criterion=<span class="string">'entropy'</span>, max_depth=2, random_state=0).fit(X, y)</span><br></pre></td></tr></table></figure><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_decision_tree(tree2)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_03.png" alt="max_depth=2로 한 후의 분할 - 01"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_regions(X, y, tree2, <span class="string">"Depth 2"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_04.png" alt="max_depth=2로 한 후의 분할 - 02"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusion_matrix(y, tree2.predict(X))</span><br></pre></td></tr></table></figure><h5 id="결과-17"><a href="#결과-17" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[50,  0,  0],</span><br><span class="line">       [ 0, 49,  1],</span><br><span class="line">       [ 0,  5, 45]])</span><br></pre></td></tr></table></figure><hr><ul><li>max_depth=3으로 변경한 후의 결과</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tree3 = DecisionTreeClassifier(</span><br><span class="line">    criterion=<span class="string">'entropy'</span>, max_depth=3, random_state=0).fit(X, y)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_decision_tree(tree3)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_05.png" alt="max_depth=3로 한 후의 분할 - 01"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_regions(X, y, tree3, <span class="string">"Depth 3"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_06.png" alt="max_depth=3로 한 후의 분할 - 01"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusion_matrix(y, tree3.predict(X))</span><br></pre></td></tr></table></figure><h5 id="결과-18"><a href="#결과-18" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[50,  0,  0],</span><br><span class="line">       [ 0, 47,  3],</span><br><span class="line">       [ 0,  1, 49]])</span><br></pre></td></tr></table></figure><hr><ul><li>max_depth=5로 변경한 후 결과</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tree5 = DecisionTreeClassifier(</span><br><span class="line">    criterion=<span class="string">'entropy'</span>, max_depth=5, random_state=0).fit(X, y)</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">draw_decision_tree(tree5)</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_07.png" alt="max_depth=5로 한 후의 분할 - 01"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_decision_regions(X, y, tree5, <span class="string">"Depth 5"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><hr><p><img src="/image/iris_data_first_divide_on_plot_08.png" alt="max_depth=5로 한 후의 분할 - 02"></p><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusion_matrix(y, tree5.predict(X))</span><br></pre></td></tr></table></figure><h5 id="결과-19"><a href="#결과-19" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[50,  0,  0],</span><br><span class="line">       [ 0, 49,  1],</span><br><span class="line">       [ 0,  0, 50]])</span><br></pre></td></tr></table></figure><hr><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import classification_report</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classification_report(y, tree5.predict(X)))</span><br></pre></td></tr></table></figure><h5 id="결과-20"><a href="#결과-20" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       1.00      1.00      1.00        50</span><br><span class="line">           1       1.00      0.98      0.99        50</span><br><span class="line">           2       0.98      1.00      0.99        50</span><br><span class="line"></span><br><span class="line">    accuracy                           0.99       150</span><br><span class="line">   macro avg       0.99      0.99      0.99       150</span><br><span class="line">weighted avg       0.99      0.99      0.99       150</span><br></pre></td></tr></table></figure><hr><ul><li>교차검증을 통해 최종모형의 성능을 살펴보면 아래와 같다.</li></ul><hr><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import KFold, cross_val_score</span><br><span class="line"></span><br><span class="line">cv = KFold(5, shuffle=True, random_state=0)</span><br><span class="line">model = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, max_depth=5,</span><br><span class="line">                               random_state=0)</span><br><span class="line">cross_val_score(model, X, y, scoring=<span class="string">"accuracy"</span>, cv=cv).mean()</span><br></pre></td></tr></table></figure><h5 id="결과-21"><a href="#결과-21" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.9466666666666667</span><br></pre></td></tr></table></figure><hr><h3 id="Regression-tree"><a href="#Regression-tree" class="headerlink" title="Regression tree"></a>Regression tree</h3><ul><li>regression tree는 종속변수가 연속형인 것만 제외했을땐 아래에서 보는 것과 같이 동일한 개념을 가진다.</li></ul><p><img src="/image/Regression_Tree_concept_01.png" alt="Regression Tree의 개념"></p><ul><li>Regression Tree는 <code>예측시에 각각의 영역에 대해 특정 실수값을 주는 방식</code>이다. 아래 수식에서 $c_{m}$은 오른쪽 그림에서와 같이 해당 영역의 높이라고 할 수 있다.</li></ul><p><img src="/image/Regression_Tree_concept_02.png" alt="Regression Tree 시각적으로 이해하기"></p><ul><li>classification의 경우에는 entropy로 정했지만, Regression의 경우에는 아래 2번째 수식에서 볼 수 있듯이 기본적인 회귀의 성능지표를 사용하여 변수를 선택하게 된다.</li></ul><p><img src="/image/Regression_Tree_concept_03.png" alt="Regression Tree 변수 선택방법"></p><ul><li>이렇게 결정된 영역에 속하는 예측값은 해당 영역의 평균값을 출력해주며, 영역별로(층별로) 존재한다.</li></ul><p><img src="/image/Regression_Tree_concept_04.png" alt="Regression Tree 예측"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line">rng = np.random.RandomState(1)</span><br><span class="line">X = np.sort(5 * rng.rand(80, 1), axis=0)</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line">y[::5] += 3 * (0.5 - rng.rand(16))</span><br><span class="line"></span><br><span class="line">regtree = DecisionTreeRegressor(max_depth=3)</span><br><span class="line">regtree.fit(X, y)</span><br><span class="line"></span><br><span class="line">X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]</span><br><span class="line">y_hat = regtree.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the results</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, s=20, edgecolor=<span class="string">"black"</span>, c=<span class="string">"darkorange"</span>, label=<span class="string">"데이터"</span>)</span><br><span class="line">plt.plot(X_test, y_hat, color=<span class="string">"cornflowerblue"</span>, linewidth=2, label=<span class="string">"예측"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x"</span>)</span><br><span class="line">plt.ylabel(r<span class="string">"<span class="variable">$y</span>$ &amp; $\hat&#123;y&#125;$"</span>)</span><br><span class="line">plt.title(<span class="string">"회귀 나무"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/Regression_tree_plot.png" alt="회귀나무"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/26/machine_learning_13/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Support Vector Machine(SVM) - 02</title>
      <link>https://heung-bae-lee.github.io/2020/04/25/machine_learning_12/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/25/machine_learning_12/</guid>
      <pubDate>Sat, 25 Apr 2020 11:03:51 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;커널-서포트-벡터-머신-SVM의-심화적-이해&quot;&gt;&lt;a href=&quot;#커널-서포트-벡터-머신-SVM의-심화적-이해&quot; class=&quot;headerlink&quot; title=&quot;커널 서포트 벡터 머신 - SVM의 심화적 이해&quot;&gt;&lt;/a&gt;커널 서포트 벡터 머신
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="커널-서포트-벡터-머신-SVM의-심화적-이해"><a href="#커널-서포트-벡터-머신-SVM의-심화적-이해" class="headerlink" title="커널 서포트 벡터 머신 - SVM의 심화적 이해"></a>커널 서포트 벡터 머신 - SVM의 심화적 이해</h1><p><img src="/image/svm_deep_mind.png" alt="SVM 심화적 이해"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(0)</span><br><span class="line">X_xor = np.random.randn(200, 2)</span><br><span class="line">y_xor = np.logical_xor(X_xor[:, 0] &gt; 0, X_xor[:, 1] &gt; 0)</span><br><span class="line">y_xor = np.where(y_xor, 1, 0)</span><br><span class="line">plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1],</span><br><span class="line">            c=<span class="string">'b'</span>, marker=<span class="string">'o'</span>, label=<span class="string">'클래스 1'</span>, s=50)</span><br><span class="line">plt.scatter(X_xor[y_xor == 0, 0], X_xor[y_xor == 0, 1],</span><br><span class="line">            c=<span class="string">'r'</span>, marker=<span class="string">'s'</span>, label=<span class="string">'클래스 0'</span>, s=50)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.title(<span class="string">"XOR 문제"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/XOR_problem_with_svm.png" alt="XOR 문제"></p><h3 id="XOR-문제"><a href="#XOR-문제" class="headerlink" title="XOR 문제"></a>XOR 문제</h3><ul><li>위의 그림과같이 퍼셉트론이나 서포트 벡터 머신과 같은 선형판별함수 분류 모형은 다음과 같은 XOR(exclusive OR) 문제를 풀지 못한다는 단점이 있다. 이러한 경우에는 위의 그림에서 볼 수 있듯이 선형 판별평면(decision hyperplane)으로 영역을 나눌 수 없기 때문이다. 따라서 일반적인 SVM을 사용하면 XOR문제를 풀 수 없다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def plot_xor(X, y, model, title, xmin=-3, xmax=3, ymin=-3, ymax=3):</span><br><span class="line">    XX, YY = np.meshgrid(np.arange(xmin, xmax, (xmax-xmin)/1000),</span><br><span class="line">                         np.arange(ymin, ymax, (ymax-ymin)/1000))</span><br><span class="line">    ZZ = np.reshape(model.predict(</span><br><span class="line">        np.array([XX.ravel(), YY.ravel()]).T), XX.shape)</span><br><span class="line">    plt.contourf(XX, YY, ZZ, cmap=mpl.cm.Paired_r, alpha=0.5)</span><br><span class="line">    plt.scatter(X[y == 1, 0], X[y == 1, 1], c=<span class="string">'b'</span>,</span><br><span class="line">                marker=<span class="string">'o'</span>, label=<span class="string">'클래스 1'</span>, s=50)</span><br><span class="line">    plt.scatter(X[y == 0, 0], X[y == 0, 1], c=<span class="string">'r'</span>,</span><br><span class="line">                marker=<span class="string">'s'</span>, label=<span class="string">'클래스 0'</span>, s=50)</span><br><span class="line">    plt.xlim(xmin, xmax)</span><br><span class="line">    plt.ylim(ymin, ymax)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line"></span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line"></span><br><span class="line">svc = SVC(kernel=<span class="string">"linear"</span>).fit(X_xor, y_xor)</span><br><span class="line">plot_xor(X_xor, y_xor, svc, <span class="string">"선형 SVC 모형을 사용한 XOR 분류 결과"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/XOR_problem_with_svm_not_solved.png" alt="선형 SVC 모형을 사용한 XOR 분류 결과"></p><h3 id="변환함수를-사용한-비선형-판별-모형"><a href="#변환함수를-사용한-비선형-판별-모형" class="headerlink" title="변환함수를 사용한 비선형 판별 모형"></a>변환함수를 사용한 비선형 판별 모형</h3><ul><li>이러한 경우 도움이 되는 것이 원래의 $D$차원 독립 변수 벡터 $x$ 대신 비선형 함수로 변환한 $M$차원 벡터 $\phi(x)$를 독립 변수로 사용하는 방법이다.</li></ul><script type="math/tex; mode=display">\phi(\cdot): {R}^D \rightarrow {R}^M</script><script type="math/tex; mode=display">x=(x_1, x_2, \cdots, x_D) \;\;\; \rightarrow \;\;\; \phi(x) = (\phi_1(x), \phi_2(x), \cdots, \phi_M(x))</script><ul><li>앞서 XOR 문제를 풀기 위해 다음과 같이 상호 곱 (cross-multiplication) 항을 추가한 변환함수를 사용해 보자.</li></ul><script type="math/tex; mode=display">(x_1, x_2) \;\;\; \rightarrow \;\;\; \phi(x) = (x_1^2, \sqrt{2}x_1x_2, x_2^2)</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = np.arange(6).reshape(3, 2)</span><br><span class="line">X</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[0, 1],</span><br><span class="line">       [2, 3],</span><br><span class="line">       [4, 5]])</span><br></pre></td></tr></table></figure><ul><li><code>FunctionTransformer</code> 전처리 클래스로 위와 변환함수를 이용한 변환을 할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.preprocessing import FunctionTransformer</span><br><span class="line"></span><br><span class="line">def basis(X):</span><br><span class="line">    <span class="built_in">return</span> np.vstack([X[:, 0]**2, np.sqrt(2)*X[:, 0]*X[:, 1], X[:, 1]**2]).T</span><br><span class="line"></span><br><span class="line">FunctionTransformer(basis).fit_transform(X)</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.        ,  0.        ,  1.        ],</span><br><span class="line">       [ 4.        ,  8.48528137,  9.        ],</span><br><span class="line">       [16.        , 28.28427125, 25.        ]])</span><br></pre></td></tr></table></figure><ul><li>위와 같은 변환함수를 써서 XOR 문제의 데이터를 변환하면 특성 $\phi_2$를 사용하여 클래스 분류를 할 수 있다는 것을 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X_xor2 = FunctionTransformer(basis).fit_transform(X_xor)</span><br><span class="line">plt.scatter(X_xor2[y_xor == 1, 0], X_xor2[y_xor == 1, 1], c=<span class="string">"b"</span>, marker=<span class="string">'o'</span>, s=50)</span><br><span class="line">plt.scatter(X_xor2[y_xor == 0, 0], X_xor2[y_xor == 0, 1], c=<span class="string">"r"</span>, marker=<span class="string">'s'</span>, s=50)</span><br><span class="line">plt.ylim(-6, 6)</span><br><span class="line">plt.title(<span class="string">"변환 공간에서의 데이터 분포"</span>)</span><br><span class="line">plt.xlabel(r<span class="string">"$\phi_1$"</span>)</span><br><span class="line">plt.ylabel(r<span class="string">"$\phi_2$"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/transformation_space_dist_data_plot.png" alt="변환 공간에서의 데이터 분포"></p><ul><li>다음 코드는 <code>Pipeline</code>클래스로 변환함수 전처리기와 <code>SVC</code> 클래스를 합친 모형의 분류 결과이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">basismodel = Pipeline([(<span class="string">"basis"</span>, FunctionTransformer(basis)),</span><br><span class="line">                       (<span class="string">"svc"</span>, SVC(kernel=<span class="string">"linear"</span>))]).fit(X_xor, y_xor)</span><br><span class="line">plot_xor(X_xor, y_xor, basismodel, <span class="string">"변환함수 SVC 모형을 사용한 XOR 분류 결과"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/SVC_model_using_transformation_function_xor_problem_result.png" alt="변환함수 SVC 모형을 사용한 XOR 분류 결과"></p><h3 id="커널-트릭"><a href="#커널-트릭" class="headerlink" title="커널 트릭"></a>커널 트릭</h3><p><img src="/image/svm_deep_mind_01.png" alt="커널 도입"></p><ul><li>서포트 벡터 머신의 경우 목적 함수(비용 함수)와 예측 모형은 다음과 같이 dual form으로 표현할 수 있다.</li></ul><script type="math/tex; mode=display">L =  \sum_{n=1}^N a_n - \dfrac{1}{2}\sum_{n=1}^N\sum_{m=1}^N a_n a_m y_n y_m x_n^T x_m</script><script type="math/tex; mode=display">y = w^T x - w_0 = \sum_{n=1}^N a_n y_n x_n^T x - w_0</script><ul><li>이 수식에서 $x$를 변환함수 변환으로 $\phi(x)$로 바꾸면 아래와 같이 된다. 즉, 모든 변환함수는 $\phi(x_i)^T\phi(x_j)$의 형태로만 사용되며 독립적으로 사용되지 않는다. 따라서 두 개의 변환된 독립 변수 벡터를 내적(inner product)한 값 $\phi(x_i)^T\phi(x_j)$를 하나의 함수로 나타낼 수 있다.</li></ul><script type="math/tex; mode=display">L =  \sum_{n=1}^N a_n - \dfrac{1}{2}\sum_{n=1}^N\sum_{m=1}^N a_n a_m y_n y_m \phi(x_n)^T \phi(x_m)</script><script type="math/tex; mode=display">y = w^T x - w_0 = \sum_{n=1}^N a_n y_n \phi(x_n)^T \phi(x) - w_0</script><ul><li><p>이렇게 하나의 함수로 나타낸 것을 커널(kernel)이라고 한다. 대응하는 변환함수가 존재할 수만 있다면 변환함수를 먼저 정의하고 커널을 정의하는 것이 아니라 커널을 먼저 정의해도 상관없다.</p></li><li><p>커널이 제 역할을 하려면 $x_i$와 $x_j$의 유사도를 측정하는 함수여야한다. 또한 커널함수에서 도로 기저함수 포맷으로 만들어질수도 있어야한다.</p></li></ul><h3 id="커널의-의미"><a href="#커널의-의미" class="headerlink" title="커널의 의미"></a>커널의 의미</h3><ul><li>서포트 벡터 머신의 목적 함수와 예측 모형은 커널을 사용하여 표현하면 다음과 같다.</li></ul><script type="math/tex; mode=display">L =  \sum_{n=1}^N a_n - \dfrac{1}{2}\sum_{n=1}^N\sum_{m=1}^N a_n a_m y_n y_m k(x_n, x_m)</script><script type="math/tex; mode=display">y = w^T x - w_0 = \sum_{n=1}^N a_n y_n k(x_n, x) - w_0</script><ul><li><p>커널을 사용하지 않는 경우 $k(x,y) = x^{T}y$라는 점을 고려하면 커널은 다음과 같은 특징을 보인다.</p><ul><li>$x$와 $y$가 동일한 벡터일 때 가장 크고</li><li>두 벡터간의 거리가 멀어질수록 작아진다.</li></ul></li><li><p><code>즉, 두 표본 데이터간의 유사도(similarity)를 측정하는 기준으로 볼 수도 있다.</code></p></li></ul><h3 id="커널-사용의-장점"><a href="#커널-사용의-장점" class="headerlink" title="커널 사용의 장점"></a>커널 사용의 장점</h3><ul><li><p>커널을 사용하면 basis 함수를 하나씩 정의하는 수고를 덜 수 있을뿐더러 변환과 내적에 들어가는 계산량이 줄어든다. 예를 들어, 다음과 같은 변환함수의 경우 커널방법을 쓰지 않을 경우에 $\phi(x_i)^T \phi(x_j)$를 계산하려면 $4\;+\;4\;+\;3 \;=\;11$번의 곱셈을 해야 한다.</p><ul><li>$\phi(x_1)$ 계산 : 곱셈 4회</li><li>$\phi(x_2)$ 계산 : 곱셈 4회</li><li>내적 계산 : 곱셈 3회</li></ul></li></ul><script type="math/tex; mode=display">\phi(x_i) = \phi([x_{i,1}, x_{i,2}]) = (x_{i,1}^2, \sqrt{2}x_{i,1}x_{i,2}, x_{i,2}^2)</script><ul><li>그런데 이 변환함수는 다음과 같은 커널로 대체가능하다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} k(x_1, x_2) &=& (x_1^Tx_2)^2 \\ &=& (x_{1,1}x_{2,1} + x_{1,2}x_{2,2})^2 \\ &=& x_{1,1}^2x_{2,1}^2 + 2x_{1,1}x_{2,1}x_{1,2}x_{2,2} + x_{1,2}^2y_{2,2}^2 \\ &=& (x_{1,1}^2, \sqrt{2}x_{1,1}x_{1,2}, x_{1,2}^2)  (x_{2,1}^2, \sqrt{2}x_{2,1}x_{2,2}, x_{2,2}^2)^T \\ &=& \phi(x_1)^T \phi(x_2) \end{eqnarray}</script><ul><li><p>커널을 사용하면 $\phi(x_1)^T \phi(x_2)$을 계산하는데 $2\;+\;1\;=\;3$ 번의 곱셈이면 된다.</p><ul><li>$x_1^Tx_2$: 곱셈 2회</li><li>제곱 : 곱셈 1회</li></ul></li></ul><h3 id="커널의-확장-생성"><a href="#커널의-확장-생성" class="headerlink" title="커널의 확장 생성"></a>커널의 확장 생성</h3><ul><li><p><code>어떤 함수가 커널함수가 된다는 것을 증명하기 위해서는 변환함수를 하나 하나 정의할 필요없이 변환함수의 내적으로 표현할 수 있다는 것만 증명</code>하면 된다. 하지만 실제로는 다음 규칙을 이용하면 이미 만들어진 커널 $k_1(x_1, x_2), k_2(x_1, x_2)$로 부터 새로운 커널을 쉽게 만들 수 있다.</p></li><li><ol><li>커널함수를 양수배한 함수는 커널함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = ck_1(x_1, x_2)\;\;(c > 0)</script><ul><li><ol><li>커널함수에 양수인 상수를 더한 함수는 커널 함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = k_1(x_1, x_2) + c\;\;(c > 0)</script><ul><li><ol><li>두 커널함수를 더한 함수는 커널함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = k_1(x_1, x_2) + k_2(x_1, x_2)</script><ul><li><ol><li>두 커널함수를 곱한 함수는 커널함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = k_1(x_1, x_2)k_2(x_1, x_2)</script><ul><li><ol><li>커널함수를 $x\geq0$에서 단조증가(monotonically increasing)하는 함수에 적용하면 커널함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = (k_1(x_1, x_2))^n \;\; (n=1, 2, \cdots)</script><script type="math/tex; mode=display">k(x_1, x_2) = \exp(k_1(x_1, x_2))</script><script type="math/tex; mode=display">k(x_1, x_2) = \text{sigmoid}(k_1(x_1, x_2))</script><ul><li><ol><li>$x_{1}, x_{2}$ 각각의 커널함수값의 곱도 커널함수이다.</li></ol></li></ul><script type="math/tex; mode=display">k(x_1, x_2) = k_1(x_1, x_1)k_2(x_2, x_2)</script><h3 id="많이-사용되는-커널"><a href="#많이-사용되는-커널" class="headerlink" title="많이 사용되는 커널"></a>많이 사용되는 커널</h3><p><img src="/image/svm_deep_mind_02.png" alt="커널의 종류"></p><ul><li>다음과 같은 커널들이 많이 사용되는 커널들이다. 이 커널들은 대부분 변환함수로 변환하였을 때 무한대의 차원을 가지는 변환함수가 된다. 따라서 대부분의 비선형성을 처리할 수 있다. 비교를 위해 선형 서포트 벡터 머신의 경우도 추가하였다.</li></ul><blockquote><p>선형 서포트 벡터 머신</p></blockquote><script type="math/tex; mode=display">k(x_1, x_2) = x_1^Tx_2</script><blockquote><p>다항 커널 (Polynomial Kernel)</p></blockquote><script type="math/tex; mode=display">k(x_1, x_2) = (\gamma (x_1^Tx_2) + \theta)^d</script><blockquote><p>RBF(Radial Basis Function) 또는 가우시안 커널(Gaussian Kernel)</p><pre><code>- $\gamma = \frac{1}{2\sigma^{2}}$인 경우 가우시안 분포를 따르게 된다.</code></pre></blockquote><script type="math/tex; mode=display">k(x_1, x_2) = \exp \left( -\gamma ||x_1-x_2||^2 \right)</script><blockquote><p>시그모이드 커널 (Sigmoid Kernel)</p></blockquote><script type="math/tex; mode=display">k(x_1, x_2) = \tanh(\gamma (x_1^Tx_2) + \theta)</script><ul><li>앞에서 사용한 변환함수는 $\gamma \;=\;1,,\; \theta\;=\;0, \;d\;=\;2$인 다항 커널임을 알 수 있다.</li></ul><h3 id="다항-커널"><a href="#다항-커널" class="headerlink" title="다항 커널"></a>다항 커널</h3><ul><li>다항 커널은 벡터의 내적으로 정의된 커널을 확장하여 만든 커널이다. 아래에서 다항 커널이 어떤 변환함수로 되어 있는지 알아볼 것이다.</li></ul><ul><li>간단한 경우로 $\gamma \;=\;1,,\; \theta\;=\;1, \;d\;=\;4$이고 $x$가 스칼라인 경우에는 아래와 같으며, 마지막 수식에서 볼 수 있듯이 변환함수의 내적이 된다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} k(x_1, x_2) &=& (x_1^Tx_2 + 1)^4 \\ &=& x_1^4x_2^4 + 4x_1^3x_2^3 + 6x_1^2x_2^2 + 4x_1x_2 + 1 \\ &=& (x_1^4, 2x_1^3, \sqrt{6}x_1, 2x_1, 1)^T (x_2^4, 2x_2^3, \sqrt{6}x_2, 2x_2, 1) \ \\ \end{eqnarray}</script><ul><li>즉, 변환함수는 다음 5개가 된다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} \phi_1(x) &=& x^4 \\ \phi_2(x) &=& 2x^3 \\ \phi_3(x) &=& \sqrt{6}x^2 \\ \phi_4(x) &=& 2x \\ \phi_5(x) &=& 1 \\ \end{eqnarray}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">x1 = 1</span><br><span class="line">x2 = np.linspace(-3, 3, 100)</span><br><span class="line"></span><br><span class="line">def poly4(x1, x2):</span><br><span class="line">    <span class="built_in">return</span> (x1 * x2 + 1) ** 4</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8, 4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.plot(x2, poly4(x1, x2), ls=<span class="string">"-"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.title(<span class="string">"4차 다항커널의 예"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(122)</span><br><span class="line">plt.plot(x2, x2 ** 4)</span><br><span class="line">plt.plot(x2, 2 * x2 ** 3)</span><br><span class="line">plt.plot(x2, np.sqrt(6) * x2 ** 2)</span><br><span class="line">plt.plot(x2, 2 * x2)</span><br><span class="line">plt.plot(x2, np.ones_like(x2))</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.title(<span class="string">"4차 다항커널의 변환함수들"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/4th_polynomial_function_kernel_and_transformation_functions.png" alt="4차 다항 커널과 그에 따른 변환함수들"></p><h3 id="RBF-커널"><a href="#RBF-커널" class="headerlink" title="RBF 커널"></a>RBF 커널</h3><ul><li>RBF 커널은 가우시안 커널이라고도 한다. 문제를 간단하게 하기 위해 다음과 같이 가정할 것이다.</li></ul><script type="math/tex; mode=display">\gamma=\frac{1}{2}</script><script type="math/tex; mode=display">\|x_1\| = \|x_2\| = 1</script><ul><li>그러면 RBF 커널은 아래와 같은 차수가 무한대인 다항커널과 같아진다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} k(x_1, x_2) &=& \exp{\left(-\frac{||x_1 - x_2||^2}{2}\right)} \\ &=& \exp{\left(-\frac{x_1^Tx_1}{2} - \frac{x_2^Tx_2}{2} + 2x_1^Tx_2 \right)} \\ &=& \exp{\left(-\frac{x_1^Tx_1}{2}\right)}\exp{\left(-\frac{x_2^Tx_2}{2}\right)}\exp{(x_1^Tx_2)} \\ &=& C \exp{(x_1^Tx_2)} \\ &\approx& C \left( 1 + (x_1^Tx_2) + \dfrac{1}{2!}(x_1^Tx_2)^2 +  \dfrac{1}{3!}(x_1^Tx_2)^3 + \cdots \right) \\ \end{eqnarray}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">x1 = 0.0</span><br><span class="line">x2 = np.linspace(-7, 7, 100)</span><br><span class="line"></span><br><span class="line">def rbf(x1, x2, gamma):</span><br><span class="line">    <span class="built_in">return</span> np.exp(-gamma * np.abs(x2 - x1) ** 2)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8, 4))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.plot(x2, rbf(x1, x2, 1), ls=<span class="string">"-"</span>, label=<span class="string">"gamma = 1"</span>)</span><br><span class="line">plt.plot(x2, rbf(x1, x2, 0.5), ls=<span class="string">":"</span>, label=<span class="string">"gamma = 0.5"</span>)</span><br><span class="line">plt.plot(x2, rbf(x1, x2, 5), ls=<span class="string">"--"</span>, label=<span class="string">"gamma = 5"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x2 - x1"</span>)</span><br><span class="line">plt.xlim(-3, 7)</span><br><span class="line">plt.legend(loc=1)</span><br><span class="line">plt.title(<span class="string">"RBF 커널"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(122)</span><br><span class="line">plt.plot(x2, rbf(-4, x2, 1))</span><br><span class="line">plt.plot(x2, rbf(-2, x2, 1))</span><br><span class="line">plt.plot(x2, rbf(0, x2, 1))</span><br><span class="line">plt.plot(x2, rbf(2, x2, 1))</span><br><span class="line">plt.plot(x2, rbf(4, x2, 1))</span><br><span class="line">plt.xlabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.title(<span class="string">"RBF 커널의 변환함수들"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/RBF_kernel_function_for_many_parameters.png" alt="RBF 커널과 그에따른 변환함수들"></p><h3 id="scikit-learn의-커널-SVM"><a href="#scikit-learn의-커널-SVM" class="headerlink" title="scikit-learn의 커널 SVM"></a>scikit-learn의 커널 SVM</h3><ul><li>scikit-learn의 <code>SVM</code> 클래스는 <code>kernel</code>인수를 지정하여 커널을 설정할 수 있다.<ul><li><code>kernel = &quot;linear&quot;</code> : 선형 SVM. $k(x_{1},\;x_{2})\;=\;x_{1}^{T}x_{2}$  </li><li><code>kernel = &quot;poly&quot;</code> : 다항 커널. $k(x_{1},\;x_{2})\;=\;(\gamma \;(x_{1}^{T} x_{2})\; +\; \theta)^{d}$<ul><li><code>gamma</code>: $\gamma$</li><li><code>coef0</code>: $\theta$</li><li><code>degree</code>: $d$</li></ul></li><li><code>kernel = &quot;rbf&quot;</code> 또는 <code>kernel = None</code>: RBF 커널. $k(x_1, x_2) = \exp \left( -\gamma ||x_1-x_2||^2 \right)$<ul><li><code>\gamma</code></li></ul></li><li><code>kernel = &quot;sigmoid&quot;</code> 시그모이드 커널. $k(x_1, x_2) = \tanh(\gamma (x_1^Tx_2) + \theta)$<ul><li><code>gamma</code> : $\gamma$</li><li><code>coef0</code> : $\theta$</li></ul></li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">polysvc = SVC(kernel=<span class="string">"poly"</span>, degree=2, gamma=1, coef0=0).fit(X_xor, y_xor)</span><br><span class="line">rbfsvc = SVC(kernel=<span class="string">"rbf"</span>).fit(X_xor, y_xor)</span><br><span class="line">sigmoidsvc = SVC(kernel=<span class="string">"sigmoid"</span>, gamma=2, coef0=2).fit(X_xor, y_xor)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8, 12))</span><br><span class="line">plt.subplot(311)</span><br><span class="line">plot_xor(X_xor, y_xor, polysvc, <span class="string">"다항커널 SVC를 사용한 분류 결과"</span>)</span><br><span class="line">plt.subplot(312)</span><br><span class="line">plot_xor(X_xor, y_xor, rbfsvc, <span class="string">"RBF커널 SVC를 사용한 분류 결과"</span>)</span><br><span class="line">plt.subplot(313)</span><br><span class="line">plot_xor(X_xor, y_xor, sigmoidsvc, <span class="string">"시그모이드커널 SVC를 사용한 분류 결과"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/XOR_problem_result_of_using_polynomial_kernel.png" alt="다항커널 SVC를 사용한 분류 결과"></p><p><img src="/image/XOR_problem_result_of_using_RBF_kernel.png" alt="RBF커널 SVC를 사용한 분류 결과"></p><p><img src="/image/XOR_problem_result_of_using_sigmoid_kernel.png" alt="Sigmoid 커널 SVC를 사용한 분류 결과"></p><h3 id="커널-파라미터의-영향"><a href="#커널-파라미터의-영향" class="headerlink" title="커널 파라미터의 영향"></a>커널 파라미터의 영향</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(8, 8))</span><br><span class="line">plt.subplot(221)</span><br><span class="line">plot_xor(X_xor, y_xor, SVC(kernel=<span class="string">"rbf"</span>, gamma=2).fit(X_xor, y_xor), <span class="string">"RBF SVM (gamma=2)"</span>)</span><br><span class="line">plt.subplot(222)</span><br><span class="line">plot_xor(X_xor, y_xor, SVC(kernel=<span class="string">"rbf"</span>, gamma=10).fit(X_xor, y_xor), <span class="string">"RBF SVM (gamma=10)"</span>)</span><br><span class="line">plt.subplot(223)</span><br><span class="line">plot_xor(X_xor, y_xor, SVC(kernel=<span class="string">"rbf"</span>, gamma=50).fit(X_xor, y_xor), <span class="string">"RBF SVM (gamma=50)"</span>)</span><br><span class="line">plt.subplot(224)</span><br><span class="line">plot_xor(X_xor, y_xor, SVC(kernel=<span class="string">"rbf"</span>, gamma=100).fit(X_xor, y_xor), <span class="string">"RBF SVM (gamma=100)"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li>$\gamma$의 값이 커질수록 hyperplane의 경계가 더 둥글어지면서, 그 값이 너무 높아지면 overfitting이 되게 된다.</li></ul><p><img src="/image/RBF_kernel_parameters_difference_plot.png" alt="RBF 커널의 감마값의 비교"></p><h3 id="iris-데이터에-적용"><a href="#iris-데이터에-적용" class="headerlink" title="iris 데이터에 적용"></a>iris 데이터에 적용</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.preprocessing import StandardScaler</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data[:, [2, 3]]</span><br><span class="line">y = iris.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)</span><br><span class="line">sc = StandardScaler()</span><br><span class="line">sc.fit(X_train)</span><br><span class="line">X_train_std = sc.transform(X_train)</span><br><span class="line">X_test_std = sc.transform(X_test)</span><br><span class="line">X_combined_std = np.vstack((X_train_std, X_test_std))</span><br><span class="line">y_combined = np.hstack((y_train, y_test))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def plot_iris(X, y, model, title, xmin=-2.5, xmax=2.5, ymin=-2.5, ymax=2.5):</span><br><span class="line">    XX, YY = np.meshgrid(np.arange(xmin, xmax, (xmax-xmin)/1000),</span><br><span class="line">                         np.arange(ymin, ymax, (ymax-ymin)/1000))</span><br><span class="line">    ZZ = np.reshape(model.predict(np.array([XX.ravel(), YY.ravel()]).T), XX.shape)</span><br><span class="line">    plt.contourf(XX, YY, ZZ, cmap=mpl.cm.Paired_r, alpha=0.5)</span><br><span class="line">    plt.scatter(X[y == 0, 0], X[y == 0, 1], c=<span class="string">'r'</span>, marker=<span class="string">'^'</span>, label=<span class="string">'0'</span>, s=100)</span><br><span class="line">    plt.scatter(X[y == 1, 0], X[y == 1, 1], c=<span class="string">'g'</span>, marker=<span class="string">'o'</span>, label=<span class="string">'1'</span>, s=100)</span><br><span class="line">    plt.scatter(X[y == 2, 0], X[y == 2, 1], c=<span class="string">'b'</span>, marker=<span class="string">'s'</span>, label=<span class="string">'2'</span>, s=100)</span><br><span class="line">    plt.xlim(xmin, xmax)</span><br><span class="line">    plt.ylim(ymin, ymax)</span><br><span class="line">    plt.xlabel(<span class="string">"꽃잎의 길이"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"꽃잎의 폭"</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model1 = SVC(kernel=<span class="string">'linear'</span>).fit(X_test_std, y_test)</span><br><span class="line">model2 = SVC(kernel=<span class="string">'poly'</span>, random_state=0,</span><br><span class="line">             gamma=10, C=1.0).fit(X_test_std, y_test)</span><br><span class="line">model3 = SVC(kernel=<span class="string">'rbf'</span>, random_state=0, gamma=1,</span><br><span class="line">             C=1.0).fit(X_test_std, y_test)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8, 12))</span><br><span class="line">plt.subplot(311)</span><br><span class="line">plot_iris(X_test_std, y_test, model1, <span class="string">"선형 SVC"</span>)</span><br><span class="line">plt.subplot(312)</span><br><span class="line">plot_iris(X_test_std, y_test, model2, <span class="string">"다항커널 SVC"</span>)</span><br><span class="line">plt.subplot(313)</span><br><span class="line">plot_iris(X_test_std, y_test, model3, <span class="string">"RBF커널 SVM"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/iris_problem_result_of_using_linear_SVC.png" alt="선형 SVC를 사용한 분류 결과"></p><p><img src="/image/iris_problem_result_of_using_polynomial_kernel.png" alt="다항커널 SVC를 사용한 분류 결과"></p><p><img src="/image/iris_problem_result_of_using_RBF_kernel.png" alt="RBF커널 SVC를 사용한 분류 결과"></p><p><img src="/image/svm_deep_mind_03.png" alt="커널 사용시 유의할 점"></p><p><img src="/image/svm_deep_mind_04.png" alt="SVM과 LDA 비교"></p><h3 id="One-Class-Support-Vector-Machine"><a href="#One-Class-Support-Vector-Machine" class="headerlink" title="One Class Support Vector Machine"></a>One Class Support Vector Machine</h3><ul><li>종속변수가 없다는 의미이다. 고로 다 같은 범주이므로 classifying 정보가 없다는 것과 동일한 의미를 갖는다. <code>One Class SVM은 우리가 가진 자료들을 요약하는데 사용</code>한다. 마치 Unsupervised learning의 clustering 처럼 활용하는 방법이다. 기본적으로 원을 활용한 모델을 사용한다. 간단히 말해서 원안에는 자료가 있고, 원 바깥은 자료가 없다는 식으로 활용한다는 의미이다.  </li></ul><p><img src="/image/one_class_svm_concept.png" alt="one class svm 개념 - 01"></p><ul><li>SVM에서 서포트 벡터를 통해 $\beta$를 구해 hyperplane을 구했듯이 해당 원에 가장 가까운 $x_{k}$ 서포트 벡터를 통해 $R^{2}$ (반지를)을 계산한다.</li></ul><p><img src="/image/one_class_svm_concept_01.png" alt="one class svm 개념 - 02"></p><ul><li>아래의 그림처럼 바나나모양으로 데이터가 있는 경우에 임의의 반지름을 갖는 원안에 데이터를 모두 넣고 싶은데, 최소한의 반지름 값을 갖는 원을 찾는 문제가 된다.</li></ul><p><img src="/image/one_class_svm_concept_02.png" alt="one class svm 개념 - 03"></p><ul><li>내적을 kernel로 바꾼 후 polynomial kernel을 사용한 경우 차원이 높아 질수록 자유도도 높아짐(원보다는 데이터 하나하나에 영향을 더 많이 받음)을 확인할 수 있다.</li></ul><p><img src="/image/one_class_svm_concept_03.png" alt="one class svm 개념 - 04"></p><ul><li>또한, RBF 커널에서도 마찬가지로 C값이 높아질수록 허용하는 error가 낮아져 support vector안에 데이터들이 존재하게되며, 감마를 낮출수록($\sigma$를 높일수록) 두 벡터 $x_{i}$와 $x_{j}$간의 차이의 정도에 덜 민감해 지기 때문에 모양이 단순해 지게 된다.</li></ul><p><img src="/image/one_class_svm_concept_04.png" alt="one class svm 개념 - 05"></p><h3 id="SVR"><a href="#SVR" class="headerlink" title="SVR"></a>SVR</h3><ul><li>종속변수가 범주형이 아닌 연속형인 경우에는 SVR을 사용해야 한다. 아래 수식과 같이 margin과 hyperplane을 정의하는 것은 동일하다. 이번에는 <code>margin 밖에 존재하는 데이터들과 decision boundary와의 차이(error)를 최소화 하도록하는 방식</code>으로 동작한다. 그러므로 margin 안에 많은 데이터를 포함하고 있을 수록 error가 최소화 될 것이다. 결과적으론 아래와 같은 목적함수를 최소화하는데 마지막 수식과 같이 직관적으로 margin과의 차이만을 error로 보는 함수를 사용할 수 있다.</li></ul><p><img src="/image/svr_conception.png" alt="SVR의 정의"></p><ul><li>허나, 이전 그림에서의 error를 정의하는데 수학적인 계산에 용이성을 더하기 위해 아래와 같은 수식으로 error를 계산한다. 앞에서와 같이 미분불가능한 점이 없고 계산하기 편하기 때문에 아래와 같은 수식으로 사용한다.</li></ul><p><img src="/image/svr_conception_01.png" alt="SVR의 정의"></p><p><img src="/image/svr_conception_02.png" alt="SVR의 정의"></p><ul><li>SVM과 마찬가지로 SVR도 커널을 사용하여 다음과 같은 곡선의 형태로 fitting할 수 있다. 기본적으로 linear model을 사용했을 때는 전반적인 패턴을 잡아주고 다른 커널들에 대해서는 각 커널의 특성에 맞게 적합시켜 준다.</li></ul><p><img src="/image/svr_conception_03.png" alt="SVR의 정의"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/25/machine_learning_12/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Support Vector Machine(SVM) - 01</title>
      <link>https://heung-bae-lee.github.io/2020/04/22/machine_learning_11/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/22/machine_learning_11/</guid>
      <pubDate>Wed, 22 Apr 2020 09:04:12 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Support-Vector-Machine-SVM&quot;&gt;&lt;a href=&quot;#Support-Vector-Machine-SVM&quot; class=&quot;headerlink&quot; title=&quot;Support Vector Machine(SVM)&quot;&gt;&lt;/a&gt;Support
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Support-Vector-Machine-SVM"><a href="#Support-Vector-Machine-SVM" class="headerlink" title="Support Vector Machine(SVM)"></a>Support Vector Machine(SVM)</h1><ul><li>데이터의 분포가 정규분포를 띈다고 보이지 않는다면 이전에 말했던 LDA나 QDA를 사용하기 힘들다. 이런 경우 클래스간 분류를 하려고 할 때 사용될 수 있는 방법 중 하나가 SVM이다. 아래 그림과 같이 클래스 집단을 분류할 수 있는 벡터를 기준으로 최대한의 마진을 주어 분류하는 방식이다.</li></ul><p><img src="/image/background_of_svm_concept_01.png" alt="Support Vector Machine의 배경 - 01"></p><ul><li>아래 그림과 같이 두 클래스 집단간의 데이터 분포가 혼용되어있다면 어떤 방식으로 접근해야 할까? 두 클래스 집단간의 거리를 최대화하면서 혼용되어있는 데이터들에 대한 error를 적당히 허용하는 선에서 decision boundary를 결정해야 할 것이다.</li></ul><p><img src="/image/background_of_svm_concept_02.png" alt="Support Vector Machine의 배경 - 02"></p><ul><li>SVM은 regression 문제도 사용하지만 보통은 범주형 변수에 대한 classification 문제에 많이 사용한다. Support vector regression(SVR)은 최대한 많은 데이터를 margin 안에 포함하고자 하는 것이다. 이에 대해 margin 바깥에 존해하는 데이터에 대해 error를 줄어서 그 error를 최소화하는 방향으로 회귀를 진행한다. 일반적인 Regression은 해당 데이터를 설명할 수 있는 선에 대해 error를 계산하는 방식이라면, SVR은 margin 바깥에 존재하는 데이터들에 대해서만 error를 계산하는 방식이다.</li></ul><p><img src="/image/background_of_svm_concept_03.png" alt="Support Vector Machine의 배경 - 03"></p><p><img src="/image/background_of_svm_concept_04.png" alt="Support Vector Machine의 배경 - 04"></p><ul><li>초평면에 부등호를 도입하면 다음과 같이 영역으로 데이터를 구분지을 수 있게 된다.</li></ul><p><img src="/image/how_to_make_decision_boundary_with_svm_01.png" alt="Support Vector Machine의 Decision boundary - 01"></p><p><img src="/image/how_to_make_decision_boundary_with_svm_02.png" alt="Support Vector Machine의 Decision boundary - 02"></p><p><img src="/image/how_to_make_decision_boundary_with_svm_03.png" alt="Support Vector Machine의 Decision boundary - 03"></p><h2 id="나그랑주-승수-Lagrange-multiplier"><a href="#나그랑주-승수-Lagrange-multiplier" class="headerlink" title="나그랑주 승수(Lagrange multiplier)"></a>나그랑주 승수(Lagrange multiplier)</h2><ul><li>최적화 문제(예를 들어서 극대값이나 극소값)를 푸는데 특정조건하에서 문제를 풀 수 있도록 하는 방법이다. 아래 그림에서 보면, 아무런 제한이 없었다면 x와 y의 값에 따라 $-\infty$에서 $\infty$로 움직일 수 있을 것이다. 허나, $g(x,y)=c$라는 함수 범위 내에서만 움직일 수 있다고 제한을 주면 해당 제한영역하에서의 최적화를 풀어야할 것이다.</li></ul><p><img src="/image/Lagrange_multiplier_01.png" alt="Lagrange multiplier - 01"></p><p><img src="/image/Lagrange_multiplier_02.png" alt="Lagrange multiplier - 02"></p><p><img src="/image/Lagrange_multiplier_03.png" alt="Lagrange multiplier - 03"></p><ul><li>위에서 언급하는 최적화 문제를 수학적으로 살펴보려면, 아래와 같이 최적화문제에 대한 설명이 필요하다.</li></ul><h3 id="제한조건이-있는-최적화-문제"><a href="#제한조건이-있는-최적화-문제" class="headerlink" title="제한조건이 있는 최적화 문제"></a>제한조건이 있는 최적화 문제</h3><ul><li>제한조건(constraint)을 가지는 최적화 문제를 풀어본다. 제한 조건은 연립방적식 또는 연립부등식이다. <code>연립방정식 제한조건이 있는 경우에는 라그랑주 승수법을 사용하여 새로운 최적화 문제를 풀어야</code> 한다. <code>연립부등식 제한조건의 경우에는 KKT조건이라는 것을 만족하도록 하는 복잡한 과정을 거쳐야 한다.</code></li></ul><h4 id="등식-제한조건이-있는-최적화-문제"><a href="#등식-제한조건이-있는-최적화-문제" class="headerlink" title="등식 제한조건이 있는 최적화 문제"></a>등식 제한조건이 있는 최적화 문제</h4><ul><li>현실의 최적화 문제에서는 여러가지 제한조건이 있는 최적화(constrained optimization) 문제가 많다. 가장 간단한 경우는 다음과 같이 연립방정식 제한조건이 있는 경우다. 등식(equality)제한 조건이라고도 한다.</li></ul><script type="math/tex; mode=display">\begin{align} x^{\ast} = \text{arg} \min_x f(x) \end{align}</script><script type="math/tex; mode=display">\begin{align} x \in \mathbf{R}^N \end{align}</script><script type="math/tex; mode=display">\begin{align} g_j(x) = 0 \;\; (j=1, \ldots, M) \end{align}</script><ul><li>첫 번째 식만 보면 단순히 목적함수 $f(x)$를 가장 작게 하는 N차원 벡터 x값을 찾는 문제다. 하지만 마지막 식에 있는 M개의 등식 제한 조건이 있으면 M개 연립 방정식을 동시에 모두 만족시키면서 목적함수 $f(x)$를 가장 작게하는 $x$값을 찾아야 한다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} g_1(x) &= 0 \\ g_2(x) &= 0 \\ &\vdots \\ g_M(x) &= 0 \\ \end{aligned} \end{align}</script><h5 id="예제"><a href="#예제" class="headerlink" title="예제"></a>예제</h5><blockquote><p>목적 함수 $f$와 등식 제한조건 $g$가 다음과 같은 경우를 생각하자. 이 문제는 다음 그림 처럼 $g(x_{1}, x_{2}) = 0$으로 정의되는 직선상에서 가장 $f(x_{1},x_{2})$값이 작아지는 점 $(x_1^{\ast}, x_2^{\ast})$을 찾는 문제가 된다.</p></blockquote><script type="math/tex; mode=display">\begin{align} f(x_1, x_2) = x_1^2 + x_2^2 \end{align}</script><script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = x_1 + x_2 - 1 = 0 \end{align}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 목적함수 f(x) = x1^2 + x2^2</span></span><br><span class="line">def f1(x1, x2):</span><br><span class="line">    <span class="built_in">return</span> x1 ** 2 + x2 ** 2</span><br><span class="line"></span><br><span class="line">x1 = np.linspace(-5, 5, 100)</span><br><span class="line">x2 = np.linspace(-3, 3, 100)</span><br><span class="line">X1, X2 = np.meshgrid(x1, x2)</span><br><span class="line">Y = f1(X1, X2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 등식 제한조건 방정식 g(x) = x1 + x2 - 1 = 0</span></span><br><span class="line">x2_g = 1 - x1</span><br><span class="line"></span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>, levels=[0.5, 2, 8, 32])</span><br><span class="line">plt.plot(x1, x2_g, <span class="string">'g-'</span>)</span><br><span class="line"></span><br><span class="line">plt.plot([0], [0], <span class="string">'rP'</span>)</span><br><span class="line">plt.plot([0.5], [0.5], <span class="string">'ro'</span>, ms=10)</span><br><span class="line"></span><br><span class="line">plt.xlim(-5, 5)</span><br><span class="line">plt.ylim(-3, 3)</span><br><span class="line">plt.xticks(np.linspace(-4, 4, 9))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"등식 제한조건이 있는 최적화 문제"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/Lagrange_multiplier_optimization_problem.png" alt="최적화 문제 예시"></p><h4 id="라그랑주-승수법"><a href="#라그랑주-승수법" class="headerlink" title="라그랑주 승수법"></a>라그랑주 승수법</h4><ul><li>이렇게 등식 제한조건이 있는 최적화 문제는 라그랑주 승수법(Lagrange multiplier)을 사용하여 최적화할 수 있다. 라그랑주 승수 방법에서는 목적함수를 원래의 목적함수 $f(x)$를 사용하지 않는다. 대신 제한조건 등식에 $\lambda$라는 새로운 변수를 곱해서 더한 함수를 목적함수로 간주하여 최적화한다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} h(x, \lambda) &= h(x_1, x_2, \ldots , x_N, \lambda_1, \ldots , \lambda_M) \\ &= f(x) + \sum_{j=1}^M \lambda_j g_j(x) \end{aligned} \end{align}</script><ul><li>이때 제한조건 등식 하나마다 새로운 $\lambda_{i}$를 추가해주어야 한다. 따라서 만약 제한조건이 $M$개이면 $\lambda_{1}, \cdots, \lambda_{M}$개의 변수가 새로 생긴 것과 같다. 이렇게 확장된 목적함수 $h$는 입력변수가 더 늘어났기 때문에 그레디언트 벡터를 영벡터로 만드는 최적화 필요 조건이 다음처럼 $N\;+\;M$개가 된다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= \dfrac{\partial f}{\partial x_1} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_1} = 0 \\ \dfrac{\partial h}{\partial x_2} &= \dfrac{\partial f}{\partial x_2} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_2} = 0 \\ & \vdots  \\ \dfrac{\partial h}{\partial x_N} &= \dfrac{\partial f}{\partial x_N} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_N} = 0 \\ \dfrac{\partial h}{\partial \lambda_1} &= g_1 = 0 \\ & \vdots  \\ \dfrac{\partial h}{\partial \lambda_M} &= g_M = 0 \end{aligned} \end{align}</script><ul><li>이 $N\;+\;M$개의 연립 방정식을 풀면 $N\;+\;M$개의 미지수를 구할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} x_1, x_2, \ldots, x_N, , \lambda_1, \ldots , \lambda_M \end{align}</script><ul><li>구한 결과에서 찾는 최소값 $x$를 구할 수 있다. 라그랑주 승수값은 필요없다.</li></ul><blockquote><p>예제) 위에서 제시한 예제를 라그랑주 승수법으로 풀어보자. 새로운 목적함수는 다음과 같다.</p></blockquote><script type="math/tex; mode=display">\begin{align} h(x_1, x_2, \lambda) = f(x_1, x_2) + \lambda g(x_1, x_2) = x_1^2 + x_2^2 + \lambda ( x_1 + x_2 - 1 )  \end{align}</script><ul><li>라그랑주 승수법을 적용하여 그레디언트 벡터가 영벡터인 위치를 구한다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= 2{x_1} + \lambda = 0 \\ \dfrac{\partial h}{\partial x_2} &= 2{x_2} + \lambda = 0 \\ \dfrac{\partial h}{\partial \lambda}  &= x_1 + x_2 - 1 = 0 \end{aligned} \end{align}</script><ul><li>방정식의 해는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} x_1 = x_2 = \dfrac{1}{2}, \;\;\; \lambda = -1 \end{align}</script><blockquote><p>연습문제 제한조건이 $x_{1}+x_{2}\; = \; 1$일 때 목적 함수 $f(x) = - log x_{1} - log x_{2} x_{1},x_{2} &gt; 0$ 을 최소화하는 x_{1}, x_{2}값을 라그랑주 승수법으로 계산하라.</p></blockquote><ul><li>위의 문제에서 목저함수는 아래와 같다.</li></ul><script type="math/tex; mode=display">\begin{align} h(x_1, x_2, \lambda) = f(x_1, x_2) + \lambda g(x_1, x_2) = - log x_1 - log x_2 + \lambda ( x_1 + x_2 - 1 ) \end{align}</script><ul><li>라그랑주 승수법은 적용하여 그레디언트 벡터가 영벡터인 위치를 구한다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= \lambda x_1 -1 = 0 \\ \dfrac{\partial h}{\partial x_2} &= \lambda x_2 -1 = 0 \\ \dfrac{\partial h}{\partial \lambda} &= x_1 + x_2 - 1 = 0 \end{aligned} \end{align}</script><ul><li>위 방정식을 풀면 해는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} x_1 = x_2 = \dfrac{1}{2}, \;\;\; \lambda = 2 \end{align}</script><h5 id="scipy를-사용하여-등식-제한조건이-있는-최적화-문제-계산하기"><a href="#scipy를-사용하여-등식-제한조건이-있는-최적화-문제-계산하기" class="headerlink" title="scipy를 사용하여 등식 제한조건이 있는 최적화 문제 계산하기"></a>scipy를 사용하여 등식 제한조건이 있는 최적화 문제 계산하기</h5><ul><li>scipy의 optimize 서브패키지는 제한조건이 있는 최적화 문제를 푸는 <code>fmin_slsqp()</code>명령을 제공한다. 목적함수와 초기값, 그리고 제한조건 함수의 리스트를 인수로 받는다. 목적함수는 배열인 인수를 받도록 구현되어야 하고 제한조건 함수의 경우에는 항상 <code>eqcons</code>인수를 명시해야 한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fmin_slsqp(func_objective, x0, eqcons=[func_constraint1, func_constraint2])</span><br></pre></td></tr></table></figure><ul><li>위에서의 두 문제를 scipy를 통해서 풀어보겠다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def f1array(x):</span><br><span class="line">    <span class="built_in">return</span> x[0] ** 2 + x[1] ** 2</span><br><span class="line"></span><br><span class="line">def eq_constraint(x):</span><br><span class="line">    <span class="built_in">return</span> x[0] + x[1] - 1</span><br><span class="line"></span><br><span class="line">scipy.optimize.fmin_slsqp(f1array, np.array([1, 1]), eqcons=[eq_constraint])</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.    (Exit mode 0)</span><br><span class="line">            Current <span class="keyword">function</span> value: 0.5000000000000002</span><br><span class="line">            Iterations: 2</span><br><span class="line">            Function evaluations: 8</span><br><span class="line">            Gradient evaluations: 2</span><br><span class="line">array([0.5, 0.5])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def f1array(x):</span><br><span class="line">    <span class="built_in">return</span> -np.log(x[0]) -np.log(x[1])</span><br><span class="line"></span><br><span class="line">def eq_constraint(x):</span><br><span class="line">    <span class="built_in">return</span> x[0] + x[1] - 1</span><br><span class="line"></span><br><span class="line">scipy.optimize.fmin_slsqp(f1array, np.array([1, 1]), eqcons=[eq_constraint])</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.    (Exit mode 0)</span><br><span class="line">            Current <span class="keyword">function</span> value: 1.3862943611198901</span><br><span class="line">            Iterations: 2</span><br><span class="line">            Function evaluations: 8</span><br><span class="line">            Gradient evaluations: 2</span><br><span class="line">array([0.5, 0.5])</span><br></pre></td></tr></table></figure><h3 id="라그랑주-승수의-의미"><a href="#라그랑주-승수의-의미" class="headerlink" title="라그랑주 승수의 의미"></a>라그랑주 승수의 의미</h3><ul><li>만약 최적화 문제에서 등식 제한조건 $g_{i}가 있는가 없는가에 따라 해의 값이 달라진다면 이 등식 제한조건에 대응하는 라그랑주 승수 $\lambda_{i}$는 0이 아닌 값이어야 한다. $\lambda_{i} = 0$일 때만 원래의 문제와 제한조건이 있는 문제의 최적화 조건이 같아지므로 최적화 해의 위치도 같게 나오기 때문이다.</li></ul><script type="math/tex; mode=display">\begin{align} \lambda_i \neq 0 \end{align}</script><h4 id="예제-1"><a href="#예제-1" class="headerlink" title="예제"></a>예제</h4><ul><li>목적함수가 아래와 같은 최소화 문제의 답은 $x_{1} = x_{2} = 0$이다.</li></ul><script type="math/tex; mode=display">\begin{align} f(x) = x_1^2 + x_2^2 \end{align}</script><ul><li>여기에 다음 제한 조건이 있다고 하자.</li></ul><script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = x_1 + x_2 = 0 \end{align}</script><ul><li>라그랑주 승수법에서 새로운 목적함수는 아래와 같다.</li></ul><script type="math/tex; mode=display">\begin{align} h(x_1, x_2, \lambda) = f(x_1, x_2) + \lambda g(x_1, x_2) = x_1^2 + x_2^2 + \lambda ( x_1 + x_2) \end{align}</script><ul><li>이에 따른 최적화 조건은 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= 2{x_1} + \lambda = 0 \\ \dfrac{\partial h}{\partial x_2} &= 2{x_2} + \lambda = 0 \\ \dfrac{\partial h}{\partial \lambda} &= x_1 + x_2 = 0 \end{aligned} \end{align}</script><ul><li>이에 대한 해는 $x_{1} = x_{2} = \lambda = 0$으로 <code>제한조건이 있으나 없으나 해는 동일하며, 라그랑주 승수는 0이 된다.</code> 즉, 제한조건이 의미가 없는 경우는 라그랑주 승수가 0이된다는 의미이다.</li></ul><h3 id="부등식-제한조건이-있는-최적화-문제"><a href="#부등식-제한조건이-있는-최적화-문제" class="headerlink" title="부등식 제한조건이 있는 최적화 문제"></a>부등식 제한조건이 있는 최적화 문제</h3><ul><li>이번에는 다음과 같이 부등식(inequality) 제한조건이 있는 최적화 문제를 생각하자.</li></ul><script type="math/tex; mode=display">\begin{align} x^{\ast} = \text{arg} \min_x f(x) \end{align}</script><script type="math/tex; mode=display">\begin{align} x \in \mathbf{R}^N \end{align}</script><script type="math/tex; mode=display">\begin{align} g_j(x) \leq 0 \;\; (j=1, \ldots, M) \end{align}</script><ul><li>만약 부등식이 $g_j(x) \geq 0$과 같다면 양변에 $-1$을 곱하여 부등호의 방향을 바꾼다. 이렇게 부등식 제한조건이 있는 최적화 문제도 라그랑주 승수 방법과 목적함수를 다음처럼 바꾸어 푼다. 이렇게 부등식 제한 조건이 있는 최적화 문제도 라그랑주 승수 방법과 목적함수를 다음처럼 바꾸어 푼다.</li></ul><script type="math/tex; mode=display">\begin{align} h(x, \lambda) = f(x) + \sum_{j=1}^M \lambda_j g_j(x) \end{align}</script><h5 id="다만-이-경우-최적화-해의-필요조건은-방정식-제한조건이-있는-최적화-문제와-다르게-KKT-Karush-Kuhn-Tucker-조건이라고-하며-다음처럼-3개의-조건으로-이루어진다"><a href="#다만-이-경우-최적화-해의-필요조건은-방정식-제한조건이-있는-최적화-문제와-다르게-KKT-Karush-Kuhn-Tucker-조건이라고-하며-다음처럼-3개의-조건으로-이루어진다" class="headerlink" title="다만, 이 경우 최적화 해의 필요조건은 방정식 제한조건이 있는 최적화 문제와 다르게 KKT(Karush-Kuhn-Tucker)조건이라고 하며 다음처럼 3개의 조건으로 이루어진다."></a>다만, 이 경우 최적화 해의 필요조건은 방정식 제한조건이 있는 최적화 문제와 다르게 <code>KKT(Karush-Kuhn-Tucker)조건</code>이라고 하며 다음처럼 3개의 조건으로 이루어진다.</h5><ul><li>1) 모든 독립변수 $x_{1}, x_{2}, \ldots, \x_{N}$에 대한 미분값이 0이다.<ul><li>첫 번째 조건은 방정식 제한조건의 경우와 같다. 다만 변수 $x$들에 대한 미분값만 0이어야 한다. 라그랑주 승수 $\lambda$에 대한 미분은 0이 아니어도 된다.</li></ul></li></ul><script type="math/tex; mode=display">\begin{align} \dfrac{\partial h(x, \lambda)}{\partial x_i} = 0 \end{align}</script><ul><li>2) 모든 라그랑주 승수 $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{M}$과 제한조건 부등식($\lambda$에 대한 미분값)의 곱이 0이다.<ul><li>두 번째 조건을 보면 확장된 목적함수를 나그랑주 승수로 미분한 값은 변수 $x$들에 대한 미분값과는 달리 반드시 0이 될 필요는 없다는 것을 알 수 있다. 이렇게 하려면 두 경우가 가능한데 등식 제한조건의 경우처럼 라그랑주 승수 $\lambda$에 대한 미분값이 0이어도 되고 아니면 라그랑주 승수 $\lambda$값 자체가 0이 되어도 된다.</li></ul></li></ul><script type="math/tex; mode=display">\begin{align} \lambda_j \cdot \dfrac{\partial h(x, \lambda)}{\partial \lambda_j} = \lambda_j \cdot g_j = 0  \end{align}</script><ul><li>3) 라그랑주 승수는 음수가 아니어야 한다.<ul><li>마지막 조건은 KKT 조건이 실제로 부등식 제한조건이 있는 최적화 문제와 같은 문제임을 보장하는 조건이다.</li></ul></li></ul><script type="math/tex; mode=display">\begin{align} \lambda_j \geq 0 \end{align}</script><h4 id="예제-2"><a href="#예제-2" class="headerlink" title="예제"></a>예제</h4><ul><li>부등식 제한조건을 가지는 최적화의 예를 풀어보자. 목적함수는 아래와 같다.</li></ul><script type="math/tex; mode=display">\begin{align} f(x_1, x_2) = x_1^2 + x_2^2 \end{align}</script><ul><li>이 예제에서 두 가지 제한 조건을 고려해 볼 텐데 하나는 다음 그림 중 왼쪽 그림처럼 부등식 제한조건이 아래와 같은 경우이다.</li></ul><script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = x_1 + x_2 - 1 \leq 0 \end{align}</script><ul><li>다른 하나의 제한조건은 아래와 같고 이에 대한 그림은 오른쪽에 해당한다.</li></ul><script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = -x_1 - x_2 + 1 \leq 0 \end{align}</script><ul><li>아래 그림에서 제한조건을 만족하는 영역을 어둡게 표시했다. 최적점의 위치는 점으로 표시했다. 첫 번째 제한조건의 경우에는 부등식 제한조건이 있기는 하지만 원래의 최적화 문제의 해가 부등식 제한조건이 제시하는 영역 안에 있기 때문에 최적점의 위치가 달라지지 않는다. 두 번째 제한조건의 경우에는 원래의 최적화 문제의 해가 부등식 제한조건이 제시하는 영역 바깥에 있기 때문에 최적점의 위치가 달라졌다. 하지만 최적점의 위치가 영역의 경계선(boundary line)에 있다는 점에 주의하라.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(13, 7))</span><br><span class="line">ax1 = plt.subplot(121)</span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>, levels=[0.5, 2, 8])</span><br><span class="line">plt.plot(x1, x2_g, <span class="string">'g-'</span>)</span><br><span class="line">ax1.fill_between(x1, -20, x2_g, alpha=0.5)</span><br><span class="line">plt.plot([0], [0], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.xlim(-3, 3)</span><br><span class="line">plt.ylim(-5, 5)</span><br><span class="line">plt.xticks(np.linspace(-4, 4, 9))</span><br><span class="line">plt.yticks(np.linspace(-5, 5, 11))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"최적해가 부등식과 관계없는 경우"</span>)</span><br><span class="line">ax2 = plt.subplot(122)</span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>, levels=[0.5, 2, 8])</span><br><span class="line">plt.plot(x1, x2_g, <span class="string">'g-'</span>)</span><br><span class="line">ax2.fill_between(x1, 20, x2_g, alpha=0.5)</span><br><span class="line">plt.plot([0.5], [0.5], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.xlabel(<span class="string">"x_1"</span>)</span><br><span class="line">plt.xlim(-3, 3)</span><br><span class="line">plt.ylim(-5, 5)</span><br><span class="line">plt.xticks(np.linspace(-4, 4, 9))</span><br><span class="line">plt.yticks(np.linspace(-5, 5, 11))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"최적해가 부등식에 의해 결정되는 경우"</span>)</span><br><span class="line">plt.suptitle(<span class="string">"부등식 제한조건이 있는 최적화 문제"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/inequality_having_constraint_optimization_problem.png" alt="부등식 제한조건이 있는 최적화 문제"></p><ul><li><p>그림에서 보듯이 부등식 제한조건이 있는 최적화 문제를 풀면 그 제한조건은 다음 두 가지 경우의 하나가 되어 버린다.</p><ul><li>최적화 결과에 전혀 영향을 주지 않는 <code>쓸모없는</code> 제한조건</li><li>최적화 결과에 영향을 주는 <code>등식(equality)</code>인 제한조건</li></ul></li><li><p>어느 경우이든 부등식 제한조건 문제로 시작했지만 결과는 제한조건이 없거나 등식 제한조건 문제를 푸는 것과 같아진다. KKT조건 중 두 번째 조건이 뜻하는 바는 다음과 같다. 다음 식에서 $x^{\ast}, \lambda^{\ast}$는 KKT 조건을 풀어서 구한 최적해의 값이다.</p></li></ul><script type="math/tex; mode=display">\begin{align} \lambda^{\ast} = 0 \;\; \text{or} \;\;  g(x^{\ast}) = 0 \end{align}</script><ul><li>만약 $g_{i} = 0$이면 이 조건은 부등식 제한조건이 아닌 등식 제한조건이 된다. 그리고 등식 제한조건에서 말한 바와 같이 (이 제한조건이 있으나 없으나 해가 바뀌지 않는 특수한 경우를 제외하면) 라그랑주 승수는 0이 아닌값을 가진다.</li></ul><script type="math/tex; mode=display">\begin{align} g_i = 0 \;\; \rightarrow \;\; \lambda_i \neq 0 \; (\lambda_i > 0) \end{align}</script><ul><li>반대로 $g_i \neq 0 \; (g_i &lt; 0)$이면 해가 $g_{i}$가 표현하는 곡선으로부터 떨어져 있기 때문에 부등식 제한조건이 아무런 의미가 없어진다. 즉, 제한조건이 있을 때와 없을 때의 해가 같다. 따라서 목적함수 $h(x,\lambda)$는 $\lambda_{i}g_{i}(g_{i} \neq 0)$항이 있으나 없으나 상관없이 같은 해를 가진다. 따라서 $\lambda_{i} = 0$이 된다.</li></ul><script type="math/tex; mode=display">\begin{align} g_i \neq 0 \;\; \rightarrow \;\; \lambda_i = 0  \end{align}</script><ul><li>따라서 <code>부등식 제한조건이 있는 최적화 문제는 각 제한조건에 대해 위의 두 가지 경우를 가정하여 각각 풀어보면서 최적의 답을 찾는다</code>.</li></ul><blockquote><p>예제) 다음은 복수의 부등식 제한조건이 있는 또다른 2차원 최적화 문제의 예이다.</p></blockquote><script type="math/tex; mode=display">\begin{align} \text{arg} \min_x \; (x_1-4)^2 + (x_2-2)^2 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_1(x) = x_1 + x_2 - 1\leq 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_2(x) = -x_1 + x_2 - 1\leq 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_3(x) = -x_1 - x_2 - 1\leq 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_4(x) = x_1 - x_2 - 1\leq 0 \end{align}</script><ul><li>위의 4가지 제한조건은 다음과 같은 하나의 부등식으로 나타낼 수도 있다.</li></ul><script type="math/tex; mode=display">\begin{align} g(x) = \left\vert\, x_1 \right\vert + \left\vert\, x_2 \right\vert - 1 = \sum_{i=1}^{2} \left\vert\, x_i \right\vert - 1 \leq 0 \end{align}</script><ul><li><p>아래 예제에서 최적해가 $x_{1}\;=\;1,  x_{2}\; = \;0$이라는 사실을 이용하여 라그랑주 승수 $\lambda_{1}, \lambda_{2}, \lambda_{3}, \lambda_{4}$ 중 어느 값이 0이 되는지 말해보자.</p><ul><li>이에 대한 답은 $\lambda_{2} = \lambda_{3} = 0$이 될 것이다. 해를 찾는데 아무런 영향을 미치지 않기 때문이다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def f2plt(x1, x2):</span><br><span class="line">    <span class="built_in">return</span> np.sqrt((x1 - 4) ** 2 + (x2 - 2) ** 2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x1 = np.linspace(-2, 5, 100)</span><br><span class="line">x2 = np.linspace(-1.5, 3, 100)</span><br><span class="line">X1, X2 = np.meshgrid(x1, x2)</span><br><span class="line">Y = f2plt(X1, X2)</span><br><span class="line"></span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>,</span><br><span class="line">            levels=np.arange(0.5, 5, 0.5) * np.sqrt(2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 제한 조건의 상수</span></span><br><span class="line">k = 1</span><br><span class="line">ax = plt.gca()</span><br><span class="line">x12 = np.linspace(-k, 0, 10)</span><br><span class="line">x13 = np.linspace(0, k, 10)</span><br><span class="line">ax.fill_between(x12, x12 + k, -k - x12, color=<span class="string">'g'</span>, alpha=0.5)</span><br><span class="line">ax.fill_between(x13, x13 - k, k - x13, color=<span class="string">'g'</span>, alpha=0.5)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 최적점 위치</span></span><br><span class="line">x1_sol = 1</span><br><span class="line">x2_sol = 0</span><br><span class="line">plt.plot(x1_sol, x2_sol, <span class="string">'ro'</span>, ms=20)</span><br><span class="line"></span><br><span class="line">plt.xlim(-2, 5)</span><br><span class="line">plt.ylim(-1.5, 3)</span><br><span class="line">plt.xticks(np.linspace(-2, 5, 8))</span><br><span class="line">plt.yticks(np.linspace(-1, 3, 5))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"$|x_1| + |x_2| \leq &#123;&#125;$ 제한조건을 가지는 최적화 문제"</span>.format(k))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/inequality_optimization_problem_plot.png" alt="부등식 최적화 문제"></p><h4 id="Scipy를-사용하여-부등식-제한조건이-있는-최적화-문제-계산하기"><a href="#Scipy를-사용하여-부등식-제한조건이-있는-최적화-문제-계산하기" class="headerlink" title="Scipy를 사용하여 부등식 제한조건이 있는 최적화 문제 계산하기"></a>Scipy를 사용하여 부등식 제한조건이 있는 최적화 문제 계산하기</h4><ul><li><code>fmin_slsqp()</code>명령은 이렇게 부등식 제한조건이 있는 경우에도 사용할 수 있다. 제한조건 인수의 이름이 <code>ieqcons</code>로 달라졌다. 단, <code>ieqcons</code> 인수에 들어가는 부등호는 우리가 지금까지 사용한 방식과 달리 0 또는 양수이어야 한다.</li></ul><script type="math/tex; mode=display">\begin{align} g \geq 0 \end{align}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fmin_slsqp(func_objective, x0, ieqcons=[func_constraint1, func_constraint2])</span><br></pre></td></tr></table></figure><ul><li>이렇듯, <code>fmin_slsqp()</code> 명령은 등식 제한조건과 부등식 제한조건을 동시에 사용할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def f2(x):</span><br><span class="line">    <span class="built_in">return</span> np.sqrt((x[0] - 4) ** 2 + (x[1] - 2) ** 2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 제한 조건 상수</span></span><br><span class="line">k = 1</span><br><span class="line">def ieq_constraint(x):</span><br><span class="line">    <span class="built_in">return</span> np.atleast_1d(k - np.sum(np.abs(x)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sp.optimize.fmin_slsqp(f2, np.array([0, 0]), ieqcons=[ieq_constraint])</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.    (Exit mode 0)</span><br><span class="line">            Current <span class="keyword">function</span> value: 3.6055512804550336</span><br><span class="line">            Iterations: 11</span><br><span class="line">            Function evaluations: 77</span><br><span class="line">            Gradient evaluations: 11</span><br><span class="line"></span><br><span class="line">array([9.99999982e-01, 1.79954011e-08])</span><br></pre></td></tr></table></figure><h1 id="서포트-벡터-머신"><a href="#서포트-벡터-머신" class="headerlink" title="서포트 벡터 머신"></a>서포트 벡터 머신</h1><ul><li>변수가 1개있다면, $x^T$는 N차원을 갖는 헹벡터라면, $\beta$도 같은 차원을 갖는 열벡터로서 내적을 통해 그에 따른 벡터의 영역이 초평면(hyperplane)을 이루게 될 것이다.</li></ul><p><img src="/image/SVM_conception_hyperplan_01.png" alt="SVM 정의"></p><ul><li><code>퍼셉트론</code>은 가장 단순하고 빠른 판별 함수 기반 분류 모형이지만 <code>판별 경계선(decision hyperplane)이 유니크하게 존재하지 않는다</code>는 특징이 있다. 서포트 벡터 머신(SVM: Support vector machine)은 퍼셉트론 기반의 모형에 가장 안정적인 판별 경계선을 찾기 위한 제한 조건을 추가한 모형이라고 볼 수 있다.</li></ul><p><img src="/image/svm_decision_boundary_hyperplane.png" alt="SVM의 decision boundary"></p><h3 id="서포트와-마진"><a href="#서포트와-마진" class="headerlink" title="서포트와 마진"></a>서포트와 마진</h3><ul><li>다음과 같이 $N$개의 학습용 데이터가 있다고 하자.</li></ul><script type="math/tex; mode=display">(x_{1}, y_{1}), (x_{2}, y_{2}), \ldots, (x_{i}, y_{i}), \ldots,(x_{N}, y_{N})</script><ul><li>판별함수 모형에서 $y$는 $+1,\; -1$ 두 개의 값을 가진다.</li></ul><script type="math/tex; mode=display">y = \begin{cases} +1 \\ -1 \end{cases}</script><ul><li>$x$ 데이터 중에서 $y$값이 $+1$인 데이터를 $x_{+}$, $y$값이 $-1$인 데이터를 $x_{-}$라고 하자. 판별함수 모형에서 직선인 판별 함수 $f(x)$는 다음과 같은 수식으로 나타낼 수 있다.</li></ul><script type="math/tex; mode=display">f(x) = w^Tx-w_0</script><ul><li>혹시라도 판별함수의 직선의 방정식이 어떻게 나온건지 이해가 안가시는 분들에게 설명을 드리고자 잠깐 선형대수의 직선의 방정식에 관한 설명을 하도록 하겠다.</li></ul><h3 id="직선의-방정식"><a href="#직선의-방정식" class="headerlink" title="직선의 방정식"></a>직선의 방정식</h3><ul><li><p>어떤 벡터 $w$가 있을 때</p><ul><li>원점에서 출발한 벡터 $w$가 가리키는 점을 지나면서</li><li>벡터 $w$에 수직인</li></ul></li><li><p>직선의 방정식을 구해보자.</p></li></ul><ul><li>위 두 조건을 만족하는 직선상의 임의의 점을 가리키는 벡터를 $x$라고 하면, 벡터 $x$가 가리키는 점과 벡터 $w$가 가리키는 점을 이은 벡터 $x - w$는 조건에 따라 벡터 $w$와 직교해야 한다. 따라서 다음 식이 성립한다.</li></ul><script type="math/tex; mode=display">\begin{align} w^T(x - w) = 0 \end{align}</script><ul><li>정리하면 다음과 같아진다.</li></ul><script type="math/tex; mode=display">\begin{align} w^T(x - w) = w^Tx - w^Tw = w^Tx - \| w \|^2 \end{align}</script><script type="math/tex; mode=display">\begin{align} w^Tx - \| w \|^2 = 0 \end{align}</script><ul><li>이 직선과 원점 사이의 거리는 벡터 $w$의 norm $|w|$이다.</li></ul><blockquote><p>연습문제) 만약 $v$가 원점을 지나는 직선의 방향을 나타내는 단위벡터라고 하자. 이때 그 직선 위에 있지 않는 어떤 점 $x$와 그 직선과의 거리의 제곱이 다음과 같음을 증명하라.</p></blockquote><script type="math/tex; mode=display">\begin{align} \| x \|^2 - (x^Tv)^2 \end{align}</script><ul><li><p>$ x \; - \; v \perp v$이기 때문에</p></li><li><p>$a^{\Vert b} = | x | cos \theta = \frac{| v | | x | cos \theta}{| v |} = \frac{x^{T} v}{| v |}$</p></li><li><p>벡터 $v$는 단위벡터이므로 $a^{\Vert b} = x^{T}v$가 된다. 여기서 피타고라스 정리를 사용하면 우리가 증명해야 하는 식을 구할 수 있다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">v = np.array([2, 1]) / np.sqrt(5)</span><br><span class="line">x = np.array([1, 3])</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=v, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.plot([-2, 8], [-1, 4], <span class="string">'b--'</span>, lw=2)</span><br><span class="line">plt.plot([1, 2], [3, 1], <span class="string">'g:'</span>, lw=2)</span><br><span class="line">plt.plot(x[0], x[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.text(0.1, 0.5, <span class="string">"<span class="variable">$v</span>$"</span>)</span><br><span class="line">plt.text(0.6, 3.2, <span class="string">"<span class="variable">$x</span>$"</span>)</span><br><span class="line">plt.xticks(np.arange(-3, 15))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-3, 7)</span><br><span class="line">plt.ylim(-1, 5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/line_with_distance_dot.png" alt="벡터 v 의 스칼라배한 직선위에 존재하지 않는 점과의 거리"></p><ul><li>예를 들어 아래와 같을 때</li></ul><script type="math/tex; mode=display">\begin{align} w = \begin{bmatrix}1 \\ 2\end{bmatrix} \tag{3.1.49} \end{align}</script><script type="math/tex; mode=display">\begin{align} \| w \|^2 = 5 \end{align}</script><script type="math/tex; mode=display">\begin{align} \begin{bmatrix}1 & 2\end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \end{bmatrix} - 5 = x_1 + 2x_2 - 5 = 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} x_1 + 2x_2 = 5 \end{align}</script><ul><li>이 방정식은 벡터 $w$가 가리키는 점 (1,2)를 지나면서 벡터 $w$에 수직인 직선을 뜻한다. 이 직선과 원점 사이의 거리는 $ |w|=\sqrt{5} $이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([1, 2])</span><br><span class="line">x1 = np.array([3, 1])</span><br><span class="line">x2 = np.array([-1, 3])</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=w, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x1, xytext=(0, 0), arrowprops=green)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x2, xytext=(0, 0), arrowprops=green)</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.plot(w[0], w[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot(x1[0], x1[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot(x2[0], x2[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot([-3, 5], [4, 0], <span class="string">'r-'</span>, lw=5)</span><br><span class="line">plt.text(-0.2, 1.5, <span class="string">"벡터 <span class="variable">$w</span>$"</span>)</span><br><span class="line">plt.text(1.55, 0.25, <span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.text(-0.9, 1.40, <span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.text(1.8, 1.8, <span class="string">"<span class="variable">$x_1</span> - w$"</span>)</span><br><span class="line">plt.text(-0.2, 2.8, <span class="string">"<span class="variable">$x_2</span> - w$"</span>)</span><br><span class="line">plt.text(3.6, 0.8, <span class="string">"직선 <span class="variable">$x</span>$"</span>)</span><br><span class="line">plt.xticks(np.arange(-2, 5))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-2, 5)</span><br><span class="line">plt.ylim(-0.6, 3.6)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/starting_zero_point_orthogonal_vector_distance.png" alt="원점에서 출발하는 벡터가 가리키는 점을 지나는 수직인 직선과의 거리"></p><ul><li>이번에는 벡터 $w$가 가리키는 점을 지나야 한다는 조건을 없애고 단순히<ul><li>벡터 $w$에 수직인</li></ul></li></ul><ul><li>직선 $x$의 방정식을 구하면 이때는 직선이 $w$가 아니라 $w$와 방향이 같고 길이가 다른 벡터 $w’=cw$을 지날 것이다. c는 양의 실수이다. 위에서 했던 방법으로 다시 직선의 방정식을 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} w'^Tx - \| w' \|^2 =  cw^Tx - c^2 \| w \|^2 = 0  \end{align}</script><script type="math/tex; mode=display">\begin{align} w^Tx - c \| w \|^2 = 0 \end{align}</script><ul><li>여기에서 $c | w |^2$는 임의의 수가 될 수 있으므로 단순히 벡터 $w$에 수직인 직선의 방정식은 다음과 같이 나타낼 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} w^Tx - w_0 = 0 \end{align}</script><ul><li>이 직선과 원점 사이의 거리는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} c \| w \| = \dfrac{w_0}{\|w\|}  \end{align}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([1, 2])</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=w, xytext=(0, 0), arrowprops=gray)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=0.5 * w, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.plot(0.5 * w[0], 0.5 * w[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot([-2, 5], [2.25, -1.25], <span class="string">'r-'</span>, lw=5)</span><br><span class="line">plt.text(-0.7, 0.8, <span class="string">"벡터 <span class="variable">$cw</span>$"</span>)</span><br><span class="line">plt.text(-0.1, 1.6, <span class="string">"벡터 <span class="variable">$w</span>$"</span>)</span><br><span class="line">plt.text(1, 1, <span class="string">"직선 <span class="variable">$x</span>$"</span>)</span><br><span class="line">plt.xticks(np.arange(-2, 5))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-2, 5)</span><br><span class="line">plt.ylim(-0.6, 3.6)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/orthogonal_vector_with_w.png" alt="벡터 w에 수직인 직선"></p><ul><li>예를 들어 $c=0.5$이면 벡터 $w=[1, 2]^T$에 수직이고 원점으로부터의 거리가 $\frac{\sqrt{5}}{2}$인 직선이 된다.</li></ul><script type="math/tex; mode=display">\begin{align} x_1 + 2x_2 - 2.5 = 0 \end{align}</script><h4 id="직선과-점의-거리"><a href="#직선과-점의-거리" class="headerlink" title="직선과 점의 거리"></a>직선과 점의 거리</h4><ul><li>이번에는 직선 $w^Tx - |w|^2 = 0$과 이 직선 위에 있지 않은 점 $x’$ 사이의 거리를 구해볼 것이다. 벡터 $w$에 대한 벡터 $x’$의 투영성분 $x’^{\Vert w}$의 길이는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} \|x'^{\Vert w}\| = \dfrac{w^Tx'}{\|w\|} \end{align}</script><ul><li>직선과 점 $x’$ 사이의 거리는 이 길이에서 원점에서 직선까지의 거리 $|w|$를 뺀 값의 절대값이다.</li></ul><script type="math/tex; mode=display">\begin{align} \left|  \|x'^{\Vert w}\| - \|w\| \right| = \left| \dfrac{w^Tx'}{\|w\|} - \|w\| \right| = \dfrac{\left|w^Tx' - \|w\|^2 \right|}{\|w\|} \end{align}</script><ul><li>직선의 방정식이 $w^Tx - w_0 = 0$이면 직선과 점의 거리는 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{align} \dfrac{\left|w^Tx' - w_0 \right|}{\|w\|} \end{align}</script><ul><li><code>이 공식은 아래 내용중 SVM의 판별함수의 직선과 서포트벡터간의 거리를 계산하는데에서 사용</code>된다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([1, 2])</span><br><span class="line">x1 = np.array([4, 3])</span><br><span class="line">x2 = np.array([1, 2]) * 2</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x1, xytext=(0, 0), arrowprops=gray)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x2, xytext=(0, 0), arrowprops=gray)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=w, xytext=(0, 0), arrowprops=red)</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.plot(w[0], w[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot(x1[0], x1[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot([-3, 7], [4, -1], <span class="string">'r-'</span>, lw=5)</span><br><span class="line">plt.plot([2, 4], [4, 3], <span class="string">'k:'</span>, lw=2)</span><br><span class="line">plt.plot([3, 4], [1, 3], <span class="string">'k:'</span>, lw=2)</span><br><span class="line">plt.text(0.1, 0.9, <span class="string">"<span class="variable">$w</span>$"</span>)</span><br><span class="line">plt.text(4.2, 3.1, <span class="string">"<span class="variable">$x</span>'$"</span>)</span><br><span class="line">plt.text(1.5, 2.4, <span class="string">"<span class="variable">$x</span>'^&#123;\Vert w&#125;$"</span>)</span><br><span class="line">plt.xticks(np.arange(-3, 15))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-3, 7)</span><br><span class="line">plt.ylim(-1, 5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/distance_between_dots_and_line.png" alt="점과 직선사이의 거리"></p><ul><li>다시 SVM 판별함수를 살펴보면 정의에 따라 $y$값이 $+1$인 그 데이터 $x_{+}$에 대한 판별함수 값은 양수가 된다.</li></ul><script type="math/tex; mode=display">f(x_+) = w^Tx_+ - w_0 > 0</script><ul><li>반대로 y값이 -1인 그 데이터 $x_{-}$에 대한 판별함수 값은 음수가 된다.</li></ul><script type="math/tex; mode=display">f(x_-) = w^Tx_- - w_0 < 0</script><ul><li>$y$ 값이  $+1$인 데이터 중에서 판별 함수의 값이 가장 작은 데이터를 $x^{+}$라고 하고 $y$값이 $-1$인 데이터 중에서 판별함수의 값이 가장 큰 데이터를 $x^{-}$라고 하자. 이 데이터들은 각각의 클래스에 속한 데이터 중에서 가장 경계선에 가까이 붙어있는 최전방(most front)의 데이터들이다. 이러한 데이터를 서포트(support) 혹은 서포트 벡터(support vector)라고 한다. 물론 이 서포트에 대해서도 부호 조건은 만족되어야 한다.</li></ul><script type="math/tex; mode=display">f(x^+) = w^Tx^+ - w_0 > 0</script><script type="math/tex; mode=display">f(x^-) = w^Tx^- - w_0 < 0</script><ul><li>서포트에 대한 판별 함수의 값 $f(x^{+}), f(x^{-})$값은 부호 조건만 지키면 어떤 값이 되어도 괜찮다, 따라서 다음과 같은 조건을 만족하도록 판별 함수를 구한다.</li></ul><script type="math/tex; mode=display">f(x^+) = w^T x^{+} - w_0 = +1</script><script type="math/tex; mode=display">f(x^-) = w^T x^{-} - w_0 = -1</script><p><img src="/image/decision_function_value_of_SVM.png" alt="Support vector의 판별함수 값"></p><ul><li>이렇게 되면 모든 Support vector $(x_{+}, x_{-})$  데이터들에 대한 판별함수의 값의 절대값이 1보다 커지므로 다음 부등식이 성립한다.</li></ul><script type="math/tex; mode=display">w^Tx_+ - w_o \geq 1</script><script type="math/tex; mode=display">w^Tx_- - w_o \leq -1</script><ul><li>판별 경계선 $w^{T}x -  w_{0} = 0$과 점 $x^{+}, x^{-}$ 사이의 거리는 다음과 같이 계산할 수 있다.</li></ul><script type="math/tex; mode=display">\dfrac{w^T x^{+} - w_0}{\| w \|} = \dfrac{1}{\| w \|}</script><script type="math/tex; mode=display">-\dfrac{w^T x^{-} - w_0}{\| w \|} = \dfrac{1}{\| w \|}</script><ul><li>이 거리의 합을 마진(margin)이라고 하며 마진값이 클 수록 더 경계선이 안정적이라고 볼 수 있다. 그런데 위에서 정한 스케일링에 의해 마진은 다음과 같이 정리된다.</li></ul><script type="math/tex; mode=display">\dfrac{w^T x^{+} - w_0}{\| w \|}  -\dfrac{w^T x^{-} - w_0}{\| w \|} = \dfrac{2}{\| w \|}</script><ul><li>마진 값이 최대가 되는 경우는 $| w |$ 즉, $| w |^{2}$가 최소가 되는 경우와 같다. 다음과 같은 목적함수를 최소화하면 된다.</li></ul><script type="math/tex; mode=display">L = \dfrac{1}{2} ||w||^2 = \dfrac{1}{2} w^T w</script><ul><li>또한 모든 표본 데이터에 대해 분류는 제대로 되어야 하므로 모든 데이터 $x_{i}, y_{i} (i = 1,\ldots, N)$에 대해 다음 조건을 만족해야 한다. 위에서 스케일링을 사용하여 모든 데이터에 대해 $f(x_i) = w^Tx_i - w_o$가 $1$보다 크거나 $-1$보다 작게 만들었다는 점을 이용한다.</li></ul><script type="math/tex; mode=display">y_i \cdot f(x_i) = y_i \cdot( w^Tx_i - w_o) \geq 1 \;\;\; ( i = 1, \ldots, N )</script><script type="math/tex; mode=display">y_i \cdot ( w^Tx_i - w_o) - 1 \geq 0 \;\;\; ( i = 1, \ldots, N )</script><ul><li><code>라그랑주 승수법을 사용하면 최소화 목적함수를 다음과 같이 고치면 된다.</code> 즉, 위의 조건을 만족하는 w의 최소화 문제를 푸는 것과 같게 된다. $a_{i}$은 각각의 부등식에 대한 라그랑주 승수이다. 이 최적화 문제를 풀어 $w, w_{0}, a$를 구하면 판별함수를 얻을 수 있다.</li></ul><script type="math/tex; mode=display">L = \dfrac{1}{2} w^T w - \sum_{i=1}^N a_i \{ y_i \cdot ( w^Tx_i - w_o) - 1 \}</script><ul><li>KKT(Karush-Kuhn-Tucker) 조건에 따르면 부등식 제한 조건이 있는 경우에는 등식 제한조건을 가지는 라그랑주 승수 방법과 비슷하지만 $i$번째 부등식이 있으나 없으나 답이 같은 경우에는 해당 라그랑주 승수의 값이 $a_{i}=0$이 된다. 이 경우는 판별함수의 값 $w^Tx_i - w_o$이 $-1$보다 작거나 1보다 큰 경우이다.<br>즉, <code>마진안에 포함되지 않고 바깥에 있는 데이터들 같은 경우는 해당 조건식이 해를 찾는데 영향을 주지 않아 등식이 있는 최적화 문제를 푸는 것과 같다는 의미</code>이다.</li></ul><script type="math/tex; mode=display">y_i(w^Tx_i - w_o) - 1  > 0</script><ul><li>학습 데이터 중에서 최전방 데이터인 서포트 벡터가 아닌 모든 데이터들에 대해서는 이 조건이 만족되므로 서포트 벡터가 아닌 데이터는 라그랑지 승수가 $0$이라는 것을 알 수 있다.</li></ul><script type="math/tex; mode=display">a_i = 0 \;\; \text{if} \;\; x_i \notin \{ x^{+}, x^{-} \}</script><h3 id="듀얼-형식"><a href="#듀얼-형식" class="headerlink" title="듀얼 형식"></a>듀얼 형식</h3><ul><li>최적화 조건은 목적함수 $L$을 $w, w_{0}$로 미분한 값이 0이 되어야 하는 것이다.</li></ul><script type="math/tex; mode=display">\dfrac{\partial L}{\partial w} = 0</script><script type="math/tex; mode=display">\dfrac{\partial L}{\partial w_0} = 0</script><ul><li>이 식을 풀어서 정리하면 다음과 같아진다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} \dfrac{\partial L}{\partial w} &=& \dfrac{\partial}{\partial w} \left( \dfrac{1}{2} w^T w \right) - \dfrac{\partial}{\partial w} \sum_{i=1}^N \left( a_i y_i w^Tx_i - a_i y_i w_o - a_i \right) \\ &=& w - \sum_{i=1}^N  a_i y_i x_i \\ &=& 0 \end{eqnarray}</script><script type="math/tex; mode=display">\begin{eqnarray} \dfrac{\partial L}{\partial w_0} &=& \dfrac{\partial}{\partial w_0} \left( \dfrac{1}{2} w^T w \right) - \dfrac{\partial}{\partial w_0} \sum_{i=1}^N \left( a_i y_i w^Tx_i - a_i y_i w_o - a_i \right) \\&=& \sum_{i=1}^N  a_i y_i \\ &=& 0 \end{eqnarray}</script><ul><li>정리해보면, 다음과 같다.</li></ul><script type="math/tex; mode=display">w = \sum_{i=1}^N a_i y_i x_i</script><script type="math/tex; mode=display">0 = \sum_{i=1}^N a_i y_i</script><ul><li>이 두 수식을 원래의 목적함수에 대입하여 $w, w_{0}$을 없애면 다음과 같다.</li></ul><script type="math/tex; mode=display">\begin{eqnarray} L &=& \dfrac{1}{2} w^T w - \sum_{i=1}^N a_i \{ y_i \cdot ( w^Tx_i - w_o) - 1 \}  \\ &=& \dfrac{1}{2} \left( \sum_{i=1}^N a_i y_i x_i \right)^T \left( \sum_{j=1}^N a_j y_j x_j \right) - \sum_{i=1}^N a_i \left\{ y_i \cdot \left( \left( \sum_{j=1}^N a_j y_j x_j \right)^Tx_i - w_o \right) - 1 \right\}  \\ &=& \dfrac{1}{2} \sum_{i=1}^N \sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j - \sum_{i=1}^N \sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j + w_0 \sum_{i=1}^N a_i y_i + \sum_{i=1}^N a_i   \\ &=& \sum_{i=1}^N a_i - \dfrac{1}{2}\sum_{i=1}^N\sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j \end{eqnarray}</script><script type="math/tex; mode=display">L = \sum_{i=1}^N a_i - \dfrac{1}{2}\sum_{i=1}^N\sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j</script><ul><li>이 떄 $a$는 다음 조건을 만족한다.</li></ul><script type="math/tex; mode=display">\sum_{i=1}^N a_i y_i = 0</script><script type="math/tex; mode=display">a_i \geq 0 \;\;\;  ( i = 1, \ldots, N )</script><ul><li>이 문제는 $w$를 구하는 문제가 아니라 $a$만을 구하는 문제로 바뀌었으므로 듀얼형식(dual form)이라고 한다. 듀얼형식으로 바꾸면 수치적으로 박스(Box)제한 조건이 있는 이차프로그래밍(QP: Quadratic programming)문제가 되므로 원래의 문제보다는 효율적으로 풀 수 있다.</li></ul><p><a href="https://datascienceschool.net/view-notebook/0fca28c71c13460fb7168ee2adb9a8be/" target="_blank" rel="noopener">선형계획법 문제와 이차계획법 문제</a></p><ul><li>듀얼 형식 문제를 풀어 함수 $L$을 최소화하는 $a$를 구하면 예측 모형을 다음과 같이 쓸 수 있다.</li></ul><script type="math/tex; mode=display">f(x) = w^T x - w_0 = \sum_{i=1}^N a_i y_i x_i^T x - w_0</script><ul><li>$w_{0}$는 아래와 같이 구한다.</li></ul><script type="math/tex; mode=display">w_0 = w^T x^{+} - 1 또는 w_0 = w^T x^{-} + 1 또는 w_0 = \dfrac{1}{2} w^T (x^+ + x^{-})</script><ul><li>라그랑주 승수 값이 0 즉, $a_{i} = 0$이면 해당 데이터는 예측 모형, 즉 $w$ 계산에 아무런 기여를 하지 않으므로 위의 식은 실제로는 다음과 같다.</li></ul><script type="math/tex; mode=display">f(x) = a^+ x^T x^+ - a^- x^T x^- - w_0</script><ul><li>여기에서 $x^{T}x^{+}$는 $x$와 $x^{+}$ 사이의 코사인 유사도, $x^{T}x^{-}$는 $x$와 $x^{-}$ 사이의 코사인 유사도이므로 결국 두 <code>서포트 벡터와의 유사도를 측정해서 값이 큰쪽으로 판별</code>하게 된다.</li></ul><h3 id="Scikit-Learn의-서포트-벡터-머신"><a href="#Scikit-Learn의-서포트-벡터-머신" class="headerlink" title="Scikit-Learn의 서포트 벡터 머신"></a>Scikit-Learn의 서포트 벡터 머신</h3><ul><li>Scikit-Learn의 <code>svm</code> 서브패키지는 서포트 벡터 머신 모형인 <code>SVC</code>(Support Vector Classifier) 클래스를 제공한다. 이와 동시에 SVR(Support Vector Regressor)도 제공을 하지만 SVR은 추후에 설명하고 먼저, SVC에 대해 다루어 볼 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">X, y = make_blobs(n_samples=50, centers=2, cluster_std=0.5, random_state=4)</span><br><span class="line">y = 2 * y - 1</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y == -1, 0], X[y == -1, 1], marker=<span class="string">'o'</span>, label=<span class="string">"-1 클래스"</span>)</span><br><span class="line">plt.scatter(X[y == +1, 0], X[y == +1, 1], marker=<span class="string">'x'</span>, label=<span class="string">"+1 클래스"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">"학습용 데이터"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/Support_Vector_Machine_train_data_set.png" alt="Support Vector Machine train data set"></p><ul><li><code>SVC</code> 클래스는 커널(Kernel)을 선택하는 인수 <code>kernel</code>과 슬랙변수 가중치(slack variable weight)를 선택하는 인수 <code>C</code>를 받는데 지금까지 공부한 서포트 벡터 머신을 사용하려면 인수를 다음처럼 넣어준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.svm import SVC</span><br><span class="line">model = SVC(kernel=<span class="string">'linear'</span>, C=1e10).fit(X, y)</span><br></pre></td></tr></table></figure><ul><li><code>SVC</code>를 사용하여 모형을 구하면 다음과 같은 속성값을 가진다.<ul><li><code>n_support</code> : 각 클래스의 서포트 벡터의 개수</li><li><code>support</code> : 각 클래스의 서포트 벡터의 인덱스</li><li><code>support_vectors_</code> : 각 클래스의 서포트의 $x$값.$(x^{T}, x^{-})$</li><li><code>coef</code> : $w$벡터</li><li><code>intercept</code> : $- w_{0}$</li><li><code>dual_coef</code> : 각 원소가 $a_{i} \dot y_{i}$로 이루어진 벡터</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.n_support_</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1, 1], dtype=int32)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.support_</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([42,  1], dtype=int32)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.support_vectors_</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[9.03715314, 1.71813465],</span><br><span class="line">       [9.17124955, 3.52485535]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y[model.support_]</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-1,  1])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">xmin = X[:, 0].min()</span><br><span class="line">xmax = X[:, 0].max()</span><br><span class="line">ymin = X[:, 1].min()</span><br><span class="line">ymax = X[:, 1].max()</span><br><span class="line">xx = np.linspace(xmin, xmax, 10)</span><br><span class="line">yy = np.linspace(ymin, ymax, 10)</span><br><span class="line">X1, X2 = np.meshgrid(xx, yy)</span><br><span class="line"></span><br><span class="line">Z = np.empty(X1.shape)</span><br><span class="line"><span class="keyword">for</span> (i, j), val <span class="keyword">in</span> np.ndenumerate(X1):</span><br><span class="line">    x1 = val</span><br><span class="line">    x2 = X2[i, j]</span><br><span class="line">    p = model.decision_function([[x1, x2]])</span><br><span class="line">    Z[i, j] = p[0]</span><br><span class="line">levels = [-1, 0, 1]</span><br><span class="line">linestyles = [<span class="string">'dashed'</span>, <span class="string">'solid'</span>, <span class="string">'dashed'</span>]</span><br><span class="line">plt.scatter(X[y == -1, 0], X[y == -1, 1], marker=<span class="string">'o'</span>, label=<span class="string">"-1 클래스"</span>)</span><br><span class="line">plt.scatter(X[y == +1, 0], X[y == +1, 1], marker=<span class="string">'x'</span>, label=<span class="string">"+1 클래스"</span>)</span><br><span class="line">plt.contour(X1, X2, Z, levels, colors=<span class="string">'k'</span>, linestyles=linestyles)</span><br><span class="line">plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, alpha=0.3)</span><br><span class="line"></span><br><span class="line">x_new = [10, 2]</span><br><span class="line">plt.scatter(x_new[0], x_new[1], marker=<span class="string">'^'</span>, s=100)</span><br><span class="line">plt.text(x_new[0] + 0.03, x_new[1] + 0.08, <span class="string">"테스트 데이터"</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">"SVM 예측 결과"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/prediction_of_test_data_svm.png" alt="SVM 모델 test data 예측 결과"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = [10, 2]</span><br><span class="line">model.decision_function([x_new])</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-0.61101582])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.coef_.dot(x_new) + model.intercept_</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-0.61101582])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dual_coef_ = a_i * y_i</span></span><br><span class="line">model.dual_coef_</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[-0.60934379,  0.60934379]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.dual_coef_[0][0] * model.support_vectors_[0].dot(x_new) + \</span><br><span class="line">    model.dual_coef_[0][1] * model.support_vectors_[1].dot(x_new) + \</span><br><span class="line">    model.intercept_</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-0.61101582])</span><br></pre></td></tr></table></figure><blockquote><p>iris 문제를 서포트 벡터 머신으로 풀어보자. 다음과 같은 데이터만 사용한 이진 분류 문제로 바꾸어 풀어본다. 위의 예제와 마찬가지로 커널 인수 <code>kernel</code>과 슬랙변수 가중치 인수 <code>C</code>는 각각 <code>linear</code>, <code>1e10</code>으로 한다.</p></blockquote><pre><code>- 특징 변수를 꽃받침의 길이와 폭만 사용한다.- 붓꽆 종을 Setosa와 Versicolour만 대상으로 한다.</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix, classification_report</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X_data = iris.data[(iris.target == 0) | (iris.target == 1), :2]</span><br><span class="line">y = iris.target[(iris.target == 0) | (iris.target == 1)]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.3)</span><br><span class="line">svm = SVC(kernel=<span class="string">"linear"</span>, C=1e10)</span><br><span class="line">svm.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,</span><br><span class="line">    decision_function_shape=<span class="string">'ovr'</span>, degree=3, gamma=<span class="string">'auto_deprecated'</span>,</span><br><span class="line">    kernel=<span class="string">'linear'</span>, max_iter=-1, probability=False, random_state=None,</span><br><span class="line">    shrinking=True, tol=0.001, verbose=False)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred_y = svm.predict(X_test)</span><br><span class="line">confusion_matrix(pred_y, y_test)</span><br></pre></td></tr></table></figure><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[14,  0],</span><br><span class="line">       [ 0, 16]])</span><br></pre></td></tr></table></figure><ul><li><code>위의 조건에서 kernel=&quot;linear&quot;로 유지한채 C값만 [0.01, 0.1, 1, 10, 100]으로 변화를 주며 결과를 살펴보니 C값이 높아지면 Slack 변수로 줄 수 있는 값이 줄어들어 서포트 벡터의 수가 줄어든다. 반대로 C값을 낮추어 줄수록 Slack 변수가 갖는 값이 크게 되어 서포트 벡터는 많아지며 마진이 줄어든다.</code></li></ul><h3 id="슬랙변수"><a href="#슬랙변수" class="headerlink" title="슬랙변수"></a>슬랙변수</h3><ul><li><p>만약 데이터가 직선인 판별 경계선으로 나누어지지 않는 즉, 선형분이(linear seperable)가 불가능한 경우에는 다음과 같이 슬랙변수(slack variable)를 사용하여 개별적인 오차를 허용할 수 있다.</p></li><li><p>원래 판별 함수의 값은 클래스 $x^{T}$ 영역의 샘플 $x_{+}$에 대해선 첫번째 수식과 같고, 클래스 -1 영역의 샘플 $x_{-}$에 대해서는 두 번째 수식과 같아야한다.</p></li></ul><script type="math/tex; mode=display">w^Tx_+ - w_0 \geq 1</script><script type="math/tex; mode=display">w^Tx_- - w_0 \leq -1</script><ul><li>양수인 슬랙변수 $\xi \geq 0$를 사용하면 이 조건을 다음과 같이 완화할 수 있다.</li></ul><script type="math/tex; mode=display">w^Tx_+ - w_0 \geq +1-\xi_i</script><script type="math/tex; mode=display">w^Tx_- - w_0 \leq -1+\xi_i</script><ul><li>모든 슬랙변수는 0보다 같거나 크다.</li></ul><script type="math/tex; mode=display">\xi_i \geq 0 \;\;\; (i=1, \ldots, N)</script><ul><li>위의 부등식 조건을 모두 고려한 최적화 목적함수는 다음과 같아진다. 아래 식에서 $C \sum_{i=1}^N \xi_i$ 항은 슬랙변수의 합이 너무 커지지 않도록 제한하는 역할을 한다.</li></ul><script type="math/tex; mode=display">L = \dfrac{1}{2} ||w||^2 - \sum_{i=1}^N a_i (y_i \cdot ( w^Tx_i - w_o) - 1 + \xi_i ) - \sum_{i=1}^N \mu_i \xi_i  + C \sum_{i=1}^N \xi_i</script><p><img src="/image/Slack_variable_C_difference_each_value.png" alt="슬랙변수의 C값에 따른 오차 허용의 차이"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(0)</span><br><span class="line">X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]</span><br><span class="line">Y = [-1] * 20 + [1] * 20</span><br><span class="line"></span><br><span class="line">plotnum = 1</span><br><span class="line"><span class="keyword">for</span> name, penalty <span class="keyword">in</span> ((<span class="string">'C=10'</span>, 10), (<span class="string">'C=0.1'</span>, 0.1)):</span><br><span class="line">    clf = SVC(kernel=<span class="string">'linear'</span>, C=penalty).fit(X, Y)</span><br><span class="line">    xx = np.linspace(-5, 5)</span><br><span class="line"></span><br><span class="line">    x_jin = -5</span><br><span class="line">    x_jax = 5</span><br><span class="line">    y_jin = -9</span><br><span class="line">    y_jax = 9</span><br><span class="line">    XX, YY = np.mgrid[x_jin:x_jax:200j, y_jin:y_jax:200j]</span><br><span class="line"></span><br><span class="line">    levels = [-1, 0, 1]</span><br><span class="line">    linestyles = [<span class="string">'dashed'</span>, <span class="string">'solid'</span>, <span class="string">'dashed'</span>]</span><br><span class="line">    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])</span><br><span class="line">    Z = Z.reshape(XX.shape)</span><br><span class="line"></span><br><span class="line">    plt.subplot(1, 2, plotnum)</span><br><span class="line">    plt.contour(XX, YY, Z, levels, colors=<span class="string">'k'</span>, linestyles=linestyles)</span><br><span class="line">    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=120, linewidth=4)</span><br><span class="line">    plt.scatter(X[:, 0], X[:, 1], c=Y, s=60, linewidth=1, cmap=plt.cm.Paired)</span><br><span class="line">    plt.xlim(x_jin, x_jax)</span><br><span class="line">    plt.ylim(y_jin, y_jax)</span><br><span class="line">    plt.title(name)</span><br><span class="line"></span><br><span class="line">    plotnum += 1</span><br><span class="line"></span><br><span class="line">plt.suptitle(<span class="string">"슬랙변수 가중치 C의 영향"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/influence_of_weighted_value_C_with_slack_variables.png" alt="슬랙변수 가중치 C의 영향"></p><ul><li>다시 한번 정리하자면, 아래 그림에서 살펴보면 직선의 방정식 $ x^{T} \beta + \beta_{0} $와 각 서포트 벡터 $ x(x^{+}, x^{-}) $와의 거리가 $ \frac{1}{\beta}$ 이므로 결굴 마진을 크게 하는 것은 $ \beta $를 작게 하는 것과 동일한 의미이다. 그러한 측면에서도 위에서 자세히 언급했듯이 Cost function(목적함수, 비용함수)가 아래와 같이 나온다는 것을 알 수 있다. 여기서 동시에 error를 최소화하고 싶으므로 라그랑주 승수 $ C $를 크게가져가면서 error를 허용하는 slack변수를 최소화하는 동시에 $ \beta $ 의 값도 최소화하는 optimization 문제를 풀 수 있다.</li></ul><p><img src="/image/calculate_svm.png" alt="SVM 계산 - 01"></p><ul><li>위에서의 조건하에 최적화를 하는 것이므로 라그랑주 승수를 도입하여 부등식이 있는 최적화 문제를 풀게 된다.</li></ul><p><img src="/image/calculate_svm_01.png" alt="SVM 계산 - 02"></p><p><img src="/image/calculate_svm_02.png" alt="SVM 계산 - 03"></p><ul><li>KKT조건을 만족함으로서, <code>global minimum을 보장</code>받을 수 있다.</li></ul><p><img src="/image/calculate_svm_03.png" alt="SVM 계산 - 04"></p><ul><li>즉, KKT의 2번째 조건에 의해서 서포트벡터인 경우는 조건식이 의미가 있기 때문에 라그랑지 승수 $\alpha_{i} \neq 0$이 된다는 의미이다.</li></ul><p><img src="/image/calculate_svm_04.png" alt="SVM 계산 - 05"></p><blockquote><p>얼굴 이미지 인식</p></blockquote><ul><li>총 40명이 각각 10장의 조금씩 다른 표정이나 모습으로 찍은 이미지 데이터이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_olivetti_faces</span><br><span class="line">faces = fetch_olivetti_faces()</span><br><span class="line"></span><br><span class="line">N = 2</span><br><span class="line">M = 5</span><br><span class="line">np.random.seed(0)</span><br><span class="line">fig = plt.figure(figsize=(9, 5))</span><br><span class="line">plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05)</span><br><span class="line">klist = np.random.choice(range(len(faces.data)), N * M)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(M):</span><br><span class="line">        k = klist[i * M + j]</span><br><span class="line">        ax = fig.add_subplot(N, M, i * M + j + 1)</span><br><span class="line">        ax.imshow(faces.images[k], cmap=plt.cm.bone)</span><br><span class="line">        ax.grid(False)</span><br><span class="line">        ax.xaxis.set_ticks([])</span><br><span class="line">        ax.yaxis.set_ticks([])</span><br><span class="line">        plt.title(faces.target[k])</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/fetch_olivetti_faces_01.png" alt="랜덤하게 뽑은 이미지"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(faces.data, faces.target, test_size=0.4, random_state=0)</span><br><span class="line"></span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">svc = SVC(kernel=<span class="string">'linear'</span>).fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">N = 2</span><br><span class="line">M = 5</span><br><span class="line">np.random.seed(4)</span><br><span class="line">fig = plt.figure(figsize=(9, 5))</span><br><span class="line">plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05)</span><br><span class="line">klist = np.random.choice(range(len(y_test)), N * M)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(M):</span><br><span class="line">        k = klist[i * M + j]</span><br><span class="line">        ax = fig.add_subplot(N, M, i * M + j + 1)</span><br><span class="line">        ax.imshow(X_test[k:(k + 1), :].reshape(64, 64), cmap=plt.cm.bone)</span><br><span class="line">        ax.grid(False)</span><br><span class="line">        ax.xaxis.set_ticks([])</span><br><span class="line">        ax.yaxis.set_ticks([])</span><br><span class="line">        plt.title(<span class="string">"%d =&gt; %d"</span> %</span><br><span class="line">                  (y_test[k], svc.predict(X_test[k:(k + 1), :])[0]))</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/fetch_olivetti_faces_02.png" alt="랜덤하게 뽑은 이미지의 예측"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import classification_report, accuracy_score</span><br><span class="line"></span><br><span class="line">y_pred_train = svc.predict(X_train)</span><br><span class="line">y_pred_test = svc.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_train, y_pred_train))</span><br></pre></td></tr></table></figure><h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       1.00      1.00      1.00         4</span><br><span class="line">           1       1.00      1.00      1.00         5</span><br><span class="line">           2       1.00      1.00      1.00         6</span><br><span class="line">           3       1.00      1.00      1.00         8</span><br><span class="line">           4       1.00      1.00      1.00         8</span><br><span class="line">           5       1.00      1.00      1.00         5</span><br><span class="line">           6       1.00      1.00      1.00         4</span><br><span class="line">           7       1.00      1.00      1.00         7</span><br><span class="line">           8       1.00      1.00      1.00         8</span><br><span class="line">           9       1.00      1.00      1.00         7</span><br><span class="line">          10       1.00      1.00      1.00         4</span><br><span class="line">          11       1.00      1.00      1.00         6</span><br><span class="line">          12       1.00      1.00      1.00         6</span><br><span class="line">          13       1.00      1.00      1.00         6</span><br><span class="line">          14       1.00      1.00      1.00         4</span><br><span class="line">          15       1.00      1.00      1.00         4</span><br><span class="line">          16       1.00      1.00      1.00         8</span><br><span class="line">          17       1.00      1.00      1.00         4</span><br><span class="line">          18       1.00      1.00      1.00         9</span><br><span class="line">          19       1.00      1.00      1.00         4</span><br><span class="line">          20       1.00      1.00      1.00         9</span><br><span class="line">          21       1.00      1.00      1.00         6</span><br><span class="line">          22       1.00      1.00      1.00         7</span><br><span class="line">          23       1.00      1.00      1.00         5</span><br><span class="line">          24       1.00      1.00      1.00         6</span><br><span class="line">          25       1.00      1.00      1.00         5</span><br><span class="line">          26       1.00      1.00      1.00         5</span><br><span class="line">          27       1.00      1.00      1.00         8</span><br><span class="line">          28       1.00      1.00      1.00         6</span><br><span class="line">          29       1.00      1.00      1.00         4</span><br><span class="line">          30       1.00      1.00      1.00         6</span><br><span class="line">          31       1.00      1.00      1.00         5</span><br><span class="line">          32       1.00      1.00      1.00         6</span><br><span class="line">          33       1.00      1.00      1.00         7</span><br><span class="line">          34       1.00      1.00      1.00         4</span><br><span class="line">          35       1.00      1.00      1.00         7</span><br><span class="line">          36       1.00      1.00      1.00         6</span><br><span class="line">          37       1.00      1.00      1.00         6</span><br><span class="line">          38       1.00      1.00      1.00         9</span><br><span class="line">          39       1.00      1.00      1.00         6</span><br><span class="line"></span><br><span class="line">    accuracy                           1.00       240</span><br><span class="line">   macro avg       1.00      1.00      1.00       240</span><br><span class="line">weighted avg       1.00      1.00      1.00       240</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred_test))</span><br></pre></td></tr></table></figure><h5 id="결과-14"><a href="#결과-14" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.86      1.00      0.92         6</span><br><span class="line">           1       1.00      1.00      1.00         5</span><br><span class="line">           2       1.00      1.00      1.00         4</span><br><span class="line">           3       0.50      1.00      0.67         2</span><br><span class="line">           4       1.00      0.50      0.67         2</span><br><span class="line">           5       1.00      1.00      1.00         5</span><br><span class="line">           6       0.83      0.83      0.83         6</span><br><span class="line">           7       1.00      0.67      0.80         3</span><br><span class="line">           8       0.67      1.00      0.80         2</span><br><span class="line">           9       1.00      1.00      1.00         3</span><br><span class="line">          10       1.00      1.00      1.00         6</span><br><span class="line">          11       1.00      1.00      1.00         4</span><br><span class="line">          12       0.67      1.00      0.80         4</span><br><span class="line">          13       1.00      1.00      1.00         4</span><br><span class="line">          14       1.00      1.00      1.00         6</span><br><span class="line">          15       1.00      0.33      0.50         6</span><br><span class="line">          16       0.67      1.00      0.80         2</span><br><span class="line">          17       1.00      1.00      1.00         6</span><br><span class="line">          18       1.00      1.00      1.00         1</span><br><span class="line">          19       1.00      1.00      1.00         6</span><br><span class="line">          20       1.00      1.00      1.00         1</span><br><span class="line">          21       1.00      0.75      0.86         4</span><br><span class="line">          22       1.00      1.00      1.00         3</span><br><span class="line">          23       0.71      1.00      0.83         5</span><br><span class="line">          24       1.00      1.00      1.00         4</span><br><span class="line">          25       1.00      1.00      1.00         5</span><br><span class="line">          26       1.00      1.00      1.00         5</span><br><span class="line">          27       1.00      1.00      1.00         2</span><br><span class="line">          28       1.00      1.00      1.00         4</span><br><span class="line">          29       1.00      1.00      1.00         6</span><br><span class="line">          30       1.00      1.00      1.00         4</span><br><span class="line">          31       1.00      1.00      1.00         5</span><br><span class="line">          32       1.00      1.00      1.00         4</span><br><span class="line">          33       1.00      1.00      1.00         3</span><br><span class="line">          34       1.00      0.83      0.91         6</span><br><span class="line">          35       1.00      0.67      0.80         3</span><br><span class="line">          36       1.00      1.00      1.00         4</span><br><span class="line">          37       1.00      1.00      1.00         4</span><br><span class="line">          38       0.50      1.00      0.67         1</span><br><span class="line">          39       0.67      0.50      0.57         4</span><br><span class="line"></span><br><span class="line">    accuracy                           0.93       160</span><br><span class="line">   macro avg       0.93      0.93      0.91       160</span><br><span class="line">weighted avg       0.95      0.93      0.92       160</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/22/machine_learning_11/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
