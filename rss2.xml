<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DataLatte&#39;s IT Blog</title>
    <link>https://heung-bae-lee.github.io/</link>
    
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>DataLatte가 Data Science 공부를 하면서 정리해 놓는 블로그</description>
    <pubDate>Fri, 10 Jan 2020 17:41:42 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>Python 웹 크롤링 - Scrapy 01 환경설정 및 기초</title>
      <link>https://heung-bae-lee.github.io/2020/01/09/Crawling_00/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/09/Crawling_00/</guid>
      <pubDate>Thu, 09 Jan 2020 12:08:12 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Scrapy-VS-Beautiful-Soup&quot;&gt;&lt;a href=&quot;#Scrapy-VS-Beautiful-Soup&quot; class=&quot;headerlink&quot; title=&quot;Scrapy VS Beautiful Soup&quot;&gt;&lt;/a&gt;Scrapy VS Beau
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Scrapy-VS-Beautiful-Soup"><a href="#Scrapy-VS-Beautiful-Soup" class="headerlink" title="Scrapy VS Beautiful Soup"></a>Scrapy VS Beautiful Soup</h2><h3 id="Beautiful-Soup"><a href="#Beautiful-Soup" class="headerlink" title="Beautiful Soup"></a>Beautiful Soup</h3><ul><li>Beautiful Soup는 웹 상의 정보를 빠르게 크롤링 하기위한 도구이며, <code>정적인 정보를 가져 올 수 있다. 즉, 해당 API(URL)에 요청했을때 바로 가져올수 있는 정보들만 가져올 수 있다. 시간이 좀 더 걸린 후에 나오는 정보들은 가져올 수 없다는 것이다.</code> 진입 장벽이 매우 낮고 간결해서, 입문 개발자에게 안성맞춤이다. 그리고 이 라이브러리는 스스로 크롤링을 하는 것이 아니라 <code>urlib2</code> 또는 <code>requests</code> 모듈을 통해 HTML 소스를 가져와야 한다.</li></ul><h3 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h3><ul><li>Scrapy는 Python으로 작성된 Framework이며, spider(bot)을 작성해서 크롤링을 한다. Scrapy에서는 직접 <code>Beautiful Soup</code> 이나 <code>lxml</code>을 사용할 수 있다. 하지만 Beautiful Soup에서는 지원하지 않는 <code>Xpath</code>를 사용할 수 있다. 또한, Xpath를 사용함으롴써 복잡한 HTML소스를 쉽게 크롤링 할 수 있게 해준다. 또한 Xpath를 통한 crawling이 가능한 모듈로는 selenium도 존재한다. selenium도 Scrapy와 연동해서 가능하다.</li></ul><h3 id="Anaconda-env"><a href="#Anaconda-env" class="headerlink" title="Anaconda env"></a>Anaconda env</h3><ul><li>먼저 사전에 anaconda를 통해 가상환경을을 만들어준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># env 생성</span></span><br><span class="line">conda create -n env_name python=3.5</span><br><span class="line"></span><br><span class="line"><span class="comment"># env 리스트 보기</span></span><br><span class="line">conda env list</span><br><span class="line"></span><br><span class="line"><span class="comment"># env 활성화</span></span><br><span class="line">conda activate env_name</span><br><span class="line"></span><br><span class="line"><span class="comment"># env 비활성화</span></span><br><span class="line">conda deavtivate</span><br><span class="line"></span><br><span class="line"><span class="comment"># env 삭제</span></span><br><span class="line">conda env remove -n env_name</span><br></pre></td></tr></table></figure><h3 id="Scrapy-환경설정"><a href="#Scrapy-환경설정" class="headerlink" title="Scrapy 환경설정"></a>Scrapy 환경설정</h3><ul><li>먼저 가상환경을 활성화시켜준 후에, spider bot을 만들 폴더의 상위 폴더에서 다음의 명령어를 실행시켜준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda activate env_name</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line">scrapy startproject project_name</span><br></pre></td></tr></table></figure><ul><li>다음과 같이 설정한 project명을 갖는 폴더가 만들어지며, 필자는 section01_2라고 명명했다.</li></ul><p><img src="/image/scrapy_startproject.png" alt="create project folder"></p><ul><li>위의 단계까지 실행했다면, 다음과 같은 출력이 보일 것이다. 빨간줄 아래에 나와있는 예시 명령어를 따라서 실행시키면 spider bot을 만들수 있다.</li></ul><p><img src="/image/scrapy_startproject_then.png" alt="scrapy startproject"></p><ul><li>또한, <code>모든 앞으로의 모든 명령어는 scrapy.cfg라는 파일이 존재하는 directory path에서 해야한다.</code></li></ul><p><img src="/image/scrapy_commend_site.png" alt="scrapy 명령어 실행하는 path"></p><h3 id="genspider"><a href="#genspider" class="headerlink" title="genspider"></a>genspider</h3><ul><li>scrapy에서 가장 중요한 spider class를 만들어준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spider를 만들기 위해 명령어를 실행하려면 scrapy.cfg파일의 경로로 이동해야하기 때문에</span></span><br><span class="line"><span class="built_in">cd</span> section01_2</span><br><span class="line"></span><br><span class="line"><span class="comment"># scrapy.cfg파일이 존재하는지 다시 한번 확인</span></span><br><span class="line">ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># https://blog.scrapinghub.com/은 crawling 테스트를 위한 사이트로 유명하다.</span></span><br><span class="line"><span class="comment"># https://blog.scrapinghub.com/이라는 사이트를 크롤링할 testspider라는 이름으로 spider 을 만들어라는 명령어</span></span><br><span class="line">scrapy genspider testspider blog.scrapinghub.com</span><br></pre></td></tr></table></figure><ul><li>다음과 같은 출력결과를 볼 수 있으며, section01_2에 spiders라는 폴더의 testspider라고 만들어졌다는 것을 의미한다.<br><img src="/image/genspider.png" alt="genspider"></li></ul><p><img src="/image/genspider_then.png" alt="genspider 실행 후 파일 변경"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 만들어진 spider 파일 확일을 위해 이동</span></span><br><span class="line"><span class="built_in">cd</span> section01_2/spiders</span><br><span class="line"></span><br><span class="line"><span class="comment"># 위에서 만들어 놓았던 testspider라는 spider가 있는지 확인</span></span><br><span class="line">ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># testspider.py 확인</span></span><br><span class="line">vim testspider.py</span><br></pre></td></tr></table></figure><p><img src="/image/testspider.png" alt="만들어진 spider 파일 확인"></p><ul><li><p>먼저 straturl과 우리가 크롤링하려는 URL endpoint가 https인지 확인한 후 고쳐준다.(여기서 필자는 vim으로 수정하였기에 pep8에 의거하여 space 4번으로 indent를 사용하였다. <code>space와 tap을 번갈아가며 사용하면 python interpreter가 다르게 인식하므로 에러를 발생시킨다!</code>)</p></li><li><p>앞으로의 실습에 헷갈림을 방지하기 위해서 name을 test1으로 변동해주었고, allowed_domains과 start_urls를 보면 설정해 놓은 대로 들어가 있는 것을 알 수 있다. 여기서 <code>scrapy는 allowed_domains과 start_urls가 리스트 구조로 되어있는데 다른 URL과 도메인들을 추가하면 해당 사이트들을 돌아가며 크롤링을 할 수 있는 병렬처리가 가능하다는 것이 가장 큰 장점</code>이다. 추후에 설명하겠지만, 눈치 빠르신 분들은 아래 <code>parse</code>함수에서 response를 parameter로 받는 함수이므로 이 함수에 크롤링하고 싶은 부분에 대한 코드를 만들면 크롤링이 가능하다는 것을 알 것이다!! 혹시 response에서 어떤 명령어가 사용가능한지 보고 싶다면</p></li></ul><p><img src="/image/testspider_in.png" alt="크롤링 상태 확인을 하기 위해 parse에 print문 추가"></p><h3 id="runspider-vs-crawl"><a href="#runspider-vs-crawl" class="headerlink" title="runspider vs crawl"></a>runspider vs crawl</h3><ul><li>runspider와 crawl의 차이점은 <code>runspider는 spiders폴더에서 실행할 수 있고, crawl은 scrapy.cfg파일이 존재하는 폴더에서 실행하여햐 한다는 점이 차이점이다!!</code> genspider 명령어를 통해 spider bot spider는 단위 테스트라고 소위 불리는 방식을 할 때 유용하고 crawl은 우리가 원하는 구조를 다 만들어 놓은 후 테스트를 할 때나 실제로 크롤링을 할 경우 사용하는 것이 유용하다.  </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># runspider는 spiders 폴더에서 실행하여야한다.</span></span><br><span class="line">scrapy runspider testspider.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># crawl은 scrapy.cfg파일이 존재하는 path에서 실행시켜주어야한다.</span></span><br><span class="line">scrapy crawl test1 --nolog</span><br></pre></td></tr></table></figure><p><img src="/image/dir.png" alt="spider 실행후 결과 중 parse함수에서 reponse뒤에 사용할 수 있는 명령어의 종류"></p><h3 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a>settings.py</h3><p><img src="/image/settings_py.png" alt="settings.py의 위치"></p><ul><li>spider의 속성에 관련된 parameter들이 있는 파일이라고 생각하면 된다. 예를 들면, 아래의 그림에서 볼 수 있듯이 <code>SPIDER MODULES</code>는 현재 SPIDER의 위치를 의미하고, <code>NEWSPIDER MODULE</code>은 Spider를 새로 생성시 어느 위치에 추가되는지를 의미한다. <code>ROBOTSTXT_OBEY</code>는 robots.txt의 규칙에 의거하여 crawling을 하겠다는 의미이며, <code>DOWNLOAD_DELAY</code>는 몇초간격으로 서버에 요청을 할지에 대한 수치이다. 필자는 1로 정했는데 여기서는 1초마다라는 의미이다. <code>만약에 0.2라고 하게 되면 0.2초마다 서버에 요청하게 되어 서버에 부하를 일으키게 되면 심할경우 영구 van을 당할 수도 있기에 간격을 1초이상으로 하는 것을 권장한다.</code></li></ul><p><img src="/image/settings_py_in.png" alt="settings.py"></p><h3 id="실습-blog-scrapinghub-com에서-기사-제목들만-크롤링-하기"><a href="#실습-blog-scrapinghub-com에서-기사-제목들만-크롤링-하기" class="headerlink" title="실습)blog.scrapinghub.com에서 기사 제목들만 크롤링 하기!"></a>실습)blog.scrapinghub.com에서 기사 제목들만 크롤링 하기!</h3><ul><li><p>위의 실습주제로 실습을 진행하기 위해서는 앞서 만들어본 spider 파일에서 parse함수를 수정해야할 것이다. 그에 앞서 크롤링할 blog.scrapinghub.com의 제목에 해당하는 css path를 보면 전체 html의 body 부분에서 div element 중 class의 이름이 post-header인 부분에만 존재하는 것을 개발자 도구를 통해 알아내었다. <code>다른 부분에 동일한 element나 class명을 가질 수도 있으므로 find를 해보아야한다!</code></p></li><li><p>다음과 같이 제목을 크롤링하기 위해 testspider.py을 수정해 주었다.</p></li></ul><p><img src="/image/modify_spider_00.png" alt="크롤링을 위한 testspider.py 수정"></p><ul><li>참고로 결과를 파일로 저장할때 scrapy가 지원하는 파일 형식은 <code>json, jsonlines, jl, csv, xml, marshal, pickle</code>이다. <code>결과를 저장할때 동일한 파일명과 확장자명을 가진 파일이 이미 존재한다면 그 파일에 데이터를 추가해주므로 주의</code>하자!</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 필자는 spider 폴더에서 실행함.</span></span><br><span class="line">scrapy runspider testspider.py -o result.csv -t csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># result 파일인 result.csv가 만들어진 것을 확인할 수 있다.</span></span><br><span class="line">ls</span><br></pre></td></tr></table></figure><h3 id="Requests를-사용하여-페이지-순회하며-크롤링"><a href="#Requests를-사용하여-페이지-순회하며-크롤링" class="headerlink" title="Requests를 사용하여 페이지 순회하며 크롤링"></a>Requests를 사용하여 페이지 순회하며 크롤링</h3><ul><li><p>우리가 예를 들어 어떤 페이지내에서 여러 항목에 대해 해당 url로 이동 후 크롤링하고 다시 그 전 페이지로 돌아가서 다음 항목의 url로 이동 후 크롤링하는 이런 순회를 거쳐야하는 작업은 request나 selenium으로 하게되면 iterable한 코드를 통해 가능하게 되며, 그렇지 않다면, 동일한 구성을 지닌 페이지들이라면 함수를 여러번 실행하는 등의 multiprocessing을 통해 병렬처리를 따로 해주어여하는 불편함이 있다. 허나, scrapy는 코드 몇 줄로 가능하다.</p></li><li><p>먼저 앞의 spider말고 새로운 spider를 만들어 사용할 것이다. spider를 만든 설정은 위에서 만든 것과 동일하다. spider 이름만 pagerutine이라고 명명했을 뿐이다.</p></li></ul><p><img src="/image/new_spider.png" alt="새로운 spider 만들기"></p><ul><li><p>이전에 blog.scrapinghub.com의 페이지에 해당 10개의 기사들에 대한 path가 “div.post-header h2 &gt; a” 이며, a tag의 href 속성 값들을 통해 각각의 기사에 해당 페이지로 이동이 가능할 수 있다는 사실을 확인 할 수 있다.   </p></li><li><p>첫번째 <code>parse 함수</code>에서 미리 추출한 url을 request하여 얻은 response를 다른 함수로 전달해주기 위해 Request 명령어를 사용하였으며, urljoin을 사용한 이유는 절대주소가 아닌 상대주소로 되어있는 경우 위의 start_urls에 설정해 놓은 주소를 앞에 붙여 절대 주소로 바꿔주는 기능이다. 물론 절대주소인 경우는 이런 작업을 생략한다.</p></li></ul><p><img src="/image/pagerutine_py_in.png" alt="pagerutine.py 파일"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/09/Crawling_00/#disqus_thread</comments>
    </item>
    
    <item>
      <title>모형 성능 평가 지표</title>
      <link>https://heung-bae-lee.github.io/2020/01/09/machine_learning_03/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/09/machine_learning_03/</guid>
      <pubDate>Thu, 09 Jan 2020 06:12:42 GMT</pubDate>
      <description>
      
        
        
          &lt;h3 id=&quot;회귀-regression-평가-지표&quot;&gt;&lt;a href=&quot;#회귀-regression-평가-지표&quot; class=&quot;headerlink&quot; title=&quot;회귀(regression) 평가 지표&quot;&gt;&lt;/a&gt;회귀(regression) 평가 지표&lt;/h3&gt;&lt;ul
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h3 id="회귀-regression-평가-지표"><a href="#회귀-regression-평가-지표" class="headerlink" title="회귀(regression) 평가 지표"></a>회귀(regression) 평가 지표</h3><ul><li>회귀의 평가를 위한 지표는 실제 값과 회귀 예측값의 차이 값을 기반으로 한 지표가 중심이다. 실제값과 예측값의 차이를 그냥 더하면 잔차의 합은 0이므로 지표로 쓸 수 없다. 이 때문에 잔차의 절대값 평균이나 제곱, 또는 제곱한 뒤 다시 루트를 씌운 평균값을 성능 지표로 사용한다.</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">평가 지표</th><th>수식</th></tr></thead><tbody><tr><td style="text-align:center">MAE(Mean Absolute Error)</td><td>$MAE =  \frac{1}{n} \sum_{i=1}^{n} \lvert Y_{i} - \hat{Y_{i}} \rvert$</td></tr><tr><td style="text-align:center">MSE(Mean Squared Error)</td><td>$MSE =  \frac{1}{n} \sum_{i=1}^{n} (Y_{i} - \hat{Y_{i}})$</td></tr><tr><td style="text-align:center">RMSE(Root Mean Squared Error)</td><td>$RMSE = $\sqrt{\frac{1}{n} \sum_{i=1}^{n} (Y_{i} - \hat{Y_{i}})^{2}}$</td></tr><tr><td style="text-align:center">$R^{2}$</td><td>$R^{2} = \frac{예측값의 Variance}{실제값 Variance} = \frac{SSR}{SST}$</td></tr></tbody></table></div><p><img src="/image/MSE.png" alt="MSE"></p><p><img src="/image/MAPE.png" alt="MAPE"></p><p><img src="/image/R_squared.png" alt="결정계수"></p><p><img src="/image/adj_R_squared.png" alt="수정된 결정계수"></p><ul><li>이전에도 언급했던 것처럼 변수가 추가된다면 당연히 SSR의 수치가 높아지기 때문에 $R^{2}$값은 올라갈 수 밖에 없다. 그러므로 변수 수에 영향을 받지 않고 서로 비교할 수 있게끔 만들어 준 것이 수정된 결정계수이다.</li></ul><p><img src="/image/AIC.png" alt="AIC"></p><p><img src="/image/BIC.png" alt="BIC"></p><h3 id="분류-classification-성능-지표"><a href="#분류-classification-성능-지표" class="headerlink" title="분류(classification) 성능 지표"></a>분류(classification) 성능 지표</h3><p><img src="/image/confusion_matrix_00.png" alt="Accuarcy"></p><p><img src="/image/confusion_matrix_01.png" alt="Recall, Precision, Specificity"></p><ul><li><p>특히 imbalanced data에서 모형의 성능을 정확도 하나만을 가지고 성능을 평가한다면, 예를 들어, 100개중 90개는 세모고 10개는 네모라고 할 때 100개 모두 세모라고 예측해버리게 되면 정확도는 90%이므로 좋은 성능 지표라고 할 수 없다. 그러므로 <code>imbalanced data에서의 성능 지표는 정확도(accuarcy) 보다는 정밀도(precision), 재현율(Recall)를 더 선호한다.</code></p></li><li><p>정밀도(precision)와 재현율(recall) 지표 중에 분류 모델의 업무 특성에 따라서 특정 평가 지표가 더 중요한 지표로 간주 될 수 있다. <code>재현율(recall)이 중요 지표인 경우는 실제 Positive 양성 데이터를 Negative로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우</code>이다. 예를 들어 <code>암 판단 모형</code>은 재현율(recall)이 훨씬 중요한 지표이다. 왜냐하면 실제 Positive인 암 환자를 Positive 양성이 아닌 Negative 음성으로 잘못 판단했을 경우 오류의 대가가 생명을 앗아갈 정도로 심각하기 때문이다. 반면에 실제 Negative인 건강한 환자를 암 환자인 Positive로 예측한 경우면 다시 한번 재검사를 하는 수준의 비용이 소모될 것이다.  </p></li></ul><p><img src="/image/G_maen_and_F1_score.png" alt="G-maen, F1-score"></p><p><img src="/image/ROC_and_AUC.png" alt="ROC, AUC"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/09/machine_learning_03/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Regression(02)</title>
      <link>https://heung-bae-lee.github.io/2020/01/08/machine_learning_02/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/08/machine_learning_02/</guid>
      <pubDate>Wed, 08 Jan 2020 14:22:36 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;다중-선형-회귀&quot;&gt;&lt;a href=&quot;#다중-선형-회귀&quot; class=&quot;headerlink&quot; title=&quot;다중 선형 회귀&quot;&gt;&lt;/a&gt;다중 선형 회귀&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/image/Multiple_linear_regression.pn
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="다중-선형-회귀"><a href="#다중-선형-회귀" class="headerlink" title="다중 선형 회귀"></a>다중 선형 회귀</h2><p><img src="/image/Multiple_linear_regression.png" alt="다중 선형 회귀"></p><p><img src="/image/Multiple_linear_regression_01.png" alt="다중 선형 회귀의 계수 추정"></p><p><img src="/image/Multiple_linear_regression_01.png" alt="다중 선형 회귀의 계수의 해석"></p><ul><li><p><code>다중회귀방정식에서 회귀계수에 대한 해석은 자주 혼동되는 것 중 하나이다. 단순회귀방정식은 직선을 표현하지만 다중회귀방정식은 평면(독립(설명)변수가 두개인 경우) 혹은 초평면(독립(설명)변수가 두개보다 많은 경우)을 표현</code>한다. 위의 예에서 회귀계수의 해석은 다른 변수들이 고정되어 있을때 TV가 1단위 증가할 때 매출액은 0.046단위 증가한다고 해석할 수 있다.</p></li><li><p>회귀 계수 $\beta_{j}$는 $X_{j}$를 제외한 나머지 모든 예측 변수들을 상수로 고정시킨 상태에서 $X_{j}$의 한 단위 증가에 따른 Y의 증분으로 해석될 수 있다. 변화의 크기는 다른 예측 변수들이 어떤 값으로 고정되어 있는지에 의존하지 않는다. 또 다른 해석은 $\beta_{j}$가 다른 독립(설명)변수들에 의하여 종속(반응)변수 Y가 조정된 후에 Y에 대한 $X_{j}$의 공헌도를 의미한다. 이는 예를 들어 $Y = \beta{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \epsilon $일 때, $[Y~X_{1}로 얻은 잔차] ~ [X_{2}~X_{1}]$에서의 계수와 동일하다.</p></li></ul><h5 id="Y와-X-2-각각으로부터-X-1-의-선형효과를-제거한-후-Y에-미치는-X-2-의-효과를-나타내기-때문이다"><a href="#Y와-X-2-각각으로부터-X-1-의-선형효과를-제거한-후-Y에-미치는-X-2-의-효과를-나타내기-때문이다" class="headerlink" title="Y와 $X_{2}$ 각각으로부터 $X_{1}$의 선형효과를 제거한 후 Y에 미치는 $X_{2}$의 효과를 나타내기 때문이다."></a>Y와 $X_{2}$ 각각으로부터 $X_{1}$의 선형효과를 제거한 후 Y에 미치는 $X_{2}$의 효과를 나타내기 때문이다.</h5><h3 id="다중-선형-회귀-계수-검정"><a href="#다중-선형-회귀-계수-검정" class="headerlink" title="다중 선형 회귀 계수 검정"></a>다중 선형 회귀 계수 검정</h3><p><img src="/image/Multiple_linear_regression_coefficient_check.png" alt="다중 선형 회귀 계수 검정 - 01"></p><ul><li>단순 선형 회귀와 동일하게 각각의 회귀계수가 통계적으로 유의미한지를 검정하는 것은 동일하다. 하지만, 다중 선형 회귀는 다음과 같이 전체 회귀계수가 의미있는지에 대한 검정도 하게 된다. <code>여기서 회귀계수가 0에 가깝고 standard error를 더하고 뺀 범위내에 0이 포함된다면 그 변수 또한 유의미하더라도 제거해야할 것이다.</code> 또한 여기서 <code>잔차의 정규성이라는 가정을 만족할 때 t 통계량의 절대값이 크거나 대응되는 p-value가 더 작다면 독립(설명)변수와 종속(반응)변수 사이의 선형관계가 더 강함을 의미</code>한다.</li></ul><p><img src="/image/Multiple_linear_regression_coefficient_check_01.png" alt="다중 선형 회귀 계수 검정 - 02"></p><ul><li><p>위에서 언급하고 있는 개별적인 회귀계수 $\beta$에 대한 검정 이외에, 여러 가지 다른 형태의 가설들이 선형모형의 분석과 관련하여 고려될 수 있다. 통상적으로 고려될 수 있는 가설들은 다음과 같다.</p><ul><li><p>1) 독립 변수의 모든 회귀 계수들이 0이다.</p></li><li><p>2) 독립 변수의 회귀 계수들 중 일부분이 0이다.</p></li><li><p>3) 회귀계수들 중 일부분이 서로 같은 값을 가진다.</p></li><li><p>4) 회귀모수들이 특정한 제약조건을 만족한다.</p></li></ul></li><li><p>이런 가설들은 하나의 통합된 접근방법을 통해 동일한 방식으로 검정될 수 있다. 먼저, 모든 독립변수를 포함한 모형을 완전모형(FM)이라고 하자. 그리고 귀무가설에 가정된 내용들을 완전모형에 대입해서 얻은 모형을 축소모형(RM)이라고 하자. 완전모형의 변수들이 상대적으로 축소모형에 비해 많으므로 SSR값이 커져 잔차제곱합(SSE)을 감소시킬 것이므로 $SSE(RM) \geq SSE(FM)$이 된다. 따라서 차이 $SSE(RM) - SSE(FM)$은 축소모형을 적합함으로써 증가하는 잔차제곱합(SSE)을 의미한다. <code>만약 이 차이가 크다면 축소모형은 적절하지 않다.</code></p></li></ul><script type="math/tex; mode=display">F = \frac{[SSE(RM) - SSE(FM) ]/(p + 1 - k)}{SSE(FM)/(n-p-1)}</script><ul><li><p>위의 식을 통해 다 위에서 언급했던 가설들을 모두 검정할 수 있다.</p><ul><li><p>가설 1)독립 변수의 모든 회귀 계수들이 0이다.의 귀무가설은 $H_{0}:\beta_{1} = \beta_{2} = \cdots = \beta_{p} = 0$이며 대립가설은 $H_{1}: 최소한 하나의 계수는 0이 아니다. $이다. 이 가설은 축소모형의 변수는 1개이므로 해당 모형을 fitting한 후에 나오는 분산분석표에서의 F 통계량 값을 보고 검정 할 수 있다.</p></li><li><p>가설 2)독립 변수의 회귀 계수들 중 일부분이 0이다.의 귀무가설은 $H_{0}:\beta_{1} =  \beta_{3} = \beta_{5} = 0 $ 이고, 대립가설은 $H_{1}:\beta_{1}, \beta_{3}, \beta_{5} 중 최소한 하나는 0이 아니다. $ 이다. 이 가설은 위에서 F 통계량을 구하는 방식에 변형을 주어 생각해보면 $R^{2}$값을 통해 구할 수 있음을 알 수 있다.</p></li></ul></li></ul><script type="math/tex; mode=display">F = \frac{({R_{p}}^{2}-{R_{q}}^{2})/(p-q)}{(1 - {R_{p}}^{2})/(n-p-q)}, df=(p-q,n-p-1)</script><p><img src="/image/Multiple_linear_regression_coefficient_check_02.png" alt="다중 선형 회귀 계수 검정 - 03"></p><ul><li>여기서 주목할 점은 <code>SST는 정해져 있어 고정되어 있는데, SSR은 변수를 추가할수록 점점 더 커지므로 귀무가설을 기각하기 더 쉬워진다는 것이며, 우리가 추후에 말할</code>$R^{2}$<code>값도 변수를 추가할수록 높아지므로 이값으로 모형의 성능을 평가할때 무조건 이값이 높다고 좋은 모형이라고 생각하지 않아야 한다.</code> 또한 수정결정계수(adjusted R-squared) ${R_{adj}}^{2}$도 적합도를 평가하기 위해 사용될 수 있다. ${R_{adj}}^{2}$은 모형안에 있는 독립(설명)변수들의 수가 다르다는 것을 조정하므로 F값과 같이 서로 다른 모형들(포함된 독립변수가 다르거나 갯수가 다른)을 비교하기 위해 사용된다. <code>이 값은 결정계수 값과 다르게 Y의 전체 변이 중에서 독립변수들에 의하여 설명되는 비율로 해석 될 수 없다!</code></li></ul><script type="math/tex; mode=display">{R_{adj}}^{2} = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)}</script><h3 id="원점을-통과하는-회귀선"><a href="#원점을-통과하는-회귀선" class="headerlink" title="원점을 통과하는 회귀선"></a>원점을 통과하는 회귀선</h3><ul><li><p>일반적으로 고려되는 단순선형회귀모형은 $ Y = \beta_{0} + \beta_{1}X + \epsilon $과 같이 절편항을 가지고 있다. 그러나 원점을 통과하는 다음과 같은 모형 $ Y = \beta_{1}X + \epsilon $ 에 데이터를 적합시킬 필요가 있을 때도 있다.</p></li><li><p>이 모형은 절편항이 없는 모형으로 불린다. <code>문제의 성격이나 외적 상황에 의해 회귀선이 원점을 지나야만하는 경우가 있다.</code> 예를 들어, 시간(X)의 함수로서 여행 거리(Y)는 상수항을 가지지 않아야 한다. 이때는 <code>SSE의 자유도가 확률 변수인 절편항이 하나 빠지므로 N-p(전체 확률변수가 설명변수p개 절편항 1개 이었던 N-p-1에서)로 바뀌게 된다.</code> 또한 이때는 우리가 알고 있는 SST=SSR+SSE라는 공식이 더 이상 성립되지 않는다. 그러므로 $R^{2}$와 같은 절편항을 갖는 모형에 대한 몇몇 성능 평가 지표들은 절편항이 없는 모형에 대해서는 더 이상 적절하지 않다. <code>절편항이 없는 모형에 대한 적절한 항등식은 y의 평균을 0으로 대체함으로써 얻어진다.</code></p></li></ul><script type="math/tex; mode=display">\sum_{i = 1}^{n} y_{i}^{2} = \sum_{i = 1}^{n} \hat{y_{i}^{2}} + \sum_{i = 1}^{n} e_{i}^{2}</script><ul><li>그러므로 $R^{2}$ 또한 재정의 된다.</li></ul><script type="math/tex; mode=display">R^{2} = \frac{\sum \hat{ y_{i}^{2} }}{\sum {y_{i}^{2}} = 1 - \frac{\sum e_{i}^{2}}{\sum y_{i}^{2}}</script><ul><li><p><code>절편항을 가진 모형의 경우</code> $R^{2}$가 Y를 그의 평균으로 조정한 후에 Y의 전체 변동성 중에서 독립(설명)변수 X에 의하여 설명되는 비율로 해석될 수 있다. <code>절편항이 없는 모형의 경우</code>에는 Y에 대한 조정이 없다.</p></li><li><p><code>이 처럼 절편항이 없는 모형은 풀고자하는 문제와 관련된 이론 혹은 물리적 상황에 부합되는 경우에만 사용되어야만 한다!</code> 그러나 몇몇 응용에서는 어떤 모형을 사용해야 할지가 분명하지 않을 수 있다. 이러한 경우</p><ul><li>1) 관측값과 예측값의 가까운 정도를 측정하는 것이 잔차제곱평균이므로 두 모형에 의해 산출되는 잔차평균제곱(SSE를 각각의 모형에 대한자유도로 나눈 값)을 비교하여 평가한다.</li><li>2) 데이터 모형을 적합하고 절편항의 유의성을 검정하여(t통계량을 바탕으로) 검정이 유의하다면 절편항을 가진 모형을 사용하고 그렇지 않으면 절편항이 없는 모형을 사용한다. <code>그러나, 일반적으로 회귀모형에서는 상수항이 통계적으로 유의하지 않더라도, 강한 이론적 근거가 존재하지 않는다면, 상수항은 모형에 포함되어야 한다. 특히 분석에 사용되는 데이터가 원점을 포함하지 않는 경우 더욱 강조되는데 그 이유는 상수항이 종속(반응)변수의 기본적인 수준(평균)을을 나타내기 때문</code>이다.</li></ul></li><li><p>참고로 필자는 ANCOVA 분석 즉, 회귀식에서 설명변수들 중 질적인 변수(혹은 더미변수)가 포함되어 있어 그런 질적인 변수와 더미변수가 절편항(상수항)을 대신해줄 것이라고 착각하여 상수항을 생성하지 않고, 모형을 적합시켰던 경험이 있다. 프로젝트였는데 멘토분께서 왜 절편항을 포함하지 않았냐고 물어보보셨는데 위와 같은 답변을 했었는데 잘못된 접근법이라고 조언을 해주셨었다. 그 당시에는 이해가 가지 않았지만 이제는 나의 접근법이 말이 안된다는 것부터 깨달았다. 왜냐하면 SGD 방법으로 확률변수인 절편항과 계수항들을 업데이트해 나가는 방식으로 회귀모형을 짜는데 필자는 이미 상수항 취급을 하는 질적변수나 더미변수 자체를 절편항이라고 생각했으니 말 자체가 안되는 것이다.</p></li></ul><h3 id="모형에서-중심화-centering-와-척도화-scaling"><a href="#모형에서-중심화-centering-와-척도화-scaling" class="headerlink" title="모형에서 중심화(centering)와 척도화(scaling)"></a>모형에서 중심화(centering)와 척도화(scaling)</h3><ul><li><p>회귀분석에서는 <code>회귀계수의 크기가 변수의 측정 단위에 영향을 받게 되므로 중심화와 척도화를 해야한다.</code>예를 들어 달러 단위로 측정된 소득의 회귀계수가 5.123이라면, 소득이 1,000달러 단위로 측정 되었을때는 5123으로 바뀌게 된다. <code>절편항(상수항)이 있는 모형을 다룰 때는 변수에 대한 중심화와 척도화가 필요하지만, 절편이 없는 모형을 다룰 때는 변수의 척도화만 필요하다.</code> 또한 이는 다른 선형성을 가정하는 모델(RBF kernel을 사용하는 SVM, logistic regression)에선 피처를 정규성을 띄게 해주어야하는 모형 뿐만아니라 피처 scaling을 하여 과적합을 방지하는 방법이므로 알고있어야한다.</p></li><li><p><code>중심화(centering)</code> 변수는 <code>각 관측값에서 모든 관측값의 평균을 빼는 것</code>으로 얻어진다. 중심화된 변수 <code>척도화</code> 또한 가능하다. <code>두 가지 형태의 척도화(scaling)가 통상적으로 가능한데, 단위 길이 척도화(unit length scaling or normalization)</code>와 <code>표준화(Standardization)</code>이다. <code>단위길이 척도화는 피처 벡터의 길이로 나누어주거나 min-max scaling 같은 것을 의미</code>한다. <code>표준화는 말 그대로 편차를 표준편차로 나누어 표준정규분포를 띄게끔해주는 작업</code>을 의미한다.</p></li></ul><h3 id="다중-공선성-Multi-collinearity"><a href="#다중-공선성-Multi-collinearity" class="headerlink" title="다중 공선성(Multi-collinearity)"></a>다중 공선성(Multi-collinearity)</h3><p><img src="/image/multicollinearity_00.png" alt="다중 공선성 - 01"></p><p><img src="/image/multicollinearity_01.png" alt="다중 공선성 - 02"></p><p><img src="/image/multicollinearity_02.png" alt="다중 공선성 - 03"></p><ul><li><p>Ordinary Least Squares(OLS) 즉 최소 제곱법 기반의 회귀 계수 계산은 독립 변수(입력 피처)의 독립성에 많은 영향을 받는다. <code>피처간의 상관관계가 매우 높은 경우 분산이 매우 커져서 오류에 매우 민감해지며 선형대수의 관점에서 보면, 모든 컬럼들이 linearly independent해야 최소한 하나 이상의 해가 존재하기 때문</code>이다. 만약 위의 말이 이해가 가지 않는다면, 필자가 추전하는 선형대수학 강의를 듣는 것을 권한다. 다음 페이지를 가면 찾을 수 있다. <a href="https://heung-bae-lee.github.io/2019/12/14/linear_algebra_00/">선형대수학 강의 추천</a></p></li><li><p>위와 같은 다중 공선성 문제가 있을 경우, <code>일반적으로 상관관계가 높은 독립 변수(입력 피처)가 많은 경우 독립적인 중요한 독립 변수(입력 변수)만을 남기고 제거하거나 규제를 적용</code>한다. 또한 <code>매우 많은 피처가 다중 공선성 문제를 가지고 있다면 PCA를 통해 차원 축소를 수행하는 것도 고려해 볼 수 있다.</code></p></li></ul><h3 id="다중-공선성-검사하는-방법들"><a href="#다중-공선성-검사하는-방법들" class="headerlink" title="다중 공선성 검사하는 방법들"></a>다중 공선성 검사하는 방법들</h3><p><img src="/image/vif.png" alt="VIF"></p><ul><li>위에서 VIF가 10이상인 경우 다중공선성이 있는 변수라고 판단할 수 있다고 했는데, 그렇다면 다중공선성이 있다고 판단되는 변수를 무조건적으로 제거해야 하나라는 의문이 들 수 있을 것이다. 그에 대한 답은 <code>무조건적으로 제거하면 안된다라는 것</code>이다. <code>VIF가 높더라도 통계적으로 유의미한 변수(p-value가 유의수준 보다 낮은 변수)라면 제거하지 않는 것이 적절하다. 추가적으로 다른 변수들 중에 VIF가 높고, 유의미하지 않은 변수가 있다면 그 변수들을 제거해 본 뒤 VIF를 계산해 보아야 할 것이다. 이 과정을 거쳐서도 아마도 VIF는 높은 수치이겠지만, 제거를 해서는 안된다.</code></li></ul><p><img src="/image/correlation_matrix.png" alt="상관 행렬"></p><ul><li><p>다중 공선성을 검사하는 방법에는 VIF외에도 상관계수 행렬을 구해서 위와 같이 산점도와 같이 그려서 보아야한다. 상관계수는 공분산을 각각의 표준편차로 나누어준 수치인데, 공분산은 예를 들어 두 변수 X와 Y가 있다면, Y와 X 사이의 선형 관계에 대한 방향을 나타낸다. <code>Cov(X, Y)는 측정단위의 변화에 영향을 받기 때문에 우리에게 관계의 강도가 얼마나 되는 지를 알려주지는 않고 방향만을 알려준다.</code> 이러한 문제를 해결하기 위해 <code>표준편차로 나누어 Standardization을 해주어 단위에 대한 영향을 없애준 것</code>이 <code>상관계수</code>이다. 참고로 여기서 <code>Corr(X, Y)=0 가 반드시 Y와 X 사이에 관계가 없음을 의미하는 것이 아님을 주의하자! 상관계수는 오직 선형 관계를 측정하기에 선형적으로 관계가 없음을 의미한다. 즉, X와 Y가 비선형적으로 관련되어 있을 때에도 Corr(X, Y)가 0이 될 수 있다.</code><br>또한 <code>상관계수도 평균과 분산과 마찬가지로 극단값에 민감하다. 그러므로 이러한 요약 통계량에만 의존하는 분석으로는 전체적인 패턴을 보는데에 있어서 차이를 발견할 수 없게 할 것이다. 따라서 필자는 개인적으로 데이터 EDA 과정에서 독립(설명)변수들과 반응 변수 사이의 관계를 꼭 산점도로 그려 확인한 뒤, 상관계수와 의미가 일치하는지 확인해보는 작업이 필수라고 여긴다.</code></p></li><li><p>통계를 공부하는 Beginner들이 많이들 오해할 만한 사실은 <code>Corr(X, Y)는</code> 한 변수의 값이 주어졌을 때 다른 변수의 값을 예측하기 위해서 사용할 수 없다. <code>단지 대응(pairwise)관계만 측정</code>한다. <code>예측을 하고 관계를 설명하기 위해서 우리는 회귀분석을 하는 것!!!!</code></p></li></ul><p><img src="/image/multicollinearity_03.png" alt="다중 공선성을 완전히 해소하는 방법은 없다!">  </p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/08/machine_learning_02/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP를 공부하는데 도움되는 사이트 모음</title>
      <link>https://heung-bae-lee.github.io/2020/01/07/NLP_00/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/07/NLP_00/</guid>
      <pubDate>Tue, 07 Jan 2020 11:57:26 GMT</pubDate>
      <description>
      
        
        
          &lt;h3 id=&quot;자연어-처리&quot;&gt;&lt;a href=&quot;#자연어-처리&quot; class=&quot;headerlink&quot; title=&quot;자연어 처리&quot;&gt;&lt;/a&gt;자연어 처리&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;자연어 처리에 대해 공부할 수 있게 도움이 될 만한 사이트&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h3 id="자연어-처리"><a href="#자연어-처리" class="headerlink" title="자연어 처리"></a>자연어 처리</h3><ul><li>자연어 처리에 대해 공부할 수 있게 도움이 될 만한 사이트</li></ul><h4 id="자연어-처리-강의"><a href="#자연어-처리-강의" class="headerlink" title="자연어 처리 강의"></a>자연어 처리 강의</h4><ul><li>딥러닝을 이용한 자연어 처리:<a href="https://www.edwith.org/deepnlp" target="_blank" rel="noopener">https://www.edwith.org/deepnlp</a></li></ul><h4 id="자연어-처리-오프라인-스터디-모임"><a href="#자연어-처리-오프라인-스터디-모임" class="headerlink" title="자연어 처리 오프라인 스터디 모임"></a>자연어 처리 오프라인 스터디 모임</h4><ul><li><p>DeepNLP(모두의연구소 자연어 처리 스터디):<a href="http://www.modulabs.co.kr/information" target="_blank" rel="noopener">http://www.modulabs.co.kr/information</a></p></li><li><p>바벨피쉬(싸이그래머 스터디) : <a href="https://www.facebook.com/groups/babelPish/" target="_blank" rel="noopener">https://www.facebook.com/groups/babelPish/</a></p></li></ul><h4 id="온라인-참고-자료"><a href="#온라인-참고-자료" class="headerlink" title="온라인 참고 자료"></a>온라인 참고 자료</h4><ul><li><p>스탠퍼드 자연어 처리 강의 : <a href="http://web.stanford.edu/class/cs224n" target="_blank" rel="noopener">http://web.stanford.edu/class/cs224n</a></p></li><li><p>Jacob Eisenstein 교수님의 자연어 처리 강의 : <a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf" target="_blank" rel="noopener">https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf</a>    </p></li><li><p>YSDA 자연어 처리 : <a href="https://github.com/yandexdataschool/nlp_course" target="_blank" rel="noopener">https://github.com/yandexdataschool/nlp_course</a></p></li><li><p>조경현 교수님의 자연어 처리 강의 노트 : <a href="https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf" target="_blank" rel="noopener">https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf</a></p></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/07/NLP_00/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Regression(01)</title>
      <link>https://heung-bae-lee.github.io/2020/01/04/machine_learning_01/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/04/machine_learning_01/</guid>
      <pubDate>Sat, 04 Jan 2020 02:44:20 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;회귀분석이란&quot;&gt;&lt;a href=&quot;#회귀분석이란&quot; class=&quot;headerlink&quot; title=&quot;회귀분석이란?&quot;&gt;&lt;/a&gt;회귀분석이란?&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/image/what_is_regression.png&quot; alt=&quot;회귀분석 -
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="회귀분석이란"><a href="#회귀분석이란" class="headerlink" title="회귀분석이란?"></a>회귀분석이란?</h2><p><img src="/image/what_is_regression.png" alt="회귀분석 - 01"></p><ul><li>지도 학습은 두 가지 유형으로 나뉘는데, 바로 분류(classification)와 회귀(regression)이다. 이 두 가지 기법의 가장 큰 차이는 분류는 예측값이 카테고리와 같은 이산형 클래스 값이고, 회귀는 연속형 숫자 값이라는 것이다.</li></ul><p><img src="/image/regression_01.png" alt="회귀분석 - 02"></p><ul><li><p>회귀(regression)은 현대 통계학을 떠받치고 있는 주요 기중 중 하나이다. 여러분이 회귀분석시에 많이 들어봤을 예시는 부모의 키와 자식의 키에대한 예시가 있을 것이다. 부모의 키가 아주 크더라도 자식의 키가 부모보다 더 커서 세대를 이어가면서 무한정 커지는 것은 아니며, 부모의 키가 아주 작더라도 자식의 키가 부모보다 더 작아서 세대를 이어가며 무한정 작아지는 것이 아니라는 것이다. <code>즉, 사람의 키는 평균 키로 회귀하려는 경향을 가진다는 자연의 법칙이라는 의미이며, 회귀분석은 이처럼 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법이다.</code></p></li><li><p>머신러닝 관점에서 보면 독립변수는 피처에 해당되며, 종속변수는 결정 값이다. <code>머신러닝 회귀 예측의 핵심은 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 최적의 회귀 계수를 찾아내는 것</code>이다.</p></li><li><p>회귀에서 가장 중요한 것은 바로 회귀 계수이다. 이 회귀 계수가 선형이나 아니냐에 따라 <code>선형회귀</code>와 <code>비선형 회귀</code>로 나눌수 있으며, 독립변수의 개수가 한개 인지 여러개인지에 따라 <code>단일 회귀</code>, <code>다중 회귀</code>로 나뉜다.</p></li></ul><h4 id="선형-비선형-이라는-용어는-Y와-X-1-X-2-…-X-p-의-관계를-묘사하는-것이-아니라는-것에-주목하여라-회귀계수가-방정식에-선형적-비선형적-으로-삽입되어-있다는-것과-관련이-있다"><a href="#선형-비선형-이라는-용어는-Y와-X-1-X-2-…-X-p-의-관계를-묘사하는-것이-아니라는-것에-주목하여라-회귀계수가-방정식에-선형적-비선형적-으로-삽입되어-있다는-것과-관련이-있다" class="headerlink" title="선형(비선형)이라는 용어는 Y와 $X_{1},X_{2},…,X_{p}의 관계를 묘사하는 것이 아니라는 것에 주목하여라! 회귀계수가 방정식에 선형적(비선형적)으로 삽입되어 있다는 것과 관련이 있다."></a>선형(비선형)이라는 용어는 Y와 $X_{1},X_{2},…,X_{p}의 관계를 묘사하는 것이 아니라는 것에 주목하여라! 회귀계수가 방정식에 선형적(비선형적)으로 삽입되어 있다는 것과 관련이 있다.</h4><ul><li>선형 함수의 예</li></ul><script type="math/tex; mode=display">Y = \beta_{0} + \beta_{1}X_{1} + \epsilon</script><pre><code>- `Y와 X 사이의 관계는 비선형이지만, 모수들이 선형적으로 삽입되어 있기 때문에 선형 함수`</code></pre><script type="math/tex; mode=display">Y = \beta_{0} + \beta_{1}X_{1} + \beta_{2}{X_{2}}^{2}  + \epsilon</script><script type="math/tex; mode=display">Y = \beta_{0} + \beta_{1}\log X_{1} + \epsilon</script><ul><li>비선형 함수의 예</li></ul><script type="math/tex; mode=display">Y = \beta_{0} + e^{\beta_{1}X_{1}} + \epsilon</script><ul><li><p>각 독립변수들은 양적(quantitative) 혹은 질적(qualitative)으로 분류 될 수 있다.</p><ul><li>양적 변수의 예) : 주택 가격, 침실의 개수, 연수, 세금 등</li><li>질적 변수의 예) : 이웃의 형태(좋은 혹은 나쁜 이웃), 집의 형태(정원이있는, 고풍스러운 등)</li><li>독립변수들은 양적 질적 변수 모두 취할 수 있는데, 질적 변수들이 있다면, 계산상의 이유로 더미 변수(dummy variable)로 코딩을 해주어야 한다. <code>단, 질적인 변수들도 예를 들어 전문가들에 의해 이미 규정되어 신뢰성 있는 공식이나 규칙을 통해 연속적인 수치로 변환될 수 있다면 더미 변수로 만들어 주지 않고 사용해도 된다.</code></li></ul></li><li><p><code>모든 독립변수들이 질적인 경우 분산 분석(ANOVA : analysis of variance)기법</code>이라고 한다. 분산 분석은 그 자신의 고유한 방법으로써 소개되고 통계학부생들이 통계적 자료분석이라는 주제로 수업을 수강할때 나오는 개념으로써 설명되고 있는데 <code>회귀분석의 특별한 경우임을 알고있어라!!!!</code> 또한, <code>어떤 예측변수들이 양적이고 반면에 다른 변수들이 질적이라면, 이러한 경우의 회귀분석을 공분산분석(ANCOVA : analysis of covariance)이라고 한다.</code></p></li></ul><div class="table-container"><table><thead><tr><th>회귀의 유형</th><th>조건</th></tr></thead><tbody><tr><td>- 일변량(Univariate)</td><td>- 오직 하나의 양적 독립변수(설명변수)</td></tr><tr><td>- 다변량(Multivariate)</td><td>- 두 개 이상의 양적 독립변수(설명변수)</td></tr><tr><td>- 단순(Simple)</td><td>- 오직 하나의 종속변수(반응변수)</td></tr><tr><td>- 다중(Multiple)</td><td>- 두 개 이상의 종속변수(반응변수)</td></tr><tr><td>- 선형(Linear)</td><td>- 데이터에 대하여 가능한 변환을 취한 후, 모든 계수들이 방정식에 선형적으로 삽입되어 있음.</td></tr><tr><td>- 비선형(Nonlinear)</td><td>- 종속변수(반응변수)와 일부 독립변수들의 관계가 비선형이거나 일부 계수들이 비선형적으로 나타남.  계수들을 선형적으로 나타나게 하는 어떤 변환도 가능하지 않음.</td></tr><tr><td>- 분산분석(ANOVA)</td><td>- 모든 독립변수들이 질적 변수임.</td></tr><tr><td>- 공분산분석(ANCOVA)</td><td>- 어떤 독립변수들은 양적변수이고 다른 독립변수들은 질적변수임.</td></tr><tr><td>- 로지스틱(Logistic)</td><td>- 종속변수(반응변수)가 질적변수임.</td></tr></tbody></table></div><h4 id="대표적인-선형-회귀-모형은-다음과-같다"><a href="#대표적인-선형-회귀-모형은-다음과-같다" class="headerlink" title="대표적인 선형 회귀 모형은 다음과 같다."></a>대표적인 선형 회귀 모형은 다음과 같다.</h4><h4 id="일반-선형-회귀"><a href="#일반-선형-회귀" class="headerlink" title="일반 선형 회귀"></a>일반 선형 회귀</h4><ul><li>예측값과 실제값의 잔차 제곱합을 최소화할 수 있도록 회귀 계수를 최적화하며, 규제(Regularization)를 적용하지 않은 모델이다.</li></ul><h4 id="릿지-Ridge"><a href="#릿지-Ridge" class="headerlink" title="릿지(Ridge)"></a>릿지(Ridge)</h4><ul><li>Ridge 회귀는 선형 회귀에 <code>L2 Regularization을 추가한 모형</code>이다. Ridge 회귀는 L2 Regularization을 적용하는데, L2 Regularization은 <code>상대적으로 큰 회귀 계수 값의 예측 영향도를 감소 시키기 위해서 회귀 계수값을 더 작게 만드는 Regularization 모형이다.</code></li></ul><h4 id="라쏘-Lasso"><a href="#라쏘-Lasso" class="headerlink" title="라쏘(Lasso)"></a>라쏘(Lasso)</h4><ul><li>Lasso 회귀는 선형 회귀에 <code>L1 Regularization을 적용한 방식</code>이다. L2 Regularization이 회귀 계수 값의 크기를 줄이는 데 반해, <code>L1 Regularization은 예측 영향력이 작은 피처의 회귀계수를 0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 것</code>이다. 이러한 특성 때문에 L1 Regularization은 <code>피처 선택 기능</code>으로도 불린다.</li></ul><h4 id="엘라스틱넷-ElasticNet"><a href="#엘라스틱넷-ElasticNet" class="headerlink" title="엘라스틱넷(ElasticNet)"></a>엘라스틱넷(ElasticNet)</h4><ul><li><code>L2, L1 Regularization을 함께 결합한 모형</code>이다. 주로 피처가 많은 데이터 세트에서 적용되며, <code>L1 Regularization으로 피처의 개수를 줄임과 동시에 L2 Regularization으로 계수의 값의 크기를 조정한다.</code></li></ul><h4 id="로지스틱-Logistic"><a href="#로지스틱-Logistic" class="headerlink" title="로지스틱(Logistic)"></a>로지스틱(Logistic)</h4><ul><li>로지스틱 회귀는 회귀라는 이름이 붙어 있지만, 사실은 분류에 사용되는 선형 모형이다. 로지스틱 회귀는 매우 강력한 분류 알고리즘이다. <code>일반적으로 이진 분류 뿐만아니라 희소 영역의 분류, 예를 들어 텍스트 분류와 같은 영역에서 뛰어난 예측 성능을 보인다.</code></li></ul><p><img src="/image/regression_02.png" alt="단순 선형 회귀분석"></p><h4 id="가정에서-잔차-epsilon-i-와-target-값인-Y가-정규분포를-따른다는-것이-중요하다"><a href="#가정에서-잔차-epsilon-i-와-target-값인-Y가-정규분포를-따른다는-것이-중요하다" class="headerlink" title="가정에서 잔차( $\epsilon_{i}$ )와 target 값인 Y가 정규분포를 따른다는 것이 중요하다."></a>가정에서 잔차( $\epsilon_{i}$ )와 target 값인 Y가 정규분포를 따른다는 것이 중요하다.</h4><p><img src="/image/regression_03.png" alt="단순 선형 회귀분석 - 01"></p><h3 id="회귀-계수-추정"><a href="#회귀-계수-추정" class="headerlink" title="회귀 계수 추정"></a>회귀 계수 추정</h3><p><img src="/image/estimation_of_regression_coefficient.png" alt="단순 선형 회귀 계수 추정"></p><p><img src="/image/why_square_sum.png" alt="단순 선형 회귀 계수 추정시 제곱합 형태로 추정하는 이유"></p><p><img src="/image/estimation_of_regression_coefficient_01.png" alt="단순 선형 회귀 계수 추정"></p><p><img src="/image/estimation_of_regression_coefficient_02.png" alt="단순 선형 회귀 계수 추정"></p><p><img src="/image/estimation_of_regression_coefficient_03.png" alt="단순 선형 회귀 계수 추정"></p><h3 id="회귀-계수의-의미"><a href="#회귀-계수의-의미" class="headerlink" title="회귀 계수의 의미"></a>회귀 계수의 의미</h3><p><img src="/image/mean_of_coefficient.png" alt="단순 선형 회귀계수의 해석"></p><p><img src="/image/evaluation_of_regression.png" alt="단순 선형 회귀의 평가 metric"></p><p><img src="/image/mean_of_residuals.png" alt="잔차의 의미"></p><p><img src="/image/mean_of_residuals_01.png" alt="잔차의 의미"></p><p><img src="/image/mean_of_residuals_02.png" alt="잔차의 의미"></p><h3 id="회귀-계수의-검정"><a href="#회귀-계수의-검정" class="headerlink" title="회귀 계수의 검정"></a>회귀 계수의 검정</h3><p><img src="/image/check_of_coefficient.png" alt="단순 선형 회귀 계수의 검정"></p><p><img src="/image/check_of_coefficient_01.png" alt="단순 선형 회귀 계수의 검정"></p><p><img src="/image/check_of_coefficient_02.png" alt="단순 선형 회귀 계수의 검정"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/04/machine_learning_01/#disqus_thread</comments>
    </item>
    
    <item>
      <title>머신러닝의 개요</title>
      <link>https://heung-bae-lee.github.io/2019/12/30/machine_learning_00/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/30/machine_learning_00/</guid>
      <pubDate>Mon, 30 Dec 2019 08:20:19 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Machine-learning&quot;&gt;&lt;a href=&quot;#Machine-learning&quot; class=&quot;headerlink&quot; title=&quot;Machine learning&quot;&gt;&lt;/a&gt;Machine learning&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/ima
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Machine-learning"><a href="#Machine-learning" class="headerlink" title="Machine learning"></a>Machine learning</h2><p><img src="/image/machine_learning_concept.png" alt="머신러닝의 개념 - 01"></p><h3 id="Machine-Learning으로-할-수-있는-것들"><a href="#Machine-Learning으로-할-수-있는-것들" class="headerlink" title="Machine Learning으로 할 수 있는 것들"></a>Machine Learning으로 할 수 있는 것들</h3><div class="table-container"><table><thead><tr><th>독립변수</th><th>반응변수</th><th>모형</th></tr></thead><tbody><tr><td>고객들의 개인 정보 및 금융 관련 정보</td><td>대출 연체 여부</td><td>대출 연체자 예측 탐지 모델, 대출 연체 관련 주요 feature 추출</td></tr><tr><td>게임 유저들의 게임 내 활동 정보</td><td>게임 이탈 여부. 어뷰징 여부</td><td>이상 탐지 모델(anomaly detection model)</td></tr><tr><td>숫자 손 글씨 데이터</td><td>숫자 라벨(0~9)</td><td>숫자 이미지 분류 모델</td></tr><tr><td>상품 구매 고객 특성 정보</td><td>군집화를 통한 고객 특성에 따른 Segmentation</td><td>Segmentation 모델</td></tr><tr><td>고객들의 상품 구매내역</td><td>매장내 상품 진열 위치 리뉴얼을 통한 매출 증대</td><td></td></tr><tr><td>쇼핑몰 페이지 검색 및 클릭 로그 기록</td><td>맞춤 상품 추천 시스템</td><td>recommendation system</td></tr><tr><td>SNS 데이터 및 뉴스 데이터</td><td>소셜 및 사회 이슈 파악</td></tr></tbody></table></div><p><img src="/image/what_is_f.png" alt="머신러닝의 개념 - 02"></p><h2 id="Supervised-Learning-VS-Unspervised-Learning"><a href="#Supervised-Learning-VS-Unspervised-Learning" class="headerlink" title="Supervised Learning VS Unspervised Learning"></a>Supervised Learning VS Unspervised Learning</h2><p><img src="/image/supervised.png" alt="지도학습"></p><p><img src="/image/unsupervised.png" alt="비지도학습"></p><p><img src="/image/reinforcement_learning.png" alt="강화학습"></p><p><img src="/image/examples.png" alt="비교"></p><p><img src="/image/curriculum.png" alt="데이터분석 기법의 전체적인 그림"></p><ul><li>머신러닝과 딥러닝이 실무에서 사용되어지는 경우에 팀의 구분을 두고 회사에서 운영하는 경우가 많지만, 서로 구분해서 공부하거나 이해하진 않았으면 좋겠다는것이 필자의 바람이다. 물론, 최근에는 딥러닝의 유행으로 대부분의 분석직군에서 딥러닝을 위한 인원을 채용하고 있긴 하지만 머신러닝또한 중요하지 않은 것이 아니면, 아직도 많은 곳에서 머신러닝 엔지니어들을 뽑고 있다.</li></ul><h2 id="모형의-적합성-평가-및-실험-설계"><a href="#모형의-적합성-평가-및-실험-설계" class="headerlink" title="모형의 적합성 평가 및 실험 설계"></a>모형의 적합성 평가 및 실험 설계</h2><ul><li>전체적인 머신러닝의 작업과정은 다음과 같다.<br><img src="/image/procedure.png" alt="모형의 적합성 평가 및 실험 설계"></li></ul><h3 id="전처리"><a href="#전처리" class="headerlink" title="전처리"></a>전처리</h3><ul><li>Raw 데이터를 모델링 할 수 있도록 데이터를 병합 및 파생 변수 생성<br><img src="/image/preprocessing_example.png" alt="전처리 예시"></li></ul><h3 id="실험-설계"><a href="#실험-설계" class="headerlink" title="실험 설계"></a>실험 설계</h3><p><img src="/image/train_test_split_00.png" alt="데이터 분할"></p><p><img src="/image/train_test_split.png" alt="데이터 분할 - 01"></p><ul><li>위에서 실제로 우리가 모델을 적용을 한다는 것은 예를 들어 기업에서 상용화를 한다는 가정이라는 의미이다. <code>test정보가 Train과 validation 데이터에 없어야 한다는 점은 쉽게 비유하면 시험을 보는데 우리가 이미 학습한 내용(training data)이 시험에 똑같이 나온다면 시험을 잘 볼 확률이 높아지기 때문</code>이다. <code>우리가 data를 나누었던 이유는 training data를 통해 학습된 알고리즘 모형이 학습하지 않은 새로운 데이터에서도 잘 예측할 수 있도록 하기 위한 작업이었다는 것을 잊지 말아라!</code> 또한, validation data는 parameter들을 조절하면서 최적의 모형을 선택하기 위한 데이터셋이라고 생각하면된다.</li></ul><p><img src="/image/train_test_split_check.png" alt="데이터 분할 검사"></p><ul><li><code>데이터가 잘 나누어졌는지 어느 한 쪽으로 치우쳐져 있는지 확인하는 방법으로 위의 오른쪽 그래프처럼 확인 할 수도 있을 것 같다!</code></li></ul><p><img src="/image/k_hold_cross_validation.png" alt="K-hold cross validation"></p><p><img src="/image/LOOCV.png" alt="LOOCV"></p><p><img src="/image/train_test_split_example_01.png" alt="실험 설계 예시 - 01"></p><ul><li>무조건 위에서 언급했던 k-hold cross validation으로 training data set과 validation set을 나눠서 진행하는 것이 아니고 데이터의 성격에 따라 다르게 설계를 해야한다. 예를 들면 위의 반도체 두께를 예측하는 문제에 있어서는 반도체 두께가 각 개체가 만들어지는 순서에 의해 영향을 받는다. 즉 Y2는 Y1에 의해 영향을 받고 Y3는 Y1,Y2에 의해 영향을 받는다. 그러므로 이러한 데이터의 경우에는 <code>무작정 k-hold cross validation으로 나누어주면 안된다.</code> Y2를 예측하는데 Y3에 관한 정보를 사용하게 되기 때문이다.</li></ul><p><img src="/image/train_test_split_example_02.png" alt="실험 설계 예시 - 02"></p><ul><li>또 다른 예로는 Imbalanced data에 대해 말할 수 있을 것이다. Imbalanced data는 target variable이 말 그대로 불균형한 데이터를 의미한다. 조금이라도 불균형하면 Imbalanced datas냐는 의문이 들겠지만 그런의미가 아니라 예를들면 보험회사의 보험사기라던지, 금융사기 같이 전체 데이터에서 target variable에 해당하는 데이터가 10% 정도로 희박하게 나타나는 데이터를 의미한다. <code>이런 데이터에서는 먼저 처음의 비율대로 train과 validation set으로 나누어준다. 그 다음 train data에서만 resampling을 하여 model을 학습시킨후에 추후에 validation data로 예측해보는 것이다.</code> 그러므로 그냥 k-hold가 아닌 stratified k-Fold를 통해 데이터를 train과 validation set으로 나누어 주어야한다.</li></ul><h3 id="모형-학습-및-선택"><a href="#모형-학습-및-선택" class="headerlink" title="모형 학습 및 선택"></a>모형 학습 및 선택</h3><p><img src="/image/model_learning.png" alt="모형 학습"></p><p><img src="/image/model_selection.png" alt="모형 선택"></p><h2 id="과적합-Overfitting"><a href="#과적합-Overfitting" class="headerlink" title="과적합(Overfitting)"></a>과적합(Overfitting)</h2><p><img src="/image/what_is_overfitting.png" alt="과적합"></p><p><img src="/image/Variance_and_bias_trade_off.png" alt="분산과 편향의 트레이드오프 관계"></p><p><img src="/image/Variance_and_bias_trade_off_01.png" alt="분산과 편향의 트레이드오프 관계 - 01"></p><ul><li>위의 편향과 분산의 트레이드 오프 관계에 의해 우리는 둘 중 하나를 좀더 생각해야만하는 상황에 놓이게될 것이다. 그렇다면 위의 4가지 그래프 중 어떤 모형을 선택하는 것이 좋은 것인가? 나의 개인적인 생각은 모형의 분산은 적고 편향이 높은 모형 같은 경우는 치우쳐 있는 데이터에 대한 예측성능만 높고 여러 다양한 데이터에 대한 예측 성능은 낮아지기 때문에 <code>궁극적으로 목표해야할 모형은 분산은 높지만 편향이 낮은 모형이다.</code></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/30/machine_learning_00/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering (DB에 table 만들기)</title>
      <link>https://heung-bae-lee.github.io/2019/12/17/data_engineering_05/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/17/data_engineering_05/</guid>
      <pubDate>Tue, 17 Dec 2019 08:30:11 GMT</pubDate>
      <description>
      
        
        
          &lt;h3 id=&quot;DB-Table-생성&quot;&gt;&lt;a href=&quot;#DB-Table-생성&quot; class=&quot;headerlink&quot; title=&quot;DB Table 생성&quot;&gt;&lt;/a&gt;DB Table 생성&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h3 id="DB-Table-생성"><a href="#DB-Table-생성" class="headerlink" title="DB Table 생성"></a>DB Table 생성</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## artists에 관한 table</span></span><br><span class="line"><span class="comment"># 이모지까지 커버 하고 싶은 경우</span></span><br><span class="line">mysql&gt; create table artists (id VARCHAR(255), name VARCHAR(255), followers INTEGER, popularity INTEGER, url VARCHAR(255), image_url VARCHAR(255), PRIMARY KEY(id)) ENGINE=InnoDB DEFAULT CHARSET=<span class="string">'utf8mb4'</span> COLLATE <span class="string">'utfmb4_unicode_ci'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 이모지는 제외하고 문자만 커버하는 경우</span></span><br><span class="line">mysql&gt; create table artists (id VARCHAR(255), name VARCHAR(255), followers INTEGER, popularity INTEGER, url VARCHAR(255), image_url VARCHAR(255), PRIMARY KEY(id)) ENGINE=InnoDB DEFAULT CHARSET=<span class="string">'utf8'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## artist_genres에 관한 테이블</span></span><br><span class="line">mysql&gt; create table artist_genres (artist_id VARCHAR(255), genre VARCHAR(255)) ENGINE=InnoDB DEFAULT CHARSET=<span class="string">'utf8'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 위에서 만든 artists table에 대한 info를 보고 싶은 경우</span></span><br><span class="line">mysql&gt; show create table artists;</span><br></pre></td></tr></table></figure><ul><li>앞으로 artist_genres 테이블에 어떤 artist의 장르가 추가된다면, 데이터를 추가해주어야 하는데 지속적으로 추가해주어야하므로 자동화를 할 것이다. 추가해 주는 값이 테이블에 이미 있는 값을 갖는 데이터가 들어온다면 무의미할 것이다. <code>아무런 column에 제약을 주지 않았기에 insert를 통한 데이터 추가 방식은 무의미하다.</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터 추가</span></span><br><span class="line">mysql&gt; insert into artist_genres (artist_id, genre) values (<span class="string">'1234'</span>, <span class="string">'pop'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 테이블 확인</span></span><br><span class="line">mysql&gt; select * from artist_genre;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터 추가</span></span><br><span class="line">mysql&gt; insert into artist_genres (artist_id, genre) values (<span class="string">'1234'</span>, <span class="string">'pop'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 테이블 확인</span></span><br><span class="line">mysql&gt; select * from artist_genre;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 테이블 값만 삭제</span></span><br><span class="line">mysql&gt; delete from artist_genres;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 테이블 자체를 삭제</span></span><br><span class="line"><span class="comment"># 실무에서는 drop은 잘 사용하지 않고, Alter를 사용한다.</span></span><br><span class="line">mysql&gt; drop table artist_genres;</span><br></pre></td></tr></table></figure><ul><li><code>앞에서 말한 추후에 데이터 입력시 동일한 데이터를 계속 추가하는 문제를 방지하기 위해 column에 unique key 속성을 추가</code>해 주었다.</li></ul><ul><li><code>이전의 방법과 동일하게 추가했을 경우 insert into구문은 오류를 발생하여서 추가 데이터를 저장하진 않지만, Python script가 도중에 멈추게 되는 문제가 생긴다는 점을 확인 할 수 있다.</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unique key 속성 부여</span></span><br><span class="line">mysql&gt; create table artist_genres (artist_id VARCHAR(255), genre VARCHAR(255), unique key(artist_id, genre)) ENGINE=InnoDB DEFAULT CHARSET=<span class="string">'utf8'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 속성 추가 확인</span></span><br><span class="line">mysql&gt; show create table artist_genres;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터 추가</span></span><br><span class="line">mysql&gt; insert into artist_genres (artist_id, genre) values (<span class="string">'1234'</span>, <span class="string">'pop'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터 확인</span></span><br><span class="line">mysql&gt; select * from artist_genres;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터 추가</span></span><br><span class="line">mysql&gt; insert into artist_genres (artist_id, genre) values (<span class="string">'1234'</span>, <span class="string">'pop'</span>);</span><br><span class="line"></span><br><span class="line">ERROR 1062 (23000): Duplicate entry <span class="string">'1234-pop'</span> <span class="keyword">for</span> key <span class="string">'artist_id'</span></span><br></pre></td></tr></table></figure><ul><li><p>insert into 구문 대신 <code>update set 구문을 사용해보았다.</code> 우선 키값이 있기 때문에 중복되는 값이 저장되지는 않는다. <code>update set 구문은 NULL데이터를 가지고 있었다면 변경되지 않는다는 문제점이 있다.</code></p></li><li><p>replace into 구문을 통해 값이 변경되기 했지만 성능적인 측면에서, 데이터가 많을땐 먼저 키값이 있는지 찾고, 존재한다면 지금처럼 그 행을 지우고 새로운 행으로 바꿔준다. 또한,</p></li><li><p>primary_key auto_increment를 통해 설정되어있던 테이블이라면 원래 primary_key로 인해 부여 받은 값이 아닌 새로운 값을 부여받게된다는 문제점이 있다.</p></li><li><p><code>on duplicate key update</code>를 통해 문제점을 해결 할 수 있다!</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터 추가</span></span><br><span class="line">mysql&gt; update artist_genres <span class="built_in">set</span> genre=<span class="string">'pop'</span> <span class="built_in">where</span> artist_id=<span class="string">'1234'</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># columns 추가</span></span><br><span class="line">mysql&gt; alter table artist_genres add column country VARCHAR(255);</span><br><span class="line"></span><br><span class="line"><span class="comment"># update 되는 시점을 갖는 column 추가</span></span><br><span class="line"><span class="comment"># 자동적으로 데이터가 추가 될때마다 그 시점이 저장됨</span></span><br><span class="line">mysql&gt; alter table artist_genres add column updated_at timestamp default current_timestamp on update current_timestamp;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 동일한 키값을 갖는 데이터가 이미 존재하므로 오류를 방생시킴.</span></span><br><span class="line">mysql&gt; INSERT INTO artist_genres (artist_id, genre, country) VALUES (<span class="string">'1234'</span>, <span class="string">'pop'</span>, <span class="string">'UK'</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment"># replace into 구문을 통해 값이 변경되기 했지만 성능적인 측면에서, 데이터가 많을땐 먼저 키값이 있는지 찾고, 존재한다면 지금처럼 그 행을 지우고 새로운 행으로 바꿔준다.</span></span><br><span class="line">mysql&gt; REPLACE INTO artist_genres (artist_id, genre, country) VALUES (<span class="string">'1234'</span>, <span class="string">'pop'</span>, <span class="string">'UK'</span>);</span><br><span class="line">Query OK, 2 rows affected (0.28 sec)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 위에서 replace into 구문을 통해 값이 변경되었음을 확인 할 수 있다.</span></span><br><span class="line">mysql&gt; select * from artist_genres;</span><br><span class="line"></span><br><span class="line"><span class="comment"># artist_id, genre가 Unique key인데 두 컬럼을 동시에 동일한 값을 갖는 row가 없으므로 새로 추가 해준다.</span></span><br><span class="line">mysql&gt; REPLACE INTO artist_genres (artist_id, genre, country) VALUES (<span class="string">'1234'</span>, <span class="string">'rock'</span>, <span class="string">'UK'</span>);</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from artist_genres;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 말 그대로 동일한 키값이 있으면 그냥 insert into 구문은 오류를 발생시켰지만, insert ignore into 구문은 오류를 발생시키지 않고 무시한다.</span></span><br><span class="line">mysql&gt; insert ignore into artist_genres (artist_id, genre, country) VALUES (<span class="string">'1234'</span>, <span class="string">'rock'</span>, <span class="string">'FR'</span>);</span><br><span class="line"></span><br><span class="line">mysql&gt; select * from artist_genres;</span><br><span class="line"></span><br><span class="line"><span class="comment"># on duplicate key update</span></span><br><span class="line">mysql&gt; insert into artist_genres (artist_id, genre, country) values (<span class="string">'1234'</span>, <span class="string">'rock'</span>, <span class="string">'FR'</span>) on duplicate key update artist_id=<span class="string">'1234'</span>, genre=<span class="string">'rock'</span>, country=<span class="string">'FR'</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 우리가 입의로 넣어 주었던 country는 지워줄 것이다.</span></span><br><span class="line">mysql&gt; alter table artist_genres drop column country;</span><br></pre></td></tr></table></figure><h4 id="결론은-다음-구문을-query문으로-사용하겠다는-것이다"><a href="#결론은-다음-구문을-query문으로-사용하겠다는-것이다" class="headerlink" title="결론은 다음 구문을 query문으로 사용하겠다는 것이다."></a>결론은 다음 구문을 query문으로 사용하겠다는 것이다.</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; insert into artist_genres (artist_id, genre, country) values (<span class="string">'1234'</span>, <span class="string">'rock'</span>, <span class="string">'FR'</span>) on duplicate key update artist_id=<span class="string">'1234'</span>, genre=<span class="string">'rock'</span>, country=<span class="string">'FR'</span></span><br></pre></td></tr></table></figure><h3 id="결과-이미지"><a href="#결과-이미지" class="headerlink" title="결과 이미지"></a>결과 이미지</h3><ul><li>python script를 짜고 추후에 script를 실행하여 바로 RDS에 저장하고 table이 제대로 생성됬는지 확인하였다.</li></ul><p><img src="/image/artist_table.png" alt="artist 테이블 생성"><br><img src="/image/artist_genres_table.png" alt="artist genres 테이블 생성"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/17/data_engineering_05/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering (AWS로 DB 만들기)</title>
      <link>https://heung-bae-lee.github.io/2019/12/15/data_engineering_04/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/15/data_engineering_04/</guid>
      <pubDate>Sun, 15 Dec 2019 06:16:36 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;AWS-RDB-만들기&quot;&gt;&lt;a href=&quot;#AWS-RDB-만들기&quot; class=&quot;headerlink&quot; title=&quot;AWS RDB 만들기&quot;&gt;&lt;/a&gt;AWS RDB 만들기&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Spotify data를 크롤링 하고난 후에 AWS
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="AWS-RDB-만들기"><a href="#AWS-RDB-만들기" class="headerlink" title="AWS RDB 만들기"></a>AWS RDB 만들기</h2><ul><li>Spotify data를 크롤링 하고난 후에 AWS RDB에 저장하기 위해서 먼저 DB를 만들어 줄 것이다.<br><a href="https://www.youtube.com/results?search_query=%EC%B4%88%EB%B3%B4%EC%9E%90%EB%A5%BC+%EC%9C%84%ED%95%9C+AWS+%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0" target="_blank" rel="noopener">Youtube 초보자를 위한 AWS 시작하기!</a></li></ul><h3 id="AWS-RDS-생성"><a href="#AWS-RDS-생성" class="headerlink" title="AWS RDS 생성"></a>AWS RDS 생성</h3><ul><li><p>개발자나 필자 처럼 데이터를 분석하는 분들을 제외한 아마 Amazon이라는 단어를 듣게 된다면, 물건을 사고파는 뭐 그런 웹사이트 페이지를 떠올리는 분들이 많을 것이다. 허나, Amazon Web Service(AWS)는 Amazon의 그런 이미지와는 다르다. <code>Cloud Service</code>를 제공해주는 것이다.</p></li><li><p>우선 가입을 해야한다. 참고로 대학생인 분들은 <a href="https://www.google.com/search?hl=ko&amp;sxsrf=ACYBGNTcqJdihtFh_XYEH08MOfxSkRO-QQ%3A1576396697970&amp;ei=mef1XeXrOtDmwQOPgYvQDw&amp;q=aws+%ED%95%99%EC%83%9D%EA%B3%84%EC%A0%95&amp;oq=AWS&amp;gs_l=psy-ab.3.0.35i39l3j0i67j0i20i263j0i67j0j0i67j0j0i67.76947.77683..78517...3.0..0.136.371.0j3......0....1..gws-wiz.....10..35i362i39j0i131..21%3A1j22%3A0.2vj_C1Bu8zw" target="_blank" rel="noopener">AWS educate</a>로 가입하면 Credit을 받는 방법이 있는데, 뭐 꼭 현재 재학중이지 않아도 자신의 대학교 이메일로 인증이 가능하다면 AWS Educate에 Student신분으로 가입이 가능하다. 만약 대학생이 아닌 분들은 그냥 AWS(AWS와 AWS educate는 다르다.)를 가입해서 사용하면 된다. 참고로 1년 동안은 어느정도 free tier를 주어서 몇몇 서비스들은 무료로 오래 이용가능할 것이다. 필자도 작년 이용했었는데 기간이 만료되어 이번에 다시 다른 계정을 만들었는데, 새로운 계정을 만들면 또 free tier 이용이 가능한 것 같다.(개인적인 생각이지만 아마도 가입시에 적는 신용카드 번호가 다른 것이면 가능한듯 하다.) 또한, 가입시에 적는 신용카드는 결제 카드로 설정되며 free tier로 이용하는 것을 제외한 다른 이용료를 결제할 수 있다. 물론, 자동결제는 아니고 자신이 결제해줘야 하며, 결제를 해주지 않는다면 휴면 계정으로 전환시켜 서비스 이용이 불가능하다.(처음 결제되는 $1는 결제가 되는 카드인지 확인하는 확인용으로 알고있는데 나중에 결제 취소 해주므로 걱정하지 않아도 된다.)</p></li></ul><ul><li>다음의 서비스 중에 우선 RDM을 생성해 줄 것이다.</li></ul><p><img src="/image/service.png" alt="AWS에서 제공하는 서비스"></p><ul><li><code>step 1)</code> Create database를 클릭, method에서 직접 Customize하려면 Standard를 체크!(easy 방법은 이미 Instancd Size와 ram 등 사양들을 AWS Image처럼 만들어 놓은 형태로 되어있다.) 또한, Python을 통해 사용할 것이므로 MySQL로 만들 것이다.</li></ul><p><img src="/image/create_database.png" alt="AWS에서 DB 생성"></p><ul><li>version은 제일 stable한 5.7.22 version을 선택! Templete은 Free-Tier를 선택! 만약 바로 실무에서 사용해야 한다면 Production을 사용하면 된다. 필자는 연습용으로 만드는 것이므로 Free를 선택 !!!</li></ul><p><img src="/image/version_tier.png" alt="AWS에서 DB 생성 1"></p><ul><li>DB instance identifier는 DB의 이름이고, 그 아래 Credentials Settings의 Master username은 DB 접속시 Master 권한을 인증할 ID와 password이다.</li></ul><p><img src="/image/settings.png" alt="AWS에서 DB 생성 2"></p><ul><li>아래로 내려 갈수록 저사양 DB이며, 아마 필자와 동일하게 Free-Tier를 선택했다면, 이미 맨 아래 단계로 설정 되어있을 것이다. Free-Tier의 경우에는 다른 사양을 선택할 수 없다.</li></ul><p><img src="/image/DB_instance_size.png" alt="AWS에서 DB 생성 3"></p><ul><li>Storage type<ul><li>General Purpose : 주로 저장할 때 사용</li><li>Provisioned IOPS(Input Output Per Second): 데이터의 입출력을 빠르게 접근할 수 있게 해야할 경우 사용</li></ul></li></ul><ul><li>Storage autoscaling<ul><li>Enable storage autoscaling은 할당한 자원이 초과되어 다른 여유자원이 있다면, 자동으로 여유자원을 가져와 사용할 수 있게끔해주는 설정이다.</li></ul></li></ul><p><img src="/image/Storage.png" alt="AWS에서 DB 생성 4">    </p><p>아래에 있는 설정 사항들은 모두 기본값을 설정했다. 이제 맨 아래로 가서 생성을 클릭하면 된다.</p><ul><li><p>참고) Multi-AZ deployment</p><ul><li>접속하는 User의 지역에 상관없이 동일하게 Performance를 내도록 할 때 사용</li></ul></li><li><p>왼쪽의 Database 탭을 클릭하면, 다음과 같이 본인의 DB에 대한 창이 나올 것이다. 아직 생성중일 것이다. 우선, 본인의 DB명을 클릭하자.</p></li></ul><p><img src="/image/creating_database.png" alt="AWS에서 DB 생성 5-1"><br><img src="/image/database_spotify.png" alt="AWS에서 DB 생성 5-2"></p><ul><li>Connectivity &amp; Security 탭을 살펴보면 <code>Public accessibility가 No로 되어있을 텐데 이것을 Yes로 설정을 바꿔주어야 접속이 가능</code>하다.</li></ul><p><img src="/image/Public_accessibility.png" alt="AWS에서 DB 생성 5-3"><br><img src="/image/instance_modify.png" alt="AWS에서 DB 생성 5-4"><br><img src="/image/Public_accessibility_01.png" alt="AWS에서 DB 생성 5-5"></p><ul><li>아래 Connectivity &amp; Security 탭에서 Security의 빨간네모칸 부분을 누르면 <code>앞으로 DB에 접속이 가능한 프로토콜 설정하거나 관리할 수 있는 페이지로 이동한다.</code></li></ul><p><img src="/image/Connectivity_Security.png" alt="AWS에서 DB 생성 6"></p><ul><li>edit 버튼을 눌러 <code>DB에 connect 할 때 접속가능한 프로토콜을 설정</code>해준다.</li></ul><p><img src="/image/Inbound_edit.png" alt="AWS에서 DB 생성 7"></p><ul><li>MySQL로 접속이 가능하게끔 설정을 추가해주고 저장한다.</li></ul><p><img src="/image/Inbound_add.png" alt="AWS에서 DB 생성 8-1"><br><img src="/image/Inbound_after.png" alt="AWS에서 DB 생성 8-2"></p><ul><li>다시 Database 탭으로 돌아가면, 아마도 여러분의 DB가 만들어졌을 것이다.</li></ul><p><img src="/image/DB_available.png" alt="AWS에서 DB 생성 9"></p><h3 id="Command-Line으로-DB-접속하기"><a href="#Command-Line으로-DB-접속하기" class="headerlink" title="Command Line으로 DB 접속하기"></a>Command Line으로 DB 접속하기</h3><h4 id="command-도움말을-먼저-확인하여-접속시-필요한-옵션들을-알아보자"><a href="#command-도움말을-먼저-확인하여-접속시-필요한-옵션들을-알아보자" class="headerlink" title="command 도움말을 먼저 확인하여 접속시 필요한 옵션들을 알아보자."></a>command 도움말을 먼저 확인하여 접속시 필요한 옵션들을 알아보자.</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><h4 id="접속시에-필요한-간단한-옵션들은-다음과-같다"><a href="#접속시에-필요한-간단한-옵션들은-다음과-같다" class="headerlink" title="접속시에 필요한 간단한 옵션들은 다음과 같다."></a>접속시에 필요한 간단한 옵션들은 다음과 같다.</h4><div class="table-container"><table><thead><tr><th style="text-align:center">-h, —host=name</th><th>=&gt; Connect to host.</th></tr></thead><tbody><tr><td style="text-align:center">-p,—password[=name]</td><td>=&gt; Password to use when connecting to server.  If password is not given it’s asked from the tty.</td></tr><tr><td style="text-align:center">-P, —port=#</td><td>=&gt; Port number to use for connection or 0 for default to,  in order of preference, my.cnf, $MYSQL_TCP_PORT,  /etc/services, built-in default (3306).</td></tr><tr><td style="text-align:center">-u, —user=name</td><td>=&gt; User for login if not current user.</td></tr></tbody></table></div><h4 id="접속"><a href="#접속" class="headerlink" title="접속!!"></a>접속!!</h4><ul><li>-p 옵션까지만 치면 password를 입력하라고 할 텐데, 입력하면 접속이 된다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql -h end-point -P 3306 -u userId -p</span><br><span class="line"></span><br><span class="line">show databases;</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/15/data_engineering_04/#disqus_thread</comments>
    </item>
    
    <item>
      <title>선형 대수 공부할 때 도움되는 사이트</title>
      <link>https://heung-bae-lee.github.io/2019/12/14/linear_algebra_00/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/14/linear_algebra_00/</guid>
      <pubDate>Sat, 14 Dec 2019 07:24:18 GMT</pubDate>
      <description>
      
        
        
          &lt;h3 id=&quot;기초-선형-대수-공부할-때-도움되는-사이트&quot;&gt;&lt;a href=&quot;#기초-선형-대수-공부할-때-도움되는-사이트&quot; class=&quot;headerlink&quot; title=&quot;기초 선형 대수 공부할 때 도움되는 사이트&quot;&gt;&lt;/a&gt;기초 선형 대수 공부할 때 도움
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h3 id="기초-선형-대수-공부할-때-도움되는-사이트"><a href="#기초-선형-대수-공부할-때-도움되는-사이트" class="headerlink" title="기초 선형 대수 공부할 때 도움되는 사이트"></a>기초 선형 대수 공부할 때 도움되는 사이트</h3><ul><li><p>아래의 설명들은 사이트들에 대한 주관적인 의견이므로, 개인마다 차이가 있을 것이다.</p></li><li><p><a href="https://towardsdatascience.com/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c" target="_blank" rel="noopener">선형대수 cheat sheet</a></p><ul><li>엄청 기본적인 개념들을 모르는 분들께만 추천</li></ul></li></ul><ul><li><a href="https://ko.khanacademy.org/math/linear-algebra" target="_blank" rel="noopener">칸 아카데미</a><ul><li>기본적인 개념부터 시작해서 그래프를 통해 기하학적인 부분을 많이 보여주는 강의.</li></ul></li></ul><ul><li><a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/" target="_blank" rel="noopener">Gilbert Strang 교수님 강의</a><ul><li>기본적인 개념부터 차근차근 그리고 조금은 직관적으로 선형대수를 공부하고 싶다면 추천</li></ul></li></ul><ul><li><a href="http://www.kocw.net/home/search/kemView.do?kemId=977757" target="_blank" rel="noopener">한양대 이상화 교수님 선형대수 강의(Kocw)</a><ul><li>개인적으로 이 강의를 제일 먼저 접하게 되었었고, 추후에 길버트 교수님의 강의와 비교하면 개념에 대한 설명이 좀 더 공학적이지만, 예시를 통한 설명으로 극복이 가능하다. 필자는 머리가 뛰어난 편은 아니어서 처음 들었을 땐 솔직히 노트에 적으면서 공부했어도 이해를 하지 못했으나, 두번째 들으면서 각 개념들의 연결고리를 생각하고 이해하며 듣게되어 훨씬 좋았다.</li></ul></li></ul><ul><li><a href="http://cs231n.github.io/python-numpy-tutorial/" target="_blank" rel="noopener">CS231n Numpy tutorial</a><ul><li>기본 개념을 조금은 익힌 후 Python을 통해 실습해 보고 싶은 분들께 추천</li></ul></li></ul><ul><li><a href="https://www.edwith.org/linearalgebra4ai" target="_blank" rel="noopener">인공지능을 위한 선형대수 in edwith</a><ul><li>위의 개념을 위주로 한 강의를 들은 후에 복습차원에서 빠르게 듣는 것을 추천한다. 개념에 대한 설명이 부족한 강의는 절대적으로 아니지만 위의 개념적인 강의를 들으면서 스스로 먼저 생각하고 고민한 뒤에 이 강의를 수강하면 더 효과적일 것 이다. 또한 중간 중간 Python에 의한 실습도 제공한다.</li></ul></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/14/linear_algebra_00/#disqus_thread</comments>
    </item>
    
    <item>
      <title>추천시스템(Recommendation System)</title>
      <link>https://heung-bae-lee.github.io/2019/12/14/Recommendation_System_00/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/14/Recommendation_System_00/</guid>
      <pubDate>Sat, 14 Dec 2019 02:34:02 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;1-추천-시스템-Recommendation-System-이란&quot;&gt;&lt;a href=&quot;#1-추천-시스템-Recommendation-System-이란&quot; class=&quot;headerlink&quot; title=&quot;1) 추천 시스템(Recommendation S
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="1-추천-시스템-Recommendation-System-이란"><a href="#1-추천-시스템-Recommendation-System-이란" class="headerlink" title="1) 추천 시스템(Recommendation System)이란?"></a>1) 추천 시스템(Recommendation System)이란?</h2><p><a href="https://ko.wikipedia.org/wiki/%EC%B6%94%EC%B2%9C_%EC%8B%9C%EC%8A%A4%ED%85%9C" target="_blank" rel="noopener">위키백과의 정의를 통해 먼저 정리해보자!</a></p><ul><li>정보 필터링 (IF) 기술의 일종</li><li>특정 사용자가 관심을 가질만한 정보 (영화, 음악, 책, 뉴스, 이미지, 웹 페이지 등)를 추천하는 것</li></ul><h2 id="ㄴ-종류-Different-Types-of-Recommendation-Engines"><a href="#ㄴ-종류-Different-Types-of-Recommendation-Engines" class="headerlink" title="ㄴ.종류(Different Types of Recommendation Engines) :"></a>ㄴ.종류(Different Types of Recommendation Engines) :</h2><h3 id="1-협업-필터링-기법-Collaborative-filtering"><a href="#1-협업-필터링-기법-Collaborative-filtering" class="headerlink" title="1) 협업 필터링 기법(Collaborative filtering)"></a>1) <code>협업 필터링 기법(Collaborative filtering)</code></h3><ul><li><p>기본적인 가정이 <code>과거에 동의한 사람들이 미래에도 동의하고 그들이 과거에 좋아했던 것들을 좋아할 것이라는 가정</code>을 바탕으로 <code>다른 사용자와의 비슷함에 기초를 두고 사용자들이 무엇을 좋아할 지를 예측하는 것</code>에 기초에 두고 있다. Linkedin, facebook과 같은 SNS는 collaboprative filtering을 친구 추천 등에 사용한다.</p></li><li><p>가장 큰 장점인 machine analyzable content에 의존하고 있지 않다는 점으로 인해 정확하게 <code>item 그 자체를 이해하지 않고도 영화와 같은 복잡한 item들을 추천 할 수 있다.</code></p></li><li><p>주로 사용되는 알고리즘 : <code>KNN, Pearson Correlation</code></p><ul><li>모델을 만들 때, feature는 Explicit하거나 Implicit한 data collection 사이에서 만들어진다.</li><li>Explicit data collection의 예<ul><li>사용자에게 item을 평가하게하기, 검색하게 하기, 가장 선호하는 것과 가장 덜 선호하는 것을 순위매기게 하기 등</li></ul></li><li>Implicit data collection의 예<ul><li>사용자가 본 item을 관찰하고 분석하기, 사용자가 구매한 item을 기록하기, 사용자의 SNS를 분석하고 비슷한 likes와 dislike를 찾아내기!</li></ul></li></ul></li></ul><p><img src="/image/Collaborative_filtering.png" alt="Collaborative filtering">    </p><h3 id="2-컨텐츠-기반-필터링-Content-based-filtering"><a href="#2-컨텐츠-기반-필터링-Content-based-filtering" class="headerlink" title="2) 컨텐츠 기반 필터링(Content-based filtering)"></a>2) <code>컨텐츠 기반 필터링(Content-based filtering)</code></h3><ul><li><p><code>Keyword</code>(item을 설명(describe)하는데 사용함.), <code>Profile</code>(사용자가 좋아하는 type의 item을 가리키게(indicate) 만들어짐.)을 통해 <code>과거에 사용자자가 좋아했던 것들 (또는 현재 보고 있는 것들)과 비슷한 items을 추천하려고 한다</code>. 다양한 후보 items는 사용자에 의해 평가되는(rated) items와 비교되고 그 중 best-matching items를 추천한다. Pandora Radio는 첫 seed와 같이 사용자에 의해 제공된 노래와 비슷한 특징의 음악을 재생해 주는 content-based recommendation System이다.</p></li><li><p>이 접근법은 집합적 정보로부터 원하는 내용이나 관련된 내용을 가져오는 <code>Inforamtion retrieval</code>과 필요없는 정보를 제거하는 <code>Information filtering</code>에 뿌리를 두고 있다. Items의 특징(Keyword)을 끌어내기 위해 <code>TF-IDF(Term frequency-inverse document frequency)</code>를 사용한다</p></li><li><p>User의 profil을 만들기 위해서, 그 시스템은 대게 두가지 정보에 집중한다.</p><ul><li>1) 사용자의 선호의 model</li><li><p>2) 추천시스템과 사용자의 상호작용 정보(history)</p></li><li><p>기본적으로 이런 방법들은 시스템 안에서 item에 특성을 부여하면서 item profile(이산적 feature와 attributes)을 사용한다. 그 시스템은 item 특성의 <code>weighted vector을 기반으로 한 사용자의 content-based profile</code>을 만든다. <code>Weights</code>는 <code>사용자에게 각각의 feature의 중요도</code>를 나타내고 개별적으로 점수 매겨진(rated) content vector로 부터 다양한 방법으로 계산될 수 있다. 사용자가 좋아할 것 같 확률을 계산하기 위해 복잡한 방법들(베이지안 분류, 클러스터 분석, 결정트리, 그리고 인공 신경망 네트워크왁 같은 머신러닝 기술을 사용하는 반면에, 간단한 접근법들은 그 점수 매겨진 item vector의 평균 값을 사용한다.</p></li><li><p>보통 ‘좋아요’와 ‘싫어요’와 같은 형태로 사용자로부터 직접적인 피드백은 특정한 속성(attribute)의 중요도에 대한 더 높거나 낮은 weight를 할당하는데 사용될 수 있다.</p></li></ul></li><li><p><code>Content-based filtering의 중요한 문제점</code>은, <code>하나의 content source에 관련된 사용자들 행동으로부터 사용자 선호도를 배울 수 있고 다른 content 종류(type)에 대해서도 배운 사용자 선호도들을 적용시킬 수 있는지에 대한 여부이다.</code> 그 시스템이 다른 서비스의 다른 종류의 content를 추천할 수 있는 것보다 사용자가 이미 사용한 것과 같은 종류의 content를 추천하는 것에 한정돼 있다면 해당 추천 시스템의 가치는 상당히 낮을 것이다. 예를 들어, news browsing에 기반한 추천 뉴스 기사는 유용하지만, news browsing에 기반해 추천될 수 있는 다른 서비스의 음악, 비디오, 제품 토론에서 더 유용하다.</p></li></ul><p><img src="/image/Content_Based_Filtering.png" alt="Content_Based_Filtering"></p><h3 id="참고-TF-IDF"><a href="#참고-TF-IDF" class="headerlink" title="참고) TF-IDF"></a>참고) TF-IDF</h3><ul><li><p>정보검색과 텍스트 마이닝에서 이용하는 가중치로, 여러 문서로 이루어진 문서군이 있을 때 어떤 단어가 특정 문서 내에서 얼마나 중요한 것인지를 나타내는 통계적 수치이다. 문서의 핵심어를 추출하거나 검색엔진에서 검색 결과의 순위를 결정하거나,문서들 사이의 비슷한 정도를 구하는 등의 용도로 사용할 수 있다.</p></li><li><p>IDF 값은 문서군의 성격에 따라 결정된다. 예를 들어, ‘원자’라는 낱말은 일반적으로 문서들 사이에서는 잘 나오지 않기 때문에 IDF 값이 높아지고 문서의 핵심어가 될 수 있지만, 원자에 대한 문서를 모아놓은 문서군의 경우 이 낱말은 상투어가 되어 각 문서들을 세분화하여 구분할 수 있는 다른 낱말들이 높은 가중치를 얻게 된다.</p></li><li><p><code>특정 문서 내에서 단어 빈도가 높을수록, 그리고 전체 문서들 중 그 단어를 포함한 문서가 적을수록  TF-IDF값이 높아진다. 따라서 이 값을 이용하면 모든 문서에 흔하게 나타나는 단어를 걸러내는 효과를 얻을 수 있다. IDF의 로그 함수값은 항상 1이상이므로, IDF값과 TF-IDF값을 항상 0 이상이 된다. 특정 단어를 포함하는 문서들이 많을 수록 로그 함수 안의 값이 1에 가까워지게 되고, 이 경우 IDF값과 TF-ID값은 0에 가까워지게 된다.</code></p></li></ul><script type="math/tex; mode=display">TF-IDF = TF \times IDF</script><ul><li><code>TF(Term Frequency, 단어 빈도)</code><ul><li><code>특정한 단어가 문서 내에 얼마나 자주 등장하는지를 나타내는 값</code></li><li>산출 방식<ul><li>ㄴ. TF :<script type="math/tex; mode=display">tf(t, d) = f(t, d) (f(t, d) : 문서 d 내에서 단어 t의 총 빈도)</script></li><li>ㄴ. Boolean TF :<script type="math/tex; mode=display">tf(t, d) = t가 d에 한 번이라도 나타나면 1, 아니명 0</script></li><li>ㄴ. log scale TF :<script type="math/tex; mode=display">tf(t, d) = \log(f(t, d) +1)</script></li><li>ㄴ. 증가 빈도 TF :<ul><li>일반적으로는 문서의 길이가 상대적으로 길 경우, 단어 빈도값을 조정하기 위해 사용</li></ul></li></ul></li></ul></li></ul><script type="math/tex; mode=display">tf(t, d) =  0.5 + \frac{0.5 \times f(t, d)}{max{f(w, d) : w \in d}} = 0.5 + \frac{0.5 \times target 단어에 대한 TF}{동일 문서(문장)내에서의 최빈단어의 빈도수}</script><ul><li><p><code>IDF(Inverse Document Frequency, 역문서 빈도)</code></p><ul><li><p>TF 값이 높을 수록 문서에서 중요하다고 생각될 수도 있지만 단순히 흔하게 등장하는 것일 수도 있다. 이값을 DF(Document Frequency, 문서 빈도)라고한다. 영어문장에서 예를 들자면 가령 I, you 같은 단어들을 예로 들 수 있을 것이다. <code>DF의 역수를 IDF(Inverse Document Frequency, 역문서 빈도)</code>라고 한다. <code>한 단어가 문서 집합 전체에서 얼마나 공통적으로 나타나는지를 나타내는 값</code></p></li><li><p>IDF 값은 문서군의 성격에 따라 결정된다. 예를 들어, ‘원자’라는 낱말은 일반적으로 문서들 사이에서는 잘 나오지 않기 때문에 IDF 값이 높아지고 문서의 핵심어가 될 수 있지만, 원자에 대한 문서를 모아놓은 문서군의 경우 이 낱말은 상투어가 되어 각 문서들을 세분화하여 구분할 수 있는 다른 낱말들이 높은 가중치를 얻게 된다.</p></li><li><p>산출 방식</p><ul><li>$ \mid D \mid$ : 문서 집합 D의 크기, 또는 전체 문서의 수</li><li>$ \mid d \in D : t \in d  \mid$ : 단어 t가 포함된 문서의 수(즉, $tf(t,0) \neq 0$). <code>단어가 전체 말뭉치(Corpus)안에 존재하지 않을 경우 이는 분모가  0이 되는 결과를 가져온다. 이를 방지하기 위해</code> $1 +  \mid d \in D : t \in d  \mid $로 쓰는 것이 일반적이다.</li></ul></li></ul></li></ul><script type="math/tex; mode=display">idf(t, D) = \log \frac{ \mid D  \mid}{ \mid d \in D : t \in d  \mid} =  \log \frac{전체 문서의 수}{해당 Target단어를 포함한 문서의 수}</script><h3 id="3-Hybrid-Recommendation-Systems"><a href="#3-Hybrid-Recommendation-Systems" class="headerlink" title="3) Hybrid Recommendation Systems"></a>3) <code>Hybrid Recommendation Systems</code></h3><ul><li><p>위의 2가지 Recommendation System들은 각각의 장점과 단점이 존재함을 살펴보았다. 그래서 최근 연구는 <code>Collaborative filtering과 content-based filtering을 섞은 Hybrid 접근법</code>이 몇몇의 상황(<code>Cold start(충분한 정보가 없어서 필요한 정보를 얻지 못하는 것)) Sparsity</code>)에서 <code>더 효과적</code>일 수 있다고 설명한다.</p></li><li><p><code>Hybrid 추천 시스템</code>이란 용어는 아래의 각 시스템별 단점들을 보완하기 위해 <code>다중의 추천 기술을 함께 섞는 어떠한 추천 시스템</code>을 의미하며 다중의 추천 기술이 내포하고 있는 의미는 동일한 기술을 여러개 겹치는 것도 포함된다.</p></li></ul><div class="table-container"><table><thead><tr><th>Collaborative</th><th>이 시스템은 다른 사용자들과 items에 대한 profiles을 평가하는 정보만 사용하면서 추천을 한다.  이 시스템은 현재의 사용자나 items과 비슷한 평가 기록(history)와 함께 비슷한(peer) 사용자  또는 items을 배치하고, 이 근접이웃(neighborhood)를 이요해서 추천을 만든다. 사용자 기반과  item기반의 가장 가까운 이웃 알고리즘은 cold-start문제를 해결하기 위해 합쳐질 수 있고 추천  결과를 향상시킬 수 있다.</th></tr></thead><tbody><tr><td>Content-based</td><td>이 시스템은 사용자가 그들에게 준 평가와 제품들과 관련된 특징이라는 두가지 Sources로부터 추천을  만든다. Content-based 추천자는 추천을 user-specific 분류 문제처럼 다루고 제품의 특징에  기반한 사용자의 좋아요와 싫어요의 분류자를 학습한다.</td></tr><tr><td>Demographic</td><td>Demographic(인구 통계학적) 추천은 사용자의 인구통계학적 정보(profile)를 기반으로 추천을  제공한다. 추천된 제품은 그 영역의 사용자들의 평가들을 합침으로써 다른 인구통계학적 영역을 위해  만들어 질 수 있다.</td></tr><tr><td>Knowledge-based</td><td>이 추천자는 사용자의 선호와 요구(needs)에 대한 추론을 기반으로 한 제품을 제안한다.  이 지식(knowledge)는 때때로 얼마나 특정한 제품 특징이 사용자의 요구를 충족시키는  지에 대한 뚜렷한 기능적(functional) 지식을 포함한다.</td></tr></tbody></table></div><ul><li>Netflix는 Hybrid 추천 시스템의 좋은 예이다. 사용자가 높게 평가했던(Content-based)영화와 비슷한 feature를 띄는 영화를 추천하고, 비슷한 사용자(collaborate)들의 검색 습관과 시청을 비교함으로서 추천을 한다.</li></ul><p><img src="/image/Hybrid_Recommendation.png" alt="Hybrid Recommendation"></p><h3 id="정확도를-넘어서"><a href="#정확도를-넘어서" class="headerlink" title="정확도를 넘어서"></a>정확도를 넘어서</h3><p>전형적으로, 추천 시스템에 대한 연구는 가장 정확한 추천 알고리즘을 찾는 것에 관심을 둔다. 하지만, 많은 중요한 요소들이 있다.</p><ul><li><p><code>Diversity</code></p><ul><li>사용자들은 자신이 선택한 Item과 유사성이 높은 intra-list에 포함된 다른 아티스들을 보이는 다양성을 갖춘 추천 시스템에 더 만족하는 경향을 보인다.</li></ul></li><li><p><code>Recommender persistence</code></p><ul><li>어떤 상황에서, 추천시스템이 그 이전의 추천과 동일한 추천을 다시 보여주거나 사용자가 다시 items을 평가하게 하는 것이 더 효과적이다.</li></ul></li><li><p><code>Privacy</code></p><ul><li>추천 시스템은 대게 privacy 문제를 해결해야 한다. 왜냐하면 사용자들은 민감한 정보를 공개해야하기 때문이다. Collaborative filtering을 사용해 사용자의 Profiles을 만드는 것은 privacy의 관점에서 문제가 될 수 있다. 많은 유럽 국가들은 data privacy에 대한 강한 문화를 가지고 있고, 사용자의 profile을 만드는 어떠한 단계를 소개하려는 시도는 부정적인 사용자 반응을 초래할 수 있다. 실제로 Netflix는 데이터 대회를 통해 데이터를 공개했다가 비식별화된 데이터와 다른 데이터를 연결함을써 개인을 식별할 수 있게 됨을 확인했고, 고소까지 당했었다.</li></ul></li><li><p><a href="https://ko.wikipedia.org/wiki/%EC%B6%94%EC%B2%9C_%EC%8B%9C%EC%8A%A4%ED%85%9C" target="_blank" rel="noopener">그 이외의 주의사항</a></p></li></ul><h4 id="참고-문헌-및-사이트"><a href="#참고-문헌-및-사이트" class="headerlink" title="참고 문헌 및 사이트"></a>참고 문헌 및 사이트</h4><ul><li><p>웹 사이트</p><ul><li><a href="https://www.fun-coding.org/recommend_basic1.html" target="_blank" rel="noopener">잔재미 코딩</a></li><li><a href="https://leebaro.tistory.com/category/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%28Machine%20Learning%29/%EC%B6%94%EC%B2%9C%20%EC%8B%9C%EC%8A%A4%ED%85%9C%28Recommendation%20System%29" target="_blank" rel="noopener">leebaro blog</a></li><li><a href="https://darkpgmr.tistory.com/106" target="_blank" rel="noopener">svd의 활용에 관한 darkpgmr님의 블로그</a></li><li><a href="https://bcho.tistory.com/1216" target="_blank" rel="noopener">NMF 알고리즘을 이용한 유사한 문서검색과 구현</a></li><li><a href="http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/" target="_blank" rel="noopener">Python을 이용한 행렬의 분해 예제</a></li><li><a href="https://tv.naver.com/v/2297146" target="_blank" rel="noopener">naver 검색엔진 추천시스템 airs개발기(2017 Deview)</a></li><li><a href="http://datasqz.com/movie-names/movie-similar" target="_blank" rel="noopener">Movie recommendation system 예시</a></li><li><a href="http://sanghyukchun.github.io/73/" target="_blank" rel="noopener">sanghyukchun님의 github blog(Recommendation System)</a></li><li><a href="https://ebaytech.berlin/deep-learning-for-recommender-systems-48c786a20e1a" target="_blank" rel="noopener">추천시스템을 위한 Deep learning</a></li></ul></li><li><p>문헌</p><ul><li>Building Recommendation Engines</li><li>Recommender Systems in E-CommerceJ. Ben Schafer, Joseph Konstan, John Riedl</li><li>State of the Art Recommender System. Laurent Candillier, Kris Jack</li><li>Recommender Systems in E-Commerce. Sanjeevan Sivapalan, Alireza Sadeghian</li><li>A Survey of e-Commerce Recommender Systems Farida Karimova, PhD</li><li>Low-Rank Matrix Completion (2013) by Ryan Kennedy</li><li>Exact Matrix Completion via Convex Optimization Emmanuel J. Cand`es</li></ul></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/14/Recommendation_System_00/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering (API는 무엇인가?!?)</title>
      <link>https://heung-bae-lee.github.io/2019/12/13/data_engineering_03/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/13/data_engineering_03/</guid>
      <pubDate>Fri, 13 Dec 2019 04:25:42 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;REST-API의-정의와-예제들&quot;&gt;&lt;a href=&quot;#REST-API의-정의와-예제들&quot; class=&quot;headerlink&quot; title=&quot;REST API의 정의와 예제들&quot;&gt;&lt;/a&gt;REST API의 정의와 예제들&lt;/h2&gt;&lt;h3 id=&quot;API-A
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="REST-API의-정의와-예제들"><a href="#REST-API의-정의와-예제들" class="headerlink" title="REST API의 정의와 예제들"></a>REST API의 정의와 예제들</h2><h3 id="API-Application-Programming-Interface"><a href="#API-Application-Programming-Interface" class="headerlink" title="API(Application Programming Interface)"></a>API(Application Programming Interface)</h3><ul><li>두 개의 시스템이 서로 상호 작용하기 위한 인터페이스<ul><li>데이터를 주고 받는 인터페이스</li><li>API라고 하면 보통 REST API를 지칭</li></ul></li></ul><p><img src="/image/Web_API.png" alt="Web_API"></p><ul><li>웹사이트는 HTTP(S)프로토콜을 사용하는 REST API 기반으로 구축</li></ul><h2 id="API-접근-권한"><a href="#API-접근-권한" class="headerlink" title="API 접근 권한"></a>API 접근 권한</h2><h3 id="Authentication-VS-Authorization"><a href="#Authentication-VS-Authorization" class="headerlink" title="Authentication VS Authorization"></a>Authentication VS Authorization</h3><ul><li>Authentication : Identity(정체)가 맞다는 증명</li><li>Authorization : API를 통한 어떠한 액션을 허용</li></ul><h5 id="API가-Authentication으로-하여도-어떠한-액션에-대해서는-Authorization을-혀용하지-않을-수-있음"><a href="#API가-Authentication으로-하여도-어떠한-액션에-대해서는-Authorization을-혀용하지-않을-수-있음" class="headerlink" title="API가 Authentication으로 하여도 어떠한 액션에 대해서는 Authorization을 혀용하지 않을 수 있음"></a><code>API가 Authentication으로 하여도 어떠한 액션에 대해서는 Authorization을 혀용하지 않을 수 있음</code></h5><h3 id="API의-필수는-첫째도-둘째도-Security"><a href="#API의-필수는-첫째도-둘째도-Security" class="headerlink" title="API의 필수는 첫째도 둘째도 Security"></a>API의 필수는 첫째도 둘째도 Security</h3><ul><li>어떠한 Security 방안이 없을 경우<ul><li>DELETE request를 통해서 다른 이용자의 정보를 지울 수도 있음</li><li>제 3자에게 데이터 유출로 이어질 수 있음</li><li>누가 API를 사용하는지, 어떤 정보를 가져가는지 트래킹 할 수가 없음</li></ul></li></ul><h3 id="API-Key란-무엇인가"><a href="#API-Key란-무엇인가" class="headerlink" title="API Key란 무엇인가?"></a>API Key란 무엇인가?</h3><ul><li>API Key란 보통 Request URL혹은 Request 헤더에 포함되는 긴 String</li></ul><h3 id="Basic-Auth"><a href="#Basic-Auth" class="headerlink" title="Basic Auth"></a>Basic Auth</h3><p><img src="/image/BAsic_Auth.png" alt="Basic Auth"></p><h3 id="OAuth-2-0"><a href="#OAuth-2-0" class="headerlink" title="OAuth 2.0"></a>OAuth 2.0</h3><ul><li>설명하자면 Web API는 우리가 직접 어떤 Action을 하는 것이기 때문에 해당 Web에만 접근 권한을 받으면 되지만, 그와는 다르게 어떤 Action을 취할 Web을 다른 앱에게 접근 권한을 주어 End User인 우리 대신 정보를 제공하게 해주는 방식이다. 예를 듬면 우리가 어떤 서비스를 가입하려고 할때 SNS로 가입이 가능하게 할 수 있는 접근 권한 방식이라고 할 수 있다.</li></ul><p><img src="/image/OAuth2.png" alt="OAuth2.0"></p><h2 id="Endpoints-amp-Methods"><a href="#Endpoints-amp-Methods" class="headerlink" title="Endpoints &amp; Methods"></a>Endpoints &amp; Methods</h2><ul><li><code>Resource</code>는 API를 통해 리턴된 정보이며, 하나의 <code>Resource</code> 안에 여러개의 Endpoints가 존재한다.</li></ul><p><img src="/image/Endpoints_Method.png" alt="Endpoint_Method"></p><p><img src="/image/Methods.png" alt="Methods 정의"></p><h2 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h2><ul><li>Parameters는 Endpoint를 통해 Requests 할때 같이 전달하는 옵션들<ul><li>Request Body안에 포함되는 Parameter들은 post 방식에서 주로 많이 사용한다.</li></ul></li></ul><p><img src="/image/types_of_parameters.png" alt="4가지 Parameter 타입들"></p><h2 id="Spotify"><a href="#Spotify" class="headerlink" title="Spotify"></a>Spotify</h2><ul><li>필자의 프로젝트의 주된 data를 제공받을 Spotify를 먼저 소개하기 전에, 왜 국내의 Melon과 Genie를 택하지 않았는지를 말하려고 한다. SK플래닛이 2012년부터 운영하던 개발자센터 내 오픈 API 서비스를 2018년 3월부터 중단한다고 발표함으로써 Melon의 API 서비스를 시기적으로 사용하지 못하게 되었으며, Genie 또한 API 서비스를 더 이상 제공하지 않고 있다. 단순히 곡명과 아티스트명, 해당 곡에 대한 댓글등 이런 것들은 Selenium이나 그냥 기본적인 requests 모듈을 통해 가능하지만, <code>이번 토이 프로젝트의 목표는 Spotify API를 통하여 엔터티간의 관계를 내가 직접 설계해 보고 이미 만들어 있긴 하지만 각 곡들의 특징을 수치적으로 분류해 놓은 특징들로 유사도를 계산하여 User에게 추천하는 Facebook 앱을 만들어 보고 싶어서이다.</code></li></ul><ul><li>그렇다면, Spotify는 무엇인가? 스포티파이는 프리웨어이다. 본래에는 무료로 이용하면 시간 제한이 있었으나, 2014년 폐지되었다. 스포티파이는 Spotify 웹사이트에서 바로 다운받을 수 있다. 제공되는 곡들은 음반사들이 라이선스하여 합법적으로 제공한 것이다. 하지만, 사용자가 한 달 9.99 유로의 서비스 사용료를 내지 않는다면, 소프트웨어 상에 광고가 표시되며, 곡과 곡 사이에 광고가 삽입된다. 가입자가 서비스 사용료를 냈다면, 가입자는 자동적으로 “프리미엄 사용자” 상태가 된다. 프리미엄 사용자들은 특별히 뉴스나 프리뷰를 들을 수 있다. 또한 developer API를 제공함으로써, API 사용법에 대한 상세한 설명이 있다.<a href="https://developer.spotify.com/" target="_blank" rel="noopener">Spotify developer</a>앞으로 이 사이트에 있는 API 사용법을 활용하여 데이터를 crawling한 후에 RDS에 저장할 것이다.</li></ul><h3 id="Spotify-for-Developers"><a href="#Spotify-for-Developers" class="headerlink" title="Spotify for Developers"></a>Spotify for Developers</h3><ul><li>API를 사용하기 위해서는 접근 권한이 있는 Access ID와 password를 발급받아야 하므로 먼저, APP을 만들것이다. 위의 탭란에서 DashBoard를 클릭해보면 다음과 같은 페이지로 이동할 것이다. login이 필요하므로 먼저 가입을 해야 할 것이다. 옆에 있는 Sign up for free Spotify account here 버튼을 눌러</li></ul><p><img src="/image/Authorization.png" alt="Authorization"></p><ul><li>필자가 사용할 Access 방법은 위에서 언급했던 Oauth 2.0을 활용한 방식은 아니고, 그냥 Access ID와 Password를 발급받은 후 Access하는 방식을 택했다. <code>약간의 주의사항은 발급받은 후 1시간 경과 후에는 Password를 재발급받은 후 사용하여야 한다.</code></li></ul><p><img src="/image/Client_Credentials_Flow.png" alt="Client Credentials Flow"></p><p><img src="/image/Client_Credentials_Flow_01.png" alt="Client Credentials Flow - 1"></p><p><img src="/image/dashboard_login.png" alt="Dash Board"></p><ul><li>여기서 문제가 생겼다. 국제적인 서비스여서 국내에서도 이용 제한이 없을 것이라고 생각했지만, 국내에서는 서비스를 아직 하지 않고 있다한다……. 결국 방법을 찾아보다 VPN을 사용하여 우회한 후에 가입을 하면 서비스 이용이 가능하다는 사실을 알게 되었고, VPN을 이용하여 가입하였다.</li></ul><p><img src="/image/why_VPN.png" alt="국내 이용제한"></p><ul><li>가입한 후에 Application을 등록해보자. App의 이름과 설명 개발 용도를 입력해주는데, App 개발 용도는 미정이므로 모른다고 설정했다.</li></ul><p><img src="/image/Spotify_for_RDS.png" alt="App 등록"></p><ul><li>다 만들어 졌다면, 다음과 같이 새로운 앱이 생성되었음을 확인 할 수 있다.</li></ul><p><img src="/image/new_APP.png" alt="App 생성"></p><ul><li>우리가 API에 접근할 때 필요한 ID와 password 정보를 보려면 앱을 클릭하면 확인 할 수 있다. password는 hide되어있는데 hide를 풀면 볼 수 있다. <code>1시간 마다 password는 reset해서 사용해야한다.</code></li></ul><p><img src="/image/ID_and_password.png" alt="ID와 password 확인"></p><h4 id="API를-사용법을-통해-결과적으로-만들-ERD-Entity-Relationship-Diagram-는-다음과-같다"><a href="#API를-사용법을-통해-결과적으로-만들-ERD-Entity-Relationship-Diagram-는-다음과-같다" class="headerlink" title="API를 사용법을 통해 결과적으로 만들 ERD(Entity Relationship Diagram)는 다음과 같다."></a>API를 사용법을 통해 결과적으로 만들 ERD(Entity Relationship Diagram)는 다음과 같다.</h4><p><img src="/image/create_db_model.png" alt="ERD"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/13/data_engineering_03/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Basic ConvNN(VGG-16모방한 기본)구현</title>
      <link>https://heung-bae-lee.github.io/2019/12/13/deep_learning_05/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/13/deep_learning_05/</guid>
      <pubDate>Fri, 13 Dec 2019 03:58:02 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Basic-ConvNN-구현&quot;&gt;&lt;a href=&quot;#Basic-ConvNN-구현&quot; class=&quot;headerlink&quot; title=&quot;Basic ConvNN 구현&quot;&gt;&lt;/a&gt;Basic ConvNN 구현&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;참고로 저는 mac을 
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Basic-ConvNN-구현"><a href="#Basic-ConvNN-구현" class="headerlink" title="Basic ConvNN 구현"></a>Basic ConvNN 구현</h1><ul><li>참고로 저는 mac을 사용하기에 local에서말고 GPU를 사용허게끔 Google Colab을 사용하였다. 제가 구현한 방식은 tensorflow 2.0 version이므로(tf.function을 사용하느라) colab의 tensorflow의 version이 뭔지 먼저 확인했습니다. 1.15 version이어서 2.0으로 설치를 진행한 후 코드를 실행하였습니다. <code>참고로 2.0으로 설치하고 난 후에는 꼭 반드시 런타임을 재시작 해주셔야 업데이트 한 2.0 version으로 사용하실 수 있습니다.</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br></pre></td></tr></table></figure><h2 id="런타임-재시작-후"><a href="#런타임-재시작-후" class="headerlink" title="런타임 재시작 후"></a>런타임 재시작 후</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install tensorflow==2.0.0-beta1</span><br></pre></td></tr></table></figure><h2 id="기본-합성곱-신경망-구현"><a href="#기본-합성곱-신경망-구현" class="headerlink" title="기본 합성곱 신경망 구현"></a>기본 합성곱 신경망 구현</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br></pre></td></tr></table></figure><h2 id="하이퍼-파라미터"><a href="#하이퍼-파라미터" class="headerlink" title="하이퍼 파라미터"></a>하이퍼 파라미터</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EPOCHS = 10</span><br></pre></td></tr></table></figure><h3 id="참고로-conv-layer을-통과한-출력의-dimension을-계산하는-것은-다음과-같다"><a href="#참고로-conv-layer을-통과한-출력의-dimension을-계산하는-것은-다음과-같다" class="headerlink" title="참고로 conv layer을 통과한 출력의 dimension을 계산하는 것은 다음과 같다."></a>참고로 conv layer을 통과한 출력의 dimension을 계산하는 것은 다음과 같다.</h3><h4 id="padding-2N-1-kernel-size-Filter-size-로-N을-구한다"><a href="#padding-2N-1-kernel-size-Filter-size-로-N을-구한다" class="headerlink" title="padding : 2N+1 = kernel_size(Filter_size)로 N을 구한다."></a>padding : 2N+1 = kernel_size(Filter_size)로 N을 구한다.</h4><h4 id="output-dimension"><a href="#output-dimension" class="headerlink" title="output dimension :"></a>output dimension :</h4><script type="math/tex; mode=display">\frac{input\hspace{0.1cm} size + (padding\hspace{0.1cm} size * 2) - filter\hspace{0.1cm}size}{strid} + 1</script><h2 id="모델-정의"><a href="#모델-정의" class="headerlink" title="모델 정의"></a>모델 정의</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">class ConvNet(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(ConvNet, self).__init__()</span><br><span class="line">        self.sequence = []</span><br><span class="line">        conv2d = tf.keras.layers.Conv2D</span><br><span class="line">        max_pool = tf.keras.layers.MaxPool2D</span><br><span class="line">        flatten =  tf.keras.layers.Flatten</span><br><span class="line">        <span class="comment"># filters = 16 (출력되는 channel의 수)</span></span><br><span class="line">        <span class="comment"># kernel_size = 3 * 3</span></span><br><span class="line">        <span class="comment"># padding의 default값인 'valid'는 zero-padding을 해주지 않음으로써 영상의 크기가 Conv layer를 통과함으로써 줄어들 수 있다.</span></span><br><span class="line">        <span class="comment"># 'same'은 zero-padding을 의미 여기서는 동일한 크기를 유지하기 위해</span></span><br><span class="line">        <span class="comment"># input data는 (28x28x1)을 갖는 MNIST이다.</span></span><br><span class="line">        <span class="comment"># VGG-16의 가장 큰 특징은 Pooling을 하기 전에 동일한 Conv Layer를 반복해서 사용하는 것이다.</span></span><br><span class="line">        self.sequence.append(conv2d(16, (3,3), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)) <span class="comment"># output dimension (28x28x16)</span></span><br><span class="line">        self.sequence.append(conv2d(16, (3,3), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)) <span class="comment"># output dimension (28x28x16)</span></span><br><span class="line">        <span class="comment"># 2x2 pooling을 한다. maxpooling을 이용하여 영상의 크기를 줄여준다.</span></span><br><span class="line">        self.sequence.append(max_pool((2,2))) <span class="comment"># output dimension (14x14x16)</span></span><br><span class="line"></span><br><span class="line">        self.sequence.append(conv2d(32, (3,3), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)) <span class="comment"># output dimension (14x14x32)</span></span><br><span class="line">        self.sequence.append(conv2d(32, (3,3), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)) <span class="comment"># output dimension (14x14x32)</span></span><br><span class="line">        self.sequence.append(max_pool((2,2))) <span class="comment"># output dimension (7x7x32)</span></span><br><span class="line"></span><br><span class="line">        self.sequence.append(conv2d(64, (3,3), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)) <span class="comment"># output dimension (7x7x64)</span></span><br><span class="line">        self.sequence.append(conv2d(64, (3,3), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>)) <span class="comment"># output dimension (7x7x64)</span></span><br><span class="line"></span><br><span class="line">        self.sequence.append(flatten()) <span class="comment"># 1568x1</span></span><br><span class="line">        self.sequence.append(tf.keras.layers.Dense(2028, activation=<span class="string">'relu'</span>))</span><br><span class="line">        self.sequence.append(tf.keras.layers.Dense(10, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line">    def call(self, x, training=False, mask=None):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.sequence:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="built_in">return</span> x</span><br></pre></td></tr></table></figure><h2 id="학습-테스트-루프-정의"><a href="#학습-테스트-루프-정의" class="headerlink" title="학습, 테스트 루프 정의"></a>학습, 테스트 루프 정의</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement training loop</span></span><br><span class="line">@tf.function</span><br><span class="line">def train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy):</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        predictions = model(images)</span><br><span class="line">        loss = loss_object(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line"></span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(labels, predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Implement algorithm test</span></span><br><span class="line">@tf.function</span><br><span class="line">def test_step(model, images, labels, loss_object, test_loss, test_accuracy):</span><br><span class="line">    predictions = model(images)</span><br><span class="line"></span><br><span class="line">    t_loss = loss_object(labels, predictions)</span><br><span class="line">    test_loss(t_loss)</span><br><span class="line">    test_accuracy(labels, predictions)</span><br></pre></td></tr></table></figure><h2 id="데이터셋-준비"><a href="#데이터셋-준비" class="headerlink" title="데이터셋 준비"></a>데이터셋 준비</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력 영상이 총 8bit 즉, 0~255 사이의 값들로 이루어져 있으므로</span></span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0~1표현으로 바꿔준다.</span></span><br><span class="line">x_train, x_test = x_train / 255.0, x_test / 255.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 입력 영상 하나의 사이즈는 28x28이므로 channel을 하나 더 늘려 주어야한다.</span></span><br><span class="line"><span class="built_in">print</span>(x_train.shape)</span><br><span class="line"><span class="built_in">print</span>(x_train[0].shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># x_train : (NUM_SAMPLE, 28, 28) -&gt; (NUM_SAMPLE, 28, 28 , 1)</span></span><br><span class="line"><span class="comment"># ...은 해당 데이터 객체의 모든 axis를 표현하는 것이다.</span></span><br><span class="line"><span class="comment"># 위에서 255.0으로 나누어주게 되면 float64로 되므로 자료형을 float32로 해야 error가 없다.</span></span><br><span class="line"><span class="comment">## x_train[:,:,:, tf.newaxis]</span></span><br><span class="line">x_train = x_train[..., tf.newaxis].astype(np.float32)</span><br><span class="line">x_test = x_test[..., tf.newaxis].astype(np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Numpy object나 Tensor로 부터 데이터셋을 구축할 수 있다.</span></span><br><span class="line">train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)</span><br><span class="line"><span class="comment"># test data는 suffle할 필요없다.</span></span><br><span class="line">test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)</span><br></pre></td></tr></table></figure><h2 id="학습-환경-정의"><a href="#학습-환경-정의" class="headerlink" title="학습 환경 정의"></a>학습 환경 정의</h2><h3 id="모델-생성-손실함수-최적화-알고리즘-평가지표-정의"><a href="#모델-생성-손실함수-최적화-알고리즘-평가지표-정의" class="headerlink" title="모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"></a>모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create model</span></span><br><span class="line">model = ConvNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define loss and optimizer</span></span><br><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">optimizer = tf.keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define performance metrics</span></span><br><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">test_loss = tf.keras.metrics.Mean(name=<span class="string">'test_loss'</span>)</span><br><span class="line">test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">'test_accuracy'</span>)</span><br></pre></td></tr></table></figure><h2 id="학습-루프-동작"><a href="#학습-루프-동작" class="headerlink" title="학습 루프 동작"></a>학습 루프 동작</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCHS):</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_ds:</span><br><span class="line">        train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> test_images, test_labels <span class="keyword">in</span> test_ds:</span><br><span class="line">        test_step(model, test_images, test_labels, loss_object, test_loss, test_accuracy)</span><br><span class="line"></span><br><span class="line">    template = <span class="string">'Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;'</span></span><br><span class="line">    <span class="built_in">print</span>(template.format(epoch + 1,</span><br><span class="line">                          train_loss.result(),</span><br><span class="line">                          train_accuracy.result() * 100,</span><br><span class="line">                          test_loss.result(),</span><br><span class="line">                          test_accuracy.result() * 100))</span><br><span class="line">    <span class="comment"># reset_state는 새로운 값들을 받기 위해 하는 건가?</span></span><br><span class="line">    train_loss.reset_states()</span><br><span class="line">    train_accuracy.reset_states()</span><br><span class="line">    test_loss.reset_states()</span><br><span class="line">    test_accuracy.reset_states()</span><br></pre></td></tr></table></figure><p><img src="/image/basic_convNN_result.png" alt="result"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/13/deep_learning_05/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Convolution Neural Network(1)</title>
      <link>https://heung-bae-lee.github.io/2019/12/10/deep_learning_04/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/10/deep_learning_04/</guid>
      <pubDate>Tue, 10 Dec 2019 04:50:24 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;합성곱-연산과-이미지-필터&quot;&gt;&lt;a href=&quot;#합성곱-연산과-이미지-필터&quot; class=&quot;headerlink&quot; title=&quot;합성곱 연산과 이미지 필터&quot;&gt;&lt;/a&gt;합성곱 연산과 이미지 필터&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/image/analo
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="합성곱-연산과-이미지-필터"><a href="#합성곱-연산과-이미지-필터" class="headerlink" title="합성곱 연산과 이미지 필터"></a>합성곱 연산과 이미지 필터</h2><p><img src="/image/analog_signal.png" alt="아날로그 신호처리"></p><ul><li>아날로그 신호처리는 <code>선형이고 시불변인 시스템</code>에 의존해서 개발이 되게 되는데, Noise가 있는 입력이 들어왔을 때 넣어주면 Noise가 제거된 출력이 나오는 이런 시스템을 모두 LTI system이라고 부른다. <code>디지털 신호가 아닌 아날로그 신호로부터 LTI system이 정의</code>되어있다. 선형이라는 것은 대부분 알고 있듯이 선형대수에서 나오는 linearity를 만족시키면 되는 것이고, <code>시불변이라는 의미는 시간이 지나도 동일한 결과를 내보내준다는 의미이다.</code> 확률과정에서 step에 영향을 받지 않는다라고 보면 좋을 것 같다. 사람은 대표적으로 LTI 시스템이 아닌 시스템이다.</li></ul><p><img src="/image/Dirac_delta_function.png" alt="Dirac 델타 함수"></p><ul><li>수학적으로는 엄밀하진 않지만, 공학에선 많이 사용한다. 왼쪽의 삼각형을 모든 구간에 대해 전부해준다면 값은 1이 될 것이다. 여기서 $h\to\infty$가 되면, Dirac 델타 함수가 된다.<ul><li>시간 t=0만 임의의 값을 갖고, 나머지 구간은 0을 갖는다.</li><li>모든 구간에서 적분한 값이 1</li></ul></li></ul><p><img src="/image/impulse_response.png" alt="임펄스 응답"></p><p><img src="/image/convolution_operation.png" alt="합성곱 연산"></p><ul><li><code>convolution을 한다는 것은 임의의 두 함수 중 한 함수를 좌우로 뒤집고 이동시키면서 두함수의 곱을 적분하여 계산한다.</code></li></ul><p><img src="/image/conv_operation_and_LTI_system.png" alt="합성곱 연산과 LTI 시스템"></p><p><img src="/image/2_Dimension_2_channel.png" alt="이차원 신호와 흑백 이미지"></p><p><img src="/image/2_Dimension_color_image.png" alt="이차원 신호와 컬러 이미지"></p><p><img src="/image/image_conv.png" alt="영상의 합성곱 계산"><br><a href="http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html" target="_blank" rel="noopener">합성곱 계산 animation</a></p><p><img src="/image/noise_removal_filter.png" alt="잡음 제거 필터"></p><p><img src="/image/differentiation_filter.png" alt="미분 필터"></p><ul><li>vertical Sobel Filter가 왜 미분 필터이냐고 의문이 든다면, 1차원 신호를 data로 생각하고 앞서 했었던 수치 미분을 떠올려 보자. 그렇다면, 단숨에 이해가 갔을 것이다. 지금은 vertical Sobel Filter이므로 가로의 Edge는 추출하지 못한 것을 확인할 수 있다. 그에 반해 세로 성분들은 잘 검출된 것을 확인 할 수 있다. 만일 위의 필터를 90도 rotate해주게되면 가로로 미분하는 horizonal Sobel Filter가 되어 위의 결과와는 반대의 결과를 보여줄 것이다.</li></ul><h2 id="합성곱-계층"><a href="#합성곱-계층" class="headerlink" title="합성곱 계층"></a>합성곱 계층</h2><p><img src="/image/product_to_conv.png" alt="곱에서 합성곱으로"></p><ul><li><code>입력</code>이 이제는 <code>영상</code>으로 여러개 들어오게 되어 각각의 입력층의 뉴런 하나 하나가 <code>channel</code>이라고 불린다. <code>필터 가중치</code>는 보통 <code>3x3, 5x5, 7x7등을 주로 사용</code>한다. 2D signal과 2D signal을 곱(<code>element-wise product or Hadamard product</code>)해야하므로 <code>합성곱</code>을 사용한다.  </li></ul><p><img src="/image/FC_Layer_conv.png" alt="전결합 계층"></p><p><img src="/image/Conv_layer.png" alt="합성곱 계층"></p><ul><li>$kernel_{Height} \times kernel_{Width} \times channel_{in} \times channel_{out}$ 만큼의 parameter가 필요하다. filter는 $Channel_{in} \times Chaeel_{out}$개 만큼 있을 것이다.</li></ul><p><img src="/image/Conv_Layer_meaning.png" alt="합성곱 계층의 의미"></p><ul><li>kernel(Filter)에 나타나는 모양과 유사한 모양을 한 위치가 높은 값으로 나타나게 된다.</li></ul><h2 id="기본적인-합성곱-신경망"><a href="#기본적인-합성곱-신경망" class="headerlink" title="기본적인 합성곱 신경망"></a>기본적인 합성곱 신경망</h2><p><img src="/image/basic_conv_structure.png" alt="합성곱 신경망의 기본 구조"></p><p><img src="/image/Conv_Layer_feature_map.png" alt="합성곱 계층"></p><ul><li>stride요소를 넣지 않으면, 합성곱 계층에서는 영상의 크기는 그대로이며, <code>영상의 채널 수</code>가 달라진다.</li><li><code>kernel(filter)가 돌아다니면서 포착하는 형태이기 때문에 공간적인 특징</code>이 있고, 따라서 <code>Feature Map</code>이라고 한다.</li></ul><p><img src="/image/Pooling_Layer.png" alt="풀링 계층"></p><p><img src="/image/Pooling_Layers.png" alt="Pooling Layers"></p><ul><li>classification에서는 Max Pooling이 주로 잘 먹힌다!</li></ul><p><img src="/image/Flatten.png" alt="평탄화"><br>-<code>Convolutional Layer와 FC Layer를 연결해주기 위해 필요하다</code></p><p><img src="/image/FC_Layer_why.png" alt="전결합 계층"></p><p><img src="/image/why_use_this_structure_conv.png" alt="그러면 왜 이런 구조를 쓰나?"></p><ul><li><p>먼저, 맨 처음 언급했던 머신러닝과 딥러닝의 가장 큰 차이는 사람이 feature를 넣어주느냐 그렇지 않느냐의 차이라고 했다. 크게 보면 위의 그래프에서 합성곱 계층과 활성함수의 과정을 N번 반복하는 것은 shallowNN의 input으로 넣어 줄 Feature를 뽑는 과정이라고 직관적으로 이해할 수 있다.</p></li><li><p>앞의 합성곱 계층에서 activation function 까지를 계속해서 <code>진행 할수록 Feature Map의 크기(width와 height)는 Kernel과 Pooling에 의해서 줄어들고 channel(depth)는 늘어나게 될 것이다. 또한 처음부터 끝까지 동일한 크기의 kernel(Filter)을 사용한다고 가정한다면, 영상에서 더 넓은 영역을 커버하는 효과를 주는 것과 동일하다.</code></p></li></ul><p><img src="/image/Receptive_Field.png" alt="Receptive Field"></p><ul><li><code>그래서 Feature Map을 한번 뽑을 때마다 Pooling을 해주면서 처음에는 좁은 영역을 점점 더 넓은 영역을 본다. 점점 더 넓은 영역을 본다는 의미는 Pooling을 함으로써 결국에는 더 넓은 범위를 대표하는 값들을 가진 2-D Matrix인 Feature Map이 될 것이기 때문이다.</code></li></ul><p><img src="/image/LeNet_5.png" alt="LeNet-5"></p><ul><li>98년도의 르쿤 교수님의 LeNet-5는 pooling 대신 subsampling을 사용하여 같은 Feature Map의 크기를 줄여주었다.</li></ul><p><img src="/image/VGG_16.png" alt="VGG-16"></p><h2 id="합성곱-신경망의-심화-이해"><a href="#합성곱-신경망의-심화-이해" class="headerlink" title="합성곱 신경망의 심화 이해"></a>합성곱 신경망의 심화 이해</h2><p><img src="/image/importance_of_conv.png" alt="합성곱 게층의 필요성"></p><ul><li>간단히 생각하면 $kernel_{height} \times kernel_{width} \times Channel_{in} \times Channel_{out}$ 만큼 어마어마하게 많은 Parameter가 필요하므로 계산해야 할 Parameter가 상대적으로 적은 FC Layer로 하는 것이 더 좋은 방법이지 않을까라고 생각하시는 분들이 있을 것이다. 허나, 그것은 잘못된 생각이다. <code>Convolutional Layer를 사용하기 때문에 우리가 Image를 처리할 수 있는 것이다. FC Layer를 사용하게 되면 오히려 계산해야 할 Parameter의 개수가 어마어마하게 늘어난다.</code>$(Height_{in} \times Width_{in} \times Channel_{in}) \times (Height_{out} \times Width_{out} \times C_{out})$ 얼핏 보기엔 비슷해보이겠지만, 예를 들어보자. 입력으로 RGB channel을 갖는 1024 * 1024 image를 받는다면, FC Layer를 사용한다면, $(1024 \times 1024 \times 16) \times (1024 \times 1024 \times 32)$이지만 Convolutional Layer를 사용하고 $3 \times 3$ kernel을 사용한다면 $(3 \times 3 \times 16 \times 32)$로 훨씬 적은 연산을 한다. 이러한 이유로 우리가 영상을 입력으로 하는 것은 절대로 FC Layer를 통해 해결할 수 없다.</li></ul><p><img src="/image/mathmatical_expression_of_FC_Layer_00.png" alt="전결합 계층의 수학적 표현"></p><p><img src="/image/mathmatical_expression_of_Conv_Layer_00.png" alt="합성곱 계층의 수학적 표현"></p><ul><li>위에서 W는 kernel들을 $C_{in} \times C_{out}$ Matrix로 이루어진 tensor이다. 즉, $W_{i,j}$들이 각각의 kernel을 나타내고 $X_{i}$와 convolution operation을 해주므로 편향은 FC Layer와 동일하게 channel 1개마다 1개씩 존재한다.</li></ul><p><img src="/image/importance_of_Padding.png" alt="Padding의 필요성"></p><p><img src="/image/Padding.png" alt="Padding"></p><ul><li>위의 그림의 예를 보면 kernel size가 $3 = 2N+1$이므로 입력에 상하좌우 1개의 Zero-Padding을 해준 것이다.</li></ul><p><img src="/image/Stride.png" alt="Stride"></p><ul><li>Stride를 하는 것은 결과를 미리 Convolution을 Full로 다 연산을 한 다음에 하나씩 Subsample하는 것과 동일한 결과를 가져온다. 그러므로 다 연산한 후에 subsampling을 하면 연산은 다하지만 결국엔 버리는 값이 생기기 때문에 Stride를 사용한다.</li></ul><p><img src="/image/conv_layer_feature.png" alt="합성곱 계층의 특징"></p><ul><li>학습 초반에는 위쪽의 Feature Map들 처럼 좁은 범위의 Feature들을 추출하지만, 학습의 후반 부에는 넓은 범위의 Feature들을 학습한다.</li></ul><h2 id="Batch-Normalization-배치-정규화"><a href="#Batch-Normalization-배치-정규화" class="headerlink" title="Batch Normalization(배치 정규화)"></a>Batch Normalization(배치 정규화)</h2><p><img src="/image/vanilla_gradient_descent.png" alt="일반 경사 하강법(vanilla Gradient Descent)"></p><ul><li><code>일반 경사 하강법의 경우, Gradient를 한번 업데이트 하기 위해 모든 학습 데이터를 사용한다.</code> 하지만 데이터가 엄청나게 많다면?? 그렇다면 Gradient를 업데이트하는데 오랜시간이 소요될 것이다. 그렇다면 SGD는??!! Stochastic은???</li></ul><p><img src="/image/stochastic_gradient_descent.png" alt="확률적 경사 하강법(Stochastic Gradient Descent)"></p><p><img src="/image/minibatch_learning.png" alt="미니 배치 학습법"></p><ul><li>Epoch마다 데이터 순서를 섞어주기도 하는 이유는 random성을 더 강하게 해주기 위해서이다.</li></ul><p><img src="/image/internal_Covariate_Shift.png" alt="internal Covariate Shift"></p><ul><li><code>이런 현상을 해결하기 위한 것이 batch normalization이다.</code></li></ul><p><img src="/image/Batch_normalization.png" alt="배치 정규화(Batch Normalization)"></p><p><img src="/image/Training_Phase.png" alt="학습단계(Training Phase)"></p><ul><li><p>또한, 동일한 scale과 동일한 zero-mean을 가지게 되기 때문에 학습률 결정에 유리하다 말의 의미는 <code>학습을 할 때 더 scale이 큰 경우에는 학습이 많이 되고, scale이 작으면 학습이 적게 되는 문제가 발생할 수 있다. 학습률을 너무 크게 할 경우 Gradient가 크게 나오는 곳에 Gradient exploding이 발생할 수가 있고, 반대로 학습률을 너무 작게 할 경우 Gradient Vanishing이 발생되서 학습이 안되는 곳이 발생되는 문제가 있는데 Normalization을 해주게 되면 모든 각각의 계층들이 동일한 scale로 학습되기 때문에 학습률 결정에 유리하다는 것이다.</code><br>(미분을 할때 입력에 대해서 출력값이 커지게 되면, Gradient값도 커질 것이다.)</p></li><li><p>각각의 batch를 normalization하면, 말 그대로 normalization이 된 것이므로 모수가 $\mu=0, \sigma^2=1$인 gaussian distribution을 갖게 될 것이다. 그런데 activation함수는 LeRu를 사용한다면 0미만인 것들은 모조리 0값으로 반환될것이다. 이미 연산을 해놓은 값들을 연산 하기 전이 아닌 연산후에 0으로 만들어 의미 없게 만드는 것 보다 그렇게 0으로 반환되는 개수를 조절하기 위해 추가 스케일링 계수인 $\gamma$와 $\beta$를 만들고, 역전파 알고리즘으로 학습시켜준다.</p></li></ul><p><img src="/image/Inference_Phase.png" alt="추론 단계(Inference Phase)"></p><ul><li>학습과정에서 이동평균을 구해놓는데, 최근 N개에 대한 이동평균을 사용한다. <code>최근 N개만 사용하고 그 전에 것들은 자연스럽게 날라가기 때문에 최근 N개가 충분한 sample이 아닐 경우</code>$\mu$<code>,</code>$\sigma$<code>가 적절하지 않게 결정되는 문제가 있는데 이런 상황을 해결하는 것은 지수평균을 사용한다.</code></li></ul><h2 id="심화-합성곱-신경망"><a href="#심화-합성곱-신경망" class="headerlink" title="심화 합성곱 신경망"></a>심화 합성곱 신경망</h2><p><img src="/image/Conv_NN.png" alt="합성곱 신경망"></p><h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><p><img src="/image/GoogLeNet.png" alt="GoogLeNet(Inception)"></p><ul><li>2014년도에 GoogLeNet과 VGG-19가 나왔는데, GoogLeNet이 에러율이 좀 더 낮고 층이 더 깊은 것을 알 수 있다. GoogLeNet이 좀 더 복잡해서 VGG가 더 많이 알려져 있지만, 다양한 스킬들을 공부하려면 GoogLeNet을 조금 살펴보는 것도 좋을 것이다.</li></ul><p><img src="/image/sturcture_of_GoogLeNet.png" alt="GoogLeNet의 구조"></p><ul><li><code>Let&#39;s Go Deeper and Deeper라는 모토를 가지고 만들어진 것과 같이 좀더 hidden_layer가 깊어진 것을 알 수 있다.</code></li></ul><p><img src="/image/Inception_Module.png" alt="Inception 모듈(naive version)"></p><ul><li>1x1, 3x3, 5x5의 feature들을 다 나누어서 학습한다. <code>즉, 다양한 크기의 Filter들이 잘 학습된다. 또한 3x3 Max pooling 같은 경우는 convolution Layer를 거치지않고도 단순히 max pooling을 통한 후에도 다음 단계에서 의미있는 feature로 작동된다는 것을 보여주었다!!</code></li></ul><p><img src="/image/Inception_Module_01.png" alt="Inception 모듈(dimension reductions)"></p><ul><li>naive 한 Inception 모듈 구조에서 먼저 <code>단순히 1x1 conv를 통과시켜 동일한 channel 영역(Receptive Field)을 가져가면서도 channel을 줄여 연산량을 줄여 주는 구조인 Bottleneck를 구현</code>하고 있다.</li></ul><p><img src="/image/Bottle_neck_sturcture.png" alt="Bottleneck 구조"></p><p><img src="/image/extra_clssifier.png" alt="추가 분류기 사용"></p><ul><li>맨 마지막 출력층에서만 inference를 한다면 Input에 가까운 층일수록 점점 더 Vanishing Gradient문제로 인해 학습이 저하 될 것을 우려하여 중간 feature들로도 classification을 하도록 하였다.</li></ul><h4 id="GoogLeNet-중간-요약"><a href="#GoogLeNet-중간-요약" class="headerlink" title="GoogLeNet 중간 요약"></a>GoogLeNet 중간 요약</h4><ul><li>Inception 구조</li><li>Battleneck 구조</li><li>중간 중간에 inference</li></ul><h3 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h3><p><img src="/image/ResNet.png" alt="Residual Network(ResNet)"></p><ul><li>이제는 거의 일반적이고, 기본구조로 많이 사용하는 구조이다.</li></ul><p><img src="/image/structure_of ResNet.png" alt="ResNet의 구조"></p><p><img src="/image/Skip_connection.png" alt="Skip Connection"></p><ul><li>왼쪽 구조에서 표현가능한 것은 오른쪽 구조인 Residual 구조에서도 표현 가능함이 증명 되어 있다. 직관적으로 봤을때는 Feature를 뽑아서 이전 Feature와 더한 다는 것이 잘 이해가 안갈 수 도 있겠지만, 이런식으로 했을때도 좌측의 일반적인 Conv Layer의 구조와 수학적으로 동치를 이룬다는 것을 알고 있자.</li></ul><p><img src="/image/Identity_Mapping.png" alt="Identity Mapping"></p><p><img src="/image/Pre_activation_00.png" alt="Pre-Activation_00"></p><ul><li>맨 왼쪽이 Original ResidualNN의 구조이고 가운데가 Pre-Activation을 사용하는 구조이다.  </li></ul><p><img src="/image/Pre_activation_01.png" alt="Pre-Activation_01"></p><h2 id="Densely-Connected-ConvNets-DenseNet"><a href="#Densely-Connected-ConvNets-DenseNet" class="headerlink" title="Densely Connected ConvNets(DenseNet)"></a>Densely Connected ConvNets(DenseNet)</h2><p><img src="/image/DenseNet.png" alt="Densely Connected ConvNets"></p><ul><li>간단히 말하자면, 모든 Layer들이 다 연결되어 있는 구조라고 할 수 있다.</li></ul><p><img src="/image/structure_of_DenseNet.png" alt="DenseNet 구조"></p><ul><li><p>처음에 일반적인 Conv Layer를 통해 Feature Map을 만들고 그런 뒤에 Dense Block을 이용해서 다른 모든 Conv Layer들과 Dense하게 연결시켜준다. 그 다음 Conv Layer를 이용해서 channel 개수를 조정해주고, Max Pooling을 이용해서 영상크기를 줄여준다. 이런 과정을 여러번 반복해서 Feature를 추출한 후, 맨 마지막은 FC Layer로 구성해주었다.</p></li><li><p><code>위의 구조에서 Dense Block들이 residual block으로 바뀐다면 ResNet인 것이다. Pre-Activation구조를 사용한다는 것이 ResNet을 계승하고 있는다는 것을 알 수 있는 명확한 내용이다.</code></p></li></ul><p><img src="/image/Dense_Block.png" alt="Dense Block"></p><p><img src="/image/Bottle_neck_structure.png" alt="Bottleneck 구조"></p><p><img src="/image/practice_of_DenseNet.png" alt="DensNet의 구현"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/10/deep_learning_04/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering basic(SQL Basic)</title>
      <link>https://heung-bae-lee.github.io/2019/12/10/data_engineering_02/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/10/data_engineering_02/</guid>
      <pubDate>Tue, 10 Dec 2019 02:23:24 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;SQL-Structured-Query-Language&quot;&gt;&lt;a href=&quot;#SQL-Structured-Query-Language&quot; class=&quot;headerlink&quot; title=&quot;SQL(Structured Query Language)&quot;&gt;&lt;/
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="SQL-Structured-Query-Language"><a href="#SQL-Structured-Query-Language" class="headerlink" title="SQL(Structured Query Language)"></a>SQL(Structured Query Language)</h1><h3 id="DB-Database"><a href="#DB-Database" class="headerlink" title="DB (Database)"></a>DB (Database)</h3><ul><li>데이터를 통합하여 관리하는 데이터의 집합</li></ul><h3 id="DBMS-Database-Management-system"><a href="#DBMS-Database-Management-system" class="headerlink" title="DBMS (Database Management system)"></a>DBMS (Database Management system)</h3><ul><li>DB를 관리하는 미들웨어 시스템을 의미</li></ul><h3 id="Database-분류"><a href="#Database-분류" class="headerlink" title="Database 분류"></a>Database 분류</h3><div class="table-container"><table><thead><tr><th style="text-align:center">RDBMS(Relational Database Management System)</th><th style="text-align:center">NoSQL</th></tr></thead><tbody><tr><td style="text-align:center">- 데이터 테이블 사이에 키값으로 관계를 가지고 있는 데이터베이스        ex) Oracle, Mysql, Postgresql, Sqlite -데이터 사이의관계 설정으로 최적화된 스키마를 설계 가능</td><td style="text-align:center">- 데이터 테이블 사이에 관계가 없이 저장하는 데이터베이스  - 데이터 사이의 관계가 없으므로 복잡성이 줄고 많은 데이터를 저장 가능</td></tr></tbody></table></div><h3 id="RDBMS"><a href="#RDBMS" class="headerlink" title="RDBMS"></a>RDBMS</h3><p><img src="/image/RDBMS.png" alt="RDBMS"></p><ul><li><p>table</p><ul><li>행(row)과 열(column)로 이루어져 있는 데이터 베이스를 이루는 기본 단위</li><li>Storage Engine<ul><li>MyISAM : full text index 지원, table 단위 lock, select가 빠름, 구조 단순</li><li>InnoDB : transaction 지원, row 단위 lock, 자원을 많이 사용, 구조 복잡</li></ul></li></ul></li><li><p>Column</p><ul><li>테이블의 세로축 데이터</li><li>Field, Attribute 라고도 불림</li></ul></li><li><p>Row</p><ul><li>테이블의 가로축 데이터</li><li>Tuple, Recode 라고도 불림</li></ul></li><li><p>Value</p><ul><li>행(row)과 열(column)에 포함되어있는 데이터</li></ul></li><li><p>Key</p><ul><li>행(row)의 식별자로 사용</li></ul></li><li><p>Relationship<br><img src="/image/relationship.png" alt="relationship"></p></li><li><p>Schema</p><ul><li>스키마(schema)는 데이터 베이스의 구조를 만드는 디자인<br><img src="/image/schema.png" alt="Schema"></li></ul></li></ul><h3 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h3><ul><li>NoSQL(Not Only SQL)<ul><li>RDBMS의 의존적인 관계가 갖는 한계를 극복하기 위해 만들어진 데이터베이스</li><li>확장성이 좋음<ul><li>데이터 분산처리 용이</li></ul></li><li>데이터 저장이 유연함<ul><li>RDMBS와 다르게 구조의 변경이 불필요</li></ul></li><li>Schema 및 Join이 없음<ul><li>Join 기능이 없으므로 각각의 테이블만 사용가능</li><li>collection 별로 관계가 없기 때문에 모든 데이터가 들어있어야 하므로 RDBMS보다 저장공간이 더 필요</li><li>저장되는 데이터는 Key-value 형태의 JSON 포멧을 사용</li></ul></li><li>select는 RDBMS보다 느리지만 insert가 빨라 대용량 데이터 베이스에 많이 사용</li><li>트랜젝션(transaction)이 지원되지 않음(<code>동시수정에 대한 신뢰성이 지원되지 않음</code>)</li></ul></li></ul><p><img src="/image/NoSQL.png" alt="NoSQL"></p><p><img src="/image/DB_ranking.png" alt="DB 순위"><br><a href="https://db-engines.com/en/ranking_trend" target="_blank" rel="noopener">https://db-engines.com/en/ranking_trend</a></p><h2 id="Install-MySQL-for-Mac-OS"><a href="#Install-MySQL-for-Mac-OS" class="headerlink" title="Install MySQL(for Mac OS)"></a>Install MySQL(for Mac OS)</h2><ul><li>주의) 2가지 방법을 소개하지만, <code>Python에서 MySQL을 활용할 User들에게는 1번 방법으로 설치를 해야한다는 것을 알려드립니다!!!</code></li><li><code>brew(1번방법)로 설치해야 python의 mysql client를 사용할수 있습니다.</code></li></ul><h3 id="방법-1"><a href="#방법-1" class="headerlink" title="방법 1)"></a>방법 1)</h3><ul><li>reference<ul><li><a href="https://gist.github.com/operatino/392614486ce4421063b9dece4dfe6c21" target="_blank" rel="noopener">https://gist.github.com/operatino/392614486ce4421063b9dece4dfe6c21</a></li></ul></li></ul><h4 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ brew install mysql@version_num</span><br><span class="line">$ brew tap homebrew/services $ brew services start mysql@version_num</span><br><span class="line">$ brew services list</span><br><span class="line">$ brew link mysql@version_num --force</span><br><span class="line">$ mysql -V</span><br></pre></td></tr></table></figure><h4 id="앞으로-SQL-접속시-사용할-Password"><a href="#앞으로-SQL-접속시-사용할-Password" class="headerlink" title="앞으로 SQL 접속시 사용할 Password!!"></a>앞으로 SQL 접속시 사용할 Password!!</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mysqladmin -u root password <span class="string">'yourpassword'</span></span><br></pre></td></tr></table></figure><h4 id="Connect-mysql-server"><a href="#Connect-mysql-server" class="headerlink" title="Connect mysql server"></a>Connect mysql server</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -u root -p</span><br></pre></td></tr></table></figure><h3 id="방법-2-dmg-파일-받아서-install"><a href="#방법-2-dmg-파일-받아서-install" class="headerlink" title="방법 2) dmg 파일 받아서 install"></a>방법 2) dmg 파일 받아서 install</h3><ul><li><p>step 1) <a href="https://dev.mysql.com/downloads/mysql/5.7.html#downloads" target="_blank" rel="noopener">https://dev.mysql.com/downloads/mysql/5.7.html#downloads</a>에서 <code>DMG 파일 다운로드</code></p></li><li><p>step 2) 다운 받은 DMG 파일을 실행</p><ul><li><code>설치 중간에 임시 패스워드를 기억</code></li></ul></li></ul><p><img src="/image/MySQL_installer.png" alt="MySQL installer"></p><ul><li>step 3) 시스템 환경설정에 가면 MySQL이 설치 된것을 확인</li></ul><h3 id="MySQL-서버의-인스턴스를-정지-및-실행-초기화-제거등을-할수-있다"><a href="#MySQL-서버의-인스턴스를-정지-및-실행-초기화-제거등을-할수-있다" class="headerlink" title="MySQL 서버의 인스턴스를 정지 및 실행, 초기화, 제거등을 할수 있다."></a>MySQL 서버의 인스턴스를 정지 및 실행, 초기화, 제거등을 할수 있다.</h3><ul><li>Start MySQL Server 버튼을 클릭하여 실행</li></ul><h4 id="아래의-경로로-이동"><a href="#아래의-경로로-이동" class="headerlink" title="아래의 경로로 이동"></a>아래의 경로로 이동</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cd /usr/local/mysql/bin</span><br></pre></td></tr></table></figure><h4 id="Mysql-서버에-접속"><a href="#Mysql-서버에-접속" class="headerlink" title="Mysql 서버에 접속"></a>Mysql 서버에 접속</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ./mysql -p</span><br></pre></td></tr></table></figure><h4 id="Password-관리자-권한으로-실행을-위한-PC의-패스워드"><a href="#Password-관리자-권한으로-실행을-위한-PC의-패스워드" class="headerlink" title="Password: (관리자 권한으로 실행을 위한 PC의 패스워드)"></a>Password: (관리자 권한으로 실행을 위한 PC의 패스워드)</h4><h4 id="Enter-password-임시로-발급받은-DB의-패스워드-입력"><a href="#Enter-password-임시로-발급받은-DB의-패스워드-입력" class="headerlink" title="Enter password: (임시로 발급받은 DB의 패스워드 입력)"></a>Enter password: (임시로 발급받은 DB의 패스워드 입력)</h4><ul><li>아래의 mysql 프롬프트가 나오면 정상!! 설치 완료!!<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure></li></ul><h4 id="패스워드-변경-qwer1234-로-변경할-경우"><a href="#패스워드-변경-qwer1234-로-변경할-경우" class="headerlink" title="패스워드 변경 ( qwer1234 로 변경할 경우 )"></a>패스워드 변경 ( qwer1234 로 변경할 경우 )</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;qwer1234&apos;;</span><br><span class="line">mysql&gt; FLUSH PRIVILEGES; mysql&gt; quit;</span><br></pre></td></tr></table></figure><h4 id="변경한-패스워드로-다시-로그인"><a href="#변경한-패스워드로-다시-로그인" class="headerlink" title="변경한 패스워드로 다시 로그인"></a>변경한 패스워드로 다시 로그인</h4><h2 id="Mysql-Basic-Command"><a href="#Mysql-Basic-Command" class="headerlink" title="Mysql Basic Command"></a>Mysql Basic Command</h2><h3 id="system"><a href="#system" class="headerlink" title="system"></a>system</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mysql명령어 리스트 확인</span></span><br><span class="line">mysql&gt; <span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 현재 상태 보기</span></span><br><span class="line">mysql&gt; status</span><br><span class="line"></span><br><span class="line"><span class="comment"># mysql 접속 종료</span></span><br><span class="line">mysql&gt; <span class="built_in">exit</span></span><br><span class="line">mysql&gt; quit</span><br><span class="line"></span><br><span class="line"><span class="comment"># 패스워드 변경 ( qwer1234 로 변경하는 경우 )</span></span><br><span class="line">mysql&gt; ALTER USER <span class="string">'root'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'qwer1234'</span></span><br><span class="line">2.2 Database</span><br></pre></td></tr></table></figure><h3 id="Database"><a href="#Database" class="headerlink" title="Database"></a>Database</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DB 목록 보기</span></span><br><span class="line">mysql&gt; show databases;</span><br><span class="line"></span><br><span class="line"><span class="comment"># DB 만들기 ( DB이름을 test라고 하려면 )</span></span><br><span class="line">mysql&gt; create database <span class="built_in">test</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># DB 접속하기 ( DB 이름 test )</span></span><br><span class="line">mysql&gt; use <span class="built_in">test</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 현재 접속중인 DB 확인하기</span></span><br><span class="line">mysql&gt; select database();</span><br><span class="line"></span><br><span class="line"><span class="comment"># DB 지우기</span></span><br><span class="line">mysql&gt; drop database <span class="built_in">test</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># DB 삭제 확인</span></span><br><span class="line">mysql&gt; show databases;</span><br></pre></td></tr></table></figure><h3 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 테이블 만들기</span></span><br><span class="line"><span class="comment"># 문자열 name 20자, age 숫자 3자 컬럼이 있는 테이블이 생성</span></span><br><span class="line">mysql&gt; create table user ( name char(20), age int(3) );</span><br><span class="line"></span><br><span class="line"><span class="comment"># 테이블 목록 확인</span></span><br><span class="line">mysql&gt; show tables;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 테이블 구조 확인</span></span><br><span class="line">mysql&gt; desc user;</span><br><span class="line">mysql&gt; describe user;</span><br><span class="line">mysql&gt; explain user;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 테이블 이름 바꾸기(another로 바꾸기)</span></span><br><span class="line">mysql&gt; rename table user to another;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 테이블 이름 바뀐것 확인 mysql&gt; show tables;</span></span><br><span class="line"><span class="comment"># 테이블에 데이터 추가하기</span></span><br><span class="line">mysql&gt; insert into another(name, age) values(<span class="string">"alice"</span>, 23);</span><br><span class="line">mysql&gt; insert into another(name, age) values(<span class="string">"peter"</span>, 30);</span><br><span class="line"></span><br><span class="line"><span class="comment"># 추가된 데이터 확인하기</span></span><br><span class="line">mysql&gt; select * from anther;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 테이블 지우기</span></span><br><span class="line">mysql&gt; drop table anther;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 테이블 삭제된것 확인</span></span><br><span class="line">mysql&gt; show tables;</span><br></pre></td></tr></table></figure><h2 id="Database-Management-Application-for-Mac-OS"><a href="#Database-Management-Application-for-Mac-OS" class="headerlink" title="Database Management Application for Mac OS"></a>Database Management Application for Mac OS</h2><h4 id="step-1-Install-Sequel-Pro"><a href="#step-1-Install-Sequel-Pro" class="headerlink" title="step 1) Install Sequel Pro"></a>step 1) Install Sequel Pro</h4><ul><li><a href="https://www.sequelpro.com/" target="_blank" rel="noopener">https://www.sequelpro.com/</a> 경로에서 sequelpro를 다운 받아서 설치</li></ul><h3 id="step-2-Connect-Database-Server"><a href="#step-2-Connect-Database-Server" class="headerlink" title="step 2) Connect Database Server"></a>step 2) Connect Database Server</h3><ul><li>아래와 같이 Host, Username, Password를 설정하여 연결<br><img src="/image/Connection_DB_Server.png" alt="Connection_DB_Server"></li></ul><h2 id="Sample-Database-Download"><a href="#Sample-Database-Download" class="headerlink" title="Sample Database Download"></a>Sample Database Download</h2><ul><li><a href="https://dev.mysql.com/doc/index-other.html" target="_blank" rel="noopener">https://dev.mysql.com/doc/index-other.html</a>링크에서 Sample database 를 다운<ul><li>혹시라도 앞으로 저의 블로그를 보시면서 따라해보실 분들은 world database, sakila database 를 다운받아 주세요.</li></ul></li></ul><p><img src="/image/example_db.png" alt="example_db"></p><ul><li>sql 파일 추가<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/usr/<span class="built_in">local</span>/mysql/bin 디렉토리에서 아래와 같이 실행하면 sql 파일을 import - import 하기 전에 world 데이터 베이스가 있어야 함</span><br><span class="line">$ sudo ./mysql -p world &lt; (sql 파일 경로)</span><br><span class="line">- brew로 설치한 경우 아래와 같이 추가</span><br><span class="line">$ mysql -u root -p world &lt; (sql 파일 경로)</span><br></pre></td></tr></table></figure></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/10/data_engineering_02/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering basic(Unix환경 및 커맨드)</title>
      <link>https://heung-bae-lee.github.io/2019/12/09/data_engineering_01/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/09/data_engineering_01/</guid>
      <pubDate>Mon, 09 Dec 2019 08:48:07 GMT</pubDate>
      <description>
      
        
        
          &lt;h3 id=&quot;Pipes-and-Filters&quot;&gt;&lt;a href=&quot;#Pipes-and-Filters&quot; class=&quot;headerlink&quot; title=&quot;Pipes and Filters&quot;&gt;&lt;/a&gt;Pipes and Filters&lt;/h3&gt;&lt;p&gt;cat : 해당 파
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h3 id="Pipes-and-Filters"><a href="#Pipes-and-Filters" class="headerlink" title="Pipes and Filters"></a>Pipes and Filters</h3><p>cat : 해당 파일 전체를 print<br>head : 해당 파일 앞의 10줄 정도를 print<br>tail : 해당 파일 뒤의 20줄 정도를 print</p><p>command &gt; file : 기존의 파일 내용은 지우고 현재 command한 결과 파일에 저장<br>command &gt;&gt; file : 기존의 파일에 덮붙여서 결과를 저장(python append같은 느낌!)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># example.py를 python3로 run하고 그 결과를 result.txt파일로 저장</span></span><br><span class="line">python3 example.py &gt; result.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># example.py를 python3로 run하고 그 결과를 result.txt파일에 덮붙여서 저장</span></span><br><span class="line">python3 example.py &gt;&gt; result.txt</span><br></pre></td></tr></table></figure><h3 id="Shell-script"><a href="#Shell-script" class="headerlink" title="Shell script"></a>Shell script</h3><ul><li><p><code>terminal에서 바로 명령어를 여러개 사용하고 싶을때 shell script를 사용하면 된다.</code></p></li><li><p>예를들어 아래의 example.py를 실행시켜 위에서 command를 한번에 실행시키고 싶다면 다음과 같이 먼저 example.py를 작성한 후에 command.sh 파일에는 command들을 작성한 후에 shell script 파일을 run하면 된다.</p></li></ul><h4 id="example-py"><a href="#example-py" class="headerlink" title="example.py"></a>example.py</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    <span class="comment"># command 뒤에 따라오는 첫번째 글자를 print</span></span><br><span class="line">    <span class="built_in">print</span>(sys.argv[1])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="command-sh"><a href="#command-sh" class="headerlink" title="command.sh"></a>command.sh</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line"></span><br><span class="line">python3 example.py 1 &gt; result.txt</span><br><span class="line">python3 example.py 2 &gt;&gt; result.txt</span><br><span class="line">head result.txt</span><br><span class="line">rm -rf result.txt example.py</span><br></pre></td></tr></table></figure><h4 id="terminal창"><a href="#terminal창" class="headerlink" title="terminal창"></a>terminal창</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#권한을 설정</span></span><br><span class="line">chmod +x command.sh</span><br><span class="line">./command.sh</span><br></pre></td></tr></table></figure><h4 id="보통은-우리가-deploy-sh라는-파일로-만들어-그-안에서-작업을-한다-예를-들어서"><a href="#보통은-우리가-deploy-sh라는-파일로-만들어-그-안에서-작업을-한다-예를-들어서" class="headerlink" title="보통은 우리가 deploy.sh라는 파일로 만들어 그 안에서 작업을 한다. 예를 들어서,"></a>보통은 우리가 deploy.sh라는 파일로 만들어 그 안에서 작업을 한다. 예를 들어서,</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># zip형식으로 되어있는 모든(*)파일을 삭제해라</span></span><br><span class="line">rm *.zip</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모든 파일을 lisztfever라는 이름으로 압축해라. -r 옵션은 파일이 있을 수 있으므로 붙여준다.</span></span><br><span class="line">zip lisztfever.zip -r *</span><br><span class="line"></span><br><span class="line"><span class="comment"># aws s3라는 storage에 s3://areha/lisztfever/lisztfever.zip 에 해당 path의 파일을 삭제</span></span><br><span class="line">aws s3 rm s3://areha/lisztfever/lisztfever.zip</span><br><span class="line"></span><br><span class="line"><span class="comment"># s3에게 다시 copy해라</span></span><br><span class="line">aws s3 cp ./listzfever.zip s3://areha/lisztfever/listzfever.zip</span><br><span class="line"></span><br><span class="line"><span class="comment"># aws lambda function을 update해라.</span></span><br><span class="line">aws lambda update-function-code --<span class="keyword">function</span>-name listzfever --s3-buket areha --s3-key listzfever/listzfever.zip</span><br></pre></td></tr></table></figure><h2 id="AWS-Cloud-Service"><a href="#AWS-Cloud-Service" class="headerlink" title="AWS Cloud Service"></a>AWS Cloud Service</h2><p>먼저, <code>IAM</code>(Identity and Access Management)에 대해서 설명하겠다. <code>내가 누구이고 어떤 Access를 가지고 있는지를 관리하는 곳이라고 생각할 수 있다.</code> 여기서 새로운 User를 등록 할 수 있다.</p><p><img src="/image/IAM_00.png" alt="그림00"></p><ul><li>위의 Add User를 통해서 새로운 User를 등록해보자.</li></ul><p><img src="/image/IAM_01.png" alt="그림01"></p><ul><li>Access type은 우리가 AWS cli를 통해서도 관리하므로 첫번째인 Programmatic access로 설정한다.</li></ul><p><img src="/image/IAM_02.png" alt="그림02"></p><ul><li><p>Permission을 주는 방식에 대한 설정하는 부분이다.</p><ul><li><p>Add user to group : 한 Project를 여러명이 같이 진행하여 여러명이 관리할 경우 사용.</p></li><li><p>Copy permissions from existing user : 말 그대로 이미 존재하는 user의 permission들 중 하나를 선택하여 Copy할 경우 사용</p></li><li><p>Attach existing policies directly : AWS에 존재하는 정책들 중 하나를 선택하여 바로 사용하는 경우 사용</p></li></ul></li><li><p>예전에 있던 계정이 만료된걸 모르고 있다가 결제를 안해버려서…. ㅠㅠㅠ 새롭게 만든 계정으로 하느라 등록된 User들이 없다. 그러므로 3번째 설정으로 들어가서 모든 최상위 permission을 갖는 AdministratorAccess를 주었다.</p></li><li><p>다음으로 넘어가게 되면, tag를 설정하게 되는데, 우선 넘어가겠다.(이 부분은 나중에 설정할 것이다.)</p></li></ul><p><img src="/image/IAM_03.png" alt="그림03"></p><ul><li>앞에서 설정한 사항들을 확인하고, Create User를 누르게 되면 설정한대로 User를 생성하게 되는 것이다.</li></ul><p><img src="/image/IAM_03.png" alt="그림04"></p><ul><li>Access key ID, Secret access key가 생성되어 나오는데, <code>이 창이 닫히면, 볼수 없으므로 Download csv를 하는 것을 권장한다.</code></li></ul><ul><li>설치 프로그램을 실행한다. /usr/local/aws에 AWS CLI를 설치하고 /usr/local/bin 디렉터리에 symlink aws를 생성한다. <code>-b 옵션을 사용하여 symlink를 생성하면 사용자의 $PATH 변수에 설치 디렉터리를 지정할 필요가 없다. 이렇게 하면 모든 사용자가 임의 디렉터리에서 aws를 입력하여 AWS CLI를 호출 가능하게 한다.</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl <span class="string">"https://s3.amazonaws.com/aws-cli/awscli-bundle.zip"</span> -o <span class="string">"awscli-bundle.zip"</span></span><br><span class="line">unzip awscli-bundle.zip</span><br><span class="line">sudo ./awscli-bundle/install -i /usr/<span class="built_in">local</span>/aws -b /usr/<span class="built_in">local</span>/bin/aws</span><br></pre></td></tr></table></figure><ul><li>위의 설치가 다 끝나면, 이제 aws cli의 configure를 설정해 볼 것이다. 이를 통해 우리가 console에 접속하지 않고도 cli 환경에서도 aws를 조작할 수 있게 된다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">aws configure</span><br><span class="line">AWS Access Key ID [None]: Access key ID</span><br><span class="line">AWS Secret Access Key [None]: Secret access key</span><br><span class="line">Default region name [None]: ap-northeast-2</span><br><span class="line">Default output format [None]:</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/09/data_engineering_01/#disqus_thread</comments>
    </item>
    
    <item>
      <title>심층 신경망의 구조</title>
      <link>https://heung-bae-lee.github.io/2019/12/08/deep_learning03/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/08/deep_learning03/</guid>
      <pubDate>Sat, 07 Dec 2019 15:00:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;심층-신경망의-구조&quot;&gt;&lt;a href=&quot;#심층-신경망의-구조&quot; class=&quot;headerlink&quot; title=&quot;심층 신경망의 구조&quot;&gt;&lt;/a&gt;심층 신경망의 구조&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/image/Neuron_01.png&quot; alt=&quot;N
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="심층-신경망의-구조"><a href="#심층-신경망의-구조" class="headerlink" title="심층 신경망의 구조"></a>심층 신경망의 구조</h2><p><img src="/image/Neuron_01.png" alt="Neuron"></p><p><img src="/image/shallowNN_01.png" alt="shallow NN"></p><p><img src="/image/Deep_Neural_Network.png" alt="심층 신경망(Deep Neural Network)"></p><p><img src="/image/what_is_difference with_DNN.png" alt="심층신경망은 무엇이 다를까?"></p><ul><li><p><code>은닉 계층 추가 = 특징의 비선형 변환 추가!!</code><br><a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" target="_blank" rel="noopener">선형 변환의 이해</a></p></li><li><p>선형대수의 선형 변환을 함수의 개념에서 보았을때, 입력 차원(n)이 출력 차원(m)보다 크다면 Onto(전사함수: 모든 공역이 치역이 되있는 상태)가 될 수 있지만, 그 반대인 경우는 적은 차원을 갖는 입력벡터의 차원으로 일부분의 출력 벡터의 차원을 커버하는 것이 되는 것이므로, 전사함수가 될 수 없다. 또한, 이런 입력차원(n)이 출력 차원(m)보다 작은 경우의 구조를 우리는 딥러닝 네트워크 구조에서도 볼 수 있다. 예를 들면 GAN이나 Auto Encoder의 decoder구조가 가장 쉬운 예시일 것이다. 여기서의 의문은 그렇다면, 일부분의 차원으로 피처를 잘 배울 수 있는지가 의문일 것이다. <code>허나, 그 일부의 차원이 원래 갖고 있던 특성에서 나올법한 특성들만을 생성해 주므로 걱정하지 않아도 된다.</code></p></li><li><p>또한, 선형시스템의 곱으로 노드들의 연산을 표현할 수 있는데, 여기서, 예를 들어, 특징벡터1과 특징벡터2간의 방향이 비슷한 즉, Orthogonal하지 않고 방향이 비슷한 벡터를 통해 연산을 진행하면 다음 층에서는 노드들 중에 비슷한 특징에 대한 정보를 포함하고 있을 것이다. Inner product를 projection의 개념에서 살펴보면, 어떠한 벡터가 다른 방향의 벡터에 projection을 하는 것은 그 projection한 벡터가 그 방향의 벡터가 어느 정도의 성분을 가지고 있는지를 의미하므로 <code>선형대수 측면에서 위에서 각 피처들간의 곱의 연산들에 의한 새로운 피처들의 생성은 projection된 길이를 비교하는 행위와 동일할 것이다.</code></p></li></ul><h2 id="역전파-학습법의-개념"><a href="#역전파-학습법의-개념" class="headerlink" title="역전파 학습법의 개념"></a>역전파 학습법의 개념</h2><p><img src="/image/Algorithm_learning_and_differentiation.png" alt="알고리즘의 학습과 미분"></p><p><img src="/image/compute_function_with_dependency.png" alt="의존성이 있는 함수의 계산"></p><ul><li>y를 구하려면 x와 z를 알아야 하는데, x와 z에는 중복된 연산이 있어서 비효율적이다.</li></ul><p><img src="/image/Dynamic_Programming.png" alt="동적 계획법(Dynamic Programming)"></p><ul><li><code>처음 계산할 때 값을 저장해주어서 중복계산이 발생하지 않도록 해준다.</code></li></ul><p><img src="/image/chain_rule.png" alt="Chain rule"></p><p><img src="/image/differentiation_of_DNN.png" alt="심층 신경망의 미분(출력계층)"></p><p><img src="/image/differentiation_of_DNN_01.png" alt="심층 신경망의 미분(은닉계층1)"></p><p><img src="/image/differentiation_of_DNN_02.png" alt="심층 신경망의 미분(은닉계층2)"></p><p><img src="/image/Forward_inference.png" alt="순방향 전파(Forward Propagation)"></p><ul><li>학습을 마친 후 validation set이나 test set에 적용할 때는 더 이상 학습을 하지 않으므로 이 순방향 추론만을 사용한다.</li></ul><p><img src="/image/Back_Propagation.png" alt="역전파 학습법(Back-Propagation)"></p><h2 id="심층-신경망의-수학적-이해"><a href="#심층-신경망의-수학적-이해" class="headerlink" title="심층 신경망의 수학적 이해"></a>심층 신경망의 수학적 이해</h2><p><img src="/image/FC_Layer.png" alt="전결합 계층"></p><p><img src="/image/DNN_00.png" alt="심층 신경망"></p><h2 id="역전파-학습의-필요성"><a href="#역전파-학습의-필요성" class="headerlink" title="역전파 학습의 필요성"></a>역전파 학습의 필요성</h2><p><img src="/image/BlackBoxModel.png" alt="블랙박스 모델"></p><p><img src="/image/learning_BlackBoxModel.png" alt="블랙박스 모델의 학습"></p><p><img src="/image/Numerical_Gradient.png" alt="수치적 기울기(Numerical Gradient"></p><p><img src="/image/Numerical_Gradient_of_BlackBoxmodel.png" alt="블랙박스 모델의 수치적 기울기"></p><ul><li>(N+1번) 손실함수를 평가한다고 하는데 그 이유는 기준점이 되는 손실함수를 먼저 한번 계산하고 나머지 편미분시에 가각 N번 평가하기 때문이다.</li></ul><p><img src="/image/Numerical_Gradient_of_DNN.png" alt="심층 신경망의 수치적 기울기"></p><h2 id="합성함수와-연쇄-법칙"><a href="#합성함수와-연쇄-법칙" class="headerlink" title="합성함수와 연쇄 법칙"></a>합성함수와 연쇄 법칙</h2><p><img src="/image/chain_rule_01.png" alt="연쇄 법칙"></p><p><img src="/image/series_connection_of_two_function.png" alt="직렬 연결된 두 함수의 미분"></p><p><img src="/image/differentiation_and_chain_rule.png" alt="미분과 연쇄 법칙"></p><p><img src="/image/expansion_of_chain_rule.png" alt="연쇄법칙의 확장"></p><h2 id="역전파-학습법의-수식적-이해"><a href="#역전파-학습법의-수식적-이해" class="headerlink" title="역전파 학습법의 수식적 이해"></a>역전파 학습법의 수식적 이해</h2><p><img src="/image/DNN_aspect_of_composite function.png" alt="합성 함수로서의 심층 신경망"></p><p><img src="/image/DNN_aspect_of_learning.png" alt="학습관점에서 본 심층 신경망"></p><p><img src="/image/DNN_chain_rule.png" alt="심층신경망의 연쇄법칙"></p><ul><li><code>미분하고자 하는 경로 사이에 있는 모든 미분값을 알아야 원하는 미분을 구할 수 있다는 의미이다.</code></li></ul><p><img src="/image/FCLayer_differentiation.png" alt="전결합 계층의 미분(1)"></p><p><img src="/image/FCLayer_differentiation_01.png" alt="전결합 계층의 미분(2)"></p><p><img src="/image/differentiation_of_sigmoid_function.png" alt="Sigmoid 함수의 미분"></p><p><img src="/image/Back_Propagation_algorithm.png" alt="역전파 알고리즘"></p><ul><li><code>수치적 미분에서는 N+1번을 계산하여야 했지만, 역전파 알고리즘으로 인해 단 한번의 손실함수 평가로 미분을 구할 수 있다.</code></li></ul><h2 id="수치-미분을-이용한-심층-신경망-학습"><a href="#수치-미분을-이용한-심층-신경망-학습" class="headerlink" title="수치 미분을 이용한 심층 신경망 학습"></a>수치 미분을 이용한 심층 신경망 학습</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 수치 미분을 이용한 심층 신경망 학습</span></span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"><span class="comment">## 유틸리티 함수</span></span><br><span class="line">epsilon = 0.0001</span><br><span class="line"></span><br><span class="line">def _t(x):</span><br><span class="line">    <span class="built_in">return</span> np.transpose(x)</span><br><span class="line"></span><br><span class="line">def _m(A, B):</span><br><span class="line">    <span class="built_in">return</span> np.matmul(A, B)</span><br><span class="line"></span><br><span class="line">def sigmoid(x):</span><br><span class="line">    <span class="built_in">return</span> 1 / (1 + np.exp(-x))</span><br><span class="line"></span><br><span class="line">def mean_squared_error(h, y):</span><br><span class="line">    <span class="built_in">return</span> 1 / 2 * np.mean(np.square(h - y))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 뉴런 구현</span></span><br><span class="line">class Neuron:</span><br><span class="line">    def __init__(self, W, b, a):</span><br><span class="line">        self.W = W</span><br><span class="line">        self.b = b</span><br><span class="line">        self.a = a</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Gradient</span></span><br><span class="line">        self.dW = np.zeros_like(self.W)</span><br><span class="line">        self.db = np.zeros_like(self.b)</span><br><span class="line"></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        <span class="built_in">return</span> self.a(_m(_t(self.W), x) + self.b) <span class="comment"># activation((W^T)x + b)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 심층신경망 구현</span></span><br><span class="line">class DNN:</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    hidden_depth : hidden_layer의 갯수</span></span><br><span class="line"><span class="string">    num_neuron : hidden_layer 하나당 neuron의 갯수</span></span><br><span class="line"><span class="string">    num_input : input_layer의 neuron의 갯수</span></span><br><span class="line"><span class="string">    num_output : output_layer의 neuron의 갯수</span></span><br><span class="line"><span class="string">    activation : activation funtion으로 사용할 함수</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid):</span><br><span class="line">        <span class="comment"># W, b initialize</span></span><br><span class="line">        def init_var(i, o):</span><br><span class="line">            <span class="built_in">return</span> np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))</span><br><span class="line"></span><br><span class="line">        self.sequence = list()</span><br><span class="line">        <span class="comment"># First hidden layer</span></span><br><span class="line">        W, b = init_var(num_input, num_neuron)</span><br><span class="line">        self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Hidden layers</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(hidden_depth - 1):</span><br><span class="line">            W, b = init_var(num_neuron, num_neuron)</span><br><span class="line">            self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Output layer</span></span><br><span class="line">        <span class="comment"># 단순히 심층신경망 구현 후에 수치미분을 사용한 역전파학습을 보이기 위한 코드이므로</span></span><br><span class="line">        <span class="comment"># Output layer의 activation function을 따로 바꾸지 않고 sigmoid로 사용하겠다.</span></span><br><span class="line">        W, b = init_var(num_neuron, num_output)</span><br><span class="line">        self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        <span class="comment"># layer를 call하는 것은 결국 위에서 정의한 Neuron의 call이 될 것이고</span></span><br><span class="line">        <span class="comment"># x는 activation((W^T)x + b)이 될 것이다.</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.sequence:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="built_in">return</span> x</span><br><span class="line"></span><br><span class="line">    def calc_gradient(self, x, y, loss_func):</span><br><span class="line">        def get_new_sequence(layer_index, new_neuron):</span><br><span class="line">        <span class="comment"># 특정한 변수하나(weight나 bias)만 변화를 줘서 그 때 loss가 얼마나 변하는지를 보고</span></span><br><span class="line">        <span class="comment"># numerical gradient를 계산하려하기 때문에 변화된 변수가 있는 새로운 Sequence가 필요하다.</span></span><br><span class="line">            new_sequence = list()</span><br><span class="line">            <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(self.sequence):</span><br><span class="line">                <span class="keyword">if</span> i == layer_index:</span><br><span class="line">                    new_sequence.append(new_neuron)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    new_sequence.append(layer)</span><br><span class="line">            <span class="built_in">return</span> new_sequence</span><br><span class="line"></span><br><span class="line">        def eval_sequence(x, sequence):</span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> sequence:</span><br><span class="line">                x = layer(x)</span><br><span class="line">            <span class="built_in">return</span> x</span><br><span class="line"></span><br><span class="line">        loss = loss_func(self(x), y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer_id, layer <span class="keyword">in</span> enumerate(self.sequence): <span class="comment"># iterate layer</span></span><br><span class="line">            <span class="keyword">for</span> w_i, w <span class="keyword">in</span> enumerate(layer.W): <span class="comment"># iterate W (row)</span></span><br><span class="line">                <span class="keyword">for</span> w_j, ww <span class="keyword">in</span> enumerate(w): <span class="comment"># iterate W (col)</span></span><br><span class="line">                    W = np.copy(layer.W)</span><br><span class="line">                    W[w_i][w_j] = ww + epsilon</span><br><span class="line"></span><br><span class="line">                    new_neuron = Neuron(W, layer.b, layer.a)</span><br><span class="line">                    new_seq = get_new_sequence(layer_id, new_neuron)</span><br><span class="line">                    h = eval_sequence(x, new_seq)</span><br><span class="line"></span><br><span class="line">                    num_grad = (loss_func(h, y) - loss) / epsilon  <span class="comment"># (f(x+eps) - f(x)) / epsilon</span></span><br><span class="line">                    layer.dW[w_i][w_j] = num_grad</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> b_i, bb <span class="keyword">in</span> enumerate(layer.b): <span class="comment"># iterate b</span></span><br><span class="line">                    b = np.copy(layer.b)</span><br><span class="line">                    b[b_i] = bb + epsilon</span><br><span class="line"></span><br><span class="line">                    new_neuron = Neuron(layer.W, b, layer.a)</span><br><span class="line">                    new_seq = get_new_sequence(layer_id, new_neuron)</span><br><span class="line">                    h = eval_sequence(x, new_seq)</span><br><span class="line"></span><br><span class="line">                    num_grad = (loss_func(h, y) - loss) / epsilon  <span class="comment"># (f(x+eps) - f(x)) / epsilon</span></span><br><span class="line">                    layer.db[b_i] = num_grad</span><br><span class="line">        <span class="comment"># gradient를 계산할 때 loss를 return해야 학습과정에 loss가 어떻게 되는지를 알 수 있기때문에 return 해준다.  </span></span><br><span class="line">        <span class="built_in">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment">## 경사하강법</span></span><br><span class="line">def gradient_descent(network, x, y, loss_obj, alpha=0.01):</span><br><span class="line">    loss = network.calc_gradient(x, y, loss_obj)</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> network.sequence:</span><br><span class="line">        layer.W += -alpha * layer.dW</span><br><span class="line">        layer.b += -alpha * layer.db</span><br><span class="line">    <span class="built_in">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment">## 동작 테스트</span></span><br><span class="line">x = np.random.normal(0.0, 1.0, (10,))</span><br><span class="line">y = np.random.normal(0.0, 1.0, (2,))</span><br><span class="line"></span><br><span class="line">dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=sigmoid)</span><br><span class="line"></span><br><span class="line">t = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(100):</span><br><span class="line">    loss = gradient_descent(dnn, x, y, mean_squared_error, 0.01)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'Epoch &#123;&#125;: Test loss &#123;&#125;'</span>.format(epoch, loss))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'&#123;&#125; seconds elapsed.'</span>.format(time.time() - t))</span><br></pre></td></tr></table></figure><h2 id="역전파-알고리즘을-이용한-심층-신경망-학습"><a href="#역전파-알고리즘을-이용한-심층-신경망-학습" class="headerlink" title="역전파 알고리즘을 이용한 심층 신경망 학습"></a>역전파 알고리즘을 이용한 심층 신경망 학습</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 역전파 학습을 이용한 심층 신경망 학습</span></span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"><span class="comment">## 유틸리티 함수</span></span><br><span class="line">def _t(x):</span><br><span class="line">    <span class="built_in">return</span> np.transpose(x)</span><br><span class="line"></span><br><span class="line">def _m(A, B):</span><br><span class="line">    <span class="built_in">return</span> np.matmul(A, B)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Sigmoid 구현</span></span><br><span class="line">class Sigmoid:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="comment"># 곱의 형태로 나오게 되므로 처음에 1로해서 추후에 입력될 수치에 영향을 덜 주게 해준다.</span></span><br><span class="line">        self.last_o = 1</span><br><span class="line"></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        self.last_o =  1 / (1.0 + np.exp(-x))</span><br><span class="line">        <span class="built_in">return</span> self.last_o</span><br><span class="line"></span><br><span class="line">    def grad(self):</span><br><span class="line">        <span class="comment"># sigmoid(x) * (1- sigmoid(x))</span></span><br><span class="line">        <span class="built_in">return</span> self.last_o*(1-self.last_o)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Mean Squared Error 구현</span></span><br><span class="line">class MeanSquaredError:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="comment"># chain rule을 할 때 MSE로 부터 gradient를 계속해서 가져와야하므로 저장해놓기 위해</span></span><br><span class="line">        self.dh = 1</span><br><span class="line">        self.last_diff = 1</span><br><span class="line"></span><br><span class="line">    def __call__(self, h, y): <span class="comment"># 1/2 * mean((h - y)^2)</span></span><br><span class="line">            self.last_diff = h - y</span><br><span class="line">            <span class="built_in">return</span> 1 / 2 * np.mean(np.square(h - y))</span><br><span class="line"></span><br><span class="line">    def grad(self): <span class="comment"># h - y</span></span><br><span class="line">        <span class="built_in">return</span> self.last_diff</span><br><span class="line"></span><br><span class="line"><span class="comment">## 뉴런 구현</span></span><br><span class="line">class Neuron:</span><br><span class="line">    def __init__(self, W, b, a_obj):</span><br><span class="line">        self.W = W</span><br><span class="line">        self.b = b</span><br><span class="line">        <span class="comment"># activation이 이전과 다르게 class로 작성되었으므로 instanctiation을 해주어야한다.  </span></span><br><span class="line">        self.a = a_obj()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># gradient</span></span><br><span class="line">        self.dW = np.zeros_like(self.W)</span><br><span class="line">        self.db = np.zeros_like(self.b)</span><br><span class="line">        self.dh = np.zeros_like(_t(self.W))</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 아래의 grad_W를 위해 저장해놓는다.</span></span><br><span class="line">        <span class="comment">## W로 미분했을 경우 이전 입력을 갖고 있어야 바로 사용할 수 있으므로</span></span><br><span class="line">        self.last_x = np.zeros((self.W.shape[0]))</span><br><span class="line">        self.last_h = np.zeros((self.W.shape[1]))</span><br><span class="line"></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        self.last_x = x</span><br><span class="line">        self.last_h = _m(_t(self.W), x) + self.b</span><br><span class="line">        <span class="built_in">return</span> self.a(self.last_h)</span><br><span class="line"></span><br><span class="line">    def grad(self): <span class="comment"># dy/dh = W</span></span><br><span class="line">        <span class="built_in">return</span> self.W * self.a.grad()</span><br><span class="line"></span><br><span class="line">    def grad_W(self, dh):</span><br><span class="line">        grad = np.ones_like(self.W)</span><br><span class="line">        grad_a = self.a.grad()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(grad.shape[1]): <span class="comment"># dy/dw = x</span></span><br><span class="line">            grad[:, j] = dh[j] * grad_a[j] * self.last_x</span><br><span class="line">        <span class="built_in">return</span> grad</span><br><span class="line"></span><br><span class="line">    def grad_b(self, dh): <span class="comment"># dy/db = 1</span></span><br><span class="line">        <span class="built_in">return</span> dh * self.a.grad() * 1</span><br><span class="line"></span><br><span class="line"><span class="comment">## 심층신경망 구현</span></span><br><span class="line">class DNN:</span><br><span class="line">    def __init__(self, hidden_depth, num_neuron, input, output, activation=Sigmoid):</span><br><span class="line">        def init_var(i, o):</span><br><span class="line">            <span class="built_in">return</span> np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))</span><br><span class="line"></span><br><span class="line">        self.sequence = list()</span><br><span class="line">        <span class="comment"># First hidden layer</span></span><br><span class="line">        W, b = init_var(input, num_neuron)</span><br><span class="line">        self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Hidden Layers</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hidden_depth):</span><br><span class="line">            W, b = init_var(num_neuron, num_neuron)</span><br><span class="line">            self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Output Layer</span></span><br><span class="line">        W, b = init_var(num_neuron, output)</span><br><span class="line">        self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.sequence:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="built_in">return</span> x</span><br><span class="line"></span><br><span class="line">    def calc_gradient(self, loss_obj):</span><br><span class="line">        loss_obj.dh = loss_obj.grad()</span><br><span class="line">        <span class="comment"># for문에서 한번에 처리하기 위해서 loss object를 넣어준다.</span></span><br><span class="line">        self.sequence.append(loss_obj)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># back_propagation loop</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.sequence) -1, 0 , -1):</span><br><span class="line">            l1 = self.sequence[i]</span><br><span class="line">            l0 = self.sequence[i - 1]</span><br><span class="line"></span><br><span class="line">            l0.dh = _m(l0.grad(), l1.dh)</span><br><span class="line">            l0.dw = l0.grad_W(l1.dh)</span><br><span class="line">            l0.db = l0.grad_b(l1.dh)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># loss object가 들어 있으면 출력을 얻지 못하고 loss 만 얻게 될 것이기 때문이다.</span></span><br><span class="line">        self.sequence.remove(loss_obj)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 경사하강 학습법</span></span><br><span class="line">def gradient_descent(network, x, y, loss_obj, alpha=0.01):</span><br><span class="line">    loss = loss_obj(network(x), y)  <span class="comment"># Forward inference</span></span><br><span class="line">    network.calc_gradient(loss_obj)  <span class="comment"># Back-propagation</span></span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> network.sequence:</span><br><span class="line">        layer.W += -alpha * layer.dW</span><br><span class="line">        layer.b += -alpha * layer.db</span><br><span class="line">    <span class="built_in">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment">## 동작 테스트</span></span><br><span class="line">x = np.random.normal(0.0, 1.0, (10,))</span><br><span class="line">y = np.random.normal(0.0, 1.0, (2,))</span><br><span class="line"></span><br><span class="line">t = time.time()</span><br><span class="line">dnn = DNN(hidden_depth=5, num_neuron=32, input=10, output=2, activation=Sigmoid)</span><br><span class="line">loss_obj = MeanSquaredError()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(100):</span><br><span class="line">    loss = gradient_descent(dnn, x, y, loss_obj, alpha=0.01)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'Epoch &#123;&#125;: Test loss &#123;&#125;'</span>.format(epoch, loss))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'&#123;&#125; seconds elapsed.'</span>.format(time.time() - t))</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/08/deep_learning03/#disqus_thread</comments>
    </item>
    
    <item>
      <title>쉽게 배우는 경사하강 학습법</title>
      <link>https://heung-bae-lee.github.io/2019/12/08/deep_learning02/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/08/deep_learning02/</guid>
      <pubDate>Sat, 07 Dec 2019 15:00:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;쉽게-배우는-경사하강-학습법&quot;&gt;&lt;a href=&quot;#쉽게-배우는-경사하강-학습법&quot; class=&quot;headerlink&quot; title=&quot;쉽게 배우는 경사하강 학습법&quot;&gt;&lt;/a&gt;쉽게 배우는 경사하강 학습법&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/image/s
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="쉽게-배우는-경사하강-학습법"><a href="#쉽게-배우는-경사하강-학습법" class="headerlink" title="쉽게 배우는 경사하강 학습법"></a>쉽게 배우는 경사하강 학습법</h2><p><img src="/image/supervised_learning_vs_unsupervised_learning.png" alt="지도 학습 vs 비지도 학습"></p><p><img src="/image/supervised_learning_of_human.png" alt="사람의 지도학습"></p><p><img src="/image/supervised_learning_of_machine.png" alt="머신러닝의 지도학습"></p><p><img src="/image/hyper_parameters.png" alt="학습 매개변수"></p><p><img src="/image/Loss_function.png" alt="손실 함수"></p><ul><li><code>어떤 손실 함수를 사용하느냐에 따라서 학습이 어떻게 이루어질 것인지, 그리고 학습을 할 때 정답의 형태를 결정하기 때문에 손실 함수는 중요하다!</code></li></ul><p><img src="/image/another_aspect_about_algorithm_learning.png" alt="알고리즘 학습을 달리 말하면"></p><ul><li>Traning Data를 Model에 입력해 우리가 학습시키고자 하는 Trainable Parameters를 얻게 되는데 <code>Trainable Parameters</code>들을 <code>inputs</code>으로 보고 <code>outputs</code>을 학습결과인 <code>Loss Function</code>으로 생각하면, <code>알고리즘 학습은 입력을 바꿔가면서, 출력값이 점점 작아지게 하는 것이라고 볼 수 있다.</code></li></ul><p><img src="/image/optimization_theory_and_algorithm_learning.png" alt="최적화 이론과 알고리즘 학습"></p><ul><li>결국 알고리즘 학습은 입력을 바꿔가면서, 출력값이 점점 작아지게 하는 것이라는 관점에서 <code>최적화 이론의 목표와 동일</code>하다는 사실을 알 수 있다.</li></ul><h2 id="경사-하강-학습법"><a href="#경사-하강-학습법" class="headerlink" title="경사 하강 학습법"></a>경사 하강 학습법</h2><p><img src="/image/Brute_Force.png" alt="무차별 대입법(Brute-Force)"></p><ul><li><code>무차별 대입법</code>은 범위를 알아야하고 범위를 안다해도 step을 촘촘히 조사해야 하므로 <code>계산 복잡도가 높다</code>. <code>적게 대입해 보고 답을 찾을 수 있는 방법을 생각하다 최적화 알고리즘이 발전 하게 되었다.</code></li></ul><p><img src="/image/Graadient_Descent.png" alt="경사하강법(Gradient Descent)"></p><p><img src="/image/Graadient_Descent_01.png" alt="경사하강법(Gradient Descent)"></p><p><img src="/image/selection_of_learning_rate.png" alt="학습률의 선택 00"></p><p><img src="/image/selection_of_learning_rate_01.png" alt="학습률의 선택 01"></p><p><img src="/image/selection_of_learning_rate_02.png" alt="학습률의 선택 02"></p><p><img src="/image/Convex_Function.png" alt="Convex Function"></p><p><img src="/image/Non_Convex_Function.png" alt="Non-convex Function"></p><h2 id="최적화-이론과-수학적-표현"><a href="#최적화-이론과-수학적-표현" class="headerlink" title="최적화 이론과 수학적 표현"></a>최적화 이론과 수학적 표현</h2><p><img src="/image/optimization_theory.png" alt="최적화 이론(Optimization Theory)"></p><p><img src="/image/analysis_and_numerical_method.png" alt="분석적 vs 수치적 방법"></p><ul><li>수치적 방법의 대표적인 방법이 경사하강법이다.</li></ul><p><img src="/image/global_and_local_solution.png" alt="Global vs local solution"></p><p><img src="/image/deep_learning_and_optimization_theory.png" alt="딥러닝과 최적화 이론"></p><h2 id="심화-경사-하강-학습법"><a href="#심화-경사-하강-학습법" class="headerlink" title="심화 경사 하강 학습법"></a>심화 경사 하강 학습법</h2><ul><li>경사하강 학습법의 단점들을 극복한 알고리즘에 대해서 알아보자.</li></ul><p><img src="/image/Non_convex_function_01.png" alt="Non_convex Finction"></p><p><img src="/image/Local_minimum.png" alt="Local Minimum"></p><p><img src="/image/Saddle_point.png" alt="Saddle Point"></p><ul><li>경사하강법은 안장점에서 기울기가 0이 되므로 벗어나지 못하게 되는 문제점이 있다.</li></ul><p><img src="/image/Momentum.png" alt="Momentum"></p><ul><li><p><code>이동 벡터가 이전 기울기에 영향을 받도록 하는 방법 이전의 속도에 영향을 받는 방법이라고 할 수 있다.</code></p></li><li><p>장점 : <code>Local minimum과 noise에 대처 가능</code></p></li><li><p>단점 : <code>경사하강법은 단순히</code>$x_{t-1}$<code>이동벡터(</code>$v_{t}$<code>)를 추가로 사용하므로, 경사 하강법 대비 2배의 메모리를 사용</code></p></li></ul><p><img src="/image/AdaGrad.png" alt="AdaGrad"></p><ul><li><p><code>변수별로 learning rate가 달라지게 조절한다.</code> 예를 들어서 $x=[x_{1}, x_{2}, x_{3},…,x_{n}]$이 존재할때 어떤 변수는 기울기를 크게 가져가고 어떤 변수는 기울기를 작게 가져갈 경우 <code>처음에 기울기를 크게 가져가지 못한다면 local minimum에 빠지기 쉬운 문제점이 있다.</code> 이런 문제점을 해결하고자 변수별로 learning rate를 다르게 가져가는 알고리즘인 Ada Grad 탄생된 것이다.</p></li><li><p>장점 : $g_{t}$가 누적되어 커진 것은 학습이 그만큼 많이 된 것이므로 <code>학습이 많이 변수는 학습율을 감소시켜, 다른 변수들이 잘 학습되도록 한다.</code></p></li><li><p>단점 : $g_{t}$ <code>가 계속해서 커져서 학습이 오래 진행되면 learning rate가 0ㅇ에 가까워지므로 더이상 학습이 이루어지지 않는 단점이 있다.</code></p></li></ul><p><img src="/image/RMSprop.png" alt="RMSProp"></p><ul><li>gradient의 크기를 제곱한 벡터(gradient벡터의 L2-norm)를 누적합을 해서 적게 학습되는 변수들을 더 학습시켜 주도록했지만 <code>epoch나 batchsize등 반복 시키는 parameter의 value가 높아질수록 오래 진행되어 누적합이 커지게 되면 더 이상 학습이 되지 않는 문제점을 개선한 방법</code>이다. 위의 식에서 $\gamma$<code>값은 0~1값을 갖게 되며, 이 값을 통해 이전의 gradient 누적합을 감소시키는 효과를 주면서 새로운 gradient의 값을 쫓아갈 수 있도록 개선하였다. 그러므로, 변수 간의 상대적인 학습율 차이는 유지하면서</code>$g_{t}$<code>가 무한정 커지지 않아 학습을 오래 할 수 있다.</code></li></ul><p><img src="/image/Adam.png" alt="Adam"></p><ul><li><code>RMSprop과 Momentum의 장점을 결합한 알고리즘이다. 대부분의 코드에 이 Adam optimization을 사용한다.</code></li></ul><h2 id="경사-하강법을-이용한-얕은-신경망-학습"><a href="#경사-하강법을-이용한-얕은-신경망-학습" class="headerlink" title="경사 하강법을 이용한 얕은 신경망 학습"></a>경사 하강법을 이용한 얕은 신경망 학습</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 경사 하강법을 이용한 얕은 신경망 학습</span></span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"><span class="comment">## 하이퍼 파라미터 설정</span></span><br><span class="line">epochs = 1000</span><br><span class="line"></span><br><span class="line"><span class="comment">## 네트워크 구조 정의</span></span><br><span class="line"><span class="comment">### 얕은 신경망</span></span><br><span class="line"><span class="comment">#### 입력 계층 : 2, 은닉 계층 : 128 (Sigmoid activation), 출력 계층 : 10 (Softmax activation)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># keras의 모듈을 상속해서 Model을 구현</span></span><br><span class="line">class MyModel(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="comment"># 상속을 한 경우에는 상속을 한 상위 class를 initialize하는 것을 잊어버리지 말자!</span></span><br><span class="line">        super(MyModel, self).__init__()</span><br><span class="line">        <span class="comment"># 아래의 input_dim을 적어줄 필요는 없다. 실제 데이터가 들어올때 정의 되기 떄문이다.</span></span><br><span class="line">        self.d1 = tf.keras.layers.Dense(128, input_dim=2, activation=<span class="string">"sigmoid"</span>)</span><br><span class="line">        self.d2 = tf.keras.layers.Dense(10, input_dim=128, activation=<span class="string">"softmax"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Model이 실제 call이 될때 입력에서 출력으로 어떻게 연결이 될 것인지를 정의</span></span><br><span class="line">    def call(self, x, training=None, mask=None):</span><br><span class="line">        x = self.d1(x)</span><br><span class="line">        <span class="built_in">return</span> self.d2(x)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 학습 루프 정의</span></span><br><span class="line">@tf.function</span><br><span class="line"><span class="comment"># tensorflow의 Auto Graph를 통해 쉽게 구현가능하다.</span></span><br><span class="line"><span class="comment"># function 내의 python 문법으로 입력된 모든 tensor 연산들을 tf.function에 의해서</span></span><br><span class="line"><span class="comment"># 최적화된다.</span></span><br><span class="line">def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_metric):</span><br><span class="line">    <span class="comment"># Gradient를 계산하기위한</span></span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        predictions = model(inputs)</span><br><span class="line">        loss = loss_object(labels, predictions)</span><br><span class="line">    <span class="comment"># loss를 model의 trainable_variables(W,b)로 각각 미분해서 gradient를 구한것.</span></span><br><span class="line">    <span class="comment"># loss는 scalar이고, model.trainable_variables는 벡터이므로 결과 또한 벡터가 될 것이다.</span></span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 각 gradient와 trainable_variables들이 optimizer로 학습</span></span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># loss를 종합</span></span><br><span class="line">    train_loss(loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># matric</span></span><br><span class="line">    train_metric(labels, predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 데이터셋 생성, 전처리</span></span><br><span class="line">np.random.seed(0)</span><br><span class="line"></span><br><span class="line">pts = []</span><br><span class="line">labels = []</span><br><span class="line"></span><br><span class="line">center_pts =  np.random.uniform(-8.0, 8.0, size=(10, 2))</span><br><span class="line"><span class="keyword">for</span> label, center_pt <span class="keyword">in</span> enumerate(center_pts):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(100):</span><br><span class="line">        pts.append(center_pt + np.random.randn(*center_pt.shape))</span><br><span class="line">        labels.append(label)</span><br><span class="line"></span><br><span class="line"><span class="comment"># GPU를 사용하게 된다면 위의 MyModel class에서 initialize 할때</span></span><br><span class="line"><span class="comment"># Layer에 따로 dtype을 지정하지 않으면 float32로 설정되므로 동일하게 해주기 위해 type 재설정</span></span><br><span class="line">pts =  np.stack(pts, axis=0).astype(np.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 이미 integer이므로 바꿀 필요가 없음.</span></span><br><span class="line">labels =  np.stack(labels, axis=0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 위에서 만든 데이터를 train data set으로 변형</span></span><br><span class="line"><span class="comment"># train_ds는 iterable한 object가 된다.</span></span><br><span class="line"><span class="comment"># 1000개를 섞어 batch_size를 32개로 해서 구성해준다.</span></span><br><span class="line">train_ds =  tf.data.Dataset.from_tensor_slices((pts, labels)).shuffle(1000).batch(32)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pts.shape)</span><br><span class="line"><span class="built_in">print</span>(labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 모델 생성</span></span><br><span class="line">model = MyModel()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 손실 함수 및 최적화 알고리즘 설정</span></span><br><span class="line"><span class="comment">### CrossEntropy, Adam Optimizer</span></span><br><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">optimizer = tf.keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 평가 지표 설정</span></span><br><span class="line"><span class="comment">### Accuracy</span></span><br><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 학습 루프</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line"></span><br><span class="line">    <span class="comment">#위에서 batch_size를 32로 했으므로 한번 실행시 32개씩 나옴.</span></span><br><span class="line">    <span class="keyword">for</span> x, label <span class="keyword">in</span> train_ds:</span><br><span class="line">        train_step(model, x, label, loss_object, optimizer, train_loss, train_accuracy)</span><br><span class="line"></span><br><span class="line">    template = <span class="string">'Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;'</span></span><br><span class="line">    <span class="built_in">print</span>(template.format(epoch+1, train_loss.result(), train_accuracy.result()*100))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 데이터셋 및 학습 파라미터 저장</span></span><br><span class="line"><span class="comment"># 압축해서 여러개의 Numpy Object들을 저장할 수 있다.</span></span><br><span class="line">np.savez_compressed(<span class="string">'ch2_dataset.npz'</span>, inputs=pts, labels=labels)</span><br><span class="line"></span><br><span class="line">W_h, b_h = model.d1.get_weights()</span><br><span class="line">W_o, b_o = model.d2.get_weights()</span><br><span class="line"></span><br><span class="line"><span class="comment"># weight는 tensorflow에서 사용하고 있는 convention이랑</span></span><br><span class="line"><span class="comment"># shallowNN을 구현할 때 사용했던 convention이 좀 다르다.</span></span><br><span class="line">W_h = np.transpose(W_h)</span><br><span class="line">W_o = np.transpose(W_o)</span><br><span class="line"></span><br><span class="line">np.savez_compressed(<span class="string">'ch2_parameters.npz'</span>, W_h=W_h, b_h=b_h, W_o=W_o, b_o=b_o)</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/08/deep_learning02/#disqus_thread</comments>
    </item>
    
    <item>
      <title>가장 단순한 신경망을 통한 작동원리</title>
      <link>https://heung-bae-lee.github.io/2019/12/06/deep_learning01/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/06/deep_learning01/</guid>
      <pubDate>Thu, 05 Dec 2019 15:00:00 GMT</pubDate>
      <description>
      
        
        
          &lt;p&gt;&lt;img src=&quot;/image/Neuron.png&quot; alt=&quot;신경 세포&quot;&gt;&lt;br&gt;&lt;img src=&quot;/image/Graph_of_Neuron.png&quot; alt=&quot;Neuron의 그래프 표현&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Node가 단일 뉴런 연산을 의미한
        
      
      </description>
      
      
      <content:encoded><![CDATA[<p><img src="/image/Neuron.png" alt="신경 세포"><br><img src="/image/Graph_of_Neuron.png" alt="Neuron의 그래프 표현"></p><ul><li>Node가 단일 뉴런 연산을 의미한다고 했는데 여기서의 단일 뉴런 연산이란 input에 가중치를 곱하고 합계를 낸 후에 activation function까지 통과시키는 과정을 의미한다.</li></ul><p><img src="/image/Artificial_NN.png" alt="인공신경망"></p><p><img src="/image/Fully_connected_Layer.png" alt="Fully-Connected Layer(Dense Layer)"></p><p><img src="/image/ShallowNN.png" alt="얕은 신경망"></p><p><img src="/image/what_will_you_do_with_shallowNN.png" alt="얕은 신경망으로 무엇을 할 수 있을까?"></p><p><img src="/image/regression.png" alt="회귀"></p><p><img src="/image/classification.png" alt="분류"></p><p><img src="/image/regression_with_shallowNN.png" alt="얕은 신경망을 이용한 회귀"></p><p><img src="/image/binary_classification_with_shallowNN.png" alt="얕은 신경망을 이용한 이진분류"></p><p><img src="/image/Multi_classification_with_shallowNN.png" alt="얕은 신경망을 이용한 다중 클래스 분류"></p><p><img src="/image/mathmatical_expression_of_Neuron.png" alt="뉴런의 수학적 표현"></p><ul><li>위의 식에서 편향을 잊어버리지 말자!! 예를들면, 편향이 없다면 원점을 지나는 선만 표현할 수 있지만 <code>편향을 통해 원점을 지나지 않는 선들도 표현할 수 있게 할 수 있다.</code> 참고로 특별히 편향이 없는 경우도 있을 순 있다.</li></ul><p><img src="/image/mathmatical_expression_of_Fully_connecter_layer.png" alt="전결합 계층의 수학적 표현"></p><p><img src="/image/input_Layer.png" alt="입력 계층"></p><p><img src="/image/hidden_Layer.png" alt="은닉계층"></p><p><img src="/image/output_Layer.png" alt="출력계층"></p><h2 id="회귀-문제"><a href="#회귀-문제" class="headerlink" title="회귀 문제"></a>회귀 문제</h2><p><img src="/image/regression01.png" alt="회귀"></p><ul><li>어떤 입력이 들어왔을 떄 출력이 연속적인 값을 가질 때 Regression을 사용한다.</li></ul><p><img src="/image/simple_linear_regression.png" alt="단순 선형 회귀"><br><img src="/image/multi_linear_regression.png" alt="다중 선형 회귀"><br><img src="/image/aspect_of_geometry_about_multi_linear_regression.png" alt="다중 선형 회귀의 기하학적 해석"><br><img src="/image/shallowNN_and_regression_algorithm.png" alt="얕은 신경망과 회귀 알고리즘"><br><img src="/image/hidden_Layer_and_regression.png" alt="은닉 계층과 회귀"></p><h2 id="이진-분류-문제"><a href="#이진-분류-문제" class="headerlink" title="이진 분류 문제"></a>이진 분류 문제</h2><p><img src="/image/classification_01.png" alt="분류"><br><img src="/image/logistic_regression.png" alt="로지스틱 회귀"><br><img src="/image/sigmoid_function.png" alt="sigmoid function"><br><img src="/image/binary_cross_entropy.png" alt="교차 엔트로피 오차"><br><img src="/image/aspect_of_geometry _about_multi_logistic_regression.png" alt="다중 로지스틱 회귀의 기하학적 해석"><br><img src="/image/shallowNN_and_classification_algorithm.png" alt="얕은 신경망과 분류 알고리즘"><br><img src="/image/hidden_Layer_and_classification.png" alt="은닉계층과 분류"></p><h2 id="다중-분류-문제"><a href="#다중-분류-문제" class="headerlink" title="다중 분류 문제"></a>다중 분류 문제</h2><p><img src="/image/multi_classification.png" alt="다중 클래스 분류"></p><p><img src="/image/one_hot_encoding.png" alt="one-hot Encoding"><br><img src="/image/sparse_expression_of_one_hot_encoding.png" alt="one-hot Encoding의 희소 표현"><br><img src="/image/multi_classificatio_with_shallowNN.png" alt="얕은 신경망을 이용한 다중 클래스 분류"><br><img src="/image/how_do_shallowNN_compute_output.png" alt="어떻게 출력을 계산할 것인가?"><br><img src="/image/softmax_vs_sigmoid.png" alt="Softmax vs Sigmoid"><br><img src="/image/how_do_shallowNN_compare_Truth_with_outputs.png" alt="정답과 출력을 어떻게 비교할까?"><br><img src="/image/Cross_Entropy_Error.png" alt="Cross Entropy Error"></p><ul><li>Softmax의 분모에 의해서 다른 클래스에 대한 학습에도 영향을 준다는 의미이다. 분모는 다른 클래스로 예측한 확률또한 더해주기 때문이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 얕은 신셩망을 이용한 다중 분류 문제</span></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"><span class="comment">## 함수 구현</span></span><br><span class="line"><span class="comment"># Sigmoid 함수</span></span><br><span class="line"></span><br><span class="line">def sigmoid(x):</span><br><span class="line">    <span class="built_in">return</span> 1/(1+np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Softmax 함수</span></span><br><span class="line">def softmax(x):</span><br><span class="line">    <span class="built_in">return</span> np.exp(x)/np.sum(np.exp(x))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 네트워크 구조 정의</span></span><br><span class="line">class ShallowNN:</span><br><span class="line">    <span class="comment"># 아래의 W와 b에 적절한 값은 추후에 넣어주기 때문에 현재는 0으로 잡음</span></span><br><span class="line">    def __init__(self, num_input, num_hidden, num_output):</span><br><span class="line">        self.W_h = np.zeros((num_hidden, num_input), dtype=np.float32)</span><br><span class="line">        self.b_h = np.zeros((num_hidden, 1), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">        self.W_o = np.zeros((num_output, num_hidden), dtype=np.float32)</span><br><span class="line">        self.b_o = np.zeros((num_output, 1), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># NN의 연산을 call 형태로 해서 작성</span></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        h = sigmoid(np.matmul(self.W_h, x) + self.b_h)</span><br><span class="line">        <span class="built_in">return</span> softmax(np.matmul(self.W_o, h) + self.b_o)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터셋 불러오기</span></span><br><span class="line">dataset = np.load(<span class="string">'ch2_dataset.npz'</span>)</span><br><span class="line">inputs = dataset[<span class="string">'inputs'</span>]</span><br><span class="line">labels = dataset[<span class="string">'labels'</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(labels.shape)</span><br><span class="line"><span class="built_in">print</span>(inputs.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ShallowNN Model 생성</span></span><br><span class="line">model=ShallowNN(num_input=inputs.shape[1], num_hidden=128, num_output=10)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 사전에 학습된 파라미터 불러오기</span></span><br><span class="line">weights = np.load(<span class="string">'ch2_parameters.npz'</span>)</span><br><span class="line">model.W_h = weights[<span class="string">'W_h'</span>]</span><br><span class="line">model.b_h = weights[<span class="string">'b_h'</span>]</span><br><span class="line">model.W_o = weights[<span class="string">'W_o'</span>]</span><br><span class="line">model.b_o = weights[<span class="string">'b_o'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델 결과 프린트</span></span><br><span class="line">outputs = []</span><br><span class="line"><span class="keyword">for</span> point, label <span class="keyword">in</span> zip(inputs, labels):</span><br><span class="line">    output = model(point)</span><br><span class="line">    outputs.append(np.argmax(output))</span><br><span class="line">    <span class="built_in">print</span>(np.argmax(output), label)</span><br><span class="line"></span><br><span class="line">outputs =  np.stack(outputs, axis=0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 정답 클래스 scatter plot</span></span><br><span class="line">plt.figure()</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> range(10):</span><br><span class="line">    mask = labels == idx</span><br><span class="line">    plt.scatter(inputs[mask, 0], inputs[mask, 1])</span><br><span class="line">plt.title(<span class="string">'True Label'</span>)</span><br><span class="line"><span class="comment"># plt.grid()</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 모델 출력 클래스 scatter plot</span></span><br><span class="line">plt.figure()</span><br><span class="line"><span class="keyword">for</span> idx <span class="keyword">in</span> range(10):</span><br><span class="line">    mask = outputs == idx</span><br><span class="line">    plt.scatter(inputs[mask, 0], inputs[mask, 1])</span><br><span class="line">plt.title(<span class="string">'Model output'</span>)</span><br><span class="line"><span class="comment"># plt.grid()</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/06/deep_learning01/#disqus_thread</comments>
    </item>
    
    <item>
      <title>딥러닝이 무엇인가?</title>
      <link>https://heung-bae-lee.github.io/2019/12/05/deep_learning00/</link>
      <guid>https://heung-bae-lee.github.io/2019/12/05/deep_learning00/</guid>
      <pubDate>Wed, 04 Dec 2019 15:00:00 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;딥러닝의-이해&quot;&gt;&lt;a href=&quot;#딥러닝의-이해&quot; class=&quot;headerlink&quot; title=&quot;딥러닝의 이해&quot;&gt;&lt;/a&gt;딥러닝의 이해&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/image/deeplearning_vs_machinelearning_v
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="딥러닝의-이해"><a href="#딥러닝의-이해" class="headerlink" title="딥러닝의 이해"></a>딥러닝의 이해</h2><p><img src="/image/deeplearning_vs_machinelearning_vs_AI.png" alt="딥러닝 vs 기계학습 vs 인공지능"></p><p><img src="/image/deepleaaning_vs_bigdata.png" alt="딥러닝 vs 빅데이터"></p><p><img src="/image/why_deeplearning_is_so_special.png" alt="딥러닝은 왜 특별한가요?"></p><ul><li><p>기계학습의 경우에는 위의 고양이와 개를 구분하기 위해서 이진 분류기를 구현할 것인데, 이런 이진 분류기를 구현하기 위해서는 Feature Extractor가 필요하다. 여기서 말하는 Feature Extractor 란 구분에 용이한 특징을 추출하여 feature vector를 만드는 데 사용하는 것이다. 이렇게 잘 추출한 특징 벡터를 가지고 분류기를 개와 고양이를 구분한다. <code>특징 추출기를 통해 사람이 직접 Feature vector들을 만들고 Classifier 부분만 기계가 학습하는 방식이 Machine Learning이다.</code></p></li><li><p>반면에 딥러닝은 개와 고양이의 row data를 받아서 Feature Extractor가 네트워크 구조 내부에 포함되어 있다. <code>특징 추출도 컴퓨터가 하고 classifier 부분도 컴퓨터가 알아서 분류하므로 전체 네트워크 구조가 학습대상이 된다.</code></p></li></ul><p><img src="/image/what_will_you_do_with_deeplearning00.png" alt="딥러닝으로 무엇을 할 수 있나요?01"><br><img src="/image/what_will_you_do_with_deeplearning01.png" alt="딥러닝으로 무엇을 할 수 있나요?02"><br><img src="/image/what_will_you_do_with_deeplearning02.png" alt="딥러닝으로 무엇을 할 수 있나요?03"><br><img src="/image/deeplearning_consist_of.png" alt="딥러닝의 구성 요소"></p><p>딥러닝은 과거 몇번의 고비(XOR문제를 다층 퍼셉트론으로 극복, 기울기 소실문제는 심층믿음 신경망을 통해 극복)을 극복하고 현재는 많은 이들의 관심 속에 발전해가고 있다. 딥러닝의 대중화를 이끈 요소들을 다음 그림들에서 볼 수있다.<br><img src="/image/tensorflow_and_pytorch.png" alt="TensorFlow&amp;PyTorch"><br><img src="/image/Cloud_Platform.png" alt="딥러닝의대중화-Cloud Platform"><br><img src="/image/GPU.png" alt="딥러닝의대중화-GPU"><br><img src="/image/Cloud_Platform1.png" alt="딥러닝의대중화-Cloud Platform1"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/12/05/deep_learning00/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering basic</title>
      <link>https://heung-bae-lee.github.io/2019/11/29/data_engineering_basic/</link>
      <guid>https://heung-bae-lee.github.io/2019/11/29/data_engineering_basic/</guid>
      <pubDate>Fri, 29 Nov 2019 10:57:36 GMT</pubDate>
      <description>
      
        
        
          &lt;h3 id=&quot;데이터-분석가와-엔지니어링-차이점&quot;&gt;&lt;a href=&quot;#데이터-분석가와-엔지니어링-차이점&quot; class=&quot;headerlink&quot; title=&quot;데이터 분석가와 엔지니어링 차이점&quot;&gt;&lt;/a&gt;데이터 분석가와 엔지니어링 차이점&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h3 id="데이터-분석가와-엔지니어링-차이점"><a href="#데이터-분석가와-엔지니어링-차이점" class="headerlink" title="데이터 분석가와 엔지니어링 차이점"></a>데이터 분석가와 엔지니어링 차이점</h3><ul><li>데이터 분석가는 갖춰진 데이터 시스템과 데이터를 통해서 다양한 분석을 하는 업무이며, 엔지니어링은 그와 다르게 비즈니스에 맞는 데이터를 추출하고 그에 따라 분석하는 환경을 만들어 나가는 업무라고 생각할 수 있을 것이다. 특히, 데이터 전처리나 추출, 정제를 담당하는 업무이다.</li></ul><h3 id="데이터-엔지니어링이-중요한-이유"><a href="#데이터-엔지니어링이-중요한-이유" class="headerlink" title="데이터 엔지니어링이 중요한 이유"></a>데이터 엔지니어링이 중요한 이유</h3><ul><li><code>비즈니스 모델과 가장 연관이 깊은 업무</code>이다. 왜냐하면 회사의 비즈니스 모델에 맞는 데이터를 가져와야하고 가져온 데이터를 통해 어떤 환경을 갖출 것인지, 그에따라 데이터 분석가들이 전략을 짤 때 기반을 갖출 수 있도록 해주려면 어떻게 해야할지가 중요하기 때문이다.<code>그래서 엔지니어링을 뽑을 경우 해당 비즈니스의 Knowledge가 어느 정도 있는 것이 좋을 거라고 생각이들고 실제로 그렇게 면접도 보는(?)것 같다.</code></li></ul><p><img src="/image/facebook.png" alt="페이스북"></p><p><img src="/image/ecommerce.png" alt="이커머스"></p><ul><li>페이스북은 User와 관련 세밀한 데이터가 중요했지만 e-commerce는 User 관련 데이터 보다는 마케팅, CRM, 물류 데이터가 상대적으로 더 중요할 수도 있다.</li></ul><h3 id="데이터-아키텍쳐시-고려사항"><a href="#데이터-아키텍쳐시-고려사항" class="headerlink" title="데이터 아키텍쳐시 고려사항"></a>데이터 아키텍쳐시 고려사항</h3><h4 id="1-비즈니스-모델-상-가장-중요한-데이터는-무엇인가"><a href="#1-비즈니스-모델-상-가장-중요한-데이터는-무엇인가" class="headerlink" title="1.비즈니스 모델 상 가장 중요한 데이터는 무엇인가?"></a>1.비즈니스 모델 상 가장 중요한 데이터는 무엇인가?</h4><ul><li>발생되는 데이터 양 대비 초점을 맞춰야 하는 데이터는 어떤 것인지 즉, <code>비용 대비 비즈니스 임팩트가 가장 높으 데이터를 확보하는 것이 제일 중요</code>하다.</li></ul><h4 id="2-Data-Governance"><a href="#2-Data-Governance" class="headerlink" title="2.Data Governance"></a>2.Data Governance</h4><p><img src="/image/datagovernece.png" alt="데이터 거버넌스"></p><h4 id="3-유연하고-변화-가능한-환경-구축"><a href="#3-유연하고-변화-가능한-환경-구축" class="headerlink" title="3.유연하고 변화 가능한 환경 구축"></a>3.유연하고 변화 가능한 환경 구축</h4><ul><li>특정 기술 및 솔루션에 얽매여져 있지 않고 새로운 테크를 빠르게 적용할 수 있는 아키텍쳐를 만드는 것<ul><li>생성되는 데이터의 형식이 변화할 수 있는 것처럼 그에 맞는 Tool들과 solution들도 빠르게 변화할 수 있는 시스템을 구축하는 것</li></ul></li></ul><h4 id="4-Real-Time-실시간-데이터-핸들링이-가능한-시스템"><a href="#4-Real-Time-실시간-데이터-핸들링이-가능한-시스템" class="headerlink" title="4. Real Time(실시간) 데이터 핸들링이 가능한 시스템"></a>4. Real Time(실시간) 데이터 핸들링이 가능한 시스템</h4><ul><li><p>밀리세컨 단위의 스트리밍 데이터가 됐건 하루에 한번 업데이트 되는 데이터든 데이터 아키텍쳐는 모든 스피드의 데이터를 핸들링 해야한다.</p><ul><li>Real Time Streaming Data Processing</li><li>Cronjob</li><li>Serverless Triggered Data Processing</li></ul></li></ul><h4 id="5-시큐리티"><a href="#5-시큐리티" class="headerlink" title="5. 시큐리티"></a>5. 시큐리티</h4><ul><li>내부와 외부 모든 곳에서부터 발생할 수 있는 위험요소들을 파악하여 <code>어떻게 데이터를 안전하게 관리할 수 있는지 아키텍쳐 안에 포함</code></li></ul><h4 id="6-셀프-서비스-환경-구축"><a href="#6-셀프-서비스-환경-구축" class="headerlink" title="6. 셀프 서비스 환경 구축"></a>6. 셀프 서비스 환경 구축</h4><ul><li>데이터 엔지니어 한명만 엑세스가 가능한 데이터 시스템은 확장성이 없는 데이터 분석 환경이다. 이런 환경에서는 예를 들어, 데이터 분석가들이라던지, 데이터 사이언티스트들, 비즈니스팀들 등 다른 사람들도 BI Tool, Query System for Analysis, Front-end application등 이 가능하게끔 <code>확장성이 있도록 환경을 구축하는 것이 중요하다.</code></li></ul><h3 id="데이터-시스템의-옵션들"><a href="#데이터-시스템의-옵션들" class="headerlink" title="데이터 시스템의 옵션들"></a>데이터 시스템의 옵션들</h3><h4 id="API시대"><a href="#API시대" class="headerlink" title="API시대"></a>API시대</h4><ul><li>현재 마케팅, CRM, ERP등 다양한 플랫폼 및 소프트웨어들은 API라는 송신방법을 통해 데이터를 주고 받을 수 있는 환경을 구축하여 생태계를 생성되어있다. 예를들면, facebook, google, Spotify같은 서비스들이 회사자체에 DB시스템을 구축하고 있는데, 이런 데이터들을 API를 통해 바로 DB에 엑세스해서 서비스를 제공할 수도 있고, 아니면 DB를 새로 생성해 거기에 받아서 저장해놓은 후 정제 및 분석 환경을 구축하여 다양한 서비스를 제공할 수 있다. 이런 환경에서 현재 많은 서비스들이 있으며, 새로운 서비스를 개발하는 입장에서는 필요한 여러가지 서비스들이 있는데, 일일이 다 만들수 없으므로 만들어져 있는 것들, 써드 파티라고 하는 서비스들을 이용하는 것이다. 이러한 서비스를 이용하는 데이터를 가지고도 또다른 분석환경을 구축해야 한다. ex) CaFe24(호스팅업체), facebook Ads,Google Ads(마케팅분야)</li></ul><h4 id="Relational-Databases"><a href="#Relational-Databases" class="headerlink" title="Relational Databases"></a>Relational Databases</h4><ul><li><p>데이터의 관계도를 기반으로 한 디지털 데이터베이스로 데이터의 저장을 목적으로 생겨났다.</p></li><li><p>SQL이라고 하는 스탠다드 방식을 통해 자료를 열람하고 유지한다.</p></li><li><p>현재 대부분의 서비스들이 가장 많이 쓰고 있는 데이터 시스템.</p></li></ul><h4 id="NoSQL-Databases"><a href="#NoSQL-Databases" class="headerlink" title="NoSQL Databases"></a>NoSQL Databases</h4><ul><li><p>관계형 데이터 베이스에서는 Schema 형식에 맞춰 데이터를 추출 및 저장했다면, <code>이제는 너무나 다양한 형식이 없는 데이터 부터 틀에 맞출 수 없는 데이터들이 생성되어 NoSQL이 대두되었다.</code> 예를 들면 메신저에서 많이 사용된다.</p></li><li><p>Not Only NoSQL</p></li><li><p>Unstructured, Schema Less Databases</p></li><li><p>Scale horizontally</p></li><li><p>Highly scalable</p></li></ul><h4 id="Haddop-Spark-Presto-등-빅데이터-처리"><a href="#Haddop-Spark-Presto-등-빅데이터-처리" class="headerlink" title="Haddop / Spark / Presto 등 빅데이터 처리"></a>Haddop / Spark / Presto 등 빅데이터 처리</h4><h5 id="Distribtion-Storage-System-MapReduce를-통한-병렬-처리-Spark"><a href="#Distribtion-Storage-System-MapReduce를-통한-병렬-처리-Spark" class="headerlink" title="Distribtion Storage System / MapReduce를 통한 병렬 처리 Spark"></a>Distribtion Storage System / MapReduce를 통한 병렬 처리 Spark</h5><ul><li><p><code>Hadoop의 진화된 버전으로 빅데이터 분석 환경에서 Real Time 데이터를 프로세싱하기에 더 최적</code></p></li><li><p>java, Python, Scala를 통한 API를 제공하여 Application 생성</p></li><li><p>SQL Query 환경을 서포트하여 분석가들에세 더 각광</p></li></ul><h4 id="서버리스-프레임워크"><a href="#서버리스-프레임워크" class="headerlink" title="서버리스 프레임워크"></a>서버리스 프레임워크</h4><ul><li><p>Triggered by http requests, database events, queuing services</p><ul><li>DB가 됐건, 어떤 server가 됐건 어떠한 하나의 가상 클라우드상에서 server가 필요하게 되는데, 서버를 생성하고 유지및 관리할 때 데이터가 발생하는 event가 발생할 때 Trigger가 되는 부분들을 처리하기 위해 사용한다.</li></ul></li><li><p>Pay as you User</p><ul><li>항상 Server를 띄워놓고 있지 않기 때문에 쓰는 만큼만 비용을 지불하기에 좋다.</li></ul></li><li><p>Form of functions</p><ul><li>하나의 Function이라고 생각하는 것이 좋다. 예를 들어서, 서버리스 프레임 워크를 통해서 어떠한 event가 들어왔을 경우, 어떤 것으로 Trigger가 됐을때, 어떠한 Algorithm을 실행시키는 function이라고 생각하면 된다.</li></ul></li><li><p>3rd Party 앱들 및 다양한 API를 통해 데이터를 수집 정제하는데 유용</p></li></ul><h3 id="데이터-파이프라인"><a href="#데이터-파이프라인" class="headerlink" title="데이터 파이프라인"></a>데이터 파이프라인</h3><h4 id="데이터-파이프라인-1"><a href="#데이터-파이프라인-1" class="headerlink" title="데이터 파이프라인"></a>데이터 파이프라인</h4><ul><li>데이터를 한 장소에서 다른 장소로 옮기는 것을 의미</li></ul><p>ex) API -&gt; DB, DB -&gt; DB, DB -&gt; BI Tool</p><h4 id="데이터-파이프라인이-필요한-경우"><a href="#데이터-파이프라인이-필요한-경우" class="headerlink" title="데이터 파이프라인이 필요한 경우"></a>데이터 파이프라인이 필요한 경우</h4><ul><li><p>1) <code>다양한 데이터 소스들로부터 많은 데이터를 생성하고 저장하는 서비스를 구축</code>할 경우 필요하다!</p></li><li><p>2) <code>데이터 사일로</code>: 마케팅, 어카운팅, 세일즈, 오퍼레이션 등 각 영역의 데이터가 서로 고립되어 있는 경우 (ex)대기업의 각 부서를 생각해보면 이해하기 쉬울 것이다.즉, 각각의 팀들이 따로 존재하여 공유가 어려운경우)</p></li><li><p>3) <code>실시간 혹은 높은 수준의 데이터 분석이 필요한 비즈니스 모델 ex)facebook등</code></p></li><li><p>4) <code>클라우드 환경으로 데이터 저장</code></p></li></ul><p><img src="/image/datapipeline.png" alt="데이터 파이프라인 예시"></p><h4 id="데이터-파이프라인-구축시-고려사항"><a href="#데이터-파이프라인-구축시-고려사항" class="headerlink" title="데이터 파이프라인 구축시 고려사항"></a>데이터 파이프라인 구축시 고려사항</h4><ul><li><p>Scalability : 데이터가 기하급수적으로 늘어났을때도 작동하는가?</p></li><li><p>Stability : 에러, 데이터플로우 등 다양한 모니터링 관리</p></li><li><p>Security : 데이터 이동간 보안에 대한 리스크는 무엇인가?</p></li></ul><h4 id="데이터-프로세싱-자동화란"><a href="#데이터-프로세싱-자동화란" class="headerlink" title="데이터 프로세싱 자동화란?"></a>데이터 프로세싱 자동화란?</h4><ul><li>데이터 프로세싱 자동화란 필요한 데이터를 <code>추출, 수집, 정제하는 프로세싱을 최소한의 사람 인풋으로 머신이 운영하는 것을 의미</code></li></ul><p>ex) Spotify 데이터를 하루에 한번 API를 통해서 클라우드 데이터베이스로 가져온다고 했을 때 매번 사람이 데이터 파이프라인을 작동하는 것이 아니라 Crontab 등 머신 스케쥴링을 통해 자동화</p><h4 id="자동화를-위해-고려할-사항"><a href="#자동화를-위해-고려할-사항" class="headerlink" title="자동화를 위해 고려할 사항"></a>자동화를 위해 고려할 사항</h4><ul><li><p>error가 뜨는 것이든, 추출을 했으면 분석을 한다던지 사람이 하면 순서나 여러가지 고랴를 할 수 있지만, 자동으로 헀을경우는 머신이 모르기 떄문에 다음과 같은 사항들을 고려해야한다.</p></li><li><p>1) 데이터 프로세싱 스텝들<br><img src="/image/dataprocessingstep.png" alt="데이터 프로세싱 스텝들"></p><ul><li>Spotify API에서 어떠한 데이터를 가져와야되고, 그 중에서 어떠한 것들은 걸러내고, 어떤 알고리즘을 돌리고, 그 후에 시각화를 한겠다는 말 그대로 프로세싱 스텝을 의미.</li></ul></li><li><p>2) 에러 핸들링 및 모니터링</p><ul><li>에러가 생성이 됐을때, 어떻게 반응을 하게끔할 것인지, 에러나 퍼포먼스 또는 데이터 추출이 얼마나 걸렸는지 같은 사항을 모니터링 할수 있게끔 구축해야한다.</li></ul></li></ul><p><img src="/image/erroeandhandling.png" alt="에러 핸들링 및 모니터링"></p><ul><li>exampl.log라는 파일에 다양한 로그들을 저장하는데, 그 생성된 로그들도 CloudWatch에도 생성되어 모니터링이 가능하다.</li></ul><ul><li>3) Trigger/ Scheduling<br><img src="/image/triggerandscheduling.png" alt="트리거&amp;스케줄링"><ul><li>다음 단계를 실행하기 위해 어떻게 Trigger가 되어서 실행을 시킬지, 하루에 한번 돌릴지, 아니면 한달에 한번 돌릴지 등에 관한 스케줄을 고려해야한다.</li></ul></li></ul><p><img src="/image/netflex.png" alt="넷플릭스 데이터 시스템 예시"></p><p><img src="/image/uberarchitecture.png" alt="우버 데이터 아키텍쳐"></p><h3 id="Spotify-프로젝트-데이터-아키텍쳐"><a href="#Spotify-프로젝트-데이터-아키텍쳐" class="headerlink" title="Spotify 프로젝트 데이터 아키텍쳐"></a>Spotify 프로젝트 데이터 아키텍쳐</h3><h4 id="Ad-hoc-VS-Automated"><a href="#Ad-hoc-VS-Automated" class="headerlink" title="Ad hoc VS Automated"></a>Ad hoc VS Automated</h4><ul><li><p>Ad hoc 분석 환경 구축은 서비스를 지속적으로 빠르게 변화시키기 위해 필수적인 요소</p><ul><li>Ad hoc 분석은 쉽게 말해 분석을 하고 싶을 때만 하는 것이다. 이런 <code>Ad hoc 분석이 필수적인 이유는 구축한 분석환경을 통해서 다양한 사람들이 분석을 할 수 있게끔해야 하기 때문</code>이다.</li></ul></li><li><p><code>이니셜 데이터 삽입, 데이터 Backfill 등을 위해 Ad hoc 데이터 프로세싱 시스템 구축 필요</code></p></li><li><p>Automated : 이벤트, 스케쥴 등 트리거를 통해 자동화 시스템 구축</p></li></ul><p><img src="/image/dataextractprocessing.png" alt="아티스트 관련 데이터 수집 프로세스"></p><p><img src="/image/dataanalysisenvirment.png" alt="데이터 분석 환경 구축"></p><p><img src="/image/servicerelationprocess.png" alt="서비스 관련 데이터 프로세스"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2019/11/29/data_engineering_basic/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
