<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DataLatte&#39;s IT Blog</title>
    <link>https://heung-bae-lee.github.io/</link>
    
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>DataLatte가 Data Science 공부를 하면서 정리해 놓는 블로그</description>
    <pubDate>Fri, 17 Apr 2020 21:12:25 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>K-Nearest Neighbors(KNN)</title>
      <link>https://heung-bae-lee.github.io/2020/04/17/machine_learning_08/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/17/machine_learning_08/</guid>
      <pubDate>Fri, 17 Apr 2020 09:11:42 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;K-Nearest-Neighbors-KNN&quot;&gt;&lt;a href=&quot;#K-Nearest-Neighbors-KNN&quot; class=&quot;headerlink&quot; title=&quot;K-Nearest Neighbors(KNN)&quot;&gt;&lt;/a&gt;K-Nearest Neighb
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="K-Nearest-Neighbors-KNN"><a href="#K-Nearest-Neighbors-KNN" class="headerlink" title="K-Nearest Neighbors(KNN)"></a>K-Nearest Neighbors(KNN)</h1><ul><li><p><code>k의 개수만큼 주변에 있는 sample들의 정보를 이용해서 새로운 관측치의 종속변수를 예측하는 방법</code>이다. 아래 그림에서 기존의 파란색 네모와 빨간색 세모라는 2가지 클래스를 갖는 데이터 집합이 있다고 할 때, 여기서 새로운 관측치인 녹색에 대해 어떻게 예측할지를 생각해 보자.</p><ul><li>k=3인 경우(실선): 빨간색 세모로 분류될 가능성이 높다.</li><li>k=5인 경우(점선): 파란색 네모로 분류될 가능성이 높다.<ul><li>허나, k=5인 경우 빨간색 세모와의 거리가 더 가깝기 때문에 그만큼의 weight를 주어서 갯수를 떠나서 빨간색으로 분류될 가능성도 알고리즘에 따라 존재한다.</li></ul></li></ul></li></ul><p><img src="/image/KNN_algorithm_concept.png" alt="KNN 알고리즘"></p><h2 id="K-Nearest-neighborhood"><a href="#K-Nearest-neighborhood" class="headerlink" title="K-Nearest neighborhood"></a>K-Nearest neighborhood</h2><p><img src="/image/what_kinds_of_case_KNN_with_some_k.png" alt="K에 따른 KNN의 결과"></p><p><img src="/image/KNN_algorithm_how_to_decide_k.png" alt="파라미터 K의 상황 및 voting 방법"></p><p><img src="/image/how_to_decide_target_variable_in_KNN.png" alt="종속변수에 따른 KNN 알고리즘의 결과값"></p><ul><li>그렇다면 voting 방식외에도 관측치들간의 distance에 따른 가중치를 주는데 여기서 거리를 구하는 방법은 아래 그림과 같은 방법들이 존재한다. 물론, 이 밖에도 데이터의 거리를 구하는 방법은 더 많이 존재한다. 대표적인 유클리디안 거리와 맨하탄 거리, 그리고 범주형 변수에 사용되는 Hamming distance를 간단히 보여 줄 것이다. 유클리디안 거리는 우리가 잘 알고있는 피타고라스 정리에 의해 두 지점 사이의 최단 거리를 구하는 공식으로 계산되며, 맨하탄 거리는 맨하탄 같이 블록별로 되어있는 곳에서의 거리를 계산할 경우 사용된다. 마지막으로 Hamming distance를 계산하는 방식은 Indicator함수안의 조건이 참인 경우만을 1로 값을 산출하여 합한 값이다. 즉, 쉽게 말하면 해당 이진값들의 자리에서 다른 곳이 존재하는 개수를 의미한다.</li></ul><p><img src="/image/how_to_calculate_distance_for_each_type_of_variables.png" alt="독립변수에 따른 각 관측치들간 거리 계산 방법"></p><ul><li>먼저 독립 변수와 새로이 예측하려는 관측값과의 거리를 따져 가장 가까운 데이터부터 순서대로 정렬해 놓는다. 통계적으로 순서 통계량이라고 할 수 있다. 그에 따른 순서로 독립변수와 종속 변수의 쌍으로 정렬한다. 여기서의 거리는 위에서 언급했던 방법들을 사용한다.</li></ul><p><img src="/image/KNN_order_statistics_and_distance_cal.png" alt="KNN 알고리즘의 순서"></p><ul><li>종속변수가 범주형이라면 아래와 같이 소프트 보팅방법으로 확률을 구해 확률값이 가장 큰 클래스를 예측값으로 채택한다. 물론, 범주형 변수도 연속형 변수처럼 거리에 의한 가중치를 사용할 수 있다.</li></ul><p><img src="/image/categorical_variable_method_in_KNN.png" alt="종속변수가 범주형인 경우의 KNN"></p><ul><li>종속변수가 연속형인 경우는 평균을 구하는데, 거리에 따른 가중치를 두어 가중평균을 구하는 것이 일반적이다. 가중치는 거리에 반비례하게 하여 거리가 짧을수록 큰 가중치를 갖게 한다.</li></ul><p><img src="/image/numerical_variable_method_in_KNN.png" alt="종속변수가 연속형인 경우의 KNN"></p><h3 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross validation"></a>Cross validation</h3><ul><li><p>교차검증(Cross validation)은 기본적으로 과적합에 대한 방지를 위해서 실행하며, 또 다른 이유로는 sample loss에 관한 측면이 존재한다.</p></li><li><p>과적합(Overfitting)문제는 Training set에만 최적화 되어있어 새로운 데이터가 들어왔을 때 기존의 Training set에 이질적으로 처리하기 때문이다. 이는 모델의 복잡도와도 밀접한 연관이 있다. 모델의 복잡도가 높을수록 Training set은 잘 맞추지만, Test set에 대한 성능은 낮을 가능성이 높다. 그렇다고 너무 간단하게 만들어버려도 Training set과 Test set 모두에 대해 대한 성능이 좋지 않을 것이다.</p></li><li><p>Training error는 error를 과소추정하는 성향이 있다는 말은 아래 그래프를 살펴보면서 설명하겠다. KNN 모델의 경우 K가 작을수록 모델의 복잡도는 높을 것이다. 가장 복잡한 k=1인 경우 Training error는 거의 0에 수렴한다. 허나, Test error는 상당히 높다. 즉, 과적합이 발생되었다는 이야기이다. 여기서 알 수 있는 것은 <code>Training error로 모델을 선택한다면 너무 복잡한 모델을 선택하여 과적합을 발생시켜 Test error가 커지게 될 시킬 수 있다</code>는 점이다.</p></li></ul><p><img src="/image/overfitting_problem_with_KNN.png" alt="과적합 문제"></p><ul><li>또한, Test error를 구하기 위해 처음에 Train set, Test set으로 나누어야 하기에 Test set으로 나눈만큼의 데이터 소실로 인해 Test error는 증가하게 된다. 이렇게 Error를 과소추정하지 않기 위해서는 교차검증을 해야 할 것이다.</li></ul><p><img src="/image/How_to_overcome_overfitting.png" alt="과적합 문제 보완 방안"></p><ul><li>이러한 과적합을 방지하고자 CV를 실행하는데 그 중 K-hold Cross validation은 다음과 같은 방법으로 진행한다.</li></ul><p><img src="/image/K_hold_crosvalidation_with_KNN_01.png" alt="K-hold 교차검증 - 01"></p><ul><li>Loss function은 예로 들어놓은 것인데, 해당 문제의 성능지표를 의미한다.</li></ul><p><img src="/image/K_hold_crosvalidation_with_KNN_02.png" alt="K-hold 교차검증 - 02"></p><h2 id="KNN의-심화적-이해"><a href="#KNN의-심화적-이해" class="headerlink" title="KNN의 심화적 이해"></a>KNN의 심화적 이해</h2><ul><li>데이터에 따라 적절한 K가 다른데 아래 그림에서와 같이 너무 K를 작게 설정하면, Train set은 굉장히 잘 설명하지만 Test set에 대해선 성능이 안좋게 되는 과적합이 발생될 수 있다. 그 다음으로는 K=1로 설정했을 경우 이상치와의 거리가 가장 가까운 데이터에 영향을 줄 것이다. 아래의 예시 그래프 처럼 영역이 불연속적이어서 영역을 나누는 의미가 없어 보이는 경우가 발생 될 수 있다. 반대로 K가 너무 크다면 미세한 경계에 분류가 아쉬울 것이다.</li></ul><p><img src="/image/what_kinds_of_case_KNN_with_some_k.png" alt="모수 K의 결정"></p><ul><li>데이터에 따라 적절한 K가 다르므로 Test error를 작게하는 k를 선택해야 할 것이다. Cross-validation을 이용하여 보통 모형의 모수들을 조절하게 된다. 그 중 검증 데이터에 대한 성능이 좋은 모수를 채택하여 최종적으로 test 데이터를 예측하는 것이다.</li></ul><p><img src="/image/K_decision_on_KNN.png" alt="K의 결정"></p><ul><li>지난 번에 언급했던 차원의 저주와 KNN 알고리즘도 밀접한 관련이 있다. 차원의 저주는 차원이 늘어남에 따라 우리가 설명하고 싶은 공간 대비 설명할 수 있는 공간이 줄어드는 문제인데, 아래 그림에서와 같이 가로축 변수만을 사용한다면 충분히 두 클래스를 나눌 수 있는 기준을 설정할 수 있지만, 오히려 세로축에 의해서는 클래스를 구분하는데 아무런 영향을 주지 않는데 세로축 변수도 고려하게 되면서 다른 예측을 하게 된다.</li></ul><p><img src="/image/demasion_of_curse_on_knn.png" alt="차원의 저주"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/17/machine_learning_08/#disqus_thread</comments>
    </item>
    
    <item>
      <title>나이브 베이즈 분류모형</title>
      <link>https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/</guid>
      <pubDate>Tue, 14 Apr 2020 05:59:44 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;분류모형&quot;&gt;&lt;a href=&quot;#분류모형&quot; class=&quot;headerlink&quot; title=&quot;분류모형&quot;&gt;&lt;/a&gt;분류모형&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/image/classification_problem.png&quot; alt=&quot;분류 문제란?&quot;&gt;&lt;/p
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="분류모형"><a href="#분류모형" class="headerlink" title="분류모형"></a>분류모형</h1><p><img src="/image/classification_problem.png" alt="분류 문제란?"></p><ul><li>현실적인 문제로 바꾸어 말하면 어떤 표본에 대한 데이터가 주어졌을 때 그 표본이 어떤 카테고리 혹은 클래스에 속하는지를 알아내는 문제이기도 하다.</li></ul><p><img src="/image/what_kinds_of_clssification_model.png" alt="분류모형의 종류"></p><ul><li>흔히들 많이 사용하는 모형들의 분류모형 종류를 아래의 표로 기재해 놓았다. 이전에 기재했던 로지스틱 회귀 분석도 실질적으론 분류문제에 많이 사용된다.</li></ul><div class="table-container"><table><thead><tr><th>모형</th><th>방법론</th></tr></thead><tbody><tr><td>나이브 베이지안</td><td>확률적 생성모형</td></tr><tr><td>LDA/QDA</td><td>확률적 생성모형</td></tr><tr><td>로지스틱회귀</td><td>확률적 판별모형</td></tr><tr><td>의사결정나무</td><td>확률적 판별모형</td></tr><tr><td>퍼셉트론</td><td>판별함수 모형</td></tr><tr><td>서포트벡터머신</td><td>판별함수 모형</td></tr><tr><td>인공신경망</td><td>판별함수 모형</td></tr></tbody></table></div><p><img src="/image/classification_graph.png" alt="분류모형의 종류 도식화"></p><h1 id="나이브-베이즈-분류-모형"><a href="#나이브-베이즈-분류-모형" class="headerlink" title="나이브 베이즈 분류 모형"></a>나이브 베이즈 분류 모형</h1><ul><li>아래 그림과 같은 질문에 대한 확률을 계산하려면, 아래 결합확률을 계산해야 할 것이다. 독립변수가 2개 즉, Feature의 차원이 2개인 지금은 크게 어렵진 않겠지만, 현실에서는 Feature의 개수가 훨씬 더 많을 것이다. 차원이 커진다면 이에따라 해당 조건부 함수의 가능도함수를 추정하는데 어려움을 겪을 것이다.</li></ul><script type="math/tex; mode=display">\begin{align} P( X = weather state, Humidity | Y = playing Tennis) \\ = \frac{P( X_{1} = weather state, Humidity \cap Y = playing Tennis)}{P(Y = playing Tennis)} \end{align}</script><p><img src="/image/naive_bayesian_model_assumption.png" alt="나이브 베이지안 모형의 가정"></p><ul><li>위의 수식에서 분자부분의 확률인 결합확률(Joint probability)을 구하는 것은 쉬운 문제가 아니다. 결합되어있는 상황을 나누어서 확률을 계산할 순 없을까라는 가정에서 나이브 베이지안 모형은 출발한다. 그렇다면, 어떻게 나눌수 있을까? 통계를 조금이라도 아시는 분들은 독립이라는 개념을 알고계실 것이다. 예를 들어 A: 코로나가 집단 발병한다, B: 2주간 재택근무를 시행한다. 이 두가지 사건은 서로 발현될 확률에 영향을 미치기 때문에 두 사건은 종속(dependent) 관계라고 할 수 있다. 허나, A: 비가온다, B: 대출심사에서 승인을 받는다 라는 두 사건은 서로 둘 중 어떤 사건에도 발현될 확률에 영향을 주지 않는다. 이러한, 경우에 우리는 두 사건이 확률적으로 독립이라고 정의한다. 두 사건 A,B가 독립일 필요충분조건은 아래와 같다.</li></ul><script type="math/tex; mode=display">P(A \cap B) = P(A)P(B)</script><ul><li>이렇게 두 사건이 독립이라면 좀 더 추정하기 쉬울 것이다. 허나, 위의 상황에선 조건부이기 때문에 <code>조건부 독립</code>의 가정이 맞을 것이다. 조건부 독립(conditional independence)은 일반적인 독립과 달리 조건이 되는 별개의 확률변수 C가 존재해야한다. 아래 수식이 성립되면 조건부 독립이라고 한다.</li></ul><script type="math/tex; mode=display">A \text{⫫} B \;\vert\; C \Longrightarrow P(A, B | C) = P(A|C)P(B|C)</script><ul><li>조건부독립과 비교하여 일반적인 독립은 무조건부독립이라고 한다. 무조건부 독립은 다음과 같이 표기하기도 한다.</li></ul><script type="math/tex; mode=display">A \text{⫫} B \;\vert\; \emptyset</script><ul><li>A, B가 C에 대해 조건부 독립이면 다음도 만족한다.</li></ul><script type="math/tex; mode=display">P(A \;\vert\; B, C) = P(A \vert C)</script><script type="math/tex; mode=display">P(B \;\vert\; A, C) = P(B \vert C)</script><script type="math/tex; mode=display">\begin{align} P(A|B,C) &= \frac{P(A,B,C)}{P(B,C)} \\ &= \frac{P(A,B|C)P(C)}{P(B|C)P(C)} \\ &= \frac{P(A|C)P(B|C)}{P(B|C)} \\ &= P(A|C) \end{align}</script><ul><li><code>주의할 점은 조건부 독립과 무조건부 독립은 관계가 없다는 점</code>이다. 즉, 두 확률변수가 독립이라고 항상 조건부 독립이 되는 것도 아니고 조건부 독립이라고 꼭 독립이 되는 것도 아니다.</li></ul><script type="math/tex; mode=display">P(A,B) = P(A)P(B) \;\; \nRightarrow \;\; P(A,B|C) = P(A|C)P(B|C)</script><script type="math/tex; mode=display">P(A,B|C) = P(A|C)P(B|C) \;\; \nRightarrow \;\; P(A,B) = P(A)P(B)</script><h4 id="베이즈-정리-Bayes’-rule"><a href="#베이즈-정리-Bayes’-rule" class="headerlink" title="베이즈 정리(Bayes’ rule)"></a>베이즈 정리(Bayes’ rule)</h4><ul><li>베이즈 정리는 사건 B가 발생함으로써 사건 A의 확률이 어떻게 변화하는지를 표현한 정리이다. 따라서 <code>베이즈 정리는 새로운 정보가 기존의 추론에 어떻게 영향을 미치는지를 나타낸다.</code></li></ul><script type="math/tex; mode=display">P(A|B) = \frac{P(B|A) P(A)}{P(B)}</script><ul><li>$P(A|B)$ : 사후확률(Posterior). 사건 B가 발생한 후 갱신된 사건 A의 확률</li><li>$P(A)$ : 사전확률(Prior). 사건 B가 발생하기 전에 가지고 있던 사건 A의 확률</li><li>$P(B|A)$ : 가능도(likelihood). 사건 A가 발생한 경우 사건 B의 확률</li><li>$P(B)$ : 정규화 상수(normalizing constant) 또는 증거(evidence). 확률의 크기 조정역할을 함.</li></ul><blockquote><p>이렇게 독립변수(Feature)들간에 서로 조건부독립임을 가정하여 조건을 나이브하게 하였고, 베이즈정리를 사용하여 MLE를 통해 가장 큰 확률값을 갖는 모수를 추정해내는 모형이 나이브 베이지안 모형이다.</p></blockquote><script type="math/tex; mode=display">\begin{align} P(y = k \mid x) &= \dfrac{ P(x_1, \ldots, x_D \mid y = k) P(y = k) }{P(x)} \\ &= \dfrac{ \left( \prod_{d=1}^D P(x_{d} \mid y = k) \right) P(y = k) }{P(x)} \end{align}</script><ul><li>제일 처음 예시로 들었던 날씨의 상태와 습도를 독립변수로 하고, 종속변수를 테니스를 치는지에 대한 여부에 대한 문제를 푼다고 가정해보자. 먼저 테니스를 치는 경우에 대한 확률을 계산할 경우 테니스를 치는 사건에 대한 확률을 높게 해주는 두가지 경우의 수를 아래와 같이 생각해 볼 수 있다. 아래 두 그림을 수식으로 정리해보면 다음과 같다는 사실을 쉽게 생각해 볼 수 있다. <code>수식의 마지막에서 분자부분에서 likelihood와 Prior 확률값을 살펴보면 그림에서 언급한 두 가지 경우의 사건들의 확률 값의 곱으로 되어있음</code>을 볼 수 있다. 즉, <code>사전확률에 새롭게 추가되는 likelihood가 추론의 어떤 영향을 미치는지를 나타내는 베이즈 정리의 의미를 생각해 볼 수 있다</code>.</li></ul><script type="math/tex; mode=display">\begin{align} P(weather state, Humidity | playing Tennis) \\ &= \frac{P(weather state)}{P(playing Tennis)} \frac{P(Humidity)}{P(playing Tennis)} \\ &= \frac{P(playing Tennis|weather state)P(weather state)}{P(playing Tennis)} \frac{P(playing Tennis|Humidity)P(Humidity)}{P(playing Tennis)} \end{align}</script><p><img src="/image/naive_bayesian_model_example_01.png" alt="나이브 베이지안 모델의 모수 추정 - 01"></p><p><img src="/image/naive_bayesian_model_example_02.png" alt="나이브 베이지안 모델의 모수 추정 - 02"></p><ul><li>아래 그림과 같이 데이터가 주어졌을 때의 베이즈 정리를 이용해 계산하면 다음과 같다.</li></ul><p><img src="/image/Naive_bayesian_probability.png" alt="베이즈 정리 확률 예시"></p><ul><li>예시로 계산해 본 것과 같이 아래의 그림처럼 독립변수들이 조건부 독립이라는 조건을 통해 곱으로 표현 될 수 있다. 예측값은 마지막 수식에서 보는 것과 같이 해당 가능도함수의 확률값을 최대로 해주는 class를 선택하여 예측값으로 출력해준다.</li></ul><p><a href="https://datascienceschool.net/view-notebook/864a2cc43df44531be32e3fa48769501/" target="_blank" rel="noopener">참조 : MLE 개념</a></p><p><img src="/image/Naive_bayesian_model_estimaion_01.png" alt="나이브 베이지안 모형 모수 추정식"></p><ul><li>이제 하나씩 앞에서 언급했던 순서대로 계산하여 최종적으로 출력을 내는 단계까지 그림을 통해 살펴볼 것이다.</li></ul><p><img src="/image/Naive_bayesian_model_estimaion_02.png" alt="나이브 베이지안 모형 계산 - 01"></p><p><img src="/image/Naive_bayesian_model_estimaion_03.png" alt="나이브 베이지안 모형 계산 - 02"></p><p><img src="/image/Naive_bayesian_model_estimaion_04.png" alt="나이브 베이지안 모형 계산 - 03"></p><p><img src="/image/Naive_bayesian_model_estimaion_05.png" alt="나이브 베이지안 모형 계산 - 04"></p><p><img src="/image/Naive_bayesian_model_estimaion_06.png" alt="나이브 베이지안 모형 계산 - 05"></p><h3 id="나이브-베이지안-모형의-종류"><a href="#나이브-베이지안-모형의-종류" class="headerlink" title="나이브 베이지안 모형의 종류"></a>나이브 베이지안 모형의 종류</h3><p><img src="/image/Naive_bayesian_classifier_type.png" alt="나이브 베이지안 모형의 종류"></p><h4 id="나이브-가정"><a href="#나이브-가정" class="headerlink" title="나이브 가정"></a>나이브 가정</h4><ul><li>독립변수 $x$가 $D$차원이라고 가정하자.</li></ul><script type="math/tex; mode=display">x = (x_1, \ldots, x_D)</script><ul><li>가능도 함수는 $x_{1}, \ldots, x_{D}$의 결합확률이 된다.</li></ul><script type="math/tex; mode=display">P(x \mid y = k) = P(x_1, \ldots, x_D \mid y = k)</script><ul><li>원리상으로는 $y=k$인 데이터만 모아서 이 가능도함수의 모양을 추정할 수 있다. 하지만 차원 $D$가 커지면 가능도함수의 추정이 현실적으로 어려워진다. 따라서 나이브베이즈 분류모형(Naive Bayes classification model)에서는 모든 차원의 개별 독립변수가 서로 조건부 독립(conditional independent)이라는 가정을 사용한다. 이러한 가정을 나이브 가정(naive assumption)이라고 한다.</li></ul><ul><li>나이브 가정으로 사용하면 벡터 $x$의 결합확률분포함수는 개별 스칼라 원소 $x_{d}$의 확률분포함수의 곱이 된다.</li></ul><script type="math/tex; mode=display">P(x_1, \ldots, x_D \mid y = k) = \prod_{d=1}^D P(x_d \mid y = k)</script><ul><li>스칼라 원소 $x_{d}$의 확률분포함수는 결합확률분포함수보다 추정하기 훨씬 쉽다. 가능도함수를 추정한 후에는 베이즈 정리를 사용하며 조건부확률을 계산할 수 있다.</li></ul><script type="math/tex; mode=display">\begin{align} P(y = k \mid x) &= \dfrac{ P(x_1, \ldots, x_D \mid y = k) P(y = k) }{P(x)} \\ &= \dfrac{ \left(\prod_{d=1}^D P(x_{d} \mid y = k) \right) P(y = k) }{P(x)} \end{align}</script><h4 id="가우시안-분포-정규분포-가능도-모형"><a href="#가우시안-분포-정규분포-가능도-모형" class="headerlink" title="가우시안 분포(정규분포) 가능도 모형"></a>가우시안 분포(정규분포) 가능도 모형</h4><ul><li>$x$벡터의 원소가 모두 실수이고 클래스마다 특정한 값 주변에서 발생한다고 하면 가능도 분포로 정규분포를 사용한다. 각 독립변수 $x_{d}$마다, 그리고 클래스 $k$마다 정규 분포의 기대값 $\mu_{d,k}$, 표준편차 $\sigma^{2}_{d,k}$가 달라진다. QDA모형과는 달리 모든 독립변수들이 서로 조건부독립이라고 가정한다.</li></ul><script type="math/tex; mode=display">P(x_d \mid y = k) = \dfrac{1}{\sqrt{2\pi\sigma_{d,k}^2}} \exp \left(-\dfrac{(x_d-\mu_{d,k})^2}{2\sigma_{d,k}^2}\right)</script><p><img src="/image/gaussian_dist_likelihood_model.png" alt="정규분포 가능도 추정 방법"></p><h4 id="베르누이-분포-가능도-모형"><a href="#베르누이-분포-가능도-모형" class="headerlink" title="베르누이 분포 가능도 모형"></a>베르누이 분포 가능도 모형</h4><ul><li><p>베이누이분포 가능도 모형에서는 각각의 $x = (x_1,\ldots, x_D)$의 각 원소 $x_{d}$가 0 또는 1이라는 값만을 가질 수 있다. 독립변수는 $D$개의 독립적인 베르누이 확률변수, 동전으로 구성된 동전 세트로 표현할 수 있다. 이 동전들의 모수 $\mu_{d}$는 동전 $d$마다 다르다.</p></li><li><p>그런데 class $y=k (k = 1,\ldots, K)$마다도 $x_{d}$가 1이 될 확률이 다르다. 즉, 동전의 모수 $\mu_{d,k}$는 동전 $d$마다 다르고 class $k$마다도 다르다. 즉, 전체 $ D \times K $의 조합의 동전이 존재하며 같은 class에 속하는 D개의 동전이 하나의 동전 세트를 구성하고 이러한 동전 세트가 $K$개 있다고 생각할 수 있다.</p></li></ul><script type="math/tex; mode=display">P(x_d \mid y = k) = \mu_{d,k}^{x_d} (1-\mu_{d,k})^{(1-x_d)}</script><script type="math/tex; mode=display">P(x_1, \ldots, x_D \mid y = k)= \prod_{d=1}^D \mu_{d,k}^{x_d} (1-\mu_{d,k})^{(1-x_d)}</script><ul><li>이러한 동전 세트마다 확률 특성이 다르므로 베르누이분포 가능도 모형을 기반으로 하는 나이브베이즈 모형은 동전 세트를 $N$번 던진 결과로부터 $1, \ldots, K$ 중 어느 동전 세트를 던졌는지를 찾아내는 모형이라고 할 수 있다.</li></ul><h4 id="다항분포-Multinomial-Distribution-가능도-모형"><a href="#다항분포-Multinomial-Distribution-가능도-모형" class="headerlink" title="다항분포(Multinomial Distribution) 가능도 모형"></a>다항분포(Multinomial Distribution) 가능도 모형</h4><ul><li>다항분포 모형에서는 $x$ 벡터가 다항분포의 표본이라고 가정한다. 즉, $D$개의 면을 가지는 주사위를 $\sum_{d=1}^D x_d$ 번 던져서 나온 결과로 본다. 예를 들어 $x$가 다음과 같다면, $x=(1, 4, 0, 5)$ 4면체 주사위를 10번 던져서 1인 면이 1번, 2인 면이 4번, 4인 면이 5번 나온 결과로 해석한다. 각 class 마다 주사위가 다르다고 가정하므로 $K$개의 class를 구분하는 문제에서는 $D$개의 면을 가진 주사위가 $K$개 있다고 본다.</li></ul><script type="math/tex; mode=display">P(x_1, \ldots, x_D \mid y = k) \;\; \propto \;\; \prod_{d=1}^D \mu_{d,k}^{x_{d,k}}</script><script type="math/tex; mode=display">\sum_{d=1}^{D} \mu_{d,k} = 1</script><ul><li>따라서 다항분포 가능도 모형을 기반으로 하는 나이브 베이즈 모형은 주사위를 던진 결과로부터 $1, \ldots, K$ 중 어느 주사위를 던졌는지를 찾아내는 모형이라고 할 수 있다.</li></ul><h2 id="Naive-Bayes-실습"><a href="#Naive-Bayes-실습" class="headerlink" title="Naive Bayes 실습"></a>Naive Bayes 실습</h2><h3 id="1-Gaussian-Naive-Bayes"><a href="#1-Gaussian-Naive-Bayes" class="headerlink" title="1. Gaussian Naive Bayes"></a>1. Gaussian Naive Bayes</h3><ul><li>데이터, 모듈 불러오기<ul><li>iris(붓꽃)데이터를 가지고 실습을 진행할 것이다.</li><li>붓꽃의 종류는 3가지(‘setosa’, ‘versicolor’, ‘virginica’)이며, 각 feature는 sepal length (cm),    sepal width (cm), petal length (cm), petal width (cm) 로 이루어져있다. 모든 피처가 실수의 값을 갖기 때문에 가우시안 나이브 베이즈 모형을 사용할 것이다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">df_X=pd.DataFrame(iris.data, columns=iris.feature_names)</span><br><span class="line">df_Y=pd.DataFrame(iris.target, columns=[<span class="string">"target"</span>])</span><br></pre></td></tr></table></figure><ul><li>예측의 성능은 항상 테스트 데이터를 통해 측정해야 하므로 분리해준다. 지금은 연습삼아 해보는 것이므로 필자는 따로 validation set을 구축하지 않겠다. 허나 실제로 데이터를 분석하고 그에 따른 성능을 측정하려면 꼭 validation set을 따로 두어 베이스라인 모델의 parameter들을 튜닝을 통해 모델의 성능을 높이도록 한 뒤 최종적으로 test set을 예측하여 성능을 검증해야 할 것이다.  </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, test_X, train_Y, test_Y = train_test_split(df_X, df_Y, train_size=0.8, test_size=0.2, random_state=123)</span><br></pre></td></tr></table></figure><ul><li>모델 피팅</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GNB = GaussianNB()</span><br><span class="line">fitted = GNB.fit(train_X, train_Y)</span><br><span class="line">y_pred = fitted.predict(test_X)</span><br><span class="line">y_pred</span><br></pre></td></tr></table></figure><ul><li>확률 구하기<ul><li>먼저 예측한 클래스와 해당 예측 데이터의 클래스별 확률을 살펴 볼 것이다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.predict(test_X)[:1]</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.predict_proba(test_X)[:1]</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[7.24143720e-126, 9.23061979e-001, 7.69380215e-002]])</span><br></pre></td></tr></table></figure><ul><li>위의 확률 직접 구해보기</li></ul><p>1) 위의 확률 값이 나오게 된 중간과정을 살펴보자. 우선 추정된 독립변수의 모수와 정규 분포의 확률밀도 함수를 사용하여 가능도를 구할 수 있다.</p><script type="math/tex; mode=display">p(x_1, x_2, x_3, x_4 | y) \propto p(x_1) p(x_2) p(x_3) p(x_4)</script><pre><code>- 위에서 학습했을 때도 말했던 것 처럼, (class개수 * 변수의 개수)개의 조합의 모수를 갖고 있으므로 아래와 같이 12개의 모수를 갖는다. 아래에서는 각 class별로 정규분포의 모수인 평균과 분산을 보여준다.</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict_data = np.array(test_X.iloc[0])</span><br><span class="line">predict_data</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([6.3, 2.5, 4.9, 1.5])</span><br></pre></td></tr></table></figure><ul><li>추정한 모델의 클래스별 모수(평균과 분산)을 다음과 같이 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.theta_[0], fitted.sigma_[0]</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([5.01621622, 3.43243243, 1.46756757, 0.25945946]),</span><br><span class="line"> array([0.10568298, 0.14975895, 0.02705625, 0.01214025]))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.theta_[1], fitted.sigma_[1]</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([5.95      , 2.78409091, 4.24090909, 1.32272727]),</span><br><span class="line"> array([0.27068182, 0.10042872, 0.22741736, 0.04221075]))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.theta_[2], fitted.sigma_[2]</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([6.58717949, 2.95897436, 5.57948718, 2.02820513]),</span><br><span class="line"> array([0.39752795, 0.11011177, 0.29188692, 0.0774096 ]))</span><br></pre></td></tr></table></figure><ul><li>위의 모수들을 통해 class별 가능도를 구하면 아래와 같을 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">likelihood = [</span><br><span class="line">(sp.stats.norm(fitted.theta_[0][0], np.sqrt(fitted.sigma_[0][0])).pdf(predict_data[0]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[0][1], np.sqrt(fitted.sigma_[0][1])).pdf(predict_data[1]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[0][2], np.sqrt(fitted.sigma_[0][2])).pdf(predict_data[2]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[0][3], np.sqrt(fitted.sigma_[0][3])).pdf(predict_data[3])),\</span><br><span class="line">(sp.stats.norm(fitted.theta_[1][0], np.sqrt(fitted.sigma_[1][0])).pdf(predict_data[0]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[1][1], np.sqrt(fitted.sigma_[1][1])).pdf(predict_data[1]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[1][2], np.sqrt(fitted.sigma_[1][2])).pdf(predict_data[2]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[1][3], np.sqrt(fitted.sigma_[1][3])).pdf(predict_data[3])),\</span><br><span class="line">(sp.stats.norm(fitted.theta_[2][0], np.sqrt(fitted.sigma_[0][0])).pdf(predict_data[0]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[2][1], np.sqrt(fitted.sigma_[0][1])).pdf(predict_data[1]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[2][2], np.sqrt(fitted.sigma_[0][2])).pdf(predict_data[2]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[2][3], np.sqrt(fitted.sigma_[0][3])).pdf(predict_data[3]))    </span><br><span class="line">]</span><br><span class="line">likelihood</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2.0700298536453225e-126, 0.2218869448618605, 7.497361843154609e-09]</span><br></pre></td></tr></table></figure><ul><li>여기에 사전확률을 곱하면 사후 확률에 비례하는 값이 나온다.</li></ul><script type="math/tex; mode=display">p(y|x_1, x_2) \propto p(x_1, x_2|y) p(y)</script><blockquote><p>아직 정규화 상수 $p(x)$로 나누어주지 않았으므로 두 값의 합이 1이 아니다. 즉, 확률이라고 부를 수는 없다. 하지만 크기를 비교하면 이 데이터는 $y=1$일 확률이 가장 높다는 것을 알 수 있다.</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.class_prior_</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.30833333, 0.36666667, 0.325     ])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">posterior = likelihood * fitted.class_prior_</span><br><span class="line">posterior</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([6.38259205e-127, 8.13585464e-002, 2.43664260e-009])</span><br></pre></td></tr></table></figure><ul><li>이 값을 정규화하면 predict_proba 메서드로 구한 것과 같은 값이 나온다. 물론 완벽히 일치하진 않지만 그에 근사하는 값을 추정값으로 계산해내서 얻을 수 있다. 이는 계산시 소수점을 어느정도까지 사용하였는지에 따라 다르기 때문에 이 정도의 오차는 문제가 없다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">posterior / np.sum(posterior, axis=0)</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([7.84501707e-126, 9.99999970e-001, 2.99494353e-008])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.predict_proba(test_X)[:1]</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[7.24143720e-126, 9.23061979e-001, 7.69380215e-002]])</span><br></pre></td></tr></table></figure><ul><li>Confusion matrix 구하기</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line">confusion_matrix(y_pred, test_Y)</span><br></pre></td></tr></table></figure><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[13,  0,  0],</span><br><span class="line">       [ 0,  6,  1],</span><br><span class="line">       [ 0,  0, 10]])</span><br></pre></td></tr></table></figure><ul><li>Prior 설정하기<ul><li>이번에는 class가 발생되는 사전확률을 미리 알고 있었던 경우라고 가정하고 문제를 풀어볼 것이다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GNB2 = GaussianNB(priors=[0.01, 0.01, 0.98])</span><br><span class="line">set_prior_fitted_01 = GNB2.fit(train_X, train_Y)</span><br><span class="line">set_prior_pred_01 = set_prior_fitted_01.predict(test_X)</span><br><span class="line">confusion_matrix(set_prior_pred_01, test_Y)</span><br></pre></td></tr></table></figure><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[13,  0,  0],</span><br><span class="line">       [ 0,  4,  0],</span><br><span class="line">       [ 0,  2, 11]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GNB3 = GaussianNB(priors=[0.01, 0.98, 0.01])</span><br><span class="line">set_prior_fitted_02 = GNB3.fit(train_X, train_Y)</span><br><span class="line">set_prior_pred_02 = set_prior_fitted_02.predict(test_X)</span><br><span class="line">confusion_matrix(set_prior_pred_02, test_Y)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[13,  0,  0],</span><br><span class="line">       [ 0,  6,  4],</span><br><span class="line">       [ 0,  0,  7]])</span><br></pre></td></tr></table></figure><h3 id="2-Bernoulli-naive-bayes"><a href="#2-Bernoulli-naive-bayes" class="headerlink" title="2. Bernoulli naive bayes"></a>2. Bernoulli naive bayes</h3><ul><li>e-mail과 같은 문서 내에 특정한 단어가 포함되어 있는지의 여부는 베르누이 확률변수로 모형화할 수 있다. 이렇게 <code>독립변수가 0 또는 1의 값을 가지면 베르누이 나이브베이즈 모형을 사용</code>한다.</li></ul><ul><li><p>python의 sklearn의 베르누이분포 나이브베이즈 모형 클래스 <code>BernoulliNB</code>는 가능도 추정과 관련하여 다음 속성을 가진다.</p><ul><li><code>feature_count_</code> : 각 class k에 대해 d번째 동전이 앞면이 나온 횟수 $N_{d,k}$</li><li><code>feature_log_prob_</code> : 베르누이분포 모수의 로그값</li></ul></li></ul><script type="math/tex; mode=display">\log \mu_k = (\log \mu_{1,k}, \ldots, \log \mu_{D, k}) = \left( \log \dfrac{N_{1,k}}{N_k}, \ldots, \log \dfrac{N_{D,k}}{N_k} \right)</script><ul><li>여기에서 $N_{k}$은 클래스 k에 대해 동전을 던진 총 횟수이다. <code>표본 데이터의 수가 적은 경우에는 모수에 대해 다음처럼 스무딩(smoothing)을 할 수도 있다.</code></li></ul><h4 id="스무딩-Smoothing"><a href="#스무딩-Smoothing" class="headerlink" title="스무딩(Smoothing)"></a>스무딩(Smoothing)</h4><ul><li>표본 데이터의 수가 적은 경우에는 베르누이 모수가 0 또는 1이라는 극단적인 모수 추정값이 나올 수도 있다. 하지만 현실적으로는 실제 모수값이 이런 극단적인 값이 나올 가능성이 적다. <code>따라서 베르누이 모수가 0.5인 가장 일반적인 경우를 가정하여 0이 나오는 경우와 1이 나오는 경우, 두 개의 가상 표본 데이터를 추가</code>한다. 그러면 0이나 1과 같은 극단적인 추정값이 0.5에 가까운 다음과 같은 값으로 변한다. 이를 <code>라플라스 스무딩(Laplace smoothing)</code> 또는 <code>애드원(Add-One) 스무딩</code>이라고 한다.</li></ul><script type="math/tex; mode=display">\hat{\mu}_{d,k} = \frac{ N_{d,k} + \alpha}{N_k + 2 \alpha}</script><ul><li><p>가중치 $\alpha$를 사용하여 스무딩의 정도를 조절할 수도 있다. 가중치 $\alpha$는 정수가 아니라도 괜찮다. 가중치가 1인 경우는 무정보 사전확률을 사용한 베이즈 모수추정의 결과와 같다.</p></li><li><p>아래의 데이터는 4개의 key word를 사용하여 정상 메일 4개와 spam 메일 6개를 BOW 인코딩한 행렬이다. 예를 들어 첫번째 메일은 정상 메일이고 1번, 4번 key word는 포함하지 않지만 2번,3번 key word를 포함한다고 볼 수 있다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.naive_bayes import BernoulliNB</span><br><span class="line">X = np.array([</span><br><span class="line">    [0, 1, 1, 0],</span><br><span class="line">    [1, 1, 1, 1],</span><br><span class="line">    [1, 1, 1, 0],</span><br><span class="line">    [0, 1, 0, 0],</span><br><span class="line">    [0, 0, 0, 1],</span><br><span class="line">    [0, 1, 1, 0],</span><br><span class="line">    [0, 1, 1, 1],</span><br><span class="line">    [1, 0, 1, 0],</span><br><span class="line">    [1, 0, 1, 1],</span><br><span class="line">    [0, 1, 1, 0]])</span><br><span class="line">y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])</span><br><span class="line"></span><br><span class="line">BNB = BernoulliNB()</span><br><span class="line">fitted = BNB.fit(X, y)</span><br></pre></td></tr></table></figure><ul><li>y 클래스의 종류와 각 클래스에 속하는 표본의 수, 그리고 그 값으로부터 구한 사전 확률의 값은 다음과 같다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.classes_</span><br></pre></td></tr></table></figure><h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0, 1])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.class_count_</span><br></pre></td></tr></table></figure><h5 id="결과-14"><a href="#결과-14" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([4., 6.])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.exp(fitted.class_log_prior_)</span><br></pre></td></tr></table></figure><h5 id="결과-15"><a href="#결과-15" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.4, 0.6])</span><br></pre></td></tr></table></figure><ul><li>각 클래스 k 별로, 그리고 각 독립변수 d 별로, 각각 다른 베르누이 확률변수라고 가정하여 모두 8개의 베르누이 확률변수의 모수를 구하면 다음과 같다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 행은 클래스의 개수, 열은 변수의 개수를 의미</span></span><br><span class="line">count = fitted.feature_count_</span><br><span class="line">count</span><br></pre></td></tr></table></figure><h5 id="결과-16"><a href="#결과-16" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[2., 4., 3., 1.],</span><br><span class="line">       [2., 3., 5., 3.]])</span><br></pre></td></tr></table></figure><ul><li>위의 각 변수의 클래스별로 몇번 나온지에 대한 행렬에 각 클래스의 전체 개수로 나누어 주어야 해당 변수들의 모수인 p값을 알 수 있으므로 class_count의 배열의 모양을 변형시켜주어야 한다. 현재는 1차원의 벡터(2,)이므로 2차원의 (2,1)의 모양을 갖도록 해주어야 나누어 줄 수 있기 때문이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count / fitted.class_count_[:,np.newaxis]</span><br></pre></td></tr></table></figure><h5 id="결과-17"><a href="#결과-17" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.5       , 1.        , 0.75      , 0.25      ],</span><br><span class="line">       [0.33333333, 0.5       , 0.83333333, 0.5       ]])</span><br></pre></td></tr></table></figure><ul><li>위에서는 필자는 Numpy의 brodcasting 연산을 사용하여 구한것인데, 혹시 count 행렬의 모양과 동일하게 만들어 확실하게 연산하고 싶다면, 아래와 같이 실행하면 동일한 결과를 얻을 수 있는 것을 볼 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count / np.repeat(fitted.class_count_[:,np.newaxis], 4, axis=1)</span><br></pre></td></tr></table></figure><h5 id="결과-18"><a href="#결과-18" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.5       , 1.        , 0.75      , 0.25      ],</span><br><span class="line">       [0.33333333, 0.5       , 0.83333333, 0.5       ]])</span><br></pre></td></tr></table></figure><ul><li>그런데 이 값은 모형 내에서 구한 값과 다르다. 모형 내에서 스무딩(smoothing)이 이루어지기 때문이다. 스무딩은 동전의 각 면 즉, 0과 1이 나오는 가상의 데이터를 추가함으로서 추정한 모수의 값이 좀 더 0.5에 가까워지도록 하는 방법이다. 이 때 사용한 스무딩 가중치 값은 다음처럼 확인할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.alpha</span><br></pre></td></tr></table></figure><h5 id="결과-19"><a href="#결과-19" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta = np.exp(fitted.feature_log_prob_)</span><br><span class="line">theta</span><br></pre></td></tr></table></figure><h5 id="결과-20"><a href="#결과-20" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.5       , 0.83333333, 0.66666667, 0.33333333],</span><br><span class="line">       [0.375     , 0.5       , 0.75      , 0.5       ]])</span><br></pre></td></tr></table></figure><ul><li>이에 모형이 완성되었으니 테스트 데이터를 사용하여 예측을 해 본다. 예를 들어 1번, 2번 키워드를 포함한 메일이 정상 메일인지 스팸 메일인지 알아보자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = np.array([0, 1, 1, 1])</span><br><span class="line">fitted.predict_proba([x_new])</span><br></pre></td></tr></table></figure><h5 id="결과-21"><a href="#결과-21" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.34501348, 0.65498652]])</span><br></pre></td></tr></table></figure><ul><li>위 결과에서 정상 메일일 가능성이 약 3배임을 알 수 있다. 이 값은 다음처럼 구할 수도 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># np.prod(axis=1)로 해준 이유는 아래 각 변수별로 독립이므로 가능도를 구하려면 곱을 해주어야 하기 때문</span></span><br><span class="line">p = ((theta ** x_new) * (1 - theta) ** (1 - x_new)).prod(axis=1) * np.exp(fitted.class_log_prior_)</span><br><span class="line">p / p.sum()</span><br></pre></td></tr></table></figure><h5 id="결과-22"><a href="#결과-22" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.34501348, 0.65498652])</span><br></pre></td></tr></table></figure><ul><li>반대로 3번, 4번 keyword가 포함된 메일은 스팸일 가능성이 약 90%이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = np.array([0, 0, 1, 1])</span><br><span class="line">fitted.predict_proba([x_new])</span><br></pre></td></tr></table></figure><h5 id="결과-23"><a href="#결과-23" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.09530901, 0.90469099]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = ((theta ** x_new) * (1 - theta) ** (1 - x_new)).prod(axis=1) * np.exp(fitted.class_log_prior_)</span><br><span class="line">p / p.sum()</span><br></pre></td></tr></table></figure><h5 id="결과-24"><a href="#결과-24" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.09530901, 0.90469099]])</span><br></pre></td></tr></table></figure><ul><li>MNIST 숫자 분류문제에서 sklearn.preprocessing.Binarizer로 x값을 0, 1로 바꾼다(값이 8 이상이면 1, 8 미만이면 0). 즉 흰색과 검은색 픽셀로만 구성된 이미지로 만든다(다음 코드 참조)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">digits = load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line">X = Binarizer(7).fit_transform(X)</span><br></pre></td></tr></table></figure><ul><li><p>이 이미지에 대해 베르누이 나이브베이즈 모형을 적용하자. 분류 결과를 분류보고서 형식으로 나타내라.</p><ul><li><p>(1) BernoulliNB 클래스의 binarize 인수를 사용하여 같은 문제를 풀어본다.</p></li><li><p>(2) 계산된 모형의 모수 벡터 값을 각 클래스별로 8x8 이미지의 형태로 나타내라. 이 이미지는 무엇을 뜻하는가?</p></li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">digits.images[0]</span><br></pre></td></tr></table></figure><h5 id="결과-25"><a href="#결과-25" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],</span><br><span class="line">       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],</span><br><span class="line">       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],</span><br><span class="line">       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],</span><br><span class="line">       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],</span><br><span class="line">       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],</span><br><span class="line">       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape</span><br></pre></td></tr></table></figure><h5 id="결과-26"><a href="#결과-26" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(1797, 64)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BNB =  BernoulliNB()</span><br><span class="line">fitted = BNB.fit(X, y)</span><br><span class="line">theta = np.exp(fitted.feature_log_prob_)</span><br><span class="line">theta = theta.reshape((10, 8, 8))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(2, 5, figsize=(12, 3),</span><br><span class="line">                         subplot_kw=&#123;<span class="string">'xticks'</span>: [], <span class="string">'yticks'</span>: []&#125;)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(5):</span><br><span class="line">    axes[0][i].set_title(<span class="string">"class &#123;&#125;"</span>.format(i))</span><br><span class="line">    axes[0][i].imshow(theta[i], interpolation=<span class="string">'nearest'</span>, cmap=plt.cm.Blues)</span><br><span class="line">    axes[1][i].set_title(<span class="string">"class &#123;&#125;"</span>.format(i+5))</span><br><span class="line">    axes[1][i].imshow(theta[i+5], interpolation=<span class="string">'nearest'</span>, cmap=plt.cm.Blues)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/parameter_vector_of_MNIST_visualization.png" alt="MNIST 베르누이 나이브 베이즈 모형 모수벡터 시각화"></p><ul><li>위의 이미지에서는 모수값이 높은 변수가 진한 파란색을 띄게 된다. sklearn.preprocessing.Binarizer를 통해 x값을 값이 8 이상이면 1, 8 미만이면 0으로 바꾸어주었으므로 <code>8 미만인 데이터보다는 8이상인 데이터가 각 클래스를 구분하는데 좀 더 영향을 주는 공간을 알 수 있게 해준다.</code></li></ul><h3 id="3-Multinomial-naive-bayes"><a href="#3-Multinomial-naive-bayes" class="headerlink" title="3. Multinomial naive bayes"></a>3. Multinomial naive bayes</h3><ul><li>다항분포 나이브베이즈 모형 클래스 <code>MultinomialNB</code>는 가능도 추정과 관련하여 다음 속성을 가진다.<ul><li><code>feature_count_</code> : 각 클래스 $k$에서 $d$번째 면이 나온 횟수 $N_{d,k}$</li><li><code>feature_log_prob_</code> : 다항분포의 모수의 로그</li></ul></li></ul><script type="math/tex; mode=display">\log \mu_k = (\log \mu_{1,k}, \ldots, \log \mu_{D, k}) = \left( \log \dfrac{N_{1,k}}{N_k}, \ldots, \log \dfrac{N_{D,k}}{N_k} \right)</script><ul><li>여기에서 $N_{k}$은 클래스 $k$에 대해 주사위를 던진 총 회수를 뜻한다.</li></ul><ul><li>스무딩공식은 아래와 같다.</li></ul><script type="math/tex; mode=display">\hat{\mu}_{d,k} = \frac{ N_{d,k} + \alpha}{N_k + D \alpha} , \; (D=변수의 개수)</script><ul><li>이번에도 스팸 메일 필터링을 예로 들어보다. 다만 BOW 인코딩을 할 때, 각 키워드가 출현한 빈도를 직접 입력 변수로 사용한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([</span><br><span class="line">    [3, 4, 1, 2],</span><br><span class="line">    [3, 5, 1, 1],</span><br><span class="line">    [3, 3, 0, 4],</span><br><span class="line">    [3, 4, 1, 2],</span><br><span class="line">    [1, 2, 1, 4],</span><br><span class="line">    [0, 0, 5, 3],</span><br><span class="line">    [1, 2, 4, 1],</span><br><span class="line">    [1, 1, 4, 2],</span><br><span class="line">    [0, 1, 2, 5],</span><br><span class="line">    [2, 1, 2, 3]])</span><br><span class="line">y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">MNB = MultinomialNB()</span><br><span class="line">fitted = MNB.fit(X, y)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.classes_</span><br></pre></td></tr></table></figure><h5 id="결과-27"><a href="#결과-27" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0, 1])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.class_count_</span><br></pre></td></tr></table></figure><h5 id="결과-28"><a href="#결과-28" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([4., 6.])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.exp(fitted.class_log_prior_)</span><br></pre></td></tr></table></figure><h5 id="결과-29"><a href="#결과-29" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.4, 0.6])</span><br></pre></td></tr></table></figure><ul><li>다음으로 각 클래스에 대한 가능도 확률분포를 구한다. 다항분포 모형을 사용하므로 각 클래스를 4개의 면을 가진 주사위로 생각할 수 있다. 그리고 각 면이 나올 확률은 각 면이 나온 횟수를 주사위를 던진 전체 횟수로 나누면 된다. 우선 각 클래스 별로 각각의 면이 나온 횟수는 다음과 같다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">count = fitted.feature_count_</span><br><span class="line">count</span><br></pre></td></tr></table></figure><h5 id="결과-30"><a href="#결과-30" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[12., 16.,  3.,  9.],</span><br><span class="line">       [ 5.,  7., 18., 18.]])</span><br></pre></td></tr></table></figure><ul><li>이 데이터에서 클래스 $Y=0$인 주사위를 던진 횟수는 첫번째 행의 값의 합인 40이므로 클래스 $Y=0$인 주사위를 던져 1이라는 면이 나올 확률은 다음처럼 계산할 수 있다.</li></ul><script type="math/tex; mode=display">\mu_{1,Y=0} = \dfrac{12}{40} = 0.3</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count / np.repeat(count.sum(axis=1)[:,np.newaxis], 4, axis=1)</span><br></pre></td></tr></table></figure><h5 id="결과-31"><a href="#결과-31" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.3       , 0.4       , 0.075     , 0.225     ],</span><br><span class="line">       [0.10416667, 0.14583333, 0.375     , 0.375     ]])</span><br></pre></td></tr></table></figure><ul><li>실제로는 극단적인 추정을 피하기 위해 이 값을 가중치 1인 스무딩을 한 추정값을 사용한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.alpha</span><br></pre></td></tr></table></figure><h5 id="결과-32"><a href="#결과-32" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(count + fitted.alpha) / \</span><br><span class="line">(np.repeat(count.sum(axis=1)[:,np.newaxis], 4, axis=1) + fitted.alpha * X.shape[1])</span><br></pre></td></tr></table></figure><h5 id="결과-33"><a href="#결과-33" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.29545455, 0.38636364, 0.09090909, 0.22727273],</span><br><span class="line">       [0.11538462, 0.15384615, 0.36538462, 0.36538462]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta = np.exp(fitted.feature_log_prob_)</span><br><span class="line">theta</span><br></pre></td></tr></table></figure><h5 id="결과-34"><a href="#결과-34" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.29545455, 0.38636364, 0.09090909, 0.22727273],</span><br><span class="line">       [0.11538462, 0.15384615, 0.36538462, 0.36538462]])</span><br></pre></td></tr></table></figure><ul><li>이제 이 값을 사용하여 예측을 해 보자. 만약 어떤 메일에 1번부터 4번까지의 키워드가 각각 10번씩 나왔다면 다음처럼 확률을 구할 수 있다. 구해진 확률로부터 이 메일이 스팸임을 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = np.array([10, 10, 10, 10])</span><br><span class="line">fitted.predict_proba([x_new])</span><br></pre></td></tr></table></figure><h5 id="결과-35"><a href="#결과-35" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.38848858, 0.61151142]])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = (theta ** x_new).prod(axis=1) * np.exp(fitted.class_log_prior_)</span><br><span class="line">p / p.sum(axis=0)</span><br></pre></td></tr></table></figure><h5 id="결과-36"><a href="#결과-36" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.38848858, 0.61151142])</span><br></pre></td></tr></table></figure><ul><li>MNIST 숫자 분류문제를 다항분포 나이브베이즈 모형을 사용하여 풀고 이진화(Binarizing)를 하여 베르누이 나이브베이즈 모형을 적용했을 경우와 성능을 비교하라.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line">digits = load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.3, random_state=123)</span><br><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line">binary_train_X = Binarizer(7).fit_transform(train_X)</span><br><span class="line">binary_test_X = Binarizer(7).fit_transform(test_X)</span><br><span class="line">BNB = BernoulliNB().fit(binary_train_X, train_Y)</span><br><span class="line">bnb_pred = BNB.predict(binary_test_X)</span><br><span class="line">MNB = MultinomialNB().fit(train_X, train_Y)</span><br><span class="line">mnb_pred = MNB.predict(test_X)</span><br></pre></td></tr></table></figure><h5 id="이진화-한-베르누이-나이브베이즈-모형-성능"><a href="#이진화-한-베르누이-나이브베이즈-모형-성능" class="headerlink" title="이진화 한 베르누이 나이브베이즈 모형 성능"></a>이진화 한 베르누이 나이브베이즈 모형 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(bnb_pred, test_Y))</span><br></pre></td></tr></table></figure><h5 id="결과-37"><a href="#결과-37" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">            precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.98      0.98      0.98        59</span><br><span class="line">           1       0.82      0.84      0.83        55</span><br><span class="line">           2       0.87      0.90      0.88        51</span><br><span class="line">           3       0.80      0.93      0.86        40</span><br><span class="line">           4       0.95      0.97      0.96        60</span><br><span class="line">           5       0.84      0.94      0.89        51</span><br><span class="line">           6       0.96      1.00      0.98        55</span><br><span class="line">           7       1.00      0.89      0.94        56</span><br><span class="line">           8       0.85      0.80      0.83        51</span><br><span class="line">           9       0.87      0.74      0.80        62</span><br><span class="line"></span><br><span class="line">    accuracy                           0.90       540</span><br><span class="line">   macro avg       0.90      0.90      0.90       540</span><br><span class="line">weighted avg       0.90      0.90      0.90       540</span><br></pre></td></tr></table></figure><h5 id="다항-분포-나이브-베이즈-모형-성능"><a href="#다항-분포-나이브-베이즈-모형-성능" class="headerlink" title="다항 분포 나이브 베이즈 모형 성능"></a>다항 분포 나이브 베이즈 모형 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(mnb_pred, test_Y))</span><br></pre></td></tr></table></figure><h5 id="결과-38"><a href="#결과-38" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.98      1.00      0.99        58</span><br><span class="line">           1       0.77      0.88      0.82        49</span><br><span class="line">           2       0.83      0.88      0.85        50</span><br><span class="line">           3       0.89      1.00      0.94        41</span><br><span class="line">           4       0.97      0.92      0.94        64</span><br><span class="line">           5       0.74      1.00      0.85        42</span><br><span class="line">           6       0.98      0.98      0.98        57</span><br><span class="line">           7       1.00      0.89      0.94        56</span><br><span class="line">           8       0.81      0.74      0.77        53</span><br><span class="line">           9       0.89      0.67      0.76        70</span><br><span class="line"></span><br><span class="line">    accuracy                           0.89       540</span><br><span class="line">   macro avg       0.89      0.90      0.89       540</span><br><span class="line">weighted avg       0.89      0.89      0.89       540</span><br></pre></td></tr></table></figure><ul><li><p>텍스트 분석에서 TF-IDF 인코딩을 하면 단어의 빈도수가 정수가 아닌 실수값이 된다. 이런 경우에도 다항분포 모형을 적용할 수 있는가?</p><ul><li>정수가 아니더라도 해당 적용 가능하다! <code>다항분포의 모수를 추정할 때 해당 관측치의 변수가 갖는 값의 합으로 나누어주어 모수값을 구했는데, TF-idf 행렬은 row가 문서를 의미하고 전체 문서에서의 토큰들이 열을 이루게 되므로 해당 문서에서 어떠한 단어가 몇번 나온것인지에 대해 다항분포를 통해 계산할 수 있기 때문</code>이다.</li></ul></li><li><p><code>MultinomialNB를 사용할 경우 범주형으로 정수이면 사용하는 것이라고 생각하지말고 위의 예시 처럼 데이터 당 각 피처가 유기적으로 하나의 사건에서 파생되어 이루어질 수 있는지에 대해서 먼저 생각해보자. 통계적인 분포를 다항분포로 생각할 수 있는지를 확인해보자는 이야기</code>이다.</p></li><li><p>아래의 뉴스그룹 분류 문제를 통해 검증해보자.</p></li></ul><h4 id="뉴스그룹-분류"><a href="#뉴스그룹-분류" class="headerlink" title="뉴스그룹 분류"></a>뉴스그룹 분류</h4><ul><li>다음은 뉴스그룹 데이터에 대해 나이브베이즈 분류모형을 적용한 결과이다.<ul><li>문서는18846건</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_20newsgroups</span><br><span class="line"></span><br><span class="line">news = fetch_20newsgroups(subset=<span class="string">"all"</span>)</span><br><span class="line">X = news.data</span><br><span class="line">y = news.target</span><br><span class="line"></span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">model1 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, CountVectorizer()),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br><span class="line">model2 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, TfidfVectorizer()),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br><span class="line">model3 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, TfidfVectorizer(stop_words=<span class="string">"english"</span>)),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br><span class="line">model4 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, TfidfVectorizer(stop_words=<span class="string">"english"</span>,</span><br><span class="line">                             token_pattern=r<span class="string">"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b"</span>)),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">from sklearn.model_selection import cross_val_score, KFold</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, model <span class="keyword">in</span> enumerate([model1, model2, model3, model4]):</span><br><span class="line">    scores = cross_val_score(model, X, y, cv=5)</span><br><span class="line">    <span class="built_in">print</span>((<span class="string">"Model&#123;0:d&#125;: Mean score: &#123;1:.3f&#125;"</span>).format(i + 1, np.mean(scores)))</span><br></pre></td></tr></table></figure><h5 id="결과-39"><a href="#결과-39" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Model1: Mean score: 0.855</span><br><span class="line">Model2: Mean score: 0.856</span><br><span class="line">Model3: Mean score: 0.883</span><br><span class="line">Model4: Mean score: 0.888</span><br><span class="line">CPU <span class="built_in">times</span>: user 1min 35s, sys: 4.54 s, total: 1min 40s</span><br><span class="line">Wall time: 1min 53s</span><br></pre></td></tr></table></figure><ul><li>(1) 만약 독립변수로 실수 변수, 0 또는 1 값을 가지는 변수, 자연수 값을 가지는 변수가 섞여있다면 사이킷런에서 제공하는 나이브베이즈 클래스를 사용하여 풀 수 있는가?</li></ul><pre><code>- 위에서 likelihood를 직접 계산했던 것과 같이 `likelihood만을 각각 계산하여 각 변수들은 독립이라는 가정을 전제로하기 때문에 서로 곱한뒤에 베이즈정리식에 따라 최종적으로 확률값을 구해 클래스를 구분`할 수 있다.</code></pre><ul><li>(2) 사이킷런에서 제공하는 분류문제 예제 중 숲의 수종을 예측하는 covtype 분류문제는 연속확률분포 특징과 베르누이확률분포 특징이 섞여있다. 이 문제를 사이킷런에서 제공하는 나이브베이즈 클래스를 사용하여 풀어라.</li></ul><h3 id="대표-수종-데이터-covtype"><a href="#대표-수종-데이터-covtype" class="headerlink" title="대표 수종 데이터(covtype)"></a>대표 수종 데이터(covtype)</h3><ul><li>대표 수종 데이터는 미국 삼림을 30×30m 영역으로 나누어 각 영역의 특징으로부터 대표적인 나무의 종류(species of tree)을 예측하기위한 데이터이다. 수종은 7종류이지만 특징 데이터가 54종류, 표본 데이터의 갯수가 581,012개에 달하는 대규모 데이터이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_covtype</span><br><span class="line">covtype = fetch_covtype()</span><br><span class="line"><span class="comment"># print(covtype.DESCR)</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(covtype.data,</span><br><span class="line">                  columns=[<span class="string">"x&#123;:02d&#125;"</span>.format(i + 1) <span class="keyword">for</span> i <span class="keyword">in</span> range(covtype.data.shape[1])],</span><br><span class="line">                  dtype=int)</span><br><span class="line">sy = pd.Series(covtype.target, dtype=<span class="string">"category"</span>)</span><br><span class="line">df[<span class="string">'covtype'</span>] = sy</span><br></pre></td></tr></table></figure><ul><li>각 특징 데이터가 가지는 값의 종류를 보면 1번부터 10번 특징은 실수값이고 11번부터 54번 특징은 이진 카테고리값이라는 것을 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.iloc[:, 10:54] = df.iloc[:, 10:54].astype(<span class="string">'category'</span>)</span><br></pre></td></tr></table></figure><ul><li>다음 플롯은 카테고리값에 따라 “x14” 특징의 값이 어떻게 변하는지 나타낸 것이다. “x14” 특징이 0인가 1인가를 사용하면 1, 5, 7번 클래스와 4번 클래스는 완벽하게 분류할 수 있다는 것을 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(10,12))</span><br><span class="line">df_count = df.pivot_table(index=<span class="string">"covtype"</span>, columns=<span class="string">"x14"</span>, aggfunc=<span class="string">"size"</span>)</span><br><span class="line">sns.heatmap(df_count, cmap=sns.light_palette(<span class="string">"gray"</span>, as_cmap=True), annot=True, fmt=<span class="string">"0"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/categorical_variable_of_tree_binary.png" alt="x14 피처로 분류할 수 있는 클래스"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Gaussian_df_X = df.iloc[:, :10]</span><br><span class="line">Bern_df_X = df.iloc[:, 10:-1]</span><br><span class="line">df_Y = df.iloc[:, -1]</span><br><span class="line"></span><br><span class="line">GNB = GaussianNB()</span><br><span class="line">fitted_GNB = GNB.fit(Gaussian_df_X, df_Y)</span><br><span class="line"></span><br><span class="line">theta = fitted_GNB.theta_</span><br><span class="line">sigma = fitted_GNB.sigma_</span><br></pre></td></tr></table></figure><ul><li>가능도를 계산하기 위한 함수를 작성하였다.<ul><li>아래 함수는 반복문을 통해 실행하는 방식인데 데이터 수가 많다면 너무 비효율적이다. 그러므로, 데이터(관측치)의 수가 적은 경우에만 이용하는 것을 권장한다.</li><li>그래서 또 직접적으로 for문을 돌리지 않고 사용할 수 있는 방식의 함수를 다시 구현하였다. for문으로 반복문을 작성한것과 비교했을 때는 직관적으로 어떻게 가능도를 구하는 지 알 수 있다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def Gaussian_likelihood_cal(predict_data, theta, sigma, class_count, feature_count):</span><br><span class="line">    likelihood = []</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> np.arange(class_count):</span><br><span class="line">        prod = 1</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> np.arange(feature_count):</span><br><span class="line">            prod = prod * sp.stats.norm(theta[c][f], np.sqrt(sigma[c][f])).pdf(predict_data[f])</span><br><span class="line">        likelihood.append(prod)</span><br><span class="line">    <span class="built_in">return</span> likelihood</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">def Gaussian_likelihood_cal(predict_data, theta, sigma):</span><br><span class="line">    likelihood = [(sp.stats.norm(theta[0][0], np.sqrt(sigma[0][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][1], np.sqrt(sigma[0][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][2], np.sqrt(sigma[0][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][3], np.sqrt(sigma[0][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][4], np.sqrt(sigma[0][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][5], np.sqrt(sigma[0][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][6], np.sqrt(sigma[0][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][7], np.sqrt(sigma[0][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][8], np.sqrt(sigma[0][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][9], np.sqrt(sigma[0][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[1][0], np.sqrt(sigma[1][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][1], np.sqrt(sigma[1][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][2], np.sqrt(sigma[1][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][3], np.sqrt(sigma[1][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][4], np.sqrt(sigma[1][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][5], np.sqrt(sigma[1][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][6], np.sqrt(sigma[1][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][7], np.sqrt(sigma[1][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][8], np.sqrt(sigma[1][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][9], np.sqrt(sigma[1][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[2][0], np.sqrt(sigma[2][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][1], np.sqrt(sigma[2][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][2], np.sqrt(sigma[2][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][3], np.sqrt(sigma[2][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][4], np.sqrt(sigma[2][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][5], np.sqrt(sigma[2][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][6], np.sqrt(sigma[2][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][7], np.sqrt(sigma[2][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][8], np.sqrt(sigma[2][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][9], np.sqrt(sigma[2][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[3][0], np.sqrt(sigma[3][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][1], np.sqrt(sigma[3][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][2], np.sqrt(sigma[3][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][3], np.sqrt(sigma[3][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][4], np.sqrt(sigma[3][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][5], np.sqrt(sigma[3][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][6], np.sqrt(sigma[3][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][7], np.sqrt(sigma[3][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][8], np.sqrt(sigma[3][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][9], np.sqrt(sigma[3][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[4][0], np.sqrt(sigma[4][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][1], np.sqrt(sigma[4][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][2], np.sqrt(sigma[4][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][3], np.sqrt(sigma[4][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][4], np.sqrt(sigma[4][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][5], np.sqrt(sigma[4][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][6], np.sqrt(sigma[4][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][7], np.sqrt(sigma[4][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][8], np.sqrt(sigma[4][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][9], np.sqrt(sigma[4][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[5][0], np.sqrt(sigma[5][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][1], np.sqrt(sigma[5][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][2], np.sqrt(sigma[5][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][3], np.sqrt(sigma[5][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][4], np.sqrt(sigma[5][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][5], np.sqrt(sigma[5][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][6], np.sqrt(sigma[5][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][7], np.sqrt(sigma[5][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][8], np.sqrt(sigma[5][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][9], np.sqrt(sigma[5][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[6][0], np.sqrt(sigma[6][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][1], np.sqrt(sigma[6][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][2], np.sqrt(sigma[6][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][3], np.sqrt(sigma[6][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][4], np.sqrt(sigma[6][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][5], np.sqrt(sigma[6][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][6], np.sqrt(sigma[6][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][7], np.sqrt(sigma[6][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][8], np.sqrt(sigma[6][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][9], np.sqrt(sigma[6][9])).pdf(predict_data[9]))]</span><br><span class="line">    <span class="built_in">return</span> likelihood</span><br></pre></td></tr></table></figure><ul><li>위의 함수를 사용하여 10개의 실수 변수들에 대한 모수를 계산하여 가능도를 구하는 반복문을 작성하였다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">total_num = np.array(Gaussian_df_X).shape[0]</span><br><span class="line">gaussian_likelihood_matrix = []</span><br><span class="line">percentage = 0</span><br><span class="line"><span class="keyword">for</span> num, predict_data <span class="keyword">in</span> enumerate(np.array(Gaussian_df_X)):</span><br><span class="line">    <span class="keyword">if</span> (percentage != int(num / total_num * 100)) and (int(num / total_num * 100) <span class="keyword">in</span> list(np.arange(10,101,10))):</span><br><span class="line">        percentage = int(num / total_num * 100)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"완성도 &#123;&#125; %"</span>.format(percentage))</span><br><span class="line">    likelihood = Gaussian_likelihood_cal(predict_data, theta, sigma)</span><br><span class="line">    gaussian_likelihood_matrix.append(likelihood)</span><br></pre></td></tr></table></figure><ul><li>위에서 가우시안 나이브 베이즈 모형의 가능도를 구했으므로 이젠 베르누이 나이브 베이즈 모형의 가능도를 구할 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BNB = BernoulliNB()</span><br><span class="line">fitted_BNB = BNB.fit(Bern_df_X, df_Y)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta = np.exp(fitted_BNB.feature_log_prob_)</span><br><span class="line">theta.shape</span><br></pre></td></tr></table></figure><ul><li>상대적으로 가우시안 나이브 베이즈 모형의 가능도를 계산하는 것보단 단순 연산으로 이루어져 있어 속도가 훨씬 빠르다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">bern_likelihood_matrix = []</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> np.array(Bern_df_X):</span><br><span class="line">    bern_likelihood_matrix.append(list(((theta ** data) * (1 - theta) ** (1 - data)).prod(axis=1)))</span><br></pre></td></tr></table></figure><h5 id="결과-40"><a href="#결과-40" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CPU <span class="built_in">times</span>: user 49.2 s, sys: 1.28 s, total: 50.5 s</span><br><span class="line">Wall time: 50.4 s</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">likelihood = np.array(gaussian_likelihood_matrix) * np.array(bern_likelihood_matrix)</span><br><span class="line">posterior = likelihood * np.exp(BNB.class_log_prior_)</span><br><span class="line">prob = posterior / np.repeat(posterior.sum(axis=1)[:, np.newaxis], 7, axis=1)</span><br><span class="line">result = np.argmax(prob, axis=1)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result_bern = fitted_BNB.predict(Bern_df_X)</span><br><span class="line">result_gaussian = fitted_GNB.predict(Gaussian_df_X)</span><br></pre></td></tr></table></figure><h5 id="가우시안-나이브-베이즈-모형의-성능"><a href="#가우시안-나이브-베이즈-모형의-성능" class="headerlink" title="가우시안 나이브 베이즈 모형의 성능"></a>가우시안 나이브 베이즈 모형의 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(result_gaussian, df_Y))</span><br></pre></td></tr></table></figure><h5 id="결과-41"><a href="#결과-41" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           1       0.67      0.63      0.65    225973</span><br><span class="line">           2       0.66      0.73      0.69    255934</span><br><span class="line">           3       0.65      0.50      0.56     46785</span><br><span class="line">           4       0.47      0.41      0.44      3099</span><br><span class="line">           5       0.22      0.18      0.20     11425</span><br><span class="line">           6       0.31      0.33      0.32     16424</span><br><span class="line">           7       0.28      0.27      0.28     21372</span><br><span class="line"></span><br><span class="line">    accuracy                           0.63    581012</span><br><span class="line">   macro avg       0.47      0.44      0.45    581012</span><br><span class="line">weighted avg       0.63      0.63      0.63    581012</span><br></pre></td></tr></table></figure><h5 id="베르누이-나이브-베이즈-모형의-성능"><a href="#베르누이-나이브-베이즈-모형의-성능" class="headerlink" title="베르누이 나이브 베이즈 모형의 성능"></a>베르누이 나이브 베이즈 모형의 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(result_bern, df_Y))</span><br></pre></td></tr></table></figure><h5 id="결과-42"><a href="#결과-42" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           1       0.67      0.63      0.65    225973</span><br><span class="line">           2       0.66      0.73      0.69    255934</span><br><span class="line">           3       0.65      0.50      0.56     46785</span><br><span class="line">           4       0.47      0.41      0.44      3099</span><br><span class="line">           5       0.22      0.18      0.20     11425</span><br><span class="line">           6       0.31      0.33      0.32     16424</span><br><span class="line">           7       0.28      0.27      0.28     21372</span><br><span class="line"></span><br><span class="line">    accuracy                           0.63    581012</span><br><span class="line">   macro avg       0.47      0.44      0.45    581012</span><br><span class="line">weighted avg       0.63      0.63      0.63    581012</span><br></pre></td></tr></table></figure><h5 id="가우시안-나이브-베이즈-모형과-베르누이-나이브-베이즈-모형의-가능도를-곱해-확률을-계산한-모형의-성능"><a href="#가우시안-나이브-베이즈-모형과-베르누이-나이브-베이즈-모형의-가능도를-곱해-확률을-계산한-모형의-성능" class="headerlink" title="가우시안 나이브 베이즈 모형과 베르누이 나이브 베이즈 모형의 가능도를 곱해 확률을 계산한 모형의 성능"></a>가우시안 나이브 베이즈 모형과 베르누이 나이브 베이즈 모형의 가능도를 곱해 확률을 계산한 모형의 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(result, df_Y))</span><br></pre></td></tr></table></figure><h5 id="결과-43"><a href="#결과-43" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           1       0.03      0.25      0.06     26481</span><br><span class="line">           2       0.93      0.48      0.63    554531</span><br><span class="line">           3       0.00      0.00      0.00         0</span><br><span class="line">           4       0.00      0.00      0.00         0</span><br><span class="line">           5       0.00      0.00      0.00         0</span><br><span class="line">           6       0.00      0.00      0.00         0</span><br><span class="line">           7       0.00      0.00      0.00         0</span><br><span class="line"></span><br><span class="line">    accuracy                           0.47    581012</span><br><span class="line">   macro avg       0.14      0.10      0.10    581012</span><br><span class="line">weighted avg       0.89      0.47      0.60    581012</span><br></pre></td></tr></table></figure><ul><li>위의 성능 보면 3가지 모형 다 성능이 좋지 않다는 것을 확인 할 수 있다. 이는 적절한 피처의 선택이 이루어지지 않은 모형이기 때문일 것이며, 또한 아래 그림에서와 같이 클래스간의 비율차이가 극심하게 차이가 나는데, 특히 1,2 클래스가 대다수를 이루고 있기 때문에 1, 2클래스에 대한 학습이 많이 된 결과라고 해석 할 수 있을 것이다. 이는 마지막 두 나이브 베이즈 모형의 성능을 보아도 확인 할 수 있다. 마지막 모형의 성능은 다른 클래스로 예측한 데이터는 존재하지 않고 오로지 1과 2로 예측을 했다.</li></ul><p><img src="/image/class_dist_sovya.png" alt="클래스별 분포"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/#disqus_thread</comments>
    </item>
    
    <item>
      <title>PCA를 이해하기 위한 기본적 선형대수</title>
      <link>https://heung-bae-lee.github.io/2020/04/03/machine_learning_06/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/03/machine_learning_06/</guid>
      <pubDate>Fri, 03 Apr 2020 06:40:52 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;차원의-저주&quot;&gt;&lt;a href=&quot;#차원의-저주&quot; class=&quot;headerlink&quot; title=&quot;차원의 저주&quot;&gt;&lt;/a&gt;차원의 저주&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;먼저 PCA를 하는 이유에 대해 설명해 볼 것이다. 전에 언급했던 변수(또는 피처)들이
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="차원의-저주"><a href="#차원의-저주" class="headerlink" title="차원의 저주"></a>차원의 저주</h1><ul><li>먼저 PCA를 하는 이유에 대해 설명해 볼 것이다. 전에 언급했던 변수(또는 피처)들이 많아질수록 변수들이 있는 공간의 차원수 또한 점차적으로 늘어나게 된다. <code>차원의 저주</code>는<code>차원이 늘어남에 따라서 같은 영역의 자료를 갖고 있음에도 전체 영역 대비 모델을 통해 변수로 설명할 수 있는 데이터의 패턴은 줄어들게 되는 것</code>을 의미한다.</li></ul><p><img src="/image/curse_of_dimensionality_01.png" alt="차원의 저주 - 01"></p><ul><li>다른 관점에서 한번 살펴보면, 관측치의 수가 한정되어 있다고 할 경우에 먼저, 1차원 상의 공간(변수가 1개인 경우)에서는 10개의 데이터만 있어도 해당 차원의 절반을 설명할 수 있기에 공간을 설명함에 있어서 부족함이 없다고 판단할 수 있다. 허나 2차원상의 공간으로 살펴보았을 경우 1차원의 경우보다 관측치 간의 간격이 멀게 있어 사이에 빈 공간이 많이 나타남을 알 수 있다. 이렇게 빈 공간이 나타남은 사실상 우리가 모델을 통해 예측은 가능하지만 모형에 따라 변동이 심해지는 어디까지나 예측하는 값을 의미한다. 3차원 상에서는 2차원 공간상에서 보다 훨씬 더 관측치가 떨어져있음을 확인할 수 있다. 그러므로 모델의 변동성 즉, 모델의 분산이 더 커지게 되는 것이다. 간단히 말하자면, 점과 점 사이의 공간이 차원이 늘어날수록 멀어지는데, 빈 공간에 위치한 예측값을 채워넣기 위해서는 임의로 채워넣기 때문에 모델이 어려워진다는 것이다.  </li></ul><p><img src="/image/curse_of_dimensionality_02.png" alt="차원의 저주 - 02"></p><ul><li>예를 들어, 아래와 같이 KNN 알고리즘(가장 가까이 있는 변수가 예측값으로 결과를 갖는 알고리즘)을 통한 비지도학습을 진행한다고 가정해보자. x1 변수의 공간만을 생각하여 1차원적으로 생각하면, 아래 그림에서 검정색 점과 가장 가까운 점은 노란색 테두리안에 있는 2개의 점들 중 하나일 것이다. 허나 x2 변수의 공간도 같이 고려하여 2차원적으로 살펴보면, 파란색 원 안에 놓여있는 1개의 점이 가장 가까운 점이 될 것이다. 여기서 주목해야 할 점은 <code>동일한 데이터를 통해서 2차원으로 살펴보면 최단거리의 길이가 늘어났다는 점</code>이다. <code>KNN 알고리즘의 결과로 1차원으로 살펴본 것 보다 2차원으로 살펴보는 것이 최단거리의 길이가 더 길기 때문에 값이 좀 더 이질적이라고 볼 수 있으며, 결과적으로 과연 2차원적으로 예측한 값을 KNN알고리즘의 결과값으로 사용해도 문제가 없을지에 의문점이 생기게 된다.</code> x2가 Y와 관련된 변수라면 크게 문제가 없겠지만, x2가 전혀 관련없는 변수라면 굉장히 모델이 낭비가되며, 좋지 않은 모델이 될 것이기 때문이다.</li></ul><ul><li>위에서 언급했던 것처럼 차원이 증가하게 되면 좋지 않은 측면이 있고, 이것을 차원의 저주라고 부르며 <code>차원을 될 수 있는 한 축소하는 것이 필요</code>하다.</li></ul><p><img src="/image/necessity_for_reduction_of_dimensionality.png" alt="차원축소의 필요성"></p><ul><li>기본적으로 차원을 축소하는 방법으로 아래와 같은 것들을 예로 들 수 있다. 먼저, 상관계수가 높은 변수들이라면 동일한 움직임의 방향을 갖기 때문에 그들 중 하나의 변수만을 선택해도 큰 문제가 없다고 보는 측면이다. 이 방법에서의 문제점은 예를 들어 두 변수의 상관계수가 0.8이라면, 나머지 0.2에 해당하는 정보는 버려지게 되는 점이다. 이런 부분을 보완한 것이 PCA이다. <code>PCA는 차원을 줄이면서도 정보의 손실을 최소화</code>하는 것이다.</li></ul><p><img src="/image/kinds_of_reduction_of_dimensonality.png" alt="차원축소법의 종류"></p><h1 id="공분산-행렬의-이해"><a href="#공분산-행렬의-이해" class="headerlink" title="공분산 행렬의 이해"></a>공분산 행렬의 이해</h1><ul><li>공분산 행렬은 쉽게 말해 행렬의 내적이라고 할 수 있다. 아래 그림에서 처럼 계산할 수 있다. 대각 요소를 제외한 나머지 비대각요소들은 두 변수간의 움직임의 방향을 알 수 있다. 그러므로 상관관계를 미리 알고 있다면 두 변수간의 공분산의 부호도 알 수 있다.</li></ul><p><img src="/image/covariance_matrix_concept_01.png" alt="공분산 행렬의 이해 - 01"></p><ul><li>공분산 행렬의 활용은 위에서 언급했던 것 처럼 두 변수간의 상관계수의 부호를 알 수 있는 것 말고도 존재한다. 아래 그림과 같이 특정 점들과의 내적연산을 시행하면, 점의 위치를 이동시키는데 이 때 이동된 점들의 분포가 공분산 구조와 비슷한 형태를 가지게 된다. <code>즉 공분산 행렬과 점(벡터 또는 행렬)들의 연산은 공분산 구조로 점들을 분포시키는 기능</code>을 한다.</li></ul><p><img src="/image/covariance_matrix_concept_02.png" alt="공분산 행렬의 이해 - 02"></p><ul><li>공분산 행렬이 대칭행렬이지만, positive definite가 아닌 행렬인 경우를 다음 그림에서 보여주고 있다. 아래 수식이 성립한다면 행렬 A가 positive definite하다고 한다.</li></ul><script type="math/tex; mode=display">x^{T} A  x > 0</script><p><a href="https://datascienceschool.net/view-notebook/d6205659aff0413797c22552947aec83/" target="_blank" rel="noopener">참고 - 행렬의 성질</a></p><p><img src="/image/covariance_matrix_concept_03.png" alt="공분산 행렬의 이해 - 03"></p><ul><li>아래의 공분산 행렬은 행렬식(determinant)가 0인 경우이다. 이렇게 행렬식이 0인 공분산행렬과의 내적은 오른쪽 그림과 같이 원점을 기준으로 일직선의 형태를 이루게 된다.</li></ul><p><img src="/image/covariance_matrix_concept_04.png" alt="공분산 행렬의 이해 - 04"></p><h1 id="Principal-Components의-이해"><a href="#Principal-Components의-이해" class="headerlink" title="Principal Components의 이해"></a>Principal Components의 이해</h1><ul><li>아래 그림을 살펴보면 x1과 x2는 음의 강한 상관관계를 갖는다는 것을 확인 할 수 있다. x1이 증가하면 x2가 감소하는 움직을 갖는다. 결론적으로는 정보가 중복되어있다. 이렇게 중복되어있는 정보를 요약해서 갖고있을수 있을 순 없을까에 대한 방법을 고안한 것이 바로 PCA이다. 아래 그림과 같이 두 변수간의 가장 큰 분산을 나타내는 벡터에 대해 각 데이터를 projection을 취한다면 해당 projection 된 데이터들의 벡터만을 가지고도 기존의 데이터에 대한 분산을 표현할 수 있게 된다.</li></ul><ul><li><code>차원을 줄이면서 정보의 손실을 최소화하는 방법은 자료의 변동을 가장 잘 설명하는 어떤 축을 찾는 것</code>이다.</li></ul><p><img src="/image/PCA_conception_01.png" alt="Principal Components의 개념 - 01"></p><ul><li>Principal Components를 찾아내는 방법을 간단히 말하자면 자료의 분산을 가장 많이 설명하는 축인 장축과 자료의 분산을 가장 적게 설명하는 단축을 찾아내는 것이다.  </li></ul><p><img src="/image/PCA_conception_03.png" alt="Principal Components의 개념 - 02"></p><p><img src="/image/PCA_conception_02.png" alt="Principal Components의 개념 - 03"></p><h1 id="PCA-수학적-개념이해-행렬연산-행렬식-특성방정식"><a href="#PCA-수학적-개념이해-행렬연산-행렬식-특성방정식" class="headerlink" title="PCA 수학적 개념이해 - 행렬연산, 행렬식, 특성방정식"></a>PCA 수학적 개념이해 - 행렬연산, 행렬식, 특성방정식</h1><p><img src="/image/find_determinant.png" alt="행렬식 구하는 방법"></p><ul><li><code>행렬식</code>(determinant)은 쉽게 말해 해당 <code>행렬이 갖는 Volume(2차원인 경우는 면적)</code>을 의미한다. 아래 그림과 같이 2차원 공간상에서 면적이 1을 갖는 좌표들(파란색)을 D라는 행렬로 명명했을 경우, A행렬로 선형변환시켜주면 아래 그림의 빨간색 직사각형으로 변환되며, 부피는 각 행렬식의 곱 형태로 계산 되어 5를 갖는다.</li></ul><ul><li><code>만약 행렬식이 0을 갖는다면</code> 행렬의 곱은 1직선을 이룰 것이고, 선분의 형태로 표현될 것이다.</li></ul><p><img src="/image/deteminant_of_algebra_aspection.png" alt="행렬식의 기하하적인 요소"></p><p><img src="/image/determinant_applications_and_relations.png" alt="행렬식의 활용"></p><h1 id="고유값과-고유벡터"><a href="#고유값과-고유벡터" class="headerlink" title="고유값과 고유벡터"></a>고유값과 고유벡터</h1><ul><li><code>임의의 정방행렬 A로 선형변환하더라도 방향은 유지되면 해당 벡터를 고유벡터라고 하며, 크기(늘어난 정도)를 고유값</code>이라고 한다.</li></ul><p><img src="/image/eigen_value_and_eigen_vector_01.png" alt="고유값과 고유벡터 - 01"></p><ul><li>기하학적으로 설명해보자면, 아래 그림과 같이 고유벡터는 빨간색 선을 제외한 나머지 선형변환을 해도 벡터의 방향이 바뀌지 않는 파란색, 분홍색 벡터들이 고유벡터를 의미한다. 그리고 고유값은 스케일(크기)를 의미한다.</li></ul><p><img src="/image/eigen_value_and_eigen_vector_02.png" alt="고유값과 고유벡터 - 02"></p><ul><li>아래 그림과 같은 식으로 고유값과 고유벡터를 구할 수 있다. 먼저 고유값을 구하는 방법은 $det(A - \lambda I)=0$을 만족하는 고유값을 찾으면 되는데, 이유는 우리가 구하고 싶은 고유값과 고유벡터는 0벡터를 제외한 다른 벡터를 구하고 싶기 때문에 행렬식이 0이아닌 다른 값을 갖는다면 역행렬이 존재하게 되어 0벡터가 해당 선형시스템의 유일한 해가 되지 않도록하려면 non-trivial solution(해가 무수히 많아야)을 갖어야 한다. 그러므로 해당 행렬식이 0의 값을 갖게되어 역행렬이 존재하지 않도록 해야하기 때문에 아래와 같은 수식을 통해 고유값을 구할 수 있다.</li></ul><ul><li>고유값은 크기를 기준으로 내림차순으로(큰 것부터 작은 것 순으로)적어주면, 그에따른 고유벡터도 동일한 인덱스를 매칭해준다. 여기서 주의할 점은 <code>고유벡터는 해당 고유벡터를 normalization을 통해 크기가 1인 벡터를 기준으로 스칼라배를 해도 동일한 고유벡터라고 여긴다는 점</code>이다.</li></ul><p><img src="/image/eigen_value_and_eigen_vector_03.png" alt="고유값과 고유벡터 - 03"></p><h1 id="PCA-수학적-개념이해-Singular-Value-Decomposition-SVD"><a href="#PCA-수학적-개념이해-Singular-Value-Decomposition-SVD" class="headerlink" title="PCA 수학적 개념이해 - Singular Value Decomposition(SVD)"></a>PCA 수학적 개념이해 - Singular Value Decomposition(SVD)</h1><ul><li>PCA는 다르게 행렬식을 계산하기 때문에 정방행렬인 경우에만 계산이 가능하다는 단점이 있다. 이러한 점을 보완하기 위한 Decomposition 방법이 바로 SVD(Singular Value Decomposition)이다.</li></ul><ul><li>SVD는 임의의 행렬 X에 관해 3개의 행렬로 분해가 가능하며, 각각의 행렬의 크기는 아래와 같고, V와 U는 column들이 Orthogonal(직교)한 행렬(서로 직교하는 성분들끼리의 내적은 0)이므로 동일행렬의 내적은 Identity matrix를 갖는다. V와 D가 각각 임의의행렬 X에 공분산행렬에 해당하는 고유벡터 행렬과 고유값 행렬을 의미한다.</li></ul><p><img src="/image/Singular_value_decomposition_01.png" alt="SVD 개념"></p><ul><li>공분산 행렬인 $X^{T}X$를 SVD 한 후, 아래 그림과 같이 eigen vector들의 행렬로 곱해주면 eigen vector들의 행렬과 eigen value들의 내적 값을 구할 수 있다.</li></ul><p><img src="/image/Singular_value_decomposition_02.png" alt="SVD와 eigen vector, eigen value와의 연과성"></p><h1 id="PCA-수행과정-및-수학적-개념-적용"><a href="#PCA-수행과정-및-수학적-개념-적용" class="headerlink" title="PCA 수행과정 및 수학적 개념 적용"></a>PCA 수행과정 및 수학적 개념 적용</h1><ul><li>제일 먼저, 각 데이터에 대해 standardization을 통해 표준정규분포를 따르도록 scaling을 해준다. 이런 normalization은 각 feature에 대한 공간의 범위가 다르다면 비교하는데 어려움이 있기 때문이다. 또한, 이 과정에서 feature 벡터의 크기를 1로 갖게끔 Unit vector로 만들어 주는 것이 추후에 SVD를 계산하는 과정에서 Orthogonal한 eigen vector가 Orthonormal vector가 되어 계산함에 있어서 편리하게 된다.</li></ul><p><img src="/image/PCA_simulation_action_01.png" alt="PCA 수행과정 - 01"></p><ul><li>그 후 SVD는 우선 우리는 프로그램을 사용하여 바로 구할 수 있다는 가정을 해보자. 여기서 <code>eigen vector같은 경우에는 공분산 구조나 공분산 행렬이나 동일한 값을 갖지만, eigen value는 다음 그림에서와 같이 관측치의 개수에서 하나를 빼주어야 한다.</code></li></ul><p><img src="/image/PCA_simulation_action_02.png" alt="PCA 수행과정 - 02"></p><ul><li>PC Score는 위에서와 같이 큰 순서대로 정렬된 eigen value 행렬 D와 앞에 U행렬값의 내적이 곧 행렬 X와 V의 내적을 하여 상수인 $\lambda V$를 만드는 과정과 동일하다.</li></ul><p><img src="/image/PCA_simulation_action_03.png" alt="PCA 수행과정 - 03"></p><ul><li>아래 그림과 같이 PC Score를 구해 중요도를 따져 상위 q개만을 통해 회귀분석을 진행할 수 있다.</li></ul><p><img src="/image/PCA_simulation_action_04.png" alt="PCA 수행과정 - 04"></p><h1 id="PCA의-심화적-이해"><a href="#PCA의-심화적-이해" class="headerlink" title="PCA의 심화적 이해"></a>PCA의 심화적 이해</h1><ul><li><code>한 마디로 PCA는 데이터의 변동을 가장 잘 설명하는 한 축을 찾아서 그 축에 대해 각 관측치들을 projection한 성분을 의미한다. 그 다음에는 이러한 해당 변동축에 직각이 되는 축에 관측치들을 projection하는 과정을 반복하여 진행하게 되는데 직각이 되는 축을 찾는 이유는 이전의 축과 동일한 정보를 갖지않는 새로운 피처를 뽑기 위함이라고 필자는 생각</code>한다.</li></ul><p><img src="/image/what_is_inner_product_of_PC_SCORE.png" alt="PC Score의 본질적인 의미"></p><ul><li>PCA의 <code>단점</code> 중 하나로는 <code>해당 성분이 의미하는 바를 직관적으로 설명할 수 없다</code>는 점이다. 또한 선형회귀에 사용되는 것과 같이 <code>해당 변수들이 비선형관계를 갖는다면 잡아낼 수 없다.</code></li></ul><p><img src="/image/bad_aspection_of_PCA.png" alt="PCA의 단점"></p><ul><li>위에서 언급한 첫번째 단점(직관적으로 PC성분을 통한 피처를 설명하기 어렵다.)을 조금이나마 보완할 수 있는 방법은 grid를 통해 해당하는 곳의 범위들의 데이터를 직접 뽑아 각 성분의 의미를 설명할 수 있다. 아래와 같이 이미지 데이터인 경우는 좀 더 구별하기 쉬울 것이다.</li></ul><p><img src="/image/Complement_the_shortcomings_of_PCA.png" alt="PCA의 단점을 보완하는 방법"></p><h1 id="Kernel-PCA"><a href="#Kernel-PCA" class="headerlink" title="Kernel PCA"></a>Kernel PCA</h1><ul><li>위에서 언급했던 두 번째 단점인 비선형관계를 갖는다면 잡아내기 힘들다는 점을 보완하기 위한 방법으로서, 다음 그림과 같은 상황에서 Kernel PCA를 사용할 것이다. 아래 관측치들의 집합인 행렬 X의 공분산 구조는 0에 가까운 값을 갖을 것이다. 즉, 서로 선형관계가 아닌 상황이다.</li></ul><p><img src="/image/when_we_use_kernel_PCA.png" alt="Kernel PCA를 하는 경우"></p><ul><li>기존의 PCA는 feature들 사이의 패턴이 존재하는 것으로 간주하여 비교(각 feature들간의 움직이는 방향을 나타내는 공분산 구조를 통해 값을 구했기 때문)를 했지만, 이번에는 관측치와 관측치 사이의 패턴을 수치화하여 PC를 구하는 방법이다. 아래 그림에서 K(Kernel matrix)의 예시를 보듯이 여러가지 방법이 존재한다.</li></ul><p><img src="/image/what_is_kernel_PCA.png" alt="Kernel PCA를 계산하는 방법"></p><ul><li>아래 그림과 같이 Kernel PCA를 계산할 수 있다.</li></ul><p><img src="/image/How_to_calculate_Kernel_matrix.png" alt="Kernel matrix를 계산하는 방법"></p><ul><li>아래 그림과 같이 원형의 띠로 구분지어져 있는 형상인데도 불구하구 Kernel PCA를 사용하면 잘 구분되는지에 대한 이유는 다음과 같다. 먼저 각 관측치에대해 pair-wise하게 Kernel matrix를 구해 비슷한 값들을 갖는 관측치들로 정렬하여 보면 비슷한 값을 갖는 관측치들 끼리 모여 block matrix를 형성하는 모습을 볼 수 있다.</li></ul><p><img src="/image/Kernel_matrix_shape_how.png" alt="Kernel matrix의 형태"></p><ul><li>Kernel matrix를 구한 값으로 eigen vector의 역할을 하는 행렬 U와 각 eigen value의 $d_{i}$의 곱인 PC Score를 통해 얻은 결과를 그래프로 그려보면 아래와 같은 결과를 얻을 수 있다. 아래의 경우에서는 첫번째 성분을 선택하여 사용하면 3가지 군집을 잘 판별할 수 있을 것이다.</li></ul><p><img src="/image/result_of_kernel_PCA.png" alt="Kernel matrix의 결과"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/03/machine_learning_06/#disqus_thread</comments>
    </item>
    
    <item>
      <title>로지스틱 회귀분석</title>
      <link>https://heung-bae-lee.github.io/2020/04/03/machine_learning_05/</link>
      <guid>https://heung-bae-lee.github.io/2020/04/03/machine_learning_05/</guid>
      <pubDate>Fri, 03 Apr 2020 04:58:10 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;로지스틱-회귀분석&quot;&gt;&lt;a href=&quot;#로지스틱-회귀분석&quot; class=&quot;headerlink&quot; title=&quot;로지스틱 회귀분석&quot;&gt;&lt;/a&gt;로지스틱 회귀분석&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/image/logistic_regression.png&quot; 
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="로지스틱-회귀분석"><a href="#로지스틱-회귀분석" class="headerlink" title="로지스틱 회귀분석"></a>로지스틱 회귀분석</h1><p><img src="/image/logistic_regression.png" alt="로지스틱 회귀"></p><p><img src="/image/logistic_regression_02.png" alt="로지스틱 함수를 사용하는 이유"></p><ul><li>위의 추정식에서 가장 오른쪽에 있는 로그 오즈비(log odds ratio)를 먼저 설명하자면, 로그 안에 들어간 오즈비(odds ratio)는 베르누이 시도에서 1이 나올 확률 $\theta (x)$와 0이 나올 확률 $1-\theta (x)$의 비율(ratio)을 의미한다.</li></ul><script type="math/tex; mode=display">odds ratio = \frac{\theta (x)}{1 - \theta (x)}</script><p>0부터 1사이의 값만 가지는 확률값인 $\theta (x)$를 승산비로 변환하면 0부터 양의 무한대까지의 값을 가질 수 있다. 승산비를 로그변환한 것이 위에서 언급한 Logit function이다.</p><script type="math/tex; mode=display">y = logit(odds ratio) = log\( \frac{theta (x)}{} \)</script><p>이로써 로지트 함수의 값은 로그변환에 의해 음의 무한대 $(- \infty)$부터 양의 무한대$(\infty)$까지의 값을 가질 수 있다. 로지스틱함수(Logistic function)은 로지트 함수의 역함수이다. 즉 <code>음의 무한대부터 양의 무한대 까지의 값을 가지는 입력변수를 0부터 1사이의 값을 가지는 출력변수로 변환한 것</code>이다.</p><script type="math/tex; mode=display">logistic(z) = \theta (z) = \frac{1}{1 + exp(-z)}</script><ul><li>또한, <code>로지스틱 회귀분석에서는 판별함수 수식으로 선형함수를 사용하므로 판별 경계면 또한 선형이 됨을 유의</code>해야한다.</li></ul><script type="math/tex; mode=display">z = w^{T} x</script><p><img src="/image/Logistic_function_01.png" alt="로지스틱 함수"></p><ul><li>위의 그림에서 알 수 있듯이, X의 범위가 [$-\infty$, $+\infty$]인 경우에 Y의 범위를 [0,1]로 만들어주는 함수이다.</li></ul><p><img src="/image/logistic_regression_example.png" alt="로지스틱 회귀 예제"></p><ul><li><p>위의 그림에서 좌측그림의 적합시킨 회귀선(파란선)을 보면 예측할 확률값이 500미만이면 음수를 갖을 수 있게 되는 것을 확인할 수 있다. 이는 확률값을 예측하는 모델을 만든다는 가정 자체에 위반하는 것이므로 이전에 학습했었던 회귀모형들과는 다른 방법의 회귀모형을 만들어 적합시켜야 할 것임이 분명해졌다.</p></li><li><p>일반적으로 decision boundary(threshold)를 0.5로 하지만 class가 imbalanced 한 경우는 조절하여보면서 적합시킬 수 있다.</p></li></ul><p><img src="/image/logistic_regression_coefficient_estimation_01.png" alt="로지스틱 회귀계수 추정방법 소개"></p><p><img src="/image/logistic_regression_coefficient_estimation_02.png" alt="로지스틱 회귀계수 MLE를 사용한 추정"></p><ul><li>위의 MLE 방식을 통해 parameter의 값을 업데이트하는데, 아래와 같은 방식으로 수치적 최적화를 진행한다. 위의 로그 가능도함수 $log(l(\beta_{0}, \beta_{1}))$을 편의상 LL이라하자. 로그가능도함수 LL을 최대화하는 것은 다음 목적함수를 최소화하는 것과 같다.</li></ul><script type="math/tex; mode=display">J = - LL</script><ul><li>SGD(Steepest Gradient Descent)방식을 사용(Stochastic Gradient Descent아님!!)하여 아래와 같은 그레디언 벡터를 구할 수 있다.</li></ul><script type="math/tex; mode=display">g_{k} = \frac{d}{dw} (-LL)</script><ul><li>위에서 구한 그레디언트 방향으로 step size $(n_{k})$만큼 이동한다.</li></ul><script type="math/tex; mode=display">w_{k+1} = w_{k} - n_{k} g_{k}</script><script type="math/tex; mode=display">= w_{k} + n_{k} \sum_{i=1}^{N} (y_{i} - \theta_{i} (x_{i} ; w)) x_{i}</script><p><img src="/image/multi_logistic_regression_example_01.png" alt="다중 로지스틱 회귀 예제 - 01"></p><ul><li>위의 그림에서 요약된 다중 로지스틱 회귀분석의 결과를 살펴보면, 맨 마지막 설명과 같이 해석할 수 있지만, 좀 더 자연스러운 해석하고 싶을 경우 아래 그림과 같이 Logit 보다 exp를 취해줘 Odds로 변환하여 해석하는 것을 권한다.</li></ul><p><img src="/image/multi_logistic_regression_example_02.png" alt="다중 로지스틱 회귀 예제 - 02"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/04/03/machine_learning_05/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Python - 00 (Python의 장점 및 자료형)</title>
      <link>https://heung-bae-lee.github.io/2020/03/20/Python_00/</link>
      <guid>https://heung-bae-lee.github.io/2020/03/20/Python_00/</guid>
      <pubDate>Fri, 20 Mar 2020 12:02:48 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Python&quot;&gt;&lt;a href=&quot;#Python&quot; class=&quot;headerlink&quot; title=&quot;Python&quot;&gt;&lt;/a&gt;Python&lt;/h2&gt;&lt;h2 id=&quot;왜-Python을-배워야-할까&quot;&gt;&lt;a href=&quot;#왜-Python을-배워야-할까&quot; cla
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h2><h2 id="왜-Python을-배워야-할까"><a href="#왜-Python을-배워야-할까" class="headerlink" title="왜 Python을 배워야 할까?"></a>왜 Python을 배워야 할까?</h2><ul><li>프로그래밍 언어를 배우고 싶은데 어떤 언어를 배우면 될까?<ul><li>C, C++, C#, Java, Javascript, Python, Ruby, C#, Go, Rust, Scala Perl, Obj-C, PHP, R, Julia 등</li><li>여러가지 언어가 존재하지만 가장 진입장벽이 낮다.</li></ul></li></ul><p><img src="/image/Python_good_point_for_develop.png" alt="Python의 장점"></p><ul><li>Python 언어의 장점<ul><li>문법이 간결</li><li>다양한 운영체제 지원</li><li>GUI Application 개발(PyQT)</li><li>범용 언어(네트워크, 웹, 데이터 분석, 머신러닝 등)</li><li>방대한 라이브러리 지원<ul><li>data science : Numpy, Pandas, Matplotlib, scikit-learn, statsmodels, tensorflow, keras, Pytorch</li><li>web : Django, Flask 등</li><li>crawling : scrapy, BeautifulSoap, requests 등</li></ul></li></ul></li></ul><h2 id="Python은-어떤-언어인가"><a href="#Python은-어떤-언어인가" class="headerlink" title="Python은 어떤 언어인가?"></a>Python은 어떤 언어인가?</h2><ul><li>Python is an interpreted high-level programming language for general-purpose programming.<a href="https://en.wikipedia.org/wiki/Python_(programming_language" target="_blank" rel="noopener">wikipedia</a>)</li></ul><ul><li><p>Python의 특징</p><ul><li>플랫폼에 독립적</li><li>인터프리터 언어</li><li>객체지향적</li><li>동적타이핑</li></ul></li><li><p>위의 용어에대해 잘 모르겠다면, 먼저 컴퓨터에 대해 알아보자.</p></li></ul><h2 id="컴퓨터를-이해해-보자"><a href="#컴퓨터를-이해해-보자" class="headerlink" title="컴퓨터를 이해해 보자!"></a>컴퓨터를 이해해 보자!</h2><ul><li><p>컴퓨터는 <code>계산을 수행하는 기계</code>이다.</p></li><li><p>컴퓨터를 구성하는 기본 요소</p><ul><li>CPU(Central Processing Unit)</li><li>RAM(Random Access Memory)</li><li>ROM(Read-Only Memory)</li></ul></li><li><p>OS(Operating System : 운영체제)</p><ul><li>Kernel<ul><li>하드웨어를 컨트롤하는 소프트웨어 (운영체제의 핵심)</li></ul></li><li>CPU, RAM, ROM 자원을 사용하는 방법을 정의</li></ul></li><li><p>App(Application : 어플리케이션)</p><ul><li>OS 기반 응용 프로그램</li><li>대부분의 프로그래밍의 영역</li></ul></li></ul><p><img src="/image/computer_structure_image.png" alt="컴퓨터 구조"></p><h2 id="컴퓨터에서-프로그램의-동작원리는"><a href="#컴퓨터에서-프로그램의-동작원리는" class="headerlink" title="컴퓨터에서 프로그램의 동작원리는?"></a>컴퓨터에서 프로그램의 동작원리는?</h2><ul><li><p>CPU, RAM, ROM은 0과 1밖에 모른다는 사실은 상식적으로 다들 알고 있을 것이다.</p></li><li><p>그런데, 프로그램 언어는 숫자와 알파벡과 특수기호를 사용한다. 이는 각 언어의 Compiler가 코드를 0과 1의 이진수로 변환하여 주는 컴파일러 언어이기 때문이다.</p></li><li><p><code>컴파일러 언어</code></p><ul><li>모든 코드를 컴파일링 후에 컴퓨터에서 처리 -&gt; <code>처리속도가 빠르지만 프로그램 실행을 위해 컴파일링 시간을 기다려야한다.</code></li></ul></li><li><p><code>인터프리터 언어</code></p><ul><li>한줄씩 코드를 컴파일링 하면서 컴퓨터에서 처리 -&gt; <code>처리속도가 느리지만 컴파일링 시간 없이 바로바로 프로그램을 실행한다.</code></li></ul></li><li><p>결론적으론, 코드를 잘 만들면 컴퓨터가 효율적으로 일할 수 있다는 것이다!!!</p></li></ul><p><img src="/image/relation_with_between_computer_and_human.png" alt="Computer와 프로그래밍 언어의 상호작용 이미지"></p><h2 id="Python은-어떤-언어인가-1"><a href="#Python은-어떤-언어인가-1" class="headerlink" title="Python은 어떤 언어인가?"></a>Python은 어떤 언어인가?</h2><ul><li><p><code>플랫폼에 독립적</code></p><ul><li>어떠한 종류의 OS에도 같은 문법을 사용할 수 있다.</li><li>즉, Window, Mac OS, Linux등 여러 OS에서 사용가능하다는 의미이다.</li></ul></li><li><p><code>인터프리터 언어</code></p><ul><li>한줄씩 컴파일링 하면서 코드를 수행한다.</li></ul></li><li><p><code>객체지향적</code></p><ul><li>실제 세계를 모델링하여 공통적인 기능을 묶어서 개발하는 방식</li><li><code>추상화(abstraction)</code>, <code>캡슐화(encapsulation)</code>, <code>상속(inheritance)</code>, <code>다형성(polymorphism)</code>의 특징을 갖는다.</li><li>참고로, 반대의 개념은 절차지향이다.</li></ul></li><li><p><code>동적타이핑</code></p><ul><li>변수 선언시 데이터 타입을 지정해 주지 않아도 데이터에 따라서 자동으로 타이핑된다.</li></ul></li></ul><h2 id="Python의-종류는"><a href="#Python의-종류는" class="headerlink" title="Python의 종류는?"></a>Python의 종류는?</h2><ul><li><p>Cpython</p><ul><li>C로 만들어진 파이썬</li><li>우리가 코딩하는 부분은 인터프리터이지만 안에서는 다 컴파일러언어인 C로 동작되어 속도가 빠르다. 그러므로 인터프리터 언어가 갖는 속도가 느리다는 점을 보완할 수 있어 데이터 분석에 용이하다.</li></ul></li><li><p>Jython</p><ul><li>Java로 만들어진 파이썬</li></ul></li><li><p>IronPython</p><ul><li>C#으로 만들어진 파이썬</li></ul></li><li><p>Pypy</p><ul><li>Python으로 만들어진 파이썬</li><li>Cpython 보다 빠르게 수행되는 것을 목표로 한다. <a href="http://pypy.org/" target="_blank" rel="noopener">http://pypy.org/</a></li></ul></li></ul><h2 id="프로그래밍을-한다는-것은"><a href="#프로그래밍을-한다는-것은" class="headerlink" title="프로그래밍을 한다는 것은?"></a>프로그래밍을 한다는 것은?</h2><ul><li><code>컴퓨터와의 효율적인 커뮤니케이션으로 자신이 생각하는 목적을 컴퓨터가 잘 처리할 수 있도록 시스템의 구조를 잘 설계하고 코드를 작성하는 것</code>이다.</li></ul><h2 id="1-PEP-Python-Enhance-Proposal"><a href="#1-PEP-Python-Enhance-Proposal" class="headerlink" title="1. PEP(Python Enhance Proposal)"></a>1. PEP(Python Enhance Proposal)</h2><ul><li>python을 향상시키기 위한 제안</li><li>Zen of Python : PEP20 : <a href="https://www.python.org/dev/peps/pep-0020/" target="_blank" rel="noopener">https://www.python.org/dev/peps/pep-0020/</a>   </li><li>Style Guide for Python Code : PEP8 : <a href="https://www.python.org/dev/peps/pep-0008/" target="_blank" rel="noopener">https://www.python.org/dev/peps/pep-0008/</a></li></ul><h2 id="Python-기본-핵십-이해하기"><a href="#Python-기본-핵십-이해하기" class="headerlink" title="Python 기본 핵십 이해하기"></a>Python 기본 핵십 이해하기</h2><h3 id="값-value-처리"><a href="#값-value-처리" class="headerlink" title="값(value) 처리"></a>값(value) 처리</h3><ul><li><p>Python은 실행되는 모든것을 객체로 관리하므로 객체를 일관성을 가지고 평가할수 있는 규칙을 도입했기 때문에 모든 것을 값(value)으로 처리한다. 값을 재사용을 하기 위해선 변수에 저장해야한다.</p></li><li><p>위의 설명이 아직 까진 잘 이해가 가지 않을 것이다.</p></li></ul><h4 id="literal-리터럴"><a href="#literal-리터럴" class="headerlink" title="literal(리터럴)"></a>literal(리터럴)</h4><ul><li>literal이란 <code>프로그래밍 언어로 작성된 코드에서 값을 대표하는 용어</code>이다. 예를 들면, Python에서는 기본으로 사용되는 정수, 부동 소수점 숫자, 문자열, Boolean 등의 객체로 평가되며 모든 것을 값으로 처리하고 이를 통해 결과를 출력한다.</li></ul><h5 id="정수형-리터럴값-정의"><a href="#정수형-리터럴값-정의" class="headerlink" title="정수형 리터럴값 정의"></a>정수형 리터럴값 정의</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><h5 id="실행결과"><a href="#실행결과" class="headerlink" title="실행결과"></a>실행결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><h4 id="Expression-표현식"><a href="#Expression-표현식" class="headerlink" title="Expression(표현식)"></a>Expression(표현식)</h4><ul><li>literal은 단순히 하나의 값을 처리한다. <code>여러 개의 값을 하나로 묶어서 처리하는 방법</code>을 표현식이라고 한다. 표현식은 프로그래밍 언어가 해석하는 하나 이상의 명시적 값, 상수, 변수, 연산자 조합이고 함수도 실행되면 하나의 값이 되어 이를 조합해도 표현식으로 인식한다. <code>표현식을 평가해서 실행될 때에는 우선순위 및 연관 규칙에 따라 해석하여 실행되며 평가된 결과는 하나의 값인 literal로 표현</code>된다. <code>표현식 처리순서는 좌측부터 우측으로 가며 평가하고 () 연산자를 최우선으로 처리</code>된다.</li></ul><h5 id="표현식-평가"><a href="#표현식-평가" class="headerlink" title="표현식 평가"></a>표현식 평가</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3 + 4</span><br></pre></td></tr></table></figure><h6 id="위의-표현식-결과"><a href="#위의-표현식-결과" class="headerlink" title="위의 표현식 결과"></a>위의 표현식 결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">7</span><br></pre></td></tr></table></figure><h5 id="표현식-평가-1"><a href="#표현식-평가-1" class="headerlink" title="표현식 평가"></a>표현식 평가</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"문자"</span> + <span class="string">"열"</span></span><br></pre></td></tr></table></figure><h6 id="위의-표현식-결과-1"><a href="#위의-표현식-결과-1" class="headerlink" title="위의 표현식 결과"></a>위의 표현식 결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"문자열"</span></span><br></pre></td></tr></table></figure><h4 id="condition-expression-조건식"><a href="#condition-expression-조건식" class="headerlink" title="condition expression(조건식)"></a>condition expression(조건식)</h4><ul><li><code>표현식에서 특정 조건문 등에 제한적으로 사용되는 것</code>을 조건식이라고 한다. 주로 특정 문장인 if문이나 while 문에서 처리된다. 처리된 결과 값은 Boolean 값을 갖는다.</li></ul><h5 id="조건식-평가"><a href="#조건식-평가" class="headerlink" title="조건식 평가"></a>조건식 평가</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bool(<span class="string">""</span>)</span><br></pre></td></tr></table></figure><h5 id="조건식-평가-결과"><a href="#조건식-평가-결과" class="headerlink" title="조건식 평가 결과"></a>조건식 평가 결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure><h3 id="이름-name-처리"><a href="#이름-name-처리" class="headerlink" title="이름(name) 처리"></a>이름(name) 처리</h3><ul><li><p>프로그램을 작성한다는 것은 값들을 저장하고, 다음에 필요할 경우 이를 읽어와서 계산하고 다시 저장해서 처리한다는 뜻이다. 프로그램에서 이런 값들의 변화를 저장해서 관리하는 기준이 필요하고, 파이썬에서는 이를 위해 Name space를 만들어서 관리한다. <code>이름으로 지정할 요소들은 변수(variable), 함수(function), 클래스(class), 모듈(module), 패키지(package)등 이다.</code> 이 중에 <code>변수를 빼면 다 객체로 사용되므로 값으로 관리되는 것을 알 수 있다.</code></p></li><li><p>파이썬은 변수, 함수, 클래스 등에 대한 이름을 구별하지 않으므로 변수, 함수, 클래스 등의 <code>명명 규칙을 명확히 해서 각각을 이름으로 식별할 수 있어야 한다.</code></p></li></ul><h4 id="예약어-keyword"><a href="#예약어-keyword" class="headerlink" title="예약어(keyword)"></a>예약어(keyword)</h4><ul><li><p>파이썬에서 변수, 함수, 클래스 등은 예약어와 동일한 이름으로 정의할 수 없고 <code>파이썬 내부 문법 규칙에서만 사용할 수 있도록 정의한 것</code>을 예약어라고 한다.</p></li><li><p>파이썬에서 모듈(module)은 프로그램을 관리하는 하나의 단위이고 이 내부에 변수, 함수, 클래스 등을 지정해서 관리한다. 파이썬 내부의 예약어를 보여주기 위해 하나의 모듈로 관리하며 그 모듈의 이름이 keyword이다.</p></li></ul><h5 id="Keyword-모듈-알아보기"><a href="#Keyword-모듈-알아보기" class="headerlink" title="Keyword 모듈 알아보기"></a>Keyword 모듈 알아보기</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from pprint import pprint</span><br><span class="line">import keyword</span><br><span class="line"></span><br><span class="line">pprint(keyword.kwlist, width=60, compact=True)</span><br></pre></td></tr></table></figure><h5 id="Keyword-모듈-리스트"><a href="#Keyword-모듈-리스트" class="headerlink" title="Keyword 모듈 리스트"></a>Keyword 모듈 리스트</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'False'</span>, <span class="string">'None'</span>, <span class="string">'True'</span>, <span class="string">'and'</span>, <span class="string">'as'</span>, <span class="string">'assert'</span>, <span class="string">'async'</span>,</span><br><span class="line"> <span class="string">'await'</span>, <span class="string">'break'</span>, <span class="string">'class'</span>, <span class="string">'continue'</span>, <span class="string">'def'</span>, <span class="string">'del'</span>,</span><br><span class="line"> <span class="string">'elif'</span>, <span class="string">'else'</span>, <span class="string">'except'</span>, <span class="string">'finally'</span>, <span class="string">'for'</span>, <span class="string">'from'</span>,</span><br><span class="line"> <span class="string">'global'</span>, <span class="string">'if'</span>, <span class="string">'import'</span>, <span class="string">'in'</span>, <span class="string">'is'</span>, <span class="string">'lambda'</span>, <span class="string">'nonlocal'</span>,</span><br><span class="line"> <span class="string">'not'</span>, <span class="string">'or'</span>, <span class="string">'pass'</span>, <span class="string">'raise'</span>, <span class="string">'return'</span>, <span class="string">'try'</span>, <span class="string">'while'</span>,</span><br><span class="line"> <span class="string">'with'</span>, <span class="string">'yield'</span>]</span><br></pre></td></tr></table></figure><h4 id="명명-규칙-naming-convention"><a href="#명명-규칙-naming-convention" class="headerlink" title="명명 규칙(naming convention)"></a>명명 규칙(naming convention)</h4><ul><li><p>파이썬은 하나의 프로그램을 작성하는 기준이 모듈이므로 <code>모듈 단위로 이름을 관리할 수 있는 하나의 영역이 존재</code>한다. 이렇게 모듈 단위로 관리하는 하나의 이름 관리 영역을 <code>전역 네임스페이스(global name space)</code>라고 한다.</p></li><li><p>변수와 함수 등이 동일한 이름으로 정의될 경우 최종적으로 할당된 값을 갖으므로 <code>최대한 명명 규칙을 준수해야 버전이 변경될 경우에도 일관성 있는 규칙을 준수할 수 있다.</code></p></li></ul><h5 id="식별자-Identifiers"><a href="#식별자-Identifiers" class="headerlink" title="식별자 : Identifiers"></a>식별자 : Identifiers</h5><p>변수, 함수, 클래스, 모듈 등을 구분하기 위해 사용되는 이름을 식별자라고 합니다. 이 식별자는 몇가지의 규칙이 있습니다.</p><ul><li>대소문자를 구분</li><li>소문자(a-z), 대문자(A-Z), 숫자(0-9), 언더스코어( _ ) 사용 가능</li><li>( _ )를 제외한 특수문자는 사용 불가</li><li>가장 앞에 ( __ ) 사용 지양 (reserved global variable)</li><li>가장앞에숫자사용불가</li><li>예약어사용불가</li><li>예약어<ul><li>Fasle, class, finally, is, return, None, continue, for, lambda, try, True, def, from, nonlocal, while, and, del, global, not, with, as, elif, if, or, yield, assert, else, import, pass, break, except, in, raise</li></ul></li></ul><h5 id="파이썬-권장-명명-규칙-모듈-패키지"><a href="#파이썬-권장-명명-규칙-모듈-패키지" class="headerlink" title="파이썬 권장 명명 규칙: 모듈, 패키지"></a>파이썬 권장 명명 규칙: 모듈, 패키지</h5><ul><li><p>모듈 이름은 짧아야 하고, 전부 소문자로 작성하는 것을 권장한다. 가독성을 위해 언더스코어( _ )를 쓸 수 있다.</p></li><li><p>패키지 이름 또한 짧아야 하고, 전부 소문자로 작성하지만, 언더스코어( _ )는 권장하지 않는다.</p></li></ul><h5 id="파이썬-권장-명명-규칙-클래스"><a href="#파이썬-권장-명명-규칙-클래스" class="headerlink" title="파이썬 권장 명명 규칙: 클래스"></a>파이썬 권장 명명 규칙: 클래스</h5><ul><li>클래스 이름은 Capitalized words 형식(단어를 대문자로 시작)을 따르며, <a href="https://dpdpwl.tistory.com/55" target="_blank" rel="noopener">카멜 표기법</a>도 사용이 가능하다.</li></ul><h5 id="파이썬-권장-명명-규칙-상수-변수-함수-메서드"><a href="#파이썬-권장-명명-규칙-상수-변수-함수-메서드" class="headerlink" title="파이썬 권장 명명 규칙: 상수, 변수, 함수, 메서드"></a>파이썬 권장 명명 규칙: 상수, 변수, 함수, 메서드</h5><ul><li>변수, 함수와 메서드의 이름은 원칙적으로 소문자여야하고, 가족성을 위해서 언더스코어( _ )단어로 쓰는 것을 권장한다.<ul><li>보호 속성일 때는 맨앞에 _ 를 추가적으로 붙인다.</li><li>키워드와 동일 변수일 때는 맨 뒤에 _ 를 추가적으로 붙인다.</li><li>비공개 속성일 때는 맨 앞에 __ 하나를 붙인다.</li><li>스페셜 속성일때는 앞과 뒤에 __ 하나씩 붙인다.</li></ul></li></ul><h4 id="Variable-변수"><a href="#Variable-변수" class="headerlink" title="Variable(변수)"></a>Variable(변수)</h4><ul><li><p>파이썬에서 변수는 다른 언어의 변수와 차이가 크다. 동적타이핑의 특성을 지니므로 변수에 특정한 자료형 등을 배정하지 않는다는 점이다. 변수는 단순히 이름만 지정하고 이 변수에 값을 할당해서 사용하므로 다양한 자료형이 할당된다.</p></li><li><p><code>파이썬에서 변수는 단순히 값을 보관하는 장소가 아니라 값들의 임시 저장 장소로만 사용</code>한다. 프로그램 내에서 변수는 단순히 이름으로 구별하는 것이고 변수에는 값인 객체가 어디에 있는지에 대한 주소인 reference만 보관한다. 즉, <code>변수는 단순히 자료형으로 만들어진 객체에 대한 주소만을 관리하는 역할을 수행</code>한다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/03/20/Python_00/#disqus_thread</comments>
    </item>
    
    <item>
      <title>내가 정리하는 자료구조 00 (Node, List, Queue)</title>
      <link>https://heung-bae-lee.github.io/2020/03/19/data_structure_01/</link>
      <guid>https://heung-bae-lee.github.io/2020/03/19/data_structure_01/</guid>
      <pubDate>Thu, 19 Mar 2020 09:46:30 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;목표&quot;&gt;&lt;a href=&quot;#목표&quot; class=&quot;headerlink&quot; title=&quot;목표&quot;&gt;&lt;/a&gt;목표&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;기본 자료 구조/알고리즘 익히기&lt;ul&gt;
&lt;li&gt;알고리즘 풀이를 위해, 기본적으로 알고 있어야 하는 자료구조와 알고리
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="목표"><a href="#목표" class="headerlink" title="목표"></a>목표</h2><ul><li>기본 자료 구조/알고리즘 익히기<ul><li>알고리즘 풀이를 위해, 기본적으로 알고 있어야 하는 자료구조와 알고리즘 정리</li></ul></li></ul><h2 id="자료구조란"><a href="#자료구조란" class="headerlink" title="자료구조란?"></a>자료구조란?</h2><ul><li>용어: 자료구조 = 데이터 구조 = data structure</li><li>대량의 데이터를 효율적으로 관리할 수 있는 데이터의 구조를 의미</li><li>코드상에서 효율적으로 데이터를 처리하기 위해, 데이터 특성에 따라, 체계적으로 데이터를 구조화해야 함.<ul><li><code>어떤 데이터 구조를 사용하느냐에 따라, 코드 효율이 달라진다.</code></li></ul></li></ul><h3 id="효율적으로-데이터를-관리하는-예"><a href="#효율적으로-데이터를-관리하는-예" class="headerlink" title="효율적으로 데이터를 관리하는 예"></a>효율적으로 데이터를 관리하는 예</h3><ul><li><p>우편번호</p><ul><li>5자리 우편번호로 국가의 기초구역을 식별</li><li>5자리 우편번호에서 앞 3자리는 시,군,자치구를 의미, 뒤 2자리는 일련변호로 구성</li></ul></li><li><p>학생관리</p><ul><li>학년, 반, 번호를 학생에게 부여해서, 학생부를 관리</li><li>위같은 기법이 없었다면, 많은 학생 중 특정 학생을 찾기 위해, 전체 학생부를 모두 훑어야 하는 불편함이 생긴다.</li></ul></li></ul><h2 id="대표적인-자료-구조"><a href="#대표적인-자료-구조" class="headerlink" title="대표적인 자료 구조"></a>대표적인 자료 구조</h2><ul><li>array(배열), stack(스택), queue(큐), linked list(링크드 리스트), hash table(해쉬 테이블), heap(힙) 등</li></ul><h2 id="알고리즘이란"><a href="#알고리즘이란" class="headerlink" title="알고리즘이란?"></a>알고리즘이란?</h2><ul><li>용어: 알고리즘(algorithm)</li><li><code>어떤 문제를 풀기 위한 절차/방법</code></li><li>어떤 문제에 대해, 특정한 ‘입력’을 넣으면, 원하는 ‘출력’을 얻을 수 있도록 만드는 프로그래밍</li></ul><h2 id="자료구조와-알고리즘이-중요한-이유"><a href="#자료구조와-알고리즘이-중요한-이유" class="headerlink" title="자료구조와 알고리즘이 중요한 이유"></a>자료구조와 알고리즘이 중요한 이유</h2><ul><li><code>어떤 자료구조와 알고리즘을 쓰느냐에 따라, 성능이 많이 차이나기 때문</code>이다. 결과적으로, 자료구조와 알고리즘을 통해 프로그래밍을 잘 할 수 있는 기술과 역량을 검증할 수 있다.</li></ul><h2 id="자료구조-알고리즘-그리고-Python"><a href="#자료구조-알고리즘-그리고-Python" class="headerlink" title="자료구조/알고리즘, 그리고 Python"></a>자료구조/알고리즘, 그리고 Python</h2><ul><li>어떤 언어로든 자료구조/알고리즘을 익힐 수 있다.<ul><li>이전에는 무조건 C 또는 C++로만 작성하도록 하는 경향이 많았다.</li><li>최근에는 언어로 인한 제약/평가는 없어졌다고 봐도 무방할 것 같다.<ul><li>그러므로, 가장 쉽고 진입장벽이 상대적으로 낮은 Python을 통해 필자는 정리해 볼 것이다.</li></ul></li></ul></li></ul><h2 id="ADT-Abstract-Data-Type"><a href="#ADT-Abstract-Data-Type" class="headerlink" title="ADT(Abstract Data Type)"></a>ADT(Abstract Data Type)</h2><h2 id="추상-자료형"><a href="#추상-자료형" class="headerlink" title="추상 자료형"></a>추상 자료형</h2><p>실제 정의)</p><pre><code>- 내부 구조(object)- 기능(operation) ==&gt; 함수로 구현</code></pre><p>간단한 말로는 내가 사용하고자 하는 <code>자료구조의 함수의 사용법</code>이다.</p><h4 id="필자는-Python을-통해-각-알고리즘을-구현해-볼-것이다-물론-Python에는-이미-만들어져-놓은-library-중-대부분의-알고리즘을-구현해-놓은-것들이-존재하지만-그렇게-사용하다보면-내부적으로-뜯어보지-않는-이상-어떠한-방식으로-작동되는지-알지-못하기-때문에-각-알고리즘의-구조를-설명한-후-ADT를-작성해-보고-class를-만들어-function을-작성하여-구현하는-방식으로-글을-써-갈-것이다-물론-Python-내부에-존재하는-library의-사용법도-같이-소개할-것이다"><a href="#필자는-Python을-통해-각-알고리즘을-구현해-볼-것이다-물론-Python에는-이미-만들어져-놓은-library-중-대부분의-알고리즘을-구현해-놓은-것들이-존재하지만-그렇게-사용하다보면-내부적으로-뜯어보지-않는-이상-어떠한-방식으로-작동되는지-알지-못하기-때문에-각-알고리즘의-구조를-설명한-후-ADT를-작성해-보고-class를-만들어-function을-작성하여-구현하는-방식으로-글을-써-갈-것이다-물론-Python-내부에-존재하는-library의-사용법도-같이-소개할-것이다" class="headerlink" title="필자는 Python을 통해 각 알고리즘을 구현해 볼 것이다. 물론 Python에는 이미 만들어져 놓은 library 중 대부분의 알고리즘을 구현해 놓은 것들이 존재하지만, 그렇게 사용하다보면 내부적으로 뜯어보지 않는 이상 어떠한 방식으로 작동되는지 알지 못하기 때문에 각 알고리즘의 구조를 설명한 후 ADT를 작성해 보고 class를 만들어 function을 작성하여 구현하는 방식으로 글을 써 갈 것이다. 물론, Python 내부에 존재하는 library의 사용법도 같이 소개할 것이다."></a>필자는 Python을 통해 각 알고리즘을 구현해 볼 것이다. 물론 Python에는 이미 만들어져 놓은 library 중 대부분의 알고리즘을 구현해 놓은 것들이 존재하지만, 그렇게 사용하다보면 내부적으로 뜯어보지 않는 이상 어떠한 방식으로 작동되는지 알지 못하기 때문에 각 알고리즘의 구조를 설명한 후 ADT를 작성해 보고 class를 만들어 function을 작성하여 구현하는 방식으로 글을 써 갈 것이다. 물론, Python 내부에 존재하는 library의 사용법도 같이 소개할 것이다.</h4><h3 id="선형구조"><a href="#선형구조" class="headerlink" title="선형구조"></a>선형구조</h3><ul><li>배열</li><li>연결리스트</li><li>스택</li><li>큐</li></ul><h3 id="비선형구조"><a href="#비선형구조" class="headerlink" title="비선형구조"></a>비선형구조</h3><ul><li>트리 (BTS —&gt; B-tree, red-black tree)</li><li>그래프</li><li>테이블</li><li>etc</li></ul><h2 id="Array-배열"><a href="#Array-배열" class="headerlink" title="Array(배열)"></a>Array(배열)</h2><ul><li><code>데이터를 나열하고, 각 데이터를 인덱스에 대응하도록 구성한 데이터 구조</code></li><li>Python에서는 <code>리스트 타입이 배열 기능</code>을 제공한다.</li></ul><h3 id="1-배열은-왜-필요할까"><a href="#1-배열은-왜-필요할까" class="headerlink" title="1. 배열은 왜 필요할까?"></a>1. 배열은 왜 필요할까?</h3><ul><li><code>같은 종류</code>의 데이터를 효율적으로 관리하기 위해 사용</li><li><code>같은 종류</code>의 데이터를 <code>순차적으로 저장</code></li><li>장점:<ul><li><code>빠른 접근 가능</code></li><li>첫 데이터의 위치에서 상대적인 위치로 데이터 접근(인덱스 번호로 접근)</li></ul></li><li>단점:<ul><li><code>데이터 추가/삭제의 어려움</code></li><li>미리 최대 길이를 지정해야 함</li></ul></li></ul><h4 id="C-언어-예-영어-단어-저장"><a href="#C-언어-예-영어-단어-저장" class="headerlink" title="C 언어 예: 영어 단어 저장"></a>C 언어 예: 영어 단어 저장</h4><ul><li>배열을 생성할 때 먼저 길이를 정해주어야 하기 때문에, 아래와 같이 []안에 최대 길이를 지정해준 것을 확인할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#include &lt;stdio.h&gt;</span></span><br><span class="line"></span><br><span class="line">int main(int argc, char * argv[])</span><br><span class="line">&#123;</span><br><span class="line">    char country[3] = <span class="string">"US"</span>;</span><br><span class="line">    <span class="built_in">printf</span> (<span class="string">"%c%c\n"</span>, country[0], country[1]);</span><br><span class="line">    <span class="built_in">printf</span> (<span class="string">"%s\n"</span>, country);    </span><br><span class="line">    <span class="built_in">return</span> 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="파이썬-언어-예-영어-단어-저장"><a href="#파이썬-언어-예-영어-단어-저장" class="headerlink" title="파이썬 언어 예: 영어 단어 저장"></a>파이썬 언어 예: 영어 단어 저장</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">country = <span class="string">'US'</span></span><br><span class="line"><span class="built_in">print</span> (country)</span><br></pre></td></tr></table></figure><h3 id="2-파이썬과-배열"><a href="#2-파이썬과-배열" class="headerlink" title="2. 파이썬과 배열"></a>2. 파이썬과 배열</h3><ul><li>파이썬에서는 리스트로 배열 구현 가능</li></ul><h4 id="1차원-배열-리스트로-구현시"><a href="#1차원-배열-리스트로-구현시" class="headerlink" title="1차원 배열: 리스트로 구현시"></a>1차원 배열: 리스트로 구현시</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_list = [1, 2, 3, 4, 5]</span><br><span class="line">data_list</span><br></pre></td></tr></table></figure><h4 id="2차원-배열-리스트로-구현시"><a href="#2차원-배열-리스트로-구현시" class="headerlink" title="2차원 배열: 리스트로 구현시"></a>2차원 배열: 리스트로 구현시</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_list = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]</span><br><span class="line">data_list</span><br></pre></td></tr></table></figure><h4 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (data_list[0])</span><br><span class="line"><span class="comment"># [1, 2, 3]</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (data_list[0][0])</span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="built_in">print</span> (data_list[0][1])</span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="built_in">print</span> (data_list[0][2])</span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span> (data_list[1][0])</span><br><span class="line"><span class="comment"># 4</span></span><br><span class="line"><span class="built_in">print</span> (data_list[1][1])</span><br><span class="line"><span class="comment"># 5</span></span><br><span class="line"><span class="built_in">print</span> (data_list[1][2])</span><br><span class="line"><span class="comment"># 6</span></span><br></pre></td></tr></table></figure><h3 id="3-프로그래밍-연습"><a href="#3-프로그래밍-연습" class="headerlink" title="3. 프로그래밍 연습"></a>3. 프로그래밍 연습</h3><h4 id="연습1-위의-2차원-배열에서-9-8-7-을-순서대로-출력해보기"><a href="#연습1-위의-2차원-배열에서-9-8-7-을-순서대로-출력해보기" class="headerlink" title="연습1: 위의 2차원 배열에서 9, 8, 7 을 순서대로 출력해보기"></a>연습1: 위의 2차원 배열에서 9, 8, 7 을 순서대로 출력해보기</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(data_list[2][2], data_list[2][1], data_list[2][0])</span><br></pre></td></tr></table></figure><h4 id="연습2-아래의-dataset-리스트에서-전체-이름-안에-M-은-몇-번-나왔는지-빈도수-출력하기"><a href="#연습2-아래의-dataset-리스트에서-전체-이름-안에-M-은-몇-번-나왔는지-빈도수-출력하기" class="headerlink" title="연습2: 아래의 dataset 리스트에서 전체 이름 안에 M 은 몇 번 나왔는지 빈도수 출력하기"></a>연습2: 아래의 dataset 리스트에서 전체 이름 안에 M 은 몇 번 나왔는지 빈도수 출력하기</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">dataset = [<span class="string">'Braund, Mr. Owen Harris'</span>,</span><br><span class="line"><span class="string">'Cumings, Mrs. John Bradley (Florence Briggs Thayer)'</span>,</span><br><span class="line"><span class="string">'Heikkinen, Miss. Laina'</span>,</span><br><span class="line"><span class="string">'Futrelle, Mrs. Jacques Heath (Lily May Peel)'</span>,</span><br><span class="line"><span class="string">'Allen, Mr. William Henry'</span>,</span><br><span class="line"><span class="string">'Moran, Mr. James'</span>,</span><br><span class="line"><span class="string">'McCarthy, Mr. Timothy J'</span>,</span><br><span class="line"><span class="string">'Palsson, Master. Gosta Leonard'</span>,</span><br><span class="line"><span class="string">'Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)'</span>,</span><br><span class="line"><span class="string">'Nasser, Mrs. Nicholas (Adele Achem)'</span>,</span><br><span class="line"><span class="string">'Sandstrom, Miss. Marguerite Rut'</span>,</span><br><span class="line"><span class="string">'Bonnell, Miss. Elizabeth'</span>,</span><br><span class="line"><span class="string">'Saundercock, Mr. William Henry'</span>,</span><br><span class="line"><span class="string">'Andersson, Mr. Anders Johan'</span>,</span><br><span class="line"><span class="string">'Vestrom, Miss. Hulda Amanda Adolfina'</span>,</span><br><span class="line"><span class="string">'Hewlett, Mrs. (Mary D Kingcome) '</span>,</span><br><span class="line"><span class="string">'Rice, Master. Eugene'</span>,</span><br><span class="line"><span class="string">'Williams, Mr. Charles Eugene'</span>,</span><br><span class="line"><span class="string">'Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele)'</span>,</span><br><span class="line"><span class="string">'Masselmani, Mrs. Fatima'</span>,</span><br><span class="line"><span class="string">'Fynney, Mr. Joseph J'</span>,</span><br><span class="line"><span class="string">'Beesley, Mr. Lawrence'</span>,</span><br><span class="line"><span class="string">'McGowan, Miss. Anna "Annie"'</span>,</span><br><span class="line"><span class="string">'Sloper, Mr. William Thompson'</span>,</span><br><span class="line"><span class="string">'Palsson, Miss. Torborg Danira'</span>,</span><br><span class="line"><span class="string">'Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson)'</span>,</span><br><span class="line"><span class="string">'Emir, Mr. Farred Chehab'</span>,</span><br><span class="line"><span class="string">'Fortune, Mr. Charles Alexander'</span>,</span><br><span class="line"><span class="string">'Dwyer, Miss. Ellen "Nellie"'</span>,</span><br><span class="line"><span class="string">'Todoroff, Mr. Lalio'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">m_count = 0</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataset:</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(len(data)):</span><br><span class="line">        <span class="keyword">if</span> data[index] == <span class="string">'M'</span>:</span><br><span class="line">            m_count += 1</span><br><span class="line"><span class="built_in">print</span> (m_count)</span><br></pre></td></tr></table></figure><ul><li>위에서와 같이 간단하게 이미 만들어져 있는 Python의 자료형인 list를 사용하여 Array를 사용하는 것 말고 Node 개념을 도입하여 Array를 만들어 볼 것이다.</li></ul><h2 id="Node"><a href="#Node" class="headerlink" title="Node"></a>Node</h2><ul><li>아래와 같이 하나의 노드는 <code>실제 데이터와 다음데이터를 가리키고 있는 참조로 구성</code>되어있다.</li></ul><p><img src="/image/Node.png" alt="Node"></p><ul><li>Node Class 구현<ul><li>위의 그림과 같이 실제 데이터를 가리키는 부분과 다음 데이터를 참조하는 부분을 나누어서 정의해 준다.</li></ul></li></ul><h3 id="혼자-노드를-만들때-필요한-과정을-정리하면서-만들어보기"><a href="#혼자-노드를-만들때-필요한-과정을-정리하면서-만들어보기" class="headerlink" title="혼자 노드를 만들때 필요한 과정을 정리하면서 만들어보기"></a>혼자 노드를 만들때 필요한 과정을 정리하면서 만들어보기</h3><p>1) Node의 정의를 먼저 살펴보면 노드는 데이터를 포함하고있는 부분과 다음데이터를 가리키는 참조부분으로 이루어져 있다!</p><p>2) 그러므로 Node class에는 먼저, 데이터 부분과 참조부분을 만들어주어야 하는데, 우선 우리의 목표는 어디까지나 Node의 기능을 구현하는 것이다.</p><p>3) Node의 데이터 조회, 데이터 바꾸기와 같은 기능, 그리고 참조를 어디로 하는지 등이필요할 것이다!</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class Node_mine():</span><br><span class="line">    <span class="comment"># 생성자에는 우선적으로 노드의 component들을 필수요소로 넣어줘야할 것이다.</span></span><br><span class="line">    <span class="comment"># 여기서 처음 부터 next를 argument에서 빼 놓은 이유는?</span></span><br><span class="line">    <span class="comment"># 이어져 있기 때문에 다음 노드를 바로 참조하면 되기 때문에 argument로 넣어줄 필요는 없다.</span></span><br><span class="line">    <span class="comment"># 즉, 우리가 마음대로 4번째 노드를 1번째노드로 참조하게끔하는 것은 linked list의 정의를 벗어나므로</span></span><br><span class="line">    <span class="comment"># 참조부분을 Node를 생성하면서부터 지정할 필요는 없다는 것이다.</span></span><br><span class="line"></span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line">        self.__next=None</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 소멸자: 객체가 메모리에서 사라질때 반드시 한번은 호출해 주는 것</span></span><br><span class="line">    def __delete__(self):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"&#123;&#125; is deleted"</span>.format(self.__data))</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def get_data(self):</span><br><span class="line">        <span class="built_in">return</span> self.__data</span><br><span class="line"></span><br><span class="line">    @get_data.setter</span><br><span class="line">    def get_data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def get_next(self):</span><br><span class="line">        <span class="built_in">return</span> self.__next</span><br><span class="line"></span><br><span class="line">    @get_next.setter</span><br><span class="line">    def get_next(self, nt):</span><br><span class="line">        self.__next=nt</span><br></pre></td></tr></table></figure><ul><li>node 정의 및 생성</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n1=Node_mine(5)</span><br></pre></td></tr></table></figure><ul><li>node의 value 출력</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">n1.get_data</span><br><span class="line"></span><br><span class="line"><span class="comment">### 결과</span></span><br><span class="line"><span class="comment">### 5</span></span><br></pre></td></tr></table></figure><ul><li>node에 새로운 값 할당</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">n1.get_data=3</span><br></pre></td></tr></table></figure><ul><li>변경된 값을 제대로 할당 받았는지 확인<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n1.get_data</span><br><span class="line">### 결과</span><br><span class="line">### 3</span><br></pre></td></tr></table></figure></li></ul><h4 id="현업에서-사용하는-linked-list라-함은-dummy-duable-linked-list를-의미하고-아래에서의-single-linked-list에서-single이-의미하는-것은-참조가-하나-즉-한개의-방향만을-참조하고-duable은-양방향을-참조한다"><a href="#현업에서-사용하는-linked-list라-함은-dummy-duable-linked-list를-의미하고-아래에서의-single-linked-list에서-single이-의미하는-것은-참조가-하나-즉-한개의-방향만을-참조하고-duable은-양방향을-참조한다" class="headerlink" title="현업에서 사용하는 linked list라 함은 dummy duable linked list를 의미하고, 아래에서의 single linked list에서 single이 의미하는 것은 참조가 하나 즉 한개의 방향만을 참조하고 duable은 양방향을 참조한다."></a>현업에서 사용하는 linked list라 함은 dummy duable linked list를 의미하고, 아래에서의 single linked list에서 single이 의미하는 것은 참조가 하나 즉 한개의 방향만을 참조하고 duable은 양방향을 참조한다.</h4><h4 id="Singel-Linked-list"><a href="#Singel-Linked-list" class="headerlink" title="Singel Linked list"></a>Singel Linked list</h4><h5 id="Instance-Member"><a href="#Instance-Member" class="headerlink" title="Instance Member"></a>Instance Member</h5><ol><li>head<ul><li>리스트의 첫번째 노드를 가리킨다.</li></ul></li><li>d_size<ul><li>리스트의 요소 개수</li></ul></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">class SingleLinkedList:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.head = None</span><br><span class="line">        self.d_size = 0</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        <span class="keyword">if</span> self.d_size==0:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def size(self):</span><br><span class="line">        <span class="built_in">return</span> self.d_size</span><br><span class="line"></span><br><span class="line">    def add(self, data):</span><br><span class="line">        new_node=Node_mine(data)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 아래에서와 같이 단방향으로 이루어져 있고, 기존에 있던 Node를 새로운 Node의 다음으로 참조시켜준 뒤에, SingleLinkedList 클래스의 head값에는 새로운 Node를 추가해준다.</span></span><br><span class="line">        new_node.next=self.head</span><br><span class="line">        self.head=new_node</span><br><span class="line"></span><br><span class="line">        self.d_size+=1</span><br><span class="line"></span><br><span class="line">    def search(self, target):</span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur :</span><br><span class="line">            <span class="keyword">if</span> cur.data == target :</span><br><span class="line">                <span class="built_in">return</span> cur</span><br><span class="line">            <span class="keyword">else</span> :</span><br><span class="line">                cur=cur.next</span><br><span class="line">        <span class="built_in">return</span> cur</span><br><span class="line"></span><br><span class="line">    def delete(self):</span><br><span class="line">        <span class="comment"># head에 있는 값을 지울것이므로 head값을 기존의 head의 다음 값으로 바꾸어주면 garbage collector에 의해서 사라지게 된다.</span></span><br><span class="line">        self.head=self.head.next</span><br><span class="line">        self.d_size-=1</span><br><span class="line"></span><br><span class="line">    def traverse(self):</span><br><span class="line">        cur=self.head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            yield cur</span><br><span class="line">            cur=cur.next</span><br></pre></td></tr></table></figure><h2 id="Queue-큐"><a href="#Queue-큐" class="headerlink" title="Queue(큐)"></a>Queue(큐)</h2><ul><li>array와 함께 가장 쉬운 자료구조 중 하나<ul><li>컴퓨터에서 가장 핵심적인 OS(운영체제)에서도 많이 사용되며, 인터넷에서도 네트워크 기능에서도 많이 사용된다.</li></ul></li></ul><h3 id="1-Queue-구조"><a href="#1-Queue-구조" class="headerlink" title="1. Queue 구조"></a>1. Queue 구조</h3><ul><li><code>줄을 서는 행위와 유사</code></li><li>가장 먼저 넣은 데이터를 가장 먼저 꺼낼 수 있는 구조<ul><li>음식점에서 가장 먼저 줄을 선 사람이 제일 먼저 음식점에 입장하는 것과 동일</li><li><code>FIFO(First-In, First-Out)</code> 또는 <code>LILO(Last-In, Last-Out)</code> 방식으로 스택과 꺼내는 순서가 반대</li></ul></li></ul><p><img src="https://www.fun-coding.org/00_Images/queue.png"></p><ul><li>출처: <a href="http://www.stoimen.com/blog/2012/06/05/computer-algorithms-stack-and-queue-data-structure/" target="_blank" rel="noopener">http://www.stoimen.com/blog/2012/06/05/computer-algorithms-stack-and-queue-data-structure/</a></li></ul><h3 id="2-알아둘-용어"><a href="#2-알아둘-용어" class="headerlink" title="2. 알아둘 용어"></a>2. 알아둘 용어</h3><ul><li>Enqueue: 큐에 데이터를 넣는 기능</li><li>Dequeue: 큐에서 데이터를 꺼내는 기능</li><li><font color="#BF360C">Visualgo 사이트에서 시연해보며 이해하기 (enqueue/dequeue 만 클릭해보며): <a href="https://visualgo.net/en/list" target="_blank" rel="noopener">https://visualgo.net/en/list</a></font></li></ul><h3 id="3-파이썬-queue-라이브러리-활용해서-큐-자료-구조-사용하기"><a href="#3-파이썬-queue-라이브러리-활용해서-큐-자료-구조-사용하기" class="headerlink" title="3. 파이썬 queue 라이브러리 활용해서 큐 자료 구조 사용하기"></a>3. 파이썬 queue 라이브러리 활용해서 큐 자료 구조 사용하기</h3><ul><li><strong>queue 라이브러리에는 다양한 큐 구조로 Queue(), LifoQueue(), PriorityQueue() 제공</strong></li><li><font color="#BF360C">프로그램을 작성할 때 프로그램에 따라 적합한 자료 구조를 사용</font><ul><li>Queue(): 가장 일반적인 큐 자료 구조</li><li>LifoQueue(): 나중에 입력된 데이터가 먼저 출력되는 구조 (스택 구조라고 보면 됨)</li><li>PriorityQueue(): 데이터마다 우선순위를 넣어서, 우선순위가 높은 순으로 데이터 출력</li></ul></li></ul><blockquote><p>일반적인 큐 외에 다양한 정책이 적용된 큐들이 있음</p></blockquote><h4 id="3-1-Queue-로-큐-만들기-가장-일반적인-큐-FIFO-First-In-First-Out"><a href="#3-1-Queue-로-큐-만들기-가장-일반적인-큐-FIFO-First-In-First-Out" class="headerlink" title="3.1. Queue()로 큐 만들기 (가장 일반적인 큐, FIFO(First-In, First-Out))"></a>3.1. Queue()로 큐 만들기 (가장 일반적인 큐, FIFO(First-In, First-Out))</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import queue</span><br><span class="line"></span><br><span class="line">data_queue = queue.Queue()</span><br><span class="line">data_queue.put(<span class="string">"funcoding"</span>)</span><br><span class="line">data_queue.put(1)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.qsize()</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.get()</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'funcoding'</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.qsize()</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.get()</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'funcoding'</span></span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.qsize()</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure><h4 id="3-2-LifoQueue-로-큐-만들기-LIFO-Last-In-First-Out"><a href="#3-2-LifoQueue-로-큐-만들기-LIFO-Last-In-First-Out" class="headerlink" title="3.2. LifoQueue()로 큐 만들기 (LIFO(Last-In, First-Out))"></a>3.2. LifoQueue()로 큐 만들기 (LIFO(Last-In, First-Out))</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import queue</span><br><span class="line">data_queue = queue.LifoQueue()</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_queue.put(<span class="string">"funcoding"</span>)</span><br><span class="line">data_queue.put(1)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.qsize()</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2</span><br></pre></td></tr></table></figure><h5 id="그냥-Queue-구조가-아닌-Lifo-Queue이기-때문에-마지막에-추가해-준-1이-출력으로-나온다"><a href="#그냥-Queue-구조가-아닌-Lifo-Queue이기-때문에-마지막에-추가해-준-1이-출력으로-나온다" class="headerlink" title="그냥 Queue 구조가 아닌 Lifo Queue이기 때문에 마지막에 추가해 준 1이 출력으로 나온다."></a>그냥 Queue 구조가 아닌 Lifo Queue이기 때문에 마지막에 추가해 준 1이 출력으로 나온다.</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.get()</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure><h4 id="3-3-PriorityQueue-로-큐-만들기"><a href="#3-3-PriorityQueue-로-큐-만들기" class="headerlink" title="3.3. PriorityQueue()로 큐 만들기"></a>3.3. PriorityQueue()로 큐 만들기</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import queue</span><br><span class="line"></span><br><span class="line">data_queue = queue.PriorityQueue()</span><br></pre></td></tr></table></figure><h5 id="아래에서-보는-것과-같이-우선-순위와-값을-튜플형태로-같이-넣어준다-우선순위는-숫자가-낮을-수록-높은-우선-순위를-지닌다"><a href="#아래에서-보는-것과-같이-우선-순위와-값을-튜플형태로-같이-넣어준다-우선순위는-숫자가-낮을-수록-높은-우선-순위를-지닌다" class="headerlink" title="아래에서 보는 것과 같이 우선 순위와 값을 튜플형태로 같이 넣어준다. 우선순위는 숫자가 낮을 수록 높은 우선 순위를 지닌다."></a>아래에서 보는 것과 같이 우선 순위와 값을 튜플형태로 같이 넣어준다. 우선순위는 숫자가 낮을 수록 높은 우선 순위를 지닌다.</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_queue.put((10, <span class="string">"korea"</span>))</span><br><span class="line">data_queue.put((5, 1))</span><br><span class="line">data_queue.put((15, <span class="string">"china"</span>))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.qsize()</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.get()</span><br></pre></td></tr></table></figure><h5 id="우선순위가-제일-높은-제일-낮은-숫자인-5를-지닌-값-1이-출력된다"><a href="#우선순위가-제일-높은-제일-낮은-숫자인-5를-지닌-값-1이-출력된다" class="headerlink" title="우선순위가 제일 높은 제일 낮은 숫자인 5를 지닌 값 1이 출력된다."></a>우선순위가 제일 높은 제일 낮은 숫자인 5를 지닌 값 1이 출력된다.</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(5, 1)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_queue.get()</span><br></pre></td></tr></table></figure><h5 id="그-다음-우선-순위인-10을-지닌-korea가-출력된다"><a href="#그-다음-우선-순위인-10을-지닌-korea가-출력된다" class="headerlink" title="그 다음 우선 순위인 10을 지닌 korea가 출력된다."></a>그 다음 우선 순위인 10을 지닌 korea가 출력된다.</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(10, <span class="string">"korea"</span>)</span><br></pre></td></tr></table></figure><h3 id="참고-어디에-Queue가-많이-쓰일까"><a href="#참고-어디에-Queue가-많이-쓰일까" class="headerlink" title="참고: 어디에 Queue가 많이 쓰일까?"></a>참고: 어디에 Queue가 많이 쓰일까?</h3><ul><li><code>멀티 태스킹을 위한 프로세스 스케쥴링 방식을 구현하기 위해 많이 사용</code>된다. (운영체제 참조)</li></ul><blockquote><p>Queue의 경우에는 장단점 보다는 (특별히 언급되는 장단점이 없음), 큐의 활용 예로 프로세스 스케쥴링 방식을 함께 이해해두는 것이 좋음</p></blockquote><h3 id="4-프로그래밍-연습"><a href="#4-프로그래밍-연습" class="headerlink" title="4. 프로그래밍 연습"></a>4. 프로그래밍 연습</h3><ul><li>연습1: 리스트 변수로 큐를 다루는 enqueue, dequeue 기능 구현해보기</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">queue_list = list()</span><br><span class="line"></span><br><span class="line">def enqueue(data):</span><br><span class="line">    queue_list.append(data)</span><br><span class="line"></span><br><span class="line">def dequeue():</span><br><span class="line">    data = queue_list[0]</span><br><span class="line">    del queue_list[0]</span><br><span class="line">    <span class="built_in">return</span> data</span><br></pre></td></tr></table></figure><ul><li>위에서 만든 queue_list 안에 0~9까지의 수를 넣는다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> range(10):</span><br><span class="line">    enqueue(index)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(queue_list)</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dequeue()</span><br></pre></td></tr></table></figure><h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><ul><li>FIFO 구조이므로 제일 먼저 들어간 0이 출력된다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0</span><br></pre></td></tr></table></figure><h4 id="파이썬-리스트를-이용한-큐-구현"><a href="#파이썬-리스트를-이용한-큐-구현" class="headerlink" title="파이썬 리스트를 이용한 큐 구현"></a>파이썬 리스트를 이용한 큐 구현</h4><ul><li><p>위에서 만들었던 function들을 이용하여 좀더 많은 기능이 포함되어 있는 하나의 class로 Queue를 구현해 볼 것이다.</p></li><li><p>Python list 자료형의 pop 메서드를 통해 dequeue를 구현할 것이다.</p></li></ul><p><img src="/image/queue_basic_structure.png" alt="Queue 구조"></p><p><img src="/image/enqueue_basic_list.png" alt="Enqueue"></p><p><img src="/image/dequeue_basic_list.png" alt="Dequeue"></p><h5 id="Enqueue"><a href="#Enqueue" class="headerlink" title="Enqueue"></a>Enqueue</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class Queue:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.container=list()</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line"><span class="comment">#         if self.container is None:</span></span><br><span class="line">        <span class="keyword">if</span> not self.container:</span><br><span class="line">            <span class="built_in">return</span> True</span><br><span class="line">        <span class="built_in">return</span> False</span><br><span class="line"></span><br><span class="line">    def enqueue(self, data):</span><br><span class="line">        self.container.append(data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def dequeue(self):</span><br><span class="line">        <span class="built_in">return</span> self.container.pop(0)</span><br><span class="line"></span><br><span class="line">    def peek(self):</span><br><span class="line">        <span class="built_in">return</span> self.container[0]</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">q=Queue()</span><br><span class="line">q.enqueue(1)</span><br><span class="line">q.enqueue(2)</span><br><span class="line">q.enqueue(3)</span><br><span class="line">q.enqueue(4)</span><br><span class="line">q.enqueue(5)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> not q.empty():</span><br><span class="line">    <span class="built_in">print</span>(q.dequeue(), end=<span class="string">'  '</span>)</span><br></pre></td></tr></table></figure><h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 3 4 5</span><br></pre></td></tr></table></figure><h3 id="single-linked-list를-이용해-Queue를-구현"><a href="#single-linked-list를-이용해-Queue를-구현" class="headerlink" title="single linked list를 이용해 Queue를 구현"></a>single linked list를 이용해 Queue를 구현</h3><h4 id="ADT"><a href="#ADT" class="headerlink" title="ADT"></a>ADT</h4><ul><li><p>Queue.empty() -&gt; Boolean</p><ul><li>Queue가 비어있으면 True, 아니면 False</li></ul></li><li><p>Queue.enqueue(data) -&gt; None</p><ul><li>Queue의 맨 뒤에 데이터를 쌓는다.</li></ul></li></ul><p><img src="/image/enqueue_Node_01.png" alt="Enqueue - 01"></p><blockquote><p>맨 처음 enqueue 한 데이터는 front에 위치하고 최근에 추가로 삽입해준 데이터는 rear로 위치시킨다. 각각의 Node는 next로 연결을 시켜준다.</p></blockquote><p><img src="/image/enqueue_Node_02.png" alt="Enqueue - 02"></p><blockquote><p>먼저 새로운 Node를 추가해주기 위해서는 현재 self.rear.next를 new_node를 가리키도록하고, self.rear를 new_node로 설정해주면 된다.</p></blockquote><ul><li>Queue.dequeue() -&gt; data<ul><li>Queue 맨 앞의 데이터를 삭제하면서 반환</li></ul></li></ul><p><img src="/image/dequeue_Node_01.png" alt="Dequeue - 01"></p><p><img src="/image/dequeue_Node_02.png" alt="Dequeue - 02"></p><blockquote><p>우선 Dequeue를 하면 제일 먼저 추가해놓았던 데이터의 값을 출력해주면 해당 Node를 제거해 주어야하므로, self.front의 데이터를 cur라는 변수에 따로 저자해 주는데, Node가 하나일때를 생각해 보면 self.front와 self.rear가 동일하므로 먼저 self.rear를 None으로 만들어준다. 그리고 나서 cur에 self.front를 할당해주고, self.front는 self.front.next를 할당해주며, cur.data를 출력해주면 된다.</p></blockquote><ul><li>Queue.peek() -&gt; data<ul><li>Queue 맨 앞 데이터를 반환</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">class Node:</span><br><span class="line">    def __init__(self, data):</span><br><span class="line">        self.__data = data</span><br><span class="line">        self.__next = None</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def data(self):</span><br><span class="line">        return self.__data</span><br><span class="line"></span><br><span class="line">    @data.setter</span><br><span class="line">    def data(self, data):</span><br><span class="line">        self.__data=data</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def next(self):</span><br><span class="line">        return self.__next</span><br><span class="line"></span><br><span class="line">    @next.setter</span><br><span class="line">    def next(self, n):</span><br><span class="line">        self.__next=n</span><br><span class="line"></span><br><span class="line">class LQueue:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.front=None</span><br><span class="line">        self.rear=None</span><br><span class="line"></span><br><span class="line">    def empty(self):</span><br><span class="line">        if self.front is None:</span><br><span class="line">            return True</span><br><span class="line">        return False</span><br><span class="line"></span><br><span class="line">    def enqueue(self, data):</span><br><span class="line">        new_node=Node(data)</span><br><span class="line">        if self.empty():</span><br><span class="line">            self.front=new_node</span><br><span class="line">            self.rear=new_node</span><br><span class="line">        self.rear.next = new_node</span><br><span class="line">        self.rear = new_node</span><br><span class="line"></span><br><span class="line">    def dequeue(self):</span><br><span class="line">        if self.empty():</span><br><span class="line">            return None</span><br><span class="line"></span><br><span class="line">        if self.front is self.rear:</span><br><span class="line">            self.rear = None</span><br><span class="line">        cur = self.front</span><br><span class="line">        self.front = self.front.next</span><br><span class="line">        return cur.data</span><br><span class="line"></span><br><span class="line">    def peek(self):</span><br><span class="line">        return self.front.data</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">q = LQueue()</span><br><span class="line">q.enqueue(1)</span><br><span class="line">q.enqueue(2)</span><br><span class="line">q.enqueue(3)</span><br><span class="line">q.enqueue(4)</span><br><span class="line">q.enqueue(5)</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> not q.empty():</span><br><span class="line">    <span class="built_in">print</span>(q.dequeue(), end=<span class="string">'  '</span>)</span><br></pre></td></tr></table></figure><h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 3 4 5</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/03/19/data_structure_01/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering (데이터 모델링 및 챗봇 만들기)</title>
      <link>https://heung-bae-lee.github.io/2020/03/03/data_engineering_10/</link>
      <guid>https://heung-bae-lee.github.io/2020/03/03/data_engineering_10/</guid>
      <pubDate>Tue, 03 Mar 2020 14:29:20 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Spotify-데이터-유사도-모델링&quot;&gt;&lt;a href=&quot;#Spotify-데이터-유사도-모델링&quot; class=&quot;headerlink&quot; title=&quot;Spotify 데이터 유사도 모델링&quot;&gt;&lt;/a&gt;Spotify 데이터 유사도 모델링&lt;/h1&gt;&lt;ul&gt;

        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Spotify-데이터-유사도-모델링"><a href="#Spotify-데이터-유사도-모델링" class="headerlink" title="Spotify 데이터 유사도 모델링"></a>Spotify 데이터 유사도 모델링</h1><ul><li>모든 track을 다 유클리디안 거리를 계산해서 유사도를 측정하기에는 많은 양이기 때문에 해당 Artist의 track들의 audio feature 데이터에 대해 평균을 낸 값을 사용하여 Artist 끼리의 유사도를 계산할 것이다. 해당 유사도를 계산하기 위해 아래와 같이 먼저 RDS에 접속하여 table을 생성해 준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql -h spotify.cgaj5rvtgf25.ap-northeast-2.rds.amazonaws.com -P 3306 -u hb0619 -p</span><br><span class="line"></span><br><span class="line">CREATE TABLE related_artists (artist_id VARCHAR(255), y_artist VARCHAR(255), distance FLOAT, PRIMARY KEY(artist_id, y_artist)) ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br></pre></td></tr></table></figure><ul><li>그 다음은 우리가 미리 만들어놓았던 Athena의 DataBase는 무엇이었는지를 확인하자. 필자는 아래와 같이 따로 Database를 만들지 않고 default로 사용했다. 또한, 입력했었던 날짜를 확인해놓아야 추후에 코드 작성시 Athena로 접속하여 만들어진 테이블들을 참조할 수 있다.</li></ul><p><img src="/image/Athena_similarity_table_create.png" alt="Athena database"></p><ul><li><p>필자는 Athena에 미리 만들어놓았던 두가지 top_tracks와 audio_features 테이블을 이용하여 유사도를 구하고 해당 유사도를 MySQL DB에 insert하는 방식으로 작업을 진행 할 것이다.</p></li><li><p>data_modeling.py</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import logging</span><br><span class="line">import pymysql</span><br><span class="line">import boto3</span><br><span class="line">import time</span><br><span class="line">import math</span><br><span class="line"></span><br><span class="line">host = <span class="string">"end-point url"</span></span><br><span class="line">port = you port number</span><br><span class="line">username = <span class="string">"your MYSQL DB ID"</span></span><br><span class="line">database = <span class="string">"your MYSQL DB Name"</span></span><br><span class="line">password = <span class="string">"your MYSQL DB Password"</span></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        conn = pymysql.connect(host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=<span class="string">'utf8'</span>)</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line">    except:</span><br><span class="line">        logging.error(<span class="string">"could not connect to rds"</span>)</span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    athena = boto3.client(<span class="string">'athena'</span>)</span><br><span class="line"></span><br><span class="line">    query = <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        SELECT</span></span><br><span class="line"><span class="string">         artist_id,</span></span><br><span class="line"><span class="string">         AVG(danceability) AS danceability,</span></span><br><span class="line"><span class="string">         AVG(energy) AS energy,</span></span><br><span class="line"><span class="string">         AVG(loudness) AS loudness,</span></span><br><span class="line"><span class="string">         AVG(speechiness) AS speechiness,</span></span><br><span class="line"><span class="string">         AVG(acousticness) AS acousticness,</span></span><br><span class="line"><span class="string">         AVG(instrumentalness) AS instrumentalness</span></span><br><span class="line"><span class="string">        FROM</span></span><br><span class="line"><span class="string">         top_tracks t1</span></span><br><span class="line"><span class="string">        JOIN</span></span><br><span class="line"><span class="string">         audio_features t2 ON t2.id = t1.id AND CAST(t1.dt AS DATE) = DATE('2020-02-24') AND CAST(t2.dt AS DATE) = DATE('2020-02-24')</span></span><br><span class="line"><span class="string">        GROUP BY t1.artist_id</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    <span class="comment"># 위에서 DATE를 정하는 부분에서 CAST(t1.dt AS DATE) = CURRENT_DATE - INTERVAL '1' DAY 이렇게 현재날짜를 기준으로 차이나는 기간을 통해 정해줄 수 있다.</span></span><br><span class="line">    <span class="comment"># 필자는 여러번 Athena에 실행하지 않았기 때문에 최근에 Athena에 만들어 놓은 위의 두 테이블의 데이터를 직접 보고 날짜를 지정했다.</span></span><br><span class="line"></span><br><span class="line">    r = query_athena(query, athena)</span><br><span class="line">    results = get_query_result(r[<span class="string">'QueryExecutionId'</span>], athena)</span><br><span class="line">    artists = process_data(results)</span><br><span class="line"></span><br><span class="line">    query = <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        SELECT</span></span><br><span class="line"><span class="string">         MIN(danceability) AS danceability_min,</span></span><br><span class="line"><span class="string">         MAX(danceability) AS danceability_max,</span></span><br><span class="line"><span class="string">         MIN(energy) AS energy_min,</span></span><br><span class="line"><span class="string">         MAX(energy) AS energy_max,</span></span><br><span class="line"><span class="string">         MIN(loudness) AS loudness_min,</span></span><br><span class="line"><span class="string">         MAX(loudness) AS loudness_max,</span></span><br><span class="line"><span class="string">         MIN(speechiness) AS speechiness_min,</span></span><br><span class="line"><span class="string">         MAX(speechiness) AS speechiness_max,</span></span><br><span class="line"><span class="string">         ROUND(MIN(acousticness),4) AS acousticness_min,</span></span><br><span class="line"><span class="string">         MAX(acousticness) AS acousticness_max,</span></span><br><span class="line"><span class="string">         MIN(instrumentalness) AS instrumentalness_min,</span></span><br><span class="line"><span class="string">         MAX(instrumentalness) AS instrumentalness_max</span></span><br><span class="line"><span class="string">        FROM</span></span><br><span class="line"><span class="string">         audio_features</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    r = query_athena(query, athena)</span><br><span class="line">    results = get_query_result(r[<span class="string">'QueryExecutionId'</span>], athena)</span><br><span class="line">    avgs = process_data(results)[0]</span><br><span class="line"></span><br><span class="line">    metrics = [<span class="string">'danceability'</span>, <span class="string">'energy'</span>, <span class="string">'loudness'</span>, <span class="string">'speechiness'</span>, <span class="string">'acousticness'</span>, <span class="string">'instrumentalness'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> artists:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> artists:</span><br><span class="line">            dist = 0</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> metrics:</span><br><span class="line">                x = <span class="built_in">float</span>(i[k])</span><br><span class="line">                x_norm = normalize(x, <span class="built_in">float</span>(avgs[k+<span class="string">'_min'</span>]), <span class="built_in">float</span>(avgs[k+<span class="string">'_max'</span>]))</span><br><span class="line">                y = <span class="built_in">float</span>(j[k])</span><br><span class="line">                y_norm = normalize(y, <span class="built_in">float</span>(avgs[k+<span class="string">'_min'</span>]), <span class="built_in">float</span>(avgs[k+<span class="string">'_max'</span>]))</span><br><span class="line">                dist += (x_norm-y_norm)**2</span><br><span class="line"></span><br><span class="line">            dist = math.sqrt(dist) <span class="comment">## euclidean distance</span></span><br><span class="line"></span><br><span class="line">            data = &#123;</span><br><span class="line">                <span class="string">'artist_id'</span>: i[<span class="string">'artist_id'</span>],</span><br><span class="line">                <span class="string">'y_artist'</span>: j[<span class="string">'artist_id'</span>],</span><br><span class="line">                <span class="string">'distance'</span>: dist</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            insert_row(cursor, data, <span class="string">'related_artists'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    conn.commit()</span><br><span class="line">    cursor.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def normalize(x, x_min, x_max):</span><br><span class="line"></span><br><span class="line">    normalized = (x-x_min) / (x_max-x_min)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> normalized</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def query_athena(query, athena):</span><br><span class="line">    response = athena.start_query_execution(</span><br><span class="line">        QueryString=query,</span><br><span class="line">        QueryExecutionContext=&#123;</span><br><span class="line">            <span class="string">'Database'</span>: <span class="string">'default'</span></span><br><span class="line">        &#125;,</span><br><span class="line">        ResultConfiguration=&#123;</span><br><span class="line">            <span class="string">'OutputLocation'</span>: <span class="string">"s3://spotify-chatbot-project/athena-panomix-tables/"</span>,</span><br><span class="line">            <span class="string">'EncryptionConfiguration'</span>: &#123;</span><br><span class="line">                <span class="string">'EncryptionOption'</span>: <span class="string">'SSE_S3'</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line"><span class="comment"># 아래와 같이 Athena API는 response를 받았다고 해서 결과를 보여주는 것이 아니라 실행을 시킨 후에</span></span><br><span class="line"><span class="comment"># 해당 query id를 통해 결과를 가져오는 형식으로 이루어져 있다.</span></span><br><span class="line">def get_query_result(query_id, athena):</span><br><span class="line"></span><br><span class="line">    response = athena.get_query_execution(</span><br><span class="line">        QueryExecutionId=str(query_id)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">while</span> response[<span class="string">'QueryExecution'</span>][<span class="string">'Status'</span>][<span class="string">'State'</span>] != <span class="string">'SUCCEEDED'</span>:</span><br><span class="line">        <span class="keyword">if</span> response[<span class="string">'QueryExecution'</span>][<span class="string">'Status'</span>][<span class="string">'State'</span>] == <span class="string">'FAILED'</span>:</span><br><span class="line">            logging.error(<span class="string">'QUERY FAILED'</span>)</span><br><span class="line">            <span class="built_in">break</span></span><br><span class="line">        time.sleep(5)</span><br><span class="line">        response = athena.get_query_execution(</span><br><span class="line">            QueryExecutionId=str(query_id)</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># 중요한 점은 MaxResults가 1000이 Max라는 점이다.</span></span><br><span class="line">    response = athena.get_query_results(</span><br><span class="line">        QueryExecutionId=str(query_id)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def process_data(results):</span><br><span class="line"></span><br><span class="line">    columns = [col[<span class="string">'Label'</span>] <span class="keyword">for</span> col <span class="keyword">in</span> results[<span class="string">'ResultSet'</span>][<span class="string">'ResultSetMetadata'</span>][<span class="string">'ColumnInfo'</span>]]</span><br><span class="line"></span><br><span class="line">    listed_results = []</span><br><span class="line">    <span class="keyword">for</span> res <span class="keyword">in</span> results[<span class="string">'ResultSet'</span>][<span class="string">'Rows'</span>][1:]:</span><br><span class="line">        values = []</span><br><span class="line">        <span class="keyword">for</span> field <span class="keyword">in</span> res[<span class="string">'Data'</span>]:</span><br><span class="line">            try:</span><br><span class="line">                values.append(list(field.values())[0])</span><br><span class="line">            except:</span><br><span class="line">                values.append(list(<span class="string">' '</span>))</span><br><span class="line">        listed_results.append(dict(zip(columns, values)))</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> listed_results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def insert_row(cursor, data, table):</span><br><span class="line"></span><br><span class="line">    placeholders = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">    columns = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">    key_placeholders = <span class="string">', '</span>.join([<span class="string">'&#123;0&#125;=%s'</span>.format(k) <span class="keyword">for</span> k <span class="keyword">in</span> data.keys()])</span><br><span class="line">    sql = <span class="string">"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s"</span> % (table, columns, placeholders, key_placeholders)</span><br><span class="line">    cursor.execute(sql, list(data.values())*2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><ul><li>위의 파일을 실행시켜보자. 위의 python script 파일이 존재하는 path로 이동하여 아래 명령문을 실행시키면 실행에 완료될때까지 걸린 시간 또한 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">time python3 data_modeling.py</span><br><span class="line"></span><br><span class="line">real28m11.013s</span><br><span class="line">user1m36.141s</span><br><span class="line">sys0m24.518s</span><br></pre></td></tr></table></figure><ul><li>이제 MySQL에 접속해서 데이터가 제대로 insert 됬는지 확인해 보자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM related_artists LIMIT 20;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line">+------------------------+------------------------+-----------+</span><br><span class="line">| artist_id              | y_artist               | distance  |</span><br><span class="line">+------------------------+------------------------+-----------+</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 00FQb4jTyendYWaN8pK0wa |         0 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 01C9OoXDvCKkGcf735Tcfo |  0.366558 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 02rd0anEWfMtF7iMku9uor |  0.327869 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 02uYdhMhCgdB49hZlYRm9o |  0.595705 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 03r4iKL2g2442PT9n2UKsx |  0.632109 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 03YhcM6fxypfwckPCQV8pQ |  0.812604 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 04gDigrS5kc9YWfZHwBETP |  0.498764 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 04tBaW21jyUfeP5iqiKBVq |  0.322017 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 0543y7yrvny4KymoaneT4W |  0.365608 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 05E3NBxNMdnrPtxF9oraJm |  0.958604 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 06HL4z0CvFAxyc27GXpf02 |  0.483454 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 06nevPmNVfWUXyZkccahL8 | 0.0592581 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 06nsZ3qSOYZ2hPVIMcr1IN |   0.39567 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 085pc2PYOi8bGKj0PNjekA |  0.608243 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 08avsqaGIlK2x3i2Cu7rKH |  0.328059 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 09C0xjtosNAIXP36wTnWxd |  0.210568 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 0BvkDsjIUla7X0k6CSWh1I |  0.606556 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 0bvRYuXRvd14RYEE7c0PRW |  0.670187 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 0C0XlULifJtAgn6ZNCW2eu |   0.70478 |</span><br><span class="line">| 00FQb4jTyendYWaN8pK0wa | 0cc6vw3VN8YlIcvr1v7tBL |  0.716507 |</span><br><span class="line">+------------------------+------------------------+-----------+</span><br><span class="line">20 rows <span class="keyword">in</span> <span class="built_in">set</span> (0.01 sec)</span><br></pre></td></tr></table></figure><ul><li>JOIN을 통해 각각 이름을 볼수있게 해주면서 가장 distance가 작은 즉 유사성이 큰 데이터 순서로 보여주길 원해 아래와 같은 query를 작성하여 실행시켰다. 그 결과, audio_features로만 모델링을 했음에도 비슷한 장르의 아티스트가 묶여있음을 확인할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">SELECT p1.name, p2.name, p1.url, p2.url, p2.distance FROM artists p1 JOIN (SELECT t1.name, t1.url, t2.y_artist, t2.distance FROM artists t1 JOIN related_artists t2 ON t2.artist_id = t1.id) p2 ON p2.y_artist=p1.id WHERE distance != 0 ORDER BY p2.distance ASC LIMIT 20;</span><br><span class="line"></span><br><span class="line">+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+</span><br><span class="line">| name                          | name                          | url                                                    | url                                                    | distance  |</span><br><span class="line">+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+</span><br><span class="line">| Four Tops                     | Alan Jackson                  | https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4 | https://open.spotify.com/artist/4mxWe1mtYIYfP040G38yvS | 0.0241995 |</span><br><span class="line">| Alan Jackson                  | Four Tops                     | https://open.spotify.com/artist/4mxWe1mtYIYfP040G38yvS | https://open.spotify.com/artist/7fIvjotigTGWqjIz6EP1i4 | 0.0241995 |</span><br><span class="line">| Martha Reeves &amp; The Vandellas | Jimmy Ruffin                  | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | https://open.spotify.com/artist/0hF0PwB04hnXfYMiZWfJzy | 0.0258624 |</span><br><span class="line">| Jimmy Ruffin                  | Martha Reeves &amp; The Vandellas | https://open.spotify.com/artist/0hF0PwB04hnXfYMiZWfJzy | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | 0.0258624 |</span><br><span class="line">| George Harrison               | Martha Reeves &amp; The Vandellas | https://open.spotify.com/artist/7FIoB5PHdrMZVC3q2HE5MS | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | 0.0272287 |</span><br><span class="line">| Martha Reeves &amp; The Vandellas | George Harrison               | https://open.spotify.com/artist/1Pe5hlKMCTULjosqZ6KanP | https://open.spotify.com/artist/7FIoB5PHdrMZVC3q2HE5MS | 0.0272287 |</span><br><span class="line">| Nik Kershaw                   | Elton John                    | https://open.spotify.com/artist/7kCL98rPFsNKjAHDmWrMac | https://open.spotify.com/artist/3PhoLpVuITZKcymswpck5b | 0.0272474 |</span><br><span class="line">| Elton John                    | Nik Kershaw                   | https://open.spotify.com/artist/3PhoLpVuITZKcymswpck5b | https://open.spotify.com/artist/7kCL98rPFsNKjAHDmWrMac | 0.0272474 |</span><br><span class="line">| Tammi Terrell                 | Kim Carnes                    | https://open.spotify.com/artist/75jNCko3SnEMI5gwGqrbb8 | https://open.spotify.com/artist/5PN2aHIvLEM98XIorsPMhE | 0.0279891 |</span><br><span class="line">| Kim Carnes                    | Tammi Terrell                 | https://open.spotify.com/artist/5PN2aHIvLEM98XIorsPMhE | https://open.spotify.com/artist/75jNCko3SnEMI5gwGqrbb8 | 0.0279891 |</span><br><span class="line">| Roger Daltrey                 | Arcade Fire                   | https://open.spotify.com/artist/5odf7hjI7hyvAw66tmxhGF | https://open.spotify.com/artist/3kjuyTCjPG1WMFCiyc5IuB | 0.0291541 |</span><br><span class="line">| Arcade Fire                   | Roger Daltrey                 | https://open.spotify.com/artist/3kjuyTCjPG1WMFCiyc5IuB | https://open.spotify.com/artist/5odf7hjI7hyvAw66tmxhGF | 0.0291541 |</span><br><span class="line">| Billy Fury                    | Otis Redding                  | https://open.spotify.com/artist/7rtLZcKWGV4eaZsBwSKimf | https://open.spotify.com/artist/60df5JBRRPcnSpsIMxxwQm | 0.0292248 |</span><br><span class="line">| Otis Redding                  | Billy Fury                    | https://open.spotify.com/artist/60df5JBRRPcnSpsIMxxwQm | https://open.spotify.com/artist/7rtLZcKWGV4eaZsBwSKimf | 0.0292248 |</span><br><span class="line">| Katy Perry                    | John Fogerty                  | https://open.spotify.com/artist/6jJ0s89eD6GaHleKKya26X | https://open.spotify.com/artist/5ujCegv1BRbEPTCwQqFk6t | 0.0302168 |</span><br><span class="line">| John Fogerty                  | Katy Perry                    | https://open.spotify.com/artist/5ujCegv1BRbEPTCwQqFk6t | https://open.spotify.com/artist/6jJ0s89eD6GaHleKKya26X | 0.0302168 |</span><br><span class="line">| Dierks Bentley                | The Cadillac Three            | https://open.spotify.com/artist/7x8nK0m0cP2ksQf0mjWdPS | https://open.spotify.com/artist/1nivFfWu6oXBFDNyVfFU5x | 0.0313435 |</span><br><span class="line">| The Cadillac Three            | Dierks Bentley                | https://open.spotify.com/artist/1nivFfWu6oXBFDNyVfFU5x | https://open.spotify.com/artist/7x8nK0m0cP2ksQf0mjWdPS | 0.0313435 |</span><br><span class="line">| Sheryl Crow                   | Phil Collins                  | https://open.spotify.com/artist/4TKTii6gnOnUXQHyuo9JaD | https://open.spotify.com/artist/4lxfqrEsLX6N1N4OCSkILp | 0.0317203 |</span><br><span class="line">| Phil Collins                  | Sheryl Crow                   | https://open.spotify.com/artist/4lxfqrEsLX6N1N4OCSkILp | https://open.spotify.com/artist/4TKTii6gnOnUXQHyuo9JaD | 0.0317203 |</span><br><span class="line">+-------------------------------+-------------------------------+--------------------------------------------------------+--------------------------------------------------------+-----------+</span><br><span class="line">20 rows <span class="keyword">in</span> <span class="built_in">set</span> (1.18 sec)</span><br></pre></td></tr></table></figure><ul><li>이제 Facebook messenger API를 통해 챗봇을 서비스를 만들어 볼 것이다. 아래 그림과 같이 구글에서 Facebook messenger API를 검색하여 페이지로 접속한다.</li></ul><p><img src="/image/search_Facebook_messenger_API.png" alt="Facebook messenger API 검색"></p><ul><li>Facebook messenger API 웹페이지를 접속하면 아래 그림처럼 보이며, 그에 대한 작동원리에 대한 설명은 Introduction의 learn more를 클릭하면 두번째 그림과 같이 작동원리를 보여준다. 간단히 말하자면, User가 Facebook messenger API를 통해 질의하면 그 정보를 Business Server에 보내서 해당 질문에 따른 답변을 가져와 보여주는 형식이다.</li></ul><p><img src="/image/introduction_click_facebook_messenger_api.png" alt="Facebook messenger API 웹페이지"></p><p><img src="/image/Facebook_messenger_plattform_principal.png" alt="Facebook messenger API 작동방식"></p><ul><li>Facebook messenger API를 통해서는 다양한 방식의 답변을 제공할 수 있다. 이미 만들어져있는 UI/UX 템플릿들이 존재하기 때문에 원하는 형식에 맞춰 다양하게 서비스를 제공할 수 있다.</li></ul><p><img src="/image/Quick_answer_Facebook_messenger_api.png" alt="FaceBook messenger API의 Quick Answer"></p><p><img src="/image/Facebook_API_Messenge_template.png" alt="Facebook messenger의 message 템플릿"></p><ul><li><code>Facebook messenger API를 사용하기 위해서는 가장 먼저 페이지가 만들어져 있어야 한다.</code> 아래와 같이 새로운 페이지를 만들거나 이미 만들어져 있는 자신의 페이지를 먼저 등록시킨다.</li></ul><p><img src="/image/create_page_facebook_messenger_api_01.png" alt="페이지 생성 - 01"></p><p><img src="/image/create_page_facebook_messenger_api_02.png" alt="페이지 생성 - 02"></p><p><img src="/image/create_page_facebook_messenger_api_03.png" alt="페이지 생성 - 03"></p><p><img src="/image/create_page_facebook_messenger_api_04.png" alt="페이지 생성 - 04"></p><p><img src="/image/create_page_facebook_messenger_api_05.png" alt="페이지 생성 - 05"></p><ul><li>Lambda를 통해서 AWS와 Facebook messenger API를 연결해 볼 것이다. Lambda를 사용하는 이유는 지난번에 언급했던 것과 같이 EC2와 같이 서버를 항상 띄어놓고 정해진 resource를 통해 서비스를 관리하면 늘어나거나 줄어드는 User에 대해서 유연하게 처리하기 힘들기 때문이다. Lambda는 예를 들어 기하급수적으로 User가 늘더라도 그에따라 병렬적으로 작업하기 때문에 Traffic의 크기에 크게 영향을 받지 않는다. 반대로 EC2의 경우에는 해당 Traffic이 증가함에 따라 여러가지 장치를 구현해 놓아야한다. AWS에 로그인한 후 Lambda를 들어가서, 아래 그림과 같이 새로운 Lambda Function을 생성해 준다.</li></ul><p><img src="/image/new_create_lambda_function_aws_01.png" alt="Facebook messenger API를 사용하기 위한 Lambda Function 생성 - 01"></p><p><img src="/image/new_create_lambda_function_aws_02.png" alt="Facebook messenger API를 사용하기 위한 Lambda Function 생성 - 02"></p><p><img src="/image/new_create_lambda_function_aws_03.png" alt="Facebook messenger API를 사용하기 위한 Lambda Function 생성 - 03"></p><p><img src="/image/new_create_lambda_function_aws_04.png" alt="Facebook messenger API를 사용하기 위한 Lambda Function 생성 - 04"></p><ul><li>이제 위에서 만든 Lambda Function과 Facebook messenger API를 연결하기 위해서 AWS에서 API 관리 및 설정을 담당하는 API Gateway 페이지로 이동해서 새롭게 API Gate를 만들 것이다.</li></ul><p><img src="/image/AWS_api_gate_service.png" alt="AWS API Gateway service"></p><p><img src="/image/Amazon_API_Gateway.png" alt="AWS API Gateway 생성 - 01"></p><ul><li>필자는 REST API 방식으로 생성할 것이기 때문에 아래와 같이 설정하였으며, API이름도 설정해 주었다.</li></ul><p><img src="/image/Amazon_API_Gateway_creation_01.png" alt="AWS API Gateway 생성 - 02"></p><ul><li>그 다음은 Crawling할 때 한번씩 접해봤을 법한 GET, POST Method를 만들어 주는 과정을 거친다. 먼저 GET은 Integration type을 Lambda Function으로 설정해 주고, Lambda Function은 방금 만들어놓은 것을 사용할 것이다. 이렇게 설정한 뒤에 Integration Request 탭으로 이동하여 Mapping Templates의 Request body passthrough 아래와같이 설정하며, <code>mapping Templates에 application/json을 추가</code>해준다. Generate template을 Method Request passthrough로 설정한 후 최종적으로 save를 하여 GET method의 설정을 마친다. 필자가 사용할 서비스에서는 POST method는 Facebook API에 데이터를 주는 역할 밖에 없기 때문에 크게 설정할 것이 없다.</li></ul><p><img src="/image/Amazon_API_Gateway_creation_02.png" alt="AWS API Gateway 생성 - 03"></p><p><img src="/image/Amazon_API_Gateway_creation_03.png" alt="AWS API Gateway 생성 - 04"></p><p><img src="/image/Amazon_API_Gateway_creation_04.png" alt="AWS API Gateway 생성 - 05"></p><p><img src="/image/Amazon_API_Gateway_creation_05.png" alt="AWS API Gateway 생성 - 06"></p><p><img src="/image/Amazon_API_Gateway_creation_06.png" alt="AWS API Gateway 생성 - 07"></p><p><img src="/image/Amazon_API_Gateway_creation_07.png" alt="AWS API Gateway 생성 - 08"></p><p><img src="/image/Amazon_API_Gateway_creation_08.png" alt="AWS API Gateway 생성 - 09"></p><ul><li>GET, POST method를 다 설정했다면, 사용하기 위해서는 배포를 해야 할 것이다. 아래 그림과 같이 action버튼을 눌러 deploy api를 선택하여 stage를 새롭게 만들어주며, 이름을 설정한다.</li></ul><p><img src="/image/Action_deploy_API_AWS.png" alt="API Gateway 배포 - 01"></p><p><img src="/image/deplot_api_stage_name_AWS.png" alt="API Gateway 배포 - 02"></p><ul><li>deploy를 다 완료하게 되면, 아래 그림과 같은 화면이 나타날 것이다. 그 중 아래 빨간색 상자 안에 있는 invoke URL은 우리가 Facebook에 연결해 줄 endpoint 역할을 한다. 추후에 invoke URL 주소를 복사한 후에 아래 그림에서와 같이 Facebook에서 만들어 놓은 app의 콜백 url 추가를 눌러 추가해 줄 것이다.</li></ul><p><img src="/iamge/callback_ural_adding.png" alt="Facebook messenger API webhook url 추가"></p><ul><li>위의 그림 처럼 webhook url을 추가 해주려면 Lambda Function을 만들어 주어야 하는데 먼저 아래 그림에서와 토큰을 생성해서 복사한 후 Lambda Function을 아래 그림과 같이 작성해 준뒤에 webhook url을 추가해 줄 수 있다. 참고로 페이지 토큰은 Facebook app에서 page token을 생성하여 해당 값을 적어주고, verify token은 임으로 지정해주면 된다.</li></ul><p><img src="/image/page_token_access_create.png" alt="페이지 토큰 생성 - 01"></p><p><img src="/image/page_token_creation.png" alt="페이지 토큰 생성 - 02"></p><ul><li>아래와 같이 Lambda Function을 수정하는 이유는 <a href="https://developers.facebook.com/docs/messenger-platform/webhook#setup" target="_blank" rel="noopener">Facebook의 Webhook 사용법</a>을 살펴보면 알 수 있다.</li></ul><p><img src="/image/modify_lambda_Function_for_webhook_url.png" alt="webhook url 추가를 위한 Lambda Function 수정"></p><ul><li>위에서 지정한 verify token과 아래 그림에서 처럼 API Gateway를 클릭하여 이전에 invoke URL의 주소를 복사하여 아래 그림과 같이 webhook url을 추가해준다. 이렇게 하면 connection은 완료한 상태이다.</li></ul><p><img src="/image/API_Gateway_invoke_url.png" alt="invoke URL"></p><p><img src="/image/callback_url_adding_api.png" alt="callback URL 추가"></p><ul><li>이제 본격적으로 챗봇을 구현하기 위해 Lambda Function의 else 밑의 부분을 수정해 볼 것이다. 먼저 이전과 마찬가지로 Lambda Function은 S3에 올려 그 파일을 사용하기 위해 requirements.txt와 shell script를 포함하는 하나의 파일로 만들어 준다. 또한, AWS에서 S3에 새로운 bucket을 생성해준다. 필자는 아래와 같이 spotify-chat-bot이라는 이름으로 새롭게 bucket을 만들어 주었다.</li></ul><p><img src="/image/new_S3_bucket_creation_for_lambda_function.png" alt="새로운 S3 bucket 생성"></p><ul><li><p>전체적인 구조는 아래와 같다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chatbot</span><br><span class="line">├── deploy.sh</span><br><span class="line">├── lambda_handler.py</span><br><span class="line">└── requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>deploy.sh</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">rm -rf ./libs</span><br><span class="line">pip install -r requirements.txt -t ./libs</span><br><span class="line"></span><br><span class="line">rm *.zip</span><br><span class="line">zip spotify.zip -r *</span><br><span class="line"></span><br><span class="line">aws s3 rm s3://spotify-chat-bot/spotify.zip</span><br><span class="line">aws s3 cp ./spotify.zip s3://spotify-chat-bot/spotify.zip</span><br><span class="line">aws lambda update-function-code --<span class="keyword">function</span>-name spotify-lambda --s3-bucket spotify-chat-bot --s3-key spotify.zip</span><br></pre></td></tr></table></figure><ul><li><p>위와 같이 작성했다면 먼저 deploy.sh의 파일 권한을 바꿔준다. 모든 사용자(a)의 실행(x) 권한 추가(+)하여 준다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod +x deploy.sh</span><br></pre></td></tr></table></figure></li><li><p>requirements.txt</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">requests</span><br><span class="line">pymysql</span><br></pre></td></tr></table></figure><ul><li>lambda_hendler.py</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">import sys</span><br><span class="line">sys.path.append(<span class="string">'./libs'</span>)</span><br><span class="line">import logging</span><br><span class="line">import requests</span><br><span class="line">import pymysql</span><br><span class="line">import fb_bot</span><br><span class="line">import json</span><br><span class="line">import base64</span><br><span class="line">import boto3</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger()</span><br><span class="line">logger.setLevel(logging.INFO)</span><br><span class="line"></span><br><span class="line">PAGE_TOKEN = <span class="string">"your page token"</span></span><br><span class="line">VERIFY_TOKEN = <span class="string">"your verify code"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lambda_handler(event, context):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># event['params'] only exists for HTTPS GET</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'params'</span> <span class="keyword">in</span> event.keys():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> event[<span class="string">'params'</span>][<span class="string">'querystring'</span>][<span class="string">'hub.verify_token'</span>] == VERIFY_TOKEN:</span><br><span class="line">            <span class="built_in">return</span> int(event[<span class="string">'params'</span>][<span class="string">'querystring'</span>][<span class="string">'hub.challenge'</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            logging.error(<span class="string">'wrong validation token'</span>)</span><br><span class="line">            raise SystemExit</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        logger.info(event)</span><br></pre></td></tr></table></figure><ul><li>위와 같이 작성한 상태한 후 해당 파일이 존재하는 path에서 아래 shell script를 작동시킨다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./deploy.sh</span><br></pre></td></tr></table></figure><ul><li>AWS S3에서 해당 bucket을 확인해 보면 아래와 같이 spotify.zip 파일이 존재함을 확인할 수 있다.</li></ul><p><img src="/image/bucket_check_adding_file.png" alt="bucket 업로드 확인"></p><ul><li>다시 AWS Lambda Function으로 돌아가 해당 bucket과 연결을 시켜 볼 것이다.</li></ul><p><img src="/image/bucket_connect_with_Lambda.png" alt="Lambda Function에 bucket 파일 연결하기"></p><ul><li>Facebook App으로 돌아가서 아래 화면과 같이 필드를 추가해 주어야 한다.</li></ul><p><img src="/image/feed_add_facebook_api.png" alt="Facebook messenger API 필드 추가 - 01"></p><p><img src="/image/feed_add_facebook_api_01.png" alt="Facebook messenger API 필드 추가 - 02"></p><ul><li>이제 webhook으로 연결해 놓은 페이지로 접속하여 아래와 같이 버튼을 만들어 놓는다. 그 이유는 Lambda Function의 나머지 부분을 작성하기 위해서는 어떻게 event 구조가 구성되어져 있는지 확인해야 하기 때문이다.</li></ul><p><img src="/image/webhook_page_add_button_01.png" alt="webhook 페이지 버튼 추가 - 01"></p><p><img src="/image/webhook_page_add_button_02.png" alt="webhook 페이지 버튼 추가 - 02"></p><p><img src="/image/webhook_page_add_button_03.png" alt="webhook 페이지 버튼 추가 - 03"></p><ul><li>이제 AWS Lambda Function을 통해 어떻게 message가 들어오는지 확인하기 위해 아래 그림과 같이 page에서 버튼 테스트를 진행하고, 메세지는 간단하게 hello를 입력해보았다.</li></ul><p><img src="/image/button_test_01.png" alt="페이지 버튼 테스트 - 01"></p><p><img src="/image/button_test_02.png" alt="페이지 버튼 테스트 - 02"></p><ul><li>AWS CloudWatch에서 log를 살펴보면, 아래 그림과 같이 받아오는 것을 확인 할 수 있다. 아래 빨간색 상자안의 key 값 중 recipient는 해당 페이지의 id이며, sender의 id는 Facebook User의 id이다. 고유의 값은 아니고 각 페이지에 각 User에 대한 id이므로 동일한 User가 다른 페이지에서 요청을 했다면, 다른 id를 갖는다.</li></ul><p><img src="/image/button_test_03.png" alt="페이지 버튼 테스트 - 03"></p><ul><li>app을 관리할 수 있는 python script 파일을 fb_bot.py라는 이름으로 작성해 주었다. 이 파일 또한 위의 lambda function내에 존재할 수 있도록 path를 잡아주어야 한다. Facebook app은 <a href="https://developers.facebook.com/docs/graph-api/using-graph-api" target="_blank" rel="noopener">graph Facebook API</a>를 통해서 control할 수 있다. 아래 패키지 중 <a href="https://python.flowdas.com/library/enum.html" target="_blank" rel="noopener">Enum</a>은 고유한 이름 집합과 값을 정의하는 데 사용할 수 있는 네 가지 열거형 클래스를 정의하는데 사용되어 진다. 아래함수에서 for문을 통해 NotificationType을 작동시킨다면, NotificationType.REGULAR, NotificationType.SILENT_PUSH, NotificationType.no_push 식으로 값이 프린트 된다. 아래 탬플릿에 맞는 형식은 <a href="https://developers.facebook.com/docs/messenger-platform/send-messages/templates" target="_blank" rel="noopener">Facebook messenger API</a>에서 확인할 수 있다.</li></ul><p><img src="/image/send_API_messenger.png" alt="fb_bot.py에 사용된 템플릿 - 01"></p><ul><li>fb_bot.py</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"></span><br><span class="line">import sys</span><br><span class="line">sys.path.append(<span class="string">"./libs"</span>)</span><br><span class="line">import os</span><br><span class="line">import requests</span><br><span class="line">import base64</span><br><span class="line">import json</span><br><span class="line">import logging</span><br><span class="line">from enum import Enum</span><br><span class="line"></span><br><span class="line">DEFAULT_API_VERSION = 6.0</span><br><span class="line"></span><br><span class="line"><span class="comment">## messaging types: "RESPONSE", "UPDATE", "MESSAGE_TAG"</span></span><br><span class="line"></span><br><span class="line">class NotificationType(Enum):</span><br><span class="line">    regular = <span class="string">"REGULAR"</span></span><br><span class="line">    silent_push = <span class="string">"SILENT_PUSH"</span></span><br><span class="line">    no_push = <span class="string">"no_push"</span></span><br><span class="line"></span><br><span class="line">class Bot:</span><br><span class="line"></span><br><span class="line">    def __init__(self, access_token, **kwargs):</span><br><span class="line"></span><br><span class="line">        self.access_token = access_token</span><br><span class="line">        self.api_version = kwargs.get(<span class="string">'api_version'</span>) or DEFAULT_API_VERSION</span><br><span class="line">        self.graph_url = <span class="string">'https://graph.facebook.com/v&#123;0&#125;'</span>.format(self.api_version)</span><br><span class="line"></span><br><span class="line">    @property</span><br><span class="line">    def auth_args(self):</span><br><span class="line">        <span class="keyword">if</span> not hasattr(self, <span class="string">'_auth_args'</span>):</span><br><span class="line">            auth = &#123;</span><br><span class="line">                <span class="string">'access_token'</span>: self.access_token</span><br><span class="line">            &#125;</span><br><span class="line">            self._auth_args = auth</span><br><span class="line">        <span class="built_in">return</span> self._auth_args</span><br><span class="line"></span><br><span class="line">    def send_message(self, recipient_id, payload, notification_type, messaging_type, tag):</span><br><span class="line"></span><br><span class="line">        payload[<span class="string">'recipient'</span>] = &#123;</span><br><span class="line">            <span class="string">'id'</span>: recipient_id</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">#payload['notification_type'] = notification_type</span></span><br><span class="line">        payload[<span class="string">'messaging_type'</span>] = messaging_type</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> tag is not None:</span><br><span class="line">            payload[<span class="string">'tag'</span>] = tag</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/messages'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.post(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args,</span><br><span class="line">            json = payload</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        logging.info(payload)</span><br><span class="line">        <span class="built_in">return</span> response.json()</span><br><span class="line"></span><br><span class="line">    def send_text(self, recipient_id, text, notification_type = NotificationType.regular, messaging_type = <span class="string">'RESPONSE'</span>, tag = None):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> self.send_message(</span><br><span class="line">            recipient_id,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"message"</span>: &#123;</span><br><span class="line">                    <span class="string">"text"</span>: text</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            notification_type,</span><br><span class="line">            messaging_type,</span><br><span class="line">            tag</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def send_quick_replies(self, recipient_id, text, quick_replies, notification_type = NotificationType.regular, messaging_type = <span class="string">'RESPONSE'</span>, tag = None):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> self.send_message(</span><br><span class="line">            recipient_id,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"message"</span>:&#123;</span><br><span class="line">                    <span class="string">"text"</span>: text,</span><br><span class="line">                    <span class="string">"quick_replies"</span>: quick_replies</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            notification_type,</span><br><span class="line">            messaging_type,</span><br><span class="line">            tag</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def send_attachment(self, recipient_id, attachment_type, payload, notification_type = NotificationType.regular, messaging_type = <span class="string">'RESPONSE'</span>, tag = None):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> self.send_message(</span><br><span class="line">            recipient_id,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"message"</span>: &#123;</span><br><span class="line">                    <span class="string">"attachment"</span>:&#123;</span><br><span class="line">                        <span class="string">"type"</span>: attachment_type,</span><br><span class="line">                        <span class="string">"payload"</span>: payload</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            notification_type,</span><br><span class="line">            messaging_type,</span><br><span class="line">            tag</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def send_action(self, recipient_id, action, notification_type = NotificationType.regular, messaging_type = <span class="string">'RESPONSE'</span>, tag = None):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> self.send_message(</span><br><span class="line">            recipient_id,</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">"sender_action"</span>: action</span><br><span class="line">            &#125;,</span><br><span class="line">            notification_type,</span><br><span class="line">            messaging_type,</span><br><span class="line">            tag</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def whitelist_domain(self, domain_list, domain_action_type):</span><br><span class="line"></span><br><span class="line">        payload = &#123;</span><br><span class="line">            <span class="string">"setting_type"</span>: <span class="string">"domain_whitelisting"</span>,</span><br><span class="line">            <span class="string">"whitelisted_domains"</span>: domain_list,</span><br><span class="line">            <span class="string">"domain_action_type"</span>: domain_action_type</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/thread_settings'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.post(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args,</span><br><span class="line">            json = payload</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response.json()</span><br><span class="line"></span><br><span class="line">    def set_greeting(self, template):</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/thread_settings'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.post(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args,</span><br><span class="line">            json = &#123;</span><br><span class="line">                <span class="string">"setting_type"</span>: <span class="string">"greeting"</span>,</span><br><span class="line">                <span class="string">"greeting"</span>: &#123;</span><br><span class="line">                    <span class="string">"text"</span>: template</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line">    def set_get_started(self, text):</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/messenger_profile'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.post(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args,</span><br><span class="line">            json = &#123;</span><br><span class="line">                <span class="string">"get_started"</span>:&#123;</span><br><span class="line">                    <span class="string">"payload"</span>: text</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line">    def get_get_started(self):</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/messenger_profile?fields=get_started'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.get(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line">    def get_messenger_profile(self, field):</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/messenger_profile?fields=&#123;1&#125;'</span>.format(self.graph_url, field)</span><br><span class="line"></span><br><span class="line">        response = requests.get(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def upload_attachment(self, url):</span><br><span class="line"></span><br><span class="line">        request_endpoint = <span class="string">'&#123;0&#125;/me/message_attachments'</span>.format(self.graph_url)</span><br><span class="line"></span><br><span class="line">        response = requests.post(</span><br><span class="line">            request_endpoint,</span><br><span class="line">            params = self.auth_args,</span><br><span class="line">            json = &#123;</span><br><span class="line">                <span class="string">"message"</span>:&#123;</span><br><span class="line">                    <span class="string">"attachment"</span>:&#123;</span><br><span class="line">                        <span class="string">"type"</span>: <span class="string">"image"</span>,</span><br><span class="line">                        <span class="string">"payload"</span>: &#123;</span><br><span class="line">                            <span class="string">"is_reusable"</span>: True,</span><br><span class="line">                            <span class="string">"url"</span>: url</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> response</span><br></pre></td></tr></table></figure><ul><li><p>이제 위의 fb_bot.py를 import하여 lambda_hendler.py 파일을 아래와 같이 수정해 주었다.</p></li><li><p>lambda_hendler.py</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">import sys</span><br><span class="line">sys.path.append(<span class="string">'./libs'</span>)</span><br><span class="line">import logging</span><br><span class="line">import requests</span><br><span class="line">import pymysql</span><br><span class="line">import fb_bot</span><br><span class="line">import json</span><br><span class="line">import base64</span><br><span class="line">import boto3</span><br><span class="line"></span><br><span class="line">logger = logging.getLogger()</span><br><span class="line">logger.setLevel(logging.INFO)</span><br><span class="line"></span><br><span class="line">client_id = <span class="string">"Your Spotify ID"</span></span><br><span class="line">client_secret = <span class="string">"Your Spotify PW"</span></span><br><span class="line"></span><br><span class="line">PAGE_TOKEN = <span class="string">"Your Page Token"</span></span><br><span class="line">VERIFY_TOKEN = <span class="string">"Your verify token"</span></span><br><span class="line"></span><br><span class="line">host = <span class="string">"Your RDS End point"</span></span><br><span class="line">port = 3306</span><br><span class="line">username = <span class="string">"Your RDS ID"</span></span><br><span class="line">database = <span class="string">"Using RDS table name"</span></span><br><span class="line">password = <span class="string">"Your RDS PW"</span></span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    conn = pymysql.connect(host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=<span class="string">'utf8'</span>)</span><br><span class="line">    cursor = conn.cursor()</span><br><span class="line">except:</span><br><span class="line">    logging.error(<span class="string">"could not connect to rds"</span>)</span><br><span class="line">    sys.exit(1)</span><br><span class="line"></span><br><span class="line">bot = fb_bot.Bot(PAGE_TOKEN)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lambda_handler(event, context):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># event['params'] only exists for HTTPS GET</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'params'</span> <span class="keyword">in</span> event.keys():</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> event[<span class="string">'params'</span>][<span class="string">'querystring'</span>][<span class="string">'hub.verify_token'</span>] == VERIFY_TOKEN:</span><br><span class="line">            <span class="built_in">return</span> int(event[<span class="string">'params'</span>][<span class="string">'querystring'</span>][<span class="string">'hub.challenge'</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            logging.error(<span class="string">'wrong validation token'</span>)</span><br><span class="line">            raise SystemExit</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        messaging = event[<span class="string">'entry'</span>][0][<span class="string">'messaging'</span>][0]</span><br><span class="line">        user_id = messaging[<span class="string">'sender'</span>][<span class="string">'id'</span>]</span><br><span class="line"></span><br><span class="line">        logger.info(messaging)</span><br><span class="line">        artist_name = messaging[<span class="string">'message'</span>][<span class="string">'text'</span>]</span><br><span class="line"></span><br><span class="line">        query = <span class="string">"SELECT image_url, url FROM artists WHERE name = '&#123;&#125;'"</span>.format(artist_name)</span><br><span class="line">        cursor.execute(query)</span><br><span class="line">        raw = cursor.fetchall()</span><br><span class="line">        <span class="comment"># raw가 0인 경우는 DB안에는 존재하지 않으므로 Spotify API에서 직접 search한다.</span></span><br><span class="line">        <span class="keyword">if</span> len(raw) == 0:</span><br><span class="line">            text = search_artist(cursor, artist_name)</span><br><span class="line">            bot.send_text(user_id, text)</span><br><span class="line">            sys.exit(0)</span><br><span class="line"></span><br><span class="line">        image_url, url = raw[0]</span><br><span class="line"></span><br><span class="line">        payload = &#123;</span><br><span class="line">            <span class="string">'template_type'</span>: <span class="string">'generic'</span>,</span><br><span class="line">            <span class="string">'elements'</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">'title'</span>: <span class="string">"Artist Info: '&#123;&#125;'"</span>.format(artist_name),</span><br><span class="line">                    <span class="string">'image_url'</span>: image_url,</span><br><span class="line">                    <span class="string">'subtitle'</span>: <span class="string">'information'</span>,</span><br><span class="line">                    <span class="string">'default_action'</span>: &#123;</span><br><span class="line">                        <span class="string">'type'</span>: <span class="string">'web_url'</span>,</span><br><span class="line">                        <span class="string">'url'</span>: url,</span><br><span class="line">                        <span class="string">'webview_height_ratio'</span>: <span class="string">'full'</span></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        bot.send_attachment(user_id, <span class="string">"template"</span>, payload)</span><br><span class="line"></span><br><span class="line">        query = <span class="string">"SELECT t2.genre FROM artists t1 JOIN artist_genres t2 ON t2.artist_id = t1.id WHERE t1.name = '&#123;&#125;'"</span>.format(artist_name)</span><br><span class="line"></span><br><span class="line">        cursor.execute(query)</span><br><span class="line">        genres = []</span><br><span class="line">        <span class="keyword">for</span> (genre, ) <span class="keyword">in</span> cursor.fetchall():</span><br><span class="line">            genres.append(genre)</span><br><span class="line"></span><br><span class="line">        text = <span class="string">"Here are genres of &#123;&#125;"</span>.format(artist_name)</span><br><span class="line">        bot.send_text(user_id, text)</span><br><span class="line">        bot.send_text(user_id, <span class="string">', '</span>.join(genres))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">## 만약에 아티스트가 없을시에는 아티스트 추가</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## Spotify API hit --&gt; Artist Search</span></span><br><span class="line">        <span class="comment">## Database Upload</span></span><br><span class="line">        <span class="comment">## One second</span></span><br><span class="line">        <span class="comment">## 오타 및 아티스트가 아닐 경우</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_headers(client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    endpoint = <span class="string">"https://accounts.spotify.com/api/token"</span></span><br><span class="line">    encoded = base64.b64encode(<span class="string">"&#123;&#125;:&#123;&#125;"</span>.format(client_id, client_secret).encode(<span class="string">'utf-8'</span>)).decode(<span class="string">'ascii'</span>)</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Basic &#123;&#125;"</span>.format(encoded)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">"grant_type"</span>: <span class="string">"client_credentials"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    r = requests.post(endpoint, data=payload, headers=headers)</span><br><span class="line"></span><br><span class="line">    access_token = json.loads(r.text)[<span class="string">'access_token'</span>]</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Bearer &#123;&#125;"</span>.format(access_token)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> headers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def insert_row(cursor, data, table):</span><br><span class="line"></span><br><span class="line">    placeholders = <span class="string">', '</span>.join([<span class="string">'%s'</span>] * len(data))</span><br><span class="line">    columns = <span class="string">', '</span>.join(data.keys())</span><br><span class="line">    key_placeholders = <span class="string">', '</span>.join([<span class="string">'&#123;0&#125;=%s'</span>.format(k) <span class="keyword">for</span> k <span class="keyword">in</span> data.keys()])</span><br><span class="line">    sql = <span class="string">"INSERT INTO %s ( %s ) VALUES ( %s ) ON DUPLICATE KEY UPDATE %s"</span> % (table, columns, placeholders, key_placeholders)</span><br><span class="line">    cursor.execute(sql, list(data.values())*2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 추가적으로 S3의 top_tracks에도 업데이트해주기 위해서 top-tracks lambda function을 실행시켜주는 함수이다.</span></span><br><span class="line"><span class="comment"># payload 부분은 lambda_handler 함수 안에서 들어오는 event에 관한 부분이다.</span></span><br><span class="line">def invoke_lambda(fxn_name, payload, invocation_type=<span class="string">'Event'</span>):</span><br><span class="line"></span><br><span class="line">    lambda_client = boto3.client(<span class="string">'lambda'</span>)</span><br><span class="line"></span><br><span class="line">    invoke_response = lambda_client.invoke(</span><br><span class="line">        FunctionName = fxn_name,</span><br><span class="line">        InvocationType = invocation_type,</span><br><span class="line">        Payload = json.dumps(payload)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> invoke_response[<span class="string">'StatusCode'</span>] not <span class="keyword">in</span> [200, 202, 204]:</span><br><span class="line">        logging.error(<span class="string">"ERROR: Invoking lmabda function: '&#123;0&#125;' failed"</span>.format(fxn_name))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> invoke_response</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def search_artist(cursor, artist_name):</span><br><span class="line"></span><br><span class="line">    headers = get_headers(client_id, client_secret)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## Spotify Search API</span></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">"q"</span>: artist_name,</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"artist"</span>,</span><br><span class="line">        <span class="string">"limit"</span>: <span class="string">"1"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    r = requests.get(<span class="string">"https://api.spotify.com/v1/search"</span>, params=params, headers=headers)</span><br><span class="line"></span><br><span class="line">    raw = json.loads(r.text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> raw[<span class="string">'artists'</span>][<span class="string">'items'</span>] == []:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">"Could not find artist. Please Try Again!"</span></span><br><span class="line"></span><br><span class="line">    artist = &#123;&#125;</span><br><span class="line">    artist_raw = raw[<span class="string">'artists'</span>][<span class="string">'items'</span>][0]</span><br><span class="line">    <span class="keyword">if</span> artist_raw[<span class="string">'name'</span>] == params[<span class="string">'q'</span>]:</span><br><span class="line"></span><br><span class="line">        artist.update(</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="string">'id'</span>: artist_raw[<span class="string">'id'</span>],</span><br><span class="line">                <span class="string">'name'</span>: artist_raw[<span class="string">'name'</span>],</span><br><span class="line">                <span class="string">'followers'</span>: artist_raw[<span class="string">'followers'</span>][<span class="string">'total'</span>],</span><br><span class="line">                <span class="string">'popularity'</span>: artist_raw[<span class="string">'popularity'</span>],</span><br><span class="line">                <span class="string">'url'</span>: artist_raw[<span class="string">'external_urls'</span>][<span class="string">'spotify'</span>],</span><br><span class="line">                <span class="string">'image_url'</span>: artist_raw[<span class="string">'images'</span>][0][<span class="string">'url'</span>]</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> artist_raw[<span class="string">'genres'</span>]:</span><br><span class="line">            <span class="keyword">if</span> len(artist_raw[<span class="string">'genres'</span>]) != 0:</span><br><span class="line">                insert_row(cursor, &#123;<span class="string">'artist_id'</span>: artist_raw[<span class="string">'id'</span>], <span class="string">'genre'</span>: i&#125;, <span class="string">'artist_genres'</span>)</span><br><span class="line"></span><br><span class="line">        insert_row(cursor, artist, <span class="string">'artists'</span>)</span><br><span class="line">        conn.commit()</span><br><span class="line">        r = invoke_lambda(<span class="string">'top-tracks'</span>, payload=&#123;<span class="string">'artist_id'</span>: artist_raw[<span class="string">'id'</span>]&#125;)</span><br><span class="line">        <span class="built_in">print</span>(r)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> <span class="string">"We added artist. Please try again in a second!"</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> <span class="string">"Could not find artist. Please Try Again!"</span></span><br></pre></td></tr></table></figure><ul><li>위에서 다른 trigger lambda function을 참조할 수 있도록 설정해 주었기 때문에 아래와 같이 role을 다른 lambda function을 invoke 할 수 있도록 IAM 페이지에서 permission을 추가해 주어야 정상적으로 작동된다.</li></ul><p><img src="/image/IAM_page_goto.png" alt="IAM 페이지로 이동"></p><p><img src="/image/add_permission_for_invoke.png" alt="IAM 페이지에서 invoke를 위한 permission 추가"></p><ul><li>이제 해당 페이지의 test를 진행해 보면 아래와 같이 위에서 설정한 것 처럼 해당아티스트의 url과 장르를 보내주는 것을 확인 할 수 있다. 그림은 없지만 필자는 2PM도 검색해보았는데, 잘 검색되어 나왔다.</li></ul><p><img src="/image/BTS_find_my_chat_bot.png" alt="BTS 검색 결과"></p><ul><li>이제껏 data engineering에 관한 몇가지 기초적인 부분들을 실습해 보며, data engineer는 상황에 맞춰 resource를 사용할 수 있도록 선택과 집중을 해야 한다고 생각했다. 그러한, 상황에 맞는 선택과 집중을 위해 해당 비즈니스가 처해있는 상황과 단계를 잘 진단하고 깊게 알고있어야 할 것 같다는 생각을 하게 되는 프로젝트 였다.</li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/03/03/data_engineering_10/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering (데이터 파이프라인 자동화)</title>
      <link>https://heung-bae-lee.github.io/2020/03/01/data_engineering_09/</link>
      <guid>https://heung-bae-lee.github.io/2020/03/01/data_engineering_09/</guid>
      <pubDate>Sun, 01 Mar 2020 04:49:06 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;데이터-워크-플로우&quot;&gt;&lt;a href=&quot;#데이터-워크-플로우&quot; class=&quot;headerlink&quot; title=&quot;데이터 워크 플로우&quot;&gt;&lt;/a&gt;데이터 워크 플로우&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;이전에도 언급했었듯이 데이터 파이프라인은 아래와 같은 서비
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="데이터-워크-플로우"><a href="#데이터-워크-플로우" class="headerlink" title="데이터 워크 플로우"></a>데이터 워크 플로우</h1><ul><li>이전에도 언급했었듯이 데이터 파이프라인은 아래와 같은 서비스들을 S3에 모아 Athena같은 서비스로 분석해준 뒤 그 결과를 저장해놓은 일련의 데이터 작업의 흐름을 일컫는다.</li></ul><p><img src="/image/S3_data_pipe_line_exaple.png" alt="데이터 파이프 라인 예시"></p><ul><li>하나의 job이 시작되거나 어떠한 event에 trigger가 됬을때, 또 다른 job으로 연결이 되는 이런 정보들을 <code>DAGs(Directed Acyclic Graphs)</code>라고 부른다.</li></ul><p><img src="/image/DAGs.png" alt="DAG"></p><h1 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h1><ul><li>보통은 Extract -&gt; Transform -&gt; Load순으로 작업을 해 왔지만, 최근에는 Extract -&gt; Load -&gt; Transform 순으로 작업을 하기도 한다. 데이터 파이프 라인의 연장선이다. 하나의 예시를 들자면, 하루의 정해진 시간에 의한 스케쥴링인 Amazon CloudWatch Event와 Automation Script를 통해서 machine이 시작하면, AWS안에 AWS Step Functions는 각 과정에서 그 다음과정으로의 연결에 대한 여러가지 경우의 수에 대한 룰을 정해놓는 서비스로 쉽게 말하면, 임의의 단계에서 fail이 일어나면 어떤 event를 발생시켜야 하고, success를 하면 어떤 event를 발생시켜야 하는지를 관리할 수 있도록 도와주는 서비스이다. 이런 Step Function안의 ETL Flow state machine이 시작하고, 이후에는 다양한 job들이 작동하게 된다. 이러한 ETL job들의 log를 CloudWatch에 저장을 하고, 아래와 같은 Flow를 갖게된다.</li></ul><p><img src="/image/Extract_Transform_Load_example.png" alt="ETL 예시"></p><p><img src="/image/ETL_more_examples.png" alt="ETL 예시 - 01"></p><ul><li>AWS의 Step function에 관해 조금 더 말하자면, 아래 그림과 같이 사용할 수 있다. start가 되면 job이 submit이 되고, job이 finish될때 까지 기다려 줄 수 있게끔 Wait Seconds를 사용할 수도 있다. 왜냐하면, 예를 들어 Athena는 어느 정도 빅데이터를 처리하는 시스템이기 때문에 MySQL이나 PostgreSQL보다는 느린 부분이 있다. 이런 경우 위와 같이 time sleep을 통해 python script를 잠깐 멈춰두고 그 다음에 해당 시간이 지났을때 그 query에 대한 결과들을 가져올 수 있다. 이후에는 다시 job status를 받고 job이 끝났는지 아닌지에 따라 작업을 진행하는 flow를 볼 수 있다. 이런 service들이 없었을 때는 하나하나 monitoring을 통해서 수동으로 관리를 해야 했다.</li></ul><p><img src="/image/AWS_step_function_example.png" alt="AWS step function의 예시"></p><ul><li><p>AWS Glue가 가장 좋은 부분은 이전에는 MySQL 같은 경우에는 만들어 놓은 Schema에 맞춰서 data를 insert하였는데, 이제는 data가 너무나 방대해지고 형식도 다 다른데, 이런 것들을 통합하는는 다 Glue한다는 의미의 서비스라는 점이다. 가장 많이 쓰여지는 부분 중에 하나가 Crawler인데 <code>Crawler를 사용하면 자동으로 해당 data를 Crwaling해서 data가 어떤 형식인지에 대해서 지속적으로 Schema 관리가 들어가는 부분</code>이 있다. 그러므로 <code>data 양도 너무나 많고 column도 너무나 많은데 column이 변하는 경우도 있을 경우에 사용하면 좋다.</code></p></li><li><p>AWS Glue 페이지를 보면 아래 그림과 같이 table과 ETL, Trigger등 다양한 작업을 할 수 있다. 한 가지 예시로 S3에 저장해놓은 python Script를 Jobs 탭에서 바로 수정가능하며, Trigger들도 등록해서 관리 할 수 있다.</p></li></ul><p><img src="/iamge/AWS_Glue_example_one.png" alt="AWS Glue"></p><ul><li>해당 job들은 step function이나 Glue를 통해 관리를 하거나, EC2에서 Crontab으로 스케쥴링의 변화를 통해서 관리를 하는 등 다양한 방법으로 관리를 하지만 아래와 같이 서비스들의 지속적인 monitoring을 통해 cost를 효율적으로 사용할 선택과 집중을 해야 할 것이다. 어떤 부분까지 monitoring을 할 것인지에 대해 선택하여 집중하는 것이다.  </li></ul><p><img src="/image/AWS_RDS_monitoring_example.png" alt="RDS monitoring 예시"></p><p><img src="/image/AWS_EC2_monitoring_example.png" alt="EC2 monitoring 예시"></p><h2 id="Crontab이란"><a href="#Crontab이란" class="headerlink" title="Crontab이란?"></a>Crontab이란?</h2><ul><li><p>여러가지 만들어진 data job들을 우리가 작성한 해당 코드를 특정한 시간이나 하루에 한번 일주일에 한번이 됐건 어떠한 스케쥴링 방법을 통해서 지속적으로 작동시키려고 할때 사용한다. <a href="https://www.adminschoice.com/crontab-quick-reference" target="_blank" rel="noopener">Crontab Quick reference</a></p></li><li><p>이러한 스케쥴링을 하려면 규칙에의한 명령이 존재할 것임을 눈치챘을 것이다. 아래 그림과 같이 모두 ‘*‘이면  계속 1분마다 작동하라는 의미이며, 순서대로 분, 시간, 일, 월, 요일 순으로 지정할 수 있다.</p></li></ul><p><img src="/image/Crontab_rules.png" alt="Crontab 규칙"></p><ul><li>한가지 간단한 예로는 아래와 같이 Crontab을 실행하면 매일 오후 6시 30분에 /home/someuser/tmp안의 모든 파일을 제거하라는 job을 스케쥴링할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">30 18 * * * rm /home/someuser/tmp/*</span><br></pre></td></tr></table></figure><ul><li>이제 Crontab을 실습해 보기 위해 EC2 서버를 하나 AWS에서 생성해 볼 것이다. 먼저 service 탭에서 EC2를 클릭하여 아래 그림과 같이 EC2 service 페이지로 이동한 후 다시 instance 탭으로 이동한다. Launch instance 버튼을 클릭하면 생성할 instance의 환경을 설정하는 페이지로 이동하게된다.</li></ul><p><img src="/image/AWS_EC2_create.png" alt="EC2 instance - 01"></p><ul><li>Crontab 실습을 위한 서버이기 때문에 사양이 좋은 것을 고르진 않을 것이다. 필자는 Free tier만 사용가능한 2번째 사양을 고를 것이다. 다만, 실무에서 사용할 경우는 각자의 상황에 맞는 서버의 크기와 성능을 골라서 사용해야 할 것이다.</li></ul><p><img src="/image/AWS_EC2_create_01.png" alt="EC2 instance 생성 - 02"></p><ul><li>필자는 Free Tier만 가능한 t2.micro type을 선택하였다.</li></ul><p><img src="/image/AWS_EC2_create_02.png" alt="EC2 instance 생성 - 03"></p><ul><li>Configure instance 탭에서는 개수를 지정하는 등 instance에 대한 설정을 하는 탭인데, 필자는 default값을 사용하기로 생각해서 아무런 설정도 하지않고 넘어갔다.</li></ul><p><img src="/image/AWS_EC2_create_03.png" alt="EC2 instance 생성 - 04"></p><ul><li>Add storage 탭은 말 그대로 저장 성능을 설정하는 탭으로 필자는 이부분도 별다른 설정없이 기본값으로 하고 넘어갔다.</li></ul><p><img src="/image/AWS_EC2_create_04.png" alt="EC2 instance 생성 - 05"></p><ul><li>tag를 설정하는 탭이며, 필자는 생략하였다.</li></ul><p><img src="/image/AWS_EC2_create_05.png" alt="EC2 instance 생성 - 06"></p><ul><li>Configure Security Group 탭은 Security Group의 설정에 관한 탭이며, 새로 만들수도 있고, 이전에 생성되어있는 그룹을 사용해도 무방하다. 허나, 동일한 inbound 규칙을 사용하지 않는다면 새롭게 생성해서 사용하는 것이 좋다. 필자는 새롭게 만들어서 사용할 것이다. 또한, 접근을 ssh로 할 것이므로 아래와 같이 설정한다.</li></ul><p><img src="/image/AWS_EC2_create_06.png" alt="EC2 instance 생성 - 07"></p><ul><li>마지막으로 Review 탭은 이제까지 설정한 모든 사항을 점검하고 마지막으로 접속시 사용할 key pair를 어떤것으로 할지 정해주고 나면 모든 과정이 끝이 난다.</li></ul><p><img src="/image/AWS_EC2_create_07.png" alt="EC2 instance 생성 - 08"></p><ul><li>이제 다시 EC2의 instance 탭을 살펴보면, 새롭게 EC2 서버가 생성된 것을 확인 할 수 있다. 이제 접속을 해볼 것인데, 아래 그림에서 처럼 자신의 pem 파일이 존재하는 path에서 public DNS 서버 주소를 같이 입력하고 접속하면 된다.</li></ul><p><img src="/image/AWS_EC2_create_08.png" alt="EC2 instance 생성 - 09"></p><ul><li>아래와 같이 command를 실행하면 계속진행할 것이냐는 물음이 나올텐데 yes라고 하면 된다. 이는 앞으로 계속해서 접속하기위해 이 DNS를 추가할 것이냐는 물음이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -i pem_file_name.pem ec2-user@Public_DNS</span><br></pre></td></tr></table></figure><ul><li>이제 Crontab을 실행할 script 파일을 정해야 하는데, 필자는 이전에 만들어 두었던 script 파일 중에 top_tracks와 audio feature들에 대한 데이터를 S3에 parquet화하여 저장하게끔 코드를 작성한 파일을 사용할 것이다. 아래와 같이 필자의 pem 파일은 data_engineering이라는 파일 안에 존재한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">data_engineering</span><br><span class="line">├── Code</span><br><span class="line">│   ├── Chatbot\ Project</span><br><span class="line">│   │   ├── deploy.sh</span><br><span class="line">│   │   ├── fb_bot.py</span><br><span class="line">│   │   ├── lambda_handler.py</span><br><span class="line">│   │   └── requirements.txt</span><br><span class="line">│   ├── artist_list.csv</span><br><span class="line">│   ├── audio_features.parquet</span><br><span class="line">│   ├── create_artist_genres_table.py</span><br><span class="line">│   ├── create_artist_table.py</span><br><span class="line">│   ├── dynamodb_insert.py</span><br><span class="line">│   ├── select_dynamodb.py</span><br><span class="line">│   ├── spotify_S3.py</span><br><span class="line">│   ├── spotify_s3_artist.py</span><br><span class="line">│   ├── spotify_s3_make.py</span><br><span class="line">│   ├── top-tracks.parquet</span><br><span class="line">│   </span><br><span class="line">├── Slide</span><br><span class="line">├── foxyproxy-settings.xml</span><br><span class="line">└── spotift_chatbot.pem</span><br></pre></td></tr></table></figure><ul><li>위와 같은 구조로 되어있으므로 사용할 spotify_s3_make.py을 생성한 EC2 Server로 옮겨 줄 것이다. <code>scp은 서버로 파일을 copy하는 것이라고 생각하면 된다. 단,아래 명령어는 이미 EC2에 접속한 상태가 아닌 로컬에서 진행하여야한다.</code> 아래와 같이 옮겨진것을 볼 수 있고, EC2 서버로 접속하여 확인가능하다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scp -i spotift_chatbot.pem ./Code/spotify_s3_make.py ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.com:~/</span><br><span class="line">spotify_s3_make.py                            100% 5411   536.2KB/s   00:00</span><br><span class="line"></span><br><span class="line">ssh -i spotift_chatbot.pem ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.com</span><br><span class="line">ls</span><br></pre></td></tr></table></figure><ul><li>만일 EC2 서버의 어떤 폴더를 만들어 놓았는데 해당 폴더안으로 이동시키고 싶다면 아래와 같이 하면된다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp -i spotift_chatbot.pem ./Code/spotify_s3_make.py ec2-user@ec2-54-180-97-88.ap-northeast-2.compute.amazonaws.com:~/파일이름</span><br></pre></td></tr></table></figure><ul><li>이제 script를 실행하기에 앞서서 작동할 수 있게끔 먼저 환경을 만들어주어야 할 것이다. 가장 먼저 python3가 설치되어있는지를 확인해 보자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">sudo yum list | grep python3</span><br><span class="line"><span class="comment"># 위의 명령문을 실행한 후 python3x에 관한 리스트가 나온다면 python3가 설치되어있는 상태이다.</span></span><br><span class="line"><span class="comment"># 허나 잘 모르겠다면 아래 명령어를 통해 설치하도록하자.</span></span><br><span class="line">sudo yum install python36 -y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 그 다음은 pip를 설치해야한다</span></span><br><span class="line">curl -O https://bootstrap.pypa.io/get-pip.py</span><br><span class="line"></span><br><span class="line">sudo python3 get-pip.py</span><br><span class="line"></span><br><span class="line"><span class="comment">#이제 script파일을 run해본 후에 필요한 패키지들을 추가로 설치해준다.</span></span><br><span class="line">python3 spotify_s3_make.py</span><br><span class="line"></span><br><span class="line">pip install boto3 --user</span><br><span class="line">pip install requests --user</span><br><span class="line">pip install pymysql --user</span><br><span class="line">pip install pandas --user</span><br><span class="line">pip install jsonpath --user</span><br></pre></td></tr></table></figure><ul><li><p>이제 본격적으로 crontab을 실습해 볼 것이다. 보통 crontab은 아래 두 가지를 많이 사용한다. crontab 파일은 vim editor를 사용하는데, 해당 스케쥴을 시작할때 email을 보내주는 기능도 있다. 우선 작동시키고 싶은 파일과 파이썬의 위치를 알고있어야 한다.</p></li><li><p><a href="https://ora-sysdba.tistory.com/entry/Cloud-Computing-Amazon-EC2-%EC%9D%B8%EC%8A%A4%ED%84%B4%EC%8A%A4%EC%9D%98-TIMEZONE-%EB%B3%80%EA%B2%BD" target="_blank" rel="noopener">UTC 시간 변경</a></p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">pwd</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line">/home/ec2-user</span><br><span class="line"></span><br><span class="line"><span class="built_in">which</span> python3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line">/usr/bin/python3</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo service crond start</span><br><span class="line"></span><br><span class="line">crontab -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># crontab 파일 쓰는 vim이 켜진 후</span></span><br><span class="line"><span class="comment"># 매일 18시 30분에</span></span><br><span class="line">30 18 * * * /usr/bin/python3 /home/ec2-user/spotify_s3_make.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># vim으로 저장한 후 나오면 아래와 같은 메세지가 나와야 정상적으로 crontab을 설정한 것이다.</span></span><br><span class="line">crontab: installing new crontab</span><br><span class="line"></span><br><span class="line"><span class="comment"># 어떠한 사항을 crontab으로 스케쥴링하고 있는지 확인하기</span></span><br><span class="line">crontab -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 서버는 UTC를 사용하기 때문에 date 명령어를 통해 현재 서버가 몇시인지를 확인하고 우리나라 시간과 맞추도록 crontab을 설정해 주어야 한다.</span></span><br><span class="line">date</span><br></pre></td></tr></table></figure><h2 id="마이크로서비스에-대한-이해"><a href="#마이크로서비스에-대한-이해" class="headerlink" title="마이크로서비스에 대한 이해"></a>마이크로서비스에 대한 이해</h2><ul><li>전체적인 수집 프로세스에 대한 설명을 다시 한번 짚고가자면, Unknown Artist가 챗봇 메세지로 들어왔을 경우 AWS Serverless Lamda 서비스를 통해 Spotify API에 Access를 하고 그리고 해당 데이터가 Top Tracks, Artist Table에 가야되는지 또는 S3에 가야되는지를 관리를 하게 될 것이다. 또한, Ad Hoc Data Job을 통해 하루에 한번이라던지, 직접 로컬에서 command line을 통해 데이터를 가져올 수도 있게된다. <code>Lambda가 필요한 이유는 우리가 Unknown Artist가 챗봇 메세지로 들어왔을때 내용을 업데이트를 해야되는데, 보통 사람들이 기대하는 챗봇은 업데이트를 바로 해주어서 원하는 정보를 얻을 수 있게끔 해주어야 하기에 이렇게 바로 업데이트를 할 수 있게끔 Lambda라는 서비스를 통해서 해결 할 수 있다.</code> 이런 Lambda는 마이크로서비스의 개념인데, monolithic 이라는 개념의 반대이다. monolithic은 하나의 서비스를 만들때 크게 프로젝트 단위로 만들어 놓고 관리를 해주는 개념으로써 관리에 있어서 전체 프로세스가 보이기 때문에 컨트롤하기 쉬운 부분이 있다. 이에 반해 <code>마이크로서비스는 세세한 작업하나씩을 단위로 관리를 하는 것</code>이다.</li></ul><p><img src="/image/data_cralwer_process_related_artist.png" alt="데이터 수집 관련 프로세스"></p><ul><li>챗봇을 Lambda로 구현하는 이유는 Serverless는 하나의 Function이기 때문에 Stateless라고도 하는데 지금 상태가 어떤지 모르겠다는 의미이다. 예를들어, 어떠한 User 어떤 메시지를 보냈다고 가정하면, Lambda Function에는 이전의 어떠한 메시지를 갖고 있었는지를 담고있을 수 없다. 상태가 없는 Function이라고 생각하면 될 것 같다. 그러므로 이런 State를 관리할만한 데이터베이스가 필요할 것이다. 주로 메시지에 특화된 DynamoDB를 사용할 것이다. 또한 Lambda의 경우에는 해당 서비스의 User가 기하급수적으로 늘어났을 때 <code>병렬로 늘어나기 때문에 제한점이 서버로 구현하는것보단 덜하다는 장점</code>도 있다. 서버의 경우에는 메모리나 CPU의 제한된 성능으로 구축된 동일한 서버를 통해 1명에게 서비스하는 것과 백만명에게 서비스하는 것은 완전 다를것이다. <code>또 한가지 좋은 점은 지속적으로 띄워져 있는 것이 아니라 필요할 때 띄워서 사용한 만큼만 비용을 지불한다는 점</code>이다.</li></ul><p><img src="/image/Lambda_Athena_01.png" alt="Lambda, Athena - 01"></p><ul><li>Crontab을 통하여 Lambda를 호출할 수도 있고, Lambda가 Lambda를 호출할 수도 있다.</li></ul><p><img src="/image/Lambda_Athena_02.png" alt="Lambda, Athena - 02"></p><ul><li>본격적으로 Lambda Function을 만들 것이다. 먼저 AWS Lambda Function 페이지로 이동하여 아래와 같이 Create Function 버튼을 클릭한다.</li></ul><p><img src="/image/Click_create_Lambda_Function.png" alt="Lambda Function 생성 - 01"></p><ul><li>이번 <code>Lambda Function은 이전에 DynamoDB에 top track정보를 DynamoDB에 저장했었는데, Artist가 추가된다면 DynamoDB에도 저장되어야하므로 이 작업을 작성해 볼 것</code>이다. 아래 그림에서와 같이 Author from scratch는 기본적인 예제를 통해 시작하는 부분이고, Use a blueprint는 흔하게 사용되는 경우들을 코드로 제공하는 부분이다. 마지막은 App repository에서 바로 연결해서 사용하는 것이다. 필자는 Scratch로 진행할 것이다. Lambda는 하나의 Functiond이므로 제한점도 있을 것이다.</li></ul><p><img src="/image/choose_option_function_Lambda.png" alt="Lambda Function 생성 - 02 옵션 설정"></p><ul><li>아래 그림과 같이 Function의 이름을 정하고 function의 language를 정한다. 또한 가장 중요한 부분인 Permission부분이 남았는데, 이 부분은 현재 필자가 진행할 Function의 목표는 <code>DynamoDB에 새로운 데이터를 추가하는 것이므로 DynamoDB에 대한 permission을 갖고 있어야 오류가 없을 것</code>이다. 그렇기에 2번째 부분인 Use an exsiting role을 클릭하여 사용해야 하는데, 새롭게 규칙을 추가해서 사용하면 error가 어떻게 발생하는 지를 보기위해 우선 첫번째를 선택하였다. 물론 무조건적으로 있던 규칙을 사용하는 것이 아니라 상황에 맞춰 사용해야한다.</li></ul><p><img src="/image/Lambda_function_basic_info_and_permission.png" alt="Lambda Function 생성 - 03 옵션 설정"></p><ul><li>function을 생성하면 아래 그림과 같이 여러가지 설정 및 작업을 할 수 있는 페이지가 나온다.</li></ul><p><img src="/image/finished_function_creation.png" alt="Lambda Function 생성 - 04 생성 완료"></p><ul><li>아래 부분으로 내려보면 다음과 같이 Function의 코드를 작성할 수 있는 부분이 존재한다. 이 곳에서 Function을 정의할 것이다. <code>허나, Edit code inline</code>은 거의 사용하지 못하는 설정이라고 볼 수 있다. 해당 Lambda Function은 Linux 기반의 AMI compute system에 있는데, 함수를 동작할 수 있는 패키지들이 아무런 설치나 설정이 되어있지 않은 상태이기 때문이다. 그래서 zip file을 업로드하거나 S3에서 불러오는 방식을 보통 채택한다. 필자는 S3에서 불러오는 방식을 사용할 것이다.</li></ul><p><img src="/image/Lambda_Function_Code_typeing.png" alt="Lambda Function 생성 - 05 코드 작성"></p><ul><li>위에서 함수에 사용될 변수들을 정의할 수 있다. 예를 들어, Spotify API에 접속하기 위해서는 ID와 Secret Key가 필요했는데 이 부분을 코드에 적기 보다는 보안의 문제로 따로 변수로 처리해 둔 뒤 함수에는 그 값을 받아 사용하는 형식으로 사용된다. 예를 들면 아래 그림에서 환경 변수의 Edit을 클릭하면 변수의 Key와 Value를 입력하여 추가하게끔 되어있는데 추가했다고 가정하면 함수에서는 <code>os.environ.get(key)</code>로 값을 받으면 된다. 또한 <code>Basic settings</code>부분은 Memory(Max : 3GB)와 timeout(Max : 15분)을 설정할 수 있는 부분인데, <code>가장 간단하고 명료하게 병렬적으로 분산처리를 할 수 있도록 코드를 작성하는 것이 좋다는 Lambda의 특징을 잘 보여주는 부분</code>이다. 여기서 Memory는 크게 해 놓아도 사용한 만큼만 비용을 지불하는 것이므로 상관없다는 것에 유의하자.</li></ul><p><img src="/image/ETC_Lambda_Function.png" alt="Lambda Function 생성 - 06 변수 설정"></p><ul><li><p>이미 해당 Artist가 존재하는 것은 확인한 상태여서 해당 Artist ID에 대한 top track만 확보하고 싶은 경우라고 가정할 것이다. 그 ID 값을 이 Lambda Function에 보내 줌으로써 다시 한번 Spotify API에 hit을 하여 top track 정보를 가져오고 다시 DynamoDB에 저장하도록 할 것이다.  </p></li><li><p>본격적으로 Lambda Function을 만들어 볼 것이다. 우선 새로운 폴더를 만들어준다. 그 안에는 Lambda Function에 관한 것들만 담기 위해서이다. 위에서 언급 한 것처럼 우선 아래 패키지들을 shell script를 통해 실행하기 위해서 requirements.txt를 작성할 것이다. 헌데 AWS에는 기본적으로 boto3가 설치되어져 있고 나머지들은 기본 내장 패키지들이므로 requests만 설치해 주면 될 것 같다.</p></li><li><p>requirements.txt</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">requests</span><br></pre></td></tr></table></figure><ul><li>위에서 만들어 놓은 requirements.txt안의 패키지를 -t옵션의 target 파일에 저장한다는 의미이다. 헌데 이렇게 home이 아닌 파일에 저장하게 되면 pointer issue 때문에 error가 발생하는데 이는 새롭게 <code>setup.cfg</code>라는 파일안에 아래와 같이 작성한 후에 저장해주면 해결된다. 다시 아래의 명령문을 실행하면 error 없이 libs안에 설치가 될 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt -t ./libs</span><br></pre></td></tr></table></figure><ul><li>setup.cfg</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[install]</span><br><span class="line">prefix=</span><br></pre></td></tr></table></figure><ul><li><p>매번 우리가 AWS CLI를 통해 명령문을 치고 실행할 수 없으므로 shell script로 작성해 준다.</p></li><li><p>deploy.sh</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">rm -rf ./libs</span><br><span class="line">pip3 install -r requirements.txt -t ./libs</span><br><span class="line"></span><br><span class="line">rm *.zip</span><br><span class="line">zip top_tracks.zip -r *</span><br><span class="line"></span><br><span class="line">aws s3 rm s3://top-tracks-lambda/top_tracks.zip</span><br><span class="line">aws s3 cp ./top_tracks.zip s3://top-tracks-lambda/top_tracks.zip</span><br><span class="line">aws lambda update-function-code --<span class="keyword">function</span>-name top-tracks --s3-bucket top-tracks-lambda --s3-key top_tracks.zip</span><br></pre></td></tr></table></figure><p>또한, 위에서 lambda function을 update할 s3 bucket을 지정했으므로 새롭게 위의 이름으로 생성한다. 이 단계는 먼저 bucket을 만든뒤에 앞의 shell script를 만들어도 무관하다.</p><p><img src="/image/create_new_bucket_S3_for_lambda_function.png" alt="Lambda Function을 위한 새로운 S3 bucket 생성"></p><ul><li>lambda_function.py</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">sys.path.append(<span class="string">'./libs'</span>)</span><br><span class="line">import os</span><br><span class="line">import boto3</span><br><span class="line">import requests</span><br><span class="line">import base64</span><br><span class="line">import json</span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client_id = <span class="string">"Your Spotify Developer ID"</span></span><br><span class="line">client_secret = <span class="string">"Your Spotify Developer PW"</span></span><br><span class="line"></span><br><span class="line">try:</span><br><span class="line">    dynamodb = boto3.resource(<span class="string">'dynamodb'</span>, region_name=<span class="string">'ap-northeast-2'</span>, endpoint_url=<span class="string">'http://dynamodb.ap-northeast-2.amazonaws.com'</span>)</span><br><span class="line">except:</span><br><span class="line">    logging.error(<span class="string">'could not connect to dynamodb'</span>)</span><br><span class="line">    sys.exit(1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def lambda_handler(event, context):</span><br><span class="line"></span><br><span class="line">    headers = get_headers(client_id, client_secret)</span><br><span class="line"></span><br><span class="line">    table = dynamodb.Table(<span class="string">'top_tracks'</span>)</span><br><span class="line"></span><br><span class="line">    artist_id = event[<span class="string">'artist_id'</span>]</span><br><span class="line"></span><br><span class="line">    URL = <span class="string">"https://api.spotify.com/v1/artists/&#123;&#125;/top-tracks"</span>.format(artist_id)</span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">'country'</span>: <span class="string">'US'</span></span><br><span class="line">    &#125;</span><br><span class="line">    r = requests.get(URL, params=params, headers=headers)</span><br><span class="line"></span><br><span class="line">    raw = json.loads(r.text)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> track <span class="keyword">in</span> raw[<span class="string">'tracks'</span>]:</span><br><span class="line"></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">'artist_id'</span>: artist_id</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        data.update(track)</span><br><span class="line"></span><br><span class="line">        table.put_item(</span><br><span class="line">            Item=data</span><br><span class="line">        )</span><br><span class="line">    <span class="comment"># AWS CloudWatch에서 Log기록을 살펴볼때 확인하기 위해 return 값을 Success로 주었다.</span></span><br><span class="line">    <span class="built_in">return</span> <span class="string">"SUCCESS"</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_headers(client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    endpoint = <span class="string">"https://accounts.spotify.com/api/token"</span></span><br><span class="line">    encoded = base64.b64encode(<span class="string">"&#123;&#125;:&#123;&#125;"</span>.format(client_id, client_secret).encode(<span class="string">'utf-8'</span>)).decode(<span class="string">'ascii'</span>)</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Basic &#123;&#125;"</span>.format(encoded)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">"grant_type"</span>: <span class="string">"client_credentials"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    r = requests.post(endpoint, data=payload, headers=headers)</span><br><span class="line"></span><br><span class="line">    access_token = json.loads(r.text)[<span class="string">'access_token'</span>]</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Bearer &#123;&#125;"</span>.format(access_token)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> headers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><ul><li>위와 같이 파일을 모두 작성했다면 아래와 같은 구조로 만들어져 있을 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">top_tracks</span><br><span class="line">├── deploy.sh</span><br><span class="line">├── lambda_function.py</span><br><span class="line">├── requirements.txt</span><br><span class="line">└── setup.cfg</span><br></pre></td></tr></table></figure><ul><li>이제 top_tracks의 path에서 아래와 같이 shell script를 실행시켜준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./deploy.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과는 아래와 같이 permission denied가 발생할 것이다.</span></span><br><span class="line">-bash: ./deploy.sh: Permission denied</span><br></pre></td></tr></table></figure><ul><li><p>Permission 권한을 바꿔주기 위해 다음의 코드를 실행한뒤 다시 shell script를 실행시킨다.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chmod +x deploy.sh</span><br><span class="line"></span><br><span class="line">./deploy.sh</span><br></pre></td></tr></table></figure></li><li><p>위의 코드들을 실행한 뒤 다시 AWS Lambda Function의 페이지로 돌아가 보면 아래와 같이 해당 파일들을 사용할 수 있게 설정이 되어져 있는 것을 확인 할 수 있다. 보통은 아래와 같이 sensitive한 정보들(client_id, client_secret) 같은 정보들은 따로 config.py 파일을 만들어서 모아놓는다. 이런 config.py파일은 해당 관리자만 가지고 있도록 하여 보안에 유지하나, 필자는 혼자 사용하므로 따로 만들지 않았다.</p></li></ul><p><img src="/image/change_environment_lambda_funcition.png" alt="Lambda Function S3에서 불러오기"></p><ul><li>만약 위에서 Function Code내에서는 sensitive한 정보들이 안보이도록 하려면 아래 그림과 같이 environment variable을 추가하고, 코드를 작성하면된다. key값과 value값이 문자라도 기호를 사용하지 않고 넣으면 된다.</li></ul><p><img src="/image/environment_variables_adding.png" alt="environment variable 추가"></p><p><img src="/image/get_environment_variable.png" alt="environment 변수 받는 법"></p><ul><li>이제 Lambda Function을 Test해 볼 차례이다. Test 버튼을 클릭해 준 뒤, event의 artist_id값을 주기 위해 RDS에 접속해서 임의의 ID 하나를 필자는 가져왔다.</li></ul><p><img src="/image/test_lambda.png" alt="Lambda Function Test"></p><p><img src="/image/Lambda_Function_Test_example.png" alt="Lambda Function Test 데이터 생성"></p><ul><li>Test 데이터를 저장하면 아래 그림과 같이 테스트할 파일로 바뀌었다는 것을 확인 할 수 있다. 이제 Test 버튼을 클릭한다. Log를 보니 Error가 발생한 것 같으므로 먼저 확인해 볼 것이다. Log는 AWS CloudWatch 서비스에서 확인할 수 있다.</li></ul><p><img src="/image/Lambda_Function_test_result_error.png" alt="Lambda Funtion Test 결과 및 error 발생"></p><p><img src="/image/log_cloudwatch.png" alt="Cloudwatch 페이지"></p><ul><li>아래 로그 중 빨간색 박스 부분을 살펴보면 해당 role에 대한 permission이 없기 때문에 발생한 error임을 확인 할 수 있다.</li></ul><p><img src="/image/cloudwatch_log_error_find.png" alt="로그 중 에러 발생 부분 찾기"></p><ul><li>처음에 Lambda Function을 만들때 말했듯이 permission이 없기 때문에 발생하는 error이므로 excution role을 변경해주기 위해 아래 그림의 빨간색 박스 안의 버튼을 클릭해 준다.</li></ul><p><img src="/image/excution_rule_change_attach.png" alt="excution rule 변경"></p><ul><li>role을 변경하기 위해 IAM 페이지로 이동되며, Attach를 통해 DynamoDB에 access 할 수 있는 권한을 부여할 것이다.</li></ul><p><img src="/image/IAM_change_rule_attach.png" alt="Attach policies - 01"></p><p><img src="/image/DynamoDB_Full_Access.png" alt="Attach policies - 02"></p><ul><li>새롭게 role이 추가되어 기존의 1개에서 2개로 늘어났으며, DynamoDBFullAccess 권한을 부여받음을 확인할 수 있다.</li></ul><p><img src="/image/DynamoDB_attach_polices_result.png" alt="Attach policies - 02"></p><ul><li>이제 다시 Lambda Function 페이지로 돌아와서 실행시켜 보면 제대로 실행됨을 알 수 있다.</li></ul><p><img src="/image/Success_Lambda_Function_result.png" alt="Success Lambda Function"></p><ul><li>Lambda는 Event trigger 뿐만 아니라, Crontab과 같이 스케쥴링에 의한 job을 작동시킬수도 있으며, 적용가능한 여러가지 event들이 존재한다.</li></ul><p><img src="/image/Add_trigger.png" alt="trigger 추가하기"></p><p><img src="/image/type_of_trggers.png" alt="trigger들의 종류"></p><p><img src="/image/Scheduling_trigger_AWS_event_watch.png" alt="Scheduling trigger 방법"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/03/01/data_engineering_09/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering (Presto란?)</title>
      <link>https://heung-bae-lee.github.io/2020/02/24/data_engineering_08/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/24/data_engineering_08/</guid>
      <pubDate>Mon, 24 Feb 2020 13:11:09 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Presto란&quot;&gt;&lt;a href=&quot;#Presto란&quot; class=&quot;headerlink&quot; title=&quot;Presto란?&quot;&gt;&lt;/a&gt;Presto란?&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Spark의 단점이라 하면, 물론 Spark SQL도 있지만, 어느 정도 S
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Presto란"><a href="#Presto란" class="headerlink" title="Presto란?"></a>Presto란?</h2><ul><li>Spark의 단점이라 하면, 물론 Spark SQL도 있지만, 어느 정도 Scripting이 필요한 부분이 있다. MySQL 같이 RDS로 데이터 구축을 했을때에는 SQL을 통해서 쉽게 가져올 수 있었지만, Big data로 넘어 오면서 이전 필자의 글을 보았을 때 S3에서 두 곳에 나누어 저장을 했는데, 이런 경우 그럼 RDS와 다르게 어떻게 합칠 수 있는지를 Presto를 통해 하나의 query로 해결할 수 있다. syntax는 SQL과 비슷하다. <code>다양한 multiple data source를 single query를 통해서 진행 할 수 있는 것</code>이다. <code>Hadoop의 경우는 performance나 여러가지 data analytics 할때 여러가지 issue들이있으며 이전 방식이기 때문에 최근에는 Spark와 Presto로 넘어오는 추세이다. AWS는 Presto기반인 Athena를 통해서 S3의 데이터를 작업할 수 있다</code>.</li></ul><h2 id="Serverless란"><a href="#Serverless란" class="headerlink" title="Serverless란?"></a>Serverless란?</h2><ul><li>말 그대로 server가 없다라고 할 수 있으며, Severless라고 하는 부분은 보통 어떠한 서비스를 만들 때, 우리의 Desktop PC를 계속해서 켜두는 경우가 아니므로 EC2라고 하는 계속 지속적으로 띄어져있는 가상의 서버를 만들게 된다. 이때 서버의 용량을 결정해야하는데, 예를들어 chat bot을 통해 User와 소통을할때 어떤 날은 User가 1명 다른날은 100명 어떤날은 10,000명으로 늘어날수있어서 무작정 큰 서버를 사용하게 되거나 순차적으로 Docker를 통해 병렬적으로 어떤 기준이상이 되면 용량을 늘리는 이러한 작업도 다 비용이 되므로 이러한 문제들을 보완하고자 <code>Serverless라는 개념이 도입된다. 어떠한 요청이 들어올때 server를 띄우는데 지속적으로 요청이 들어온다면 계속적으로 병렬적인 server를 띄운다는 것이다. server안에서 용량을 정하는 것을 알아서 자동적으로 해결해 주므로 비용적인 문제를 보완해준다. AWS에서 EC2같은 경우는 server 하나를 띄우는 것이고, Lambda가 Serverless의 개념을 갖는 서비스이다. 또한 Athena도 Serverless의 개념을 갖는 서비스이다.</code></li></ul><h2 id="AWS-Athena의-개요"><a href="#AWS-Athena의-개요" class="headerlink" title="AWS Athena의 개요"></a>AWS Athena의 개요</h2><ul><li><p>AWS Athena에서도 data lake의 시스템 형태로 데이터를 작업하더라도 query를 통해 작업을 하려면 data warehouse 처럼 table의 형식을 안만들 수는 없다.</p></li><li><p>먼저, AWS Athena 필자와 같이 처음 Athena를 사용하는 것이라면, create table 버튼을 클릭하면 아래와 같은 형태로 query문을 입력하는 페이지 보일 것이다.</p></li></ul><p><img src="/image/query_result_settings.png" alt="table 생성 및 query문 결과 저장 setting"></p><ul><li>여기서 query문의 결과를 저장하는 곳을 먼저 설정해 주어야 하는데, 이전의 S3의 spotify-chatbot-project bucket에 새로운 폴더를 추가해 주고 아래 그림과 같이 path를 설정해준다.</li></ul><p><img src="/image/S3_new_folder_add.png" alt="S3 query문 결과 저장 폴더 생성"></p><p><img src="/image/query_path_settings.png" alt="query문 path 설정"></p><ul><li>이제 아래 그림과 같이 이전에 만들어 놓은 parquet형태의 데이터가 존재하는 folder의 path로 query문을 날려준다. 이전에 존재하지 않았던 테이블이 왼쪽의 tab에 생성되며 아래 부분에는 결과를 알려주는 부분이 보여질 것이다. 그 부분에는 partition되어진 부분은 직접적으로 load를 해주어야 한다는 결과를 알려주고있다.</li></ul><p><img src="/image/query_s3_result_partition_load_directly.png" alt="query문 결과"></p><ul><li>위의 그림에서 맨 아래 부분의 모든 partition을 보려면, MSCK REPAIR TABLE) command 사용하라고한 것 처럼, query문을 추가해서 사용하였다.</li></ul><p><img src="/image/Repair_table_mck.png" alt="MSCK REPAIR TABLE query문 추가"></p><ul><li>일반적인 query문과 같이 작성해서 볼 수 있다. 먼저 top_tracks 테이블의 상위 10개만 불러와 볼 것이다. 그리도 partition을 dt로 했기 때문에 dt도 같이 불러와 지는 것을 확인 할 수 있다.</li></ul><p><img src="/image/limit_10_table.png" alt="top_tracks의 상위 10개 데이터"></p><p><img src="/image/dt_with_table_partition.png" alt="dt도 같이 불러옴을 확인"></p><ul><li>audio_features도 동일한 방식으로 만들어 볼 것이다.</li></ul><p><img src="/image/create_external_table_audio_features.png" alt="audio 테이블 생성"></p><p><img src="/image/audio_table_partition_date_load.png" alt="audio 테이블 파티션 로드"></p><p><img src="/image/top_10_load_audio_features.png" alt="audio 테이블 상위 10개 records"></p><ul><li>이밖으 사용법은 아래 문서를 통해 사용법을 확인할 수 있으며, <code>유의할점은 우리가 partition으로 나누어 놓은 것을 전체로 불러 왔기에 최근의 partition만을 살펴보기 위해 date를 어떤 값으로 설정했는지 확인</code>해주어야 한다.</li><li><a href="https://prestodb.io/docs/current/functions.html" target="_blank" rel="noopener">prestodb documents</a></li></ul><p><img src="/image/presto_subscribtion.png" alt="presto 문법을 통한 통계치"></p><h2 id="Apache-Spark"><a href="#Apache-Spark" class="headerlink" title="Apache Spark"></a>Apache Spark</h2><ul><li>데이터를 처리하는 하나의 시스템이다. 데이터는 항상 늘어나고 그리고 너무나 큰 방대한 양을 처리를 해야하기 <code>데이터가 늘어나면 늘어날수록 속도,시간,비용 여러면에서 효율적으로 처리</code>해야한다. 필자는 제플린을 사용할 것인데, 제플린은 Spark를 기반으로 한 Web UI이다. 그래서 Spark를 통해서 처리된 데이터를 시각화한다던지 어떤식으로 output이 나오는지를 볼 수 있다. Jupyter notebook과 비슷하다고 생각하면 된다.</li></ul><p><img src="/image/what_is_Spark.png" alt="Spark란?"></p><h2 id="Map-Reduce"><a href="#Map-Reduce" class="headerlink" title="Map Reduce"></a>Map Reduce</h2><ul><li>Map Reduce는 데이터가 방대한 양으로 늘어날때 처리하는 방식에 issue가 생길 수 있다. 이런 issue들을 보완하기 위해서 데이터가 여러군데 분산처리 되어있는 형태로 저장되어있는데, S3 bucket에 저장한 방식처럼 partition으로 구분된 데이터를 function이나 어떠한 방식에 의해서 mapping을 해서 필요한 부분만을 줄이는 Reduce 과정을 거치게 된다. 이 방식은 처음 <a href="https://ko.wikipedia.org/wiki/%EA%B5%AC%EA%B8%80_%ED%8C%8C%EC%9D%BC_%EC%8B%9C%EC%8A%A4%ED%85%9C" target="_blank" rel="noopener">Google File System</a>으로 사용되어지다가 그 뒤에 Map-Reduce방식으로 Hadoop을 사용했으며, <code>속도에 특화된 Spark를 현재는 주로 사용</code>하고 있다. 예를 들면,</li></ul><p><img src="/image/Map_reduce_what.png" alt="Map Reduce"></p><ul><li>예를들면, 구글같이 다양한 web page를 크롤링해서 각 페이지들의 노출 랭킹을 분석해야 하는 <code>Page Rank</code>라는 알고리즘을 사용할때 html안에 들어가는 tag라던지 이런 문법적인 요소들과 contents들을 한 곳에 몰아서 분석하기 보다는 아래 그림과 같이 Input을 병렬적으로 나누어 진행하고 그 다음 어떠한 Suffling process를 통해서 Reduce하여 결과를 낸다.</li></ul><p><img src="/image/map_reduce_example.png" alt="Map Reduce example"></p><ul><li><p>필자는 AWS의 <code>EMR</code>(Elastic Map Reduce)서비스를 통해 <code>Spark와 제플린을 설치 해 볼 것</code>이다. EMR은 Spark나 Hadoop 같은 시스템에 cluster를 만드는 곳이라고 생각할 수 있다. cluster는 서버들이라고 말 할 수 있다. EC2를 base로한 EMR을 cluster화해서 하나의 instance ECS server가 아니라 master 아니면 다양한 core Node들이 여러개가 생성이 되어서 방대한 양의 데이터를 처리하기 위해서 필요한 setting을 구성할 수 있는 곳이라고 생각할 수 있다.</p></li><li><p>서버를 사용하기 위해서는 다양한 권한이라던지 key file들이 필요하므로 security와 같은 부분을 다루어야 하는데 이런 부부은 AWS에서 IAM 서비스에서 작업할 수 있다.</p></li><li><p>아래 그림과 같이 먼저 AWS에서 EMR 페이지로 이동한다.</p></li></ul><p><img src="/image/EMR_where.png" alt="EMR 서비스"></p><p><img src="/image/create_cluster_EMR.png" alt="EMR 서비스 cluster 생성"></p><ul><li>cluster 이름을 정하고, Application은 Spark를 사용할 것이므로 아래 그림과 같이 설정해 주었다. 또한, hardware 부분은 memory optimization등 여러가지 옵션이 존재하지만 모든 것은 다 비용이므로 우선 간단하게 c4 large를 사용해 보려고 한다.</li></ul><p><img src="/image/create_clustre_quick_options.png" alt="EMR 서비스 quick 옵션들"></p><ul><li>EC2 서버 안에서 다양한 작업들을 하기 위해서 key pair 필요하다. 필자와 같이 한번도 key pair를 생성한 적이 없다면 아래 그림과 같이 Learn how to key pair 버튼을 클릭하여 EC2 페이지로 넘어가 key pair를 생성한다.</li></ul><p><img src="/image/key_pair_setting.png" alt="key pair"></p><p><img src="/image/go_to_EC2_console.png" alt="key pair 생성하는 방법"></p><p><img src="/image/EC2_key_pair_page.png" alt="EC2 key pair 페이지"></p><p><img src="/image/create_key_pair.png" alt="EC2 key pair 생성"></p><p><img src="/image/create_key_pair_process.png" alt="EC2 key pair 생성 방법"></p><p><img src="/image/key_pair_result.png" alt="key pair 생성 결과"></p><ul><li>key pair가 생성되면 동시에 pem 파일이 다운로드 되어질 것이다. 그 파일을 현재 project를 진행하는 폴더로 옮겨 놓는것을 추천한다. 옮겨 놓았다면, 아래 그림과 같이 파일의 모드를 변경해 주어야 한다.</li></ul><p><img src="/image/chmod_key_pair.png" alt="key pair pem 파일 모드 변경"></p><ul><li>해당 pem 파일이 존재하는 path에서 아래와 같이 변경한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chmod og-rwx spotift_chatbot.pem</span><br></pre></td></tr></table></figure><ul><li>key pair를 생성하였으므로 다시 cluster를 생성하는 페이지로 돌아가 key pair를 설정한 뒤에 아래 생성 버튼을 클릭한다.</li></ul><p><img src="/image/security_access_create_cluster.png" alt="cluster 생성"></p><ul><li>cluster가 생성되면 아래 그림과 같이 cluster가 속성들에 대한 페이지가 보일 것이다. 또한, Master 권한으로 접속을 하려면 SSH 방식으로 인증 후 접속해야하기 때문에 security group을 관리 해 주어야 한다. Security groups for Master의 값을 클릭하여 EC2의 security group 페이지로 이동하여 Inbound 규칙에 Master와 slave 둘다 아래 그림과 같이 ssh 규칙을 추가해 주어야 한다.</li></ul><p><img src="/image/EC2_security_groups_inbound_rule_add.png" alt="EC2 security group Inbound rule 추가"></p><p><img src="/image/ssh_port_tcp_ip_add.png" alt="Inbound rule에 ssh 설정"></p><ul><li>ssh로 접속하는 방법은 아래그림과 같이 enable web connection을 클릭하면 확인 할 수 있다.</li></ul><p><img src="/image/enable_web_connection_ssh.png" alt="ssh로 접속하는 방식"></p><ul><li>pem 파일이 존재하는 path로 가서 아래 그림에서 빨간색 줄이 쳐져있는 command를 실행해 놓은 뒤, 동일한 파일 path에 아래 그림에서 처럼 생성하라는 foxyproxy-settings.xml를 회색 네모칸의 내용들을 복사하여 생성한다. 동일한 파일 path에서 하라고 추천하는 것은 이 path에서 계속 접속할 것이기 때문이다.</li></ul><p><img src="/image/connection_ssh_with_web.png" alt="ssh로 접속하는 방식 - 01"></p><ul><li>그 다음은 chrome web store에 가서 <code>foxy proxy</code>를 검색한 후  standard 버젼을 설치해준다. 그러므로 chrome 브라우저를 사용해야 할 것임은 당연히 알 것이라고 생각한다.</li></ul><p><img src="/image/chrome_foxy_proxy_add.png" alt="ssh로 접속하는 방식 - 02"></p><ul><li>foxy proxy를 chrome browser에 추가했다면, 오른쪽 상단에 여우모양의 아이콘이 생성되었을 것이다. 클릭한 후 option 버튼을 눌러준다.</li></ul><p><img src="/image/foxy_proxy_option.png" alt="ssh로 접속하는 방식 - 03"></p><ul><li>그 다음은 왼쪽 tab에서 import/export를 눌러 준 후에 이전에 만들어 준 xml파일을 눌러 replace해 주면 된다. 그런 다음 public dns로 접속해 보면 맨 아래 그림과 같이 test page를 볼 수 있을 것이다. 또한 dns위에 connection들이 활성화 된 것을 볼 수 있다.</li></ul><p><img src="/image/import_export_foxy_proxy.png" alt="ssh로 접속하는 방식 - 03"></p><p><img src="/image/import_export_result_replace.png" alt="ssh로 접속하는 방식 - 04"></p><p><img src="/image/master_public_dns.png" alt="ssh로 접속하는 방식 - 05"></p><p><img src="/image/master_public_dns_success.png" alt="ssh로 접속하는 방식 - 06"></p><ul><li>아래 그림에서와 같이 connection들 중 제플린을 클릭하면 제플린 페이지로 이동한다.</li></ul><p><img src="/image/activatio_connection.png" alt="제플린 접속"></p><p><img src="/image/zeppelin_page.png" alt="zeppelin page"></p><p><img src="/image/zeppelin_notebooke_create.png" alt="notebook 생성"></p><ul><li>모든 명령문이 <code>%pyspark</code>로 시작해야 한다. 필자는 기존의 c4.large로 진행시에는 connection error가 생겨 원인을 찾다가 해결치 못하고 우선 r3.xlarge를 선택하여 다시 cluster를 만든 결과 connection error가 발생하지 않았다.</li></ul><ul><li>2번째 cell까지는 python에서 진행하던 방식이고 3번째 cell에서 구현하는 방식이 spark의 방식인데 spark는 rdd를 기반으로 방대한 데이터를 분산시켜서 mapping한 후 apply해서 얻은 값을 통합을 해서 받는 구조이다. sc(spark context)를 통해 parallelize하게 3개의 데이터를 쪼개서 rdd로 나누어 준 결과이다.</li></ul><p><img src="/image/zeppelin_spark_python.png" alt="zeppelin python 방식과 spark방식의 차이"></p><ul><li>아래와 같이 rdd로 쪼개서 map함수를 통해 쪼개놓은 데이터에 각각 mapping 시켜주고 선택한다. 또한, sqlContext를 통해 S3에 이전에 저장해 놓았던 데이터를 dataFrame형태로 불러와 작업할 수 있다. <code>printSchema()함수를 통해 각각의 값들이 어떤 형태로 들어있는지에 주의</code>를 갖고 살펴봐야 추후에 작업에 어려움이 없다.</li></ul><p><img src="/image/zeppelin_spark_method.png" alt="zeppeplin spark 함수 및 rdd 개념"></p><p><img src="/image/zeppelin_spark_method_show.png" alt="zeppelin spark 함수 및 dataFrame"></p><p><img src="/image/zeppelin_spark_df_show.png" alt="zeppelin spark 함수 및 dataFrame - 01"></p><ul><li>이렇게 불러온 데이터를 내장 함수를 활용해 기본적인 통계량값들을 계산할 수 있으며, 필요에 따라 사용자 정의 함수(UDF : User Definition Function)을 사용하여 전처리를 할 수 있다. UDF를 통해 정의한 함수를 Boolean값으로 처리하여 다음과 같이 filter함수에 적용시켜 condition을 줄 수도 있다.</li></ul><p><img src="/image/spark_zeppelin_function_udf_01.png" alt="sql function 및 udf"></p><p><img src="/image/spark_zeppelin_function_udf_02.png" alt="sql function 및 udf - 01"></p><ul><li>본격적으로 S3에 저장해 놓은 모든 데이터들을 join한 master table을 만들기 위해 아래와 같은 작업을 한다. 가장 먼저, artists parquet 데이터를 불러와 아래와 같이 DataFrame으로 만들 수 있다.</li></ul><p><img src="/image/S3_artists_table_DF.png" alt="artist table"></p><ul><li>허나, <code>artists 데이터는 잘 변하지 않기 때문에 zeppelin에서 python을 통해 RDS에서 바로 불러오는 것이 좋을 수도 있다.</code> 아래에서 함수 안의 argument값들은 자신의 값에 맞는 것들로 먼저 정해놓고 실행해야 한다.</li></ul><p><img src="/image/S3_artist_another_method.png" alt="artist table을 불러오는 다른 방법"></p><ul><li>최종적으로 artists, top-tracks, 그리고 audio_features 모두 join한 table을 만들 것 이다. 참고로 아래 cell을 실행하기 이전에 master node에 접속해서 먼저 sudo pip install pandas와 sudo pip install pyspark 명령어를 통해 설치해 주어야 한다. zeppelin의 장점 중 하나로 <code>바로 sql table로 지정하여 sql query문으로 작업할 수 있다.</code> 해당 값을 <code>바로 시각화할 수 있는 점도 장점 중의 하나</code>이다. <code>옵션에서 없는 그래프를 그리는 것은 python이나 다른 library를 활용하여 보완</code>할 수 있다.</li></ul><p><img src="/image/S3_master_table_join.png" alt="모든 데이터를 join한 master table 생성"></p><p><img src="/image/query_success_zeppelin.png" alt="query문 작성"></p><p><img src="/image/query_success_zeppelin_avg_count.png" alt="query문을 통한 분포 시각화"></p><ul><li>데이터의 audio feature의 분포를 통해 예를 들어 가수의 인기도와 트랙의 인기도의 차이가 거의 없는 해당 가수의 대표적인 트랙을 알고 싶다면 아래와 같은 EDA를 먼저 실행하여 audio feature들의 특징을 파악하는 것이 중요하다. 아래 그림에서 처럼 acousticness는 전체적으로 0쪽으로 치우쳐있어 중심을 대표하는 값으로는 median을 사용해야 될 것이라고 판단할 수 있으며, danceability는 정규분포 꼴을 띄고 있어 mean을 사용해도 무방할 것으로 판단 할 수 있다.</li></ul><p><img src="/image/AVG_audio_features.png" alt="audio features의 평균값들"></p><p><img src="/image/acousticness_dist.png" alt="전체 acousticness의 분포"></p><p><img src="/image/danceability_dist.png" alt="전체 danceability의 분포"></p><p><a href="https://github.com/awskrug/datascience-group/tree/master/workshop-sustainable_data_analysis" target="_blank" rel="noopener">참고하면 좋을 문서 - 01</a></p><p><a href="https://github.com/ravieeeee/serverless-KBOleague-analysis" target="_blank" rel="noopener">참고하면 좋을 문서 - 02</a></p><p><a href="https://aws.amazon.com/ko/blogs/big-data/running-an-external-zeppelin-instance-using-s3-backed-notebooks-with-spark-on-amazon-emr/" target="_blank" rel="noopener">참고하면 좋을 문서 - 03</a></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/24/data_engineering_08/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering (데이터 웨어하우스 vs 데이터 레이크)</title>
      <link>https://heung-bae-lee.github.io/2020/02/22/data_engineering_07/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/22/data_engineering_07/</guid>
      <pubDate>Fri, 21 Feb 2020 16:14:31 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;데이터-웨어하우스-vs-데이터-레이크&quot;&gt;&lt;a href=&quot;#데이터-웨어하우스-vs-데이터-레이크&quot; class=&quot;headerlink&quot; title=&quot;데이터 웨어하우스 vs 데이터 레이크&quot;&gt;&lt;/a&gt;데이터 웨어하우스 vs 데이터 레이크&lt;/h1&gt;&lt;
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="데이터-웨어하우스-vs-데이터-레이크"><a href="#데이터-웨어하우스-vs-데이터-레이크" class="headerlink" title="데이터 웨어하우스 vs 데이터 레이크"></a>데이터 웨어하우스 vs 데이터 레이크</h1><ul><li><p>데이터 레이크라는 개념은 비교적 최신의 개념이다. 데이터 웨어하우스라고 하는 MySQL, PostgreSQL 같은 RDBMS 프로그램들을 넘어서 데이터들이 너무나 방대해졌기 때문에 나온 시스템이라고 할 수 있다.</p></li><li><p>이전의 <code>데이터 웨어하우스는 미리 짜여진 구조를 통해 가공해서 저장했기에 좀 더 접근</code>하기 쉬었다. 반면에 <code>데이터 레이크는 데이터가 너무 방대하기 때문에 어떤 데이터를 어떻게 사용할지 모르므로 Raw 데이터를 저장</code>한다. 그렇기 때문에 데이터에 대한 접근성이 조금 떨어지는 면이 있었지만 <code>Hadoop이나 Spark 같은 다양한 서비스들을 통해 그런 단점을 보완하여 처리</code>할 수 있게 되었다.</p></li><li><p>관계형 DB를 사용할 경우에는 데이터가 기하급수적으로 늘어날 수록 비용이나 관리비용도 많이 소요될 것이다.</p></li></ul><p><img src="/image/datalake_and_datawarehouse.png" alt="시대의 변화에 따른 데이터 저장 방시의 진화"></p><ul><li>ETL, 즉 Extract하고, Transform 한 후에 Load하는 과정을 일컫는 용어이며, 이전의 데이터 웨어하우스에서는 이런 순서를 거쳐 작업했지만 최근에는 ELT, 우선 Extract하고, Load한뒤에 데이터 레이크에 넣은 다음에 Transformer하자라고 많이들 이야기 하고 있다.</li></ul><ul><li>아래 그림은 하나의 예로서, 먼저 여러 곳에 산재해 있는 정형/비정형 데이터들을 데이터 레이크에 한곳으로 모아 그 다음 Spark가 됐던 다른 빅데이터 처리 시스템을 통해 재가공을 한 후 다른 애플리케이션이나 BI TooL들에 활용할 수 있다.</li></ul><p><img src="/image/ETL_after_ELT.png" alt="ETL"></p><h2 id="데이터-레이크-아키텍쳐"><a href="#데이터-레이크-아키텍쳐" class="headerlink" title="데이터 레이크 아키텍쳐"></a>데이터 레이크 아키텍쳐</h2><ul><li>여러 어플리케이션에서 나오는 데이터나 해당 API를 통해서 얻게되는 데이터들을 모아 어떻게 재가공해야할지를 고민하는 것이 가장 큰 문제일 것이다. 또한 이런 것들 통해서 redash같은 시각화를 통해 insight를 얻어야 할 것이다.</li></ul><p><img src="/image/redash_and_various_tools.png" alt="다양한 협업 툴 그리고 데이터 그리고 API"></p><ul><li><p>또한, error가 나온다면, 그 날에 데이터만 어떻게 backfill을 통해서 확보를 할 것인지를 고민해야한다.</p></li><li><p>챗봇을 통해서 구축을 할 때, 아티스트가 늘어났을때, 새로운 artist가 입력을 들어왔을 때, 저장되어있던 정보에는 없던 Unknown artist이면, Trigger base의 Lambda에 의해서 Unknown artist에 대한 데이터를 확보를 하고, 데이터 레이크는 Latency(데이터를 주고받는 속도의 개념)가 느리기 때문에 다양한 DB에 RDBMS를 구축하여 상황에 맞게 저장한다. 그러나 이 모든 데이터들이 존재하는 DB를 한곳으로 묶어야 되는 부분이 데이터 레이크이다. 그 모든걸 필자는 AWS S3에 옮길 것이다. 옮기는 과정에서 스케쥴링은 어떻게 할 것인지 그리고, 어떤 이벤트가 있을때 업데이트를 할 것인지를 정해주어야 할 것이다.  </p></li></ul><p><img src="/image/schedule_data_pipeline.png" alt="데이터 파이프라인"></p><h2 id="S3-Simple-Storage-System"><a href="#S3-Simple-Storage-System" class="headerlink" title="S3(Simple Storage System)"></a>S3(Simple Storage System)</h2><ul><li>데이터 레이크 역할을 할 S3 bucket을 만들 것이다. 일종의 폴더라고 생각하여도 될 것 같다. 아래와 같은 단계로 bucket을 만든다.</li></ul><p><img src="/image/AWS_S3.png" alt="AWS S3"></p><ul><li>create bucket 버튼을 클릭한다.</li></ul><p><img src="/image/create_bucket_s3.png" alt="S3 bucket 생성 과정 - 01"></p><ul><li>생성할 bucket 이름을 설정한다.</li></ul><p><img src="/image/bucket_name_setting.png" alt="S3 bucket 생성 과정 - 02"></p><ul><li>bucket에 관련된 설정 중 tag 부분을 설정할 수 있는 부분인데, 생성후에도 설정 가능하므로 다음단계로 넘어간다.</li></ul><p><img src="/image/logging_and_name_and_region_bucket_setting.png" alt="S3 bucket 생성 과정 - 03"></p><ul><li>configure option을 설정하는 단계이며, 아래와 같이 모두 public access 하게끔 default 설정값으로 선택하였다.</li></ul><p><img src="/image/configure_option_setting.png" alt="S3 bucket 생성 과정 - 04"></p><ul><li>마지막으로 앞의 과정에서 설정한 부분들을 요약해서 보여준다. 맨 아래 Create bucket 버튼을 누르면 bucket 생성이 완료된다.</li></ul><p><img src="/image/review_bucket.png" alt="S3 bucket 생성 과정 - 05"></p><ul><li>위의 단계를 다 거치면 처음과 다르게 하나의 bucket이 생성된 것을 확인할 수 있다. 물론 이전에 이미 bucket을 생성하신 분들은 여러개의 bucket 리스트가 보일 것이다.</li></ul><p><img src="/image/S3_bucket_create_finish.png" alt="bucket 생성 완료"></p><ul><li><code>AWS Glue</code>는 어떨때는 데이터가 형식이 없으니까 원래는 키값이 3개였는데, 그 후 점점 키값이 늘어날 수 도 있다. 이런 경우 다양한 Table의 스키마를 관리 할 수 있다. 데이터 레이크 경우에는 지속적으로 변할수 있는 시스템이라는 것을 인지하고 있어야하기 때문에, 위와 같은 경우들을 자동화 할 수 있는 서비스이다. 가장 큰 부분이 <code>Crwaler</code>이다. 어떠한 Table에 변형이 일어났을 때, 이 Crwaler가 감지를 해서, 그것을 반영을 해준다.</li></ul><p><img src="/image/aws_glue_introduction.png" alt="AWS Glue"></p><ul><li>S3에 올릴 파일작업 Python Script 작성<ul><li><code>bucket의 key값</code>이라고 하는 <code>dt(data type)</code>을 정해야한다. top tracks와 같이 지속적으로 변화하는 데이터는 방대한양으로 늘어났을때는 결국엔 쪼개서 scan을 해야하므로 어떠한 형식을 통해서 Spark나 Hadoop이 readable한 형식으로 partition을 만들어놔야 Spark나 Haddep에서 최근의 데이터를 갖고있는 마지막 partition만 확인하면 되기 때문이다. <code>필자는 날짜를 통해 시점이 언제일지 알 수 있도록 partition을 구분지어 줄 것</code>이다.</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import base64</span><br><span class="line">import boto3</span><br><span class="line">import requests</span><br><span class="line">import logging</span><br><span class="line">import json</span><br><span class="line">import pymysql</span><br><span class="line">import sys, os, argparse</span><br><span class="line">from datetime import datetime</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">def main(host, user, passwd, db, port, client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        <span class="comment"># use_unicode=True를 써야 한글같은 경우는 깨지지 않는다.</span></span><br><span class="line">        conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=<span class="string">'utf8'</span>)</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line">    except:</span><br><span class="line">        logging.error(<span class="string">"could not connect to rds"</span>)</span><br><span class="line">        <span class="comment"># 보통 문제가 없으면 0</span></span><br><span class="line">        <span class="comment"># 문제가 있으면 1을 리턴하도록 안에 숫자를 넣어준다.</span></span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    headers = get_headers(client_id, client_secret)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># RDS - 아티스트 ID를 가져오고</span></span><br><span class="line">    cursor.execute(<span class="string">"SELECT id FROM artists"</span>)</span><br><span class="line"></span><br><span class="line">    dt =  datetime.utcnow().strftime(<span class="string">"%Y-%m-%d"</span>)</span><br><span class="line">    <span class="built_in">print</span>(dt)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (id, ) <span class="keyword">in</span> cursor.fetchall():</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Spotify API를 통해서 데이터를 불러오고</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># .json형태로 저장한뒤에</span></span><br><span class="line">    with open(<span class="string">'top_tracks.json'</span>, <span class="string">'w'</span>) as f:</span><br><span class="line">        <span class="keyword">for</span> top_track <span class="keyword">in</span> top_tracks:</span><br><span class="line">            json.dump(top_track, f)</span><br><span class="line">            f.write(os.linesep)</span><br><span class="line">    <span class="comment">#  S3에 import를 시킨다.</span></span><br><span class="line">    s3 =  boto3.resource(<span class="string">'s3'</span>)</span><br><span class="line">    object = s3.Object(<span class="string">'spotify-chatbot-project'</span>, <span class="string">'dt=&#123;&#125;/topt-racks.json'</span>.format(dt))</span><br><span class="line"></span><br><span class="line">def get_headers(client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    endpoint = <span class="string">"https://accounts.spotify.com/api/token"</span></span><br><span class="line">    encoded = base64.b64encode(<span class="string">"&#123;&#125;:&#123;&#125;"</span>.format(client_id, client_secret).encode(<span class="string">'utf-8'</span>)).decode(<span class="string">'ascii'</span>)</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Basic &#123;&#125;"</span>.format(encoded)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">"grant_type"</span>: <span class="string">"client_credentials"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    r = requests.post(endpoint, data=payload, headers=headers)</span><br><span class="line"></span><br><span class="line">    access_token = json.loads(r.text)[<span class="string">'access_token'</span>]</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Bearer &#123;&#125;"</span>.format(access_token)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> headers</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--client_id'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'Spotify app client id'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--client_secret'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'Spotify client secret'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--host'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'end point host'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--username'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'AWS RDS id'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--database'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'DB name'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--password'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'AWS RDS password'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    port = 3306</span><br><span class="line">    main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret)</span><br></pre></td></tr></table></figure><ul><li>필자가 사용할 <code>Spark는 Parquet이라는 format을 더 선호</code>하기에, <code>Parquet으로 변형을 한후, compression(압축)을 통해서 데이터 Volume도 줄이면서 더 Performance도 좋게끔 할 것</code>이다. 위에서 만든 top_tracks.json 로컬 파일을 S3에 저장을 할 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import base64</span><br><span class="line">import boto3</span><br><span class="line">import requests</span><br><span class="line">import logging</span><br><span class="line">import json</span><br><span class="line">import pymysql</span><br><span class="line">import sys, os, argparse</span><br><span class="line">from datetime import datetime</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">def main(host, user, passwd, db, port, client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        <span class="comment"># use_unicode=True를 써야 한글같은 경우는 깨지지 않는다.</span></span><br><span class="line">        conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=<span class="string">'utf8'</span>)</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line">    except:</span><br><span class="line">        logging.error(<span class="string">"could not connect to rds"</span>)</span><br><span class="line">        <span class="comment"># 보통 문제가 없으면 0</span></span><br><span class="line">        <span class="comment"># 문제가 있으면 1을 리턴하도록 안에 숫자를 넣어준다.</span></span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    headers = get_headers(client_id, client_secret)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># RDS - 아티스트 ID를 가져오고</span></span><br><span class="line">    cursor.execute(<span class="string">"SELECT id FROM artists LIMIT 10"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Top tracks를 Spotify에서 가져오고</span></span><br><span class="line">    top_tracks = []</span><br><span class="line">    <span class="keyword">for</span> (id, ) <span class="keyword">in</span> cursor.fetchall():</span><br><span class="line"></span><br><span class="line">        URL = <span class="string">'https://api.spotify.com/v1/artists/&#123;id&#125;/top-tracks'</span>.format(id)</span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'country'</span> : <span class="string">'US'</span></span><br><span class="line">        &#125;</span><br><span class="line">        r = requests.get(URL, params=params, headers=headers)</span><br><span class="line">        raw = json.loads(r.text)</span><br><span class="line">        top_tracks.extend(raw[<span class="string">'tracks'</span>])</span><br><span class="line"></span><br><span class="line">    top_tracks = pd.DataFrame(top_tracks)</span><br><span class="line">    top_tracks.to_parquet(<span class="string">'top-tracks.parquet'</span>, engine=<span class="string">'pyarrow'</span>, compression=<span class="string">'snappy'</span>)</span><br><span class="line">    sys.exit(0)</span><br><span class="line"></span><br><span class="line">    dt =  datetime.utcnow().strftime(<span class="string">"%Y-%m-%d"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  S3에 import를 시킨다.</span></span><br><span class="line">    s3 =  boto3.resource(<span class="string">'s3'</span>)</span><br><span class="line"></span><br><span class="line">    object = s3.Object(<span class="string">'spotify-chatbot-project'</span>, <span class="string">'dt=&#123;&#125;/top-tracks.parquet'</span>.format(dt))</span><br><span class="line"></span><br><span class="line">    data = open(<span class="string">'top-tracks.parquet'</span>, <span class="string">'rb'</span>)</span><br><span class="line">    object.put(Body=data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_headers(client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    endpoint = <span class="string">"https://accounts.spotify.com/api/token"</span></span><br><span class="line">    encoded = base64.b64encode(<span class="string">"&#123;&#125;:&#123;&#125;"</span>.format(client_id, client_secret).encode(<span class="string">'utf-8'</span>)).decode(<span class="string">'ascii'</span>)</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Basic &#123;&#125;"</span>.format(encoded)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">"grant_type"</span>: <span class="string">"client_credentials"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    r = requests.post(endpoint, data=payload, headers=headers)</span><br><span class="line"></span><br><span class="line">    access_token = json.loads(r.text)[<span class="string">'access_token'</span>]</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Bearer &#123;&#125;"</span>.format(access_token)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> headers</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--client_id'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'Spotify app client id'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--client_secret'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'Spotify client secret'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--host'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'end point host'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--username'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'AWS RDS id'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--database'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'DB name'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--password'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'AWS RDS password'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    port = 3306</span><br><span class="line">    main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret)</span><br></pre></td></tr></table></figure><ul><li>위의 방법처럼 한다면 error가 발생되는데, 그 이유는 nested column이라 해서 아래 그림처럼 어떠한 key값의 안에 list형식으로 존재하는 struct형식이기 때문에 발생된다. parquet화 해서 사용하면 데이터를 통해서 performance가 빨라지지만, 그렇게 performance를 좋게하려면 정확하게 define을 해 주어야 한다. 보통 json 형식으로 가장 raw 형태로 저장한다음, processing job이 한 번 돌은후에, 새로운 data가 가장 raw data가 S3에 들어왔을때, trigger가 되어서 해당 parquet화를 하고 싶은 몇개의 데이터만 뽑은 후 다시 돌아서 다른 S3 bucket안에 저장하는 데이터 파이프라인을 거친다.</li></ul><p><img src="/image/top_tracks_struct.png" alt="top tracks API response 구조"></p><ul><li>앞으로의 작업은 jsonpath라는 package가 필요하므로 아래의 코드처럼 설치를 해주어야 한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jsonpath --user</span><br></pre></td></tr></table></figure><ul><li>이제 top_tracks 뿐만 아니라 Audio Feature들도 추가해서 parquet 형태로 S3에 저장해 줄 것이다. 필자의 경우 국가코드는 US에 대해서만 우선 실행했으며, artist_id들 중 Audio feature가 US에서는 존재하지 않는 artist들이 있어 이 부분은 나중에 다른 국가나 artist들에 대해 동일한 현상으로 error가 발생되는 경우를 방지하기 위해 if문으로 Null값이 포함되어있는지 아닌지를 check해본뒤 리스트에 추가해주는 방식으로 코드를 작성했다.</li></ul><ul><li>spotify_make_s3.py</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import base64</span><br><span class="line">import boto3</span><br><span class="line">import requests</span><br><span class="line">import logging</span><br><span class="line">import json</span><br><span class="line">import pymysql</span><br><span class="line">import sys, os, argparse</span><br><span class="line">from datetime import datetime</span><br><span class="line">import pandas as pd</span><br><span class="line">import jsonpath</span><br><span class="line">from pandas.io.json import json_normalize</span><br><span class="line"></span><br><span class="line">def main(host, user, passwd, db, port, client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        <span class="comment"># use_unicode=True를 써야 한글같은 경우는 깨지지 않는다.</span></span><br><span class="line">        conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=<span class="string">'utf8'</span>)</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line">    except:</span><br><span class="line">        logging.error(<span class="string">"could not connect to rds"</span>)</span><br><span class="line">        <span class="comment"># 보통 문제가 없으면 0</span></span><br><span class="line">        <span class="comment"># 문제가 있으면 1을 리턴하도록 안에 숫자를 넣어준다.</span></span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    headers = get_headers(client_id, client_secret)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># RDS - 아티스트 ID를 가져오고</span></span><br><span class="line">    cursor.execute(<span class="string">"SELECT id FROM artists"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># jsonpath라는 package를 통해서 해당 path안에 어떤 데이터를 insert했을때,</span></span><br><span class="line">    <span class="comment"># key 값을 자동으로 찾아서 그에 해당하는 value값을 가져오기 때문이다.</span></span><br><span class="line">    top_track_keys = &#123;</span><br><span class="line">        <span class="string">'id'</span> : <span class="string">'id'</span>,</span><br><span class="line">        <span class="string">'name'</span> : <span class="string">'name'</span>,</span><br><span class="line">        <span class="string">'popularity'</span> : <span class="string">'popularity'</span>,</span><br><span class="line">        <span class="string">'external_url'</span> : <span class="string">'external_urls.spotify'</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Top tracks를 Spotify에서 가져오고</span></span><br><span class="line">    top_tracks = []</span><br><span class="line">    <span class="keyword">for</span> (id, ) <span class="keyword">in</span> cursor.fetchall():</span><br><span class="line"></span><br><span class="line">        URL = <span class="string">'https://api.spotify.com/v1/artists/&#123;&#125;/top-tracks'</span>.format(id)</span><br><span class="line">        params = &#123;</span><br><span class="line">            <span class="string">'country'</span> : <span class="string">'US'</span></span><br><span class="line">        &#125;</span><br><span class="line">        r = requests.get(URL, params=params, headers=headers)</span><br><span class="line">        raw = json.loads(r.text)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> raw[<span class="string">'tracks'</span>]:</span><br><span class="line">            top_track = &#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> top_track_keys.items():</span><br><span class="line">                top_track.update(&#123;k: jsonpath.jsonpath(i, v)&#125;)</span><br><span class="line">                <span class="comment"># 데이터를 mapping하기 위해서 artist_id를 추가한다.</span></span><br><span class="line">                top_track.update(&#123;<span class="string">'artist_id'</span>: id&#125;)</span><br><span class="line">                top_tracks.append(top_track)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># track_id</span></span><br><span class="line">    track_ids = [i[<span class="string">'id'</span>][0] <span class="keyword">for</span> i <span class="keyword">in</span> top_tracks]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># parquet화 할 수 있는 방법은 여러가지 package가 있지만 pandas를 사용할 것 이다.</span></span><br><span class="line">    <span class="comment"># 필자가 사용할 Spark는 Parquet이라는 format을 더 선호하기에, Parquet으로 변형을 한후,</span></span><br><span class="line">    <span class="comment"># compression(압축)을 통해서 데이터 Volume도 줄이면서 더 Performance도 좋게끔 할 것이다.</span></span><br><span class="line">    <span class="comment"># 위에서 만든 top_tracks.json local 파일을 S3에 저장을 할 것이다.</span></span><br><span class="line">    top_tracks = pd.DataFrame(top_tracks)</span><br><span class="line">    top_tracks.to_parquet(<span class="string">'top-tracks.parquet'</span>, engine=<span class="string">'pyarrow'</span>, compression=<span class="string">'snappy'</span>)</span><br><span class="line"></span><br><span class="line">    dt =  datetime.utcnow().strftime(<span class="string">"%Y-%m-%d"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#  S3에 import를 시킨다.</span></span><br><span class="line">    s3 =  boto3.resource(<span class="string">'s3'</span>)</span><br><span class="line">    <span class="comment"># bucket의 key값이라고 하는 data type을 정해야한다.</span></span><br><span class="line">    <span class="comment"># top tracks와 같이 지속적으로 변화하는 데이터는 방대한양으로 늘어났을때는</span></span><br><span class="line">    <span class="comment"># 결국엔 쪼개서 scan을 해야하므로 어떠한 형식을 통해서 Spark나 Hadoop이 readable한 형식으로</span></span><br><span class="line">    <span class="comment"># partition을 만들어놔야 Spark나 Haddep에서 최근의 데이터를 갖고있는 마지막 partition만 확인하면 되기 때문이다.</span></span><br><span class="line">    <span class="comment"># 필자는 날짜를 통해 시점이 언제일지 알 수 있도록 partition을 구분지어 줄 것이다.</span></span><br><span class="line">    object = s3.Object(<span class="string">'spotify-chatbot-project'</span>, <span class="string">'top-tracks/dt=&#123;&#125;/top-tracks.parquet'</span>.format(dt))</span><br><span class="line"></span><br><span class="line">    data = open(<span class="string">'top-tracks.parquet'</span>, <span class="string">'rb'</span>)</span><br><span class="line">    object.put(Body=data)</span><br><span class="line"></span><br><span class="line">    tracks_batch =  [track_ids[i: i+100] <span class="keyword">for</span> i <span class="keyword">in</span> range(0, len(track_ids), 100)]</span><br><span class="line"></span><br><span class="line">    audio_features = []</span><br><span class="line">    null_features = []</span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> tracks_batch:</span><br><span class="line">        ids = <span class="string">','</span>.join(batch)</span><br><span class="line">        URL = <span class="string">'https://api.spotify.com/v1/audio-features/?ids=&#123;&#125;'</span>.format(ids)</span><br><span class="line"></span><br><span class="line">        r = requests.get(URL, headers=headers)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'null'</span> <span class="keyword">in</span> r.text:</span><br><span class="line">            raw = json.loads(r.text)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> raw[<span class="string">'audio_features'</span>]:</span><br><span class="line">                <span class="keyword">if</span> pd.isnull(i) == False:</span><br><span class="line">                    <span class="comment"># audio_features는 dictionary key값 안에 또 다른 list형식으로 되어있지 않으므로</span></span><br><span class="line">                    <span class="comment"># 그냥 사용해도 된다.</span></span><br><span class="line">                    null_features.append(i)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            raw = json.loads(r.text)</span><br><span class="line">            audio_features.extend(raw[<span class="string">'audio_features'</span>])</span><br><span class="line"></span><br><span class="line">    audio_features.extend(null_features)</span><br><span class="line">    audio_features = json_normalize(audio_features)</span><br><span class="line">    audio_features.to_parquet(<span class="string">'audio_features.parquet'</span>, engine=<span class="string">'pyarrow'</span>, compression=<span class="string">'snappy'</span>)</span><br><span class="line"></span><br><span class="line">    s3 =  boto3.resource(<span class="string">'s3'</span>)</span><br><span class="line">    object = s3.Object(<span class="string">'spotify-chatbot-project'</span>, <span class="string">'audio_features/dt=&#123;&#125;/top-tracks.parquet'</span>.format(dt))</span><br><span class="line">    data = open(<span class="string">'audio_features.parquet'</span>, <span class="string">'rb'</span>)</span><br><span class="line">    object.put(Body=data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_headers(client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    endpoint = <span class="string">"https://accounts.spotify.com/api/token"</span></span><br><span class="line">    encoded = base64.b64encode(<span class="string">"&#123;&#125;:&#123;&#125;"</span>.format(client_id, client_secret).encode(<span class="string">'utf-8'</span>)).decode(<span class="string">'ascii'</span>)</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Basic &#123;&#125;"</span>.format(encoded)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">"grant_type"</span>: <span class="string">"client_credentials"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    r = requests.post(endpoint, data=payload, headers=headers)</span><br><span class="line"></span><br><span class="line">    access_token = json.loads(r.text)[<span class="string">'access_token'</span>]</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Bearer &#123;&#125;"</span>.format(access_token)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> headers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--client_id'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'Spotify app client id'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--client_secret'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'Spotify client secret'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--host'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'end point host'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--username'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'AWS RDS id'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--database'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'DB name'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--password'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'AWS RDS password'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    port = 3306</span><br><span class="line">    main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret)</span><br></pre></td></tr></table></figure><ul><li>위의 코드 실행 결과 아래 그림과 같이 S3에 각각의 파일 형식으로 저장되어 업데이트시에 해당 시간과 날짜에 의해 partition되어지는 데이터 저장 결과를 볼 수 있다.</li></ul><p><img src="/image/S3_bucket_result_01.png" alt="S3 bucket에 저장 결과 - 01"></p><p><img src="/image/S3_bucket_result_02.png" alt="S3 bucket에 저장 결과 - 01"></p><ul><li>여기에 top-tracks 데이터와 audio feature 데이터 뿐만아니라 artist 데이터도 S3에 parquet형식으로 저장해 줄 것이다.</li></ul><ul><li>spotify_s3_artist.py</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import base64</span><br><span class="line">import boto3</span><br><span class="line">import requests</span><br><span class="line">import logging</span><br><span class="line">import json</span><br><span class="line">import pymysql</span><br><span class="line">import sys, os, argparse</span><br><span class="line">from datetime import datetime</span><br><span class="line">import pandas as pd</span><br><span class="line">import jsonpath</span><br><span class="line">from pandas.io.json import json_normalize</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main(host, user, passwd, db, port, client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        <span class="comment"># use_unicode=True를 써야 한글같은 경우는 깨지지 않는다.</span></span><br><span class="line">        conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=<span class="string">'utf8'</span>)</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line">    except:</span><br><span class="line">        logging.error(<span class="string">"could not connect to rds"</span>)</span><br><span class="line">        <span class="comment"># 보통 문제가 없으면 0</span></span><br><span class="line">        <span class="comment"># 문제가 있으면 1을 리턴하도록 안에 숫자를 넣어준다.</span></span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># RDS - 아티스트 ID를 가져오고</span></span><br><span class="line">    cursor.execute(<span class="string">"SELECT * FROM artists"</span>)</span><br><span class="line">    colnames = [d[0] <span class="keyword">for</span> d <span class="keyword">in</span> cursor.description]</span><br><span class="line">    artists = [dict(zip(colnames, row)) <span class="keyword">for</span> row <span class="keyword">in</span> cursor.fetchall()]</span><br><span class="line">    artists = pd.DataFrame(artists)</span><br><span class="line"></span><br><span class="line">    artists.to_parquet(<span class="string">'artists.parquet'</span>, engine=<span class="string">'pyarrow'</span>, compression=<span class="string">'snappy'</span>)</span><br><span class="line"></span><br><span class="line">    dt =  datetime.utcnow().strftime(<span class="string">"%Y-%m-%d"</span>)</span><br><span class="line"></span><br><span class="line">    s3 =  boto3.resource(<span class="string">'s3'</span>)</span><br><span class="line">    object = s3.Object(<span class="string">'spotify-chatbot-project'</span>, <span class="string">'artists/dt=&#123;&#125;/artists.parquet'</span>.format(dt))</span><br><span class="line">    data = open(<span class="string">'artists.parquet'</span>, <span class="string">'rb'</span>)</span><br><span class="line">    object.put(Body=data)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--client_id'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'Spotify app client id'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--client_secret'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'Spotify client secret'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--host'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'end point host'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--username'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'AWS RDS id'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--database'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'DB name'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--password'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'AWS RDS password'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    port = 3306</span><br><span class="line">    main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret)</span><br></pre></td></tr></table></figure><p><img src="/image/artists_s3_parquet.png" alt="artist 데이터 parquet 형식으로 s3에 저장"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/22/data_engineering_07/#disqus_thread</comments>
    </item>
    
    <item>
      <title>data engineering (AWS DynamoDB 사용해서 오디오 feature 활용하기)</title>
      <link>https://heung-bae-lee.github.io/2020/02/20/data_engineering_06/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/20/data_engineering_06/</guid>
      <pubDate>Thu, 20 Feb 2020 09:25:18 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;NoSQL&quot;&gt;&lt;a href=&quot;#NoSQL&quot; class=&quot;headerlink&quot; title=&quot;NoSQL&quot;&gt;&lt;/a&gt;NoSQL&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;RDBMS가 제한되는 점들을 보완하기 위해 활용하기 시작했다. NoSQL 같은 경우는 RDB 
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="NoSQL"><a href="#NoSQL" class="headerlink" title="NoSQL"></a>NoSQL</h1><ul><li>RDBMS가 제한되는 점들을 보완하기 위해 활용하기 시작했다. NoSQL 같은 경우는 RDB 처럼 특정 Structure가 정해져 있지 않기 때문에 <code>데이터의 추가가 용이</code>하다.</li></ul><p><img src="/image/NoSQL_SPotify.png" alt="NoSQL"></p><ul><li><p>Scalability</p><ul><li><p>SQL Databases are <code>vertically</code> scalable - CPU, RAM or SSD</p><ul><li>여러가지 사양들을 정해놓아서 정해진 resource를 초과할 경우 또다른 resource를 추가 시켜주어야 한다.</li></ul></li><li><p>NoSQL Databases are horizontally scalable - Sharding / Partitioning</p><ul><li><code>NoSQL은 Partitioning을 통해 다른 resource에서 남는 부분을 부족한 부분에 넘겨줄 수 있다.</code></li></ul></li></ul></li></ul><ul><li>그렇다면 partition이 무엇이냐 조금 더 자세하게 알아보자면, 우선 해석 그대로 나눈다는 의미가 있다. partition을 해야하는 이유 중 가장 중요한 이유는 데이터가 늘어나면 늘어날수록 SQL을 통하여 query문의 performance가 느려질수 밖에 없는데, 이런 부분을 방지하고자 나누어 주는 것이다. 데이터의 양이 늘어나는데 매번 모든 데이터를 다 읽어와서 작업을 하는 것은 너무 비효율적이기 때문이다. 물론 무조건적으로 NoSQL이 좋다는 의미는 아니다. NoSQL의 경우에는 아래의 그림에서 오른쪽 그림처럼 애초에 우리가 lookup(참조)해야 하는 데이터의 양을 줄여서 나누어 놓는 것이다. 이는 Spark의 경우에도 동일하다.</li></ul><p><img src="/image/what_is_partition.png" alt="파티션이란"></p><ul><li><code>Vertical partitioning을 하는 이유</code>는 중복적인 데이터는 ERD를 Table을 분리해서 관리를 하게끔 해서 Normalization을 하였는데, Normalization을 진행하고도 column 수가 늘어나면, 해당 Table을 읽는데 속도가 느려질수가 있다. 또한, <code>어떤 column들은 지속적으로 업데이트가 될 수도 있지만, 어떤 column들은 지속적으로 업데이트가 되지 않을 수 있기 때문에 사용</code>한다. <code>RDBM에서는 나눌수는 있지만, 데이터가 엄청나게 많은 양이어서 데이터를 처리하는데 속도의 향상을 기대한다면 사용하지만, 그렇지 않은 경우는 많이 사용하지 않는다.</code></li></ul><p><img src="/image/vertical_partition_spotify.png" alt="버티컬 파티션이란"></p><ul><li><code>Horizontal Partition은 NoSQL에서 무조건 사용</code>된다. 어떤 데이터를 검색할 때 <code>Sharded Key로 빠르게 Access하기 위해서 사용되는 것</code>이다. 마치 RDB에서 primary key를 통한 빠른 search와 유사한 느낌이다. <code>동일한 컬럼이지만 데이터의 양을 나누어서 각각 저장하는 방식</code>이다. <code>Partition key를 통해 데이터를 나누어</code> 주는데, AWS안의 DynamoDB를 만들면서 확인해 볼 것이다.</li></ul><p><img src="/image/Horicontal_partition_spotify.png" alt="호리즌탈 파티션이란"></p><ul><li><a href="https://developer.spotify.com/documentation/web-api/reference/artists/get-artists-top-tracks/" target="_blank" rel="noopener">spotify Developer artist’s top tracks</a>를 들어가보면, 아래 그림과 같이 path patameter로 해당 artist id를 받아서 결과를 출력 해준다. NoSQL이므로 track 전체를 가져와도 되지만, 그 안에서 필요한 데이터를 또 작업을 해야하는데, <code>artist 한명이 여러개의 track을 가지고 있는 경우도 있으므로 artist id만을 partition key로 사용할 수 없다. 왜냐하면 partition key로 사용되는 경우 해당 column에 동일한 값을 사용할 수 없기 때문이다. 그러므로 artist_id를 partition key로 해놓고, track id를 sort key로 사용할 것이다.</code></li></ul><p><img src="/image/top_tracks_method_spotify.png" alt="top tracks API 사용법"></p><h2 id="AWS-DynamoDB-사용하기"><a href="#AWS-DynamoDB-사용하기" class="headerlink" title="AWS DynamoDB 사용하기"></a>AWS DynamoDB 사용하기</h2><ul><li>먼저 AWS에 로그인을 마친 후, 아래 그림과 같이 service 탭을 눌러 내려보면 이전의 RDS를 만들었을 때 보았던, Database란에서 <code>DynamoDB</code>를 볼 수 있다. 클릭하면 하게 되면 아래 그림과 같은 페이지가 나올 것이다.</li></ul><p><img src="/image/aws_service_tab_find_dynamodb.png" alt="AWS service에서 DynamoDB 찾기"></p><p><img src="/image/DynamoDB_page.png" alt="DynamoDB 페이지"></p><ul><li>그 다음은, 위의 페이지에서 보이는 것처럼 <code>create table</code>을 클릭하면 다음과 같은 페이지로 이동 될 것이다. 여기서 Table name은 말 그대로 Table의 이름을 지정해주는 부분이고, <code>Partition key</code> 부분은 RDS에서 Primary Key와 동일한 역할(빠르게 참조할 수 있게 해주는)을 하는 부분이라고 생각하면된다. 그러므로 <code>해당 artist_id에 해당하는 row는 유일</code>해야 한다. 하지만 <code>sort key를 사용할 수 있는데, 이 부분은 해당 세션이 언제 생성되었는지를 알려주기 위한 역할</code>이라고 할 수 있다. 위에서 말했던 것 처럼 partition key는 artist_id로 sort key는 track id를 사용할 것이다.</li></ul><ul><li><p><code>Provisioned</code>는 server를 띄우기 때문에 그 상황 안에서 free tier를 사용가능한 사람만 사용할 수 있는데, Auto-Scale이 가능한 부분이 있다. 하지만 Auto-Scale이 실질적으로 적용이 되는 시간차가 있어 해당 Scaling이 진행되는 동안은 traffic이 많아진다면 속도가 느려질 수 있다는 단점이 있을 수 있다.</p></li><li><p><code>On-demand</code>는 어느 정도 필요한지를 모를 경우에 aws에서 알아서 scaling을 해주고 쓰이는 만큼만 돈을 지불할 상황일 때 사용한다.  </p></li></ul><ul><li>필자는 아래와 같이 설정한 후에, DynamoDB를 생성하였다.</li></ul><p><img src="/image/generate_DynamoDB.png" alt="DynamoDB 생성 페이지"></p><ul><li>생성한 후에는 생성된 DynamoDB의 페이지가 아래와 같이 나온다.</li></ul><p><img src="/image/create_DynamoDB_page.png" alt="DynamoDB 페이지"></p><ul><li><p><code>Read capacity units</code>(읽기 요청 단위 1)은 강력히 일관된 읽기 요청 1 또는 최종적 일관된 읽기 요청 2(최대 4 KB 크기 항목의 경우)를 나타냅니다. 트랜잭션 읽기 요청은 최대 4 KB 크기 항목의 1회 읽기를 수행하는 데 2개의 읽기 요청 단위가 필요합니다.</p></li><li><p><code>Write capacity units</code>(쓰기 요청 단위 1)은 최대 1 KB 크기의 항목에 대해 1회 쓰기를 나타냅니다.</p></li><li><p><a href="https://docs.aws.amazon.com/ko_kr/amazondynamodb/latest/developerguide/HowItWorks.ReadWriteCapacityMode.html#HowItWorks.requests" target="_blank" rel="noopener">참고</a></p></li></ul><p><img src="/image/read_capacity_and_write_capacity.png" alt="Capacity"></p><ul><li>DB를 사용하다가 primary key가 Table내에서 유일한 RDBMS와는 다르게 partition key를 추가할 수도 있다. 필자의 경우 추가를 할 필요가 없었기에 하진 않았지만, 추가하여 사용할 경우는 아래와 같이 생성해주면된다.</li></ul><p><img src="/image/add_partition_key_and_sort_key.png" alt="Partition Key와 Sort Key 추가"></p><h2 id="Audio-Feature-DynamoDB에-insert-하기"><a href="#Audio-Feature-DynamoDB에-insert-하기" class="headerlink" title="Audio Feature DynamoDB에 insert 하기"></a>Audio Feature DynamoDB에 insert 하기</h2><ul><li>AWS service를 사용하기 위한 패키지인 <code>boto3</code>를 설치한다. boto3는 AWS가 제공하는 Python SDK의 이름이다. DynamoDB에 insert를 하거나 select해오거나 할 때 필요하다고 간단하게 이해해도 좋을 것 같다. console에서는 로그인을 통해 인증절차를 거치지만 boto3를 통한 인증방식은 이전의 우리가 설정해 놓았던 aws configure를 통해 설정되있는 상태를 통해 인증하므로 AWS를 console에 로그인하지 않고도 컨트롤할 수 있는 것이다.</li></ul><ul><li><a href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html" target="_blank" rel="noopener">boto3 documents</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#pip3 install boto3 --user</span></span><br><span class="line">pip install boto3 --user</span><br></pre></td></tr></table></figure><h3 id="오디오-피처-살펴보기"><a href="#오디오-피처-살펴보기" class="headerlink" title="오디오 피처 살펴보기"></a>오디오 피처 살펴보기</h3><ul><li>전체적인 flow는 이미 RDS받아놓은 artists Table에서 artist_id들을 받아온 후, Spotify API를 통해 해당 artist_id의 top track 정보를 받아서 AWS의 DynamoDB에 저장할 것이다.</li></ul><ul><li>Spotify API에서는 해당 artist_id 뿐만아니라 국가에대한 parameter도 받는데, 이는 국가마다 해당 artist의 top track이 다를 수도 있기 때문이다. 필자는 우선 US(미국)과 CA(캐나다)에대해서만 살펴 볼 것이다.</li></ul><p><img src="/image/top_track_reference_page.png" alt="top tracks 페이지"></p><ul><li><a href="https://boto3.amazonaws.com/v1/documentation/api/latest/guide/dynamodb.html" target="_blank" rel="noopener">boto3에서 DynamoDB 사용법 documents</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import boto3</span><br><span class="line">import requests</span><br><span class="line">import base64</span><br><span class="line">import json</span><br><span class="line">import logging</span><br><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line">def main(host, user, passwd, db, port, client_id, client_secret):</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        dynamodb = boto3.resource(<span class="string">'dynamodb'</span>, region_name=<span class="string">'ap-northeast-2'</span>, endpoint_url=<span class="string">'http://dynamodb.ap-northeast-2.amazonaws.com'</span>)</span><br><span class="line">    except:</span><br><span class="line">        logging.error(<span class="string">'could not connect to dynamodb'</span>)</span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        conn = pymysql.connect(host=host, user=username, passwd=password, db=database, port=port, use_unicode=True, charset=<span class="string">'utf8'</span>)</span><br><span class="line">        cursor = conn.cursor()</span><br><span class="line">    except:</span><br><span class="line">        logging.error(<span class="string">"could not connect to rds"</span>)</span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    headers = get_headers(client_id, client_secret)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 사용할 DynamoDB로 만든 Table 이름</span></span><br><span class="line">    table = dynamodb.Table(<span class="string">'top_tracks'</span>)</span><br><span class="line"></span><br><span class="line">    cursor.execute(<span class="string">'SELECT id FROM artists'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 국가는 미국과 캐나다만을 선택할 것이다.</span></span><br><span class="line">    countries = [<span class="string">'US'</span>, <span class="string">'CA'</span>]</span><br><span class="line">    <span class="keyword">for</span> country <span class="keyword">in</span> countries:</span><br><span class="line">        <span class="keyword">for</span> (artist_id, ) <span class="keyword">in</span> cursor.fetchall():</span><br><span class="line"></span><br><span class="line">            URL = <span class="string">"https://api.spotify.com/v1/artists/&#123;&#125;/top-tracks"</span>.format(artist_id)</span><br><span class="line">            params = &#123;</span><br><span class="line">                <span class="string">'country'</span>: country</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            r = requests.get(URL, params=params, headers=headers)</span><br><span class="line"></span><br><span class="line">            raw = json.loads(r.text)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> track <span class="keyword">in</span> raw[<span class="string">'tracks'</span>]:</span><br><span class="line">                <span class="comment"># 위에서 가져온 track에서는 artist_id가 없기 때문에 아래와 같이 먼저 dictionary로 만들어줌.</span></span><br><span class="line">                data = &#123;</span><br><span class="line">                    <span class="string">'artist_id'</span>: artist_id,</span><br><span class="line">                    <span class="string">'country'</span>: country</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                data.update(track)</span><br><span class="line"></span><br><span class="line">                table.put_item(</span><br><span class="line">                    Item=data</span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_headers(client_id, client_secret):</span><br><span class="line"></span><br><span class="line">    endpoint = <span class="string">"https://accounts.spotify.com/api/token"</span></span><br><span class="line">    encoded = base64.b64encode(<span class="string">"&#123;&#125;:&#123;&#125;"</span>.format(client_id, client_secret).encode(<span class="string">'utf-8'</span>)).decode(<span class="string">'ascii'</span>)</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Basic &#123;&#125;"</span>.format(encoded)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    payload = &#123;</span><br><span class="line">        <span class="string">"grant_type"</span>: <span class="string">"client_credentials"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    r = requests.post(endpoint, data=payload, headers=headers)</span><br><span class="line"></span><br><span class="line">    access_token = json.loads(r.text)[<span class="string">'access_token'</span>]</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">"Authorization"</span>: <span class="string">"Bearer &#123;&#125;"</span>.format(access_token)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> headers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--client_id'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'Spotify app client id'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--client_secret'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'Spotify client secret'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--host'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'end point host'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--username'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'AWS RDS id'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--database'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'DB name'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--password'</span>, <span class="built_in">type</span>=str, <span class="built_in">help</span>=<span class="string">'AWS RDS password'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    port = 3306</span><br><span class="line">    main(host=args.host, user=args.username, passwd=args.password, db=args.database, port=port, client_id=args.client_id, client_secret=args.client_secret)</span><br></pre></td></tr></table></figure><ul><li>아래와 같이 AWS console 창에서도 데이터가 insert된 것을 확인 할 수 있다.</li></ul><p><img src="/image/DynamoDB_insert_result.png" alt="Spotify top tracks DynamoDb에 insert"></p><h2 id="DynamoDB-저장된-데이터-불러오기"><a href="#DynamoDB-저장된-데이터-불러오기" class="headerlink" title="DynamoDB 저장된 데이터 불러오기"></a>DynamoDB 저장된 데이터 불러오기</h2><ul><li>아래 그림에서와 같이 boto3를 이용하여 DynamoDB에서 데이터를 select하여 가져오는 방법은 <code>get_item</code> 함수를 사용하는 방법이 있다. 단, <code>해당 Table의 Partition key 나 sort key로 지정한 column의 값을 전부 입력해 주어야 error 없이 작동된다.</code></li></ul><p><img src="/image/dynamodb_get_item.png" alt="boto3 get item 메서드"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import boto3</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        dynamodb = boto3.resource(<span class="string">'dynamodb'</span>, region_name=<span class="string">'ap-northeast-2'</span>, endpoint_url=<span class="string">'http://dynamodb.ap-northeast-2.amazonaws.com'</span>)</span><br><span class="line">    except:</span><br><span class="line">        logging.error(<span class="string">'could not connect to dynamodb'</span>)</span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    table = dynamodb.Table(<span class="string">'top_tracks'</span>)</span><br><span class="line"></span><br><span class="line">    response = table.get_item(</span><br><span class="line">        Key=&#123;</span><br><span class="line">            <span class="string">'artist_id'</span> : <span class="string">'0L8ExT028jH3ddEcZwqJJ5'</span>,</span><br><span class="line">            <span class="string">'id'</span> : <span class="string">'0uppYCG86ajpV2hSR3dJJ0'</span></span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(response)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'Item'</span>: &#123;<span class="string">'is_playable'</span>: True, <span class="string">'duration_ms'</span>: Decimal(<span class="string">'282906'</span>), <span class="string">'external_ids'</span>: &#123;<span class="string">'isrc'</span>: <span class="string">'USWB19901574'</span>&#125;, <span class="string">'uri'</span>: <span class="string">'spotify:track:0uppYCG86ajpV2hSR3dJJ0'</span>, <span class="string">'country'</span>: <span class="string">'US'</span>, <span class="string">'name'</span>: <span class="string">'Give It Away'</span>, <span class="string">'album'</span>: &#123;<span class="string">'total_tracks'</span>: Decimal(<span class="string">'19'</span>), <span class="string">'images'</span>: [&#123;<span class="string">'width'</span>: Decimal(<span class="string">'640'</span>), <span class="string">'url'</span>: <span class="string">'https://i.scdn.co/image/ab67616d0000b273153d79816d853f2694b2cc70'</span>, <span class="string">'height'</span>: Decimal(<span class="string">'640'</span>)&#125;, &#123;<span class="string">'width'</span>: Decimal(<span class="string">'300'</span>), <span class="string">'url'</span>: <span class="string">'https://i.scdn.co/image/ab67616d00001e02153d79816d853f2694b2cc70'</span>, <span class="string">'height'</span>: Decimal(<span class="string">'300'</span>)&#125;, &#123;<span class="string">'width'</span>: Decimal(<span class="string">'64'</span>), <span class="string">'url'</span>: <span class="string">'https://i.scdn.co/image/ab67616d00004851153d79816d853f2694b2cc70'</span>, <span class="string">'height'</span>: Decimal(<span class="string">'64'</span>)&#125;], <span class="string">'artists'</span>: [&#123;<span class="string">'name'</span>: <span class="string">'Red Hot Chili Peppers'</span>, <span class="string">'href'</span>: <span class="string">'https://api.spotify.com/v1/artists/0L8ExT028jH3ddEcZwqJJ5'</span>, <span class="string">'id'</span>: <span class="string">'0L8ExT028jH3ddEcZwqJJ5'</span>, <span class="string">'type'</span>: <span class="string">'artist'</span>, <span class="string">'external_urls'</span>: &#123;<span class="string">'spotify'</span>: <span class="string">'https://open.spotify.com/artist/0L8ExT028jH3ddEcZwqJJ5'</span>&#125;, <span class="string">'uri'</span>: <span class="string">'spotify:artist:0L8ExT028jH3ddEcZwqJJ5'</span>&#125;], <span class="string">'release_date'</span>: <span class="string">'1991-09-24'</span>, <span class="string">'name'</span>: <span class="string">'Blood Sugar Sex Magik (Deluxe Edition)'</span>, <span class="string">'album_type'</span>: <span class="string">'album'</span>, <span class="string">'release_date_precision'</span>: <span class="string">'day'</span>, <span class="string">'href'</span>: <span class="string">'https://api.spotify.com/v1/albums/30Perjew8HyGkdSmqguYyg'</span>, <span class="string">'id'</span>: <span class="string">'30Perjew8HyGkdSmqguYyg'</span>, <span class="string">'type'</span>: <span class="string">'album'</span>, <span class="string">'external_urls'</span>: &#123;<span class="string">'spotify'</span>: <span class="string">'https://open.spotify.com/album/30Perjew8HyGkdSmqguYyg'</span>&#125;, <span class="string">'uri'</span>: <span class="string">'spotify:album:30Perjew8HyGkdSmqguYyg'</span>&#125;, <span class="string">'popularity'</span>: Decimal(<span class="string">'72'</span>), <span class="string">'artists'</span>: [&#123;<span class="string">'name'</span>: <span class="string">'Red Hot Chili Peppers'</span>, <span class="string">'href'</span>: <span class="string">'https://api.spotify.com/v1/artists/0L8ExT028jH3ddEcZwqJJ5'</span>, <span class="string">'id'</span>: <span class="string">'0L8ExT028jH3ddEcZwqJJ5'</span>, <span class="string">'type'</span>: <span class="string">'artist'</span>, <span class="string">'external_urls'</span>: &#123;<span class="string">'spotify'</span>: <span class="string">'https://open.spotify.com/artist/0L8ExT028jH3ddEcZwqJJ5'</span>&#125;, <span class="string">'uri'</span>: <span class="string">'spotify:artist:0L8ExT028jH3ddEcZwqJJ5'</span>&#125;], <span class="string">'disc_number'</span>: Decimal(<span class="string">'1'</span>), <span class="string">'href'</span>: <span class="string">'https://api.spotify.com/v1/tracks/0uppYCG86ajpV2hSR3dJJ0'</span>, <span class="string">'track_number'</span>: Decimal(<span class="string">'9'</span>), <span class="string">'external_urls'</span>: &#123;<span class="string">'spotify'</span>: <span class="string">'https://open.spotify.com/track/0uppYCG86ajpV2hSR3dJJ0'</span>&#125;, <span class="string">'artist_id'</span>: <span class="string">'0L8ExT028jH3ddEcZwqJJ5'</span>, <span class="string">'preview_url'</span>: <span class="string">'https://p.scdn.co/mp3-preview/fcdf3224d230b26b637418c2d1028bc482db7fce?cid=93b8cdc701294ab7992eaf370c7ba1cd'</span>, <span class="string">'is_local'</span>: False, <span class="string">'id'</span>: <span class="string">'0uppYCG86ajpV2hSR3dJJ0'</span>, <span class="string">'explicit'</span>: False, <span class="string">'type'</span>: <span class="string">'track'</span>&#125;, <span class="string">'ResponseMetadata'</span>: &#123;<span class="string">'RequestId'</span>: <span class="string">'KKGRB8AQD7G3H83UAA4J40TPE7VV4KQNSO5AEMVJF66Q9ASUAAJG'</span>, <span class="string">'HTTPStatusCode'</span>: 200, <span class="string">'HTTPHeaders'</span>: &#123;<span class="string">'x-amzn-requestid'</span>: <span class="string">'KKGRB8AQD7G3H83UAA4J40TPE7VV4KQNSO5AEMVJF66Q9ASUAAJG'</span>, <span class="string">'x-amz-crc32'</span>: <span class="string">'1465669044'</span>, <span class="string">'content-type'</span>: <span class="string">'application/x-amz-json-1.0'</span>, <span class="string">'content-length'</span>: <span class="string">'2296'</span>, <span class="string">'date'</span>: <span class="string">'Fri, 21 Feb 2020 12:56:26 GMT'</span>&#125;, <span class="string">'RetryAttempts'</span>: 0&#125;&#125;</span><br></pre></td></tr></table></figure><ul><li>그런데, 여기서 어떠한 형식으로든 DB를 사용하려면 query도 사용할 필요가 있을 것이다. 이런 경우 사용하는 것이 Scanning과 Querying이다. <code>Querying은 Partition key(Primary key)를 알고있을 경우에 사용</code>하고, <code>Scanning은 그 이외의 부수적인 다른 attribute(column)의 value를 알고 있을 때 사용</code>한다.</li></ul><p><img src="/image/boto3_dynamodb_scanning_and_querying.png" alt="querying, scanning"></p><h4 id="partition-key와-다른-attribute를-사용한-query문"><a href="#partition-key와-다른-attribute를-사용한-query문" class="headerlink" title="partition key와 다른 attribute를 사용한 query문"></a>partition key와 다른 attribute를 사용한 query문</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import boto3</span><br><span class="line"></span><br><span class="line">from boto3.dynamodb.conditions import Key, Attr</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        dynamodb = boto3.resource(<span class="string">'dynamodb'</span>, region_name=<span class="string">'ap-northeast-2'</span>, endpoint_url=<span class="string">'http://dynamodb.ap-northeast-2.amazonaws.com'</span>)</span><br><span class="line">    except:</span><br><span class="line">        logging.error(<span class="string">'could not connect to dynamodb'</span>)</span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    table = dynamodb.Table(<span class="string">'top_tracks'</span>)</span><br><span class="line"></span><br><span class="line">    response = table.query(</span><br><span class="line">        KeyConditionExpression=Key(<span class="string">'artist_id'</span>).eq(<span class="string">'0L8ExT028jH3ddEcZwqJJ5'</span>)</span><br><span class="line">        FilterExpression=Attr(<span class="string">'popularity'</span>).gt(80)</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(response[<span class="string">'Items'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="Attribute만-사용하는-scanning"><a href="#Attribute만-사용하는-scanning" class="headerlink" title="Attribute만 사용하는 scanning"></a>Attribute만 사용하는 scanning</h4><ul><li>허나, <code>되도록이면 querying을 사용하는 것을 추천</code>한다. 왜냐하면, scan을 key값이 없기 때문에 전체 데이터를 한번 다 돌아서 조건에 맞는 데이터들을 가져오는 것이므로, 데이터의 양이 많아 row의 수가 많다면, 속도가 느려지기 때문이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import boto3</span><br><span class="line"></span><br><span class="line">from boto3.dynamodb.conditions import Key, Attr</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line"></span><br><span class="line">    try:</span><br><span class="line">        dynamodb = boto3.resource(<span class="string">'dynamodb'</span>, region_name=<span class="string">'ap-northeast-2'</span>, endpoint_url=<span class="string">'http://dynamodb.ap-northeast-2.amazonaws.com'</span>)</span><br><span class="line">    except:</span><br><span class="line">        logging.error(<span class="string">'could not connect to dynamodb'</span>)</span><br><span class="line">        sys.exit(1)</span><br><span class="line"></span><br><span class="line">    table = dynamodb.Table(<span class="string">'top_tracks'</span>)</span><br><span class="line"></span><br><span class="line">    response = table.scan(</span><br><span class="line">        FilterExpression=Attr(<span class="string">'popularity'</span>).gt(80)</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(response[<span class="string">'Items'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/20/data_engineering_06/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 실습 Chat bot 만들기</title>
      <link>https://heung-bae-lee.github.io/2020/02/20/NLP_13/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/20/NLP_13/</guid>
      <pubDate>Thu, 20 Feb 2020 07:01:42 GMT</pubDate>
      <description>
      
        
        
          &lt;ul&gt;
&lt;li&gt;지금까지 두 가지 문제에 대해 실습을 진행하였다. 1) 텍스트를 분석해서 각 텍스트를 분류하는 문제를 실습했고, 2) 두 개의 텍스트가 있을 때 각 텍스트끼리의 유사도를 판단하는 문제를 실습했다. 마지막으로 이번에는 텍스트를 단순히 분
        
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>지금까지 두 가지 문제에 대해 실습을 진행하였다. 1) 텍스트를 분석해서 각 텍스트를 분류하는 문제를 실습했고, 2) 두 개의 텍스트가 있을 때 각 텍스트끼리의 유사도를 판단하는 문제를 실습했다. 마지막으로 이번에는 텍스트를 단순히 분석해서 분류나 유사도를 측정하는 것이 아닌 직업 문장을 생성할 수 있는 text generation 문제를 실습해 볼 것이다. text generation에도 많은 문제가 있지만 ‘자연어의 꽃’이라고 불리는 ‘Chat bot’을 제작해 볼 것이다.</li></ul><h1 id="Chat-bot-만들기"><a href="#Chat-bot-만들기" class="headerlink" title="Chat bot 만들기"></a>Chat bot 만들기</h1><ul><li><p>일반적으로 chat bot을 제작하는 방법은 매우 다양하다. 단순하게 rule based 기반으로 제작할 수도 있고, machine learning을 섞은 hybrid 기반, 특정 시나리오에서 동작 가능해지는 시나리오 기반까지 정의하는 사람에 따라 제작 방법이 매우 다양하다. 물론, 정의하는 것은 어디까지나 가용할 데이터의 성격에 매우 의존적일 것이다.</p></li><li><p>필자는 우선 제작방법 중에서 딥러닝 모델을 통한 chat bot을 만들어 볼 것이다. 또한 chat bot을 만들기 위한 딥러닝 모델에도 여러 가지가 있지만 그 중에서 번역 문제에서 이미 성능이 입증 된 Seq2seq를 활용하여 제작할 것이다.</p></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/20/NLP_13/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 실습 유사도를 반영한 검색 키워드 최적화</title>
      <link>https://heung-bae-lee.github.io/2020/02/11/NLP_12/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/11/NLP_12/</guid>
      <pubDate>Tue, 11 Feb 2020 08:16:56 GMT</pubDate>
      <description>
      
        
        
          &lt;ul&gt;
&lt;li&gt;이번 실습의 소개는 프로젝트성으로 진행 할 것이다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;프로젝트-소개&quot;&gt;&lt;a href=&quot;#프로젝트-소개&quot; class=&quot;headerlink&quot; title=&quot;프로젝트 소개&quot;&gt;&lt;/a&gt;프로젝트 소개&lt;/h3&gt;&lt;h4 
        
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>이번 실습의 소개는 프로젝트성으로 진행 할 것이다.</li></ul><h3 id="프로젝트-소개"><a href="#프로젝트-소개" class="headerlink" title="프로젝트 소개"></a>프로젝트 소개</h3><h4 id="더존-ICT-온라인-고객센터-키워드-검색-최적화-및-챗봇-구현"><a href="#더존-ICT-온라인-고객센터-키워드-검색-최적화-및-챗봇-구현" class="headerlink" title="더존 ICT 온라인 고객센터 키워드 검색 최적화 및 챗봇 구현"></a>더존 ICT 온라인 고객센터 키워드 검색 최적화 및 챗봇 구현</h4><ul><li><p>프로젝트를 하게 된 계기</p><ul><li><p>먼저, 더존 온라인 고객센터 페이지 중 smart A에 관한 페이지에서 전체 탭을 클릭한 후, 살펴본 QnA 페이지를 살펴보았다.</p><p><img src="/image/the_zone_online_center.png" alt="더존 온라인 고객센터"></p></li><li><p><code>필자는 고객들의 입장에서 생각해보았을때, 자신이 작성하는 질문(물론, 그림으로 첨부해야할 만큼 그 환경이 중요한 질문들은 제외하고)과 비슷한 질문들이 존재할 거라는 생각을 갖고 키워드를 통해 검색해볼 것</code>이다. 아래 그림은 <code>재입사자</code>라는 키워드를 smart A페이지에서 검색했을 때 출력되는 결과이다. 11건의 총 검색 결과 중 <code>재입사자에 대한 연말정산과 관련된 문건이 8건이 존재</code>한다.</p><p><img src="/image/smart_A_search_for_recruit.png" alt="smart A에서 재입사라는 키워드에 관한 검색 결과"></p></li><li><p>그래서 필자는 <code>재입사자 연말정산</code>이라는 키워드를 통해 검색을 해보았다. 위에서 재입사자라는 키워드를 통해 검색 했을 때, 재입사자의 연말정산에 대한 질문이 8건이 존재한 반면에 아래 그림에서와 같이 8건 중 5건 만을 보여준다.</p><p><img src="/image/smart_A_search_for_recruit_year.png" alt="smart A에서 재입사자 연말정산이라는 키워드에 관한 검색 결과">  </p></li><li><p>필자는 질문의 내용이 아닌 질문의 제목에 재입사자 연말정산이라는 키워드가 8건이 존재할 뿐 내용은 그와는 다를 수도 있다는 생각이 들어, 검색결과에 포함되어 있지 않는 질문들을 살펴보았다. 또한, 질문 내용 자체가 본질적으로 물어보는 의미가 검색결과에 포함되지 않은 질문들은 다를 수도 있기에 특정 알고리즘을 통해 결과를 보여줄 수 있다는 생각이 들어 검색 결과에 포함된 질문과도 비교해 보기로 했다.</p><p><img src="/image/non_search_for_recruit.png" alt="검색결과에 포함되지 않은 질문과 답변들"></p></li><li><p>검색결과에 포함되지 않은 질문과 답변이 왼쪽 그림이고, 검색결과에 포함된 질문과 답변이 오른쪽의 빨강색 네모로 되어있는 그림이다. 두 질문은 비슷한 질문이라고 보인다. 그런데도 불구하고 재입사자 연말정산이라는 키워드 검색 결과에 포함되어 있지 않는 점을 통해 필자는 <code>각각의 질문들과 검색 키워드 간의 유사성을 점수화해 유사성이 높은 질문들을 보여주는 시스템</code>도입이 필요할 것 같다는 생각이 들었다.</p><p><img src="/image/between_difference_search_non_search.png" alt="검색결과에 포함되지 않은 질문과 답변, 검색결과에 포함된 질문과 답변"></p></li><li><p>또한, 챗봇을 만드는 부분에 있어서 입력과 출력의 문장의 sequence 길이를 맞춰주어야 하는데, 그에 따라서 답변이 특정 분야(예를들면, 연말정산이나 원천징수등)에서는 긴 문장으로 이루어질 수도 있으므로, 챗봇을 구현한다면, 각 분야에 따른 문장길이를 분석해 보기도 해야 할 것 같다는 생각이들었다.</p></li><li><p>이런 제한 상황으로 인해 챗봇 구현이 힘들다면, 질문과 검색 키워드 간의 유사도를 반영한 검색 결과를 통해서라도 더존 온라인 고객센터의 질문을 하시는 고객 분들에게 조금이나마 더 편의성을 드릴수 있게끔 하면 좋을 것 같다는 생각이 들었다.</p></li><li><p><code>더존 사이트내에서 영업 문의 전화나 구매자에 대한 상담은 따로 서비스를 제공하고 있지만, 온라인 고객센터 tap부분에서만 Q&amp;A에 관한 사항을 다루는데 답변을 해주는 시간은 업무 시간내로만 제한</code> 되어있다. 이에 따라 24시간 또는 업무 이외의 시간에는 <code>챗봇 서비스를 시행한다면 고객들의 입장에서 보았을 때 조금 더 편리하게 더존의 서비스나 솔루션을 이용할 수 있을 것이라는 취지에 의해서 챗봇 구현에 관심</code>을 갖게 되었다.</p></li></ul></li><li><p>데이터 이름 : qna_smart_a.csv</p><ul><li>더존에서는 WEHAGO 플랫폼상에서 여러가지 서비스를 제공하고 있다. 그 중 더존 Smart A는 재무회계, 세무신고, 인사·급여관리, 물류관리까지 중소기업의 업무를 통합적으로 관리할 수 있는 회계프로그램로서, 이 프로그램의 질문과 답변에 의해서만 먼저 학습을 해 볼 것이다. 그 이유는 다른 프로그램들(ERP와 WEHAGO)은 사용자들의 성격에 따라 다양한 용도로 개발 되어있지만, 회계프로그램인 Smart A는 모든 기업이 공용으로 사용하기 때문에 우선적으로 학습해 볼 것이다. 또한, 가장 주요한 선택 이유는 Q&amp;A 게시판의 데이터 중 가장 많은 데이터를 포함하고 있었기 때문이다.</li></ul></li><li><p>데이터 용도 :</p></li><li><p>데이터 출처 : <a href="http://help.douzone.com/pboard/index.jsp?code=qna10&amp;pid=10&amp;s_category_id=all&amp;type=all&amp;page=1" target="_blank" rel="noopener">더존 온라인 고객센터 Smart A 전체 tap</a>의 전체 질문과 답변들을 크롤링 해서 사용하였다. 크롤링 방식은 Scrapy를 통해 페이지를 순회하게끔 코드를 작성하여 크롤링해서 얻었다.</p></li><li><p>먼저, 더존 온라인 고객 센터페이지에서 질문과 답변을 크롤링해와서 데이터 셋을 구성할 것이다.</p></li></ul><h3 id="Spider-bot-만들기"><a href="#Spider-bot-만들기" class="headerlink" title="Spider bot 만들기"></a>Spider bot 만들기</h3><ul><li><p>전체 scrapy bot의 구성은 다음과 같다.</p></li><li><p>items.py와 settings.py를 활용했으며, 마지막 결과 파일은 csv로 저장했다. 혹시 db파일로 저장하고 싶다면 추가적으로 pipelines에서 작업을 하면된다. 더존 온라인 고객센터의 게시판에서 최근 게시판에서는 답변 완료상태인 데이터가 주로 많지만 예전 데이터 중에는 간간히 답변 대기 상태인 데이터가 존재한다. 그러므로 pipeline.py에서 이를 통해 답변 완료인 상태인 데이터만을 크롤링하여도 되지만, 필자는 어떤 데이터가 답변 대기 상태인 데이터인지 눈으로 살펴보기 위해 그냥 모두 크롤링하는 것으로 처리하였다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">thezone</span><br><span class="line">├── scrapy.cfg</span><br><span class="line">└── thezone</span><br><span class="line">    ├── __init__.py</span><br><span class="line">    ├── __pycache__</span><br><span class="line">    │   ├── __init__.cpython-37.pyc</span><br><span class="line">    │   ├── items.cpython-37.pyc</span><br><span class="line">    │   ├── pipelines.cpython-37.pyc</span><br><span class="line">    │   └── settings.cpython-37.pyc</span><br><span class="line">    ├── items.py</span><br><span class="line">    ├── middlewares.py</span><br><span class="line">    ├── pipelines.py</span><br><span class="line">    ├── settings.py</span><br><span class="line">    └── spiders</span><br><span class="line">        ├── __init__.py</span><br><span class="line">        ├── __pycache__</span><br><span class="line">        │   ├── __init__.cpython-37.pyc</span><br><span class="line">        │   └── qnacrawler.cpython-37.pyc</span><br><span class="line">        ├── last_qna_smart_a.csv</span><br><span class="line">        ├── qna_smart_a.csv</span><br><span class="line">        └── qnacrawler.py</span><br></pre></td></tr></table></figure><ul><li>qnacrawler.py</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line">import sys</span><br><span class="line">sys.path.insert(0, <span class="string">'/Users/heungbaelee/workspace/project/chat_bot_project/thezone/thezone'</span>)</span><br><span class="line">from items import ThezoneItem</span><br><span class="line"></span><br><span class="line">class QnacrawlerSpider(CrawlSpider):</span><br><span class="line">    name = <span class="string">'qnacrawler'</span></span><br><span class="line">    allowed_domains = [<span class="string">'help.douzone.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://help.douzone.com/pboard/index.jsp?code=qna10&amp;pid=10&amp;s_category_id=all&amp;type=all&amp;s_listnum=50&amp;s_field=&amp;s_keyword='</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># rules = [</span></span><br><span class="line">    <span class="comment">#     Rule(LinkExtractor(allow=r'/pboard/index.jsp?code=qna10&amp;pid=10&amp;s_category_id=all&amp;type=all&amp;s_listnum=50&amp;s_field=&amp;s_keyword=&amp;page=\d+', ), callback='parse_parent', follow=True),</span></span><br><span class="line">    <span class="comment"># ]</span></span><br><span class="line">    rules = [</span><br><span class="line">        Rule(LinkExtractor(restrict_css=<span class="string">'div.page_box &gt; ul &gt; li:nth-child(n+4)'</span>,attrs=<span class="string">'href'</span>), callback=<span class="string">'parse_parent'</span>, follow=True),</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse_parent(self, response):</span><br><span class="line">        <span class="comment"># link = LinkExtractor(allow=r'/pboard/index.jsp?code=qna10&amp;pid=10&amp;s_category_id=all&amp;type=all&amp;s_listnum=50&amp;s_field=&amp;s_keyword=&amp;page=\d+')</span></span><br><span class="line">        <span class="comment"># links = link.extract_links(response)</span></span><br><span class="line">        <span class="comment"># print(links)</span></span><br><span class="line">        <span class="comment"># print(response.status)</span></span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> response.css(<span class="string">'div.tab_cnt.mt30 &gt; table &gt; tbody &gt; tr'</span>):</span><br><span class="line">            article_num = url.css(<span class="string">'td:nth-child(1)::text'</span>).extract_first().strip()</span><br><span class="line">            self.logger.info(<span class="string">'Article number : %s'</span> % article_num)</span><br><span class="line">            article_link = url.css(<span class="string">'td:nth-child(3) &gt; a::attr(href)'</span>).extract_first().strip()</span><br><span class="line">            self.logger.info(<span class="string">'Article link : %s'</span> % article_link)</span><br><span class="line">            <span class="comment"># print(article_num, response.urljoin(article_link))</span></span><br><span class="line">            yield scrapy.Request(response.urljoin(article_link), self.parse_child, meta=&#123;<span class="string">'article_num'</span>: article_num&#125;)</span><br><span class="line"></span><br><span class="line">    def parse_child(self, response):</span><br><span class="line">        <span class="comment"># 부모, 자식 수신 정보 로깅</span></span><br><span class="line">        self.logger.info(<span class="string">'----------------------------------------'</span>)</span><br><span class="line">        self.logger.info(<span class="string">'Child Response URL : %s'</span> % response.url)</span><br><span class="line">        self.logger.info(<span class="string">'Child Response Status ; %s'</span> % response.status)</span><br><span class="line">        self.logger.info(<span class="string">'----------------------------------------'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 질문 번호</span></span><br><span class="line">        article_num = response.meta[<span class="string">'article_num'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 유형</span></span><br><span class="line">        category = response.css(<span class="string">"div.qna_read.mt30 &gt; table:nth-child(1) &gt; tbody &gt; tr:nth-child(2) &gt; td &gt; dl &gt; dd:nth-child(2)::text"</span>).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 질문</span></span><br><span class="line">        question = <span class="string">""</span>.join(response.css(<span class="string">"div.qna_read.mt30 &gt; table:nth-child(1) &gt; tbody &gt; tr:nth-child(2) &gt; td &gt; div.q &gt; div.q_cnt &gt; p::text"</span>).extract()).strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 등록일</span></span><br><span class="line">        enrolled_date_time = response.css(<span class="string">"div.qna_read.mt30 &gt; table:nth-child(1) &gt; tbody &gt; tr:nth-child(1) &gt; td:nth-child(4)::text"</span>).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 작성일</span></span><br><span class="line">        answer_date_time = response.css(<span class="string">"table.mt10 &gt; tbody &gt; tr:nth-child(1) &gt; td:nth-child(4)::text"</span>).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 답변여부</span></span><br><span class="line">        answer_yes = response.css(<span class="string">"table.mt10 &gt; tbody &gt; tr:nth-child(1) &gt; td.ta_l &gt; span::text"</span>).extract_first().strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 답변</span></span><br><span class="line">        answering = <span class="string">""</span>.join(response.css(<span class="string">"table.mt10 &gt; tbody &gt; tr:nth-child(2) &gt; td.ta_l.pd20 &gt; div.a &gt; div &gt; p::text"</span>).extract()).strip()</span><br><span class="line"></span><br><span class="line">        yield ThezoneItem(article_num=article_num, category=category, enrolled_date_time=enrolled_date_time, question=question, answer_date_time=answer_date_time, answer_yes=answer_yes, answering=answering)</span><br></pre></td></tr></table></figure><ul><li>items.py</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ThezoneItem(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    <span class="comment"># 문서번호</span></span><br><span class="line">    article_num = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 유형</span></span><br><span class="line">    category = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 질문</span></span><br><span class="line">    question = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 답변</span></span><br><span class="line">    answering = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 작성일</span></span><br><span class="line">    answer_date_time = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 등록일</span></span><br><span class="line">    enrolled_date_time = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 답변여부</span></span><br><span class="line">    answer_yes = scrapy.Field()</span><br></pre></td></tr></table></figure><ul><li>settings.py</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line">BOT_NAME = <span class="string">'thezone'</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">'thezone.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'thezone.spiders'</span></span><br><span class="line"></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;<span class="string">'Referer'</span> : <span class="string">'http://help.douzone.com'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = False</span><br><span class="line"></span><br><span class="line"><span class="comment"># 쿠키사용</span></span><br><span class="line">COOKIES_ENABLED = True</span><br><span class="line"></span><br><span class="line">DOWNLOAD_DELAY = 3</span><br><span class="line"></span><br><span class="line"><span class="comment"># User-Agent 미들웨어 사용</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: None,</span><br><span class="line">    <span class="string">'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware'</span>: 400,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 파이프 라인 활성화</span></span><br><span class="line"><span class="comment"># 숫자가 작을 수록 우선순위 상위</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'thezone.pipelines.ThezonePipeline'</span>: 300,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 재시도 횟수</span></span><br><span class="line">RETRY_ENABLED = True</span><br><span class="line">RETRY_TIMES = 2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 한글 쓰기(출력 인코딩)</span></span><br><span class="line">FEED_EXPORT_ENCODING = <span class="string">'utf-8'</span></span><br></pre></td></tr></table></figure><ul><li>위의 scrapy 파일들을 통해서 데이터를 먼저 확보 했다. 필자의 로컬환경을 통해서는 10시간 정도 걸렸다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy runspider qnacrawler.py -o qna_smart_a.csv - t csv</span><br></pre></td></tr></table></figure><h2 id="데이터-소개"><a href="#데이터-소개" class="headerlink" title="데이터 소개"></a>데이터 소개</h2><ul><li><p>위의 scrapy spider bot을 통해서 얻은 데이터를 통해 다음과 같은 feature들을 얻었다.</p></li><li><p>회계프로그램인 smart_a에 대한 전체 Q&amp;A를 크롤링하여 챗봇을 만드는 것이 프로젝트의 목표이다.</p></li></ul><h3 id="raw-데이터-구성"><a href="#raw-데이터-구성" class="headerlink" title="raw 데이터 구성"></a>raw 데이터 구성</h3><ul><li>answer_date_time : 답변완료일자</li><li>answer_yes : 답변 여부</li><li>answering : 답변 내용</li><li>category : 질문의 유형</li><li>enrolled_date_time : 질문등록일자</li><li>question : 질문 내용</li></ul><p><img src="/image/thezone_raw_data_head.png" alt="thezone smart A 온라인고객센터 데이터"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_data.info()</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;class <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">RangeIndex: 12475 entries, 0 to 12474</span><br><span class="line">Data columns (total 6 columns):</span><br><span class="line">answer_date_time      12436 non-null object</span><br><span class="line">answer_yes            12436 non-null object</span><br><span class="line">answering             12420 non-null object</span><br><span class="line">category              12475 non-null object</span><br><span class="line">enrolled_date_time    12475 non-null object</span><br><span class="line">question              12409 non-null object</span><br><span class="line">dtypes: object(6)</span><br><span class="line">memory usage: 584.9+ KB</span><br></pre></td></tr></table></figure><ul><li>데이터에 null 값이 포함되어 있기 때문에 null값들을 제거해주고, 답변 대기 상태인 데이터들은 총 39건이 있었는데 답변이 작성되지 않은 데이터 이므로 답변 대기 상태인 데이터들도 같이 제거해준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">raw_data = raw_data[raw_data[<span class="string">"answer_yes"</span>]==<span class="string">"답변완료"</span>]</span><br><span class="line">raw_data.reset_index(drop=True, inplace=True)</span><br><span class="line"></span><br><span class="line">raw_data = raw_data[pd.isnull(raw_data[<span class="string">"question"</span>])!=True].reset_index(drop=True)</span><br><span class="line"><span class="built_in">print</span>(sum(raw_data[<span class="string">"question"</span>].apply(lambda x: pd.isnull(x))))</span><br><span class="line"></span><br><span class="line">raw_data = raw_data[pd.isnull(raw_data[<span class="string">"answering"</span>])!=True].reset_index(drop=True)</span><br><span class="line"><span class="built_in">print</span>(sum(raw_data[<span class="string">"question"</span>].apply(lambda x: pd.isnull(x))))</span><br></pre></td></tr></table></figure><ul><li>답변대기 상태인 데이터들을 제거하고 총 사용가능한 데이터는 12,354건의 질문과 답변 쌍이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;class <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">RangeIndex: 12354 entries, 0 to 12353</span><br><span class="line">Data columns (total 6 columns):</span><br><span class="line">answer_date_time      12354 non-null object</span><br><span class="line">answer_yes            12354 non-null object</span><br><span class="line">answering             12354 non-null object</span><br><span class="line">category              12354 non-null object</span><br><span class="line">enrolled_date_time    12354 non-null object</span><br><span class="line">question              12354 non-null object</span><br><span class="line">dtypes: object(6)</span><br><span class="line">memory usage: 579.2+ KB</span><br></pre></td></tr></table></figure><ul><li>먼저, 간단하게 데이터들의 분류 카테고리에 따라서 어떤 분포를 띄고 있는지 간략하게 살펴볼 것이다.</li></ul><p><img src="/image/category_histogram_thezone.png" alt="카테고리 별 질문 및 답변쌍의 개수"></p><h3 id="질문-데이터-전처리"><a href="#질문-데이터-전처리" class="headerlink" title="질문 데이터 전처리"></a>질문 데이터 전처리</h3><ul><li>세무/회계관련 질문들이라서 금액에 관한 질문과 답변들이 많이 있기에 숫자에 대한 내용을 제거할지 하지 말하야 할지를 두고 필자는 생각이 많았는데, 우선 프로젝트의 첫번째 목표인 검색 키워드와 질문의 내용간의 유사도를 측정하는 면에 있어서는 숫자들이 크게 중요하지 않을 것이라는 판단하에 <code>숫자부분들과 마침표같은 부호들을 제거</code>하기로 결정했다. 다만, <code>[]안의 내용은 대부분 smart A의 메뉴명을 의미하기 때문에 살려</code>두었다. <code>[메뉴명]을 하나의 명사로 인식하기 위해 형태소 분석을 할 경우에도 비지도 학습을 통한 방식을 채택하기 위해 soynlp를 사용할 것</code>이다.</li></ul><p><img src="/image/thezone_raw_data_question.png" alt="질문 데이터들"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def pattern_match(x):</span><br><span class="line"></span><br><span class="line">    pattern = <span class="string">"\d+"</span></span><br><span class="line">    reg = re.compile(pattern)</span><br><span class="line">    sentence = re.sub(reg, <span class="string">" "</span>, x)</span><br><span class="line"></span><br><span class="line">    pattern = <span class="string">"[!|,|.|?|~|※|)|(|■|+|=|-|/|*|-|&gt;|-|;|^|]|-|%|'|'|ㅠ+|ㅎ+]"</span></span><br><span class="line">    reg = re.compile(pattern)</span><br><span class="line">    sentence = re.sub(reg, <span class="string">" "</span>, sentence)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raw_data[<span class="string">'question_after'</span>] = raw_data[<span class="string">'question_after'</span>].apply(lambda x : pattern_match(str(x)))</span><br></pre></td></tr></table></figure><ul><li>기본적인 부호들과 숫자들을 제거해 주었으므로 이제 기본적인 <code>띄어쓰기 단위 어절과 음절(문자 하나하나를 의미)단위로 질문의 평균적인 길이와 한 질문당 단어의 평균적인 사용량을 대략적으로 살펴볼 것</code>이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 띄어쓰기 단위로 나눈 어절 기초통계량</span></span><br><span class="line">raw_data[<span class="string">'question_after'</span>].apply(lambda x: len(str(x).split(<span class="string">" "</span>))).describe()</span><br></pre></td></tr></table></figure><ul><li>위의 띄어쓰기 단위로 나눈 질문의 Token의 개수는 평균적으로 33개의 어절과 중앙값은 27개의 어절을 갖는다는 것을 확인 할 수 있다. 평균이 올라간것은 3사분위수가 42개인 것과 최대 어절이 791개인 것으로 미루어 보아 이상치에 의한 영향을 받아 평균이 데이터의 중심을 잘 반영하고 있지 않다고 판단해 볼 수 있다. 그러므로 이상치들의 데이터 형태를 살펴보고 문제점이 무엇인지 파악해 볼 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">count    12290.000000</span><br><span class="line">mean        35.055411</span><br><span class="line">std         33.537140</span><br><span class="line">min          1.000000</span><br><span class="line">25%         17.000000</span><br><span class="line">50%         27.000000</span><br><span class="line">75%         42.000000</span><br><span class="line">max        791.000000</span><br><span class="line">Name: question_after, dtype: float64</span><br></pre></td></tr></table></figure><ul><li>가장 높은 최댓값을 갖는 데이터를 살펴보면, 아래의 그림과 같이 공백으로 일정한 형식을 맞춰보려고 한 것 같이 되어있다. 그러나 우리는 이 질문의 내용적인 면이나 키워드가 중요한 것이므로 형식이 우리가 푸는 문제에는 큰 영향을 주지 못하므로 공백을 제거해 줄 것이다.</li></ul><p><img src="/image/many_space_bar_thezone_question.png" alt="이상치 데이터의 모습"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def pattern_match(x):</span><br><span class="line">    pattern = <span class="string">"  +"</span></span><br><span class="line">    reg = re.compile(pattern)</span><br><span class="line">    sentence = re.sub(reg, <span class="string">" "</span>, x)</span><br><span class="line">    <span class="built_in">return</span> sentence</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">raw_data[<span class="string">'question_after'</span>] = raw_data[<span class="string">'question_after'</span>].apply(lambda x : pattern_match(str(x)))</span><br><span class="line"></span><br><span class="line">sent_len_by_token = raw_data[<span class="string">'question_after'</span>].apply(lambda x: len(str(x).split(<span class="string">" "</span>)))</span><br></pre></td></tr></table></figure><ul><li>공백이 많은 데이터들을 공백을 줄여주는 함수를 통해 처리를 해준 후에 다시 질문 당 띄어쓰기 단위 어절의 길이에 관한 기초 통계량을 살펴보면 다음과 같다. 역시 함수를 통해 공백을 줄여준 후에 다시 측정해보니 평균과 중앙값의 차이가 이전과 다르게 확연히 줄어든 것을 볼 수 있으며, 평균적으로 22~23개의 어절을 사용함을 확인해 볼 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">count    12290.000000</span><br><span class="line">mean        28.416273</span><br><span class="line">std         22.040055</span><br><span class="line">min          1.000000</span><br><span class="line">25%         15.000000</span><br><span class="line">50%         23.000000</span><br><span class="line">75%         35.000000</span><br><span class="line">max        355.000000</span><br><span class="line">Name: question_after, dtype: float64</span><br></pre></td></tr></table></figure><ul><li>90%의 위치에 위치하고 있는 어절의 길이는 52개 였다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.quantile(sent_len_by_token, 0.90)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">52.0</span><br></pre></td></tr></table></figure><ul><li>또한, 위에서 355개의 어절을 갖는 데이터에 관해서도 이상치이므로 살펴보았다. 아래와 같이 오류 코드에 관한 질문이었기 때문에 공백이 많이 포함되어있을 수 밖에 없다는 것을 확인 할 수 있었다. 그러므로 이 데이터의 공백은 질문의 내용을 표현하는데 불필요한 요소가 아니므로 그대로 상태를 유지 할 것이다.</li></ul><p><img src="/image/eujeol_token_maximum_last.png" alt="355개의 어절을 갖는 질문"></p><ul><li>그 다음은 <code>음절 단위 길이를 분석</code>해 볼 것이다. 음절 단위의 기초 통계량은 아래와 같다. 평균적으로 136자를 사용하였으며, 중앙값은 112자이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">count    12279.000000</span><br><span class="line">mean       136.459647</span><br><span class="line">std        109.823703</span><br><span class="line">min          3.000000</span><br><span class="line">25%         74.000000</span><br><span class="line">50%        112.000000</span><br><span class="line">75%        167.000000</span><br><span class="line">max       2637.000000</span><br><span class="line">Name: question_after, dtype: float64</span><br></pre></td></tr></table></figure><ul><li>위에서의 기초 통계량 값을 시각화해서 간단히 살펴 보기위해서 아래와 같이 히스토그램을 활용하였다. 상식적으로도 알 수 있듯이, 음절이 어절보다 훨씬 단위가 클수밖에 없을 것이다. 여기서 볼 것은 꼬리 분포이다. 음절과 어절 단위로 살펴본 질문의 길이는 둘다 일정 수준이하에 주로 분포돼있고, 일정 수준 이상은 이상치가 존재하고 있다.</li></ul><p><img src="/image/sentence_length_by_emjeol_and_euojeol.png" alt="어절 및 음절 단위 문장 길이 히스토그램"></p><h3 id="모델-설정"><a href="#모델-설정" class="headerlink" title="모델 설정"></a>모델 설정</h3><ul><li>제일 먼저, TF-IDF 행렬을 사용해 LSA 분석의 일종인 TruncatedSVD 행렬을 이용해 문장 임베딩을 실행한 후, 키워드 검색어와의 유사한 문서들을 살펴 볼 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from soynlp.word import WordExtractor</span><br><span class="line">from soynlp.tokenizer import LTokenizer</span><br><span class="line">from sklearn.decomposition import TruncatedSVD</span><br><span class="line">from sklearn.preprocessing import normalize</span><br><span class="line">from sklearn.metrics.pairwise import cosine_similarity</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">q_sentence = list(raw_data[<span class="string">'question_after'</span>])</span><br><span class="line"></span><br><span class="line">word_extractor = WordExtractor(min_frequency=1, min_cohesion_forward=0.05, min_right_branching_entropy=0.0)</span><br><span class="line">word_extractor.train(q_sentence)</span><br><span class="line">scores = word_extractor.word_scores()</span><br><span class="line"></span><br><span class="line">cohesion_scores = &#123;key:(scores[key].cohesion_forward * math.exp(scores[key].right_branching_entropy)) <span class="keyword">for</span> key <span class="keyword">in</span> scores.keys()&#125;</span><br><span class="line">tokenizer = LTokenizer(scores=cohesion_scores)</span><br><span class="line"></span><br><span class="line">tokens = []</span><br><span class="line"><span class="keyword">for</span> q_s <span class="keyword">in</span> q_sentence:</span><br><span class="line">    tokens.append(tokenizer.tokenize(q_s))</span><br><span class="line"></span><br><span class="line">sentence_by_tokens = [<span class="string">' '</span>.join(word) <span class="keyword">for</span> word <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line"><span class="comment">## TfidfVectorizer</span></span><br><span class="line">vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,1), lowercase=True, tokenizer=lambda x : x.split(<span class="string">" "</span>))</span><br><span class="line">input_matrix = vectorizer.fit_transform(sentence_by_tokens)</span><br><span class="line"></span><br><span class="line">vocab2id = &#123;token : vectorizer.vocabulary_[token] <span class="keyword">for</span> token <span class="keyword">in</span> vectorizer.vocabulary_.keys()&#125;</span><br><span class="line"></span><br><span class="line">id2vocab = &#123;vectorizer.vocabulary_[token]: token <span class="keyword">for</span> token <span class="keyword">in</span> vectorizer.vocabulary_.keys()&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">## TruncatedSVD</span></span><br><span class="line">svd =  TruncatedSVD(n_components=100)</span><br><span class="line">vecs = svd.fit_transform(input_matrix)</span><br><span class="line"></span><br><span class="line">criterion_sentence = <span class="string">"재입사자 연말정산"</span></span><br><span class="line"></span><br><span class="line">criterion_tokens = tokenizer.tokenize(criterion_sentence)</span><br><span class="line">criterion_tokens</span><br></pre></td></tr></table></figure><ul><li><code>재입사자 연말정산</code>이라는 키워드를 tokenizing한 결과는 아래와 같다. <code>재입사</code>, <code>자</code>, <code>연말정산</code> 이렇게 3가지 형태로 형태소를 분리했다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'재입사'</span>, <span class="string">'자'</span>, <span class="string">'연말정산'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">criterion_sentence_by_token = [<span class="string">" "</span>.join(criterion_tokens)]</span><br><span class="line">criterion_vec = vectorizer.transform(criterion_sentence_by_token)</span><br><span class="line">criterion_vec = svd.transform(criterion_vec)</span><br><span class="line"></span><br><span class="line">svd_l2norm_vectors = normalize(vecs, axis=1, norm=<span class="string">'l2'</span>)</span><br><span class="line">svd_l2norm_criterion_vectors = normalize(criterion_vec, axis=1, norm=<span class="string">'l2'</span>).reshape(100,1)</span><br><span class="line">cosine_similarity = np.dot(svd_l2norm_vectors, svd_l2norm_criterion_vectors)</span><br><span class="line"></span><br><span class="line">ls=[]</span><br><span class="line"><span class="keyword">for</span> idx, cosine_similarity <span class="keyword">in</span> enumerate(cosine_similarity.tolist()):</span><br><span class="line">    ls.append((idx, cosine_similarity))</span><br><span class="line">sorted_list = sorted(ls, key= lambda x: x[1], reverse=True)</span><br><span class="line"></span><br><span class="line">criterion_tokens_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(len(sorted_list)):</span><br><span class="line">    criterion_tokens_list.append(criterion_tokens)</span><br><span class="line">show_list = []</span><br><span class="line"><span class="keyword">for</span> sorted_lists, criterion_tokens <span class="keyword">in</span> zip(sorted_list, criterion_tokens_list):</span><br><span class="line">    idx=sorted_lists[0]</span><br><span class="line">    similarity=sorted_lists[1]</span><br><span class="line">    tf_list=[]</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> criterion_tokens:</span><br><span class="line">        tf_list.append(token <span class="keyword">in</span> raw_data[<span class="string">'question'</span>].loc[idx])</span><br><span class="line">    <span class="keyword">if</span> (np.array(tf_list) == True).all():</span><br><span class="line">        show_list.append((idx, similarity))</span><br><span class="line">show_list</span><br></pre></td></tr></table></figure><ul><li>위의 show_list결과 중 몇가지 질문들을 살펴보자면, 아래와 같다.</li></ul><p><img src="/image/not_noun_score_with_show_list.png" alt="최종적으로 선택된 질문들"></p><ul><li>위에서 형태소가 <code>재입사</code>, <code>자</code>, <code>연말정산</code> 이렇게 3가지로 분리했던 것을 우리가 알 고 있듯이 <code>재입사자</code>, <code>연말정산</code> 2가지로 잘 분리하도록 명사 추출기 점수를 더한 점수를 통해서 다시 tokenize할 것이다.</li></ul><h3 id="명사-추출기를-통한-명사-점수를-합산한-score를-통한-tokenizer-활용"><a href="#명사-추출기를-통한-명사-점수를-합산한-score를-통한-tokenizer-활용" class="headerlink" title="명사 추출기를 통한 명사 점수를 합산한 score를 통한 tokenizer 활용"></a>명사 추출기를 통한 명사 점수를 합산한 score를 통한 tokenizer 활용</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">from soynlp.word import WordExtractor</span><br><span class="line">from soynlp.noun import LRNounExtractor_v2</span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from sklearn.decomposition import TruncatedSVD</span><br><span class="line">from sklearn.preprocessing import normalize</span><br><span class="line">from sklearn.metrics.pairwise import cosine_similarity</span><br><span class="line"></span><br><span class="line">noun_extractor = LRNounExtractor_v2(verbose=True)</span><br><span class="line">nouns = noun_extractor.train_extract(q_sentence)</span><br><span class="line"></span><br><span class="line">noun_scores = &#123;noun:score.score <span class="keyword">for</span> noun, score <span class="keyword">in</span> nouns.items()&#125;</span><br><span class="line">combined_scores = &#123;noun:score + cohesion_scores.get(noun, 0) <span class="keyword">for</span> noun, score <span class="keyword">in</span> noun_scores.items()&#125;</span><br><span class="line">combined_scores = combined_scores.update(</span><br><span class="line">    &#123;subword:cohesion <span class="keyword">for</span> subword, cohesion <span class="keyword">in</span> cohesion_scores.items()</span><br><span class="line">    <span class="keyword">if</span> not (subword <span class="keyword">in</span> combined_scores)&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">tokenizer = LTokenizer(scores=combined_scores)</span><br><span class="line"></span><br><span class="line">tokens = []</span><br><span class="line"><span class="keyword">for</span> q_s <span class="keyword">in</span> q_sentence:</span><br><span class="line">    tokens.append(tokenizer.tokenize(q_s))</span><br><span class="line"></span><br><span class="line">sentence_by_tokens = [<span class="string">' '</span>.join(word) <span class="keyword">for</span> word <span class="keyword">in</span> tokens]</span><br><span class="line"></span><br><span class="line">vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1,1), lowercase=True, tokenizer=lambda x : x.split(<span class="string">" "</span>))</span><br><span class="line">input_matrix = vectorizer.fit_transform(sentence_by_tokens)</span><br><span class="line"></span><br><span class="line">vocab2id = &#123;token : vectorizer.vocabulary_[token] <span class="keyword">for</span> token <span class="keyword">in</span> vectorizer.vocabulary_.keys()&#125;</span><br><span class="line"></span><br><span class="line">id2vocab = &#123;vectorizer.vocabulary_[token]: token <span class="keyword">for</span> token <span class="keyword">in</span> vectorizer.vocabulary_.keys()&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">svd =  TruncatedSVD(n_components=100)</span><br><span class="line">vecs = svd.fit_transform(input_matrix)</span><br><span class="line"></span><br><span class="line">criterion_sentence = <span class="string">"재입사자 연말정산"</span></span><br><span class="line"></span><br><span class="line">criterion_tokens = tokenizer.tokenize(criterion_sentence)</span><br><span class="line">criterion_tokens</span><br></pre></td></tr></table></figure><ul><li><code>재입사자 연말정산</code>이라는 검색 키워드를 tokenizing한 결과 아래와 같이 <code>재입사자</code>, <code>연말정산</code>이라고 분류해냈다. 허나, 위에서와 같이</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'재입사자'</span>, <span class="string">'연말정산'</span>]</span><br></pre></td></tr></table></figure><ul><li>sorted_list에 포함된 질문들을 보면 대부분 연말정산이 들어가있는 질문들이 유사도가 높다는 것을 확인할 수 있었다. <code>이런 문제점은 필자의 개인적인 생각으로 input matrix로 TF-IDF matrix를 사용했기 때문에 전체 질문 건수에서 연말정산이 차지하는 비율이 높다보니 나타나는 현상</code>이라고 생각했다. 이를 해결하기 위해 먼저 필자는 <code>각 분야의 질문의 수를 맞추거나 다른 방법의 input matrix를 사용해서 문제를 해결해야 할 것이라고 생각</code>했다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">criterion_sentence_by_token = [<span class="string">" "</span>.join(criterion_tokens)]</span><br><span class="line">criterion_vec = vectorizer.transform(criterion_sentence_by_token)</span><br><span class="line"></span><br><span class="line">criterion_vec=svd.transform(criterion_vec)</span><br><span class="line"></span><br><span class="line">svd_l2norm_vectors = normalize(vecs, axis=1, norm=<span class="string">'l2'</span>)</span><br><span class="line">svd_l2norm_criterion_vectors = normalize(criterion_vec, axis=1, norm=<span class="string">'l2'</span>).reshape(100,1)</span><br><span class="line">cosine_similarity = np.dot(svd_l2norm_vectors, svd_l2norm_criterion_vectors)</span><br><span class="line"></span><br><span class="line">ls=[]</span><br><span class="line"><span class="keyword">for</span> idx, cosine_similarity <span class="keyword">in</span> enumerate(cosine_similarity.tolist()):</span><br><span class="line">    ls.append((idx, cosine_similarity))</span><br><span class="line">sorted_list = sorted(ls, key= lambda x: x[1], reverse=True)</span><br><span class="line"></span><br><span class="line">criterion_tokens_list = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.arange(len(sorted_list)):</span><br><span class="line">    criterion_tokens_list.append(criterion_tokens)</span><br><span class="line">show_list_noun = []</span><br><span class="line"><span class="keyword">for</span> sorted_lists, criterion_tokens <span class="keyword">in</span> zip(sorted_list, criterion_tokens_list):</span><br><span class="line">    idx=sorted_lists[0]</span><br><span class="line">    similarity=sorted_lists[1]</span><br><span class="line">    tf_list=[]</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> criterion_tokens:</span><br><span class="line">        tf_list.append(token <span class="keyword">in</span> raw_data[<span class="string">'question'</span>].loc[idx])</span><br><span class="line">    <span class="keyword">if</span> (np.array(tf_list) == True).all():</span><br><span class="line">        show_list_noun.append((idx, similarity))</span><br><span class="line">show_list_noun</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">show_list_index = []</span><br><span class="line"><span class="keyword">for</span> idx, score <span class="keyword">in</span> show_list:</span><br><span class="line">    show_list_index.append(idx)</span><br><span class="line"></span><br><span class="line">show_list_noun_index = []</span><br><span class="line"><span class="keyword">for</span> idx, score <span class="keyword">in</span> show_list_noun:</span><br><span class="line">    show_list_noun_index.append(idx)</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span>(show_list_noun_index) == <span class="built_in">set</span>(show_list_index)</span><br></pre></td></tr></table></figure><ul><li>결과는 False로 처음 명사추출기 점수를 더해서 tokenizing한 결과가 더 많고 좋은 질문들을 검색해 내었다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">False</span><br></pre></td></tr></table></figure><h3 id="결론"><a href="#결론" class="headerlink" title="결론"></a>결론</h3><ul><li><p>연말정산 카테고리의 문건이 전체 문건 중 다수를 포함하고 있기 때문에, 그에 따른 영향으로 검색 키워드에 연말정산이 포함되면 유사도가 큰 문건들은 대부분 연말정산의 내용만을 담고 있었기 때문에 추후에 tokenizing한 검색 키워드를 전부 포함하고 있는 문건들을 출력해주는 방식으로 바꾸었다. 처음의 model을 최종적으로 선택할 것이며, 현재의 검색어 시스템에서 <code>재입사자</code>와 <code>재입사자 연말정산</code>이라는 두 가지 키워드에 대한 검색이 아래 그림과 같이 <code>재입사자</code>는 11건 <code>재입사자 연말정산</code>는 5건으로 <code>재입사자</code> 키워드에서 대부분이 연말정산에 관한 내용임에도 불구하고 검색이 되지 않는 문제점은 해결할 수 있다는 것에 만족할 것이다. 게다가 기존의 <code>띄어쓰기에 취약한 문제점도 보완</code>할 수 있기에 이전보다는 더 나은 검색 시스템이라고 주장한다.</p></li><li><p>아래 그림은 더존 온라인 고객센터의 smart A 게시판에서 동일한 내용이지만 띄어쓰기만 다른 <code>재입사자 연말정산</code>(위)과 <code>재입사자 연말 정산</code>(아래)이라는 두 키워드를 검색한 결과이다.</p></li></ul><p><img src="/image/correct_sentence_keyword_thezone.png" alt="재입사자 연말정산 검색 결과"></p><p><img src="/image/wrong_sentence_keyword_thezone.png" alt="재입사자 연말 정산 검색 결과"></p><ul><li>위 같이 띄어쓰기가 달라도 검색어를 입력했을 때 동일한 결과를 얻을 수 있었다.</li></ul><p><img src="/image/wrong_sentence_tokenizing.png" alt="재입사자 연말 정산 키워드 토크나이징 결과"></p><p><img src="/image/result_thezone_search_system.png" alt="모델의 결과"></p><h3 id="보완점"><a href="#보완점" class="headerlink" title="보완점"></a>보완점</h3><ul><li><code>TF-IDF를 사용하였기 때문에 단어와 단어가 사용된 문건의 수에 의한 가중치에 의해 영향을 받는다는 점을 고려</code>했었야 한다는 판단을 내렸다. 또한, 검색 키워드를 나중에 <code>유사도를 계산한 리스트 중에 필터링 역할로 사용하기에 검색 키워드를 기반으로 하되 불필요한 부분을 제거하여 사용할 수 있는 알고리즘을 만들면 더 좋은 검색 시스템을 구성할 수 있을 것으로 기대</code> 된다. 또한, <code>검색 시스템 뿐만 아니라 자신이 질문을 작성한 후에 자신의 질문과 유사도가 높은 질문들의 리스트를 보여주는 페이지로 전환시켜 주는 서비스</code>도 좋을 것 같다. 이러한 생각이 들었던 이유는 더존 온라인 고객센터의 답변들 중 연말정산 같은 회계분야의 특정 시즌 때 질문들에 대한 답변이 조금 늦는 경우(물론 하루이상을 넘기지 않고 답변을 다 달아주신다)를 보았는데, 위와 같은 시스템을 도입하면 온라인 고객센터의 직원 분들도 덜 고생하시고, 고객님들께서도 조금 더 해결방안을 빨리 찾으실 수 있을 것 같다고 생각했기 때문이다.</li></ul><h3 id="토이-프로젝트를-하면서-느낀점"><a href="#토이-프로젝트를-하면서-느낀점" class="headerlink" title="토이 프로젝트를 하면서 느낀점"></a>토이 프로젝트를 하면서 느낀점</h3><ul><li>제일 많은 것을 느낀것은 전처리부분이었다. 데이터 분석에 있어서 전처리가 80%이상이라는 말은 매번 되새기게되지만, 이번에는 특히나 더 와 닿았던 토이 프로젝트 였던 것 같다. 다음 토이 프로젝트로 질문에 대한 답변을 작성해 주는 챗봇을 구현해 보려고하는데, 질문의 카테고리별로 모델을 따로 만들어야 될 것 같다는 생각이 들었다. 챗봇을 구현할 때 먼저 입,출력 벡터의 크기를 일정하게 정해서 부족하면 패딩처리하는 방식으로 사용하여야 하는데, 각 카테고리별로 답변을 주는 평균적인 답변과 질문의 sequence의 길이가 다를 것이라고 생각 했기 때문이다.</li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/11/NLP_12/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 실습 텍스트 유사도 - 02 (XGBoost, 1D-CNN, MaLSTM)</title>
      <link>https://heung-bae-lee.github.io/2020/02/11/NLP_11/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/11/NLP_11/</guid>
      <pubDate>Mon, 10 Feb 2020 16:36:58 GMT</pubDate>
      <description>
      
        
        
          &lt;ul&gt;
&lt;li&gt;모델은 총 3가지를 종류를 만들어 볼 것이다.&lt;ul&gt;
&lt;li&gt;XGBoost&lt;/li&gt;
&lt;li&gt;CNN&lt;/li&gt;
&lt;li&gt;MaLSTM&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;XGBoost&quot;&gt;&lt;a href=&quot;#XGBoost&quot; c
        
      
      </description>
      
      
      <content:encoded><![CDATA[<ul><li>모델은 총 3가지를 종류를 만들어 볼 것이다.<ul><li>XGBoost</li><li>CNN</li><li>MaLSTM</li></ul></li></ul><h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><ul><li><p>앙상블 모델 중 하나인 XGBoost 모델은 ‘eXtream Gradient Boosting’의 약자로 캐글 사용자에 큰 인기를 얻은 모델 중 하나이다. 앙상블 기법이란 여러 개의 학습 알고즘을 사용해 더 좋은 성능을 얻는 방법을 뜻한다. 앙상블 기법에는 Bagging과 Boosting이라는 방법이 있다.</p><ul><li><p><code>Bagging</code>은 여러 개의 학습 알고리즘, 모델을 통해 각각 결과를 예측하고 모든 결과를 동등하게 보고 취합해서 결과를 얻는 방식이다. Random Forest도 여러개의 decision tree 결과값의 평균을 통해 결과를 얻는 Bagging의 일종이다.</p></li><li><p><code>Boosting</code>은 여러 알고리즘, 모델의 결과를 순차적으로 취합하는데, 단순히 하나씩 취하는 방법이 아니라 이전 알고리즘, 모델이 학습 후 잘못 예측한 부분에 가중치를 줘서 다시 모델로 가서 학습하는 방식이다.</p></li></ul></li></ul><p><img src="/image/bagging_boosting_difference.png" alt="bagging과 boosting의 차이"></p><ul><li><p><code>XGBoost는 Boosting 기법 중 Tree Bossting 기법을 활용한 모델</code>이다. 쉽게 말해 Random Forest와 비슷한 원리에 Boosting 기법을 적용했다고 생각하면된다. 여러개의 Decision Tree를 사용하지만 단순히 결과를 평균내는 것이 아니라 결과를 보고 오답에 대해 가중치를 부여한다. 그리고 가중치가 적용된 오답에 대해서는 관심을 가지고 정답이 될 수 있도록 결과를 만들고 해당 결과에 대한 다른 오답을 찾아 다시 똑같은 작업을 반복적으로 진행하는 것이다.</p></li><li><p>최종적으로는 XGBoost란 이러한 Tree Boosting 방식에 경사하강법을 통해 optimization을 하는 방법이다. 그리고 연산량을 줄이기 위해 Decision Tree를 구성할 때 병렬 처리를 사용해 빠른 시간에 학습이 가능하다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TRAIN_Q1_DATA_FILE = <span class="string">'q1_train.npy'</span></span><br><span class="line">TRAIN_Q2_DATA_FILE = <span class="string">'q2_train.npy'</span></span><br><span class="line">TRAIN_LABEL_DATA_FILE = <span class="string">'label_train.npy'</span></span><br><span class="line"></span><br><span class="line">train_q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">train_q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">train_labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, <span class="string">'rb'</span>))</span><br></pre></td></tr></table></figure><ul><li>numpy의 stack 함수를 사용해 두 질문을 하나의 쌍으로 만들었다. 예를 들어, 질문 [A]와 질문 [B]가 있을 때 이 질문을 하나로 묶어 [[A], [B]] 형태로 만들었다. 이와 같은 형태는 다음과 같이 여러가지 방법으로 구현할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_input_expand = np.concatenate((np.expand_dims(train_q1_data, 1), np.expand_dims(train_q2_data, 1)), axis=1)</span><br><span class="line">train_input_expand.shape</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(298526, 2, 31)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_input_concate = np.concatenate((train_q1_data[:,np.newaxis,:], train_q2_data[:,np.newaxis,:]), axis=1)</span><br><span class="line">train_input_concate.shape</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(298526, 2, 31)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_input_stack = np.stack((train_q1_data, train_q2_data), axis=1)</span><br><span class="line">train_input_stack.shape</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(298526, 2, 31)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_input_concate ==  train_input_stack).all() and (train_input_stack == train_input_expand).all() and (train_input_concate == train_input_expand).all()</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True</span><br></pre></td></tr></table></figure><ul><li>전체 29만개 정도의 데이터에 대해 두 질문이 각각 31개의 질문 길이를 가지고 있음을 확인 할 수 있다. 두 질문 쌍이 하나로 묶여 있는 것도 확인할 수 있다. 이제 학습 데이터의 20%를 모델 검증을 위한 validation set으로 만들어 둘 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">train_input, eval_input, train_label, eval_label = train_test_split(train_input_stack, train_labels, test_size=0.2, random_state=4242)</span><br></pre></td></tr></table></figure><ul><li><p>XGBoost를 사용하려면 입력값을 xgb 라이브러리의 데이터 형식인 DMatrix 형태로 만들어야 한다. 학습 데이터와 검증 데이터 모두 적용해서 해당 데이터 형식으로 만든다. 적용 과정에서 각 데이터에 대해 sum 함수를 사용하는데 이는 각 데이터의 두 질문을 하나의 값으로 만들어 주기 위해서이다. 그리고 두 개의 데이터를 묶어서 하나의 리스트로 만든다. 이때 학습 데이터와 검증 데이터는 각 상태의 문자열과 함께 tuple형태로 구성한다.</p></li><li><p>참고로 XGBoost와  sklearn의 ensemble.GradientBoostingClassifier은 동일하게 Tree Boosting 모델을 가지고 있지만 속도면에서 XGBoost가 훨씬 빠르다. (사용법도 조금 다름)</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import xgboost as xgb</span><br><span class="line"></span><br><span class="line">train_data = xgb.DMatrix(train_input.sum(axis=1), label=train_label)</span><br><span class="line">eval_data = xgb.DMatrix(eval_input.sum(axis=1), label=eval_label)</span><br><span class="line"></span><br><span class="line">data_list = [(train_data, <span class="string">'train'</span>), (eval_data, <span class="string">'valid'</span>)]</span><br></pre></td></tr></table></figure><ul><li>우선 모델을 만들고 학습하기 위해 몇 가지 선택해야 하는 옵션은 dictionary를 만들어 넣으면 된다. 이때 dictionary에는 모델의 objective(loss) function와 평가 지표를 정해서 넣어야 하는데 여기서는 우선 objective(loss) function의 경우 이진 로지스틱 함수를 사용한다. 평가 지표의 경우 RMSE를 사용한다. 이렇게 만든 인자와 학습 데이터, 데이터를 반복하는 횟수인 num_boost_round, 모델 검증 시 사용할 전체 데이터 쌍, 그리고 early stopping을 위한 횟수를 정한다. 데이터를 반복하는 횟수, 즉 Epoch을 의미하는 값으로는 1000을 설정했다. 전체 데이터를 1000번 반복해야 끝나도록 설정한 것이다. 그리고 early stopping을 위한 횟수값으로는 10을 설정해서 만약 10 epoch 동안 error값이 크게 줄지 않는다면 학습을 종료시키도록 하였다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;&#125;</span><br><span class="line">params[<span class="string">'objective'</span>] = <span class="string">'binary:logistic'</span></span><br><span class="line">params[<span class="string">'eval_metric'</span>] = <span class="string">'rmse'</span></span><br><span class="line"></span><br><span class="line">bst = xgb.train(params, train_data, num_boost_round = 1000, evals = data_list, early_stopping_rounds=10)</span><br></pre></td></tr></table></figure><h3 id="예측하기"><a href="#예측하기" class="headerlink" title="예측하기"></a>예측하기</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_Q1_DATA_FILE = <span class="string">'test_q1.npy'</span></span><br><span class="line">TEST_Q2_DATA_FILE = <span class="string">'test_q2.npy'</span></span><br><span class="line">TEST_ID_DATA_FILE = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA, <span class="string">'rb'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_input = np.stack((test_q1_data, test_q2_data), axis=1)</span><br><span class="line">test_data = xgb.DMatrix(test_input.sum(axis=1))</span><br><span class="line">test_predict = bst.predict(test_data)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">output = pd.DataFrame(&#123;<span class="string">'test_id'</span>: test_id_data, <span class="string">'is_duplicate'</span>: test_predict&#125;)</span><br><span class="line">output.to_csv(DATA_OUT_PATH + <span class="string">'sample_xgb.csv'</span>, index=False)</span><br></pre></td></tr></table></figure><ul><li>kaggle API를 통해서 바로 파일 올려주었다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit quora-question-pairs -f <span class="string">"sample_xgb.csv"</span> -m <span class="string">"XGBoost Model"</span></span><br></pre></td></tr></table></figure><ul><li>3294팀 중 2714등이다. 물론, 임베딩 벡터라든지 아무런 조치를 취하지 않았기 때문에 score가 안좋을 수 밖에 없다. 추후에 TF-IDF 행렬을 사용하거나 더 좋은 임베딩 기법을 사용해서 다시 올려볼 것이다. 지금은 유사도를 구하는 방법에 대한 기본 튜토리얼이므로 이 정도에서 그치겠다.</li></ul><p><img src="/image/XGBoost_first_none.png" alt="결과"></p><h2 id="CNN-텍스트-유사도-분석-모델"><a href="#CNN-텍스트-유사도-분석-모델" class="headerlink" title="CNN 텍스트 유사도 분석 모델"></a>CNN 텍스트 유사도 분석 모델</h2><ul><li>합성곱 신경망 구조를 활용해 텍스트 유사도를 측정하는 모델을 만들어 보겠다. 기본적인 구조는 이전 장의 합성곱 모델과 유사하지만 이번 경우에는 <code>각 데이터가 두 개의 텍스트 문장으로 돼 있기 때문에 병렬적인 구조를 가진 모델을 만들어야 한다.</code></li></ul><ul><li>모델에 입력하고자 하는 데이터는 문장 2개다. 문장에 대한 유사도를 보기 위해서는 기준이 되는 문장이 필요하다. 이를 ‘기준 문장’이라 정의한다. 그리고 ‘기준 문장’에 대해 비교해야 하는 문장이 있는데 이를 ‘대상문장’이라 한다. 만약 모델에 입력하고자 하는 기준 문장이 ‘I love deep NLP’이고 이를 비교할 대상 문장이 ‘Deep NLP is awesome’이라 하자. 이 두 문장은 의미가 상당히 유사하다. 만약 학습이 진행된 후에 두 문장에 대한 유사도를 측정하고하 한다마녀 아마도 높은 유사도 점수를 보일 것이다. 이처럼 문장이 의미적으로 가까우면 유사도 점수는 높게 표현 될 것이고 그렇지 않을 경우에는 낮게 표현될 것이다.</li></ul><ul><li>전반적인 유사도 분석 모델 구조에 대한 흐름을 보자. 모델에 데이터를 입력하기 전에 기준 문장과 대상 문장에 대해서 인덱싱을 거쳐 문자열 형태의 문장을 인덱스 벡터 형태로 구성한다. 인덱스 벡터로 구성된 문장 정보는 임베딩 과정을 통해 각 단어들이 임베딩 벡터로 바뀐 행렬로 구성 될 것이다. 임베딩 과정을 통해 나온 문장 행렬은 기준 문장과 대상 문장 각각에 해당하는 CNN 블록을 거치게 한다. CNN 블록은 Convolution 층과 Max Pooling층을 합친 하나의 신경망을 의미한다. 두 블록을 거쳐 나온 벡터는 문장에 대한 의미 벡터가 된다. 두 문장에 대한 의미 벡터를 가지고 여러 방식으로 유사도를 구할 수 있다. 여기서는 FC layer를 거친 후 최종적으로 logistic regression 방법을 통해 문자 유사도 점수를 측정할 것이다. 이렇게 측정한 점수에 따라 두 문장의 유사 여부를 판단할 것이다.</li></ul><h3 id="모델-구현-준비"><a href="#모델-구현-준비" class="headerlink" title="모델 구현 준비"></a>모델 구현 준비</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">import json</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">TRAIN_Q1_DATA_FILE = <span class="string">'q1_train.npy'</span></span><br><span class="line">TRAIN_Q2_DATA_FILE = <span class="string">'q2_train.npy'</span></span><br><span class="line">TRAIN_LABEL_DATA_FILE = <span class="string">'label_train.npy'</span></span><br><span class="line">DATA_CONFIGS = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line">TEST_SPLIT = 0.1</span><br><span class="line">RNG_SEED = 13371447</span><br></pre></td></tr></table></figure><ul><li>모델 파라미터 설정</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">EPOCH=10</span><br><span class="line">BATCH_SIZE=1024</span><br><span class="line"></span><br><span class="line">MAX_SEQUENCE_LENGTH = 26 <span class="comment"># 31</span></span><br><span class="line"></span><br><span class="line">WORD_EMBEDDING_DIM = 100</span><br><span class="line">CONV_FEATURE_DIM = 300</span><br><span class="line">CONV_OUTPUT_DIM = 128</span><br><span class="line">CONV_WINDOW_SIZE = 3</span><br><span class="line">SIMILARITY_DENSE_FEATURE_DIM = 200</span><br><span class="line"></span><br><span class="line">prepro_configs = None</span><br><span class="line"></span><br><span class="line">with open(DATA_IN_PATH + DATA_CONFIGS, <span class="string">'r'</span>) as f:</span><br><span class="line">    prepro_configs = json.load(f)</span><br><span class="line"></span><br><span class="line">VOCAB_SIZE = prepro_configs[<span class="string">'vocab_size'</span>] <span class="comment">#76464개</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, <span class="string">'rb'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X = np.stack((q1_data, q2_data), axis=1)</span><br><span class="line">y = labels</span><br><span class="line"></span><br><span class="line">train_X, eval_X, train_y, eval_y = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)</span><br><span class="line"></span><br><span class="line">train_Q1 = train_X[:, 0]</span><br><span class="line">train_Q2 = train_X[:, 1]</span><br><span class="line">eval_Q1 = eval_X[:,0]</span><br><span class="line">eval_Q2 = eval_X[:,1]</span><br></pre></td></tr></table></figure><ul><li><p>estimator에 활용할 데이터 입력 함수를 만들 것이다.</p><ul><li>map 함수</li><li>학습 입력 함수</li><li>검증 입력 함수</li></ul></li><li><p>우선 map 함수로 정의한 rearrange 함수부터 설명하면 3개의 값이 인자로 들어오는데, 각각 기준 질문, 대상 질문, 라벨값이다. 이렇게 들어온 인자 값을 통해 2개의 질문을 하나의 dictionary 형태의 입력값으로 만든다. 그리고 이렇게 만든 dictionary와 label을 return하는 구조로 돼 있다. 이 함수를 학습 입력함수와 검증 입력 함수에 적용할 것이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def rearrange(base, hypothesis, label):</span><br><span class="line">  features = &#123;<span class="string">'x1'</span> : base, <span class="string">'x2'</span> : hypothesis&#125;</span><br><span class="line">  <span class="built_in">return</span> features, label</span><br><span class="line"></span><br><span class="line">def train_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=100)</span><br><span class="line">  dataset = dataset.batch(16)</span><br><span class="line">  dataset = dataset.map(rearrange)</span><br><span class="line">  dataset = dataset.repeat(EPOCH)</span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br><span class="line"></span><br><span class="line">def eval_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((eval_Q1, eval_Q2, eval_y))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=100)</span><br><span class="line">  dataset = dataset.batch(16)</span><br><span class="line">  dataset = dataset.map(rearrange)</span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br></pre></td></tr></table></figure><h3 id="모델-구현"><a href="#모델-구현" class="headerlink" title="모델 구현"></a>모델 구현</h3><ul><li>CNN 블록 함수를 먼저 정의할 것이다. CNN 블록 함수는 convolution layer와 Pooling, Dense를 하나로 합친 형태로 정의할 것이다.</li></ul><ul><li>이 함수는 2개의 인자값을 받는데, 각각 입력값과 이름을 의미한다. 이 함수에서 합성곱의 경우 이전 장의 CNN 모델을 구성할 때와 동일하게 Conv1D를 사용할 것이다. Max Pooling도 마찬사지로 MaxPooling1D 객체를 활용한다. 그리고 이렇게 합성곱과 Max Pooling을 적용한 값에 대해 차원을 바꾸기 위해 Dense 층을 통과 시킨다.  </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def basic_conv_sementic_network(inputs, name):</span><br><span class="line">  conv_layer = tf.keras.layers.Conv1D(CONV_FEATURE_DIM,</span><br><span class="line">                                      CONV_WINDOW_SIZE,</span><br><span class="line">                                      activation=tf.nn.relu,</span><br><span class="line">                                      name=name + <span class="string">'conv_1d'</span>,</span><br><span class="line">                                      padding=<span class="string">'same'</span>)(inputs) <span class="comment">#1024 X 26 X 300</span></span><br><span class="line"></span><br><span class="line">  max_pool_layer = tf.keras.layers.MaxPool1D(MAX_SEQUENCE_LENGTH, 1)(conv_layer) <span class="comment"># 1024 X 1 X 300</span></span><br><span class="line"></span><br><span class="line">  output_layer = tf.keras.layers.Dense(CONV_OUTPUT_DIM, activation=tf.nn.relu, name=name + <span class="string">'dense'</span>)(max_pool_layer) <span class="comment">#1024 X 1 X 128</span></span><br><span class="line"></span><br><span class="line">  output_layer = tf.squeeze(output_layer, 1) <span class="comment"># 1024 X 128</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> output_layer</span><br></pre></td></tr></table></figure><ul><li>이제 모델 함수를 설명 할 것이다. 먼저 현재 튜토리얼은 임베딩 벡터에 크게 신경쓰지 않고 모델의 구조에 대해 집중하는 튜토리얼이므로 특별한 기법 없이 tf.keras.layers.Embedding으로 임베딩 벡터를 만들어준 뒤 Conv1D 구조를 3번 거쳐 최종적으로 dense layer를 통해 1개의 노드로 맞춰준 logit 값을 sigmoid함수를 통해 마치 로지스틱 회귀와 같은 구조를 만들어 줄 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">def model_fn(features, labels, mode):</span><br><span class="line">  TRAIN = mode == tf.estimator.ModeKeys.TRAIN</span><br><span class="line">  EVAL = mode == tf.estimator.ModeKeys.EVAL</span><br><span class="line">  PREDICT = mode == tf.estimator.ModeKeys.PREDICT</span><br><span class="line"></span><br><span class="line">  embedding = tf.keras.layers.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">  base_embedded_matrix = embedding(features[<span class="string">'x1'</span>]) <span class="comment"># 1024 X 31 X 100</span></span><br><span class="line">  hypothesis_embedded_matrix = embedding(features[<span class="string">'x2'</span>]) <span class="comment"># 1024 X 31 X 100</span></span><br><span class="line"></span><br><span class="line">  base_embedded_matrix = tf.keras.layers.Dropout(0.2)(base_embedded_matrix)</span><br><span class="line">  hypothesis_embedded_matrix = tf.keras.layers.Dropout(0.2)(hypothesis_embedded_matrix)</span><br><span class="line"></span><br><span class="line">  conv_layer_base_first = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, CONV_WINDOW_SIZE, activation=tf.nn.relu, padding=<span class="string">'same'</span>)(base_embedded_matrix) <span class="comment">#1024 X 31 X 300</span></span><br><span class="line">  max_pool_layer_base_first = tf.keras.layers.MaxPool1D(2, 1)(conv_layer_base_first) <span class="comment"># 1024 X 30 X 300</span></span><br><span class="line"></span><br><span class="line">  conv_layer_hypothesis_first = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, CONV_WINDOW_SIZE, activation=tf.nn.relu, padding=<span class="string">'same'</span>)(hypothesis_embedded_matrix) <span class="comment">#1024 X 31 X 300</span></span><br><span class="line">  max_pool_layer_hypothesis_first = tf.keras.layers.MaxPool1D(2, 1)(conv_layer_hypothesis_first) <span class="comment"># 1024 X 30 X 300</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  conv_layer_base_second = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, 5, activation=tf.nn.relu, padding=<span class="string">'same'</span>)(max_pool_layer_base_first) <span class="comment">#1024 X 30 X 300</span></span><br><span class="line">  max_pool_layer_base_second = tf.keras.layers.MaxPool1D(5, 1)(conv_layer_base_second) <span class="comment"># 1024 X 26 X 300</span></span><br><span class="line"></span><br><span class="line">  conv_layer_hypothesis_second = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, 5, activation=tf.nn.relu, padding=<span class="string">'same'</span>)(max_pool_layer_hypothesis_first) <span class="comment">#1024 X 30 X 300</span></span><br><span class="line">  max_pool_layer_hypothesis_second = tf.keras.layers.MaxPool1D(5, 1)(conv_layer_hypothesis_second) <span class="comment"># 1024 X 26 X 300</span></span><br><span class="line"></span><br><span class="line">  base_sementic_matrix = basic_conv_sementic_network(max_pool_layer_base_second, <span class="string">'base'</span>) <span class="comment"># 1024 X 128</span></span><br><span class="line">  hypothesis_sementic_matrix = basic_conv_sementic_network(max_pool_layer_hypothesis_second, <span class="string">'hypothesis'</span>) <span class="comment"># 1024 X 128</span></span><br><span class="line"></span><br><span class="line">  merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1) <span class="comment"># 1024 X 256</span></span><br><span class="line"></span><br><span class="line">  similarity_dense_layer = tf.keras.layers.Dense(SIMILARITY_DENSE_FEATURE_DIM, activation=tf.nn.relu)(merged_matrix) <span class="comment"># 1024 X 200</span></span><br><span class="line"></span><br><span class="line">  similarity_dense_layer = tf.keras.layers.Dropout(0.2)(similarity_dense_layer)</span><br><span class="line"></span><br><span class="line">  logit_layer = tf.keras.layers.Dense(1)(similarity_dense_layer) <span class="comment"># 1024 X 1</span></span><br><span class="line">  logit_layer = tf.squeeze(logit_layer, 1) <span class="comment"># (1024, )</span></span><br><span class="line">  similarity = tf.nn.sigmoid(logit_layer)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> PREDICT:</span><br><span class="line">        <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                  mode=mode,</span><br><span class="line">                  predictions=&#123;</span><br><span class="line">                      <span class="string">'is_duplicate'</span>:similarity</span><br><span class="line">                  &#125;)</span><br><span class="line"></span><br><span class="line">  loss = tf.losses.sigmoid_cross_entropy(labels, logit_layer)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> EVAL:</span><br><span class="line">      accuracy = tf.metrics.accuracy(labels, tf.round(similarity))</span><br><span class="line">      <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                mode=mode,</span><br><span class="line">                eval_metric_ops= &#123;<span class="string">'acc'</span>: accuracy&#125;,</span><br><span class="line">                loss=loss)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> TRAIN:</span><br><span class="line">      global_step = tf.train.get_global_step()</span><br><span class="line">      train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">      <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                mode=mode,</span><br><span class="line">                train_op=train_op,</span><br><span class="line">                loss=loss)</span><br></pre></td></tr></table></figure><ul><li>먼저, 변수값 등 모델과 관련된 내용을 담은 체크포인트 파일을 저장할 경로를 설정해야한다. 경로를 지정한 후 해당 경로가 없다면 생성하고 Estimator 객체를 생성할 때 해당 경로를 설정한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.akedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">est = tf.estimator.Estimator(model_fn, model_dir=DATA_OUT_PATH + <span class="string">'checkpoint'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">est.train(train_input_fn)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">est.evaluate(eval_input_fn)</span><br></pre></td></tr></table></figure><h3 id="데이터-제출"><a href="#데이터-제출" class="headerlink" title="데이터 제출"></a>데이터 제출</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_Q1_DATA_FILE = <span class="string">'test_q1.npy'</span></span><br><span class="line">TEST_Q2_DATA_FILE = <span class="string">'test_q2.npy'</span></span><br><span class="line">TEST_ID_DATA_FILE = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br><span class="line">test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br><span class="line">test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br></pre></td></tr></table></figure><ul><li>입력 함수나 검증 함수처럼 별도의 함수로 정의하지 않고 Estimator의 기본 numpy_input_fn 함수를 사용한다. 입력 형태는 앞서 다른 입력 함수와 마찬가지로 두 질문을 dictionary 형태로 만들었다. 그리고 shffle=False로 설정하는데 이는 두 개의 질문쌍이 같은 순서로 입력돼야 하기 때문이다. 이제 이 함수를 활용해 Estimator의 predict 함수를 실행할 것이다.</li></ul><ul><li>predict 함수를 활용하고 반복문을 통해 데이터에 대한 예측값을 받을 수 있게 한다. 이때 받고자 하는 예측값에 대해서는 is_duplicate라는 key 값으로 정의했기 때문에 아래와 같이 유사도 예측값만을 받을 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">"x1"</span>:test_q1_data,</span><br><span class="line">                                                         <span class="string">"x2"</span>:test_q2_data&#125;,</span><br><span class="line">                                                      shuffle=False)</span><br><span class="line"></span><br><span class="line">predictions = np.array([p[<span class="string">'is_duplicate'</span>] <span class="keyword">for</span> p <span class="keyword">in</span> est.predict(input_fn=predict_input_fn)])</span><br><span class="line"></span><br><span class="line">output = pd.DataFrame( data=&#123;<span class="string">"test_id"</span>:test_id_data, <span class="string">"is_duplicate"</span>: list(predictions)&#125; )</span><br><span class="line">output.to_csv(<span class="string">"cnn_predict.csv"</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure><h5 id="kaggle-제출"><a href="#kaggle-제출" class="headerlink" title="kaggle 제출"></a>kaggle 제출</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit quora-question-pairs -f <span class="string">"cnn_predict.csv"</span> -m <span class="string">"cnn conv1d 3layer 10 Epoches"</span></span><br></pre></td></tr></table></figure><p><img src="/image/CNN_3layer_10EPOCHs.png" alt="CNN 3 layer를 통한 결과"></p><h2 id="MaLSTM"><a href="#MaLSTM" class="headerlink" title="MaLSTM"></a>MaLSTM</h2><ul><li>마지막으로 텍스트 유사도 측정을 위해 사용할 모델은 MaLSTM 모델이다. 순서가 있는 입력 데이터에 적합하다는 평을 받는 RNN 모델을 통해 유사도를 측정한다.</li></ul><ul><li><p>유사도를 구하기 위해 활용하는 대표적인 모델인 MaLSTM 모델은 2016년 MIT에서 Jonas Mueller가 쓴 “Siamese Recurrent Architectures for Learning Sentence Similarity”라는 논문에서 처음 소개 되었다. <code>MaLSTM이란 Manhattan Distance + LSTM 의 줄임말로써, 일반적으로 문장의 유사도를 계산할 때 코사인 유사도를 사용하는 대신 맨하탄 거리를 사용하는 모델</code>이다.</p></li><li><p>이전의 합성곱 신경망 모델에서도 두 개의 문장 입력값에 대해 각각 합성곱 층을 적용한 후 최종적으로 각 문장에 대해 의미 벡터를 각각 뽑아내서 concatenate한 후 dese layer를 통해 선형 변환 해준 뒤 로지스틱 모형과 같이 값을 구해 두 문장의 유사도를 구했다. 이번에는 맨하탄 거리로 비교하는 형태의 모델로서, LSTM의 마지막 스텝의 LSTM hidden state는 문장의 모든 단어에 대한 정보가 반영된 값으로 전체 문장을 대표하는 벡터가 된다. 이렇게 뽑은 두 벡터에 대해 맨하탄 거리를 계산해서 두 문장 사이의 유사도를 측정 할 것이다. 그리고 이렇게 계산한 유사도를 실제 라벨과 비교해서 학습하는 방식으로 모델을 설계할 것이다.</p></li></ul><p><img src="/image/MaLSTM_structure.png" alt="MaLSTM의 모델 구조"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">import json</span><br></pre></td></tr></table></figure><h3 id="모델-구현-1"><a href="#모델-구현-1" class="headerlink" title="모델 구현"></a>모델 구현</h3><ul><li>미리 Global 변수를 지정하자. 파일 명, 파일 위치, 디렉토리 등이 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">TRAIN_Q1_DATA_FILE = <span class="string">'q1_train.npy'</span></span><br><span class="line">TRAIN_Q2_DATA_FILE = <span class="string">'q2_train.npy'</span></span><br><span class="line">TRAIN_LABEL_DATA_FILE = <span class="string">'label_train.npy'</span></span><br><span class="line">NB_WORDS_DATA_FILE = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 학습에 필요한 파라메터들에 대해서 지정하는 부분이다.</span></span><br><span class="line"><span class="comment">## CPU에서는 Epoch 크기를 줄이는 걸 권장한다.</span></span><br><span class="line">BATCH_SIZE = 4096</span><br><span class="line">EPOCH = 50</span><br><span class="line">HIDDEN = 64</span><br><span class="line"></span><br><span class="line">NUM_LAYERS = 3</span><br><span class="line">DROPOUT_RATIO = 0.2</span><br><span class="line"></span><br><span class="line">TEST_SPLIT = 0.1</span><br><span class="line">RNG_SEED = 13371447</span><br><span class="line">EMBEDDING_DIM = 128</span><br><span class="line">MAX_SEQ_LEN = 31</span><br></pre></td></tr></table></figure><h3 id="데이터-불러오기"><a href="#데이터-불러오기" class="headerlink" title="데이터 불러오기"></a>데이터 불러오기</h3><ul><li>데이터를 불러오는 부분이다. 효과적인 데이터 불러오기를 위해, 미리 넘파이 형태로 저장시킨 데이터를 로드한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">prepro_configs = None</span><br><span class="line"></span><br><span class="line">with open(DATA_IN_PATH + NB_WORDS_DATA_FILE, <span class="string">'r'</span>) as f:</span><br><span class="line">    prepro_configs = json.load(f)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = prepro_configs[<span class="string">'vocab_size'</span>]</span><br><span class="line">BUFFER_SIZE = len(labels)</span><br></pre></td></tr></table></figure><h3 id="테스트-및-검증-데이터-나누기"><a href="#테스트-및-검증-데이터-나누기" class="headerlink" title="테스트 및 검증 데이터 나누기"></a>테스트 및 검증 데이터 나누기</h3><ul><li>데이터를 나누어 저장하자. sklearn의 train_test_split을 사용하면 유용하다. 하지만, 쿼라 데이터의 경우는 입력이 1개가 아니라 2개이다. 따라서, np.stack을 사용하여 두개를 하나로 쌓은다음 활용하여 분류한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = np.stack((q1_data, q2_data), axis=1)</span><br><span class="line">y = labels</span><br><span class="line">train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)</span><br><span class="line"></span><br><span class="line">train_Q1 = train_X[:,0]</span><br><span class="line">train_Q2 = train_X[:,1]</span><br><span class="line">test_Q1 = test_X[:,0]</span><br><span class="line">test_Q2 = test_X[:,1]</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def rearrange(base, hypothesis, labels):</span><br><span class="line">    features = &#123;<span class="string">"base"</span>: base, <span class="string">"hypothesis"</span>: hypothesis&#125;</span><br><span class="line">    <span class="built_in">return</span> features, labels</span><br><span class="line"></span><br><span class="line">def train_input_fn():</span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y))</span><br><span class="line">    dataset = dataset.shuffle(buffer_size=len(train_Q1))</span><br><span class="line">    dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">    dataset = dataset.map(rearrange)</span><br><span class="line">    dataset = dataset.repeat(EPOCH)</span><br><span class="line">    iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> iterator.get_next()</span><br><span class="line"></span><br><span class="line">def eval_input_fn():</span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((test_Q1, test_Q2, test_y))</span><br><span class="line">    dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">    dataset = dataset.map(rearrange)</span><br><span class="line">    iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> iterator.get_next()</span><br></pre></td></tr></table></figure><h3 id="모델-설계"><a href="#모델-설계" class="headerlink" title="모델 설계"></a>모델 설계</h3><ul><li><p>양방향 LSTM을 사용할 것이다. 즉, 2개의 LSTM을 먼저 정의해야 한다. 정방향 LSTM층과 역방향 LSTM 층을 먼저 정의할 것이다. 그리고 나선 이 2개의 LSTM 층에 데이터를 적용한 후 결과값을 하나로 concatenate한다.</p></li><li><p>양방향 순환 신경망 함수의 경우 2개의 return 값이 있는데, 하나는 순환 신경망의 출력 값이고, 나머지 하나는 순환 신경망 마지막 스텝의 hidden state 벡터 값이다. 사용해야 할 것은 마지막 hidden state 벡터이므로 각각 q_output_states와 sim_output_states로 할당한다. 이렇게 뽑은 hidden state 벡터의 경우 해당 모델이 <code>양방향 순환 신경망을 활용해 2개의 hidden state 값을 concatenate하여 하나의 벡터로 만든다. 이는 순환 신경망이 문장의 순방향과 역방향 모두 학습함으로써 성능 개선에 도움을 준다.</code></p></li><li><p>맨하탄 거리의 경우 두 벡터를 뺀 후 절대값을 취하면 된다. 이렇게 뺀 값의 경우 벡터 형태이기 때문에 하나의 상수, 즉 scalar값으로 만들기 위해 reduce_sum 함수를 이용한다. 이렇게 되면 구한 값이 0~1사이의 값을 갖게 될 것이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">def Malstm(features, labels, mode):</span><br><span class="line"></span><br><span class="line">    TRAIN = mode == tf.estimator.ModeKeys.TRAIN</span><br><span class="line">    EVAL = mode == tf.estimator.ModeKeys.EVAL</span><br><span class="line">    PREDICT = mode == tf.estimator.ModeKeys.PREDICT</span><br><span class="line"></span><br><span class="line">    def basic_bilstm_network(inputs, name):</span><br><span class="line">        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):</span><br><span class="line">            lstm_fw = [</span><br><span class="line">                    tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(HIDDEN), output_keep_prob=DROPOUT_RATIO)</span><br><span class="line">                    <span class="keyword">for</span> layer <span class="keyword">in</span> range(NUM_LAYERS)</span><br><span class="line">                    ]</span><br><span class="line">            lstm_bw = [</span><br><span class="line">                    tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(HIDDEN), output_keep_prob=DROPOUT_RATIO)</span><br><span class="line">                    <span class="keyword">for</span> layer <span class="keyword">in</span> range(NUM_LAYERS)</span><br><span class="line">                    ]</span><br><span class="line"></span><br><span class="line">            multi_lstm_fw = tf.nn.rnn_cell.MultiRNNCell(lstm_fw)</span><br><span class="line">            multi_lstm_bw = tf.nn.rnn_cell.MultiRNNCell(lstm_bw)</span><br><span class="line"></span><br><span class="line">            (fw_outputs, bw_outputs), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=multi_lstm_fw,</span><br><span class="line">                                                cell_bw=multi_lstm_bw,</span><br><span class="line">                                                inputs=inputs,</span><br><span class="line">                                                dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">            outputs = tf.concat([fw_outputs, bw_outputs], 2)</span><br><span class="line"></span><br><span class="line">            <span class="built_in">return</span> outputs[:,-1,:]</span><br><span class="line"></span><br><span class="line">    embedding = tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">    base_embedded_matrix = embedding(features[<span class="string">'base'</span>])</span><br><span class="line">    hypothesis_embedded_matrix = embedding(features[<span class="string">'hypothesis'</span>])</span><br><span class="line"></span><br><span class="line">    base_sementic_matrix = basic_bilstm_network(base_embedded_matrix, <span class="string">'base'</span>)</span><br><span class="line">    hypothesis_sementic_matrix = basic_bilstm_network(hypothesis_embedded_matrix, <span class="string">'hypothesis'</span>)</span><br><span class="line"></span><br><span class="line">    base_sementic_matrix = tf.keras.layers.Dropout(DROPOUT_RATIO)(base_sementic_matrix)</span><br><span class="line">    hypothesis_sementic_matrix = tf.keras.layers.Dropout(DROPOUT_RATIO)(hypothesis_sementic_matrix)</span><br><span class="line"></span><br><span class="line"><span class="comment">#     merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1)</span></span><br><span class="line"><span class="comment">#     logit_layer = tf.keras.layers.dot([base_sementic_matrix, hypothesis_sementic_matrix], axes=1, normalize=True)    </span></span><br><span class="line"><span class="comment">#     logit_layer = K.exp(-K.sum(K.abs(base_sementic_matrix - hypothesis_sementic_matrix), axis=1, keepdims=True))</span></span><br><span class="line"></span><br><span class="line">    logit_layer = tf.exp(-tf.reduce_sum(tf.abs(base_sementic_matrix - hypothesis_sementic_matrix), axis=1, keepdims=True))</span><br><span class="line">    logit_layer = tf.squeeze(logit_layer, axis=-1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> PREDICT:</span><br><span class="line">        <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                  mode=mode,</span><br><span class="line">                  predictions=&#123;</span><br><span class="line">                      <span class="string">'is_duplicate'</span>:logit_layer</span><br><span class="line">                  &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#prediction 진행 시, None</span></span><br><span class="line">    <span class="keyword">if</span> labels is not None:</span><br><span class="line">        labels = tf.to_float(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment">#     loss = tf.reduce_mean(tf.keras.metrics.binary_crossentropy(y_true=labels, y_pred=logit_layer))</span></span><br><span class="line">    loss = tf.losses.mean_squared_error(labels=labels, predictions=logit_layer)</span><br><span class="line"><span class="comment">#     loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(labels, logit_layer))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> EVAL:</span><br><span class="line">        accuracy = tf.metrics.accuracy(labels, tf.round(logit_layer))</span><br><span class="line">        eval_metric_ops = &#123;<span class="string">'acc'</span>: accuracy&#125;</span><br><span class="line">        <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                  mode=mode,</span><br><span class="line">                  eval_metric_ops= eval_metric_ops,</span><br><span class="line">                  loss=loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> TRAIN:</span><br><span class="line"></span><br><span class="line">        global_step = tf.train.get_global_step()</span><br><span class="line">        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                  mode=mode,</span><br><span class="line">                  train_op=train_op,</span><br><span class="line">                  loss=loss)</span><br></pre></td></tr></table></figure><h3 id="학습-및-평가"><a href="#학습-및-평가" class="headerlink" title="학습 및 평가"></a>학습 및 평가</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># os.environ["CUDA_VISIBLE_DEVICES"]="0" #For GPU</span></span><br><span class="line"></span><br><span class="line">model_dir = os.path.join(os.getcwd(), DATA_OUT_PATH + <span class="string">"checkpoint/rnn2/"</span>)</span><br><span class="line">os.makedirs(model_dir, exist_ok=True)</span><br><span class="line"></span><br><span class="line">config_tf = tf.estimator.RunConfig()</span><br><span class="line"></span><br><span class="line">lstm_est = tf.estimator.Estimator(Malstm, model_dir=model_dir)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lstm_est.train(train_input_fn)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lstm_est.evaluate(eval_input_fn)</span><br></pre></td></tr></table></figure><h3 id="테스트-데이터-예측-및-캐글-제출하기"><a href="#테스트-데이터-예측-및-캐글-제출하기" class="headerlink" title="테스트 데이터 예측 및 캐글 제출하기"></a>테스트 데이터 예측 및 캐글 제출하기</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_Q1_DATA_FILE = <span class="string">'test_q1.npy'</span></span><br><span class="line">TEST_Q2_DATA_FILE = <span class="string">'test_q2.npy'</span></span><br><span class="line">TEST_ID_DATA_FILE = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br><span class="line">test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br><span class="line">test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">"base"</span>:test_q1_data,</span><br><span class="line">                                                         <span class="string">"hypothesis"</span>:test_q2_data&#125;,</span><br><span class="line">                                                      shuffle=False)</span><br><span class="line"></span><br><span class="line">predictions = np.array([p[<span class="string">'is_duplicate'</span>] <span class="keyword">for</span> p <span class="keyword">in</span> lstm_est.predict(input_fn=predict_input_fn)])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(len(predictions)) <span class="comment">#2345796</span></span><br><span class="line"></span><br><span class="line">output = pd.DataFrame( data=&#123;<span class="string">"test_id"</span>:test_id_data, <span class="string">"is_duplicate"</span>: list(predictions)&#125; )</span><br><span class="line">output.to_csv( <span class="string">"rnn_predict.csv"</span>, index=False, quoting=3 )</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit quora-question-pairs -f <span class="string">"rnn_predict.csv"</span> -m <span class="string">"MaLSTM Model with 5layers BiLSTM 50 Epoches"</span></span><br></pre></td></tr></table></figure><p><img src="/image/MaLSTM_result_with_50 Epoches.png" alt="MaLSTM 결과"></p><h5 id="구조에-대한-튜토리얼형식으로-만들다보니-임베딩의-질이-떨어진다면-머신러닝-기법이-딥러닝-방식보다-결과가-더-좋을-수-있다는-사실을-다시-한번-체감할-수-있는-작업이었다-추후에-TF-IDF행렬-Word2Vec과-문장-단위-LSA를-시행해-얻은-임베딩을-사용하여-다시-한번-결과를-비교할-것이다"><a href="#구조에-대한-튜토리얼형식으로-만들다보니-임베딩의-질이-떨어진다면-머신러닝-기법이-딥러닝-방식보다-결과가-더-좋을-수-있다는-사실을-다시-한번-체감할-수-있는-작업이었다-추후에-TF-IDF행렬-Word2Vec과-문장-단위-LSA를-시행해-얻은-임베딩을-사용하여-다시-한번-결과를-비교할-것이다" class="headerlink" title="구조에 대한 튜토리얼형식으로 만들다보니 임베딩의 질이 떨어진다면 머신러닝 기법이 딥러닝 방식보다 결과가 더 좋을 수 있다는 사실을 다시 한번 체감할 수 있는 작업이었다. 추후에 TF-IDF행렬, Word2Vec과 문장 단위 LSA를 시행해 얻은 임베딩을 사용하여 다시 한번 결과를 비교할 것이다."></a>구조에 대한 튜토리얼형식으로 만들다보니 임베딩의 질이 떨어진다면 머신러닝 기법이 딥러닝 방식보다 결과가 더 좋을 수 있다는 사실을 다시 한번 체감할 수 있는 작업이었다. 추후에 TF-IDF행렬, Word2Vec과 문장 단위 LSA를 시행해 얻은 임베딩을 사용하여 다시 한번 결과를 비교할 것이다.</h5>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/11/NLP_11/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 실습 텍스트 유사도 - 01 (데이터 EDA 및 전처리)</title>
      <link>https://heung-bae-lee.github.io/2020/02/10/NLP_10/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/10/NLP_10/</guid>
      <pubDate>Sun, 09 Feb 2020 17:34:30 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;텍스트-유사도&quot;&gt;&lt;a href=&quot;#텍스트-유사도&quot; class=&quot;headerlink&quot; title=&quot;텍스트 유사도&quot;&gt;&lt;/a&gt;텍스트 유사도&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;텍스트 유사도 문제한 두 문장(글)이 있을 때 두 문장 간의 유사도를 측정할 수
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="텍스트-유사도"><a href="#텍스트-유사도" class="headerlink" title="텍스트 유사도"></a>텍스트 유사도</h1><ul><li>텍스트 유사도 문제한 두 문장(글)이 있을 때 두 문장 간의 유사도를 측정할 수 있는 모델을 만드는 것이다.</li></ul><h2 id="문제소개"><a href="#문제소개" class="headerlink" title="문제소개"></a>문제소개</h2><ul><li>데이터 이름 : Quora Question Pairs</li><li>텍스트 용도 : 텍스트 유사도 학습을 목적으로 사용</li><li>데이터 권한 : Quora 권한을 가지고 있으며 Kaggle 가입 후 데이터를 내려받으면 문제없다.</li><li><p>데이터 출처 : <a href="https://www.kaggle.com/c/quora-question-pairs/data" target="_blank" rel="noopener">https://www.kaggle.com/c/quora-question-pairs/data</a></p></li><li><p>이번에도 Kaggle의 대회 중 하나를 해결해 보려고 한다. “Quora Questions Pairs”라는 문제를 해결해보도록 할 것이다. Quora는 질문을 하고 다른 사용자들로부터 답변을 받을 수 있는 서비스이다. 실제로 딥러닝 공뷰할 때도 Quora의 질문들은 참고하면서 많은 공부를 할 수 있다. Quora의 월 사용자는 대략 1억명 정도 된다. 매일 수 많은 질문들이 사이트에 올라올 텐데 이 많은 질문 중에는 분명히 중복된 것들이 포함될 것이다. 따라서 Quora 입장에서는 중복된 질문들을 잘 찾기만 한다면 이미 잘 작성된 답변들을 사용자들이 참고하게 할 수 있고, 더 좋은 서비스를 제공할 수 있게 된다.</p></li></ul><h3 id="목표-여러-질문들-중에서-어떤-질문이-서로-유사한지-판단하는-모델을-만드는-것"><a href="#목표-여러-질문들-중에서-어떤-질문이-서로-유사한지-판단하는-모델을-만드는-것" class="headerlink" title="목표 : 여러 질문들 중에서 어떤 질문이 서로 유사한지 판단하는 모델을 만드는 것"></a>목표 : <code>여러 질문들 중에서 어떤 질문이 서로 유사한지 판단하는 모델을 만드는 것</code></h3><p><img src="/image/quora_web_site.png" alt="Quora"></p><ul><li>캐글 API를 colab에서 사용하기 위한 인증 및 google storage에 업로드 되어있는 인증키 파일 현재 colab pwd로 복사해온 후 설정완료하기</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from google.colab import auth</span><br><span class="line">import warnings</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'retina'</span></span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line">auth.authenticate_user()</span><br><span class="line"></span><br><span class="line">!gsutil cp gs://kaggle_key/kaggle.json kaggle.json</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!mkdir -p ~/.kaggle</span><br><span class="line">!mv ./kaggle.json ~/.kaggle/</span><br><span class="line">!chmod 600 ~/.kaggle/kaggle.json</span><br><span class="line">!pip install kaggle</span><br></pre></td></tr></table></figure><h3 id="데이터-불러오기와-분석하기"><a href="#데이터-불러오기와-분석하기" class="headerlink" title="데이터 불러오기와 분석하기"></a>데이터 불러오기와 분석하기</h3><ul><li>데이터를 내려받는 것부터 시작할 것이다. 필자는 Google colab에서 kaggle API를 통해 다운로드 받을 것이다. 아래 그림과 같은 error가 발생한다면 kaggle API Token파일을 다시 받지 말고 그 전에 먼저 해당 competition의 rule을 수락을 했는지를 확이해보아야 한다. <a href="https://www.kaggle.com/c/quora-question-pairs/rules" target="_blank" rel="noopener">https://www.kaggle.com/c/quora-question-pairs/rules</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions download -c quora-question-pairs</span><br></pre></td></tr></table></figure><p><img src="/image/text_similarity_kaggle_error.png" alt="캐글 error"></p><p><img src="/image/text_similarity_kaggle_error_rule.png" alt="해당 competition rule 체크"></p><ul><li><p>해당 데이터가 잘 다운로드 됐는지 확인한다. 확인해 보면 다음과 같이 3가지 파일이 있을 것이다.</p><ul><li>sample_submission.csv.zip</li><li>test.csv.zip</li><li>train.csv.zip</li></ul></li><li><p>총 3개의 파일이 zip 형식으로 압축된 형태다. 이 파일들의 압축을 풀어주는 과정까지 할 것이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import zipfile</span><br><span class="line"></span><br><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">zip_list=[<span class="string">'sample_submission.csv.zip'</span>, <span class="string">'test.csv.zip'</span>, <span class="string">'train.csv.zip'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> zip_list:</span><br><span class="line">  zipRef = zipfile.ZipFile(DATA_IN_PATH + file, <span class="string">'r'</span>)</span><br><span class="line">  zipRef.extractall(DATA_IN_PATH)</span><br><span class="line">  zipRef.close()</span><br></pre></td></tr></table></figure><ul><li>본격적으로 데이터를 불러온 후 데이터 분석을 해보기 위해 필요한 라이브러리들을 모두 Import 할 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import os</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import pathlib as Path</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><ul><li>가장 먼저 학습 데이터를 불러와서 어떤 형태로 데이터가 구성돼 있는지 확인해 볼 것이다.</li></ul><ul><li>데이터는 ‘id’, ‘qid1’, ‘qid2’, ‘question1’, ‘question2’, ‘is_duplicate’열로 구성돼 있다. 각각의 description은 아래와 같다.<ul><li>id : 각 행 데이터의 고유한 index 값</li><li>qid1 : 질문들의 고유한 index 값</li><li>qid2 : 질문들의 고유한 index 값</li><li>question1 : 질문의 내용</li><li>question2 : 질문의 내용</li><li>is_duplicate : 0 또는 1(0이면 두 개의 질문이 중복이 아님을 의미, 1이면 두 개의 질문이 중복을 의미)</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(DATA_IN_PATH + <span class="string">'train.csv'</span>)</span><br><span class="line">train_data.head()</span><br></pre></td></tr></table></figure><p><img src="/image/top_five_train_data_Quora.png" alt="학습 데이터 상위 5개"></p><ul><li>사용할 데이터가 어떤 데이터이고, 크기는 어느 정도 되는지 알아보기 위해 데이터 파일의 이름과 크기를 각각 출력해서 확인해 볼 것이다.</li></ul><ul><li>대부분 train data가 test data 보다 크기가 큰데, 이 데이터는 test data가 train data 보다 5배 정도 더 큰 것을 알 수 있다. test data가 큰 이유는 Quora의 경우 질문에 대해 데이터의 수가 적다면 각각을 검색을 통해 중복을 찾아내는 편볍을 사용할 수 있으므로 이러한 편법을 방지하기 위해 Quora에서 직접 컴퓨터가 만든 질문 싸을 test data에 임의적으로 추가했기 때문이다. 따라서 test data가 크지만 실제 question data는 얼마 되지 않는다. 그리고 Kaggle의 경우 예측 결과를 제출하면 점수를 받을 수 있는데, 컴퓨터가 만든 질문 쌍에 대한 예측은 점수에 포함도지 않는다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'파일 크기: '</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(DATA_IN_PATH):</span><br><span class="line">  <span class="keyword">if</span> (<span class="string">'csv'</span> <span class="keyword">in</span> file) and (<span class="string">'zip'</span> not <span class="keyword">in</span> file):</span><br><span class="line">    <span class="built_in">print</span>(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + <span class="string">'MB'</span>)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">파일 크기:</span><br><span class="line">test.csv                      314.02MB</span><br><span class="line">sample_submission.csv         22.35MB</span><br><span class="line">train.csv                     63.4MB</span><br></pre></td></tr></table></figure><ul><li>전체 데이터의 개수와 학습 데이터안의 NULL값이 존재하는지 먼저 확인 할 것이다.</li></ul><ul><li>결과를 보면 전체 질문 쌍의 개수는 대략 40만개이며 3개의 데이터에 NULL값이 존재한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.info()</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;class <span class="string">'pandas.core.frame.DataFrame'</span>&gt;</span><br><span class="line">RangeIndex: 404290 entries, 0 to 404289</span><br><span class="line">Data columns (total 6 columns):</span><br><span class="line">id              404290 non-null int64</span><br><span class="line">qid1            404290 non-null int64</span><br><span class="line">qid2            404290 non-null int64</span><br><span class="line">question1       404289 non-null object</span><br><span class="line">question2       404288 non-null object</span><br><span class="line">is_duplicate    404290 non-null int64</span><br><span class="line">dtypes: int64(4), object(2)</span><br><span class="line">memory usage: 18.5+ MB</span><br></pre></td></tr></table></figure><ul><li><p>전체 질문(두 개의 질문)을 한번에 분석하기 위해 Pandas의 Series를 통해 두 개의 질문을 하나로 합친다.</p></li><li><p>각 질문을 list로 만든 뒤 하나의 Series 데이터 타입으로 만든다. 결과를 보면 아래와 같은 구조로 합쳐졌다. 기존 데이터에서 질문 쌍의 수가 40만개 정도이고 각각 질문이 2개 이므로 대략 80만개 정도의 질문이 있다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_set = pd.Series(train_data[<span class="string">'question1'</span>].to_list() + train_data[<span class="string">'question2'</span>].to_list()).astype(str)</span><br><span class="line">train_set.tail()</span><br></pre></td></tr></table></figure><ul><li>이제 질문들의 중복 여부를 확인해 볼 것이다. Numpy의 unique함수를 이용해 중복을 제거한 총 질문의 수와 반복해서 나오는 질문의 수를 확인한다.</li></ul><ul><li>결과를 보면 80만 개의 데이터에서 537,361건이 Unique한 데이터이므로 262,639건이 중복돼 있음을 알 수 있다. 그러므로 262,639개 데이터는 131,318개의 동일한 질문 쌍으로 이루어져 있음을 알 수 있다.(1쌍은 NULL값이고, 1쌍은 값이 하나만 존재하므로)</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'교육 데이터의 총 질문 수 : &#123;&#125; 건'</span>.format(len(np.unique(train_set))))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'반복해서 나타나는 질문의 수 : &#123;&#125; 건'</span>.format(np.sum(train_set.value_counts() &gt; 1)))</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">교육 데이터의 총 질문 수 : 537361 건</span><br><span class="line">반복해서 나타나는 질문의 수 : 111873 건</span><br></pre></td></tr></table></figure><ul><li>위의 결과를 시각화 해 볼 것이다. y축의 범위를 줄이기 위해 log을 사용했다. x값은 중복의 개수이며, y값은 동일한 중복 횟수를 가진 질문의 개수를 의미한다.</li></ul><ul><li>histogram을 살펴보면 우선 중복 횟수가 1인 질문들, 즉 유일한 질문들이 가장 많고 대부분의 질문이 중복 횟수가 50번 이하이다. 그리고 매우 큰 빈도를 가진 질문은 이상치가 될 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12,5))</span><br><span class="line">plt.hist(train_set.value_counts(), bins=50, alpha=0.5, color=<span class="string">'r'</span>, label=<span class="string">'word'</span>)</span><br><span class="line">plt.yscale(<span class="string">'log'</span>, nonposy=<span class="string">'clip'</span>)</span><br><span class="line">plt.title(<span class="string">'Log-Histogram of question appearance counts'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Number of questions'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/histogram_of_question_appearance_counts_Quora.png" alt="중복 질문의 개수에 관한 histogram"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">'교육 데이터의 총 질문 수 : &#123;&#125; 건'</span>.format(len(np.unique(train_set))))</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">교육 데이터의 총 질문 수 : 537361 건</span><br></pre></td></tr></table></figure><ul><li>중복이 최대로 발생한 개수는 161번이고, 평균적으로 보면 문장당 1.5개의 중복을 가지며, 표준편차는 1.9다. 중복이 발생하는 횟수의 평균이 1.5라는 것은 많은 데이터가 최소 1개 이상 중복돼 있음을 의미한다. 즉 <code>중복이 많다는 의미</code>이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_set.value_counts().describe()</span><br></pre></td></tr></table></figure><h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">count    537361.000000</span><br><span class="line">mean          1.504724</span><br><span class="line">std           1.911439</span><br><span class="line">min           1.000000</span><br><span class="line">25%           1.000000</span><br><span class="line">50%           1.000000</span><br><span class="line">75%           1.000000</span><br><span class="line">max         161.000000</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><ul><li><p>이제 box plot을 통해 중복횟수와 관련해서 데이터를 직관적으로 이해해 보자.</p></li><li><p>아래의 분포는 중복 횟수의 이상치가 너무 넓고 많이 분포해서 box plot의 다른 값을 확인하기조차 어려운 데이터이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12, 5))</span><br><span class="line">plt.boxplot([train_set.value_counts()],</span><br><span class="line">            labels=[<span class="string">'counts'</span>],</span><br><span class="line">            showmeans=True)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/box_plot_Quora.png" alt="중복 질문의 개수에관한 box plot"></p><ul><li>데이터에 어떤 단어가 포함됐는지 간단히 알아보기 위해 워드클라우드를 사용할 것이다.</li></ul><ul><li>워드 클라우드로 그려진 결과를 확인해 보면 best, way, good, difference 등의 단어들이 질문을 할 때 일반적으로 가장 많이 사용된다는 것을 알 수 있다. 특이한 점은 해당 결과에서 ‘Donald Trump’가 존재하는 것이다. ‘Donald Trump’가 존재하는 이유는 선거 기간 중 학습 데이터를 만들었기 때문이라고 많은 캐글러들이 말하고 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from wordcloud import WordCloud</span><br><span class="line">cloud = WordCloud(width=700, height=400).generate(<span class="string">' '</span>.join(train_set.astype(str)))</span><br><span class="line">plt.figure(figsize=(15,13))</span><br><span class="line">plt.imshow(cloud)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/word_cloud_Quora.png" alt="Quora 학습데이터 워드 클라우드"></p><ul><li>질문 텍스트가 아닌 데이터의 라벨인 ‘is_duplicate’에 대해 count plot을 통해 살펴볼 것이다.</li></ul><ul><li>라벨값의 개수를 확인해 보면 총 40만 개의 데이터에서 중복이 아닌 데이터가 25만개 정도이고 중복된 데이터가 약 15만개 정도로 보인다. <code>이 상태로 학습한다면 중복이 아닌 데이터 25만개에 의존도가 높아지면서 데이터가 한쪽 라벨로 편향된다. 이러한 경우 학습이 원활하게 되지 않을 수도 있으므로 최대한 라벨의 개수를 균형 있게 맞춰준 후 진행하는 것이 좋다. 많은 수의 데이터를 줄인 후 학습할 수도 있고, 적은 수의 데이터를 늘린 후 학습할 수도 있다.</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig, axe = plt.subplots(ncols=1)</span><br><span class="line">fig.set_size_inches(10, 5)</span><br><span class="line">sns.countplot(train_data[<span class="string">'is_duplicate'</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/label_count_plot_Quora.png" alt="라벨값의 분포"></p><ul><li>학습 데이터의 길이를 분석해 볼 것이다. 문자단위로 먼저 길이를 분석한 후 단어 단위로 분석 할 것이다.</li></ul><ul><li>데이터의 각 질문의 길이 분포는 15 ~ 150에 대부분 모여 있으며 길이가 150에서 급격하게 주어드는 것을 볼 때 Quora의 질문 길이 제한이 150 정도라는 것을 추정해 볼 수 있다. 길이가 150 이상인 데이터는 거의 없기 때문에 해당 데이터 때문에 문제가 되지는 않을 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_length = train_set.apply(len)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(15, 10))</span><br><span class="line">plt.hist(train_length, bins=200, range=[0, 200], facecolor=<span class="string">'r'</span>, normed=True, label=<span class="string">'train'</span>)</span><br><span class="line">plt.title(<span class="string">'Normalized histogram of chracter count in questions'</span>, fontsize=15)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'Number of characters'</span>, fontsize=15)</span><br><span class="line">plt.ylabel(<span class="string">'Probability'</span>, fontsize=15)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/char_histogram_with_Quora.png" alt="문자 단위 질문 길이에 관한 histogram"></p><ul><li>그에 따른 기초 통계량은 다음과 같으며, <code>평균적으로 길이가 60 정도라는 것을 확인할 수 있다. 그리고 중앙값의 경우 51 정도이다. 하지만 최댓값을 확인해 보면 1169로서 평균, 중앙값에 비해 매우 큰 차이를 보인다. 이런 데이터는 제외하고 학습하는 것이 좋을 것</code>이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_length.describe()</span><br></pre></td></tr></table></figure><h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">count    808580.000000</span><br><span class="line">mean         59.822548</span><br><span class="line">std          31.963751</span><br><span class="line">min           1.000000</span><br><span class="line">25%          39.000000</span><br><span class="line">50%          51.000000</span><br><span class="line">75%          72.000000</span><br><span class="line">max        1169.000000</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><ul><li>데이터의 질문 길이값에 대해서도 box plot을 그려서 확인해 볼 것이다.</li></ul><ul><li>분포를 보면 문자 수의 이상치 데이터가 너무 많이 분포해서 box plot의 다른 값을 확인하기 조차 어려운 상태다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12, 5))</span><br><span class="line">plt.boxplot(train_length, labels=[<span class="string">'char counts'</span>], showmeans=True)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/char_box_plot_with_Quora.png" alt="문자 단위 길이 box plot"></p><ul><li>이제 문자가 아닌 단어를 한 단위로 사용해 길이값을 분석해 볼 것이다. 하나의 단어로 나누는 기준은 단순히 띄어쓰기로 정의할 것이다.</li></ul><ul><li>histogram을 보면 대부분 10개 정도의 단어로 구성된 데이터가 가장 많다는 것을 볼 수 있다. 20개 이상의 단어로 구성되 데이터는 매우 적다는 것을 확인할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">train_word_counts = train_set.apply(lambda x: len(x.split(<span class="string">' '</span>)))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(15, 10))</span><br><span class="line">plt.hist(train_word_counts, bins=50, range=[0, 50], color=<span class="string">'r'</span>, label=<span class="string">'train'</span>, normed=True)</span><br><span class="line">plt.title(<span class="string">'Normalized histogram of word count in one question'</span>, fontsize=15)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'Number of Words'</span>, fontsize=15)</span><br><span class="line">plt.ylabel(<span class="string">'Probability'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/word_count_histogram_with_Quora.png" alt="질문 당 단어 단위 길이 histogram"></p><ul><li>그에 따른 기초통계량을 살펴볼 것이다.</li></ul><ul><li>데이터의 문자 단위 길이를 확인했을 때와 비슷한 분포를 갖는다. 평균 개수의 경우 11개이며, 중앙값의 경우 평균 보다 1개 적은 10개를 갖는다. 문자 길이의 최댓값인 경우 1100 정도의 값을 보인다. 단어 길이는 최대 237개이다. 해당 데이터의 경우 지나치게 긴 문자 길이와 단어 개수를 보여준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_word_counts.describe()</span><br></pre></td></tr></table></figure><h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">count    808580.000000</span><br><span class="line">mean         11.064856</span><br><span class="line">std           5.889168</span><br><span class="line">min           1.000000</span><br><span class="line">25%           7.000000</span><br><span class="line">50%          10.000000</span><br><span class="line">75%          13.000000</span><br><span class="line">max         237.000000</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.quantile(train_word_counts, 0.99)</span><br></pre></td></tr></table></figure><h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">31.0</span><br></pre></td></tr></table></figure><ul><li>box plot을 통해 데이터 분포를 다시 한번 확인해보자.</li></ul><ul><li>문자 길이에 대한 box plot과 비슷한 모양의 그래프를 보여준다. Quora 데이터의 경우 이상치가 넓고 많이 분포 돼 있음을 알 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12, 5))</span><br><span class="line">plt.boxplot(train_word_counts, labels=[<span class="string">'word counts'</span>], showmeans=True)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/image/word_count_boxplot_with_Quora.png" alt="질문 당 단어 단위 길이 box plot"></p><ul><li>몇 가지 특정 경우에 대한 비율을 확인해 볼 것이다. 특수 문자 중 구두점, 물음표, 마침표가 사용된 비율과 수학 기호가 사용된 비율, 대/소문자의 비율을 확인해 본다.</li></ul><ul><li>대문자가 첫 글자인 질문과 물음표를 동반하는 질문이 99% 이상을 차지한다. 전체적으로 질문들이 물음표와 대문자로 된 첫 문자를 가지고 있음을 알 수 있다. 그럼 여기서 생각해 볼 부분이 있다. <code>즉, 모든 질문이 보편적으로 가지고 있는 이 특징의 유지 여부에 대해서인데, 모두가 가지고 있는 보편적인 특징은 여기서 제거한다.</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">qmarks = np.mean(train_set.apply(lambda x : <span class="string">'?'</span> <span class="keyword">in</span> x))</span><br><span class="line">math = np.mean(train_set.apply(lambda x : <span class="string">'[math]'</span> <span class="keyword">in</span> x))</span><br><span class="line">fullstop = np.mean(train_set.apply(lambda x : <span class="string">'.'</span> <span class="keyword">in</span> x))</span><br><span class="line">capital_first = np.mean(train_set.apply(lambda x : x[0].isupper()))</span><br><span class="line">capitals = np.mean(train_set.apply(lambda x : max([y.isupper() <span class="keyword">for</span> y <span class="keyword">in</span> x]))) <span class="comment"># 대문자가 사용된 질문이 몇 개인지</span></span><br><span class="line">numbers = np.mean(train_set.apply(lambda x : max([y.isdigit() <span class="keyword">for</span> y <span class="keyword">in</span> x]))) <span class="comment"># 숫자가 사용된 질문이 몇 개인지</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'물음표가 있는 질문: &#123;:.2f&#125;%'</span>.format(qmarks * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'수학 태그가 있는 질문: &#123;:.2f&#125;%'</span>.format(math * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'마침표가 있는 질문: &#123;:.2f&#125;%'</span>.format(fullstop * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'첫 글자가 대문자인 질문: &#123;:.2f&#125;%'</span>.format(capital_first * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'대문자가 있는 질문: &#123;:.2f&#125;%'</span>.format(capitals * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'숫자가 있는 질문: &#123;:.2f&#125;%'</span>.format(numbers * 100))</span><br></pre></td></tr></table></figure><h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">물음표가 있는 질문: 99.87%</span><br><span class="line">수학 태그가 있는 질문: 0.12%</span><br><span class="line">마침표가 있는 질문: 6.31%</span><br><span class="line">첫 글자가 대문자인 질문: 99.81%</span><br><span class="line">대문자가 있는 질문: 99.95%</span><br><span class="line">숫자가 있는 질문: 11.83%</span><br></pre></td></tr></table></figure><h3 id="데이터-전처리"><a href="#데이터-전처리" class="headerlink" title="데이터 전처리"></a>데이터 전처리</h3><ul><li>지금까지 데이터 EDA(탐색적 데이터 분석)를 통해 데이터의 구조와 분포를 확인했다. <code>질문 데이터의 중복 여부 분포, 즉 라벨의 분포가 크게 차이나서 학습에 편향을 주므로 좋지 않은 영향을 줄 수 있다. 따라서 전처리 과정에서 분포를 맞춰줄 것이다. 그리고 대부분의 질문에 포함된 첫 번째 대문자는 소문자로 통일한다. 물음표 같은 구두점은 삭제하는 식으로 보편적인 특성은 제거함으로써 필요한 부분만 학습하게 하는 이점을 얻을 수 있다.</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">from tensorflow.python.keras.preprocessing.text import Tokenizer</span><br><span class="line">from tensorflow.python.keras.preprocessing.sequence import pad_sequences</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(DATA_IN_PATH + <span class="string">'train.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br></pre></td></tr></table></figure><ul><li>맨 먼저 진행할 전처리 과정은 앞서 분석 과정에서 확인했던 내용 중 하나인 <code>라벨 개수의 균형을 맞추는 것</code>이다. 앞서 분석 과정에서 확인했듯이 중복이 아닌 데이터의 개수가 더욱 많기 때문에 이 경우에 해당하는 데이터의 개수를 줄인 후 분석을 진행하겠다. 먼저 중복인 경우와 아닌 경우로 데이터를 나눈 후 중복이 아닌 개수가 비슷하도록 데이터의 일부를 다시 뽑는다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_duplicate_data = train_data.loc[train_data[<span class="string">'is_duplicate'</span>]==1]</span><br><span class="line">train_non_duplicate_data = train_data.loc[train_data[<span class="string">'is_duplicate'</span>]==0]</span><br><span class="line"></span><br><span class="line">class_difference = len(train_non_duplicate_data) - len(train_duplicate_data)</span><br><span class="line">sample_frac = 1 - (class_difference / len(train_non_duplicate_data))</span><br><span class="line"></span><br><span class="line">train_non_duplicate_data = train_non_duplicate_data.sample(frac = sample_frac)</span><br></pre></td></tr></table></figure><ul><li>샘플링한 후 데이터의 개수가 동일해졌다. 이제 해당 데이터를 사용하먄 균형 있게 학습할 수 있을 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"중복 질문 개수 : &#123;&#125; 건"</span>.format(len(train_duplicate_data)))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"중복이 아닌 질문 개수 : &#123;&#125; 건"</span>.format(len(train_non_duplicate_data)))</span><br></pre></td></tr></table></figure><h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">중복 질문 개수 : 149263 건</span><br><span class="line">중복이 아닌 질문 개수 : 149263 건</span><br></pre></td></tr></table></figure><ul><li>우선 라벨에 따라 나눠진 데이터를 다시 하나로 합치자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.concat([train_non_duplicate_data, train_duplicate_data])</span><br></pre></td></tr></table></figure><ul><li>앞서 전처리에서 분석한 대로 문장 문자열에 대한 전처리를 먼저 진행한다. 우선 학습 데이터의 질문 쌍을 하나의 질문 리스트로 만들고, 정규 표현식을 사용해 물음표와 마침표 같은 구두점 및 기호를 제거하고 모든 문자를 소문자로 바꾸는 처리를 한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.head()</span><br></pre></td></tr></table></figure><ul><li>물음표와 마침표 같은 기호에 대해 정규 표현식을 사용하여 전처리하기 위해 re 라이브러리를 활용한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">FILTERS = <span class="string">"([~.,!?\"':;)(])"</span></span><br><span class="line"></span><br><span class="line">change_filter = re.compile(FILTERS)</span><br><span class="line"></span><br><span class="line">questions1 = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> train_data[<span class="string">'question1'</span>]]</span><br><span class="line">questions2 = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> train_data[<span class="string">'question2'</span>]]</span><br><span class="line"></span><br><span class="line">filtered_questions1 = []</span><br><span class="line">filtered_questions2 = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> questions1:</span><br><span class="line">  filtered_questions1.append(re.sub(change_filter, <span class="string">""</span>, q).lower())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> questions2:</span><br><span class="line">  filtered_questions2.append(re.sub(change_filter, <span class="string">""</span>, q).lower())</span><br></pre></td></tr></table></figure><ul><li>이제 남은 과정은 정제된 위의 텍스트 테이터를 토크나이징하고 각 단어를 인덱스로 바꾼 후, 전체 데이터의 길이를 맞추기 위해 정의한 최대 길이보다 긴 문장은 자르고 짧은 문장은 패딩 처리를 하는 것이다. 문자열 토크나이징은 tensorflow keras에서 제공하는 NLP Processing 모듈을 활용한다.<code>객체를 만들 때는 두 질문 텍스트를 합친 리스트에 적용하고, 토크나이징은 해당 객체를 활용해 각 질문에 대해 따로 진행할 것이다. 이러한 방법은 두 질문에 대해 동일한 토크나이징 방식을 사용해야하며, 두 질문을 합친 전체 vocabulary를 만들어야 하기 때문이다.</code> 토크나이징 이후에는 패딩 처리를 한 벡터화를 진행할 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = Tokenizer()</span><br><span class="line">tokenizer.fit_on_texts(filtered_questions1 + filtered_questions2)</span><br><span class="line"></span><br><span class="line">questions1_sequence = tokenizer.texts_to_sequences(filtered_questions1)</span><br><span class="line">questions2_sequence = tokenizer.texts_to_sequences(filtered_questions2)</span><br></pre></td></tr></table></figure><ul><li><p>이제 모델에 적용하기 위해 특정 길이로 동일하게 맞춰야 한다. 따라서 최대 길이를 정한 후 그 길이보다 긴 질문은 자르고, 짧은 질문은 부족한 부분을 0으로 채우는 패딩 과정을 진행 할 것이다.</p></li><li><p>최대 길이는 앞서 EDA에서 확인했던 단어 개수의 99%인 31로 설정했다. 이렇게 설정한 이유는 이상치를 뺀 나머지를 포함하기 위해서이다.(다양한 값으로 실험했을 때 이 값이 가장 좋은 값이었다.) 전처리 모듈의 패딩 함수를 사용해 최대 길이로 자르고 짧은 데이터에 대해서는 데이터 뒤에 패딩값을 채워넣었다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MAX_SEQUENCE_LENGTH = 31</span><br><span class="line"></span><br><span class="line">q1_data = pad_sequences(questions1_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding=<span class="string">'post'</span>)</span><br><span class="line">q2_data = pad_sequences(questions2_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding=<span class="string">'post'</span>)</span><br></pre></td></tr></table></figure><ul><li>전처리가 끝난 데이터를 저장한다. 저장하기 전에 라벨값과 단어 사전을 저장하기 위해 값을 저장한 후 각 데이터의 크기를 확인해 보자. 두 개의 질문 문장의 경우 각각 길이를 31로 설정했고, vocabulary의 길이인 전체 단어 개수는 76,594개로 돼 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">word_vocab = &#123;&#125;</span><br><span class="line">word_vocab = tokenizer.word_index</span><br><span class="line"></span><br><span class="line">labels = np.array(train_data[<span class="string">'is_duplicate'</span>], dtype=int)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Shape of question1 data : &#123;&#125;'</span>.format(q1_data.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Shape of question2 data : &#123;&#125;'</span>.format(q2_data.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Shape of question1 data : &#123;&#125;'</span>.format(labels.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Words in index : &#123;&#125;'</span>.format(len(word_vocab)))</span><br></pre></td></tr></table></figure><h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Shape of question1 data : (298526, 31)</span><br><span class="line">Shape of question2 data : (298526, 31)</span><br><span class="line">Shape of question1 data : (298526,)</span><br><span class="line">Words <span class="keyword">in</span> index : 76594</span><br></pre></td></tr></table></figure><ul><li>단어 사전과 전체 단어의 개수는 dictionary 형태로 저장해 둘 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_configs = &#123;&#125;</span><br><span class="line">data_configs[<span class="string">'vocab'</span>] = word_vocab</span><br><span class="line">data_configs[<span class="string">'vocab_size'</span>] = len(word_vocab)+1</span><br></pre></td></tr></table></figure><ul><li>이제 각 데이터를 모델링 과정에서 사용할 수 있게 저장하면 된다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">TRAIN_Q1_DATA = <span class="string">'q1_train.npy'</span></span><br><span class="line">TRAIN_Q2_DATA = <span class="string">'q2_train.npy'</span></span><br><span class="line">TRAIN_LABEL_DATA = <span class="string">'label_train.npy'</span></span><br><span class="line">DATA_CONFIGS = <span class="string">'data_configs.npy'</span></span><br><span class="line"></span><br><span class="line">np.save(open(DATA_IN_PATH + TRAIN_Q1_DATA, <span class="string">'wb'</span>), q1_data)</span><br><span class="line">np.save(open(DATA_IN_PATH + TRAIN_Q2_DATA, <span class="string">'wb'</span>), q2_data)</span><br><span class="line">np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, <span class="string">'wb'</span>), labels)</span><br><span class="line"></span><br><span class="line">json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, <span class="string">'w'</span>))</span><br></pre></td></tr></table></figure><ul><li>이제 평가 데이터에 대해서도 동일한 전처리를 실행해줄 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_data = pd.read_csv(DATA_IN_PATH + <span class="string">'test.csv'</span>, encoding=<span class="string">'utf-8'</span>)</span><br><span class="line">valid_ids = [<span class="built_in">type</span>(x) == int <span class="keyword">for</span> x <span class="keyword">in</span> test_data.test_id]</span><br><span class="line">test_data = test_data[valid_ids].drop_duplicates()</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">test_questions1 = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> test_data[<span class="string">'question1'</span>]]</span><br><span class="line">test_questions2 = [str(s) <span class="keyword">for</span> s <span class="keyword">in</span> test_data[<span class="string">'question2'</span>]]</span><br><span class="line"></span><br><span class="line">filtered_test_questions1 = list()</span><br><span class="line">filtered_test_questions2 = list()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> test_questions1:</span><br><span class="line">     filtered_test_questions1.append(re.sub(change_filter, <span class="string">""</span>, q).lower())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> test_questions2:</span><br><span class="line">     filtered_test_questions2.append(re.sub(change_filter, <span class="string">""</span>, q).lower())</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">test_questions1_sequence = tokenizer.texts_to_sequences(filtered_test_questions1)</span><br><span class="line">test_questions2_sequence = tokenizer.texts_to_sequences(filtered_test_questions2)</span><br><span class="line"></span><br><span class="line">test_q1_data = pad_sequences(test_questions1_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding=<span class="string">'post'</span>)</span><br><span class="line">test_q2_data = pad_sequences(test_questions2_sequence, maxlen=MAX_SEQUENCE_LENGTH, padding=<span class="string">'post'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">test_id = np.array(test_data[<span class="string">'test_id'</span>])</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Shape of question1 data: &#123;&#125;'</span>.format(test_q1_data.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Shape of question2 data:&#123;&#125;'</span>.format(test_q2_data.shape))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Shape of ids: &#123;&#125;'</span>.format(test_id.shape))</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_Q1_DATA = <span class="string">'test_q1.npy'</span></span><br><span class="line">TEST_Q2_DATA = <span class="string">'test_q2.npy'</span></span><br><span class="line">TEST_ID_DATA = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">np.save(open(DATA_IN_PATH + TEST_Q1_DATA, <span class="string">'wb'</span>), test_q1_data)</span><br><span class="line">np.save(open(DATA_IN_PATH + TEST_Q2_DATA , <span class="string">'wb'</span>), test_q2_data)</span><br><span class="line">np.save(open(DATA_IN_PATH + TEST_ID_DATA , <span class="string">'wb'</span>), test_id)</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/10/NLP_10/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 문장 수준 임베딩 - 02</title>
      <link>https://heung-bae-lee.github.io/2020/02/08/NLP_09/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/08/NLP_09/</guid>
      <pubDate>Fri, 07 Feb 2020 16:02:58 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;ELMo-Embedding-from-Language-Models&quot;&gt;&lt;a href=&quot;#ELMo-Embedding-from-Language-Models&quot; class=&quot;headerlink&quot; title=&quot;ELMo(Embedding from La
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="ELMo-Embedding-from-Language-Models"><a href="#ELMo-Embedding-from-Language-Models" class="headerlink" title="ELMo(Embedding from Language Models)"></a>ELMo(Embedding from Language Models)</h2><ul><li><p>미국 연구기관 Allen Institute for Artificial Intelligence와 미국 워싱턴대학교 공동연구팀이 발표한 문장 임베딩 기법이다. <code>Computer vision 분야에서 널리 쓰이고 있었던 Transfer leaning을 자연어 처리에 접목하여 주목</code>받았다. Transfer learning이란 이미 학습된 모델을 다른 Deep learning 모델의 입력값 또는 부분으로 재사용하는 기법을 일컫는다. ELMo가 제안된 이후 자연어 처리 분야에서는 모델을 Pretrain한 후 이를 각종 DownStream Task에 적용하는 양상이 일반화됐다. BERT(Bidirectional Encoder Representations from Transfomer), GPT(Generative Pre-Training)등이 이 방식을 따른다. <code>Pretrain한 모델을 downstream task에 맞게 업데이트하는 과정을 Fine-tuning</code>이라고 한다.</p></li><li><p>ELMo는 Language Model이다. 단어 sequence가 얼마나 자연스러운지 확률값을 부여한다. 예를들어, ‘발 없는 말이 천리’라는 단어 sequence 다음에 ‘간다’라는 단어가 자주 등장했다면, 모델은 ‘발 없는 말이 천리’를 일력박아 ‘간다’를 출력해야 한다.</p></li><li><p>ELMo는 크게 3가지 요소로 구성돼 있다.</p><ul><li>1) <code>문자 단위 Convolution Layer</code><ul><li>각 단어 내 <code>문자들 사이의 의미적, 문법적 관계를 도출</code>한다.</li></ul></li><li>2) <code>양방향 LSTM Layer</code><ul><li><code>단어들 사이의 의미적, 문법적 관계를 추출해내는 역할</code>을 한다.</li></ul></li><li>3) <code>ELMo Layer</code><ul><li>문자 단위 convolution Layer와 양방야 LSTM Layer는 ELMo를 Pretrain하는 과정에서 학습된다. 하지만 <code>ELMo Layer는 Pretrain이 끝난 후 구체적인 DownStream task를 수행하는 과정에 학습</code>된다.</li><li><code>문자단위 conv layer와 양방향 LSTM layer의 출력벡터등을 가중합하는 방식으로 계산</code>된다. 이들 가중치들은 downstream task의 학습 손실을 최소화하는 방향으로 업데이트되면서 학습된다.</li></ul></li></ul></li></ul><h4 id="문자-단위-Convolution-Layer"><a href="#문자-단위-Convolution-Layer" class="headerlink" title="문자 단위 Convolution Layer"></a>문자 단위 Convolution Layer</h4><ul><li><p><code>ELMo 입력은 문자</code>다 구체적으로는 <code>유니코드 ID</code>이다. 그러므로 <code>corpus를 해당 단어를 유니코드로 변환</code>해야한다. 한글 유니코드 블록은 UTF-8에서 3byte로 표현되기 때문에 예를 들어 ‘밥’이라는 단어의 유니코드를 10진수로 바꾸면 3가지 숫자가 된다. 여기서 단어(문자가 아님)의 시작과 끝을 알게하기 위해 BOW와 EOW에 해당하는 값을 유니코드 앞 뒤로 붙인다. 이후에 문자 임베딩 행렬에서 각각의 ID에 해당하는 행 벡터를 참조해 붙인다. 문자의 길이가 각각 다르므로 처음에 문자의 최대 길이를 정해주면 그에따른 padding처리를 해준다. Convolution filter의 크기는 (같이 보고 싶은 문자길이) $\times$ (문자 임베딩의 차원 수)가 된다. 이를통해 피처맵을 얻고 여기서 max pooling을 해주어 결과를 낸다.</p></li><li><p>위에서 같이 보고 싶은 문자길이를 조정해 가면서 여러개의 filter를 사용해 얻은 풀링 벡터들을 concatenate한 뒤 highway network와 projection(차원 조정)을 한다.</p></li></ul><h4 id="양방향-LSTM-스코어-레이어"><a href="#양방향-LSTM-스코어-레이어" class="headerlink" title="양방향 LSTM, 스코어 레이어"></a>양방향 LSTM, 스코어 레이어</h4><ul><li>문자 단위 convolution layer가 반환 한 단어벡터 sequence(맨 하단의 보라색 벡터들)에서 시작과 끝을 알이는 <bos>, <eos> 토큰을 앞 뒤로 붙인 뒤 학습 시킨다. 순방향 LSTM layer와 역방향 LSTM layer에 모두 위의 벡터 sequence들을 입력하는데 각각 n개의 LSTM layer를 구성한다. ELMo 기본 모델은 n=2로 설정하고 있다. <code>ELMo에는 LSTM layer에 residual connection 구조를 적용</code>시켜 일부 계산 노드를 생략하게하여 효율적인 Gradient 전파를 돕는다. 프리트레인 단계에서 Word2vec에서 사용되었던 negative sampling 기법이 사용된다.</eos></bos></li></ul><p><img src="/image/ELMo_bidirectional_LSTM.png" alt="ELMo 양방향 LSTM 및 출력 레이어"></p><h4 id="ELMo-레이어"><a href="#ELMo-레이어" class="headerlink" title="ELMo 레이어"></a>ELMo 레이어</h4><ul><li><p>ELMo 입베딩은 Pretrain이 끝나고 구체적인 DownStream task를 학습하는 과정에서 도출되며, <code>각 layer별 hidden state를 가중합한 결과</code>이다. 임의의 task를 수행하기 위한 문장 k번째 Token의 ELMo 임베딩의 구체적인 수식은 다음과 같다.</p><ul><li>$h_{k,j}^{LM}$ : k번째 Token의 j번째 layer의 순방향, 역방향 LSTM hidden state를 concatenate한 벡터를 의미한다.</li><li>$s_{j}^{task}$ : j번째 layer가 해당 task 수행에 얼마나 중요한지를 의미하는 scalar값이다. downstream task를 학습하는 과정에서 loss를 최소화하는 방향을 업데이트한다.</li><li>$\gamma^{task}$ : ELMo 벡터의 크기를 scaling하여 해당 task 수행을 돕는 역할을 한다.</li><li>L : 양방향 LSTM layer 개수(보통 2로 설정함)<ul><li>j=0 -&gt; 문자 단위 convolution Layer</li><li>j=1 -&gt; 양방향 LSTM layer의 첫번째 출력</li><li>j=2 -&gt; 양방향 LSTM layer의 두번째 출력</li></ul></li></ul></li></ul><script type="math/tex; mode=display">ELMo_{k}^{task} = \gamma^{task} \sum^{L}_{j=0} s_{j}^{task} h_{k,j}^{LM}</script><h2 id="트랜스포메-네트워크"><a href="#트랜스포메-네트워크" class="headerlink" title="트랜스포메 네트워크"></a>트랜스포메 네트워크</h2><ul><li>트랜스포머 네트워크는 구글 연구 팀이 NIPS에 공개한 딥러닝 아키텍처다. 뛰어난 성능으로 주목받았다. 이후 발표된 GPT, BERT 등 기법은 트랜스포머 블록을 기본 모델로 쓰고 있다. 크게 두가지 작동원리로 나눌수 있다. <code>Multi-head Attention</code>과 <code>feedforward Network</code>이다.</li></ul><p><img src="/image/transformer_mechanism_structure_image.png" alt="트랜스포머 구조"></p><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><ul><li>Scaled Dot-Product Attention의 입력(x)는 기본적으로 행렬 형태를 가지며 그 크기는 입력 문장의 단어수 $\times$ 입력 임베딩의 차원 수이다. 트랜스 포어믜 Scaled Dot-Product Attention 매커니즘은 query, key, value 3가지 사이의 관계가 핵심이다. 입력행렬 X와 Query, Key, Value에 따르는 가중치 행렬($W^{q}, W^{k}, W^{v}$)을 각각 곱해 계산한다. 이후 query와 key가 얼마나 유사한지를 구하기 위해 두 벡터간 내적을 구해 코사인 유사도를 구한다. 이를 통해 <code>어떤 query와 key가 특정 task 수행에 중요한 역할을 하고 있다면 트랜스포머 블록은 이들 사이의 내적값을 키우는 방식으로 학습</code>한다. 아래 식에서 제곱근 스케일을 하는 이유는 <code>query-key 내적 행렬의 분산을 줄이게 돼 softmax 함수의 gradient가 지나치게 작아지는 것을 방지</code>할 수 있기 때문이다. softmax 노드의 gradient는 softmax 확률 벡터 y의 개별 요소 값에 아주 민감하기 때문에 softmax 확률 벡터의 일부 값이 지나치게 작다면 gradient vanishing 문제가 나타날 수 있다.  </li></ul><h5 id="Scaled-Dot-Product-Attention-1"><a href="#Scaled-Dot-Product-Attention-1" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h5><script type="math/tex; mode=display">Attention(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt(d_{k})}) \cdot V</script><h5 id="소프트맥스-노드의-gradient"><a href="#소프트맥스-노드의-gradient" class="headerlink" title="소프트맥스 노드의 gradient"></a>소프트맥스 노드의 gradient</h5><script type="math/tex; mode=display">\frac{\delta y_{i}}{\delta x_{i}} = y_{i}(1-y_{i})</script><script type="math/tex; mode=display">\frac{\delta y_{i}}{\delta x_{j}} = -y_{i}y_{j}</script><p><img src="/image/Scaled_dot_product_Attention_mechanism_weighted_matrix.png" alt="Scaled Dot-Product Attention"></p><ul><li>아래 그림은 Scaled Dot-Product Attention 기법으로 Query, Key, Value 사이의 관계들이 농축된 새로운 Z를 만드는 예시이다. 파란색 선으로 둘러싸인 행렬은 Query, Key 내적을 $\sqrt(d_{k})$로 나눈 뒤 softmax 함수를 취한 결과이다. 이 행렬의 <code>행은 Query 단어들에 대응</code>하며, <code>열은 Key 단어들에 대응</code>한다. 아래 그림처럼 Query와 Key값이 동일한 attention을 self-attention이라고 한다. <code>이는 같은 문장 내 모든 단어 쌍 사이의 의미적, 문법적 관계를 포착해낸다는 의미</code>이다. softmax를 취한 결과는 확률이 된다. 따라서 각 행의 합은 1이다. <code>&#39;드디어-금요일&#39;값이 가장 높아 벡터 공간상에서도 가까이 있을 가능성이 높고 두 단어사이의 관계가 task 수행(번역, 분류등)에  중요하다는 이야기</code>이다. 마지막으로는 softmax 확률을 가중치 삼아 각 값 벡터들을 가중합하는 것과 같다. <code>새롭게 만들어진 &#39;드디어&#39;에 해당하는 벡터는 해당 문장 내 단어 쌍간 관계가 모두 농축된 결과이다.</code></li></ul><p><img src="/image/Scaled_Dot_Product_Attention_example.png" alt="Scaled Dot-Product Attention"></p><ul><li>self-attention은 RNN, CNN보다 장점이 많다. CNN의 경우 사용자가 지정한 window내의 context만 살피기 때문에 문장이 길고 처음 단어와 마지막 단어 사이의 연관성 파악이 task 수행에 중요한 데이터라면 해결하기 어렵다. RNN은 sequence의 길이가 길어질수록 gradient vanishing이 일어나기 쉽기 때문에 처음 입력받았던 단어를 기억하기 쉽지 않다. 하지만 <code>self-attention은 문장 내 모든 단어쌍 사이의 관계를 늘 전체적으로 파알할 수 있다.</code></li></ul><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><ul><li><code>Scaled Dot-Product Attention을 여러 번 시행하는 것</code>을 가리킨다. <code>동일한 문장을 여러 명의 독자가 동시에 분석해 최선의 결과를 내려고 하는 것에 비유</code>할 수 있다. Multi-Head attention의 계산 과정은 아래의 수식과 그림과 같이 이루어진다. Query, Key, Value를 Scaled Dot-Product를 통해 얻은 Attention Value를 concatenate한다. 그 후 여기에 $W^{0}$를 내적해 <code>Multi-Head Attention 수행 결과 행렬의 크기를 Scaled Dot-Product Attention의 입력 행렬과 동일하게 맞춘다.</code></li></ul><h5 id="Multi-Head-Attention-수식"><a href="#Multi-Head-Attention-수식" class="headerlink" title="Multi-Head Attention 수식"></a>Multi-Head Attention 수식</h5><script type="math/tex; mode=display">MultiHead(Q, K, V) = Concat(head_{1}, \cdots, head_{h})W^{0}</script><script type="math/tex; mode=display">head_{i} = Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i})</script><p><img src="/image/multi_head_attention_structure_NLP.png" alt="Multi-Head Attention"></p><h3 id="Position-wise-Feedforward-Networks"><a href="#Position-wise-Feedforward-Networks" class="headerlink" title="Position-wise Feedforward Networks"></a>Position-wise Feedforward Networks</h3><ul><li>Multi-Head Attention Layer의 입력 행렬과 출력 행렬의 크기는 입력 단어 수 $\times$ 히든 벡터 차원 수로 동일하다. Position-wise Feedforward Networks Layer에서는 Multi-Head Attention Layer의 출력 행렬을 행 벡터 단위로, 다시 말해 단어 벡터 각각에 관해 아래의 수식을 적용한다. Multi-Head Layer의 출력 행렬 가운데 하나의 단어 벡터를 x라고 하자. 이 x에 관해 두 번의 선형변환을 하는데 그 사이에 activation을 해서 적용한다.</li></ul><h5 id="Position-wise-Feedforward-Networks-수식"><a href="#Position-wise-Feedforward-Networks-수식" class="headerlink" title="Position-wise Feedforward Networks 수식"></a>Position-wise Feedforward Networks 수식</h5><script type="math/tex; mode=display">FFN(x) = max(0, x \cdot W_{1} + b_{1})W_{2} + b_{2}</script><h3 id="트랜스포머의-학습-전략"><a href="#트랜스포머의-학습-전략" class="headerlink" title="트랜스포머의 학습 전략"></a>트랜스포머의 학습 전략</h3><ul><li>트랜스포머의 학습 전략은 warm up이다. 아래 그림과 같이 <code>사용자가 정한 step수에 이르기 까지 learning rate를 높였다가 step 수를 만족하면 조금씩 떨어끄리는 방식</code>이다. 대규모 데이터, 큰 모델 학습에 적합하다. 이 전략은 BERT 등 이후 제안된 모델에도 널리 쓰이고 있다. 이밖에 Layer Normalization 등도 트랜스포머의 안정적인 학습에 기여하고 있는 것으로 보인다. 좀더 자세한 사항을 알고 싶다면 <a href="https://heung-bae-lee.github.io/2020/01/21/deep_learning_10/">여기</a>를 눌러 공부해보자.</li></ul><p><img src="/image/warm_up_step.png" alt="warm up 트랜스포머의 learning rate 조정 방식"></p><h2 id="BERT-Bidirectional-Encoder-Representations-from-Transformer"><a href="#BERT-Bidirectional-Encoder-Representations-from-Transformer" class="headerlink" title="BERT(Bidirectional Encoder Representations from Transformer)"></a>BERT(Bidirectional Encoder Representations from Transformer)</h2><ul><li>BERT는 구글에서 공개한 모델이다. 성능이 뛰어나 널리 쓰이고 있다.</li></ul><h3 id="BERT-ELMo-GPT"><a href="#BERT-ELMo-GPT" class="headerlink" title="BERT, ELMo, GPT"></a>BERT, ELMo, GPT</h3><ul><li><code>BERT의 성공 비결은 그 performance가 검증된 트랜스포머 블록을 썼을뿐더러 모델의 속성이 양방향을 지향한다는 점</code>에 있다. 아래 그림은 BERT 이전의 모델인 GPT(Generative Pre-Training)와 ELMo 모델과의 차이점을 시각화한 것이다. <code>GPT는 단어 sequence를 왼쪽에서 오른쪽으로 한 방향으로만 보는 아키텍쳐이다. ELMo는 Bi-LSTM Layer의 상단은 양방향이지만 중간 Layer는 역시 한 방향인 모델이다. 반면 BERT의 경우 모든 Layer에서 양방향 성질을 잃지 않고 있다.</code></li></ul><p><img src="/image/BERT_GPT_ELMo.png" alt="BERT, GPT, ELMo의 아키텍쳐"></p><ul><li>BERT와 GPT 모델은 모두 트랜스포머 블록을 사용하고 있다. <code>GPT는</code> 주어진 단어 sequence를 가지고 그 다음 단어를 예측하는 과정에서 학습하는 <code>Language Model이기 때문에</code> 입력 단어 이후의 단어를 모델에게 알려주는 것을 하지 못한다. 따라서 아래 그림 중 <code>1번에 속한다</code>. 이 문제를 극복하기 위해 <code>Masked Language Model이 제안</code>되었다. <code>주어진 sequence 다음 다음를 맞추는 것에서 벗어나, 일단 문장 전체를 모델에 알려주고, Masking에 해당하는 단어가 어떤 단어일지 예측하는 과정에서 학습해보자는 아이디어</code>이다. 이는 아래 그림 중 <code>2번에 속한다</code>. Masked Language Model Task에서는 모델에 문장 전체를 다 주어도 반칙이 될 수 없다. BERT 모델은 빈칸을 채워야 하기 때문이다.</li></ul><p><img src="/image/i_am_ate.png" alt="양방향, 단바향 Language Model"></p><ul><li>아래 그림은 GPT가 Scaled Dot-Product Attention을 하는 과정을 도식화한 것이다. 예측해야 할 단어를 보지 않기 위해 softmax score 행렬의 일부 값을 0으로 만든다. 예를 들어, 입력 문장이 ‘뜨끈한, 국밥, 한 그릇’이고 이번에 예측해야 할 단어가 ‘국밥’이라면 GPT는 이전 단어인 ‘뜨끈한’만 참고해야 한다. 반면 BERT는 빈칸만 맞추면 되기 때문에 문장 내 단어 쌍 사이의 관계를 모두 볼 수 있다.</li></ul><p><img src="/image/train_with_GPT.png" alt="GPT의 학습"></p><p><img src="/image/train_with_BERT.png" alt="BERT의 학습"></p><ul><li><code>BERT 임베딩을 각종 Downstream Task에 적용해 실험한 결과 BERT의 임베딩 품질이 GPT보다 좋음을 입증했다. 또한 같은 BERT 모델이더라도 Pre-Train을 할 때 한 방향(Left-to-Right)만 보게 할 경우 그 성능이 기본 모델 대비 크게 감소하는 것을 확인할 수 있다. 그만큼 모델이 양방향 전후 context를 모두 보게 하는 것이 중요하다는 이야기</code>이다.</li></ul><h3 id="Pre-Train-Task와-학습-데이터-구축"><a href="#Pre-Train-Task와-학습-데이터-구축" class="headerlink" title="Pre-Train Task와 학습 데이터 구축"></a>Pre-Train Task와 학습 데이터 구축</h3><ul><li><p>BERT의 Pre-Train Task에는 크게 Masked Language Model과 다음 문장인지 여부 맞추기(NSP, Next Sentence Prediction)로 되어있는데, 이 두가지로 인해 BERT가 양방향 모델이 될 수 있었다.</p></li><li><p>Masked Language Model Task 수행을 위한 학습 데이터는 다음과 같이 만든다.</p><ul><li><p>1) 학습 데이터 한 문장 Token의 15%를 Masking한다.</p></li><li><p>2) Masking 대상 Token 가운데 80%는 실제 빈칸으로 만들고, 모델은 그 빈칸을 채운다.</p><ul><li>ex) 뜨끈한 국밥 [Mask] 하는게 낫지 -&gt; 한 그릇</li></ul></li><li><p>3) Masking 대상 Token 가운데 10%는 랜덤으로 다른 Token으로 대체하고, 모델은 해당 위치의 정답 단어가 무엇일지 맞추도록 한다.</p><ul><li>ex) 뜨끈한 국밥 [한 개] 하는게 낫지 -&gt; 한 그릇</li></ul></li><li><p>4) Masking 대상 Token 가운데 10%는 Token을 그대로 두고, 모델은 해당 위치의 정답 단어가 무엇일지 맞추도록 한다.</p><ul><li>ex) 뜨끈한 국밥 [한 그릇] 하는게 낫지 -&gt; 한 그릇</li></ul></li><li><p>위와 같이 학습 데이터를 만들게 되면 우리는 다음과 같은 점들을 기대하게 된다.</p><ul><li><p>‘뜨끈한 국밥 [Mask] 하는게 낫지’의 빈칸을 채워야 하기 때문에 <code>문장 내 어느 자리에 어떤 단어를 사용하는게 자연스러운지 앞뒤 문맥을 읽어낼 수 있게 된다</code>.</p></li><li><p>‘뜨끈한 국밥 한 그릇 하는게 낫지’, ‘뜨끈한 국밥 한 개 하는게 낫지’를 비교해 보면서 <code>주어진 문장이 의미/문법상 비문인지 아닌지 분별</code>할 수 있게 된다.</p></li><li><p>모델은 어떤 단어가 Masking 될지 전혀 모르기 때문에 <code>문장 내 모든 단어 사이의 의미적, 문법적 관계를 세밀히 살피게 된다</code>.</p></li></ul></li></ul></li><li><p>다음 문자인지 여부(NSP)를 맞추기 위한 학습 데이터는 다음과 같이 만든다.</p><ul><li>1) 모든 학습 데이터는 1건당 문장 2 개로 구성된다.</li><li>2) 이 가운데 절반은 동일한 문서에서 실제 이어지는 문장을 2 개 뽑고, 그 정답으로 True를 부여한다.</li><li>3) 나머지 절반은 서로 다른 문서에서 문장 1개씩 뽑고, 그 정답으로 False를 부여한다.</li><li>4) max_num_tokens를 정의한다.<ul><li>학습 데이터의 90%는 max_num_tokens가 사용자가 정한 max_sequence_length가 되도록 한다.</li><li>나머지 10%는 max_num_tokens가 max_sequence_length보다 짧게 되도록 랜덤으로 정한다.</li></ul></li><li><p>5) 이전에 뽑은 문장 2 개의 단어 총 수가 max_num_token을 넘지 못할 때까지 두 문장 중 단어 수가 많은 쪽을 50%의 확률로 문장 맨 앞 또는 맨 뒤 단어 하나씩 제거한다.</p></li><li><p>이같이 학습 데이터를 만들면 우리는 다음과 같은 점들을 기대하게 된다.</p><ul><li><p>모델은 ‘내일은 비가 올 것이다’, ‘우산을 챙겨야 할 것 같다’가 <code>이어진 문장인지 아닌지 반복 학습한다. 따라서 문장 간 의미 관계를 이해</code>할 수 있다.</p></li><li><p>NSP Task가 너무 쉬워지는 것을 방지하기 위해 문장 맨 앞 또는 맨 뒤쪽 단어 일부를 삭제했기 때문에 <code>일부 문장 성분이 없어도 전체 의미를 이해하는 데 큰 무리가 없다</code>.</p></li><li><p>학습 데이터의 10%는 사용자가 정한 최대 길이(max_sequence_length)보다 짧은 데이터로 구성돼 있기 때문에 학습 데이터에 <code>짧은 문장이 포함돼 있어도 성능이 크게 떨어지지 않는다</code>.</p></li></ul></li></ul></li></ul><h3 id="BERT-모델의-문장-구조"><a href="#BERT-모델의-문장-구조" class="headerlink" title="BERT 모델의 문장 구조"></a>BERT 모델의 문장 구조</h3><ul><li><p>BERT 모델은 트랜스포머 Encoder를 일부 변형한 아키텍쳐이다. 참고로 GPT는 트랜스포머의 Decoder 구조를 일부 변형한 아키텍쳐이다. Original 트랜스포머와 차이점을 위주로 설명할 것이다.</p></li><li><p>BERT 모델은 문장의 시작을 알리는 [CLS], 문장의 종결을 의미하는 [SEP], 마스크 Token [MASK], 배치 데이터의 길이를 맞춰주기 위한 [PAD] 등의 4가지 스페셜 Token을 사용한다. BERT 모델의 입력 Layer을 시각화하면 다음과 같다.</p></li><li><p>우선 입력 Token에 해당하는 Token 벡터를 참조해 Token 임베딩을 만든다. 여기에 첫번째 문장인지, 두 번째 문장인지에 해당하는 segment 임베딩을 참조해 더해준다. 마지막으로 입력 Token의 문장 내 절대적인 위치에 해당하는 Position Embedding을 더 한다. 이렇게 3개 임베딩을 더한 각각의 벡터에 Layer Normalization을 하고 Dropout을 시행해 첫 번째 트랜스포머 블록의 입력 행렬을 구성한다. 아래 그림처럼 Token 수가 11개인 문장이라면 트랜스포머 블록의 입력행렬의 크기는 11$\times$ Hidden 차원수가 된다. Token, Segment, Position 벡터를 만들 때 참조하는 행렬은 Pre-Train Task 수행을 잘하는 방향으로 다른 학습 parameter와 함께 업데이트된다.</p></li></ul><p><img src="/image/Input_layer_on_BERT.png" alt="BERT의 입력 Layer"></p><ul><li><p>BERT가 사용하는 트랜스포머 블록에서 Original 트랜스포머와 가장 큰 차이점을 보이는 대목은 Position-wise Feedforward Networks 부분이다. Activation function을 기존의 ReLU 대신 GELU(Gaussian Error Linear Units)를 사용한다. 정규분포의 누적분포함수(cumulative distribution functions)인 GELU는 ReLU보다 0 주위에서 부드럽게 변화해 학습 성능을 높인다.</p></li><li><p>Original 트랜스포머와 BERT가 가장 크게 차이를 보이는 또 하나의 부분은 마지막 Prediction Layer이다. Mask Language Model, NSP를 수행하기 위해서이다. Mask Language Model과 관련된 Layer는 실제 모델에서 Pre-Train이 끝나면 그 역할을 다하고 제거되어 Transfer learning의 Pine Tuning할 경우에는 사용되지 않는다.</p><ul><li><p>Mask Language Model Layer의 입력은 마지막 트랜스포머 블록의 Mask 위치에 해당하는 Token 벡터이다. 예를 들면, BERT 모델의 입력 문장이 ‘너 오늘 [MASK] 몇시에 할 꺼야’이고, 띄어쓰기 기준으로 Token을 나눈다면 3번째 벡터가 Input_tensor가 된다. 이 벡터를 입력 당시와 동일한 차원 수로 선형변환을 한 뒤 Layer Normalization을 시행한다. 이후 Vocabulary 수 만큼으로 Projection하기 위해 가중치 벡터인 output_weights를 곱하고 output_bias를 더해 logit 벡터를 만든다. 여기서 <code>주목할 점은 입력 Layer에서 Token 벡터를 만들 때 참조하는 행렬을 output_weights로 재사용한다는 점</code>이다. BERT-base 다국어 모델의 단어 수가 10만 개 안팎인 점을 고려하면 계산, 메모리 효율성을 모두 달성하기 위한 전략이라고 생각할 수 있다.</p></li><li><p>NSP Layer의 입력은 마지막 트랜스포머 블록의 첫 번째 Token([[CLS]])에 해당하는 벡터이다. 이 벡터를 2차원으로 projection하는 가중치 행렬 output_weights를 곱하고, 여기에 2차원 크기의 바이어스 벡터를 더한 뒤 softmax함수를 취한다. 이 확률 벡터와 정답(True or False)과 비교해 CrossEntropy를 구하고 이를 최소화하는 방향으로 Parameter들을 업데이트한다.</p></li></ul></li></ul><h3 id="Pre-Train-Tutorial"><a href="#Pre-Train-Tutorial" class="headerlink" title="Pre-Train Tutorial"></a>Pre-Train Tutorial</h3><ul><li><p>BERT 모델을 Pre-Train하려면 GPU가 여러 개 있어야 한다. GPU 8개를 썼을 때 12개 Layer 크기의 기본 모델(BERT-base)를 Pre-Train 하는데 10~15일 정도 소요된다. 리소스가 많지 않은 분들은 이미 공개돼 있는 BERT Pre-Train 모델을 사용하는 것을 추천한다.</p></li><li><p>자연어 처리 연구자 오연택 님께서 한국어 BERT Pre-Train 모델을 공개했다. Pre-Train 과정 및 hyper parameter 세팅 등 자세한 내용은 <a href="https://github.com/yeontaek/BERT-Korean-Model" target="_blank" rel="noopener">여기</a>에서 확인 해볼 수 있다.</p></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/08/NLP_09/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 문장 수준 임베딩 - 01</title>
      <link>https://heung-bae-lee.github.io/2020/02/06/NLP_08/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/06/NLP_08/</guid>
      <pubDate>Wed, 05 Feb 2020 15:32:51 GMT</pubDate>
      <description>
      
        
        
          &lt;h4 id=&quot;참고로-이-모든-내용은-이기창-님의-한국어-임베딩이라는-책을-기반으로-작성하고-있다&quot;&gt;&lt;a href=&quot;#참고로-이-모든-내용은-이기창-님의-한국어-임베딩이라는-책을-기반으로-작성하고-있다&quot; class=&quot;headerlink&quot; title=&quot;
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h4 id="참고로-이-모든-내용은-이기창-님의-한국어-임베딩이라는-책을-기반으로-작성하고-있다"><a href="#참고로-이-모든-내용은-이기창-님의-한국어-임베딩이라는-책을-기반으로-작성하고-있다" class="headerlink" title="참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다."></a>참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.</h4><h1 id="문장-수준-임베딩"><a href="#문장-수준-임베딩" class="headerlink" title="문장 수준 임베딩"></a>문장 수준 임베딩</h1><ul><li><p>크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다.</p></li><li><p>행렬 분해</p><ul><li>LSA(잠재 의미 분석)</li></ul></li><li><p>확률 모형</p><ul><li>LDA(잠재 디리클레 할당)</li></ul></li><li><p>Neural Network</p><ul><li>Doc2Vec</li><li>ELMo</li><li>GPT (transformer 구조 - self-attention)</li><li>BERT (transformer 구조 - self-attention)</li></ul></li></ul><h2 id="잠재-의미-분석-LSA-Latent-Semantic-Analysis"><a href="#잠재-의미-분석-LSA-Latent-Semantic-Analysis" class="headerlink" title="잠재 의미 분석(LSA, Latent Semantic Analysis)"></a>잠재 의미 분석(LSA, Latent Semantic Analysis)</h2><ul><li><p>단어 수준 임베딩에서의 LSA 방법론들은 word-documents 행렬이나 TF-IDF 행렬, word-context 행렬 또는 PMI 행렬에 SVD로 차원 축소를 시행하고, 여기에서 단어에 해당하는 벡터를 취해 임베딩을 만드는 방법이었다. <code>문장 수준 입베딩에서의 LSA 방법은 단어 수준 임베딩에서의 LSA 방법론을 통해 얻게된 정확히 말하자면 SVD를 통해 축소된 행렬에서 문서에 대응하는 벡터를 취해 문서 임베딩을 만드는 방식이다.</code></p></li><li><p>실습 대상 데이터는 ratsgo.github.uo의 아티클 하나로 markdwon 문서의 제목과 본문을 그대로 텍스트로 저장한 형태이다. 1개 라인이 1개 문서에 해당한다. 불필요한 기호나 LaTex math 패기지의 문법으로 작성되어있는 부분들이 다수 존재한다. 우선 <code>이 실습의 가정을 수식이나 기호는 분석에 있어서 큰 의미를 갖지 않는다라고 가정하고 시작하겠다.</code></p></li><li><p>우선, 형태소분석기를 어떤것을 사용하던 가능하게 함수를 하나 만들어준다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from khaiii import KhaiiiApi</span><br><span class="line">from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_tokenizer(tokenizer_name):</span><br><span class="line">    <span class="keyword">if</span> tokenizer_name == <span class="string">"komoran"</span>:</span><br><span class="line">        tokenizer = Komoran()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"okt"</span>:</span><br><span class="line">        tokenizer = Okt()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"mecab"</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"hannanum"</span>:</span><br><span class="line">        tokenizer = Hannanum()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"kkma"</span>:</span><br><span class="line">        tokenizer = Kkma()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"khaiii"</span>:</span><br><span class="line">        tokenizer = KhaiiiApi()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="built_in">return</span> tokenizer</span><br></pre></td></tr></table></figure><ul><li>한 문단 별로 구분자를 어떤것으로 했는지 확인하기 하나씩 프린트해보았다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">corpus_fname = <span class="string">"./data/processed/processed_blog.txt"</span></span><br><span class="line"></span><br><span class="line">with open(corpus_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"---------------------------------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"---------------------------------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"---------------------------------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br></pre></td></tr></table></figure><p><img src="/image/read_lines.png" alt="라인 하나씩 출력"></p><ul><li>아래 코드를 실행하면 제일 처음 문서의 임베딩과 코사인 유사도가 가장 높은 문서 임베딩의 제목을 return해준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from sklearn.decomposition import TruncatedSVD</span><br><span class="line">from sklearn.preprocessing import normalize</span><br><span class="line">from sklearn.manifold import TSNE</span><br><span class="line">from sklearn.metrics.pairwise import cosine_similarity</span><br><span class="line"></span><br><span class="line">from bokeh.io import export_png, output_notebook, show</span><br><span class="line">from bokeh.plotting import figure</span><br><span class="line">from bokeh.models import Plot, Range1d, MultiLine, Circle, HoverTool, TapTool, BoxSelectTool, LinearColorMapper, ColumnDataSource, LabelSet, SaveTool, ColorBar, BasicTicker</span><br><span class="line">from bokeh.models.graphs import from_networkx, NodesAndLinkedEdges, EdgesAndLinkedNodes</span><br><span class="line">from bokeh.palettes import Spectral8</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def LSAeval(corpus_file, doc_idx, nth_top):</span><br><span class="line">    tokenizer =  get_tokenizer(<span class="string">"mecab"</span>)</span><br><span class="line">    titles, raw_corpus, noun_corpus = [], [], []</span><br><span class="line"></span><br><span class="line">    with open(corpus_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            try:</span><br><span class="line">                title, document = line.strip().split(<span class="string">'\u241E'</span>)</span><br><span class="line">                titles.append(title)</span><br><span class="line">                raw_corpus.append(document)</span><br><span class="line">                nouns = tokenizer.nouns(document)</span><br><span class="line">                noun_corpus.append(<span class="string">' '</span>.join(nouns))</span><br><span class="line">            except:</span><br><span class="line">                <span class="built_in">continue</span></span><br><span class="line">    <span class="comment"># 문서(단락)에서 기호들과 조사를 제외하고 명사들만 추출한 데이터 중 Unigram(ngram_range(1,1)),</span></span><br><span class="line">    <span class="comment"># DF가 1이상(min_df=1)인 데이터를 추려 TF-IDF 행렬을 만들 것이다.</span></span><br><span class="line">    vectorizer = TfidfVectorizer(min_df=1,</span><br><span class="line">                             ngram_range=(1,1),</span><br><span class="line">                             <span class="comment"># tokenizing전에 모든 문자를 소문자로 바꿔준다.</span></span><br><span class="line">                             lowercase=True,</span><br><span class="line">                             <span class="comment"># analyzer == 'word'인 경우만 사용가능.</span></span><br><span class="line">                             tokenizer=lambda x: x.split())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 행은 문서, 열은 단어에 각각 대응한다. (204 x 37153)</span></span><br><span class="line">    input_matrix = vectorizer.fit_transform(noun_corpus)</span><br><span class="line"></span><br><span class="line">    id2vocab = &#123;vectorizer.vocabulary_[token]:token <span class="keyword">for</span> token <span class="keyword">in</span> vectorizer.vocabulary_.keys()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># curr_doc : Corpus 첫번 째 문서의 TF-IDF 벡터</span></span><br><span class="line">    curr_doc, result = input_matrix[doc_idx], []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># curr_doc에서 TF-IDF 값이 0이 아닌 요소들은 내림차순 정렬</span></span><br><span class="line">    <span class="comment"># curr_doc은 105개의 원소(단어)만이 저장되어 있는 Compressed Sparse Row format이다.</span></span><br><span class="line">    <span class="comment"># 그러므로 indices(CSR format index array of the matrix)로 해당 index에 위치하는 단어와 그에대한 tf-idf값을 쌍으로 tuple형태로 넣어준다.</span></span><br><span class="line">    <span class="keyword">for</span> idx, el <span class="keyword">in</span> zip(curr_doc.indices, curr_doc.data):</span><br><span class="line">        result.append((id2vocab[idx], el))</span><br><span class="line"></span><br><span class="line">    sorted(result, key=lambda x : x[1], reverse=True)[:5]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 이번에는 이 TF-IDF 행렬에 100차원 SVD를 수행할 것이다. 204 x 37153의 희소 행렬을</span></span><br><span class="line">    <span class="comment"># 204 x 100 크기의 Dense Matrix로 linear Transforamtion하는 것이다.</span></span><br><span class="line">    svd =  TruncatedSVD(n_components=100)</span><br><span class="line">    vecs = svd.fit_transform(input_matrix)</span><br><span class="line">    svd_l2norm_vectors = normalize(vecs, axis=1, norm=<span class="string">'l2'</span>)</span><br><span class="line">    cosine_similarity = np.dot(svd_l2norm_vectors, svd_l2norm_vectors[doc_idx])</span><br><span class="line">    query_sentence = titles[doc_idx]</span><br><span class="line">    <span class="built_in">return</span> titles, svd_l2norm_vectors, [query_sentence, sorted(zip(titles, cosine_similarity), key=lambda x: x[1], reverse=True)[1:nth_top + 1]]</span><br></pre></td></tr></table></figure><p><img src="/image/noun_corpus.png" alt="임베딩 벡터를 만든 Corpus는 명사만 추출"></p><ul><li>상위 5개의 벡터의 내적이 높은 순으로 내림차순 정력했을때의 결과물 출력</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">titles, svd_l2norm_vectors, top_five = LSAeval(corpus_file=<span class="string">"./data/processed/processed_blog.txt"</span>, doc_idx=0, nth_top=5)</span><br><span class="line">top_five</span><br></pre></td></tr></table></figure><p><img src="/image/top_5_similarity.png" alt="첫번째 문서와의 높은 유사도를 갖는 상위 5개의 문서"></p><h3 id="임베딩-시각화"><a href="#임베딩-시각화" class="headerlink" title="임베딩 시각화"></a>임베딩 시각화</h3><ul><li>t-SNE 기법을 사용해서 벡터공간을 2차원으로 줄여준 뒤 시각화 할 것이다. 또한 벡터들간의 전체적인 유사도는 시각적으로 그리기보다는 상관행렬 방식으로 나타내 줄 것이다.</li></ul><ul><li><p>시각화에 필요한 함수들 정의</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def visualize(titles, vectors, mode=<span class="string">"between"</span>, num_sents=30, palette=<span class="string">"Viridis256"</span>, use_notebook=False):</span><br><span class="line">        doc_idxes = random.sample(range(len(titles)), num_sents)</span><br><span class="line">        sentences = [titles[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> doc_idxes]</span><br><span class="line">        vecs = [vectors[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> doc_idxes]</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">"between"</span>:</span><br><span class="line">            visualize_between_sentences(sentences, vecs, palette, use_notebook=use_notebook)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            visualize_sentences(vecs, sentences, palette, use_notebook=use_notebook)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def visualize_between_sentences(sentences, vec_list, palette=<span class="string">"Viridis256"</span>,</span><br><span class="line">                                filename=<span class="string">"between-sentences.png"</span>,</span><br><span class="line">                                use_notebook=False):</span><br><span class="line">    df_list, score_list = [], []</span><br><span class="line">    <span class="keyword">for</span> sent1_idx, sentence1 <span class="keyword">in</span> enumerate(sentences):</span><br><span class="line">        <span class="keyword">for</span> sent2_idx, sentence2 <span class="keyword">in</span> enumerate(sentences):</span><br><span class="line">            vec1, vec2 = vec_list[sent1_idx], vec_list[sent2_idx]</span><br><span class="line">            <span class="keyword">if</span> np.any(vec1) and np.any(vec2):</span><br><span class="line">                score = cosine_similarity(X=[vec1], Y=[vec2])</span><br><span class="line">                <span class="comment"># [0][0]인 이유는 값만 뽑아 내기 위해서이다.</span></span><br><span class="line">                df_list.append(&#123;<span class="string">'x'</span>: sentence1, <span class="string">'y'</span>: sentence2, <span class="string">'similarity'</span>: score[0][0]&#125;)</span><br><span class="line">                score_list.append(score[0][0])</span><br><span class="line">    df = pd.DataFrame(df_list)</span><br><span class="line">    color_mapper = LinearColorMapper(palette=palette, low=np.max(score_list), high=np.min(score_list))</span><br><span class="line">    TOOLS = <span class="string">"hover,save,pan,box_zoom,reset,wheel_zoom"</span></span><br><span class="line">    p = figure(x_range=sentences, y_range=list(reversed(sentences)),</span><br><span class="line">                x_axis_location=<span class="string">"above"</span>, plot_width=900, plot_height=900,</span><br><span class="line">                toolbar_location=<span class="string">'below'</span>, tools=TOOLS,</span><br><span class="line">                tooltips=[(<span class="string">'sentences'</span>, <span class="string">'@x @y'</span>), (<span class="string">'similarity'</span>, <span class="string">'@similarity'</span>)])</span><br><span class="line">    p.grid.grid_line_color = None</span><br><span class="line">    p.axis.axis_line_color = None</span><br><span class="line">    p.axis.major_tick_line_color = None</span><br><span class="line">    p.axis.major_label_standoff = 0</span><br><span class="line">    p.xaxis.major_label_orientation = 3.14 / 3</span><br><span class="line">    p.rect(x=<span class="string">"x"</span>, y=<span class="string">"y"</span>, width=1, height=1,</span><br><span class="line">            <span class="built_in">source</span>=df,</span><br><span class="line">            fill_color=&#123;<span class="string">'field'</span>: <span class="string">'similarity'</span>, <span class="string">'transform'</span>: color_mapper&#125;,</span><br><span class="line">            line_color=None)</span><br><span class="line">    color_bar = ColorBar(ticker=BasicTicker(desired_num_ticks=5),</span><br><span class="line">                        color_mapper=color_mapper, major_label_text_font_size=<span class="string">"7pt"</span>,</span><br><span class="line">                        label_standoff=6, border_line_color=None, location=(0, 0))</span><br><span class="line">    p.add_layout(color_bar, <span class="string">'right'</span>)</span><br><span class="line">    <span class="keyword">if</span> use_notebook:</span><br><span class="line">        output_notebook()</span><br><span class="line">        show(p)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        export_png(p, filename)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"save @ "</span> + filename)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def visualize_sentences(vecs, sentences, palette=<span class="string">"Viridis256"</span>, filename=<span class="string">"/notebooks/embedding/sentences.png"</span>,</span><br><span class="line">                        use_notebook=False):</span><br><span class="line">    tsne = TSNE(n_components=2)</span><br><span class="line">    tsne_results = tsne.fit_transform(vecs)</span><br><span class="line">    df = pd.DataFrame(columns=[<span class="string">'x'</span>, <span class="string">'y'</span>, <span class="string">'sentence'</span>])</span><br><span class="line">    df[<span class="string">'x'</span>], df[<span class="string">'y'</span>], df[<span class="string">'sentence'</span>] = tsne_results[:, 0], tsne_results[:, 1], sentences</span><br><span class="line">    <span class="built_in">source</span> = ColumnDataSource(ColumnDataSource.from_df(df))</span><br><span class="line">    labels = LabelSet(x=<span class="string">"x"</span>, y=<span class="string">"y"</span>, text=<span class="string">"sentence"</span>, y_offset=8,</span><br><span class="line">                      text_font_size=<span class="string">"12pt"</span>, text_color=<span class="string">"#555555"</span>,</span><br><span class="line">                      <span class="built_in">source</span>=<span class="built_in">source</span>, text_align=<span class="string">'center'</span>)</span><br><span class="line">    color_mapper = LinearColorMapper(palette=palette, low=min(tsne_results[:, 1]), high=max(tsne_results[:, 1]))</span><br><span class="line">    plot = figure(plot_width=900, plot_height=900)</span><br><span class="line">    plot.scatter(<span class="string">"x"</span>, <span class="string">"y"</span>, size=12, <span class="built_in">source</span>=<span class="built_in">source</span>, color=&#123;<span class="string">'field'</span>: <span class="string">'y'</span>, <span class="string">'transform'</span>: color_mapper&#125;, line_color=None, fill_alpha=0.8)</span><br><span class="line">    plot.add_layout(labels)</span><br><span class="line">    <span class="keyword">if</span> use_notebook:</span><br><span class="line">        output_notebook()</span><br><span class="line">        show(plot)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        export_png(plot, filename)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"save @ "</span> + filename)</span><br></pre></td></tr></table></figure></li><li><p>혹시 이러한 error가 난다면, 다음과 같이 PhantomJS를 설치한다. 간단히 말하자면 PhantomJS도 Selenium같이 웹브라우져 개발용으로 만들어진 프로그램이다. bokeh는 javascript기반으로 짜여져있어서 필요한 것 같다.</p></li></ul><p><img src="/image/phantomjs_error.png" alt="phantomjs error"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge phantomjs</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visualize(titles, svd_l2norm_vectors, mode=<span class="string">"between"</span>, num_sents=30, palette=<span class="string">"Viridis256"</span>, use_notebook=True)</span><br></pre></td></tr></table></figure><p><img src="/image/sentence_between_cosine_simularity.png" alt="벡터들간의 유성성 상관행렬"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visualize(titles, svd_l2norm_vectors, mode=<span class="string">"tsne"</span>, num_sents=30, palette=<span class="string">"Viridis256"</span>, use_notebook=True)</span><br></pre></td></tr></table></figure><p><img src="/image/t_SNE_plot_bog_source.png" alt="t-SNE를 활용한 임베딩 벡터 시각화"></p><h2 id="Doc2Vec"><a href="#Doc2Vec" class="headerlink" title="Doc2Vec"></a>Doc2Vec</h2><h4 id="모델-개요"><a href="#모델-개요" class="headerlink" title="모델 개요"></a>모델 개요</h4><ul><li><p>Word2Vec에 이어 구글 연구 팀이 개발한 문서 임베딩 기법이다. <code>이전 단어 sequence k개가 주어졌을 때 그 다음 단어를 맞추는 언어 모델</code>을 만들었다. <code>이 모델은 문장 전체를 처음부터 끝까지 한 단어씩 슬라이딩해 가면서 다음 단어가 무엇일지 예측한다.</code></p></li><li><p>로그 확률 평균의 값이 커진다는 의미는 이전 k개 단어들을 입력하면 모델이 다음 단어를 잘 예측하므로 로그 확률 평균을 최대화는 과정에서 학습된다.</p></li><li><p>NPLM에서 설명했던 방식처럼 문장 전체를 한 단어씩 슬라이딩해가면서 다음 target word를 맞추는 과정에서 context word에 해당하는 $(w_{t-k}, \cdots, w_{t-1})$에 해당하는 W 행렬의 벡터들이 업데이트 한다. <code>따라서 주변 이웃 단어 집합 즉 context가 유사한 단어벡터는 벡터 공간에 가깝게 임베딩 된다.</code>학습이 종료되면 W를 각 단어의 임베딩으로 사용한다.</p></li></ul><h5 id="Doc2vec-언어-모델"><a href="#Doc2vec-언어-모델" class="headerlink" title="Doc2vec 언어 모델"></a>Doc2vec 언어 모델</h5><ul><li>$T$ : 학습 데이터 문장 하나의 단어 개수</li><li>$w_{t}$ : 문장의 t번째 단어</li><li>$y_{i}$ : corpus 전체 어휘 집합 중 i번째 단어에 해당하는 점수<ul><li>1) 이전 k개 단어들을 W라는 단어 행렬에서 참조한 뒤 평균을 취하거나 이어 붙인다. 여기에 U라는 행렬을 내적하고 bias 벡터인 b를 더해준 뒤 softmax를 취한다. U의 크기는 어휘집합 크기 $\times$ 임베딩 차원 수 이다.</li></ul></li><li>$h$ : 벡터 sequence가 주어졌을 때 평균을 취하거나 concatenate하여 고정된 길이의 벡터 하나를 반환하는 역할을 하는 함수이다.</li></ul><script type="math/tex; mode=display">L = frac{1}{T} \sum^{T-1}-{t=k} log(w_{t}|w_{t-k}, \cdots, w_{t-1})</script><h5 id="Doc2Vec-언어-모델-Score-계산"><a href="#Doc2Vec-언어-모델-Score-계산" class="headerlink" title="Doc2Vec 언어 모델 Score 계산"></a>Doc2Vec 언어 모델 Score 계산</h5><script type="math/tex; mode=display">P(w_{t}| w_{t-k}, \cdots, w_{t-1}) = \frac{exp(y_{w_{t}})}{\sum_{i} exp(y_{i})}</script><script type="math/tex; mode=display">y = b + U \cdot h(w_{t-k}, \cdots, w_{t-1}; W)</script><ul><li>위의 초기 구조에서 <code>문서 id를 추가해 이전 k개 단어들과 문서 id를 넣어서 다음 단어를 예측</code>하게 했다. <code>y를 계산할 때 D라는 문서 행렬(Paragraph matrix)에서 해당 문서 ID에 해당하는 벡터를 참조해 h 함수에 다른 단어 벡터들과 함께 입력하는 것 외에 나머지 과정은 동일</code>하다. 이런 구조를 <code>PV-DM(the Distributed Memory Model of Paragraph Vectors)</code>이라고 부른다. 학습이 종료되면 문서 수 $\times$ 임베딩 차원 수 크기를 가지는 문서 행렬 D를 각 문서의 임베딩으로 사용한다. 이렇게 만든 <code>문서 임베딩이 해당 문서의 주제 정보를 함축한다고 설명</code>한다. <code>PV-DM은 단어 등장 순서를 고려하는 방식으로 학습하기 때문에 순서 정보를 무시하는 Bag of Words 기법 대비 강점이 있다고 할 수 있을 것</code>이다.</li></ul><p><img src="/image/Doc2vec_model_principal.png" alt="Doc2vec 모델"></p><ul><li>또한, Word2Vec의 Skip-gram을 본뜬 <code>PV-DBOW</code>(the Distributed Bag of Words version of Paragraph Vectors)도 제안했다. Skip-gram은 target word를 가지고 context word들을 예측하는 과정에서 학습되었다. PV-DBOW도 문서 id를 가지고 context word들을 맞춘다. 따라서 <code>문서 id에 해당하는 문서 임베딩엔 문서에 등장하는 모든 단어의 의미 정보가 반영</code>된다.</li></ul><p><img src="/image/Doc2Vec_DBOW_model.png" alt="Doc2Vec CBOW 모델"></p><h2 id="Doc2Vec-실습"><a href="#Doc2Vec-실습" class="headerlink" title="Doc2Vec 실습"></a>Doc2Vec 실습</h2><h3 id="실습-데이터"><a href="#실습-데이터" class="headerlink" title="실습 데이터"></a>실습 데이터</h3><ul><li>영화 댓글과 해당 영화의 ID가 라인 하나를 구성하고 있다. 영화하나를 문서로 보고 Doc2Vec 모델을 학습할 예정이다. 따라서 영화 ID가 동일한 문장들을 하나의 문서로 처리해 줄 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">with open(<span class="string">"./data/processed/processed_review_movieid.txt"</span>) as f:</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    count = 4</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> f:</span><br><span class="line">        count+=1</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"총 sentence의 개수 : &#123;&#125;개"</span>.format(count))</span><br></pre></td></tr></table></figure><p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">종합 평점은 4점 드립니다.␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">원작이 칭송받는 이유는 웹툰 계 자체의 질적 저하가 심각하기 때문.  원작이나 영화나 별로인건 마찬가지.␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">나름의  감동도 있고 안타까운 마음에 가슴도 먹먹  배우들의 연기가 good 김수현 최고~␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">이런걸 돈주고 본 내자신이 후회스럽다 최악의 쓰레기 영화 김수현 밖에없는 저질 삼류영화␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">총 sentence의 개수 : 712532개</span><br></pre></td></tr></table></figure></p><ul><li>Doc2Vec 모델 학습을 위해 Python gensim 라이브러리의 Doc2Vec 클래스를 사용하는데 Doc2VecInput은 이 클래스가 요구하는 입력 형태를 맞춰주는 역할을 한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">from khaiii import KhaiiiApi</span><br><span class="line">from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma</span><br><span class="line">from gensim.models.doc2vec import TaggedDocument</span><br><span class="line"></span><br><span class="line">def get_tokenizer(tokenizer_name):</span><br><span class="line">    <span class="keyword">if</span> tokenizer_name == <span class="string">"komoran"</span>:</span><br><span class="line">        tokenizer = Komoran()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"okt"</span>:</span><br><span class="line">        tokenizer = Okt()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"mecab"</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"hannanum"</span>:</span><br><span class="line">        tokenizer = Hannanum()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"kkma"</span>:</span><br><span class="line">        tokenizer = Kkma()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"khaiii"</span>:</span><br><span class="line">        tokenizer = KhaiiiApi()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="built_in">return</span> tokenizer</span><br><span class="line"></span><br><span class="line">class Doc2VecInput:</span><br><span class="line"></span><br><span class="line">    def __init__(self, fname, tokenizer_name=<span class="string">'mecab'</span>):</span><br><span class="line">        self.fname = fname</span><br><span class="line">        self.tokenizer = get_tokenizer(tokenizer_name)</span><br><span class="line"></span><br><span class="line">    def __iter__(self):</span><br><span class="line">        with open(self.fname, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                try:</span><br><span class="line">                    sentence, movie_id = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                    tokens = self.tokenizer.morphs(sentence)</span><br><span class="line">                    tagged_doc = TaggedDocument(words=tokens, tags=[<span class="string">'movie_%s'</span> % movie_id])</span><br><span class="line">                    yield tagged_doc</span><br><span class="line">                except:</span><br><span class="line">                    <span class="built_in">continue</span></span><br></pre></td></tr></table></figure><ul><li><code>dm : 1 (default) -&gt; PV-DM, 0- &gt; PV-DBOW</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import Doc2Vec</span><br><span class="line"></span><br><span class="line">corpus_fname = <span class="string">'./data/processed/processed_review_movieid.txt'</span></span><br><span class="line">output_fname = <span class="string">'./doc2vec.model'</span></span><br><span class="line"></span><br><span class="line">corpus = Doc2VecInput(corpus_fname)</span><br><span class="line">model = Doc2Vec(corpus, dm=1, vector_size=100)</span><br><span class="line">model.save(output_fname)</span><br></pre></td></tr></table></figure><ul><li>학습이 잘 되었는지를 평가하기 위해서 아래와 같이 평가 클래스를 이용할 것이며, 평가를 하면 tag된 영화 id가 나올텐데 직관적으로 그 영화가 어떤 영화인지 모를 것이다. 그러므로 직관적으로 결과를 이해하기 위해 학습 데이터에는 없는 영화 제목을 네이버 영화 사이트에 접속해 id에 맞는 영화 제목을 스크래핑해 올 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from lxml import html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Doc2VecEvaluator:</span><br><span class="line"></span><br><span class="line">    def __init__(self, model_fname=<span class="string">"data/doc2vec.vecs"</span>, use_notebook=False):</span><br><span class="line">        self.model = Doc2Vec.load(model_fname)</span><br><span class="line">        self.doc2idx = &#123;el:idx <span class="keyword">for</span> idx, el <span class="keyword">in</span> enumerate(self.model.docvecs.doctags.keys())&#125;</span><br><span class="line">        self.use_notebook = use_notebook</span><br><span class="line"></span><br><span class="line">    def most_similar(self, movie_id, topn=10):</span><br><span class="line">        similar_movies = self.model.docvecs.most_similar(<span class="string">'movie_'</span> + str(movie_id), topn=topn)</span><br><span class="line">        <span class="keyword">for</span> movie_id, score <span class="keyword">in</span> similar_movies:</span><br><span class="line">            <span class="built_in">print</span>(self.get_movie_title(movie_id), score)</span><br><span class="line"></span><br><span class="line">    def get_titles_in_corpus(self, n_sample=5):</span><br><span class="line">        movie_ids = random.sample(self.model.docvecs.doctags.keys(), n_sample)</span><br><span class="line">        <span class="built_in">return</span> &#123;movie_id: self.get_movie_title(movie_id) <span class="keyword">for</span> movie_id <span class="keyword">in</span> movie_ids&#125;</span><br><span class="line"></span><br><span class="line">    def get_movie_title(self, movie_id):</span><br><span class="line">        url = <span class="string">'http://movie.naver.com/movie/point/af/list.nhn?st=mcode&amp;target=after&amp;sword=%s'</span> % movie_id.split(<span class="string">"_"</span>)[1]</span><br><span class="line">        resp = requests.get(url)</span><br><span class="line">        root = html.fromstring(resp.text)</span><br><span class="line">        try:</span><br><span class="line">            title = root.xpath(<span class="string">'//div[@class="choice_movie_info"]//h5//a/text()'</span>)[0]</span><br><span class="line">        except:</span><br><span class="line">            title = <span class="string">""</span></span><br><span class="line">        <span class="built_in">return</span> title</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_fname=<span class="string">'./doc2vec.model'</span></span><br><span class="line">model = Doc2VecEvaluator(model_fname)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"영화의 종류 : &#123;&#125; 개"</span>.format(len(model.doc2idx.keys())))</span><br></pre></td></tr></table></figure><p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">영화의 종류 : 14730 개</span><br></pre></td></tr></table></figure></p><ul><li>학습 데이터에 포함된 아무 영화 10개의 제목을 보여준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.get_titles_in_corpus(n_sample=14730)</span><br></pre></td></tr></table></figure><p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'movie_89743'</span>: <span class="string">'여자가 두 번 화장할 때'</span>,</span><br><span class="line"> <span class="string">'movie_16490'</span>: <span class="string">'투 문 정션 2'</span>,</span><br><span class="line"> <span class="string">'movie_100953'</span>: <span class="string">'더 퍼지'</span>,</span><br><span class="line"> <span class="string">'movie_84375'</span>: <span class="string">'퍼펙트 센스'</span>,</span><br><span class="line"> <span class="string">'movie_24203'</span>: <span class="string">'섀터드 이미지'</span>,</span><br><span class="line"> <span class="string">'movie_12054'</span>: <span class="string">'나폴레옹'</span>,</span><br><span class="line"> <span class="string">'movie_10721'</span>: <span class="string">'더티 해리'</span>,</span><br><span class="line"> <span class="string">'movie_11440'</span>: <span class="string">'킹 뉴욕'</span>,</span><br><span class="line"> <span class="string">'movie_20896'</span>: <span class="string">'미망인'</span>,</span><br><span class="line"> <span class="string">'movie_123068'</span>: <span class="string">'캠걸'</span>&#125;</span><br></pre></td></tr></table></figure></p><ul><li>해당 id와 유사한 영화 상위 5개를 보여준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.most_similar(37758, topn=5)</span><br></pre></td></tr></table></figure><p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">돈비 어프레이드-어둠 속의 속삭임 0.746237576007843</span><br><span class="line">더 퍼지:거리의 반란 0.7402248382568359</span><br><span class="line">고양이: 죽음을 보는 두 개의 눈 0.6967850923538208</span><br><span class="line">ATM 0.690518856048584</span><br><span class="line">힛쳐 0.6751468777656555</span><br></pre></td></tr></table></figure></p><h2 id="잠재-디리클레-할당-LDA-Latent-Dirichlet-Allocation"><a href="#잠재-디리클레-할당-LDA-Latent-Dirichlet-Allocation" class="headerlink" title="잠재 디리클레 할당(LDA, Latent Dirichlet Allocation)"></a>잠재 디리클레 할당(LDA, Latent Dirichlet Allocation)</h2><ul><li><code>주어진 문서에 대하여 각 문서에 어떤 topic들이 존재하는지에 대한 확률 모형</code>이다. corpus의 이면에 잠재된 topic을 추출한다는 의미에서 topic modeling이라고 부르기도 한다. <code>문서를 topic 확률 분포로 나타내 각각을 벡터화한다는 점에서 LDA를 임베딩 기법의 일종으로 이해</code>할 수 있다.</li></ul><h4 id="모델-개요-1"><a href="#모델-개요-1" class="headerlink" title="모델 개요"></a>모델 개요</h4><ul><li>LDA는 topic별 단어의 분포, 문서별 topic의 분포를 모두 추정해 낸다.</li></ul><p><img src="/image/LDA_brief_out.png" alt="LDA의 개략적 도식"></p><ul><li><p>LDA는 topic에 특정 단어가 나타날 확률을 내어 준다. 위의 그림에서 각 색깔별로 topic에 따른 단어가 등장할 확률을 보여주고있다. 문서를 보면 노란색 topic에 해당하는 단어가 많기 때문에 위 문서의 메인 주제는 노란색 topic인 유전자 관련 topic일 가능성이 클 것이다. 이렇듯 문서의 topic 비중 또한 LDA의 산출 결과가 된다.</p></li><li><p><code>LDA가 가정하는 문서 생성 과정</code>은 우리가 글을 쓸때와 같다. 실제 글을 작성할 때는 글감 내지 주제를 먼저 결정한 후 어떤 단어를 써야 할지 결정한다. 이와 마찬가지로 LDA는 <code>우선 corpus로 부터 얻은 topic 분포로부터 topic을 뽑는다. 이후 해당 topic에 해당하는 단어들을 뽑는다.</code> 그런데 corpus에 등장하는 단어들 각각에 꼬리표가 달려 있는 것은 아니기 때문에 현재 문서에 등장한 단어들은 어떤 topic에서 뽑힌 단어들인지 우리가 명시적으로 알기는 어려울 것이다. <code>하지만, LDA는 이런 corpus 이면에 존재하는 정보를 추론해낼 수 있다.</code></p></li></ul><h4 id="아키텍처"><a href="#아키텍처" class="headerlink" title="아키텍처"></a>아키텍처</h4><ul><li>LDA가 가정하는 문서 생성 과정은 아래의 그림과 같다.<ul><li>D : corpus 전체 문서 개수</li><li>K : 전체 topic 수(hyper parameter)</li><li>N : d번째 문서의 단어 수</li><li>네모칸 : 해당 횟수만큼 반복하라는 의미</li><li>$\phi_{k}$ : k번째 topic에 해당하는 벡터 $\phi_{k} \in R^{|V|}$, $\phi_{k}$는 word-topic 행렬의 k번째 열을 의미한다. $\phi_{k}$의 각 요소 값은 해당 단어가 k번째 토픽에서 차지하는 비중을 의미하며 확률값이므로 이 벡터의 요소값의 합은 1이 된다. 이런 topic의 단어비중을 의미하는 $\phi_{k}$는 디리클레 분포를 따른 다는 가정을 취하므로 $\beta$의 영향을 받는다.</li><li>$\theta_{d}$ : d번째 문서가 가진 topic 비중을 나타내는 벡터이다. 그러므로 전체 topic의 개수만큼의 길이 갖으며, 각 벡터는 확률을 의미하므로 합은 1이된다. 문서의 topic 비중 $\theta_{d}$는 디리클레 분포를 따른다는 가정을 취하므로 $\alpha$의 영향을 받는다.</li><li>$ z_{d,n}$ : d번째 문서에서 n번째 단어가 어떤 topic인지를 나타내는 변수이다. 그러므로 이 변수는 d번째 문서의 topic 확률 분포인 $\theta_{d}$에 영향을 받는다.</li><li>동그라미 : 변수를 의미</li><li>화살표가 시작되는 변수는 조건, 화살표가 향하는 변수는 결과에 해당하는 변수</li></ul></li></ul><ul><li><code>관찰 가능한 변수는 d번째 문서에 등장한 n번째 단어 $w_{d,n}$이 유일</code>하다. 우리는 이 정보만을 가지고 하이퍼파라메터(사용자 지정) α,β를 제외한 모든 잠재 변수를 추정해야 한다.</li></ul><p><img src="/image/graphical_model_LDA.png" alt="graphical model로 표현한 LDA"></p><ul><li>예시를 들자면, 아래 Document-topic 행렬을 살펴보자. 3번째 문서에 속한 단어들은 가장 높은 확률값 0.625를 갖는 topic-2일 가능성이 높다. $w_{d,n}$은 d번째 문서 내에 n번째로 등장하는 단어를 의미하며, 동시에 우리가 유일하게 corpus에서 관찰할 수 있는 데이터이다. 이는 $\phi_{k}$와 $\z_{d,n}$에 동시에 영향을 받는다. 예를 들면, $z_{3,1}$가 topic-2이라고 가정했을 경우, $w_{3,1}$은 word-topic 행렬에서 보게되면 제일 높은 확률값 0.393을 갖는 ‘코로나 바이러스’일 가능성이 높다.</li></ul><h4 id="이처럼-LDA는-topic의-word-분포-phi-와-문서의-topic-분포-theta-의-결합으로-문서-내-단어들이-생성된다고-가정한다"><a href="#이처럼-LDA는-topic의-word-분포-phi-와-문서의-topic-분포-theta-의-결합으로-문서-내-단어들이-생성된다고-가정한다" class="headerlink" title="이처럼 LDA는 topic의 word 분포($\phi$)와 문서의 topic 분포($\theta$)의 결합으로 문서 내 단어들이 생성된다고 가정한다."></a>이처럼 LDA는 topic의 word 분포($\phi$)와 문서의 topic 분포($\theta$)의 결합으로 문서 내 단어들이 생성된다고 가정한다.</h4><h5 id="Document-topic-행렬"><a href="#Document-topic-행렬" class="headerlink" title="Document-topic 행렬"></a>Document-topic 행렬</h5><div class="table-container"><table><thead><tr><th>문서</th><th>topic-1</th><th>topic-2</th><th>topic-3</th></tr></thead><tbody><tr><td>문서1</td><td>0.400</td><td>0.000</td><td>0.600</td></tr><tr><td>문서2</td><td>0.000</td><td>0.600</td><td>0.400</td></tr><tr><td>문서3</td><td>0.375</td><td>0.625</td><td>0.000</td></tr></tbody></table></div><h6 id="word-topic-행렬"><a href="#word-topic-행렬" class="headerlink" title="word-topic 행렬"></a>word-topic 행렬</h6><div class="table-container"><table><thead><tr><th>단어</th><th>topic-1</th><th>topic-2</th><th>topic-3</th></tr></thead><tbody><tr><td>코로나 바이러스</td><td>0.000</td><td>0.393</td><td>0.000</td></tr><tr><td>우한폐렴</td><td>0.000</td><td>0.313</td><td>0.000</td></tr><tr><td>AWS</td><td>0.119</td><td>0.000</td><td>0.000</td></tr><tr><td>데이터 엔지니어링</td><td>0.181</td><td>0.000</td><td>0.000</td></tr><tr><td>Hadoop</td><td>0.276</td><td>0.000</td><td>0.000</td></tr><tr><td>Spark</td><td>0.142</td><td>0.000</td><td>0.000</td></tr><tr><td>낭만 닥터 김사부2</td><td>0.000</td><td>0.012</td><td>0.468</td></tr><tr><td>tensorflow</td><td>0.282</td><td>0.000</td><td>0.000</td></tr><tr><td>마스크</td><td>0.000</td><td>0.282</td><td>0.000</td></tr><tr><td>사랑의 불시착</td><td>0.000</td><td>0.000</td><td>0.532</td></tr><tr><td>합</td><td>1.0</td><td>1.0</td><td>1.0</td></tr></tbody></table></div><ul><li>실제 관찰 가능한 corpus를 가지고 알고 싶은 topic의 word 분포, 문서의 topic 분포를 추정하는 과정을 통해 LDA는 학습한다. 즉, <code>topic의 word 분포와 문서의 topic 분포의 결합 확률이 커지는 방향으로 학습을 한다는 의미</code>이다.</li></ul><h5 id="LDA의-단어-생성-과정"><a href="#LDA의-단어-생성-과정" class="headerlink" title="LDA의 단어 생성 과정"></a>LDA의 단어 생성 과정</h5><script type="math/tex; mode=display">p(\phi_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D}) =</script><script type="math/tex; mode=display">\prod^{K}_{i=1} p(\phi_{i}|\beta) \prod^{D}_{d=1} p(\theta_{d}|\alpha) \left(\prod^{N}_{n=1} p(z_{d,n}|\theta_{d}) p(w_{d,n}|\phi_{1:K}, z_{d, n}) \right)</script><ul><li><p>우리가 구해야할 사후확률 분포는 $ p(z, \phi, \theta|w) = p(z, \phi, \theta, w)/p(w)$를 최대로 만드는 $ z, \phi, \theta$를 찾아야 한다. 이 사후확률을 직접 계산하려면 분자도 계산하기 어렵겠지만 분모가 되는 $ p(w)$도 반드시 구해야 확률값으로 만들어 줄 수 있다. $ p(w)$는 잠재변수 $ z, \phi, \theta$의 모든 경우의 수를 고려한 각 단어(w)의 등장 확률을 의미하는데, $ z, \phi, \theta$를 직접 관찰하는 것은 불가능하다. 이러한 이유로 <code>깁스 샘플링(gibbs sampling)</code>같은 표본 추출 기법을 사용해 사후확률을 근사시키게 된다. 깁스 샘플링이란 <code>나머지 변수는 고정시킨 채 하나의 랜덤변수만을 대상으로 표본을 뽑는 기법</code>이다. LDA에서는 사후확률 분포 $ p(z, \phi, \theta|w)$를 구할 때 topic의 단어 분포($\phi$)와 문서의 topic 분포($\theta$)를 계산에서 생략하고 topic(z)만을 추론한다. <code>z만 알 수 있으면 나미저 변수를 이를 통해 계산 할 수 있도록 설계 했기 때문이다.</code></p></li><li><p><a href="https://ratsgo.github.io/statistics/2017/05/31/gibbs/" target="_blank" rel="noopener">깁스 샘플링 참조</a></p></li></ul><h5 id="깁스-샘플링을-활용한-LDA"><a href="#깁스-샘플링을-활용한-LDA" class="headerlink" title="깁스 샘플링을 활용한 LDA"></a>깁스 샘플링을 활용한 LDA</h5><script type="math/tex; mode=display">p(z_{d, i} = j|z_{-i}, w) = \frac{ n_{d,k} + \alpha_{j} }{ \sum^{K}_{i=1} (n_{d,i}) + \alpha_{i} } \times \frac{ v_{k, w_{d,n} } + \beta_{w_{d,n}} }{ \sum^{V}_{ㅓ=1} ( v_{k,j} + \beta_{j} ) } = AB</script><h6 id="LDA-변수-표기법"><a href="#LDA-변수-표기법" class="headerlink" title="LDA 변수 표기법"></a>LDA 변수 표기법</h6><div class="table-container"><table><thead><tr><th>표기</th><th>내용</th></tr></thead><tbody><tr><td>$n_{d,k}$</td><td>k번째 topic에 할당된 d번째 문서의 빈도</td></tr><tr><td>$v_{k,w_{d,n}}$</td><td>전체 corpus에서 k번째 topic에 할당된 단어 $w_{d,n}$의 빈도</td></tr><tr><td>$w_{d,n}$</td><td>d번째 문서에 n번째로 등장한 단어</td></tr><tr><td>$\alpha$</td><td>문서의 topic 분포 생성을 위한 디리클레 분포 파라미터</td></tr><tr><td>$\beta$</td><td>topic의 word 분포 생성을 위한 디리클레 분포 파라미터</td></tr><tr><td>K</td><td>사용자가 지정하는 topic 개수</td></tr><tr><td>V</td><td>corpus에 등장하는 전체 word의 수</td></tr><tr><td>A</td><td>d번째 문서가 k번째 topic과 맺고 있는 연관성 정도</td></tr><tr><td>B</td><td>d번째 문서의 n번째 단어($ w_{d,n}$)가 k번째 topic과 맺고 있는 연관성 정도</td></tr></tbody></table></div><h4 id="LDA와-깁스-샘플링"><a href="#LDA와-깁스-샘플링" class="headerlink" title="LDA와 깁스 샘플링"></a>LDA와 깁스 샘플링</h4><ul><li>LDA가 각 단어에 잠재된 주제를 추론하는 방식을 살펴본다. 아래표와 같이 단어 5개로 구성된 문서1의 모든 단어에 주제(z)가 이미 할당돼 있다고 가정해보자. <code>LDA는 이렇게 문서 전체의 모든 단어의 주제를 랜덤하게 할당을 하고 학습을 시작하기 때문에 이렇게 가정하는 게 크게 무리가 없다. 또한, topic 수는 사용자가 3개로 이미 지정해 놓은 상태라고 하자.</code> 문서1의 첫 번째 단어($ w_{11} = 천주교)$ 의 주제($z_{11}$)는 3번 topic이다. 마찬가지로 문서1의 3 번째 단어($ w_{13} = 가격$)의 주제($z_{13}$)는 1번 topic이다. 이런 방식으로 Corpus 전체 문서 모든 단어에 topic이 이미 할당됐다고 가정한다. 이로부터 word-topic 행렬을 만들수 있다. <code>전체 문서 모든 단어에 달린 주제들을 일일이 세어서 만든다. 같은 단어라도 topic이 다른 배(동음다의어)같은 경우가 있으므로 각 단어별로 topic 분포가 생겨난다.</code></li></ul><h5 id="문서-1의-단어별-topic-분포-theta"><a href="#문서-1의-단어별-topic-분포-theta" class="headerlink" title="문서 1의 단어별 topic 분포($\theta$)"></a>문서 1의 단어별 topic 분포($\theta$)</h5><div class="table-container"><table><thead><tr><th>$z_{1i}$</th><th>3</th><th>2</th><th>1</th><th>3</th><th>1</th></tr></thead><tbody><tr><td>$w_{1,n}$</td><td>천주교</td><td>무역</td><td>가격</td><td>불교</td><td>시장</td></tr></tbody></table></div><h5 id="word-topic-행렬-1"><a href="#word-topic-행렬-1" class="headerlink" title="word-topic 행렬"></a>word-topic 행렬</h5><div class="table-container"><table><thead><tr><th>단어</th><th>topic-1</th><th>topic-2</th><th>topic-3</th></tr></thead><tbody><tr><td>천주교</td><td>1</td><td>0</td><td>35</td></tr><tr><td>시장</td><td>50</td><td>0</td><td>1</td></tr><tr><td>가격</td><td>42</td><td>1</td><td>0</td></tr><tr><td>불교</td><td>0</td><td>0</td><td>20</td></tr><tr><td>무역</td><td>10</td><td>8</td><td>1</td></tr><tr><td>$\cdots$</td><td>$\cdots$</td><td>$\cdots$</td><td>$\cdots$</td></tr></tbody></table></div><ul><li>깁스 샘플링으로 문서1 두 번째 단어의 잠재된 topic이 무엇인지 추론해보자면, 깁스 샘플링을 적용하기 위해 문서1의 두 번째 topic정보를 지울것이다. 그렇다면 아래와 같은 표로 변화될 것이다.</li></ul><h5 id="문서1의-단어별-topic-분포"><a href="#문서1의-단어별-topic-분포" class="headerlink" title="문서1의 단어별 topic 분포"></a>문서1의 단어별 topic 분포</h5><div class="table-container"><table><thead><tr><th>$z_{1i}$</th><th>3</th><th>?</th><th>1</th><th>3</th><th>1</th></tr></thead><tbody><tr><td>$w_{1,n}$</td><td>천주교</td><td>무역</td><td>가격</td><td>불교</td><td>시장</td></tr></tbody></table></div><h5 id="word-topic-행렬-2"><a href="#word-topic-행렬-2" class="headerlink" title="word-topic 행렬"></a>word-topic 행렬</h5><div class="table-container"><table><thead><tr><th>단어</th><th>topic-1</th><th>topic-2</th><th>topic-3</th></tr></thead><tbody><tr><td>천주교</td><td>1</td><td>0</td><td>35</td></tr><tr><td>시장</td><td>50</td><td>0</td><td>1</td></tr><tr><td>가격</td><td>42</td><td>1</td><td>0</td></tr><tr><td>불교</td><td>0</td><td>0</td><td>20</td></tr><tr><td>무역</td><td>10</td><td>7 = (8 - 1)</td><td>1</td></tr><tr><td>$\cdots$</td><td>$\cdots$</td><td>$\cdots$</td><td>$\cdots$</td></tr></tbody></table></div><ul><li>p($ z_{1,2} $)는 A와 B의 곱으로 도출된다. A값은 파란색 영역을 의미하며, 문서 내 단어들의 topic 분포에($\theta$)에 영향을 받는다. 또한, B값은 topic의 단어 분포($\phi$)에 영향을 받는다. A와 B를 각각 직사각형의 높이와 너비로 둔다면, p($ z_{1,2} $)는 아래와 같이 직사각형의 넓이로 이해할 수 있다. 이와 같은 방식으로 모든 문서, 모든 단어에 관해 깁스 샘플링을 수행하면 모든 단어마다 topic을 할당해줄 수가 있게 된다. 즉, word-topic 행렬을 완성할 수 있다는 것이다. <code>보통 1,000회 ~ 10,000회 반복  수행하면 그 결과가 수렴한다고 하며, 이를 토대로 문서의 topic 분포, topic 단어 분포 또한 구할 수 있게 된다.</code> $\theta$의 경우 각 문서에 어떤 단어사 쓰였는지 조사해 그 단어의 topic 분포를 더해주는 방식으로 계산한다. 사용자가 지정하는 하이퍼파라메터 α 존재 덕분에 A가 아예 0으로 되는 일을 막을 수 있게 된다. 일종의 smoothing 역할을 한다. 따라서 <code>α가 클수록 토픽들의 분포가 비슷해지고, 작을 수록 특정 토픽이 크게 나타나게 된다. 이는 β가 B에서 차지하는 역할도 동일</code>하다.</li></ul><p><img src="/image/LDA_probability_with_gibbs_sampling.png" alt="문서 1 두 번째 단어의 topic 추론"></p><h4 id="최적-토픽-수-찾기"><a href="#최적-토픽-수-찾기" class="headerlink" title="최적 토픽 수 찾기"></a>최적 토픽 수 찾기</h4><ul><li>LDA의 토픽수 K는 여러 실험을 통해 사용자가 지정하는 미지수인 hyper parameter이다. 최적 토픽수를 구하는 데 쓰는 Perplexity 지표있다. p(w)는 클수록 좋은 inference이므로 exp(−log(p(w)))는 작을수록 좋다. 따라서 토픽 수 K를 바꿔가면서 <code>Perplexity를 구한 뒤 가장 작은 값을 내는 K를 최적의 토픽수로 삼으면 된다.</code></li></ul><script type="math/tex; mode=display">Perplexity(w)=exp\left[ -\frac { log\left\{ p(w) \right\}  }{ \sum _{ d=1 }^{ D }{ \sum _{ j=1 }^{ V }{ { n }^{ jd } }  }  }  \right]</script><h3 id="LDA-실습"><a href="#LDA-실습" class="headerlink" title="LDA 실습"></a>LDA 실습</h3><h4 id="데이터-소개"><a href="#데이터-소개" class="headerlink" title="데이터 소개|"></a>데이터 소개|</h4><ul><li>네이버 영화 corpus를 soynlp로 띄어쓰기 교정한 결과를 LDA의 학습 데이터로 사용할 것이다.</li></ul><ul><li>아래의 코드는 LDA 모델 피처를 생성하는 역할을 한다. LDA의 입력값은 문서 내 단어의 등장 순서를 고려하지 않고 해당 단어가 몇 번 쓰였는지 그 빈도만을 따진다. 그런데 ‘노잼! 노잼! 노잼!’ 같이 특정 단어가 중복으로 사용된 문서가 있다면 해당 문서의 topic 분포가 한쪽으로 너무 쏠릴 염려가 있다. 이 때문에 token의 순서를 고려하지 않고 중복을 제거한 형태로 LDA 피처를 만들 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from gensim import corpora</span><br><span class="line">from khaiii import KhaiiiApi</span><br><span class="line">from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_tokenizer(tokenizer_name):</span><br><span class="line">    <span class="keyword">if</span> tokenizer_name == <span class="string">"komoran"</span>:</span><br><span class="line">        tokenizer = Komoran()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"okt"</span>:</span><br><span class="line">        tokenizer = Okt()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"mecab"</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"hannanum"</span>:</span><br><span class="line">        tokenizer = Hannanum()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"kkma"</span>:</span><br><span class="line">        tokenizer = Kkma()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"khaiii"</span>:</span><br><span class="line">        tokenizer = KhaiiiApi()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="built_in">return</span> tokenizer</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">corpus_fname = <span class="string">'./data/processed/corrected_ratings_corpus.txt'</span></span><br><span class="line"></span><br><span class="line">documents, tokenized_corpus = [], []</span><br><span class="line">tokenizer = get_tokenizer(<span class="string">'mecab'</span>)</span><br><span class="line"></span><br><span class="line">with open(corpus_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> f:</span><br><span class="line">        tokens = list(<span class="built_in">set</span>(tokenizer.morphs(document.strip())))</span><br><span class="line">        documents.append(document)</span><br><span class="line">        tokenized_corpus.append(tokens)</span><br><span class="line"></span><br><span class="line">dictionary = corpora.Dictionary(tokenized_corpus)</span><br><span class="line">corpus = [dictionary.doc2bow(text) <span class="keyword">for</span> text <span class="keyword">in</span> tokenized_corpus]</span><br></pre></td></tr></table></figure><p><a href="https://radimrehurek.com/gensim/corpora/dictionary.html" target="_blank" rel="noopener">corpora.Dictionary 참조</a></p><ul><li><p>dictionary형태로 vocabulary dictionary를 만들어주는 것이다.</p><ul><li>add_documents로 문서를 추가할 수도 있다.</li><li>doc2bow는 bag-of-words (BoW) 형태로 문서를 변환시켜준다. [(token_id, token_count)]형태이다.</li><li>doc2bow의 옵션으로 return_missing=True를 주면 해당 sentence의 단어중 미등록 단어와 카운트를 같이 출력해준다.</li></ul></li><li><p>아래 코드를 실행하면 LDA를 학습하고 그 결과를 확인 할 수 있다. LdaMulticore에서 num_topicss는 토픽 수(K)에 해당되는 parameter이다. get_document_topics라는 함수는 학습이 끝난 LDA 모델로부터 각 문서별 topic 분포를 리턴한다. minimum_probability 인자를 0.5를 줬는데, 이는 0.5미만의 topic 분포는 무시한다는 뜻이다. 특정 토픽의 확률이 0.5보다 클 경우에만 데이터를 리턴한다. 확률의 합은 1이기 때문에 해당 토픽이 해당 문서에서 확률값이 가장 큰 토픽이 된다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import ldamulticore</span><br><span class="line"></span><br><span class="line">LDA = ldamulticore.LdaMulticore(corpus, id2word=dictionary, num_topics=30, workers=4)</span><br><span class="line"></span><br><span class="line">all_topics = LDA.get_document_topics(corpus, minimum_probability=0.5, per_word_topics=False)</span><br></pre></td></tr></table></figure><ul><li>아래 결과를 해석하자면, 0번 문서는 전체 topic 30개 중 19번에 해당하는 topic의 확률 값이 제일 높으며 그 값은 0.7227057이다. 3번 문서 같은 경우 전체 topic 중 0.5를 넘는 topic이 없음을 확인 할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> doc_idx, topic <span class="keyword">in</span> enumerate(all_topics[:5]):</span><br><span class="line">    <span class="built_in">print</span>(doc_idx, topic)</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0 [(19, 0.7227057)]</span><br><span class="line">1 [(9, 0.5350808)]</span><br><span class="line">2 []</span><br><span class="line">3 [(19, 0.7778823)]</span><br><span class="line">4 [(14, 0.80464107)]</span><br></pre></td></tr></table></figure><h5 id="모델-저장"><a href="#모델-저장" class="headerlink" title="모델 저장"></a>모델 저장</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">output_fname = <span class="string">'./lda'</span></span><br><span class="line">all_topics = LDA.get_document_topics(corpus, minimum_probability=0.5, per_word_topics=False)</span><br><span class="line">with open(output_fname + <span class="string">".results"</span>, <span class="string">'w'</span>) as f:</span><br><span class="line">    <span class="keyword">for</span> doc_idx, topic <span class="keyword">in</span> enumerate(all_topics):</span><br><span class="line">        <span class="keyword">if</span> len(topic) == 1:</span><br><span class="line">            <span class="comment"># tuple 형태로 되어있는 데이터로 가져와서 나눠줌</span></span><br><span class="line">            topic_id, prob = topic[0]</span><br><span class="line">            f.writelines(documents[doc_idx].strip() + <span class="string">"\u241E"</span> + <span class="string">' '</span>.join(tokenized_corpus[doc_idx]) + <span class="string">"\u241E"</span> + str(topic_id) + <span class="string">"\u241E"</span> + str(prob) + <span class="string">"\n"</span>)</span><br><span class="line">LDA.save(output_fname + <span class="string">".model"</span>)</span><br></pre></td></tr></table></figure><h4 id="LDA-평가"><a href="#LDA-평가" class="headerlink" title="LDA 평가"></a>LDA 평가</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import LdaModel</span><br><span class="line">from collections import defaultdict</span><br><span class="line"></span><br><span class="line">class LDAEvaluator:</span><br><span class="line"></span><br><span class="line">    def __init__(self, model_path=<span class="string">"./lda"</span>, tokenizer_name=<span class="string">"mecab"</span>):</span><br><span class="line">        self.tokenizer = get_tokenizer(tokenizer_name)</span><br><span class="line">        self.all_topics = self.load_results(model_path + <span class="string">".results"</span>)</span><br><span class="line">        self.model = LdaModel.load(model_path + <span class="string">".model"</span>)</span><br><span class="line"></span><br><span class="line">    def load_results(self, results_fname):</span><br><span class="line">        topic_dict = defaultdict(list)</span><br><span class="line">        with open(results_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                sentence, _, topic_id, prob = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                topic_dict[int(topic_id)].append((sentence, <span class="built_in">float</span>(prob)))</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> topic_dict.keys():</span><br><span class="line">            topic_dict[key] = sorted(topic_dict[key], key=lambda x: x[1], reverse=True)</span><br><span class="line">        <span class="built_in">return</span> topic_dict</span><br><span class="line"></span><br><span class="line">    def show_topic_docs(self, topic_id, topn=10):</span><br><span class="line">        <span class="built_in">return</span> self.all_topics[topic_id][:topn]</span><br><span class="line"></span><br><span class="line">    def show_topic_words(self, topic_id, topn=10):</span><br><span class="line">        <span class="built_in">return</span> self.model.show_topic(topic_id, topn=topn)</span><br><span class="line"></span><br><span class="line">    def show_new_document_topic(self, documents):</span><br><span class="line">        tokenized_documents = [self.tokenizer.morphs(document) <span class="keyword">for</span> document <span class="keyword">in</span> documents]</span><br><span class="line">        curr_corpus = [self.model.id2word.doc2bow(tokenized_document) <span class="keyword">for</span> tokenized_document <span class="keyword">in</span> tokenized_documents]</span><br><span class="line">        topics = self.model.get_document_topics(curr_corpus, minimum_probability=0.5, per_word_topics=False)</span><br><span class="line">        <span class="keyword">for</span> doc_idx, topic <span class="keyword">in</span> enumerate(topics):</span><br><span class="line">            <span class="keyword">if</span> len(topic) == 1:</span><br><span class="line">                topic_id, prob = topic[0]</span><br><span class="line">                <span class="built_in">print</span>(documents[doc_idx], <span class="string">", topic id:"</span>, str(topic_id), <span class="string">", prob:"</span>, str(prob))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(documents[doc_idx], <span class="string">", there is no dominant topic"</span>)</span><br></pre></td></tr></table></figure><h4 id="모델-초기화"><a href="#모델-초기화" class="headerlink" title="모델 초기화"></a>모델 초기화</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = LDAEvaluator(<span class="string">'./lda'</span>)</span><br></pre></td></tr></table></figure><h4 id="topic-문서-확인"><a href="#topic-문서-확인" class="headerlink" title="topic 문서 확인"></a>topic 문서 확인</h4><ul><li>show_topic_docs 함수에 topic ID를 인자로 주어 실행하면 해당 topic 확률 값이 가장 높은 문서 상위 10개를 출력한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.show_topic_docs(topic_id=0)</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">'내가 가장 좋아하는 영화! 색감과 영상 인물들의 감정이입 대사 한마디 한마디가 너무나 완벽한. 몇번을 봐도 또 보고싶은 영화.'</span>,</span><br><span class="line">  0.9707017),</span><br><span class="line"> (<span class="string">'영화보다가 운적은 정우 주연 영화 바람말고는 없는데 이건 감정이입이 되서 그런지 몰라도 진짜 눈 충혈되도록 펑펑울었음'</span>,</span><br><span class="line">  0.9677608),</span><br><span class="line"> (<span class="string">'세 명품배우, 몰입도 최고의 현출,간결하고 시같은 대사,상처받은 사람들의 아름다운 치유!'</span>, 0.95923144),</span><br><span class="line"> (<span class="string">'"아.. 따듯하다.. ""천국의 아이들"" 못보신 분 꼭 보세요.. 같은 감독임.."'</span>, 0.957968),</span><br><span class="line"> (<span class="string">'몸이 마음처럼 움직여 주지 않는 지체장애우들의 혼신의 노력과 열연이 돋보이는 영화였다'</span>, 0.95778286),</span><br><span class="line"> (<span class="string">'영상미 아름답고 주인공의 사랑이 순수하고 풋풋하다. 한 번 더 보고싶은 영화!'</span>, 0.9539619),</span><br><span class="line"> (<span class="string">'음악이 아름답고 가슴이 뭉클하니 감동적이었어. 마음이 따뜻해지는 영화예요.'</span>, 0.9539556),</span><br><span class="line"> (<span class="string">'피아니스트와 같은 동급 영화라 생각합니다 보시면 후회없을거예요 실화영화이니요'</span>, 0.953952),</span><br><span class="line"> (<span class="string">'그냥 고민말고 보세요.진짜 이건 명작이라는 말로는 부족합니다..꼭 보세요!'</span>, 0.9491169),</span><br><span class="line"> (<span class="string">'영화를 보는내내 감정이 이입되고 첫사랑이 보고싶어지는 그런 영화입니다.'</span>, 0.9491167)]</span><br></pre></td></tr></table></figure><h4 id="topic-별-단어-확인"><a href="#topic-별-단어-확인" class="headerlink" title="topic 별 단어 확인"></a>topic 별 단어 확인</h4><ul><li>해당 topic ID에서 가장 높은 확률 값을 지니는 단어들 중 상위 n개의 목록을 확인할 수 있다.<ul><li><code>어미나 조사가 많이 끼어 있음을 확인할 수 있다. LDA의 품질을 끌어 올리기 위해 피처를 만드는 과정에서 명사만 쓰기도 한다.</code></li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.show_topic_words(topic_id=0)</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">'보'</span>, 0.03599964),</span><br><span class="line"> (<span class="string">'는'</span>, 0.03479155),</span><br><span class="line"> (<span class="string">'.'</span>, 0.030642439),</span><br><span class="line"> (<span class="string">'고'</span>, 0.030473521),</span><br><span class="line"> (<span class="string">'영화'</span>, 0.029688885),</span><br><span class="line"> (<span class="string">'이'</span>, 0.018665483),</span><br><span class="line"> (<span class="string">'로'</span>, 0.017465018),</span><br><span class="line"> (<span class="string">'내내'</span>, 0.016784767),</span><br><span class="line"> (<span class="string">'다'</span>, 0.016693212),</span><br><span class="line"> (<span class="string">'한'</span>, 0.013309637)]</span><br></pre></td></tr></table></figure><h4 id="새로운-문서의-topic-확인"><a href="#새로운-문서의-topic-확인" class="headerlink" title="새로운 문서의 topic 확인"></a>새로운 문서의 topic 확인</h4><ul><li>show_new_document_topic 함수는 새로운 문서의 topic을 확인하는 역할을 한다. 문서를 형태소 분석한 뒤 이를 LDA 모델에 넣어 topic을 추론해 가장 높은 확률 값을 지니는 topic id와 그 확률을 리턴해준다.</li></ul><ul><li>해당 문서의 topic 분포 중 0.5를 넘는 지배적인 topic이 존재하지 않을 경우 ‘there is no dominant topic’메시지를 리턴한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.show_new_document_topic([<span class="string">"너무 사랑스러운 영화"</span>, <span class="string">"인생을 말하는 영화"</span>])</span><br></pre></td></tr></table></figure><h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">너무 사랑스러운 영화 , topic id: 28 , prob: 0.8066608</span><br><span class="line">인생을 말하는 영화 , topic id: 9 , prob: 0.7323683</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/06/NLP_08/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP - 단어 수준 임베딩</title>
      <link>https://heung-bae-lee.github.io/2020/02/01/NLP_06/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/01/NLP_06/</guid>
      <pubDate>Sat, 01 Feb 2020 11:47:03 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;단어-수준-임베딩&quot;&gt;&lt;a href=&quot;#단어-수준-임베딩&quot; class=&quot;headerlink&quot; title=&quot;단어 수준 임베딩&quot;&gt;&lt;/a&gt;단어 수준 임베딩&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;예측 기반 모델&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NPLM&lt;/li&gt;
&lt;
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="단어-수준-임베딩"><a href="#단어-수준-임베딩" class="headerlink" title="단어 수준 임베딩"></a>단어 수준 임베딩</h1><ul><li><p>예측 기반 모델</p><ul><li>NPLM</li><li>Word2Vec</li><li>FastText</li></ul></li><li><p>행렬 분해 기반 모델</p><ul><li>LSA</li><li>GloVe</li><li>Swivel</li></ul></li><li><p>단어 임베딩을 문장 수준 임베딩으로 확장하는 방법</p><ul><li>가중 임베딩(Weighted Embedding)</li></ul></li></ul><h2 id="NPLM-Neural-Probabilistic-Language-Model"><a href="#NPLM-Neural-Probabilistic-Language-Model" class="headerlink" title="NPLM(Neural Probabilistic Language Model)"></a>NPLM(Neural Probabilistic Language Model)</h2><ul><li><p>NLP 분야에서 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델로서 임베딩 역사에서 차지하는 역할이 작지 않다.</p></li><li><p>‘단어가 어떤 순서로 쓰였는가’라는 가정을 통해 만들어진 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 만들어졌다는데 의의가 있으며 <code>NPLM 자체가 단어 임베딩 역할</code>을 수행할 수 있다. 다음과 같은 한계점들이 기존에는 존재했다.</p><ul><li><p>1) 학습 데이터에 존재하지 않는 데이터에 대해 나타날 확률을 0으로 부여 한다. 물론, 백오프(back-off)나 스무딩(smoothing)으로 완화시켜 줄 순 있지만 근본적인 해결방안은 아니였다.</p></li><li><p>2) 문장의 장기 의존성(long-term dependency)을 포착해내기 어렵다. 다시 말해서 n-gram모델의 n을 5 이상으로 길게 설정할 수 없다. <code>n이 커질수록 확률이 0이될 가능성이 높기 때문이다.</code></p></li><li><p>3) 단어/문장 간 유사도를 계산할 수 없다.</p></li></ul></li></ul><h3 id="NLPM의-학습"><a href="#NLPM의-학습" class="headerlink" title="NLPM의 학습"></a>NLPM의 학습</h3><ul><li>NLPM은 직전까지 등장한 n-1개(n: gram으로 묶을 수) 단어들로 다음 단어를 맞추는 <code>n-gram 언어 모델</code>이다.</li></ul><p><img src="/image/NPLM_principal.png" alt="NPLM의 학습 원리"></p><ul><li><p>NLPM 구조의 말단 출력</p><ul><li>$|V|$(corpus의 token vector, 어휘집합의 크기) 차원의 score 벡터 $y_{w_{t}}$에 softmax 함수를 적용한 $|V|$차원의 확률 벡터이다. <code>NPLM은 확률 벡터에서 가장 높은 요소의 인덱스에 해당하는 단어가 실제 정답 단어와 일치하도록 학습 한다.</code></li></ul></li></ul><script type="math/tex; mode=display">P(w_{t})|w_{t-1}, \cdots ,w_{t-n+1}) = \frac{exp(y_{w_{t}})}{\sum_{i} exp(y_{i}) }</script><script type="math/tex; mode=display">y_{w_{t}} \in R^{|V|} : w_{t}라는 단어에 해당하는 점수 벡터</script><ul><li><p>NLPM 구조의 입력</p><ul><li><p>문장 내 t번째 단어($w_{t}$)에 대응하는 단어 벡터 $x_{t}$를 만드는 과정은 다음과 같다. 먼저 $|V| \times m (m: x_{t}의 차원 수)$크기를 갖는 행렬 C에서 $w_{t}$에 해당하는 벡터를 참조하는 형태이다. <code>C 행렬의 원소값은 초기에 랜덤 설정</code>한다. 참조한다는 의미는 예를 들어 아래 그림에서 처럼 어휘 집합에 속한 단어가 5개이고 $w_{t}$가 4번째 인덱스를 의미한다고 하면 행렬 $C$와 $w_{t}$에 해당하는 one-hot 벡터를 내적한 것과 같다. 즉 $C$라는 행렬에서 $w_{t}$에 해당하는 행만 참조하는 것과 동일한 결과를 얻을 수 있다. 따라서 이 과정을 Look-up table이라는 용어로 많이 사용한다</p></li><li><p><code>문장 내 모든 단어들을 한 단어씩 훑으면서 Corpus 전체를 학습하게 된다면 NPLM 모델의 C 행렬에 각 단어의 문맥 정보를 내재할 수 있게 된다.</code></p></li></ul></li></ul><script type="math/tex; mode=display">x_{t} = w_{t} \cdot C = C(w_{t}), C \in R^{|v| \times m}</script><p><img src="/image/NPLM_C_matrix.png" alt="NPLM 입력 벡터"></p><h3 id="모델-구조-및-의미정보"><a href="#모델-구조-및-의미정보" class="headerlink" title="모델 구조 및 의미정보"></a>모델 구조 및 의미정보</h3><p>이 때, walking을 맞추는 과정에서 발생한 손실(train loss)를 최소화하는 그래디언트(gradient)를 받아 각 단어에 해당하는 행들이 동일하게 업데이트 된다. 따라서 이 단어들의 벡터는 벡터 공간에서 같은 방향으로 조금씩 움직인다고 볼 수 있다. <code>결과적으로 임베딩 벡터 공간에서 해당 단어들 사이의 거리가 가깝다는 것은 의미가 유사하다라는 의미로 해석</code>할 수 있다.</p><p><img src="/image/NLPM_input.png" alt="NPLM의 input 벡터"></p><p><img src="/image/NPLM_structure.png" alt="NPLM의 구조"></p><h3 id="NPLM의-특징"><a href="#NPLM의-특징" class="headerlink" title="NPLM의 특징"></a>NPLM의 특징</h3><ul><li><code>NPLM은 그 자체로 언어 모델 역할을 수행</code>할 수 있다. 기존의 통계 기반 n-gram 모델은 학습 데이터에 한 번도 등장하지 않은 패턴에 대해서는 그 등장 확률을 0으로 부여하는 문제점을 NPLM은 <code>문장이 Corpus에 없어도 문맥이 비슷한 다른 문장을 참고해 확률을 부여</code>하는 방식으로 극복하는데 큰 의의가 있다. 하지만, 학습 파라미터가 너무 많아 계산이 복잡해지고, 자연스럽게 모델 과적합(overfitting)이 발생할 가능성이 높다는 한계를 가진다.</li></ul><p>이 한계를 극복하기 위해 다음 임베딩 방법론인 Word2Vec이 등장하였다.</p><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>Word2vec은 가장 널리 쓰이고 있는 단어 임베딩 모델이다. <code>Skip-gram</code>과 <code>CBOW</code>라는 모델이 제안되었고, 이 두 모델을 근간으로 하되 negative sampling등 학습 최적화 기법을 제안한 내용이 핵심이다.</p><ul><li><p>CBOW</p><ul><li><p><code>주변에 있는 context word들을 가지고 target word 하나를 맟추는 과정에서 학습</code>된다.</p></li><li><p>입,출력 데이터 쌍</p><ul><li>{context words, target word}</li></ul></li></ul></li></ul><p><img src="/image/step_by_step_CBOW.png" alt="CBOW 모델 01"></p><p><img src="/image/step_by_step_CBOW_01.png" alt="CBOW 모델 02"></p><p><img src="/image/Projection_layer_in_CBOW.png" alt="CBOW 모델의 Projection Layer"></p><p><img src="/image/Output_layer_in_CBOW.png" alt="CBOW 모델의 Output Layer"></p><ul><li><p>Skip-gram</p><ul><li><p>처음 제안된 방식은 <code>target word 하나를 가지고 context word들이 무엇일지 예측하는 과정에서 학습</code>된다. 하지만, 이 방식은 정답 문맥 단어가 나타날 확률은 높이고 나머지 단어들 확률은 그에 맞게 낮춰야 한다. 그런데 어휘 집합에 속한 단어 수는 보통 수십만 개나되므로 이를 모두 계산하려면 비효율 적이다. 이런 점을 극복하기 위해 <code>negative sampling</code>이라는 <code>target word와 context word 쌍이 주어졌을 때 해당 쌍이 positive sample인지 negative sample인지 이진 분류하는 과정에서 학습하는 방식</code>을 제안했다. 이런다면 학습 step마다 1개의 positive sample과 나머지 k개(임의의 k:target 단어의 negative sampling 개수)만 계산하면 되므로 차원수가 2인 시그모이드를 k+1회만 계산하면된다. 이전의 매 step마다 어휘 집합 크기만큼의 차원을 갖는 softmax를 1회 계산하는 방법보다 <code>계산량이 훨씬 적다.</code> 또한 <code>Corpus에서 자주 등장하지 않는 희귀한 단어가 negative sample로 조금 더 잘 뽑힐 수 있도록 하고 자주 등장하는 단어는 학습에서 제외하는 subsampling이라는 기법을</code> 적용하였다. Skip-gram은 Corpus로 부터 엄청나게 많은 학습 데이터 쌍을 만들어 낼 수 있기 때문에 고빈도 단어의 경우 등장 횟수만큼 모두 학습시키는 것이 비효울적이라고 보았다. 이 또한, 학습량을 효과적으로 줄여 계산량을 감소시키는 전략이다.</p></li><li><p><code>작은 Corpus는 k=5~20, 큰 Corpus는 k=2~5로 하는 것이 성능이 좋다고 알려져 있다.</code></p></li><li><p><code>skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 CBOW보다 좋은 경향</code>이 있다.</p></li><li><p>입,출력 데이터 쌍</p><ul><li>{target word, target word 전전 단어}, {target word, target word 직전 단어}, {target word, target word 다음 단어}, {target word, target word 다다음 단어}</li></ul></li></ul></li></ul><p><img src="/image/skip_gram_model_in_word2vec.png" alt="Skip-gram 모델"></p><ul><li><p>negative sample Prob</p><script type="math/tex; mode=display">P_{negative}(w_{i}) =  \frac{U(w_{i})^{3/4}}{\sum^{n}_{j=0}} U(w_{i]}^{3/4})</script><script type="math/tex; mode=display">U(w_{i]}) = \frac{해당 단어 빈도}{전체 단어 수} = 해당 단어의 Unigram Prob</script></li><li><p>subsampling Prob</p><script type="math/tex; mode=display">P_{subsampling}(w_{i}) = 1 - \sqrt(\frac{t}{f(w_{i})}) = w_{i}가 학습에서 제외될 확률</script><script type="math/tex; mode=display">f(w_{i]}) = w_{i]}'s frequency, t = 0.00001</script></li><li><p>t, c가 positive sample(=target word 주변에 context word가 존재)일 확률</p><ul><li>target word와 context가 실제 positive sample이라면 아래의 조건부 확률을 최대화해야 한다. 모델의 학습 parameter는 U와 V 행렬 두개 인데, 둘의 크기는 어휘 집합 크기$(|V|) \times 임베딩 차원 수(d)$로 동일하다. <code>U와 V는 각각 target word와 context word에 대응</code>한다.</li></ul></li></ul><p><img src="/image/Skip_gram_model_parameter.png" alt="Skip-gram 모델의 파라미터"></p><script type="math/tex; mode=display">P(+|t, c) = \frac{1}{1 + exp(-u_{t}v_{c})}</script><ul><li>위의 식을 최대화 하려면 분모를 줄여야한다. 분모를 줄이려면 $exp(-u_{t}v_{c})$를 줄여야 한다. 그러려면 두 벡터의 내적값이 커지게 해야한다. 이는 <code>코사인유사도와 비례</code>함을 알 수 있다. 결론적으로 <code>두 벡터간의 유사도를 높인다는 의미</code>이다.</li></ul><p><img src="/image/exponential_function.png" alt="Exponential 함수"></p><ul><li>잘 이해가 가지 않는다면 아래과 그림을 보자. A 가 B에 정확히 포개어져 있을 때(θ=0도) cos(θ)는 1이다. 녹색선의 길이가 단위원 반지름과 일치하기 때문이다. B는 고정한 채 A가 y축 상단으로 옮겨간다(θ가 0도에서 90도로 증가)고 할때 cos(θ)는 점점 감소하여 0이 되게 됩니다. 아래 그림의 경우 빨간색 직선이 x축과 만나는 점이 바로 cos(θ)를 의미한다.</li></ul><p><img src="/image/cosine_value.png" alt="cosine함수와 벡터간의 내적과의 관계"></p><ul><li>t, c가 negative sample(target word와 context word가 무관할때)일 확률<ul><li>만약 학습데이터가 negative sample에 해당한다면 아래의 조건부 확률을 최대화하여야 한다. 이 때는 분자를 최대화 해주어야 하므로, 두 벡터의 내적값을 줄여야 한다.</li></ul></li></ul><script type="math/tex; mode=display">P(-|t, c) = 1 - P(+|t,c) = \frac{exp(-u_{t}v_{c})}{1 + exp(-u_{t}v_{c})}</script><ul><li><p>모델의 손실함수를 이제 알았으니 최대화를 하는 파라미터를 찾으려면 MLE를 구해야 할 것이다. 그렇다면 log likelihood function은 아래와 같을 것이다. 임의의 모수인 모델 파라미터인 $\theta$라고 가정 했을때, $\theta$를 한 번 업데이트할 때 1개 쌍의 positive sample과 k개의 negative sample이 학습된다는 의미이다. <code>Word2vec은 결국 두 단어벡터의 유사도 뿐만아니라 전체 Corpus 분포 정보를 단어 Embedding에 함축시키게 된다. 분포가 유사한 단어 쌍은 그 속성 또한 공유할 가능성이 높다.</code> 유사도 검사를 통해 비슷한 단어들을 출력 했을때, <code>그 단어들이 반드시 유의 관계를 보여준다기 보다는 동일한 속성을 갖는 관련성이 높은 단어를 출력한다는 의미로 이해해야한다.</code></p></li><li><p><code>모델 학습이 완료되면 U(target_word에 관한 행렬)만 d차원의 단어 임베딩으로 사용할 수도 있고, U+V.t 행렬을 임베딩으로 쓸 수도 있다. 혹은 concatenate([U, V.t])를 사용할 수도 있다.</code></p></li></ul><script type="math/tex; mode=display">L(\theta) = log P (+|t_{p},c_{p}) + \sum^{k}_{i=1} log P (-|t_{n_{i}},c_{n_{i}})</script><p><a href="https://radimrehurek.com/gensim/models/word2vec.html" target="_blank" rel="noopener">참고</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from gensim.models import word2vec</span></span><br><span class="line">from gensim.models import Word2Vec</span><br><span class="line">corpus = <span class="string">'원하는 텍스트를 단어를 기준으로 tokenize한 상태의 파일(가장 쉽게는 공백을 기준으로)'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model = word2vec.Word2Vec()</span></span><br><span class="line">model = Word2Vec(corpus,</span><br><span class="line">                 size=임베딩 특징 벡터 차원수,</span><br><span class="line">                 <span class="comment"># 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다.</span></span><br><span class="line">                 min_count=단어에 대한 최소 빈도 수,</span><br><span class="line">                 <span class="comment"># default negative=5, 보통 5~20을 많이 사용</span></span><br><span class="line">                 negative=negative sample을 뽑는 k,</span><br><span class="line">                 workers=학습시 사용하는 프로세스 개수,</span><br><span class="line">                 window=context window 크기,</span><br><span class="line">                 <span class="comment"># 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정한다.</span></span><br><span class="line">                 <span class="comment"># 유용한 범위는 (0, 1e-5)이며, 보통 0.001이 좋은 성능을 낸다고 한다.</span></span><br><span class="line">                 sample=다운 샘플링 비율,</span><br><span class="line">                 <span class="comment"># default sg=0 =&gt; CBOW, if sg=1 =&gt; skip-gram</span></span><br><span class="line">                 sg=1</span><br><span class="line">                 )</span><br><span class="line"></span><br><span class="line">model.save(<span class="string">"모델을 저장할 directory path"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 저장했던 모델을 불러와서 추가적으로 훈련시킬 수 있다.</span></span><br><span class="line">model = Word2Vec.load(<span class="string">"이미 존재하는 모델의 directory path"</span>)</span><br><span class="line">model.train([[<span class="string">"hello"</span>, <span class="string">"world"</span>]], total_examples=1, epochs=1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 훈련된 벡터를 KeyedVector로 분리하는 이유는 전체 모델 상태가 더 이상 필요하지 않을 경우(훈련을 계속할 필요가 없음)</span></span><br><span class="line"><span class="comment"># 모델이 폐기될 수 있기 때문에 프로세스 간에 RAM의 벡터를 빠르게 로드하고 공유할 수 있는 훨씬 작고 빠른 상태로 만드는 것이다.</span></span><br><span class="line">vector = model.wv[<span class="string">'computer'</span>]</span><br><span class="line"></span><br><span class="line">from gensim.models import KeyedVectors</span><br><span class="line"></span><br><span class="line">path = get_tmpfile(<span class="string">"wordvectors 파일명"</span>)</span><br><span class="line"></span><br><span class="line">model.wv.save(path)</span><br><span class="line">wv = KeyedVectors.load(<span class="string">"model.wv"</span>, mmap=<span class="string">'r'</span>)</span><br><span class="line">vector = wv[<span class="string">'computer'</span>]</span><br></pre></td></tr></table></figure></p><ul><li>학습이 완료된 임베딩 결과물을 활요하여 코사인 유사도가 가장 높은 단어들을 뽑아 임베딩을 평가해 볼 수도 있다. 이는 추후에 한번에 소개할 것이다.</li></ul><h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><ul><li><p>Facebook에서 개발해 공개한 <code>단어 임베딩 기법</code>이다. <code>각 단어를 문자단위 n-gram으로 표현한다. 이외의 점은 모두 Word2Vec과 같다.</code> 동일하게 negative sampling을 사용하며, 조금 다른 점은 <code>Fasttext는 target word(t), context word(c) 쌍을 학습할 때 target word(t)에 속한 문자 단위 n-gram 벡터(z)들을 모두 업데이트 한다는 점이다.</code></p></li><li><p>설치 방법은 gensim에서 FastText를 제공하고 있기에 pip를 통해 설치해주거나 이 방법이 안된다면, <a href="https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/07/06/fasttext/" target="_blank" rel="noopener">참조페이지</a>를 클릭해서 직접 C++방식으로 받아도 상관없다.</p></li></ul><h4 id="모델-기본-구조"><a href="#모델-기본-구조" class="headerlink" title="모델 기본 구조"></a>모델 기본 구조</h4><ul><li><p>예를 들어 시나브로라는 단어의 문자 단위 3-gram은 다음과 같이 n-gram 벡터의 합으로 표현한다. 아래 식에서 $G_{t}$는 target word t에 속한 문자 단위 n-gram집합을 의미한다.</p></li><li><p>Fasttext의 단어 벡터 표현(&lt;,&gt;는 단어의 경계를 나타내 주기 위해 모델이 사용하는 기호)</p><script type="math/tex; mode=display">u_{시나브로} = z_{<시나} + z_{시나브} + z_{나브로} + z_{브로>} + z_{시나브로}, u_{t}=\sum_{g \in G_{t}} z_{g}</script></li></ul><p><img src="/image/FastText_embedding_little.png" alt="FastText를 통한 임베딩"></p><ul><li><p><a href="https://wikidocs.net/21692" target="_blank" rel="noopener">n-gram 참조 및 NLP에 도움이 되는 사이트</a></p><ul><li>n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어진다. 그렇기 때문에 적절한 n을 선택해야 한다. <code>trade-off 문제로 인해 정확도를 높이려면 n은 최대 5를 넘게 잡아서는 안 된다고 권장되고 있다.</code></li></ul></li><li><p>손실함수 자체는 위의 식을 word2vec 손실함수 $u_{t}$에 대입해 주기만 하면된다.</p></li><li><p><code>FastText 모델의 강점은 조사나 어미가 발달한 한국어에 좋은 성능을 낼 수 있다는 점</code>이다. 용언(동사, 형용사)의 활용이나 그와 관계된 어미들이 벡터 공간상 가깝게 임베딩 되기 때문이다.(예를들면, ‘하였다’가 t이고, ‘공부’가 c라면 ‘공부’와 ‘했(다), 하(다), 하(였으며)’등에 해당하는 벡터도 비슷한 공간상에 있다는 의미이다.) <code>한글은 자소 단위(초성, 중성, 종성)로 분해할 수 있고, 이 자소 각각을 하나의 문자로 보고 FastText를 실행할 수 있다는 점도 강점</code>이다.</p></li><li><p><code>또한, 각 단어의 임베딩을 문자 단위 n-gram 벡터의 합으로 표현하기 때문에 오타나 미등록단어(unknown word)에도 robust하다. 그래서 미등록된 단어도 벡터를 뽑아낼수 있다.</code> 동일한 음절이나 단어를 가진 공간상의 벡터를 추출할 수 있기 때문이다. <code>다른 단어 임베딩 기법이 미등록 단어 벡터를 아예 추출할 수 없다는 사실을 감안하면 FastText는 경쟁력이 있다.</code></p></li></ul><p><a href="https://radimrehurek.com/gensim/models/fasttext.html" target="_blank" rel="noopener">Fasttext 참조</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import FastText</span><br><span class="line"></span><br><span class="line">corpus = <span class="string">'원하는 텍스트를 단어를 기준으로 tokenize한 상태의 파일(가장 쉽게는 공백을 기준으로)'</span></span><br><span class="line"></span><br><span class="line">model = Word2Vec(corpus,</span><br><span class="line">                 size=임베딩 특징 벡터 차원수,</span><br><span class="line">                 <span class="comment"># 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다.</span></span><br><span class="line">                 min_count=단어에 대한 최소 빈도 수,</span><br><span class="line">                 <span class="comment"># default negative=5, 보통 5~20을 많이 사용</span></span><br><span class="line">                 negative=negative sample을 뽑는 k,</span><br><span class="line">                 workers=학습시 사용하는 프로세스 개수,</span><br><span class="line">                 window=context window 크기,</span><br><span class="line">                 <span class="comment"># 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정한다.</span></span><br><span class="line">                 <span class="comment"># 유용한 범위는 (0, 1e-5)이며, 보통 0.001이 좋은 성능을 낸다고 한다.</span></span><br><span class="line">                 sample=다운 샘플링 비율,</span><br><span class="line">                 <span class="comment"># default sg=0 =&gt; CBOW, if sg=1 =&gt; skip-gram</span></span><br><span class="line">                 sg=1,</span><br><span class="line">                 <span class="comment"># default word_ngrams=1 =&gt; n-gram 사용, 0 =&gt; 미사용(word2vec과 동일)</span></span><br><span class="line">                 word_ngrams=1,</span><br><span class="line">                 <span class="comment"># n-gram 최소 단위</span></span><br><span class="line">                 min_n=3,</span><br><span class="line">                 <span class="comment"># n-gram 최대 단위 (최소단위보단 커야한다.)</span></span><br><span class="line">                 max_n=6,</span><br><span class="line">                 )</span><br></pre></td></tr></table></figure><h2 id="잠재-의미-분석-LSA-Latent-Semantic-Analysis"><a href="#잠재-의미-분석-LSA-Latent-Semantic-Analysis" class="headerlink" title="잠재 의미 분석(LSA, Latent Semantic Analysis)"></a>잠재 의미 분석(LSA, Latent Semantic Analysis)</h2><ul><li><p>word-document 행렬이나 TF-IDF 행렬, word-context 행렬 같은 <code>커다란 행렬에 차원 축소 방법의 일종인 특이값 분해를 수행해 데이터의 차원 수를 줄여 계산 효율성을 키우는 한편 행간에 숨어있는 잠재 의미를 추출해내는 방법론</code>이다.</p></li><li><p>예를 들면, word-documents 행렬이나 word-context 행렬 등에 SVD를 한 다음 그 결과로 도출되는 행벡터들을 단어 임베딩으로 사용할 수 있다. <code>잠재 의미 분석은 GloVe나 Swivel과 더불어 Matrix Factorization 기반의 기법으로 분류</code>된다.</p></li></ul><h4 id="PPMI-점별-상호-정보량-행렬"><a href="#PPMI-점별-상호-정보량-행렬" class="headerlink" title="PPMI(점별 상호 정보량) 행렬"></a>PPMI(점별 상호 정보량) 행렬</h4><ul><li><p>word-document 행렬, TF-IDF 행렬, word-context 행렬, PMI 행렬에 모두 LSA를 수행할 수 있다. 이 중 PMI 행렬을 보완하는 PPMI 행렬에 대해 소개하고자한다. <a href="https://heung-bae-lee.github.io/2020/01/16/NLP_01/">PMI 행렬과 위의 행렬들을 모른다면 클릭!</a></p></li><li><p>PPMI란 간단히 말해 우리가 가진 말뭉치의 크기가 충분히 크지 않다면, PMI식의 로그 안 분자가 분모보다 작을 때 음수가 되거나, 극단적으로 단어 A,B가 단 한번도 같이 등장하지 않는다면 $-inf$값을 갖게 된다. 이러한 이유로 <code>NLP 분야에서는 PMI 대신 PPMI(Positive Pointwise Mutual Information)를 지표로 사용한다.</code> PMI가 양수가 아닌 경우 그 값을 신뢰하기 힘들어 0으로 치환해 무시한다는 뜻이다.</p></li></ul><script type="math/tex; mode=display">PPMI(A, B) = max(PMI(A,B), 0)</script><ul><li><code>Shifted PMI(SPMI)는 Word2Vec과 깊은 연관이 있다는 논문이 발표되기도 했다.</code></li></ul><script type="math/tex; mode=display">SPMI(A, B) = PMI(A, B) - log k, k > 0</script><h4 id="행렬-분해로-이해하는-잠재-의미-분석"><a href="#행렬-분해로-이해하는-잠재-의미-분석" class="headerlink" title="행렬 분해로 이해하는 잠재 의미 분석"></a>행렬 분해로 이해하는 잠재 의미 분석</h4><ul><li>Eigenvalue Decomposition(고유값 분해)를 우선 알고 있다는 전제조건으로 SVD를 모르실수도 있는 분들을 위해 간략히 설명하자면, 고유값 분해는 행렬 A가 정방행렬일 경우만 가능한데, 만약 정방행렬이 아닌 행렬은 고유값 분해를 어떻게 해야 하는지에 대한 개념이라고 말할 수 있겠다. 혹시 고유값 분해도 잘 모르시겠다면 <a href="https://heung-bae-lee.github.io/2019/12/14/linear_algebra_00/">이곳</a>을 클릭해서 필자가 추천하는 강의들을 꼭 공부해 보시길 추천한다. 필자는 개인적으로 선형대수는 Computer Science(or 데이터 분석)를 하는데 기본적으로 어느 정도 알고 있어야 한다고 생각한다.</li></ul><p><a href="https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/" target="_blank" rel="noopener">참조</a></p><p><img src="/image/SVD_NLP.png" alt="특이값 분해 - SVD"></p><ul><li><p>예를 들어, 행렬 A의 m개의 word, n개 documents로 이루어져 shape이 $ m \times n $인 word-documents 행렬에 truncated SVD를 하여 LSA를 수행한다고 가정해본다. 그렇다면 <code>U는 단어 임베딩, V.t는 문서 임베딩에 대응</code>한다. 마찬가지로 <code>m개 단어, m개 단어로 이루어진 PMI 행렬에 LSA를 하면 d차원 크기의 단어 임베딩을 얻을 수 있다</code>.</p></li><li><p><code>각종 연구들에 따르면 LSA를 적용하면 단어와 문맥 간의 내재적인 의미를 효과적으로 보존할 수 있게 돼 결과적으로 문서 간 유사도 측정 등 모델의 성능 향상에 도움을 줄 수 있다고 한다. 또한 입력 데이터의 노이즈, sparsity(희소)를 줄일 수 있다.</code></p></li></ul><p><img src="/image/Truncated_SVD_NLP.png" alt="Truncated SVD"></p><h4 id="행렬-분해로-이해하는-Word2Vec"><a href="#행렬-분해로-이해하는-Word2Vec" class="headerlink" title="행렬 분해로 이해하는 Word2Vec"></a>행렬 분해로 이해하는 Word2Vec</h4><ul><li><code>negative sampling 기법으로 학습된 Word2Vec의 Skip-gram 모델(SGNS, Skip-Gram with Negative Sampling)은 Shifted PMI 행렬을 분해한 것과 같다는 것을 볼 수 있다.</code></li></ul><p><img src="/image/word2vec_whith_aspect_of_matrix_decomposition.png" alt="행렬 분해 관점에서 이해하는 word2vec"></p><ul><li>$A_{ij}$는 SPMI행렬의 i,j번째 원소이다. k는 Skip-gram 모델의 negative sample 수를 의미한다. 그러므로 k=1인 negative sample 수가 1개인 Skip-gram 모델은 PMI 행렬을 분해하는 것과 같다.</li></ul><script type="math/tex; mode=display">A^{SGNS}_{ij} = U_{i} \cdot V_{j} = PMI(i,j) - log k</script><ul><li>soynlp에서 제공하는 sent_to_word_contexts_matrix 함수를 활용하면 word-context 행렬을 구축할 수 있다. <code>dynamic_weight=True는 target word에서 멀어질수록 카운트하는 동시 등장 점수(co-occurrence score)를 조금씩 깎는다는 의미</code>이다. dynamic_weight=False라면 window 내에 포함된 context word들의 동시 등장 점수는 target word와의 거리와 관계 없이 모두 1로 계산한다. 예를 들어서 window=3이고 ‘도대체 언제쯤이면 데이터 사이언스 분야를 조금은 공부했다고 말할 수 있을까…’라는 문장의 target word가 ‘분야’라면, ‘를’과 ‘사이언스’의 동시 등장 점수는 1, ‘데이터’, ‘조금’은 0.66, ‘은’, ‘이면’은 0.33이 된다.    </li></ul><ul><li><p>word-context 행렬을 활용한 LSA</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import TruncatedSVD</span><br><span class="line">from soynlp.vectorizer import sent_to_word_contexts_matrix</span><br><span class="line"></span><br><span class="line">corpus_file</span><br><span class="line"></span><br><span class="line">corpus = [sent.replace(<span class="string">'\n'</span>, <span class="string">''</span>).strip() <span class="keyword">for</span> sent <span class="keyword">in</span> open(corpus_file, <span class="string">'r'</span>).readlines()]</span><br><span class="line"></span><br><span class="line">input_matrix, idx2vocab = sent_to_word_contexts_matrix(corpus,</span><br><span class="line">                                                       window=3,</span><br><span class="line">                                                       <span class="comment"># 최소 단어 빈도 수</span></span><br><span class="line">                                                       min_tf=10,</span><br><span class="line">                                                       dynamic_weight=True,</span><br><span class="line">                                                       verbose=True)</span><br><span class="line"></span><br><span class="line">cooc_svd = TruncatedSVD(n_components=100)</span><br><span class="line">cooc_vecs = cooc_svd.fit_transform(input_matrix)</span><br></pre></td></tr></table></figure></li><li><p>구축한 word-context 행렬에 soynlp에서 제공하는 pmi 함수를 적용한다. min_pmi 보다 낮은 PMI 값은 0으로 치환한다. 따라서 <code>min_pmi=0으로 설정하면 정확히 PPMI와 같다.</code> 또한, pmi matrix의 차원수는 어휘 수 x 어휘 수의 정방 행렬이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from soynlp import pmi</span><br><span class="line">ppmi_matrix, _, _ = pmi(input_matrix, min_pmi=0)</span><br><span class="line">ppmi_svd = TruncatedSVD(n_components=100)</span><br><span class="line">ppmi_vecs = ppmi_svd.fit_transform(input_matrix)</span><br></pre></td></tr></table></figure><h4 id="GloVe-Global-Word-Vectors"><a href="#GloVe-Global-Word-Vectors" class="headerlink" title="GloVe(Global Word Vectors)"></a>GloVe(Global Word Vectors)</h4><ul><li>미국 스탠포트대학교연구팀에서 개발한 단어 임베딩 기법이다. 임베딩된 단어 벡터 간 유사도 측정을 수월하게 하면서도 Corpus 전체의 통계 정보를 좀 더 잘 반영하는 것을 지향하여 <code>Vanilla Word2Vec과 LSA 두 기법의 단점을 극복하고자 했다.</code> LSA(잠재 의미 분석)은 Corpus 전체의 통계량을 모두 활용할 수 있지만, 그 결과물로 단어 간 유사도를 측정하기는 어렵다. 반대로 Vanilla Word2Vec은 단어 벡터 사이의 유사도를 측정하는 데는 LSA보다 유리하지만 사용자가 지정한 window 내의 local context만 학습하기 때문에 Corpus 전체의 통계 정보는 반영되기 어렵다는 단점을 지닌다. <code>물론 GloVe 이후 발표된 Skip-gram 모델이 Corpus 전체의 Global한 통계량인 SPMI 행렬을 분해하는 것과 동치라는 점을 증명하기는 했다.</code></li></ul><p><img src="/image/GloVe_model.png" alt="그림으로 이해하는 GloVe"></p><ul><li><p>손실 함수</p><ul><li><p>임베딩된 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 증장 빈도의 로그 값이 되도록 정의했다.</p></li><li><p>단어 i,j 각각에 해당하는 벡터 $U_{i}$, $V_{j}$ 사이의 내적값과 두 단어 동시 등장 빈도 $A_{ij}$의 로그값 사이의 차이가 최소화될수록 학습 손실이 작아진다. bias항 2개와 f(A_{ij})는 임베딩 품질을 높이기 위해 고안된 장치이다.</p></li><li><p>Glove는 word-context 행렬 A를 만든 후에 학습이 끝나면 <code>U를 단어 임베딩으로 사용하거나 U+V.t, concatenate([U, V.t])를 임베딩으로 사용할 수 있다.</code></p></li></ul></li></ul><script type="math/tex; mode=display">J = \sum^{|V|}_{i,j=1} f(A_{ij}) (U_{i} \cdot V_{j} + b_{i} + b+{j} - log A_{ij})^{2}</script><h2 id="Swivel"><a href="#Swivel" class="headerlink" title="Swivel"></a>Swivel</h2><ul><li><p>Google 연구팀이 발표한 행렬 분해 기반의 단어 임베딩 기법이다. PMI 행렬을 U와 V로 분해하고, 학습이 종료되면 <code>U를 단어 임베딩으로 쓸 수 있으며 U+V.t, concatenate([U, V.t])도 임베딩으로 사용할 수 있다.</code></p></li><li><p><code>PMI 행렬을 분해한다는 점에서 word-context 행렬을 분해하는 GloVe와 다르며, Swivel은 목적함수를 PMI의 단점을 보완할 수 있도록 설계했다.</code> 두 단어가 한번도 동시에 등장하지 않았을 경우 PMI가 -inf로 가능 현상을 보완하기 위해 이런경우의 손실함수를 따로 정의했다. 그 결과, <code>i,j가 각각 고빈도 단어인데 두 단어의 동시 등장빈도가 0이라면 두 단어는 정말로 등장하지 않는 의미상 무관계한 단어라고 가정하고, 단어 i,j가 저빈도 단어인데 두 단어의 동시 등장빈도가 0인 경우에는 두 단어는 의미상 관계가 일부 있을 수 있다고 가정한다.</code></p></li></ul><p><img src="/image/Swivel_model.png" alt="그림으로 이해하는 Swivel"></p><h2 id="단어-임베딩-평가-방법"><a href="#단어-임베딩-평가-방법" class="headerlink" title="단어 임베딩 평가 방법"></a>단어 임베딩 평가 방법</h2><ul><li><p>참고로 카카오브레인 박규병 님께서는 한국어, 일본어, 중국어 등 30개 언어의 단어 임베딩을 학습해 공개했다. 모델은 주로 해당 언어의 위키백과 등으로 학습됐으며 벡터 차원 수는 100, 3000차원 두 종류가 있다.</p><ul><li><a href="https://github.com/Kyubyong/wordvectors" target="_blank" rel="noopener">https://github.com/Kyubyong/wordvectors</a></li></ul></li></ul><h4 id="단어-유사도-평가-word-similarity-test"><a href="#단어-유사도-평가-word-similarity-test" class="headerlink" title="단어 유사도 평가(word similarity test)"></a>단어 유사도 평가(word similarity test)</h4><ul><li><p>일련의 단어 쌍을 미리 구성한 후에 사람이 평가한 점수와 단어 벡터 간 코사인 유사도 사이의 상관관계를 계산해 단어 임베딩의 품질을 평가하는 방법이다.</p></li><li><p><code>Word2Vec과 FastText 같은 예측 기반 임베딩 기법들이 GloVe, Swivel 등 행렬 분해 방법들에 비해 상관관계가 상대적으로 강한 것을 알 수 있다. 물론 무조건 예측기반이 좋다는 의미는 아니다. 데이터에 다르겠지만 보통은 저런 결과를 얻을 것이다.</code></p></li></ul><h4 id="단어-유추-평가-word-analogy-test"><a href="#단어-유추-평가-word-analogy-test" class="headerlink" title="단어 유추 평가(word analogy test)"></a>단어 유추 평가(word analogy test)</h4><ul><li>의미론적 유추에서 단어 벡터 간 계산을 통해 <code>갑 - 을  + 병 = 정</code>을 통해 평가하는 방법이다. <code>갑 - 을 + 병</code>에 해당하는 벡터에 대해 코사인 유사도가 가장 높은 벡터에 해당하는 단어가 실제 <code>정</code>인지를 확인한다.</li></ul><h4 id="단어-임베딩-시각화"><a href="#단어-임베딩-시각화" class="headerlink" title="단어 임베딩 시각화"></a>단어 임베딩 시각화</h4><ul><li>시각화 또한 단어 임베딩을 평가하는 방법이다. 다만 단어 임베딩은 보통 고차원 벡터이기 때문에 사람이 인식하는 2, 3차원으로 축소해 시각화를 하게 된다. <code>t-SNE(t-Stochastic Neighbor Embedding)은 고차원의 원공간에 존재하는 벡터 x의 이웃 간의 거리를 최대한 보존하는 저차원 벡터 y를 학습하는 방법론</code>이다. 원 공간의 데이터 확률 분포와 축소된 공간의 분포 사이의 차이를 최소화하는 방향으로 벡터 공간을 업데이트한다.</li></ul><h2 id="가중-임베딩"><a href="#가중-임베딩" class="headerlink" title="가중 임베딩"></a>가중 임베딩</h2><ul><li><code>단어 임베딩을 문장 수준 임베딩으로 확장하는 방법을 설명</code>하겠다. <code>아주 간단한 방법이지만 성능 효과가 좋아서 사용해볼만한 방법</code>이다. 미국 프린스턴 대학교 연구팀이 ICLR에 발표한 방법론이다.</li></ul><h4 id="모델-개요"><a href="#모델-개요" class="headerlink" title="모델 개요"></a>모델 개요</h4><ul><li><p>Arora et al.(2016)은 <code>문서 내 단어의 등장은 저자가 생각한 주제에 의존한다고 가정</code>했다. 이를 위해 <code>주제 벡터(discourse vector)</code>라는 개념을 도입했다. 주제 벡터 $c_{s}$가 주어졌을 때 어떤 단어 w가 나타날 확률을 아래와 같이 정의했다. $\tilde{c_{s}}$는 $c_{s}$로 부터 도출되는데 그 과정은 생략하고, 간단히 말하면 주제 벡터 c_{s}와 거의 비슷한 역할을 하는 임의의 어떤 벡터라고 보겠다. Z는 우변 두번째 항이 확률 값이 되도록 해주는 Normalize Factor이다.</p></li><li><p><code>우변의 첫째항은 단어 w가 주제와 상관없이 등장할 확률</code>이며, 한국어에서는 조사(은,는,이,가 등)가 P(w)가 높은 축에 속한다. <code>두 번째 항은 단어 w가 주제와 관련을 가질 확률을 의미</code>한다. 주제 벡터 $\tilde{c_{s}}$와 w에 해당하는 단어 벡터 $v_{w}$가 내적값이 클수록 그 값이 커진다. $\alpha$는 사용자가 지정하는 hyper parameter이다.</p></li><li><p>단어 등장 확률</p><script type="math/tex; mode=display">P(w|c_{s}) = \alpha P(w) + (1-\alpha) frac{ exp( \tilde{c_{s}} \cdot v_{w}) }{Z}</script></li><li><p>단어 sequence는 문장이다. <code>문장 등장 확률(단어들이 동시에 등장할 확률)은 문장에 속한 모든 단어들이 등장할 확률의 누적 곱으로 나타낼 수 있다. 그런데 확률을 누적해서 곱하면 너무 작아지는 underflow 문제가 발생하므로 로그를 취해 덧셈을 하는 것으로 대체한다.</code></p></li><li><p>문장 등장확률</p><script type="math/tex; mode=display">P(s|c_{s}) \propto \sum_{w \in s} log P(w|c_{s}) = \sum_{w \in s} f_{w}(\tilde{c_{s}})</script></li><li><p>단어 등장 확률의 Taylor Series approximation</p><script type="math/tex; mode=display">f_{w}(\tilde{c_{s}}) \approx f_{w}(0) + \triangledown f_{w}(0)^{T} \tilde{c_{s}}</script></li></ul><script type="math/tex; mode=display">= constant + frac{ (1-\alpha) / \alpha Z }{ P(w) + (1-\alpha) / \alpha Z } \tilde{c_{s}} \cdot v_{w}</script><ul><li><p>우리가 관찰하고 있는 단어 w가 등장할 확률을 최대화하는 주제벡터 $ c_{s} / \tilde{c_{s}} $를 찾는 것이 목표이다. w가 등장할 확률을 최대화하는 $ c_{s} / \tilde{c_{s}} $를 찾게 된다면 이 $ c_{s} / \tilde{c_{s}} $ 는 <code>해당 단어의 사용을 제일 잘 설명하는 주제 벡터가 될 것이다.</code></p></li><li><p>직관적으로 말하자면, 우리가 관찰하고 있는 문장이 등장할 확률을 최대화하는 주제 벡터 $ c_{s} / \tilde{c_{s}} $는 문장에 속한 단어들에 해당하는 단어 벡터에 가중치를 곱해 만든 새로운 벡터들의 합에 비례한다. <code>희귀한 단어라면 높은 가중치를 곱해 해당 단어 벡터의 크기를 키우고, 고빈도 단어라면 해당 벡터의 크기를 줄니다. 이는 정보성이 높은, 희귀한 단어에 가중치를 높게 주는 TF-IDF의 철학과도 맞닿아 있는 부분이다. 또한 문장 내 단어의 등장 순서를 고려하지 않는다는 점에서 Bag of Words 가정과도 연결된다.</code></p></li></ul><h4 id="모델-구현"><a href="#모델-구현" class="headerlink" title="모델 구현"></a>모델 구현</h4><ul><li><p>문장을 Token으로 나눈 뒤 해당 Token들에 대응하는 벡터들의 합으로 문장의 임베딩을 구한다. 예측은 테스트 문장이 들어오면 Token 벡터의 합으로 만들고, 이 벡터와 코사인 유사도가 가장 높은 학습 데이터 문장의 임베딩을 찾는다. 이후 해당 학습 데이터 문장에 달려 있는 레이블을 리턴하는 방식이다.</p></li><li><p>예를들어, ‘영화 정말 재밌다.’가 테스트 문장이고, 이 문장과 유사한 학습 데이터 임베딩이 ‘영화가 진짜 재미지네요.+긍정’이라면, 테스트 문장을 긍정이라고 예측한다는 것이다.</p></li><li><p>또한, 과연 어느정도의 효과가 있는지 비교하기위해 대조군으로 일반적인 합을 통한 임베딩 방식도 수행해볼것이다.</p></li></ul><h2 id="Weighted-Sum을-이용한-Documents-Classification-Model"><a href="#Weighted-Sum을-이용한-Documents-Classification-Model" class="headerlink" title="Weighted Sum을 이용한 Documents Classification Model"></a>Weighted Sum을 이용한 Documents Classification Model</h2><ul><li><p>참고로 해당 모델을 수행하려면 먼저 형태소 분석이 완료된 Corpus file과 Corpus를 통해 만들어진 Embedding File이 존재해야한다.</p></li><li><p>먼저 tokenizer를 선택해서 사용할 수 있도록  각 Tokenizer에 따른 객체를 생성해주는 함수를 만들어준다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from khaiii import KhaiiiApi</span><br><span class="line">from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_tokenizer(tokenizer_name):</span><br><span class="line">    <span class="keyword">if</span> tokenizer_name == <span class="string">"komoran"</span>:</span><br><span class="line">        tokenizer = Komoran()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"okt"</span>:</span><br><span class="line">        tokenizer = Okt()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"mecab"</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"hannanum"</span>:</span><br><span class="line">        tokenizer = Hannanum()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"kkma"</span>:</span><br><span class="line">        tokenizer = Kkma()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"khaiii"</span>:</span><br><span class="line">        tokenizer = KhaiiiApi()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="built_in">return</span> tokenizer</span><br></pre></td></tr></table></figure><ul><li>모델을 저장할 path가 존재하지 않는다면 directory를 만들어주는 함수를 만들어준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">def make_save_path(full_path):</span><br><span class="line">    <span class="keyword">if</span> full_path[:4] == <span class="string">"data"</span>:</span><br><span class="line">        full_path = os.path.join(os.path.abspath(<span class="string">"."</span>), full_path)</span><br><span class="line">    model_path = <span class="string">'/'</span>.join(full_path.split(<span class="string">"/"</span>)[:-1])</span><br><span class="line">    <span class="keyword">if</span> not os.path.exists(model_path):</span><br><span class="line">        os.makedirs(model_path)</span><br></pre></td></tr></table></figure><ul><li>아래 embedding_method의 default값은 fasttext이지만 실제로 필자가 실행시에는 word2vec을 사용할 것이다.</li><li><p>defaultdict은 말 그대로 처음에 값을 지정해주지 않으면 default값을 넣어준다는 의미이다.</p></li><li><p>비교 할 사항</p><ul><li>embedding method : <code>fasttext</code> vs <code>word2vec</code></li><li>sum method : <code>weighted sum</code> vs <code>sum</code></li><li>average or nor : <code>average</code> vs <code>not average</code></li></ul></li><li><p>이 글에서는 2번째 항목의 비교한 결과만을 보여 줄 것이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line">from collections import defaultdict</span><br><span class="line">from gensim.models import Word2Vec</span><br><span class="line"></span><br><span class="line">class CBoWModel(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, train_fname, embedding_fname, model_fname, embedding_corpus_fname,</span><br><span class="line">                 embedding_method=<span class="string">"fasttext"</span>, is_weighted=None, average=False, dim=100, tokenizer_name=<span class="string">"mecab"</span>):</span><br><span class="line">        <span class="comment"># configurations</span></span><br><span class="line">        make_save_path(model_fname)</span><br><span class="line">        self.dim = dim</span><br><span class="line">        <span class="comment"># 평균을 내줄것인지 아니면 합만을 사용할 것인지에 대한 옵션이다.</span></span><br><span class="line">        self.average = average</span><br><span class="line">        <span class="keyword">if</span> is_weighted:</span><br><span class="line">            model_full_fname = model_fname + <span class="string">"-weighted"</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            model_full_fname = model_fname + <span class="string">"-original"</span></span><br><span class="line">        self.tokenizer = get_tokenizer(tokenizer_name)</span><br><span class="line">        <span class="keyword">if</span> is_weighted:</span><br><span class="line">            <span class="comment"># ready for weighted embeddings</span></span><br><span class="line">            <span class="comment"># dictionary 형태로 이루어져있다. (embedding["word"]=embedding_vaector)</span></span><br><span class="line">            self.embeddings = self.load_or_construct_weighted_embedding(embedding_fname, embedding_method, embedding_corpus_fname)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"loading weighted embeddings, complete!"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># ready for original embeddings</span></span><br><span class="line">            words, vectors = self.load_word_embeddings(embedding_fname, embedding_method)</span><br><span class="line">            self.embeddings = defaultdict(list)</span><br><span class="line">            <span class="keyword">for</span> word, vector <span class="keyword">in</span> zip(words, vectors):</span><br><span class="line">                self.embeddings[word] = vector</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"loading original embeddings, complete!"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 모델이 존재하지 않는다면 새롭게 훈련시키고 존재한다면 load해 온다.</span></span><br><span class="line">        <span class="keyword">if</span> not os.path.exists(model_full_fname):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"train Continuous Bag of Words model"</span>)</span><br><span class="line">            self.model = self.train_model(train_fname, model_full_fname)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"load Continuous Bag of Words model"</span>)</span><br><span class="line">            self.model = self.load_model(model_full_fname)</span><br><span class="line"></span><br><span class="line">    def evaluate(self, test_data_fname, batch_size=3000, verbose=False):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"evaluation start!"</span>)</span><br><span class="line">        test_data = self.load_or_tokenize_corpus(test_data_fname)</span><br><span class="line">        data_size = len(test_data)</span><br><span class="line">        num_batches = int((data_size - 1) / batch_size) + 1</span><br><span class="line">        eval_score = 0</span><br><span class="line">        <span class="keyword">for</span> batch_num <span class="keyword">in</span> range(num_batches):</span><br><span class="line">            batch_sentences = []</span><br><span class="line">            batch_tokenized_sentences = []</span><br><span class="line">            batch_labels = []</span><br><span class="line">            start_index = batch_num * batch_size</span><br><span class="line">            end_index = min((batch_num + 1) * batch_size, data_size)</span><br><span class="line">            features = test_data[start_index:end_index]</span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">                sentence, tokens, label = feature</span><br><span class="line">                batch_sentences.append(sentence)</span><br><span class="line">                batch_tokenized_sentences.append(tokens)</span><br><span class="line">                batch_labels.append(label)</span><br><span class="line">            preds, curr_eval_score = self.predict_by_batch(batch_tokenized_sentences, batch_labels)</span><br><span class="line">            eval_score += curr_eval_score</span><br><span class="line">        <span class="keyword">if</span> verbose:</span><br><span class="line">            <span class="keyword">for</span> sentence, pred, label <span class="keyword">in</span> zip(batch_sentences, preds, batch_labels):</span><br><span class="line">                <span class="built_in">print</span>(sentence, <span class="string">", pred:"</span>, pred, <span class="string">", label:"</span>, label)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"number of correct:"</span>, str(eval_score), <span class="string">", total:"</span>, str(len(test_data)), <span class="string">", score:"</span>, str(eval_score / len(test_data)))</span><br><span class="line"></span><br><span class="line">    def predict(self, sentence):</span><br><span class="line">        <span class="comment"># 문장을 예측을 하기 위해서는 우선 형태소를 분석을 해야한다.</span></span><br><span class="line">        tokens = self.tokenizer.morphs(sentence)</span><br><span class="line">        <span class="comment"># 문장의 형태소들을 임베딩 벡터와 같은 크기의 영벡터를 만든후 계속해서 더해주는 방식으로 문장 임베딩 벡터를 생성한다.</span></span><br><span class="line">        <span class="comment"># 만약 average=True했다면,</span></span><br><span class="line">        sentence_vector = self.get_sentence_vector(tokens)</span><br><span class="line">        <span class="comment"># 모델의 문장 임베딩 벡터와 sentence 문장 벡터와의 내적으로 유사도를 측정한다.</span></span><br><span class="line">        scores = np.dot(self.model[<span class="string">"vectors"</span>], sentence_vector)</span><br><span class="line">        <span class="comment"># 제일높은 유사도를 지닌 라벨을 출력해준다.</span></span><br><span class="line">        pred = self.model[<span class="string">"labels"</span>][np.argmax(scores)]</span><br><span class="line">        <span class="built_in">return</span> pred</span><br><span class="line"></span><br><span class="line">    def predict_by_batch(self, tokenized_sentences, labels):</span><br><span class="line">        sentence_vectors, eval_score = [], 0</span><br><span class="line">        <span class="keyword">for</span> tokens <span class="keyword">in</span> tokenized_sentences:</span><br><span class="line">            sentence_vectors.append(self.get_sentence_vector(tokens))</span><br><span class="line">        scores = np.dot(self.model[<span class="string">"vectors"</span>], np.array(sentence_vectors).T)</span><br><span class="line">        preds = np.argmax(scores, axis=0)</span><br><span class="line">        <span class="keyword">for</span> pred, label <span class="keyword">in</span> zip(preds, labels):</span><br><span class="line">            <span class="keyword">if</span> self.model[<span class="string">"labels"</span>][pred] == label:</span><br><span class="line">                eval_score += 1</span><br><span class="line">        <span class="built_in">return</span> preds, eval_score</span><br><span class="line"></span><br><span class="line">    def get_sentence_vector(self, tokens):</span><br><span class="line">        vector = np.zeros(self.dim)</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> self.embeddings.keys():</span><br><span class="line">                vector += self.embeddings[token]</span><br><span class="line">        <span class="keyword">if</span> self.average:</span><br><span class="line">            vector /= len(tokens)</span><br><span class="line">        vector_norm = np.linalg.norm(vector)</span><br><span class="line">        <span class="keyword">if</span> vector_norm != 0:</span><br><span class="line">            unit_vector = vector / vector_norm</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            unit_vector = np.zeros(self.dim)</span><br><span class="line">        <span class="built_in">return</span> unit_vector</span><br><span class="line"></span><br><span class="line">    def load_or_tokenize_corpus(self, fname):</span><br><span class="line">        data = []</span><br><span class="line">        <span class="keyword">if</span> os.path.exists(fname + <span class="string">"-tokenized"</span>):</span><br><span class="line">            with open(fname + <span class="string">"-tokenized"</span>, <span class="string">"r"</span>) as f1:</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> f1:</span><br><span class="line">                    sentence, tokens, label = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                    data.append([sentence, tokens.split(), label])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            with open(fname, <span class="string">"r"</span>) as f2, open(fname + <span class="string">"-tokenized"</span>, <span class="string">"w"</span>) as f3:</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> f2:</span><br><span class="line">                    sentence, label = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                    tokens = self.tokenizer.morphs(sentence)</span><br><span class="line">                    data.append([sentence, tokens, label])</span><br><span class="line">                    f3.writelines(sentence + <span class="string">"\u241E"</span> + <span class="string">' '</span>.join(tokens) + <span class="string">"\u241E"</span> + label + <span class="string">"\n"</span>)</span><br><span class="line">        <span class="built_in">return</span> data</span><br><span class="line"></span><br><span class="line">    def compute_word_frequency(self, embedding_corpus_fname):</span><br><span class="line">        total_count = 0</span><br><span class="line">        <span class="comment"># &#123;단어 : 해당 단어 개수&#125;로 표현해주기 위해 다음과 같이 defaultdict을 사용했다.</span></span><br><span class="line">        <span class="comment"># defaultdict 을 사용한 이유는 값을 따로 지정해 주지 않는다면 default 값을 사용하기 위해서이다.</span></span><br><span class="line">        words_count = defaultdict(int)</span><br><span class="line">        with open(embedding_corpus_fname, <span class="string">"r"</span>) as f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                tokens = line.strip().split()</span><br><span class="line">                <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">                    words_count[token] += 1</span><br><span class="line">                    total_count += 1</span><br><span class="line">        <span class="built_in">return</span> words_count, total_count</span><br><span class="line"></span><br><span class="line">    def load_word_embeddings(self, vecs_fname, method):</span><br><span class="line">        <span class="keyword">if</span> method == <span class="string">"word2vec"</span>:</span><br><span class="line">            model = Word2Vec.load(vecs_fname)</span><br><span class="line">            words = model.wv.index2word</span><br><span class="line">            vecs = model.wv.vectors</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            words, vecs = [], []</span><br><span class="line">            with open(vecs_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f1:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">"fasttext"</span> <span class="keyword">in</span> method:</span><br><span class="line">                    next(f1)  <span class="comment"># skip head line</span></span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> f1:</span><br><span class="line">                    <span class="keyword">if</span> method == <span class="string">"swivel"</span>:</span><br><span class="line">                        splited_line = line.replace(<span class="string">"\n"</span>, <span class="string">""</span>).strip().split(<span class="string">"\t"</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        splited_line = line.replace(<span class="string">"\n"</span>, <span class="string">""</span>).strip().split(<span class="string">" "</span>)</span><br><span class="line">                    words.append(splited_line[0])</span><br><span class="line">                    vec = [<span class="built_in">float</span>(el) <span class="keyword">for</span> el <span class="keyword">in</span> splited_line[1:]]</span><br><span class="line">                    vecs.append(vec)</span><br><span class="line">        <span class="built_in">return</span> words, vecs</span><br><span class="line"></span><br><span class="line">    def load_or_construct_weighted_embedding(self, embedding_fname, embedding_method, embedding_corpus_fname, a=0.0001):</span><br><span class="line">        dictionary = &#123;&#125;</span><br><span class="line">        <span class="comment"># 이미 만들어진 가중합 embedding이 존재할 경우</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(embedding_fname + <span class="string">"-weighted"</span>):</span><br><span class="line">            with open(embedding_fname + <span class="string">"-weighted"</span>, <span class="string">"r"</span>) as f2:</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> f2:</span><br><span class="line">                    <span class="comment"># \u241E : Symbol for Record Seperator</span></span><br><span class="line">                    word, weighted_vector = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                    weighted_vector = [<span class="built_in">float</span>(el) <span class="keyword">for</span> el <span class="keyword">in</span> weighted_vector.split()]</span><br><span class="line">                    dictionary[word] = weighted_vector</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 위에서 embedding-weighted 파일이 존재하지 않는다면 훈련을 해야하므로</span></span><br><span class="line">            <span class="comment"># 우선 이미 embedding된 파일의 단어와 해당단어의 임베딩 벡터를 불러온다.</span></span><br><span class="line">            <span class="comment"># load pretrained word embeddings</span></span><br><span class="line">            <span class="comment"># 해당 임베딩 파일에 있는 단어와 그에 해당하는 임베딩벡터를 순서대로 불러온다.</span></span><br><span class="line">            words, vecs = self.load_word_embeddings(embedding_fname, embedding_method)</span><br><span class="line">            <span class="comment"># compute word frequency</span></span><br><span class="line">            words_count, total_word_count = self.compute_word_frequency(embedding_corpus_fname)</span><br><span class="line">            <span class="comment"># construct weighted word embeddings</span></span><br><span class="line">            <span class="comment"># embedding_fname - weighted로 가중합을 계산한 임베딩벡터 파일을 생성한다.</span></span><br><span class="line">            with open(embedding_fname + <span class="string">"-weighted"</span>, <span class="string">"w"</span>) as f3:</span><br><span class="line">                <span class="keyword">for</span> word, vec <span class="keyword">in</span> zip(words, vecs):</span><br><span class="line">                    <span class="keyword">if</span> word <span class="keyword">in</span> words_count.keys():</span><br><span class="line">                        word_prob = words_count[word] / total_word_count</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        word_prob = 0.0</span><br><span class="line">                    weighted_vector = (a / (word_prob + a)) * np.asarray(vec)</span><br><span class="line">                    dictionary[word] = weighted_vector</span><br><span class="line">                    f3.writelines(word + <span class="string">"\u241E"</span> + <span class="string">" "</span>.join([str(el) <span class="keyword">for</span> el <span class="keyword">in</span> weighted_vector]) + <span class="string">"\n"</span>)</span><br><span class="line">        <span class="built_in">return</span> dictionary</span><br><span class="line"></span><br><span class="line">    def train_model(self, train_data_fname, model_fname):</span><br><span class="line">        model = &#123;<span class="string">"vectors"</span>: [], <span class="string">"labels"</span>: [], <span class="string">"sentences"</span>: []&#125;</span><br><span class="line">        <span class="comment"># [sentence, tokens, label]형태로 출력</span></span><br><span class="line">        train_data = self.load_or_tokenize_corpus(train_data_fname)</span><br><span class="line">        with open(model_fname, <span class="string">"w"</span>) as f:</span><br><span class="line">            <span class="keyword">for</span> sentence, tokens, label <span class="keyword">in</span> train_data:</span><br><span class="line">                tokens = self.tokenizer.morphs(sentence)</span><br><span class="line">                sentence_vector = self.get_sentence_vector(tokens)</span><br><span class="line">                model[<span class="string">"sentences"</span>].append(sentence)</span><br><span class="line">                model[<span class="string">"vectors"</span>].append(sentence_vector)</span><br><span class="line">                model[<span class="string">"labels"</span>].append(label)</span><br><span class="line">                str_vector = <span class="string">" "</span>.join([str(el) <span class="keyword">for</span> el <span class="keyword">in</span> sentence_vector])</span><br><span class="line">                f.writelines(sentence + <span class="string">"\u241E"</span> + <span class="string">" "</span>.join(tokens) + <span class="string">"\u241E"</span> + str_vector + <span class="string">"\u241E"</span> + label + <span class="string">"\n"</span>)</span><br><span class="line">        <span class="built_in">return</span> model</span><br><span class="line"></span><br><span class="line">    def load_model(self, model_fname):</span><br><span class="line">        model = &#123;<span class="string">"vectors"</span>: [], <span class="string">"labels"</span>: [], <span class="string">"sentences"</span>: []&#125;</span><br><span class="line">        with open(model_fname, <span class="string">"r"</span>) as f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                sentence, _, vector, label = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                vector = np.array([<span class="built_in">float</span>(el) <span class="keyword">for</span> el <span class="keyword">in</span> vector.split()])</span><br><span class="line">                model[<span class="string">"sentences"</span>].append(sentence)</span><br><span class="line">                model[<span class="string">"vectors"</span>].append(vector)</span><br><span class="line">                model[<span class="string">"labels"</span>].append(label)</span><br><span class="line">        <span class="built_in">return</span> model</span><br></pre></td></tr></table></figure><h4 id="모델의-파라미터-값-설정"><a href="#모델의-파라미터-값-설정" class="headerlink" title="모델의 파라미터 값 설정"></a>모델의 파라미터 값 설정</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_fname = <span class="string">"./data/processed/processed_ratings_train.txt"</span></span><br><span class="line">embedding_fname = <span class="string">"./data/word-embeddings/word2vec/word2vec"</span></span><br><span class="line">model_fname = <span class="string">"./data/word-embeddings/cbow/word2vec"</span></span><br><span class="line">embedding_corpus_fname = <span class="string">"./data/tokenized/ratings_mecab.txt"</span></span><br><span class="line">embedding_method = <span class="string">"word2vec"</span></span><br><span class="line">test_data_fname = <span class="string">"./data/processed/processed_ratings_test.txt"</span></span><br></pre></td></tr></table></figure><h3 id="모델-학습-및-평가"><a href="#모델-학습-및-평가" class="headerlink" title="모델 학습 및 평가"></a>모델 학습 및 평가</h3><h4 id="해당-문장에-대한-단어벡터들의-합만을-가지고-예측"><a href="#해당-문장에-대한-단어벡터들의-합만을-가지고-예측" class="headerlink" title="해당 문장에 대한 단어벡터들의 합만을 가지고 예측"></a>해당 문장에 대한 단어벡터들의 합만을 가지고 예측</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">original_Model=CBoWModel(train_fname=train_fname, embedding_fname=embedding_fname, model_fname=model_fname, embedding_corpus_fname=None,</span><br><span class="line">                         embedding_method=embedding_method, is_weighted=False, average=False, dim=100, tokenizer_name=<span class="string">"mecab"</span>)</span><br><span class="line"></span><br><span class="line">original_Model.evaluate(test_data_fname)</span><br></pre></td></tr></table></figure><h4 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loading original embeddings, complete!</span><br><span class="line">train Continuous Bag of Words model</span><br><span class="line"></span><br><span class="line">evaluation start!</span><br><span class="line">number of correct: 36498 , total: 49997 , score: 0.7300038002280137</span><br></pre></td></tr></table></figure><h4 id="해당-문장에-대한-단어벡터들의-가중합을-가중합을-가지고-예측"><a href="#해당-문장에-대한-단어벡터들의-가중합을-가중합을-가지고-예측" class="headerlink" title="해당 문장에 대한 단어벡터들의 가중합을 가중합을 가지고 예측"></a>해당 문장에 대한 단어벡터들의 가중합을 가중합을 가지고 예측</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">weighted_Model=CBoWModel(train_fname=train_fname, embedding_fname=embedding_fname, model_fname=model_fname,</span><br><span class="line">                         embedding_corpus_fname=embedding_corpus_fname,embedding_method=embedding_method,</span><br><span class="line">                         is_weighted=True, average=False, dim=100, tokenizer_name=<span class="string">"mecab"</span>)</span><br><span class="line"></span><br><span class="line">weighted_Model.evaluate(test_data_fname)</span><br></pre></td></tr></table></figure><h4 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loading weighted embeddings, complete!</span><br><span class="line">train Continuous Bag of Words model</span><br><span class="line"></span><br><span class="line">evaluation start!</span><br><span class="line">number of correct: 34208 , total: 49997 , score: 0.6842010520631238</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/01/NLP_06/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03</title>
      <link>https://heung-bae-lee.github.io/2020/02/01/NLP_05/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/01/NLP_05/</guid>
      <pubDate>Sat, 01 Feb 2020 07:57:48 GMT</pubDate>
      <description>
      
        
        
          &lt;h3 id=&quot;순환신경망-분류-모델&quot;&gt;&lt;a href=&quot;#순환신경망-분류-모델&quot; class=&quot;headerlink&quot; title=&quot;순환신경망 분류 모델&quot;&gt;&lt;/a&gt;순환신경망 분류 모델&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;앞선 모델들과 달리 이미 주어진 단어 특징 벡
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h3 id="순환신경망-분류-모델"><a href="#순환신경망-분류-모델" class="headerlink" title="순환신경망 분류 모델"></a>순환신경망 분류 모델</h3><ul><li><p>앞선 모델들과 달리 이미 주어진 단어 특징 벡터를 활용해 모델을 학습하지 않고 텍스트 정보를 입력해서 문장에 대한 특징 정보를 추출한다.</p></li><li><p>RNN은 현재 정보는 이전 정보가 점층적으로 쌓이면서 정보를 표현할 수 있는 모델이다. 따라서 <code>시간에 의존적인 또는 순차적인 데이터에 대한 문제에 활용</code>된다. 이 모델은 한단에 대한 정보를 입력하면 이 단어 다음에 나올 단어를 맞추는 모델이라 순차적인 데이터에 대한 모델링이 가능한 것이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">INPUT_TRAIN_DATA_FILE_NAME = <span class="string">'train_input.npy'</span></span><br><span class="line">LABEL_TRAIN_DATA_FILE_NAME = <span class="string">'train_label.npy'</span></span><br><span class="line">DATA_CONFIGS_FILE_NAME = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line">train_input = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line">train_label = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line"></span><br><span class="line">prepro_configs = None</span><br><span class="line"></span><br><span class="line">with open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, <span class="string">'r'</span>) as f:</span><br><span class="line">  prepro_configs = json.load(f)</span><br></pre></td></tr></table></figure><h4 id="학습과-검증-데이터셋-분리"><a href="#학습과-검증-데이터셋-분리" class="headerlink" title="학습과 검증 데이터셋 분리"></a>학습과 검증 데이터셋 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">TEST_SPLIT=0.1</span><br><span class="line">RANDOM_SEED=13371447</span><br><span class="line"></span><br><span class="line">input_train, input_eval, label_train, label_eval = train_test_split(train_input, train_label, test_size=TEST_SPLIT, random_state=RANDOM_SEED)</span><br></pre></td></tr></table></figure><h4 id="데이터-입력-함수"><a href="#데이터-입력-함수" class="headerlink" title="데이터 입력 함수"></a>데이터 입력 함수</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = 16</span><br><span class="line">NUM_EPOCHS = 20</span><br><span class="line"></span><br><span class="line">def mapping_fn(X, Y):</span><br><span class="line">  inputs, labels = &#123;<span class="string">'x'</span> : X&#125;, Y</span><br><span class="line">  <span class="built_in">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line">def train_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=50000)</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  dataset = dataset.repeat(count=NUM_EPOCHS)</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br><span class="line"></span><br><span class="line">def eval_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((input_eval, label_eval))</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br></pre></td></tr></table></figure><h4 id="모델-함수"><a href="#모델-함수" class="headerlink" title="모델 함수"></a>모델 함수</h4><h5 id="모델-하이퍼파라미터-정의"><a href="#모델-하이퍼파라미터-정의" class="headerlink" title="모델 하이퍼파라미터 정의"></a>모델 하이퍼파라미터 정의</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = prepro_configs[<span class="string">'vocab_size'</span>]</span><br><span class="line"></span><br><span class="line">WORD_EMBEDDING_DIM = 100</span><br><span class="line"></span><br><span class="line">HIDDEN_STATE_DIM = 150</span><br><span class="line"></span><br><span class="line">DENSE_FEATURE_DIM = 150</span><br><span class="line"></span><br><span class="line">learning_rate = 0.001</span><br></pre></td></tr></table></figure><h5 id="모델-구현"><a href="#모델-구현" class="headerlink" title="모델 구현"></a>모델 구현</h5><ul><li><p>먼저 모델에서 배치 데이터를 받게 된다면 단어 인덱스로 구성된 Sequence 형태로 입력이 들어온다. 데이터 입력 함수에서 정의했듯이 모델 함수의 입력 인자인 features는 Python dictionary 형태로 구성돼 있다.</p></li><li><p>모델에 들어온 입력 데이터는 보통 Embedding Layer를 거친다. 구현하고자 하는 모델에서는 tf.keras.Embedding함수가 이 같은 역할을 수행한다.</p></li><li><p>Embedding Layer를 거쳐 나온 데이터는 순환 신경망 층을 거쳐 문자의 벡터를 출력한다. 여기서는 간단한 심층 순환 신경망 모델로 LSTM 모델을 통해 구현한다. 순환 신경망을 구현하기 위해서는 RNNCell이란 객체를 활용함ㄴ다. RNNCell은 순환 신경망 객체라 보면된다. LSTM으로 순환 신경망을 구현하기 위해 tf.nn.rnn_cell.LSTMCell객체를 생성하며, 이 객체는 하나의 LSTM Cell을 의미한다. 따라서 해당 Cell 객체를 여러개 생성해서 하나의 리스트로 만들어 준다. LSTMCell을 생성할 때는 은닉 상태 벡터(Hidden state vector)에 대한 차원만 정의하면 된다.</p></li><li><p>여러 LSTMCell을 쌀게 되면 이를 하나의 MultiRNN으로 묶어야, 즉 wrapping해야한다. tf.nn.rnn_cell.MultiRNNCell을 생성함으로써 Stack 구조의 LSTM 신경망을 구현할 수 있다. 단순히 RNNCell 만으로 구성해 모델 연산 그래프를 만들 수 있다. RNNCell 객체는 Sequence 한 스텝에 대한 연산만 가능하다. 따라서 여러 스텝에 대한 연산을 하기 위해서는 for 문을 활용해 연산을 할 수 있게 구현해야한다. 하지만 이보다 더 간단하게 구현할 수 있는 방법은 tf.nn.dynamic_rnn 함수를 사용하는 것이다. 이 함수는 for 문 없이 자동으로 순환 신경망을 만들어 주는 역할을 한다.</p></li><li><p>dynamic_rnn 함수에 필요한 입력 인자는 2개다. 첫 번째 순환 신경망 객체인 MultiRNNCell 객체이고, 나머지 하나는 입력값을 넣어주면된다.</p></li><li><p>Dense에 적용시키는 입력값은 LSTM 신경망의 마지막 출력값을 넣어준다. <code>출력값에 [:, -1, :]로 마지막 값만 뽑아낸 후 Dense에 적용</code>시킨다.</p></li><li><p>마지막으로 감정이 긍정인지 부정인지 판단할 수 있도록 출력값을 하나로 만들어야 한다. 보통 선형변환을 통해 입력 벡터에 대한 차원수를 바꾼다.</p></li></ul><h4 id="모델-학습-검정-및-테스트를-위한-구현"><a href="#모델-학습-검정-및-테스트를-위한-구현" class="headerlink" title="모델 학습, 검정 및 테스트를 위한 구현"></a>모델 학습, 검정 및 테스트를 위한 구현</h4><ul><li><p>앞서 모델에서 구현한 값과 정답 label을 가지고 loss 값을 구해 Adam optimizer를 활용해 모델 parameter를 최적화 해 볼 것이다.</p></li><li><p>모델 예측 loss값은 모델에서 구한 logits 변수의 경우 아직 Logistic 함수를 통해 0~1 사이의 값으로 스케일을 맞춰두지 않았다. 물론 앞서 dense 층에서 activation 인자를 tf.nn.sigmoid로 설정해둘 수 있다. 하지만 여기서는 tf.losses.sigmoid_cross_entropy 함수를 활용해 손실값을 구할 수 있기 때문에 dense 층에서 설정하지 않았다.</p></li><li><p>예측 loss값을 구하고 나면 이제 parameter optimization을 하고자 SGD를 진행한다. 여기서는 tf.train.AdamOptimizer클래스를 활용할 것이다. tf.train.AdamOptimizer.minimize 함수를 선언 할 때 전체 학습에 대한 global step값을 넣어야 한다. tf.train.get_global_step을 선언하면 현재 학습 global step을 얻을 수 있다.</p></li><li><p>보통 직접 모델 함수를 구현하게 되면 tf.estimator.EstimatorSpec 객체를 생성해서 반환하게 한다. 이 객체는 현재 함수가 어느 모드에서 실행되고 있는지 확인한다. 그리고 각 모드에 따라 필요한 입력 인자가 다르다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def model_fn(features, labels, mode):</span><br><span class="line">  TRAIN = mode == tf.estimator.ModeKeys.TRAIN</span><br><span class="line">  EVAL = mode == tf.estimator.ModeKeys.EVAL</span><br><span class="line">  PREDICT = mode == tf.estimator.ModeKeys.PREDICT</span><br><span class="line"></span><br><span class="line">  embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM)(features[<span class="string">'x'</span>])</span><br><span class="line"></span><br><span class="line">  embedding_layer = tf.keras.layers.Dropout(0.2)(embedding_layer)</span><br><span class="line"></span><br><span class="line">  rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) <span class="keyword">for</span> size <span class="keyword">in</span> [HIDDEN_STATE_DIM, HIDDEN_STATE_DIM]]</span><br><span class="line"></span><br><span class="line">  multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)</span><br><span class="line"></span><br><span class="line">  outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell, inputs=embedding_layer, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">  outputs = tf.keras.layers.Dropout(0.2)(outputs)</span><br><span class="line"></span><br><span class="line">  hidden_layer = tf.keras.layers.Dense(DENSE_FEATURE_DIM, activation=tf.nn.tanh)(outputs[:, -1, :])</span><br><span class="line">  hidden_layer = tf.keras.layers.Dropout(0.2)(hidden_layer)</span><br><span class="line"></span><br><span class="line">  logits = tf.keras.layers.Dense(1)(hidden_layer)</span><br><span class="line">  logits = tf.squeeze(logits, axis=-1)</span><br><span class="line"></span><br><span class="line">  sigmoid_logits = tf.nn.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> PREDICT:</span><br><span class="line">    predictions = &#123;<span class="string">'sentiment'</span>: sigmoid_logits&#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)</span><br><span class="line"></span><br><span class="line">  loss = tf.losses.sigmoid_cross_entropy(labels, logits)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> EVAL:</span><br><span class="line">    accuracy = tf.metrics.accuracy(labels, tf.round(sigmoid_logits))</span><br><span class="line">    eval_metric_ops = &#123;<span class="string">'acc'</span>:accuracy&#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> TRAIN:</span><br><span class="line">    global_step = tf.train.get_global_step()</span><br><span class="line">    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)</span><br></pre></td></tr></table></figure><h4 id="TF-Estimator-활용한-모델-학습-및-성능-검증"><a href="#TF-Estimator-활용한-모델-학습-및-성능-검증" class="headerlink" title="TF Estimator 활용한 모델 학습 및 성능 검증"></a>TF Estimator 활용한 모델 학습 및 성능 검증</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">est = tf.estimator.Estimator(model_fn, model_dir=DATA_OUT_PATH + <span class="string">'checkpoint'</span>)</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>]=<span class="string">"4"</span></span><br><span class="line"></span><br><span class="line">est.train(train_input_fn)</span><br></pre></td></tr></table></figure><ul><li>validation data에 대한 성능이 약 85%정도였다. 오히려 앞의 머신러닝 기법들 중 어떤 기법보다는 성능이 떨어진다는 것을 볼 수 있었지만, test data에 대한 성능을 한번 체크해 보아야 할 것 같다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">est.evaluate(eval_input_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># &#123;'acc': 0.8472, 'global_step': 18291, 'loss': 0.6007853&#125;</span></span><br></pre></td></tr></table></figure><h4 id="데이터-제출"><a href="#데이터-제출" class="headerlink" title="데이터 제출"></a>데이터 제출</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line">TEST_INPUT_DATA = <span class="string">'test_input.npy'</span></span><br><span class="line"></span><br><span class="line">test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, <span class="string">'rb'</span>))</span><br></pre></td></tr></table></figure><ul><li>estimator를 통해 예측하기 위해서는 데이터 입력 함수를 정의해야 했다. 이 경우는 tf.estimator.inputs.numpy_input_fn 함수를 활용해 데이터 입력 함수를 생성한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">"x"</span>: test_input_data&#125;, shuffle=False)</span><br><span class="line"></span><br><span class="line">predictions = np.array([p[<span class="string">'sentiment'</span>] <span class="keyword">for</span> p <span class="keyword">in</span> est.predict(input_fn=predict_input_fn)])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TEST_ID_DATA = <span class="string">'test_id.npy'</span></span><br><span class="line">test_id = np.load(open(DATA_IN_PATH + TEST_ID_DATA, <span class="string">'rb'</span>), allow_pickle=<span class="string">'True'</span>)</span><br><span class="line">output = pd.DataFrame(&#123;<span class="string">'id'</span>: test_id, <span class="string">'sentiment'</span>: list(predictions)&#125;)</span><br><span class="line">output.to_csv(DATA_OUT_PATH + <span class="string">"rnn_predic.csv"</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"rnn_predic.csv"</span> -m <span class="string">"LSTM Model with Epoch 10"</span></span><br></pre></td></tr></table></figure><p><img src="/image/BOW_LSTM_performence.png" alt="LSTM의 성능"></p><h3 id="CNN을-이용한-문장-분류"><a href="#CNN을-이용한-문장-분류" class="headerlink" title="CNN을 이용한 문장 분류"></a>CNN을 이용한 문장 분류</h3><ul><li>CNN은 보통 image에서 많이 사용된다고 생각들지만, 텍스트에서도 좋은 효과를 낼 수 있다는 점을 Yoon Kimm(2014) 박사가 쓴 “Convolutional Neural Network for Sentence Classification”을 통해 입증되었다. <code>RNN이 단어의 입력 순서를 중요하게 반영한다면 CNN은 문장의 지역정보를 보존하면서 각 문장 성분의 등장 정보를 학습에 반영하는 구조로 풀어가고 있다. 학습할 때 각 필터 크기를 조절하면서 언어의 특징 값을 추출하게 되는데, 기존의 n-gram(2그램, 3그램) 방식과 유사하다고 볼 수 있다.</code></li></ul><p><img src="/image/Convnets_with_text_classification.png" alt="합성곱 신경망"></p><p><img src="/image/2D_Convnets_with_text_classification.png" alt="2-D 합성곱 신경망"></p><p><img src="/image/1D_Convnets_with_text_classification.png" alt="1-D 합성곱 신경망"></p><p><img src="/image/Convnets_with_text_classification_explanin.png" alt="CNN을 이용한 문장 분류"></p><h4 id="모델-구현-1"><a href="#모델-구현-1" class="headerlink" title="모델 구현"></a>모델 구현</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 기본적인 라이브러리들을 불러온다</span></span><br><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import json</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow import keras</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 이전에 저장했던 학습에 필요한 디렉터리 설정 및 학습/평가 데이터를 불러온다.</span></span><br><span class="line"></span><br><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line">INPUT_TRAIN_DATA_FILE_NAME = <span class="string">'train_input.npy'</span></span><br><span class="line">LABEL_TRAIN_DATA_FILE_NAME = <span class="string">'train_label.npy'</span></span><br><span class="line">INPUT_TEST_DATA_FILE_NAME = <span class="string">'test_input.npy'</span></span><br><span class="line"></span><br><span class="line">DATA_CONFIGS_FILE_NAME = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line">train_input_data = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line">train_label_data = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line">test_input_data = np.load(open(DATA_IN_PATH + INPUT_TEST_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line"></span><br><span class="line">with open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, <span class="string">'r'</span>) as f:</span><br><span class="line">  prepro_configs = json.load(f)</span><br><span class="line">  <span class="built_in">print</span>(prepro_configs.keys())</span><br></pre></td></tr></table></figure><h4 id="학습과-검증-데이터셋-분리-1"><a href="#학습과-검증-데이터셋-분리-1" class="headerlink" title="학습과 검증 데이터셋 분리"></a>학습과 검증 데이터셋 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파라미터 변수</span></span><br><span class="line">RNG_SEED = 1234</span><br><span class="line">BATCH_SIZE = 16</span><br><span class="line">NUM_EPOCHS = 10</span><br><span class="line">VOCAB_SIZE = prepro_configs[<span class="string">'vocab_size'</span>]</span><br><span class="line">EMB_SIZE = 128</span><br><span class="line">VALID_SPLIT = 0.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습 데이터와 검증 데이터를 train_test_split 함수를 활용해 나눈다.</span></span><br><span class="line">train_input, eval_input, train_label, eval_label = train_test_split(train_input_data, train_label_data, test_size=VALID_SPLIT, random_state=RNG_SEED)</span><br></pre></td></tr></table></figure><h4 id="데이터-입력-함수-1"><a href="#데이터-입력-함수-1" class="headerlink" title="데이터 입력 함수"></a>데이터 입력 함수</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 전처리 학습을 위해 tf.data를 설정한다.</span></span><br><span class="line">def mapping_fn(X, Y=None):</span><br><span class="line">  input, label = &#123;<span class="string">'x'</span>: X&#125;, Y</span><br><span class="line">  <span class="built_in">return</span> input, label</span><br><span class="line"></span><br><span class="line">def train_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=len(train_input))</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line">  dataset = dataset.repeat(count=NUM_EPOCHS)</span><br><span class="line"></span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def eval_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((eval_input, eval_label))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=len(eval_input))</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line"></span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br></pre></td></tr></table></figure><h4 id="모델-구현-2"><a href="#모델-구현-2" class="headerlink" title="모델 구현"></a>모델 구현</h4><ul><li>합성곱 연산의 경우 케라스 모듈 중 Conv1D를 활용해 진행한다. 총 3개의 합성곱 층을 사용하는데, 각각 필터의 크기를 다르게 해서 적용한다. 즉, kernel_size를 3,4,5로 설정할 것이다. 그리고 이렇게 각각 다른 필터의 크기로 적용한 합성곱 층 출력값을 하나로 합칠 것이다. 그리고 추가로 각 합성곱 신경망 이후에 max pooling 층을 적용한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">def model_fn(features, labels, mode):</span><br><span class="line"></span><br><span class="line">  TRAIN = mode == tf.estimator.ModeKeys.TRAIN</span><br><span class="line">  EVAL = mode == tf.estimator.ModeKeys.EVAL</span><br><span class="line">  PREDICT = mode == tf.estimator.ModeKeys.PREDICT</span><br><span class="line"></span><br><span class="line">  <span class="comment"># embedding layer를 선언</span></span><br><span class="line">  embedding_layer = keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features[<span class="string">'x'</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># embedding layer에 대한 output에 대해 dropout을 취한다.</span></span><br><span class="line">  dropout_emb = keras.layers.Dropout(0.5)(embedding_layer)</span><br><span class="line"></span><br><span class="line">  <span class="comment">## filters = 128이고 kernel_size = 3,4,5이다.</span></span><br><span class="line">  <span class="comment">## 길이기ㅏ 3, 4, 5인 128개의 다른 필터를 생성한다. 3, 4, 5 gram의 효과처럼 다양한 각도에서 문장을 보는 효과가 있다.</span></span><br><span class="line">  <span class="comment">## conv1d는 (배치 크기, 길이, 채널)로 입력값을 받는데, 배치 사이즈 : 문장 숫자 | 길이 : 각 문장의 단어의 개수 | 채널 : 임베딩 출력 차원수</span></span><br><span class="line"></span><br><span class="line">  conv1 = keras.layers.Conv1D(filters=128, kernel_size=3, padding=<span class="string">'valid'</span>, activation=tf.nn.relu)(dropout_emb)</span><br><span class="line">  pool1 = keras.layers.GlobalMaxPool1D()(conv1)</span><br><span class="line"></span><br><span class="line">  conv2 = keras.layers.Conv1D(filters=128, kernel_size=4, padding=<span class="string">'valid'</span>, activation=tf.nn.relu)(dropout_emb)</span><br><span class="line">  pool2 = keras.layers.GlobalMaxPool1D()(conv2)</span><br><span class="line"></span><br><span class="line">  conv3 = keras.layers.Conv1D(filters=128, kernel_size=5, padding=<span class="string">'valid'</span>, activation=tf.nn.relu)(dropout_emb)</span><br><span class="line">  pool3 = keras.layers.GlobalMaxPool1D()(conv3)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3,4,5 gram이후 모아주기</span></span><br><span class="line">  concat = keras.layers.concatenate([pool1, pool2, pool3])</span><br><span class="line"></span><br><span class="line">  hidden = keras.layers.Dense(250, activation=tf.nn.relu)(concat)</span><br><span class="line">  dropout_hidden = keras.layers.Dropout(0.5)(hidden)</span><br><span class="line">  logits = keras.layers.Dense(1, name=<span class="string">'logits'</span>)(dropout_hidden)</span><br><span class="line">  logits = tf.squeeze(logits, axis=-1)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 최종적으로 학습, 검증, 평가의 단계로 나누어 활용</span></span><br><span class="line">  <span class="keyword">if</span> PREDICT:</span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, predictions=&#123;<span class="string">'prob'</span>: tf.nn.sigmoid(logits)&#125;)</span><br><span class="line"></span><br><span class="line">  loss = tf.losses.sigmoid_cross_entropy(labels, logits)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> EVAL:</span><br><span class="line">    pred = tf.nn.sigmoid(logits)</span><br><span class="line">    accuracy = tf.metrics.accuracy(labels, tf.round(pred))</span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=&#123;<span class="string">'acc'</span>:accuracy&#125;)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> TRAIN:</span><br><span class="line">    global_step = tf.train.get_global_step()</span><br><span class="line">    train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)</span><br></pre></td></tr></table></figure><h4 id="모델-학습"><a href="#모델-학습" class="headerlink" title="모델 학습"></a>모델 학습</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model_dir = os.path.join(os.getcwd(), <span class="string">"data_out/checkpoint/cnn"</span>)</span><br><span class="line">os.makedirs(model_dir, exist_ok=True)</span><br><span class="line"></span><br><span class="line">config_tf = tf.estimator.RunConfig(save_checkpoints_steps=200, keep_checkpoint_max=2,</span><br><span class="line">                                    log_step_count_steps=400)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Estimator 객체 생성</span></span><br><span class="line">cnn_est = tf.estimator.Estimator(model_fn, model_dir=model_dir)</span><br><span class="line">cnn_est.train(train_input_fn)</span><br></pre></td></tr></table></figure><h4 id="검증-데이터-평가"><a href="#검증-데이터-평가" class="headerlink" title="검증 데이터 평가"></a>검증 데이터 평가</h4><ul><li>검증 데이터에 대한 정확도가 약 88%정도로 측정되었다. 지금껏 간단한 모델들 중 제일 높은 성능을 보이고 있어 필자는 약간 기대하고 있었다. 이에따른 test data의 성능을 알아보기 위해 캐글에 test data의 예측값을 제출해 볼 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cnn_est.evaluate(eval_input_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># &#123;'acc': 0.8774, 'global_step': 94200, 'loss': 1.3248637&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line">TEST_INPUT_DATA = <span class="string">'test_input.npy'</span></span><br><span class="line">TEST_ID_DATA = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, <span class="string">'rb'</span>))</span><br><span class="line">ids = np.load(open(DATA_IN_PATH + TEST_ID_DATA, <span class="string">'rb'</span>), allow_pickle=True)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">'x'</span>:test_input_data&#125;, shuffle=False)</span><br><span class="line"></span><br><span class="line">predictions = np.array([p[<span class="string">'prob'</span>] <span class="keyword">for</span> p <span class="keyword">in</span> cnn_est.predict(input_fn=predict_input_fn)])</span><br><span class="line"></span><br><span class="line">output = pd.DataFrame(&#123;<span class="string">"id"</span>: list(ids), <span class="string">"sentiment"</span>:list(predictions)&#125;)</span><br><span class="line"></span><br><span class="line">output.to_csv(DATA_OUT_PATH + <span class="string">"Bag_of_Words_model_test.csv"</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"Bag_of_Words_model_test.csv"</span> -m <span class="string">"CNN 1d Model with EPOCHS 10"</span></span><br></pre></td></tr></table></figure><p><img src="/image/BOW_CNN_performence.png" alt="3,4,5-gram 을 활용한 CNN의 성능"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/01/NLP_05/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 실습 텍스트 분류(TF-IDF, CountVectorizer, Word2Vec) -02</title>
      <link>https://heung-bae-lee.github.io/2020/01/30/NLP_04/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/30/NLP_04/</guid>
      <pubDate>Wed, 29 Jan 2020 15:13:48 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;모델링-소개&quot;&gt;&lt;a href=&quot;#모델링-소개&quot; class=&quot;headerlink&quot; title=&quot;모델링 소개&quot;&gt;&lt;/a&gt;모델링 소개&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;선형모델&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;로지스틱회귀 모델&lt;ul&gt;
&lt;li&gt;입력 벡터를 wo
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="모델링-소개"><a href="#모델링-소개" class="headerlink" title="모델링 소개"></a>모델링 소개</h2><ul><li><p>선형모델</p><ul><li>로지스틱회귀 모델<ul><li>입력 벡터를 word2vec과 tf-idf를 사용해본다.</li></ul></li></ul></li><li><p>랜던포레스트</p></li></ul><h3 id="TF-IDF를-활용한-모델-구현"><a href="#TF-IDF를-활용한-모델-구현" class="headerlink" title="TF-IDF를 활용한 모델 구현"></a>TF-IDF를 활용한 모델 구현</h3><ul><li>모델의 입력값으로 TF-IDF 값을 갖는 벡터를 사용할 것이기 때문에 scikit-learn의 TfidfVectorizer를 사용할 것이다. 이를 위해서는 입력값이 텍스트로 이뤄진 데이터 형태이어야 한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">'train_clean.csv'</span>)</span><br><span class="line">reviews = list(train_data[<span class="string">'clean_review'</span>])</span><br><span class="line">sentiments = list(train_data[<span class="string">'sentiment'</span>])</span><br></pre></td></tr></table></figure><h3 id="TF-IDF-Vectorizing"><a href="#TF-IDF-Vectorizing" class="headerlink" title="TF-IDF Vectorizing"></a>TF-IDF Vectorizing</h3><ul><li>데이터에 대해 TF-IDF 값으로 벡터화를 진행한다.<ul><li>min_df : 설정한 값보다 특정 Token의 df 값이 더 적게 나오면 벡터화 과정에서 제거</li><li>anlayzer : 분석 단위를 의미, ‘word’의 경우 간어 하나를 단위로, ‘char’는 문자 하나를 단위로</li><li>sublinear_tf : 문서의 단어 빈도수(tf:term frequency)에 대한 smoothing 여부를 설정</li><li>ngram_range : 빈도의 기본 단위를 어떤 범위의 n-gram으로 설정할 것인지를 보는 인자</li><li>max_features : 각 벡터의 최대 길이(특징의 길이)를 설정</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = TfidfVectorizer(min_df=0.0, analyzer=<span class="string">'char'</span>, sublinear_tf=True, ngram_range=(1,3), max_features=5000)</span><br><span class="line"></span><br><span class="line">X = vectorizer.fit_transform(reviews)</span><br><span class="line">X</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.shape</span><br></pre></td></tr></table></figure><h4 id="학습과-검증-데이터셋-분리"><a href="#학습과-검증-데이터셋-분리" class="headerlink" title="학습과 검증 데이터셋 분리"></a>학습과 검증 데이터셋 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">RANDOM_SEED = 42</span><br><span class="line">TEST_SPLIT = 0.2</span><br><span class="line"></span><br><span class="line">y = np.array(sentiments)</span><br><span class="line"></span><br><span class="line">X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT)</span><br></pre></td></tr></table></figure><ul><li>class_wight=’balanced’로 설정해서 각 label에 대해 균형 있게 학습할 수 있게 한 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">lgs = LogisticRegression(class_weight = <span class="string">'balanced'</span>)</span><br><span class="line">lgs.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"Accuracy: &#123;&#125;"</span>.format(lgs.score(X_eval, y_eval)))</span><br></pre></td></tr></table></figure><ul><li>필자는 Accuracy: 0.8676을 출력으로 받았다. validation data에 대한 성능이 약 87%의 정확도를 갖으므로 test data에 대해서도 비슷한 수준일 것이라고 기대하며 kaggle에 test data의 예측값을 제출해 볼 것이다.</li></ul><h4 id="데이터-제출하기"><a href="#데이터-제출하기" class="headerlink" title="데이터 제출하기"></a>데이터 제출하기</h4><ul><li>만든 모델을 활용해 평가 데이터 결과를 예측하고 캐글에 제출할 수 있도록 파일로 저장할 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">test_data = pd.read_csv(<span class="string">'test_clean.csv'</span>)</span><br><span class="line"></span><br><span class="line">testDataVecs = vectorizer.transform(test_data[<span class="string">"review"</span>])</span><br><span class="line"></span><br><span class="line">test_predicted = lgs.predict(testDataVecs)</span><br><span class="line"><span class="built_in">print</span>(test_predicted)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">ids = list(test_data[<span class="string">'id'</span>])</span><br><span class="line">answer_dataset = pd.DataFrame(&#123;<span class="string">'id'</span> : ids, <span class="string">"sentiment"</span> : test_predicted&#125;)</span><br><span class="line">answer_dataset.to_csv(DATA_OUT_PATH + <span class="string">'lgs_tfidf_answer.csv'</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"lgs_tfidf_answer.csv"</span> -m <span class="string">"LogisticRegression Model with tf-idf"</span></span><br></pre></td></tr></table></figure><p><img src="/image/LogisticRegression_with_fiidf_vectorizing.png" alt="TfidfVectorizer를 사용한 LogisticRegression모델의 test data 정확도"></p><h3 id="Woed2vec-CBOW-을-활용한-모델-구현"><a href="#Woed2vec-CBOW-을-활용한-모델-구현" class="headerlink" title="Woed2vec(CBOW)을 활용한 모델 구현"></a>Woed2vec(CBOW)을 활용한 모델 구현</h3><ul><li>이번에는 word2vec을 활용해 모델을 구현할 것이다. 우선 각 단어에 대해 word2vec으로 벡터화해야 한다. word2vec의 경우 <code>단어로 표현된 리스트를 입력값</code>으로 넣어야 하기 때문에 전처리한 넘파이 배열을 바로 사용하지 않는다. 따라서 <code>전처리된 텍스트 데이터를 불러온 후 각 단어들의 리스트로 나눠야 한다.</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">"/content/"</span></span><br><span class="line"></span><br><span class="line">TRAIN_CLEAN_DATA = <span class="string">'train_clean.csv'</span></span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)</span><br><span class="line"></span><br><span class="line">reviews = list(train_data[<span class="string">'review'</span>])</span><br><span class="line">sentiments = list(train_data[<span class="string">'sentiment'</span>])</span><br><span class="line"></span><br><span class="line">sentences = []</span><br><span class="line"><span class="keyword">for</span> review <span class="keyword">in</span> reviews:</span><br><span class="line">  sentences.append(review.split())</span><br></pre></td></tr></table></figure><h4 id="word2ve-벡터화"><a href="#word2ve-벡터화" class="headerlink" title="word2ve 벡터화"></a>word2ve 벡터화</h4><ul><li>num_features : 각 단어에 대해 임베딩된 벡터의 차원을 정한다.</li><li>min_word_count : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습 하지 않기 위해 최소 빈도수를 설정한다.</li><li>num_workers : 모델 학습 시 학습을 위한 프로세스 개수를 지정한다.</li><li>context : word2vec을 수행하기 위한 context 윈도우 크기를 지정한다.</li><li>downsampling : word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 label에 대한 downsampling 비율을 지정한다. 보통 0.001이 좋은 성능을 낸다고 한다.</li></ul><h3 id="참고로-parameter-중에-sg의-default값인-0을-사용했으므로-이-모델은-Word2vec의-CBOW모델이다"><a href="#참고로-parameter-중에-sg의-default값인-0을-사용했으므로-이-모델은-Word2vec의-CBOW모델이다" class="headerlink" title="참고로 parameter 중에 sg의 default값인 0을 사용했으므로 이 모델은 Word2vec의 CBOW모델이다."></a>참고로 parameter 중에 sg의 default값인 0을 사용했으므로 이 모델은 Word2vec의 CBOW모델이다.</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_features = 300</span><br><span class="line">min_word_count = 40</span><br><span class="line">num_workers = 4</span><br><span class="line">context = 10</span><br><span class="line">downsampling = 1e-3</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install gensim</span><br></pre></td></tr></table></figure><ul><li><p>word2vec을 학습하는 과정에서 진행 상황을 확인해 보기 위해 다음과 같이 logging을 통해 확인해 볼 수 있다.</p></li><li><p>로깅을 할 때 format을 위와 같이 지정하고, 로그 수준은 INFO에 맞추면 word2vec의 학습과정에서 로그 메시지를 양식에 맞게 INFO 수준으로 보여준다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line">logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import word2vec</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Training model ...."</span>)</span><br><span class="line"></span><br><span class="line">model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)</span><br></pre></td></tr></table></figure><ul><li>word2vec으로 학습시킨 모델의 경우 모델을 따로 저장해두면 이후에 다시 사용할 수 있기 때문에 저장해 두고 이후에 학습한 값이 추가로 필요할 경우 사용하면 된다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델의 하이퍼파라미터를 설정한 내용을 모델 이름에 담는다면 나중에 참고하기에 좋다.</span></span><br><span class="line"><span class="comment"># 모델을 저장하면 Word2Vec.load()를 통해 모델을 다시 사용할 수 있다.</span></span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"300features_40minwords_10context"</span></span><br><span class="line">model.save(model_name)</span><br></pre></td></tr></table></figure><ul><li><p>word2vec 모델을 활용해서 선형 회귀 모델을 학습할 것이다. 우선 학습을 하기 위해서는 하나의 review를 같은 형태의 입력값으로 만들어야 한다. 지금은 word2vec 모델에서 각 단어가 벡터로 표현되어 있다. 그리고 review 마다 단어의 개수가 모두 다르기 때문에 입력값을 하나의 형태로 만들어야 한다.</p></li><li><p>아래 model을 통해 얻은 단어 하나의 feature는 (300,)의 shape를 갖게 될 것이다.</p></li><li><p>가장 단순한 방법은 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법이 있다.</p><ul><li>words : 단어의 모음인 하나의 review</li><li>model : 학습한 word2vec 모델</li><li>num_features : word2vec으로 임베딩할 때 정했던 벡터의 차원 수</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def get_features(words, model, num_features):</span><br><span class="line">  <span class="comment"># 출력 벡터 초기화</span></span><br><span class="line">  feature_vector = np.zeros((num_features), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">  num_words = 0</span><br><span class="line">  <span class="comment"># 어휘사전 준비</span></span><br><span class="line">  index2word_set = <span class="built_in">set</span>(model.wv.index2word)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">if</span> w <span class="keyword">in</span> index2word_set:</span><br><span class="line">      num_words +=1</span><br><span class="line">      <span class="comment"># 사전에 해당하는 단어에 대해 단어 벡터를 더함</span></span><br><span class="line">      feature_vector = np.add(feature_vector, model[w])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함</span></span><br><span class="line">  feature_vector = np.divide(feature_vector, num_words)</span><br><span class="line">  <span class="built_in">return</span> feature_vector</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def get_dataset(reviews, model, num_features):</span><br><span class="line">  dataset = list()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> s <span class="keyword">in</span> reviews:</span><br><span class="line">    dataset.append(get_features(s, model, num_features))</span><br><span class="line"></span><br><span class="line">  reviewFeatureVecs = np.stack(dataset)</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> reviewFeatureVecs</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data_vecs = get_dataset(sentences, model, num_features)</span><br></pre></td></tr></table></figure><h4 id="학습과-검증-데이터셋-분리-1"><a href="#학습과-검증-데이터셋-분리-1" class="headerlink" title="학습과 검증 데이터셋 분리"></a>학습과 검증 데이터셋 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">X = train_data_vecs</span><br><span class="line">y = np.array(sentiments)</span><br><span class="line"></span><br><span class="line">RANDOM_SEED = 42</span><br><span class="line">TEST_SPLIT = 0.2</span><br><span class="line"></span><br><span class="line">X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)</span><br></pre></td></tr></table></figure><h4 id="모델-선언-및-학습"><a href="#모델-선언-및-학습" class="headerlink" title="모델 선언 및 학습"></a>모델 선언 및 학습</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">lgs = LogisticRegression(class_weight=<span class="string">'balanced'</span>)</span><br><span class="line">lgs.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><h4 id="검증-데이터셋을-이용한-성능-평가"><a href="#검증-데이터셋을-이용한-성능-평가" class="headerlink" title="검증 데이터셋을 이용한 성능 평가"></a>검증 데이터셋을 이용한 성능 평가</h4><ul><li>이전의 TF-IDF를 사용해서 학습한 것보단 상대적으로 성능이 떨어진다. word2vec이 단어 간의 유사도를 보는 관점에서는 분명히 효과적일 수는 있지만 word2vec을 사용하는 것이 항상 가장 좋은 성능을 보장하지는 않는다는 것을 다시 한번 알 수 있다!!!</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"Accuracy: %f"</span> % lgs.score(X_eval, y_eval))</span><br></pre></td></tr></table></figure><ul><li>validation data에 대한 정확도는 83%정도로 TF-IDF로 했던 것보단 조금 떨어지지만 캐글에 제출해보고 overfitting이 발생했는지 점검해 본다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TEST_CLEAN_DATA = <span class="string">'test_clean.csv'</span></span><br><span class="line"></span><br><span class="line">test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)</span><br><span class="line"></span><br><span class="line">test_review = list(test_data[<span class="string">'review'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_sentences = []</span><br><span class="line"><span class="keyword">for</span> review <span class="keyword">in</span> test_review:</span><br><span class="line">  test_sentences.append(review.split())</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_data_vecs = get_dataset(test_sentences, model, num_features)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">test_predicted = lgs.predict(test_data_vecs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">test_data[<span class="string">'id'</span>]=test_data[<span class="string">'id'</span>].apply(lambda x : x[1:-1])</span><br><span class="line">ids = list(test_data[<span class="string">'id'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">answer_dataset = pd.DataFrame(&#123;<span class="string">'id'</span>: ids, <span class="string">'sentiment'</span>: test_predicted&#125;)</span><br><span class="line">answer_dataset.to_csv(DATA_OUT_PATH + <span class="string">'lgs_answer.csv'</span>, index=False)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"lgs_answer.csv"</span> -m <span class="string">"LogisticRegression Model with Word2vec"</span></span><br></pre></td></tr></table></figure><p><img src="/image/LogisticRegression_Model_with_Word2vec.png" alt="Word2vec Vectorizing을 사용한 LogisticRegression"></p><h3 id="랜덤포레스트-분류-모델"><a href="#랜덤포레스트-분류-모델" class="headerlink" title="랜덤포레스트 분류 모델"></a>랜덤포레스트 분류 모델</h3><h4 id="CountVectorizer를-활용한-벡터화"><a href="#CountVectorizer를-활용한-벡터화" class="headerlink" title="CountVectorizer를 활용한 벡터화"></a>CountVectorizer를 활용한 벡터화</h4><ul><li>CountVectorizer는 TF-IDF vectorizing과 동일하게 문장을 input으로 받기 때문에 Word2vec처럼 공백단위로 쪼개 단어로 사용하지 않을 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">TRAIN_CLEAN_DATA = <span class="string">'train_clean.csv'</span></span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)</span><br><span class="line">reviews = list(train_data[<span class="string">'clean_review'</span>])</span><br><span class="line">y = np.array(train_data[<span class="string">'sentiment'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer(analyzer = <span class="string">'word'</span>, max_features = 5000)</span><br><span class="line"></span><br><span class="line">train_data_features = vectorizer.fit_transform(reviews)</span><br></pre></td></tr></table></figure><h4 id="학습과-검증-데이터-분리"><a href="#학습과-검증-데이터-분리" class="headerlink" title="학습과 검증 데이터 분리"></a>학습과 검증 데이터 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TEST_SIZE = 0.2</span><br><span class="line">RANDOM_SEED = 42</span><br><span class="line"></span><br><span class="line">train_input, eval_input, train_label, eval_label = train_test_split(train_data_features, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)</span><br></pre></td></tr></table></figure><h4 id="모델-구현-및-학습"><a href="#모델-구현-및-학습" class="headerlink" title="모델 구현 및 학습"></a>모델 구현 및 학습</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 랜덤 포레스트 분류기에 100개의 의사결정 트리를 사용한다.</span></span><br><span class="line">forest = RandomForestClassifier(n_estimators=100)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 단어 묶음을 벡터화한 데이터와 정답 데이터를 가지고 학습을 시작한다.</span></span><br><span class="line">forest.fit(train_input, train_label)</span><br></pre></td></tr></table></figure><h4 id="검증-데이터셋으로-성능-평가"><a href="#검증-데이터셋으로-성능-평가" class="headerlink" title="검증 데이터셋으로 성능 평가"></a>검증 데이터셋으로 성능 평가</h4><ul><li>결과를 보면 대략 85%의 정확도를 보여준다. 앙상블 모델인데도 앞서 사용한 간단한 모델(TF_IDF보단 상대적으로)보다 좋지 않은 성능을 보여준다. 이는 모델의 문제일 수도 있고 데이터에서 특징을 추출하는 방법의 문제일 수도 있다. 즉, 모델을 바꾸지 않더라도 특징 추출 방법을 앞서 사용한 TF-IDF나 word2vec을 사용해서 입력값을 만든다면 성능이 높아질 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"Accuracy: %f"</span> % forest.score(eval_input, eval_label))</span><br></pre></td></tr></table></figure><h4 id="데이터-제출"><a href="#데이터-제출" class="headerlink" title="데이터 제출"></a>데이터 제출</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_CLEAN_DATA = <span class="string">'test_clean.csv'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">test_data = pd.read_csv(DATA_OUT_PATH + TEST_CLEAN_DATA)</span><br><span class="line"></span><br><span class="line">test_reviews = list(test_data[<span class="string">'review'</span>])</span><br><span class="line">ids = list(test_data[<span class="string">'id'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_data_features = vectorizer.transform(test_reviews)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">result = forest.predict(test_data_features)</span><br><span class="line"></span><br><span class="line">output = pd.DataFrame(&#123;<span class="string">'id'</span>: ids, <span class="string">"sentiment"</span>: result&#125;)</span><br><span class="line"></span><br><span class="line">output.to_csv(DATA_OUT_PATH + <span class="string">'Randomforest_model_with_Countvectorizer.csv'</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"Randomforest_model_with_Countvectorizer.csv"</span> -m <span class="string">"Randomforest Model with Countvectorizer"</span></span><br></pre></td></tr></table></figure><p><img src="/image/RandomForest_with_CountVectorizing.png" alt="Count vectorizing을 사용한 Random Forest 성능"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/30/NLP_04/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
