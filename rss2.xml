<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DataLatte&#39;s IT Blog</title>
    <link>https://heung-bae-lee.github.io/</link>
    
    <atom:link href="/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>DataLatte가 Data Science 공부를 하면서 정리해 놓는 블로그</description>
    <pubDate>Mon, 03 Feb 2020 10:55:10 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>NLP - 단어 수준 임베딩</title>
      <link>https://heung-bae-lee.github.io/2020/02/01/NLP_06/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/01/NLP_06/</guid>
      <pubDate>Sat, 01 Feb 2020 11:47:03 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;단어-수준-임베딩&quot;&gt;&lt;a href=&quot;#단어-수준-임베딩&quot; class=&quot;headerlink&quot; title=&quot;단어 수준 임베딩&quot;&gt;&lt;/a&gt;단어 수준 임베딩&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;예측 기반 모델&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NPLM&lt;/li&gt;
&lt;
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="단어-수준-임베딩"><a href="#단어-수준-임베딩" class="headerlink" title="단어 수준 임베딩"></a>단어 수준 임베딩</h1><ul><li><p>예측 기반 모델</p><ul><li>NPLM</li><li>Word2Vec</li><li>FastText</li></ul></li><li><p>행렬 분해 기반 모델</p><ul><li>LSA</li><li>GloVe</li><li>Swivel</li></ul></li><li><p>단어 임베딩을 문장 수준 임베딩으로 확장하는 방법</p><ul><li>가중 임베딩(Weighted Embedding)</li></ul></li></ul><h2 id="NPLM-Neural-Probabilistic-Language-Model"><a href="#NPLM-Neural-Probabilistic-Language-Model" class="headerlink" title="NPLM(Neural Probabilistic Language Model)"></a>NPLM(Neural Probabilistic Language Model)</h2><ul><li><p>NLP 분야에서 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델로서 임베딩 역사에서 차지하는 역할이 작지 않다.</p></li><li><p>‘단어가 어떤 순서로 쓰였는가’라는 가정을 통해 만들어진 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 만들어졌다는데 의의가 있으며 <code>NPLM 자체가 단어 임베딩 역할</code>을 수행할 수 있다. 다음과 같은 한계점들이 기존에는 존재했다.</p><ul><li><p>1) 학습 데이터에 존재하지 않는 데이터에 대해 나타날 확률을 0으로 부여 한다. 물론, 백오프(back-off)나 스무딩(smoothing)으로 완화시켜 줄 순 있지만 근본적인 해결방안은 아니였다.</p></li><li><p>2) 문장의 장기 의존성(long-term dependency)을 포착해내기 어렵다. 다시 말해서 n-gram모델의 n을 5 이상으로 길게 설정할 수 없다. <code>n이 커질수록 확률이 0이될 가능성이 높기 때문이다.</code></p></li><li><p>3) 단어/문장 간 유사도를 계산할 수 없다.</p></li></ul></li></ul><h3 id="NLPM의-학습"><a href="#NLPM의-학습" class="headerlink" title="NLPM의 학습"></a>NLPM의 학습</h3><ul><li>NLPM은 직전까지 등장한 n-1개(n: gram으로 묶을 수) 단어들로 다음 단어를 맞추는 <code>n-gram 언어 모델</code>이다.</li></ul><p><img src="/image/NPLM_principal.png" alt="NPLM의 학습 원리"></p><ul><li>NLPM 구조의 말단 출력<ul><li>$|V|$(corpus의 token vector, 어휘집합의 크기) 차원의 score 벡터 $y_{w_{t}}$에 softmax 함수를 적용한 $|V|$차원의 확률 벡터이다. <code>NPLM은 확률 벡터에서 가장 높은 요소의 인덱스에 해당하는 단어가 실제 정답 단어와 일치하도록 학습 한다.</code></li></ul></li></ul><script type="math/tex; mode=display">P(w_{t})|w_{t-1}, \cdots ,w_{t-n+1}) = \frac{exp(y_{w_{t}})}{\sum_{i} exp(y_{i}) }</script><script type="math/tex; mode=display">y_{w_{t}} \in R^{|V|} : w_{t}라는 단어에 해당하는 점수 벡터</script><ul><li><p>NLPM 구조의 입력</p><ul><li><p>문장 내 t번째 단어($w_{t}$)에 대응하는 단어 벡터 $x_{t}$를 만드는 과정은 다음과 같다. 먼저 $|V| x m (m: x_{t}의 차원 수)$크기를 갖는 행렬 C에서 $w_{t}$에 해당하는 벡터를 참조하는 형태이다. <code>C 행렬의 원소값은 초기에 랜덤 설정</code>한다. 참조한다는 의미는 예를 들어 아래 그림에서 처럼 어휘 집합에 속한 단어가 5개이고 $w_{t}$가 4번째 인덱스를 의미한다고 하면 행렬 $C$와 $w_{t}$에 해당하는 one-hot 벡터를 내적한 것과 같다. 즉 $C$라는 행렬에서 $w_{t}$에 해당하는 행만 참조하는 것과 동일한 결과를 얻을 수 있다. 따라서 이 과정을 Look-up table이라는 용어로 많이 사용한다</p></li><li><p><code>문장 내 모든 단어들을 한 단어씩 훑으면서 Corpus 전체를 학습하게 된다면 NPLM 모델의 C 행렬에 각 단어의 문맥 정보를 내재할 수 있게 된다.</code></p></li></ul></li></ul><script type="math/tex; mode=display">x_{t} = w_{t} \cdot C = C(w_{t}), C \in R^{|v|xm}</script><p><img src="/image/NPLM_C_matrix.png" alt="NPLM 입력 벡터"></p><h3 id="모델-구조-및-의미정보"><a href="#모델-구조-및-의미정보" class="headerlink" title="모델 구조 및 의미정보"></a>모델 구조 및 의미정보</h3><p>이 때, walking을 맞추는 과정에서 발생한 손실(train loss)를 최소화하는 그래디언트(gradient)를 받아 각 단어에 해당하는 행들이 동일하게 업데이트 된다. 따라서 이 단어들의 벡터는 벡터 공간에서 같은 방향으로 조금씩 움직인다고 볼 수 있다. <code>결과적으로 임베딩 벡터 공간에서 해당 단어들 사이의 거리가 가깝다는 것은 의미가 유사하다라는 의미로 해석</code>할 수 있다.</p><p><img src="/image/NLPM_input.png" alt="NPLM의 input 벡터"></p><p><img src="/image/NPLM_structure.png" alt="NPLM의 구조"></p><h3 id="NPLM의-특징"><a href="#NPLM의-특징" class="headerlink" title="NPLM의 특징"></a>NPLM의 특징</h3><ul><li><code>NPLM은 그 자체로 언어 모델 역할을 수행</code>할 수 있다. 기존의 통계 기반 n-gram 모델은 학습 데이터에 한 번도 등장하지 않은 패턴에 대해서는 그 등장 확률을 0으로 부여하는 문제점을 NPLM은 <code>문장이 Corpus에 없어도 문맥이 비슷한 다른 문장을 참고해 확률을 부여</code>하는 방식으로 극복하는데 큰 의의가 있다. 하지만, 학습 파라미터가 너무 많아 계산이 복잡해지고, 자연스럽게 모델 과적합(overfitting)이 발생할 가능성이 높다는 한계를 가진다.</li></ul><p>이 한계를 극복하기 위해 다음 임베딩 방법론인 Word2Vec이 등장하였다.</p><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>Word2vec은 가장 널리 쓰이고 있는 단어 임베딩 모델이다. <code>Skip-gram</code>과 <code>CBOW</code>라는 모델이 제안되었고, 이 두 모델을 근간으로 하되 negative sampling등 학습 최적화 기법을 제안한 내용이 핵심이다.</p><ul><li>CBOW<ul><li><code>주변에 있는 context word들을 가지고 target word 하나를 맟추는 과정에서 학습</code>된다.</li><li>입,출력 데이터 쌍<ul><li>{context words, target word}</li></ul></li></ul></li></ul><p><img src="/image/step_by_step_CBOW.png" alt="CBOW 모델 01"></p><p><img src="/image/step_by_step_CBOW_01.png" alt="CBOW 모델 02"></p><p><img src="/image/Projection_layer_in_CBOW.png" alt="CBOW 모델의 Projection Layer"></p><p><img src="/image/Output_layer_in_CBOW.png" alt="CBOW 모델의 Output Layer"></p><ul><li>Skip-gram<ul><li><code>target word 하나를 가지고 context word들이 무엇일지 예측하는 과정에서 학습</code>된다.</li><li><code>skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 CBOW보다 좋은 경향</code>이 있다.</li><li>입,출력 데이터 쌍<ul><li>{target word, target word 전전 단어}, {target word, target word 직전 단어}, {target word, target word 다음 단어}, {target word, target word 다다음 단어}</li></ul></li></ul></li></ul><p><img src="/image/skip_gram_model_in_word2vec.png" alt="Skip-gram 모델"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/01/NLP_06/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03</title>
      <link>https://heung-bae-lee.github.io/2020/02/01/NLP_05/</link>
      <guid>https://heung-bae-lee.github.io/2020/02/01/NLP_05/</guid>
      <pubDate>Sat, 01 Feb 2020 07:57:48 GMT</pubDate>
      <description>
      
        
        
          &lt;h3 id=&quot;순환신경망-분류-모델&quot;&gt;&lt;a href=&quot;#순환신경망-분류-모델&quot; class=&quot;headerlink&quot; title=&quot;순환신경망 분류 모델&quot;&gt;&lt;/a&gt;순환신경망 분류 모델&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;앞선 모델들과 달리 이미 주어진 단어 특징 벡
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h3 id="순환신경망-분류-모델"><a href="#순환신경망-분류-모델" class="headerlink" title="순환신경망 분류 모델"></a>순환신경망 분류 모델</h3><ul><li><p>앞선 모델들과 달리 이미 주어진 단어 특징 벡터를 활용해 모델을 학습하지 않고 텍스트 정보를 입력해서 문장에 대한 특징 정보를 추출한다.</p></li><li><p>RNN은 현재 정보는 이전 정보가 점층적으로 쌓이면서 정보를 표현할 수 있는 모델이다. 따라서 <code>시간에 의존적인 또는 순차적인 데이터에 대한 문제에 활용</code>된다. 이 모델은 한단에 대한 정보를 입력하면 이 단어 다음에 나올 단어를 맞추는 모델이라 순차적인 데이터에 대한 모델링이 가능한 것이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">INPUT_TRAIN_DATA_FILE_NAME = <span class="string">'train_input.npy'</span></span><br><span class="line">LABEL_TRAIN_DATA_FILE_NAME = <span class="string">'train_label.npy'</span></span><br><span class="line">DATA_CONFIGS_FILE_NAME = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line">train_input = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line">train_label = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line"></span><br><span class="line">prepro_configs = None</span><br><span class="line"></span><br><span class="line">with open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, <span class="string">'r'</span>) as f:</span><br><span class="line">  prepro_configs = json.load(f)</span><br></pre></td></tr></table></figure><h4 id="학습과-검증-데이터셋-분리"><a href="#학습과-검증-데이터셋-분리" class="headerlink" title="학습과 검증 데이터셋 분리"></a>학습과 검증 데이터셋 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">TEST_SPLIT=0.1</span><br><span class="line">RANDOM_SEED=13371447</span><br><span class="line"></span><br><span class="line">input_train, input_eval, label_train, label_eval = train_test_split(train_input, train_label, test_size=TEST_SPLIT, random_state=RANDOM_SEED)</span><br></pre></td></tr></table></figure><h4 id="데이터-입력-함수"><a href="#데이터-입력-함수" class="headerlink" title="데이터 입력 함수"></a>데이터 입력 함수</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = 16</span><br><span class="line">NUM_EPOCHS = 20</span><br><span class="line"></span><br><span class="line">def mapping_fn(X, Y):</span><br><span class="line">  inputs, labels = &#123;<span class="string">'x'</span> : X&#125;, Y</span><br><span class="line">  <span class="built_in">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line">def train_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=50000)</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  dataset = dataset.repeat(count=NUM_EPOCHS)</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br><span class="line"></span><br><span class="line">def eval_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((input_eval, label_eval))</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br></pre></td></tr></table></figure><h4 id="모델-함수"><a href="#모델-함수" class="headerlink" title="모델 함수"></a>모델 함수</h4><h5 id="모델-하이퍼파라미터-정의"><a href="#모델-하이퍼파라미터-정의" class="headerlink" title="모델 하이퍼파라미터 정의"></a>모델 하이퍼파라미터 정의</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = prepro_configs[<span class="string">'vocab_size'</span>]</span><br><span class="line"></span><br><span class="line">WORD_EMBEDDING_DIM = 100</span><br><span class="line"></span><br><span class="line">HIDDEN_STATE_DIM = 150</span><br><span class="line"></span><br><span class="line">DENSE_FEATURE_DIM = 150</span><br><span class="line"></span><br><span class="line">learning_rate = 0.001</span><br></pre></td></tr></table></figure><h5 id="모델-구현"><a href="#모델-구현" class="headerlink" title="모델 구현"></a>모델 구현</h5><ul><li><p>먼저 모델에서 배치 데이터를 받게 된다면 단어 인덱스로 구성된 Sequence 형태로 입력이 들어온다. 데이터 입력 함수에서 정의했듯이 모델 함수의 입력 인자인 features는 Python dictionary 형태로 구성돼 있다.</p></li><li><p>모델에 들어온 입력 데이터는 보통 Embedding Layer를 거친다. 구현하고자 하는 모델에서는 tf.keras.Embedding함수가 이 같은 역할을 수행한다.</p></li><li><p>Embedding Layer를 거쳐 나온 데이터는 순환 신경망 층을 거쳐 문자의 벡터를 출력한다. 여기서는 간단한 심층 순환 신경망 모델로 LSTM 모델을 통해 구현한다. 순환 신경망을 구현하기 위해서는 RNNCell이란 객체를 활용함ㄴ다. RNNCell은 순환 신경망 객체라 보면된다. LSTM으로 순환 신경망을 구현하기 위해 tf.nn.rnn_cell.LSTMCell객체를 생성하며, 이 객체는 하나의 LSTM Cell을 의미한다. 따라서 해당 Cell 객체를 여러개 생성해서 하나의 리스트로 만들어 준다. LSTMCell을 생성할 때는 은닉 상태 벡터(Hidden state vector)에 대한 차원만 정의하면 된다.</p></li><li><p>여러 LSTMCell을 쌀게 되면 이를 하나의 MultiRNN으로 묶어야, 즉 wrapping해야한다. tf.nn.rnn_cell.MultiRNNCell을 생성함으로써 Stack 구조의 LSTM 신경망을 구현할 수 있다. 단순히 RNNCell 만으로 구성해 모델 연산 그래프를 만들 수 있다. RNNCell 객체는 Sequence 한 스텝에 대한 연산만 가능하다. 따라서 여러 스텝에 대한 연산을 하기 위해서는 for 문을 활용해 연산을 할 수 있게 구현해야한다. 하지만 이보다 더 간단하게 구현할 수 있는 방법은 tf.nn.dynamic_rnn 함수를 사용하는 것이다. 이 함수는 for 문 없이 자동으로 순환 신경망을 만들어 주는 역할을 한다.</p></li><li><p>dynamic_rnn 함수에 필요한 입력 인자는 2개다. 첫 번째 순환 신경망 객체인 MultiRNNCell 객체이고, 나머지 하나는 입력값을 넣어주면된다.</p></li><li><p>Dense에 적용시키는 입력값은 LSTM 신경망의 마지막 출력값을 넣어준다. <code>출력값에 [:, -1, :]로 마지막 값만 뽑아낸 후 Dense에 적용</code>시킨다.</p></li><li><p>마지막으로 감정이 긍정인지 부정인지 판단할 수 있도록 출력값을 하나로 만들어야 한다. 보통 선형변환을 통해 입력 벡터에 대한 차원수를 바꾼다.</p></li></ul><h4 id="모델-학습-검정-및-테스트를-위한-구현"><a href="#모델-학습-검정-및-테스트를-위한-구현" class="headerlink" title="모델 학습, 검정 및 테스트를 위한 구현"></a>모델 학습, 검정 및 테스트를 위한 구현</h4><ul><li><p>앞서 모델에서 구현한 값과 정답 label을 가지고 loss 값을 구해 Adam optimizer를 활용해 모델 parameter를 최적화 해 볼 것이다.</p></li><li><p>모델 예측 loss값은 모델에서 구한 logits 변수의 경우 아직 Logistic 함수를 통해 0~1 사이의 값으로 스케일을 맞춰두지 않았다. 물론 앞서 dense 층에서 activation 인자를 tf.nn.sigmoid로 설정해둘 수 있다. 하지만 여기서는 tf.losses.sigmoid_cross_entropy 함수를 활용해 손실값을 구할 수 있기 때문에 dense 층에서 설정하지 않았다.</p></li><li><p>예측 loss값을 구하고 나면 이제 parameter optimization을 하고자 SGD를 진행한다. 여기서는 tf.train.AdamOptimizer클래스를 활용할 것이다. tf.train.AdamOptimizer.minimize 함수를 선언 할 때 전체 학습에 대한 global step값을 넣어야 한다. tf.train.get_global_step을 선언하면 현재 학습 global step을 얻을 수 있다.</p></li><li><p>보통 직접 모델 함수를 구현하게 되면 tf.estimator.EstimatorSpec 객체를 생성해서 반환하게 한다. 이 객체는 현재 함수가 어느 모드에서 실행되고 있는지 확인한다. 그리고 각 모드에 따라 필요한 입력 인자가 다르다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def model_fn(features, labels, mode):</span><br><span class="line">  TRAIN = mode == tf.estimator.ModeKeys.TRAIN</span><br><span class="line">  EVAL = mode == tf.estimator.ModeKeys.EVAL</span><br><span class="line">  PREDICT = mode == tf.estimator.ModeKeys.PREDICT</span><br><span class="line"></span><br><span class="line">  embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM)(features[<span class="string">'x'</span>])</span><br><span class="line"></span><br><span class="line">  embedding_layer = tf.keras.layers.Dropout(0.2)(embedding_layer)</span><br><span class="line"></span><br><span class="line">  rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) <span class="keyword">for</span> size <span class="keyword">in</span> [HIDDEN_STATE_DIM, HIDDEN_STATE_DIM]]</span><br><span class="line"></span><br><span class="line">  multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)</span><br><span class="line"></span><br><span class="line">  outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell, inputs=embedding_layer, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">  outputs = tf.keras.layers.Dropout(0.2)(outputs)</span><br><span class="line"></span><br><span class="line">  hidden_layer = tf.keras.layers.Dense(DENSE_FEATURE_DIM, activation=tf.nn.tanh)(outputs[:, -1, :])</span><br><span class="line">  hidden_layer = tf.keras.layers.Dropout(0.2)(hidden_layer)</span><br><span class="line"></span><br><span class="line">  logits = tf.keras.layers.Dense(1)(hidden_layer)</span><br><span class="line">  logits = tf.squeeze(logits, axis=-1)</span><br><span class="line"></span><br><span class="line">  sigmoid_logits = tf.nn.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> PREDICT:</span><br><span class="line">    predictions = &#123;<span class="string">'sentiment'</span>: sigmoid_logits&#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)</span><br><span class="line"></span><br><span class="line">  loss = tf.losses.sigmoid_cross_entropy(labels, logits)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> EVAL:</span><br><span class="line">    accuracy = tf.metrics.accuracy(labels, tf.round(sigmoid_logits))</span><br><span class="line">    eval_metric_ops = &#123;<span class="string">'acc'</span>:accuracy&#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> TRAIN:</span><br><span class="line">    global_step = tf.train.get_global_step()</span><br><span class="line">    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)</span><br></pre></td></tr></table></figure><h4 id="TF-Estimator-활용한-모델-학습-및-성능-검증"><a href="#TF-Estimator-활용한-모델-학습-및-성능-검증" class="headerlink" title="TF Estimator 활용한 모델 학습 및 성능 검증"></a>TF Estimator 활용한 모델 학습 및 성능 검증</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">est = tf.estimator.Estimator(model_fn, model_dir=DATA_OUT_PATH + <span class="string">'checkpoint'</span>)</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>]=<span class="string">"4"</span></span><br><span class="line"></span><br><span class="line">est.train(train_input_fn)</span><br></pre></td></tr></table></figure><ul><li>validation data에 대한 성능이 약 85%정도였다. 오히려 앞의 머신러닝 기법들 중 어떤 기법보다는 성능이 떨어진다는 것을 볼 수 있었지만, test data에 대한 성능을 한번 체크해 보아야 할 것 같다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">est.evaluate(eval_input_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># &#123;'acc': 0.8472, 'global_step': 18291, 'loss': 0.6007853&#125;</span></span><br></pre></td></tr></table></figure><h4 id="데이터-제출"><a href="#데이터-제출" class="headerlink" title="데이터 제출"></a>데이터 제출</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line">TEST_INPUT_DATA = <span class="string">'test_input.npy'</span></span><br><span class="line"></span><br><span class="line">test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, <span class="string">'rb'</span>))</span><br></pre></td></tr></table></figure><ul><li>estimator를 통해 예측하기 위해서는 데이터 입력 함수를 정의해야 했다. 이 경우는 tf.estimator.inputs.numpy_input_fn 함수를 활용해 데이터 입력 함수를 생성한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">"x"</span>: test_input_data&#125;, shuffle=False)</span><br><span class="line"></span><br><span class="line">predictions = np.array([p[<span class="string">'sentiment'</span>] <span class="keyword">for</span> p <span class="keyword">in</span> est.predict(input_fn=predict_input_fn)])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TEST_ID_DATA = <span class="string">'test_id.npy'</span></span><br><span class="line">test_id = np.load(open(DATA_IN_PATH + TEST_ID_DATA, <span class="string">'rb'</span>), allow_pickle=<span class="string">'True'</span>)</span><br><span class="line">output = pd.DataFrame(&#123;<span class="string">'id'</span>: test_id, <span class="string">'sentiment'</span>: list(predictions)&#125;)</span><br><span class="line">output.to_csv(DATA_OUT_PATH + <span class="string">"rnn_predic.csv"</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"rnn_predic.csv"</span> -m <span class="string">"LSTM Model with Epoch 10"</span></span><br></pre></td></tr></table></figure><p><img src="/image/BOW_LSTM_performence.png" alt="LSTM의 성능"></p><h3 id="CNN을-이용한-문장-분류"><a href="#CNN을-이용한-문장-분류" class="headerlink" title="CNN을 이용한 문장 분류"></a>CNN을 이용한 문장 분류</h3><ul><li>CNN은 보통 image에서 많이 사용된다고 생각들지만, 텍스트에서도 좋은 효과를 낼 수 있다는 점을 Yoon Kimm(2014) 박사가 쓴 “Convolutional Neural Network for Sentence Classification”을 통해 입증되었다. <code>RNN이 단어의 입력 순서를 중요하게 반영한다면 CNN은 문장의 지역정보를 보존하면서 각 문장 성분의 등장 정보를 학습에 반영하는 구조로 풀어가고 있다. 학습할 때 각 필터 크기를 조절하면서 언어의 특징 값을 추출하게 되는데, 기존의 n-gram(2그램, 3그램) 방식과 유사하다고 볼 수 있다.</code></li></ul><p><img src="/image/Convnets_with_text_classification.png" alt="합성곱 신경망"></p><p><img src="/image/2D_Convnets_with_text_classification.png" alt="2-D 합성곱 신경망"></p><p><img src="/image/1D_Convnets_with_text_classification.png" alt="1-D 합성곱 신경망"></p><p><img src="/image/Convnets_with_text_classification_explanin.png" alt="CNN을 이용한 문장 분류"></p><h4 id="모델-구현-1"><a href="#모델-구현-1" class="headerlink" title="모델 구현"></a>모델 구현</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 기본적인 라이브러리들을 불러온다</span></span><br><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import json</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow import keras</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 이전에 저장했던 학습에 필요한 디렉터리 설정 및 학습/평가 데이터를 불러온다.</span></span><br><span class="line"></span><br><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line">INPUT_TRAIN_DATA_FILE_NAME = <span class="string">'train_input.npy'</span></span><br><span class="line">LABEL_TRAIN_DATA_FILE_NAME = <span class="string">'train_label.npy'</span></span><br><span class="line">INPUT_TEST_DATA_FILE_NAME = <span class="string">'test_input.npy'</span></span><br><span class="line"></span><br><span class="line">DATA_CONFIGS_FILE_NAME = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line">train_input_data = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line">train_label_data = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line">test_input_data = np.load(open(DATA_IN_PATH + INPUT_TEST_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line"></span><br><span class="line">with open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, <span class="string">'r'</span>) as f:</span><br><span class="line">  prepro_configs = json.load(f)</span><br><span class="line">  <span class="built_in">print</span>(prepro_configs.keys())</span><br></pre></td></tr></table></figure><h4 id="학습과-검증-데이터셋-분리-1"><a href="#학습과-검증-데이터셋-분리-1" class="headerlink" title="학습과 검증 데이터셋 분리"></a>학습과 검증 데이터셋 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파라미터 변수</span></span><br><span class="line">RNG_SEED = 1234</span><br><span class="line">BATCH_SIZE = 16</span><br><span class="line">NUM_EPOCHS = 10</span><br><span class="line">VOCAB_SIZE = prepro_configs[<span class="string">'vocab_size'</span>]</span><br><span class="line">EMB_SIZE = 128</span><br><span class="line">VALID_SPLIT = 0.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습 데이터와 검증 데이터를 train_test_split 함수를 활용해 나눈다.</span></span><br><span class="line">train_input, eval_input, train_label, eval_label = train_test_split(train_input_data, train_label_data, test_size=VALID_SPLIT, random_state=RNG_SEED)</span><br></pre></td></tr></table></figure><h4 id="데이터-입력-함수-1"><a href="#데이터-입력-함수-1" class="headerlink" title="데이터 입력 함수"></a>데이터 입력 함수</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 전처리 학습을 위해 tf.data를 설정한다.</span></span><br><span class="line">def mapping_fn(X, Y=None):</span><br><span class="line">  input, label = &#123;<span class="string">'x'</span>: X&#125;, Y</span><br><span class="line">  <span class="built_in">return</span> input, label</span><br><span class="line"></span><br><span class="line">def train_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=len(train_input))</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line">  dataset = dataset.repeat(count=NUM_EPOCHS)</span><br><span class="line"></span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def eval_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((eval_input, eval_label))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=len(eval_input))</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line"></span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br></pre></td></tr></table></figure><h4 id="모델-구현-2"><a href="#모델-구현-2" class="headerlink" title="모델 구현"></a>모델 구현</h4><ul><li>합성곱 연산의 경우 케라스 모듈 중 Conv1D를 활용해 진행한다. 총 3개의 합성곱 층을 사용하는데, 각각 필터의 크기를 다르게 해서 적용한다. 즉, kernel_size를 3,4,5로 설정할 것이다. 그리고 이렇게 각각 다른 필터의 크기로 적용한 합성곱 층 출력값을 하나로 합칠 것이다. 그리고 추가로 각 합성곱 신경망 이후에 max pooling 층을 적용한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">def model_fn(features, labels, mode):</span><br><span class="line"></span><br><span class="line">  TRAIN = mode == tf.estimator.ModeKeys.TRAIN</span><br><span class="line">  EVAL = mode == tf.estimator.ModeKeys.EVAL</span><br><span class="line">  PREDICT = mode == tf.estimator.ModeKeys.PREDICT</span><br><span class="line"></span><br><span class="line">  <span class="comment"># embedding layer를 선언</span></span><br><span class="line">  embedding_layer = keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features[<span class="string">'x'</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># embedding layer에 대한 output에 대해 dropout을 취한다.</span></span><br><span class="line">  dropout_emb = keras.layers.Dropout(0.5)(embedding_layer)</span><br><span class="line"></span><br><span class="line">  <span class="comment">## filters = 128이고 kernel_size = 3,4,5이다.</span></span><br><span class="line">  <span class="comment">## 길이기ㅏ 3, 4, 5인 128개의 다른 필터를 생성한다. 3, 4, 5 gram의 효과처럼 다양한 각도에서 문장을 보는 효과가 있다.</span></span><br><span class="line">  <span class="comment">## conv1d는 (배치 크기, 길이, 채널)로 입력값을 받는데, 배치 사이즈 : 문장 숫자 | 길이 : 각 문장의 단어의 개수 | 채널 : 임베딩 출력 차원수</span></span><br><span class="line"></span><br><span class="line">  conv1 = keras.layers.Conv1D(filters=128, kernel_size=3, padding=<span class="string">'valid'</span>, activation=tf.nn.relu)(dropout_emb)</span><br><span class="line">  pool1 = keras.layers.GlobalMaxPool1D()(conv1)</span><br><span class="line"></span><br><span class="line">  conv2 = keras.layers.Conv1D(filters=128, kernel_size=4, padding=<span class="string">'valid'</span>, activation=tf.nn.relu)(dropout_emb)</span><br><span class="line">  pool2 = keras.layers.GlobalMaxPool1D()(conv2)</span><br><span class="line"></span><br><span class="line">  conv3 = keras.layers.Conv1D(filters=128, kernel_size=5, padding=<span class="string">'valid'</span>, activation=tf.nn.relu)(dropout_emb)</span><br><span class="line">  pool3 = keras.layers.GlobalMaxPool1D()(conv3)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3,4,5 gram이후 모아주기</span></span><br><span class="line">  concat = keras.layers.concatenate([pool1, pool2, pool3])</span><br><span class="line"></span><br><span class="line">  hidden = keras.layers.Dense(250, activation=tf.nn.relu)(concat)</span><br><span class="line">  dropout_hidden = keras.layers.Dropout(0.5)(hidden)</span><br><span class="line">  logits = keras.layers.Dense(1, name=<span class="string">'logits'</span>)(dropout_hidden)</span><br><span class="line">  logits = tf.squeeze(logits, axis=-1)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 최종적으로 학습, 검증, 평가의 단계로 나누어 활용</span></span><br><span class="line">  <span class="keyword">if</span> PREDICT:</span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, predictions=&#123;<span class="string">'prob'</span>: tf.nn.sigmoid(logits)&#125;)</span><br><span class="line"></span><br><span class="line">  loss = tf.losses.sigmoid_cross_entropy(labels, logits)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> EVAL:</span><br><span class="line">    pred = tf.nn.sigmoid(logits)</span><br><span class="line">    accuracy = tf.metrics.accuracy(labels, tf.round(pred))</span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=&#123;<span class="string">'acc'</span>:accuracy&#125;)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> TRAIN:</span><br><span class="line">    global_step = tf.train.get_global_step()</span><br><span class="line">    train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)</span><br></pre></td></tr></table></figure><h4 id="모델-학습"><a href="#모델-학습" class="headerlink" title="모델 학습"></a>모델 학습</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model_dir = os.path.join(os.getcwd(), <span class="string">"data_out/checkpoint/cnn"</span>)</span><br><span class="line">os.makedirs(model_dir, exist_ok=True)</span><br><span class="line"></span><br><span class="line">config_tf = tf.estimator.RunConfig(save_checkpoints_steps=200, keep_checkpoint_max=2,</span><br><span class="line">                                    log_step_count_steps=400)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Estimator 객체 생성</span></span><br><span class="line">cnn_est = tf.estimator.Estimator(model_fn, model_dir=model_dir)</span><br><span class="line">cnn_est.train(train_input_fn)</span><br></pre></td></tr></table></figure><h4 id="검증-데이터-평가"><a href="#검증-데이터-평가" class="headerlink" title="검증 데이터 평가"></a>검증 데이터 평가</h4><ul><li>검증 데이터에 대한 정확도가 약 88%정도로 측정되었다. 지금껏 간단한 모델들 중 제일 높은 성능을 보이고 있어 필자는 약간 기대하고 있었다. 이에따른 test data의 성능을 알아보기 위해 캐글에 test data의 예측값을 제출해 볼 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cnn_est.evaluate(eval_input_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># &#123;'acc': 0.8774, 'global_step': 94200, 'loss': 1.3248637&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line">TEST_INPUT_DATA = <span class="string">'test_input.npy'</span></span><br><span class="line">TEST_ID_DATA = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, <span class="string">'rb'</span>))</span><br><span class="line">ids = np.load(open(DATA_IN_PATH + TEST_ID_DATA, <span class="string">'rb'</span>), allow_pickle=True)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">'x'</span>:test_input_data&#125;, shuffle=False)</span><br><span class="line"></span><br><span class="line">predictions = np.array([p[<span class="string">'prob'</span>] <span class="keyword">for</span> p <span class="keyword">in</span> cnn_est.predict(input_fn=predict_input_fn)])</span><br><span class="line"></span><br><span class="line">output = pd.DataFrame(&#123;<span class="string">"id"</span>: list(ids), <span class="string">"sentiment"</span>:list(predictions)&#125;)</span><br><span class="line"></span><br><span class="line">output.to_csv(DATA_OUT_PATH + <span class="string">"Bag_of_Words_model_test.csv"</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"Bag_of_Words_model_test.csv"</span> -m <span class="string">"CNN 1d Model with EPOCHS 10"</span></span><br></pre></td></tr></table></figure><p><img src="/image/BOW_CNN_performence.png" alt="3,4,5-gram 을 활용한 CNN의 성능"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/02/01/NLP_05/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 실습 텍스트 분류(TF-IDF, CountVectorizer, Word2Vec) -02</title>
      <link>https://heung-bae-lee.github.io/2020/01/30/NLP_04/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/30/NLP_04/</guid>
      <pubDate>Wed, 29 Jan 2020 15:13:48 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;모델링-소개&quot;&gt;&lt;a href=&quot;#모델링-소개&quot; class=&quot;headerlink&quot; title=&quot;모델링 소개&quot;&gt;&lt;/a&gt;모델링 소개&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;선형모델&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;로지스틱회귀 모델&lt;ul&gt;
&lt;li&gt;입력 벡터를 wo
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="모델링-소개"><a href="#모델링-소개" class="headerlink" title="모델링 소개"></a>모델링 소개</h2><ul><li><p>선형모델</p><ul><li>로지스틱회귀 모델<ul><li>입력 벡터를 word2vec과 tf-idf를 사용해본다.</li></ul></li></ul></li><li><p>랜던포레스트</p></li></ul><h3 id="TF-IDF를-활용한-모델-구현"><a href="#TF-IDF를-활용한-모델-구현" class="headerlink" title="TF-IDF를 활용한 모델 구현"></a>TF-IDF를 활용한 모델 구현</h3><ul><li>모델의 입력값으로 TF-IDF 값을 갖는 벡터를 사용할 것이기 때문에 scikit-learn의 TfidfVectorizer를 사용할 것이다. 이를 위해서는 입력값이 텍스트로 이뤄진 데이터 형태이어야 한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(<span class="string">'train_clean.csv'</span>)</span><br><span class="line">reviews = list(train_data[<span class="string">'clean_review'</span>])</span><br><span class="line">sentiments = list(train_data[<span class="string">'sentiment'</span>])</span><br></pre></td></tr></table></figure><h3 id="TF-IDF-Vectorizing"><a href="#TF-IDF-Vectorizing" class="headerlink" title="TF-IDF Vectorizing"></a>TF-IDF Vectorizing</h3><ul><li>데이터에 대해 TF-IDF 값으로 벡터화를 진행한다.<ul><li>min_df : 설정한 값보다 특정 Token의 df 값이 더 적게 나오면 벡터화 과정에서 제거</li><li>anlayzer : 분석 단위를 의미, ‘word’의 경우 간어 하나를 단위로, ‘char’는 문자 하나를 단위로</li><li>sublinear_tf : 문서의 단어 빈도수(tf:term frequency)에 대한 smoothing 여부를 설정</li><li>ngram_range : 빈도의 기본 단위를 어떤 범위의 n-gram으로 설정할 것인지를 보는 인자</li><li>max_features : 각 벡터의 최대 길이(특징의 길이)를 설정</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = TfidfVectorizer(min_df=0.0, analyzer=<span class="string">'char'</span>, sublinear_tf=True, ngram_range=(1,3), max_features=5000)</span><br><span class="line"></span><br><span class="line">X = vectorizer.fit_transform(reviews)</span><br><span class="line">X</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data.shape</span><br></pre></td></tr></table></figure><h4 id="학습과-검증-데이터셋-분리"><a href="#학습과-검증-데이터셋-분리" class="headerlink" title="학습과 검증 데이터셋 분리"></a>학습과 검증 데이터셋 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">RANDOM_SEED = 42</span><br><span class="line">TEST_SPLIT = 0.2</span><br><span class="line"></span><br><span class="line">y = np.array(sentiments)</span><br><span class="line"></span><br><span class="line">X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT)</span><br></pre></td></tr></table></figure><ul><li>class_wight=’balanced’로 설정해서 각 label에 대해 균형 있게 학습할 수 있게 한 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">lgs = LogisticRegression(class_weight = <span class="string">'balanced'</span>)</span><br><span class="line">lgs.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"Accuracy: &#123;&#125;"</span>.format(lgs.score(X_eval, y_eval)))</span><br></pre></td></tr></table></figure><ul><li>필자는 Accuracy: 0.8676을 출력으로 받았다. validation data에 대한 성능이 약 87%의 정확도를 갖으므로 test data에 대해서도 비슷한 수준일 것이라고 기대하며 kaggle에 test data의 예측값을 제출해 볼 것이다.</li></ul><h4 id="데이터-제출하기"><a href="#데이터-제출하기" class="headerlink" title="데이터 제출하기"></a>데이터 제출하기</h4><ul><li>만든 모델을 활용해 평가 데이터 결과를 예측하고 캐글에 제출할 수 있도록 파일로 저장할 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">test_data = pd.read_csv(<span class="string">'test_clean.csv'</span>)</span><br><span class="line"></span><br><span class="line">testDataVecs = vectorizer.transform(test_data[<span class="string">"review"</span>])</span><br><span class="line"></span><br><span class="line">test_predicted = lgs.predict(testDataVecs)</span><br><span class="line"><span class="built_in">print</span>(test_predicted)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">ids = list(test_data[<span class="string">'id'</span>])</span><br><span class="line">answer_dataset = pd.DataFrame(&#123;<span class="string">'id'</span> : ids, <span class="string">"sentiment"</span> : test_predicted&#125;)</span><br><span class="line">answer_dataset.to_csv(DATA_OUT_PATH + <span class="string">'lgs_tfidf_answer.csv'</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"lgs_tfidf_answer.csv"</span> -m <span class="string">"LogisticRegression Model with tf-idf"</span></span><br></pre></td></tr></table></figure><p><img src="/image/LogisticRegression_with_fiidf_vectorizing.png" alt="TfidfVectorizer를 사용한 LogisticRegression모델의 test data 정확도"></p><h3 id="Woed2vec을-활용한-모델-구현"><a href="#Woed2vec을-활용한-모델-구현" class="headerlink" title="Woed2vec을 활용한 모델 구현"></a>Woed2vec을 활용한 모델 구현</h3><ul><li>이번에는 word2vec을 활용해 모델을 구현할 것이다. 우선 각 단어에 대해 word2vec으로 벡터화해야 한다. word2vec의 경우 <code>단어로 표현된 리스트를 입력값</code>으로 넣어야 하기 때문에 전처리한 넘파이 배열을 바로 사용하지 않는다. 따라서 <code>전처리된 텍스트 데이터를 불러온 후 각 단어들의 리스트로 나눠야 한다.</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">"/content/"</span></span><br><span class="line"></span><br><span class="line">TRAIN_CLEAN_DATA = <span class="string">'train_clean.csv'</span></span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)</span><br><span class="line"></span><br><span class="line">reviews = list(train_data[<span class="string">'review'</span>])</span><br><span class="line">sentiments = list(train_data[<span class="string">'sentiment'</span>])</span><br><span class="line"></span><br><span class="line">sentences = []</span><br><span class="line"><span class="keyword">for</span> review <span class="keyword">in</span> reviews:</span><br><span class="line">  sentences.append(review.split())</span><br></pre></td></tr></table></figure><h4 id="word2ve-벡터화"><a href="#word2ve-벡터화" class="headerlink" title="word2ve 벡터화"></a>word2ve 벡터화</h4><ul><li>num_features : 각 단어에 대해 임베딩된 벡터의 차원을 정한다.</li><li>min_word_count : 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습 하지 않기 위해 최소 빈도수를 설정한다.</li><li>num_workers : 모델 학습 시 학습을 위한 프로세스 개수를 지정한다.</li><li>context : word2vec을 수행하기 위한 context 윈도우 크기를 지정한다.</li><li>downsampling : word2vec 학습을 수행할 때 빠른 학습을 위해 정답 단어 label에 대한 downsampling 비율을 지정한다. 보통 0.001이 좋은 성능을 낸다고 한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">num_features = 300</span><br><span class="line">min_word_count = 40</span><br><span class="line">num_workers = 4</span><br><span class="line">context = 10</span><br><span class="line">downsampling = 1e-3</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install gensim</span><br></pre></td></tr></table></figure><ul><li><p>word2vec을 학습하는 과정에서 진행 상황을 확인해 보기 위해 다음과 같이 logging을 통해 확인해 볼 수 있다.</p></li><li><p>로깅을 할 때 format을 위와 같이 지정하고, 로그 수준은 INFO에 맞추면 word2vec의 학습과정에서 로그 메시지를 양식에 맞게 INFO 수준으로 보여준다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import logging</span><br><span class="line">logging.basicConfig(format=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>, level=logging.INFO)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import word2vec</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Training model ...."</span>)</span><br><span class="line"></span><br><span class="line">model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)</span><br></pre></td></tr></table></figure><ul><li>word2vec으로 학습시킨 모델의 경우 모델을 따로 저장해두면 이후에 다시 사용할 수 있기 때문에 저장해 두고 이후에 학습한 값이 추가로 필요할 경우 사용하면 된다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델의 하이퍼파라미터를 설정한 내용을 모델 이름에 담는다면 나중에 참고하기에 좋다.</span></span><br><span class="line"><span class="comment"># 모델을 저장하면 Word2Vec.load()를 통해 모델을 다시 사용할 수 있다.</span></span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"300features_40minwords_10context"</span></span><br><span class="line">model.save(model_name)</span><br></pre></td></tr></table></figure><ul><li><p>word2vec 모델을 활용해서 선형 회귀 모델을 학습할 것이다. 우선 학습을 하기 위해서는 하나의 review를 같은 형태의 입력값으로 만들어야 한다. 지금은 word2vec 모델에서 각 단어가 벡터로 표현되어 있다. 그리고 review 마다 단어의 개수가 모두 다르기 때문에 입력값을 하나의 형태로 만들어야 한다.</p></li><li><p>아래 model을 통해 얻은 단어 하나의 feature는 (300,)의 shape를 갖게 될 것이다.</p></li><li><p>가장 단순한 방법은 문장에 있는 모든 단어의 벡터값에 대해 평균을 내서 리뷰 하나당 하나의 벡터로 만드는 방법이 있다.</p><ul><li>words : 단어의 모음인 하나의 review</li><li>model : 학습한 word2vec 모델</li><li>num_features : word2vec으로 임베딩할 때 정했던 벡터의 차원 수</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def get_features(words, model, num_features):</span><br><span class="line">  <span class="comment"># 출력 벡터 초기화</span></span><br><span class="line">  feature_vector = np.zeros((num_features), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">  num_words = 0</span><br><span class="line">  <span class="comment"># 어휘사전 준비</span></span><br><span class="line">  index2word_set = <span class="built_in">set</span>(model.wv.index2word)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">if</span> w <span class="keyword">in</span> index2word_set:</span><br><span class="line">      num_words +=1</span><br><span class="line">      <span class="comment"># 사전에 해당하는 단어에 대해 단어 벡터를 더함</span></span><br><span class="line">      feature_vector = np.add(feature_vector, model[w])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함</span></span><br><span class="line">  feature_vector = np.divide(feature_vector, num_words)</span><br><span class="line">  <span class="built_in">return</span> feature_vector</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def get_dataset(reviews, model, num_features):</span><br><span class="line">  dataset = list()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> s <span class="keyword">in</span> reviews:</span><br><span class="line">    dataset.append(get_features(s, model, num_features))</span><br><span class="line"></span><br><span class="line">  reviewFeatureVecs = np.stack(dataset)</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> reviewFeatureVecs</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data_vecs = get_dataset(sentences, model, num_features)</span><br></pre></td></tr></table></figure><h4 id="학습과-검증-데이터셋-분리-1"><a href="#학습과-검증-데이터셋-분리-1" class="headerlink" title="학습과 검증 데이터셋 분리"></a>학습과 검증 데이터셋 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">X = train_data_vecs</span><br><span class="line">y = np.array(sentiments)</span><br><span class="line"></span><br><span class="line">RANDOM_SEED = 42</span><br><span class="line">TEST_SPLIT = 0.2</span><br><span class="line"></span><br><span class="line">X_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)</span><br></pre></td></tr></table></figure><h4 id="모델-선언-및-학습"><a href="#모델-선언-및-학습" class="headerlink" title="모델 선언 및 학습"></a>모델 선언 및 학습</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">lgs = LogisticRegression(class_weight=<span class="string">'balanced'</span>)</span><br><span class="line">lgs.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><h4 id="검증-데이터셋을-이용한-성능-평가"><a href="#검증-데이터셋을-이용한-성능-평가" class="headerlink" title="검증 데이터셋을 이용한 성능 평가"></a>검증 데이터셋을 이용한 성능 평가</h4><ul><li>이전의 TF-IDF를 사용해서 학습한 것보단 상대적으로 성능이 떨어진다. word2vec이 단어 간의 유사도를 보는 관점에서는 분명히 효과적일 수는 있지만 word2vec을 사용하는 것이 항상 가장 좋은 성능을 보장하지는 않는다는 것을 다시 한번 알 수 있다!!!</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"Accuracy: %f"</span> % lgs.score(X_eval, y_eval))</span><br></pre></td></tr></table></figure><ul><li>validation data에 대한 정확도는 83%정도로 TF-IDF로 했던 것보단 조금 떨어지지만 캐글에 제출해보고 overfitting이 발생했는지 점검해 본다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TEST_CLEAN_DATA = <span class="string">'test_clean.csv'</span></span><br><span class="line"></span><br><span class="line">test_data = pd.read_csv(DATA_IN_PATH + TEST_CLEAN_DATA)</span><br><span class="line"></span><br><span class="line">test_review = list(test_data[<span class="string">'review'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_sentences = []</span><br><span class="line"><span class="keyword">for</span> review <span class="keyword">in</span> test_review:</span><br><span class="line">  test_sentences.append(review.split())</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_data_vecs = get_dataset(test_sentences, model, num_features)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">test_predicted = lgs.predict(test_data_vecs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">test_data[<span class="string">'id'</span>]=test_data[<span class="string">'id'</span>].apply(lambda x : x[1:-1])</span><br><span class="line">ids = list(test_data[<span class="string">'id'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">answer_dataset = pd.DataFrame(&#123;<span class="string">'id'</span>: ids, <span class="string">'sentiment'</span>: test_predicted&#125;)</span><br><span class="line">answer_dataset.to_csv(DATA_OUT_PATH + <span class="string">'lgs_answer.csv'</span>, index=False)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"lgs_answer.csv"</span> -m <span class="string">"LogisticRegression Model with Word2vec"</span></span><br></pre></td></tr></table></figure><p><img src="/image/LogisticRegression_Model_with_Word2vec.png" alt="Word2vec Vectorizing을 사용한 LogisticRegression"></p><h3 id="랜덤포레스트-분류-모델"><a href="#랜덤포레스트-분류-모델" class="headerlink" title="랜덤포레스트 분류 모델"></a>랜덤포레스트 분류 모델</h3><h4 id="CountVectorizer를-활용한-벡터화"><a href="#CountVectorizer를-활용한-벡터화" class="headerlink" title="CountVectorizer를 활용한 벡터화"></a>CountVectorizer를 활용한 벡터화</h4><ul><li>CountVectorizer는 TF-IDF vectorizing과 동일하게 문장을 input으로 받기 때문에 Word2vec처럼 공백단위로 쪼개 단어로 사용하지 않을 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">TRAIN_CLEAN_DATA = <span class="string">'train_clean.csv'</span></span><br><span class="line"></span><br><span class="line">train_data = pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)</span><br><span class="line">reviews = list(train_data[<span class="string">'clean_review'</span>])</span><br><span class="line">y = np.array(train_data[<span class="string">'sentiment'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import CountVectorizer</span><br><span class="line"></span><br><span class="line">vectorizer = CountVectorizer(analyzer = <span class="string">'word'</span>, max_features = 5000)</span><br><span class="line"></span><br><span class="line">train_data_features = vectorizer.fit_transform(reviews)</span><br></pre></td></tr></table></figure><h4 id="학습과-검증-데이터-분리"><a href="#학습과-검증-데이터-분리" class="headerlink" title="학습과 검증 데이터 분리"></a>학습과 검증 데이터 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TEST_SIZE = 0.2</span><br><span class="line">RANDOM_SEED = 42</span><br><span class="line"></span><br><span class="line">train_input, eval_input, train_label, eval_label = train_test_split(train_data_features, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)</span><br></pre></td></tr></table></figure><h4 id="모델-구현-및-학습"><a href="#모델-구현-및-학습" class="headerlink" title="모델 구현 및 학습"></a>모델 구현 및 학습</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 랜덤 포레스트 분류기에 100개의 의사결정 트리를 사용한다.</span></span><br><span class="line">forest = RandomForestClassifier(n_estimators=100)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 단어 묶음을 벡터화한 데이터와 정답 데이터를 가지고 학습을 시작한다.</span></span><br><span class="line">forest.fit(train_input, train_label)</span><br></pre></td></tr></table></figure><h4 id="검증-데이터셋으로-성능-평가"><a href="#검증-데이터셋으로-성능-평가" class="headerlink" title="검증 데이터셋으로 성능 평가"></a>검증 데이터셋으로 성능 평가</h4><ul><li>결과를 보면 대략 85%의 정확도를 보여준다. 앙상블 모델인데도 앞서 사용한 간단한 모델(TF_IDF보단 상대적으로)보다 좋지 않은 성능을 보여준다. 이는 모델의 문제일 수도 있고 데이터에서 특징을 추출하는 방법의 문제일 수도 있다. 즉, 모델을 바꾸지 않더라도 특징 추출 방법을 앞서 사용한 TF-IDF나 word2vec을 사용해서 입력값을 만든다면 성능이 높아질 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"Accuracy: %f"</span> % forest.score(eval_input, eval_label))</span><br></pre></td></tr></table></figure><h4 id="데이터-제출"><a href="#데이터-제출" class="headerlink" title="데이터 제출"></a>데이터 제출</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_CLEAN_DATA = <span class="string">'test_clean.csv'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">test_data = pd.read_csv(DATA_OUT_PATH + TEST_CLEAN_DATA)</span><br><span class="line"></span><br><span class="line">test_reviews = list(test_data[<span class="string">'review'</span>])</span><br><span class="line">ids = list(test_data[<span class="string">'id'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">test_data_features = vectorizer.transform(test_reviews)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">result = forest.predict(test_data_features)</span><br><span class="line"></span><br><span class="line">output = pd.DataFrame(&#123;<span class="string">'id'</span>: ids, <span class="string">"sentiment"</span>: result&#125;)</span><br><span class="line"></span><br><span class="line">output.to_csv(DATA_OUT_PATH + <span class="string">'Randomforest_model_with_Countvectorizer.csv'</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"Randomforest_model_with_Countvectorizer.csv"</span> -m <span class="string">"Randomforest Model with Countvectorizer"</span></span><br></pre></td></tr></table></figure><p><img src="/image/RandomForest_with_CountVectorizing.png" alt="Count vectorizing을 사용한 Random Forest 성능"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/30/NLP_04/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 실습 텍스트 분류 -01</title>
      <link>https://heung-bae-lee.github.io/2020/01/29/NLP_03/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/29/NLP_03/</guid>
      <pubDate>Wed, 29 Jan 2020 14:40:09 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;영어-텍스트-분류&quot;&gt;&lt;a href=&quot;#영어-텍스트-분류&quot; class=&quot;headerlink&quot; title=&quot;영어 텍스트 분류&quot;&gt;&lt;/a&gt;영어 텍스트 분류&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;한국어는 띄어쓰기를 기준으로 모든 단어를 처리할 수 없으므로
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="영어-텍스트-분류"><a href="#영어-텍스트-분류" class="headerlink" title="영어 텍스트 분류"></a>영어 텍스트 분류</h2><ul><li><p>한국어는 띄어쓰기를 기준으로 모든 단어를 처리할 수 없으므로 상대적으로 전처리하기 쉬운 영어 텍스트를 가지고 먼저 감각을 키워보겠다.</p><ul><li>데이터 이름 : Bag of Words Meets Bags of Popcorn</li><li>데이터 용도 : 텍스트 분류 학습을 목적으로 사용</li><li>데이터 권한 : MIT</li><li>데이터 출처 : <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/data" target="_blank" rel="noopener">https://www.kaggle.com/c/word2vec-nlp-tutorial/data</a></li></ul></li></ul><h2 id="문제-소개"><a href="#문제-소개" class="headerlink" title="문제 소개"></a>문제 소개</h2><p>영어 텍스트 분류 문제 중 캐글의 대회인 워드팝콘 문제를 활용할 것이다. 이 문제를 해결하면서 텍스트 분류 기술을 알아볼것이다.</p><h3 id="워드-팝콘"><a href="#워드-팝콘" class="headerlink" title="워드 팝콘"></a>워드 팝콘</h3><ul><li>워드 팝콘은 인터넷 영화 데이터베이스(IMDB)에서 나온 영화 평점 데이터를 활용한 캐글 문제다. 영화 평점 데이터이므로 각 데이터는 영화 리뷰 텍스트와 평점에 따른 감정 값(긍정 혹은 부정)으로 구성돼 있다. 이 데이터는 보통 감성 분석(sentiment analysis) 문제에서 자주 활용된다.</li></ul><h3 id="목표"><a href="#목표" class="headerlink" title="목표"></a>목표</h3><ul><li>1) 데이터를 불러오고 정제되지 않은 데이터를 활용하기 쉽게 전처리하는 과정</li><li>2) 데이터 분석 과정<ul><li>데이터를 분석하여 어떻게 문제를 풀어가야 할지 접근하는 과정</li></ul></li><li>3) 실제 문제를 해결하기 위해 알고리즘을 모델링하는 과정  </li></ul><p>캐글 API를 colab에서 사용하기 위한 인증 및 google storage에 업로드 되어있는 인증키 파일 현재 colab pwd로 복사해온 후 설정완료하기</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from google.colab import auth</span><br><span class="line">import warnings</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'retina'</span></span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line">auth.authenticate_user()</span><br><span class="line"></span><br><span class="line">!gsutil cp gs://kaggle_key/kaggle.json kaggle.json</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">!mkdir -p ~/.kaggle</span><br><span class="line">!mv ./kaggle.json ~/.kaggle/</span><br><span class="line">!chmod 600 ~/.kaggle/kaggle.json</span><br><span class="line">!pip install kaggle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 캐글 competition 목록확인</span></span><br><span class="line"><span class="comment">#!kaggle competitions list</span></span><br></pre></td></tr></table></figure><ul><li>목표 competition의 데이터 다운로드</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파일 확인</span></span><br><span class="line">!kaggle competitions files -c word2vec-nlp-tutorial</span><br><span class="line"></span><br><span class="line"><span class="comment"># 파일 다운로드</span></span><br><span class="line">!kaggle competitions download -c word2vec-nlp-tutorial</span><br></pre></td></tr></table></figure><h2 id="데이터-분석-및-전처리"><a href="#데이터-분석-및-전처리" class="headerlink" title="데이터 분석 및 전처리"></a>데이터 분석 및 전처리</h2><ul><li><p>모델을 학습시키기 전에 데이터를 전처리하는 과정을 거쳐야 한다. 전처리는 데이터를 모델에 적용하기에 적합하도록 데이터를 정제하는 과정이다. 그전에 데이터를 불러오고 분석하는 과정을 선행할 것이다. EDA과정을 거친 후 분석 결과를 바탕으로 전처리 작업을 할 것이다.</p></li><li><p>참고로 데이터를 불러오는데 403 error가 출력된다면, 우선적으로 대회의 rule을 check했는지 확인해 보아야 한다.</p><ul><li>sampleSubmission.csv 파일을 제외한 나머지 파일이 zip으로 압축돼 있기 때문에 압축을 푸는 과정부터 시작한다. 압축을 풀기 위해 zipfile이라는 내장 라이브러리를 사용할 것이다.</li></ul></li></ul><pre><code>- 압축을 풀기 위해 경로와 압축을 풀 파일명을 리스트로 선언한 후 반복문을 사용해 압축을 풀 것이다.</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import zipfile</span><br><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">file_list = [<span class="string">'labeledTrainData.tsv.zip'</span>, <span class="string">'testData.tsv.zip'</span>, <span class="string">'unlabeledTrainData.tsv.zip'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> file_list:</span><br><span class="line">  <span class="comment"># 압축풀기 대상 설정 및 모드 설정</span></span><br><span class="line">  zipRef = zipfile.ZipFile(DATA_IN_PATH + file, <span class="string">'r'</span>)</span><br><span class="line">  <span class="comment"># 압축 풀기 및 저장 경로 설정</span></span><br><span class="line">  zipRef.extractall(DATA_IN_PATH)</span><br><span class="line">  <span class="comment"># 호출 종료</span></span><br><span class="line">  zipRef.close()</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import os</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 그래프를 바로 그리도록 함</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><ul><li>현재 사용할 데이터는 tap(\t)으로 구분돼 있으므로 delimeter=’\t’로 설정해주었고, 각 데이터에 각 항목명(Header)이 포함돼 있기 때문에 header인자에 0을 설정한다. R에서는 header=Ture로 하는 역할과 같다고 보면된다. 그리고 쌍따옴표를 무시하기 위해 quoting=3을 설정해 주었다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(DATA_IN_PATH+<span class="string">"labeledTrainData.tsv"</span>, header=0, delimiter=<span class="string">'\t'</span>, quoting=3)</span><br><span class="line"></span><br><span class="line">train_data.head()</span><br></pre></td></tr></table></figure><h3 id="데이터-분석-진행-순서"><a href="#데이터-분석-진행-순서" class="headerlink" title="데이터 분석 진행 순서"></a>데이터 분석 진행 순서</h3><ul><li>1) 데이터 크기</li><li>2) 데이터의 개수</li><li>3) 각 리뷰의 문자 길이 분포</li><li>4) 많이 사용된 단어</li><li>5) 긍정, 부정 데이터의 분포</li><li>6) 각 리뷰의 단어 개수 분포</li><li>7) 특수문자 및 대문자, 소문자 비율</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터 크기</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"파일 크기 : "</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(DATA_IN_PATH):</span><br><span class="line">  <span class="keyword">if</span> <span class="string">'tsv'</span> <span class="keyword">in</span> file and <span class="string">'zip'</span> not <span class="keyword">in</span> file:</span><br><span class="line">    <span class="built_in">print</span>(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + <span class="string">'MB'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습 데이터의 개수</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'전체 학습 데이터의 개수: &#123;&#125;'</span>.format(len(train_data)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># 전체 학습 데이터의 개수: 25000</span></span><br></pre></td></tr></table></figure><ul><li>각 review의 길이를 분석</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_length = train_data[<span class="string">'review'</span>].apply(len)</span><br><span class="line">train_length.head()</span><br></pre></td></tr></table></figure><ul><li>각 리뷰의 문자 길이가 대부분 6,000 이하이고 대부분 2,000이하에 분포돼 있음을 알 수 있다. 그리고 일부 데이터의 경우 이상치로 10,000 이상의 값을 가지고 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12, 5))</span><br><span class="line"></span><br><span class="line">plt.hist(train_length, bins=200, alpha=0.5, color=<span class="string">'r'</span>, label=<span class="string">'word'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y축의 범위를 log단위로 바꿔주고 non-positive에 대해서는 아주작은 양수로 클리핑한다.</span></span><br><span class="line">plt.yscale(<span class="string">'log'</span>, nonposy=<span class="string">'clip'</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Log-Histogram of length of review'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Length of review'</span>)</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'Number of review'</span>)</span><br></pre></td></tr></table></figure><p><img src="/image/log_histogram_of_length_of_review.png" alt="각 review의 길이에 대한 히스토그램"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 기초 통계량 확인</span></span><br><span class="line">train_length.describe()</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12, 5))</span><br><span class="line"></span><br><span class="line">plt.boxplot(train_length, labels=[<span class="string">'train data review length'</span>], showmeans=True)</span><br></pre></td></tr></table></figure><p><img src="/image/train_data_review_length.png" alt="학습 데이터의 길이에 대한 boxplot"></p><ul><li><p>wordcloud를 통해 시각적으로 빈도수를 확인하기 위해 설치한다.</p></li><li><p>워드 클라우드를 보면 가장 많이 사용된 단어는 br이라는 것을 확인할 수 있다. HTML 태그인 br 해당 데이터가 높은 빈도수를 보이는 것으로 미루어보아 정제되지 않은 인터넷 상의 리뷰 형태로 작성돼 있음을 알 수 있다. 이후 전처리 작업에서 이 태그들을 모두 제거하겠다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install wordcloud</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from wordcloud import WordCloud</span><br><span class="line">cloud = WordCloud(width=800, height=600).generate(<span class="string">' '</span>.join(train_data[<span class="string">'review'</span>]))</span><br><span class="line">plt.figure(figsize=(20, 15))</span><br><span class="line">plt.imshow(cloud)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure><p><img src="/image/IMDB_wordcloud.png" alt="학습데이터에 대한 wordcloud"></p><ul><li><p>이제 각 라벨의 분포를 확인해 본다. 해당 데이터의 경우 긍정과 부정이라는 두 가지 라벨만 가지고 있다. 분포의 경우 또 다른 시각화 도구인 seaborn을 사용해 시각화하겠다.</p></li><li><p>label의 분포 그래프를 보면 거의 동일한 개수로 분포돼 있음을 확인 할 수 있다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig, axe = plt.subplots(ncols=1)</span><br><span class="line">fig.set_size_inches(6, 3)</span><br><span class="line">sns.countplot(train_data[<span class="string">'sentiment'</span>])</span><br></pre></td></tr></table></figure><p><img src="/image/IMDB_count_plot.png" alt="긍정 부정에 대한 count plot"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"긍정 리뷰 개수: &#123;&#125;"</span>.format(train_data[<span class="string">'sentiment'</span>].value_counts()[1]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"부정 리뷰 개수: &#123;&#125;"</span>.format(train_data[<span class="string">'sentiment'</span>].value_counts()[0]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># 긍정 리뷰 개수: 12500</span></span><br><span class="line"><span class="comment"># 부정 리뷰 개수: 12500</span></span><br></pre></td></tr></table></figure><ul><li>각 리뷰를 단어 기준으로 나눠서 각 리뷰당 단어의 개수를 확인해 본다. 단어는 띄어쓰기 기준으로 하나의 단어라 생각하고 개수를 계산한다. 우선 각 단어의 길이를 가지는 변수를 하나 설정하자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_word_counts = train_data[<span class="string">'review'</span>].apply(lambda x: len(x.split(<span class="string">' '</span>)))</span><br></pre></td></tr></table></figure><h4 id="대부분의-단어가-1000개-미만의-단어를-가지고-있고-대부분-200개-정도의-단어를-가지고-있음을-확인할-수-있다"><a href="#대부분의-단어가-1000개-미만의-단어를-가지고-있고-대부분-200개-정도의-단어를-가지고-있음을-확인할-수-있다" class="headerlink" title="대부분의 단어가 1000개 미만의 단어를 가지고 있고, 대부분 200개 정도의 단어를 가지고 있음을 확인할 수 있다."></a>대부분의 단어가 1000개 미만의 단어를 가지고 있고, 대부분 200개 정도의 단어를 가지고 있음을 확인할 수 있다.</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(15,10))</span><br><span class="line">plt.hist(train_word_counts, bins=50, facecolor=<span class="string">'r'</span>, label=<span class="string">'train'</span>)</span><br><span class="line">plt.title(<span class="string">'Log-Histogram of word count in review'</span>, fontsize=15)</span><br><span class="line">plt.yscale(<span class="string">'log'</span>, nonposy=<span class="string">'clip'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'Number of words'</span>, fontsize=15)</span><br><span class="line">plt.ylabel(<span class="string">'Number of reviews'</span>, fontsize=15)</span><br></pre></td></tr></table></figure><p><img src="/image/word_count_histogram.png" alt="공백을 기준으로 분리한 각 review가 갖는 단어 수에 대한 histogram"></p><h4 id="review의-75-가-300개-이하의-단어를-가지고-있음을-확인-할-수-있다"><a href="#review의-75-가-300개-이하의-단어를-가지고-있음을-확인-할-수-있다" class="headerlink" title="review의 75%가 300개 이하의 단어를 가지고 있음을 확인 할 수 있다."></a>review의 75%가 300개 이하의 단어를 가지고 있음을 확인 할 수 있다.</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_word_counts.describe()</span><br></pre></td></tr></table></figure><ul><li><p>마지막으로 각 review에 대해 구두점과 대소문자 비율 값을 확인한다.</p></li><li><p>대부분 마침표를 포함하고 있고, 대문자도 대부분 사용하고 있다. 따라서 전처리 과정에서 대문자의 경우 모두 소문자로 바꾸고 특수 문자의 경우 제거한다. <code>이 과정은 학습에 방해가 되는 요소들을 제거하기 위함</code>이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 물음표가 구두점으로 사용되는 비율</span></span><br><span class="line">qmarks = np.mean(train_data[<span class="string">'review'</span>].apply(lambda x : <span class="string">'?'</span> <span class="keyword">in</span> x))</span><br><span class="line"><span class="comment"># 마침표가 구두점으로 사용되는 비율</span></span><br><span class="line">fullstop = np.mean(train_data[<span class="string">'review'</span>].apply(lambda x : <span class="string">'.'</span> <span class="keyword">in</span> x))</span><br><span class="line"><span class="comment"># 첫 번째 대문자의 비율</span></span><br><span class="line">capital_first = np.mean(train_data[<span class="string">'review'</span>].apply(lambda x : x[0].isupper()))</span><br><span class="line"><span class="comment"># 대문자 비율</span></span><br><span class="line">capitals = np.mean(train_data[<span class="string">'review'</span>].apply(lambda x : max([y.isupper() <span class="keyword">for</span> y <span class="keyword">in</span> x])))</span><br><span class="line"><span class="comment"># 숫자 비율</span></span><br><span class="line">numbers = np.mean(train_data[<span class="string">'review'</span>].apply(lambda x : max([y.isdigit() <span class="keyword">for</span> y <span class="keyword">in</span> x])))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'물음표가 있는 질문: &#123;:.2f&#125;%'</span>.format(qmarks * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'마침표가 있는 질문: &#123;:.2f&#125;%'</span>.format(fullstop * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'첫 글자가 대문자인 질문: &#123;:.2f&#125;%'</span>.format(capital_first * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'대문자가 있는 질문: &#123;:.2f&#125;%'</span>.format(capitals * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'숫자가 있는 질문: &#123;:.2f&#125;%'</span>.format(numbers * 100))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># 물음표가 있는 질문: 29.55%</span></span><br><span class="line"><span class="comment"># 마침표가 있는 질문: 99.69%</span></span><br><span class="line"><span class="comment"># 첫 글자가 대문자인 질문: 0.00%</span></span><br><span class="line"><span class="comment"># 대문자가 있는 질문: 99.59%</span></span><br><span class="line"><span class="comment"># 숫자가 있는 질문: 56.66%</span></span><br></pre></td></tr></table></figure><h3 id="데이터-전처리"><a href="#데이터-전처리" class="headerlink" title="데이터 전처리"></a>데이터 전처리</h3><ul><li><p>데이터를 모델에 적용할 수 있도록 데이터 전처리를 진행한다. 먼저 데이터 전처리 과정에서 사용할 라이브러리들을 불러올 것이다.</p></li><li><p>우선 전처리를 위해 nltk의 stopword를 이용하기 위해 nltk에서 다운로드를 받아야한다. 필자는 all를 선택하여서 모든 파일을 download를 받았지만, stopword만 받아도 상관없다.</p><ul><li>re, BeautifulSoup : 데이터 정제</li><li>stopwords : 불용어 제거</li><li>pad_sequence, Tokenizer : 데이터 전처리</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import json</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">from nltk.corpus import stopwords</span><br><span class="line">from tensorflow.python.keras.preprocessing.sequence import pad_sequences</span><br><span class="line">from tensorflow.python.keras.preprocessing.text import Tokenizer</span><br><span class="line">import nltk</span><br><span class="line">nltk.download()</span><br></pre></td></tr></table></figure><p><img src="/image/nltk_download.png" alt="nltk 다운로드"></p><ul><li><p>리뷰 데이터를 보면 문장 사이에 <br>과 같은 HTML 태그와 ‘\’, ‘…’ 같은 특수문자가 포함된 것을 확인할 수 있다. <code>문장부호 및 특수문자는 일반적으로 문자의 의미에 크게 영향을 미치지 않기 때문에 최적화된 학습을 위해 제거하자</code></p></li><li><p>BeautifulSoup의 get_text함수를 이용하면 HTML 태그를 제거한 나머지 텍스트를 얻을 수 있고 re.sub을 이용해 특수문자를 제거한다.</p></li><li><p>stopwords(불용어)를 삭제할 것이다. <code>불용어</code>란 <code>문장에서 자주 출현하나 전체적인 의미에 큰 영향을 주지 않는 단어</code>를 말한다. 예를들어, 영어에서는 조사, 관사 등과 같은 어휘가 있다. <code>데이터에 따라 불용어를 제거하는 것은 장단점이 있다.</code> 경우에 따라 불용어가 포함된 데이터를 모델링하는 데 있어 노이즈를 줄 수 있는 요인이 될 수 있어 불용어를 제거하는 것이 좋을 수 있다. 그렇지만 데이터가 많고 문장 구문에 대한 전체적인 패턴을 모델링하고자 한다면 이는 역효과를 줄 수도 있다. 지금 시행하고자 하는 분석은 감성 분석을 하고 있으므로 불용어가 감정 판단에 영향을 주지 않는다고 가정하고 불용어를 제거한다.</p></li><li><p>불용어를 제거하려면 따로 정의한 불용어 사전을 이용해야 한다. <code>사용자가 직접 정의할 수도 있지만 고려해야 하는 경우가 너무 많아서 보통 라이브러리에서 일반적으로 정의해놓은 불용어 사전을 이용</code>한다. NLTK의 불용어 사전을 이용할 것이며, <code>NLTK에서 제공하는 불용어 사전은 전부 소문자 단어로 구성돼 있기 때문에 불용어를 제거하기 위해서는 모든 단어를 소문자로 바꿔야한다.</code></p></li><li><p>review_text를 lower함수를 사용해 모두 소문자로 바꿔주었고, 이후 split 함수를 사용해 공백을 기준으로 reivew_text를 단어 리스트로 바꾼 후 불용어에 해당하지 않는 단어만 다시 모아서 리스트로 만들었다.</p></li><li><p>결과를 보면 단어 리스트가 하나의 문자열로 바뀐 것을 확인할 수 있다.</p></li><li><p>데이터를 한번에 처리하기 위해 위의 과정을 하나의 함수로 작성한다음에 apply로 적용시킨다. 함수의 경우 불용어 제거는 인자값으로 받아서 선택할 수 있게 하였다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def preprocessing(review, remove_stopwords=False):</span><br><span class="line">  <span class="comment"># 불용어 제거는 옵션으로 선택</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1. HTML 태그 제거</span></span><br><span class="line">  review_text = BeautifulSoup(review, <span class="string">'html5lib'</span>).get_text()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2. 영어가 아닌 특수문자를 공백(" ")으로 대체</span></span><br><span class="line">  review_text = re.sub(<span class="string">"[^a-zA-Z]"</span>, <span class="string">" "</span>, review_text)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3. 대문자를 소문자로 바꾸고 공백 단위로 텍스트를 나눠서 리스트로 만든다.</span></span><br><span class="line">  words = review_text.lower().split()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> remove_stopwords:</span><br><span class="line">    <span class="comment"># 4. 불용어 제거</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 영어 불용어 불러오기</span></span><br><span class="line">    stops = <span class="built_in">set</span>(stopwords.words(<span class="string">'english'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 불용어가 아닌 단어로 이뤄진 새로운 리스트 생성</span></span><br><span class="line">    words = [w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> not w <span class="keyword">in</span> stops]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 단어 리스트를 공백을 넣어서 하나의 글로 합친다.</span></span><br><span class="line">    clean_review = <span class="string">' '</span>.join(words)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 불용어를 제거하지 않을 때</span></span><br><span class="line">    clean_review = <span class="string">' '</span>.join(words)</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> clean_review</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">'clean_review'</span>]=train_data[<span class="string">'review'</span>].apply(lambda x : preprocessing(review=x, remove_stopwords=True))</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">'clean_review'</span>][0]</span><br></pre></td></tr></table></figure><p><img src="/image/preprocessing_after_and_before.png" alt="전처리 전후 비교"></p><ul><li>우선 전처리한 데이터에서 각 단어를 인덱스로 벡터화해야 한다. 그리고 모델에 따라 입력값의 길이가 동일해야 하기 때문에 일정 길이로 자르고 부족한 부분은 특정값으로 채우는 패딩 과정을 진행해야 한다. <code>하지만 모델에 따라 각 review가 단어들의 인덱스로 구성된 벡터가 아닌 텍스트로 구성돼야 하는 경우도 있다.</code> 따라서 지금까지 전처리한 데이터를 pandas의 DataFrame으로 만들어 두고 이후에 전처리 과정이 모두 끝난 후 전처리한 데이터를 저장할 때 함께 저장하게 한다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from tensorflow.python.keras.preprocessing.sequence import pad_sequences</span></span><br><span class="line"><span class="comment"># from tensorflow.python.keras.preprocessing.text import Tokenizer</span></span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer()</span><br><span class="line">tokenizer.fit_on_texts(train_data[<span class="string">'clean_review'</span>])</span><br><span class="line">text_sequences = tokenizer.texts_to_sequences(train_data[<span class="string">'clean_review'</span>])</span><br></pre></td></tr></table></figure><ul><li>위와 같이 사전에 등록되어진 인덱스로 각 review의 값들이 변경되었음을 확인할 수 있었다. 단어 사전은 앞서 정의한 tokenizer 객체에 word_index 값을 뽑으면 dictionary 형태로 구성되어 있음을 확인 할 수 있다. 또한 <code>단어 사전 뿐만 아니라 전체 단어 개수도 이후 모델에서 사용되기 때문에 저장해 둔다.</code></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">word_vocab = tokenizer.word_index</span><br><span class="line"><span class="built_in">print</span>(word_vocab)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"전체 단어 개수:"</span>, len(word_vocab))</span><br></pre></td></tr></table></figure><p><img src="/image/word_vocabulary.png" alt="단어 사전"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data_configs = &#123;&#125;</span><br><span class="line"></span><br><span class="line">data_configs[<span class="string">'vocab'</span>] = word_vocab</span><br><span class="line"></span><br><span class="line">data_configs[<span class="string">'vocab_size'</span>] = len(word_vocab) + 1</span><br></pre></td></tr></table></figure><ul><li><p>현재 각 데이터는 서로 길이가 다른데 이 길이를 하나로 통일해야 이후 모델에 바로 적용할 수 있기 때문에 특정 길이를 최대 길이로 정하고 더 긴 데이터의 경우 뒷부분을 자르고 짧은 데이터의 경우에는 0 값으로 패딩하는 작업을 진행한다.</p></li><li><p>패딩 처리에는 앞서 불러온 pad_sequences 함수를 사용한다. 이 함수를 사용할 때는 인자로 패딩을 적용할 데이터, 최대 길이값, 0 값을 데이터 앞에 넣을지 뒤에 넣을 지 여부를 설정한다. 또한, <code>제일 마지막 단어부터 단어를 카운트한다는 것에 유의하자.</code> 여기서 최대 길이를 174로 설정했는데, 이는 앞서 <code>데이터 분석 과정에서 단어 개수의 통계를 계산했을 때 나왔던 중앙값(median)이다. 보통 평균이 아닌 중앙값(median)을 사용하는 경우가 많은데, 평균은 이상치에 민감하기 때문이다.</code></p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 문장 최대 길이</span></span><br><span class="line">MAX_SEQUENCE_LENGTH = 174</span><br><span class="line"></span><br><span class="line"><span class="comment"># padding을 뒷부분에 한다.</span></span><br><span class="line">train_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=<span class="string">'post'</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Shape of train data: '</span>, train_inputs.shape)</span><br></pre></td></tr></table></figure><ul><li>마지막으로 학습 시 label 값을 넘파이 배열로 저장한다. 그 이유는 이후 전처리한 데이터를 저장할 때 넘파이 형태로 저장하기 때문이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_labels = np.array(train_data[<span class="string">'sentiment'</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Shape of label tensor: '</span>, train_labels.shape)</span><br></pre></td></tr></table></figure><ul><li><p>이제 전처리한 데이터를 이후 모델링 과정에서 사용하기 위해 저장할 것이다. 여기서는 다음과 같은 총 4개의 데이터를 저장할 것이다. 텍스트 데이터의 경우 CSV 파일로 저장하고, 벡터화한 데이터와 정답 라벨의 경우 넘파이 파일로 저장한다. 마지막 데이터 정보의 경우 dictionary 형태이기 때문에 Json 파일로 저장한다.</p><ul><li>정제된 텍스트 데이터</li><li>벡터화한 데이터</li><li>정답 라벨</li><li>데이터 정보</li></ul></li><li><p>우선 경로와 파일명을 설정하고 os 라이브러리를 통해 폴더가 없는 경우 폴더를 생성한다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line">TRAIN_INPUT_DATA = <span class="string">'train_input.npy'</span></span><br><span class="line">TRAIN_LABEL_DATA = <span class="string">'train_label.npy'</span></span><br><span class="line">TRAIN_CLEAN_DATA = <span class="string">'train_clean.csv'</span></span><br><span class="line">DATA_CONFIGS = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line"><span class="comment"># 저장하는 디렉터리가 존재하지 않으면 생성</span></span><br><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_IN_PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 전처리된 데이터를 numpy 형태로 저장</span></span><br><span class="line">np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, <span class="string">'wb'</span>), train_inputs)</span><br><span class="line">np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, <span class="string">'wb'</span>), train_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 정제된 텍스트를 CSV 형태로 저장</span></span><br><span class="line">train_data.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index=False)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터 사전을 JSON 형태로 저장</span></span><br><span class="line">json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, <span class="string">'w'</span>), ensure_ascii=False)</span><br></pre></td></tr></table></figure><ul><li>지금까지 학습 데이터에 대해서만 전처리를 했으므로 테스트 데이터에 대해서도 위와 동일한 과정을 진행하면 된다. 다른 점은 평가 데이터의 경우 라벨 값이 없기 때문에 라벨은 따로 저장하지 않아도 되며, <code>데이터 저옵인 단어 사전과 단어 개수에 대한 정보도 학습 데이터의 것을 사용하므로 저장하지 않아도 된다.</code> 추가로 <code>테스트 데이터에 대해 저장해야 하는 값이 있는데 각 review 데이터에 대해 review에 대한 &#39;id&#39;값을 저장</code>해야 한다.</li></ul><hr><h5 id="평가-데이터를-전처리-할-때-한-가지-중요한-점은-Tokenizer를-통해-인덱스-벡터로-만들-때-Tokenizing-객체로-새롭게-만드는-것이-아니라-기존에-학습-데이터에-적용한-Tokenizer-객체를-사용해야-한다는-것이다-만약-새롭게-만들-경우-학습-데이터와-평가-데이터에-대한-각-단어의들의-인덱스가-달라져서-모델에-정상적으로-적용할-수-없기-때문이다-fit-on-texts-fit-on-sequences는-사전을-업데이트하는-행위인데-아래의-코드에서-fit-on-texts를-실행하지-않는-이유는-Tokenizer-객체를-새로-생성하지-않았기에-Train-data의-사전을-갖고-만약에-Train-data에-포함되어-있지-않은-단어가-Test-data에-존재한다면-확률을-0으로-주어야-하기-때문이다"><a href="#평가-데이터를-전처리-할-때-한-가지-중요한-점은-Tokenizer를-통해-인덱스-벡터로-만들-때-Tokenizing-객체로-새롭게-만드는-것이-아니라-기존에-학습-데이터에-적용한-Tokenizer-객체를-사용해야-한다는-것이다-만약-새롭게-만들-경우-학습-데이터와-평가-데이터에-대한-각-단어의들의-인덱스가-달라져서-모델에-정상적으로-적용할-수-없기-때문이다-fit-on-texts-fit-on-sequences는-사전을-업데이트하는-행위인데-아래의-코드에서-fit-on-texts를-실행하지-않는-이유는-Tokenizer-객체를-새로-생성하지-않았기에-Train-data의-사전을-갖고-만약에-Train-data에-포함되어-있지-않은-단어가-Test-data에-존재한다면-확률을-0으로-주어야-하기-때문이다" class="headerlink" title="평가 데이터를 전처리 할 때 한 가지 중요한 점은 Tokenizer를 통해 인덱스 벡터로 만들 때 Tokenizing 객체로 새롭게 만드는 것이 아니라, 기존에 학습 데이터에 적용한 Tokenizer 객체를 사용해야 한다는 것이다. 만약 새롭게 만들 경우 학습 데이터와 평가 데이터에 대한 각 단어의들의 인덱스가 달라져서 모델에 정상적으로 적용할 수 없기 때문이다. fit_on_texts, fit_on_sequences는 사전을 업데이트하는 행위인데 아래의 코드에서 fit_on_texts를 실행하지 않는 이유는 Tokenizer 객체를 새로 생성하지 않았기에 Train data의 사전을 갖고 만약에 Train data에 포함되어 있지 않은 단어가 Test data에 존재한다면 확률을 0으로 주어야 하기 때문이다."></a>평가 데이터를 전처리 할 때 한 가지 중요한 점은 Tokenizer를 통해 인덱스 벡터로 만들 때 Tokenizing 객체로 새롭게 만드는 것이 아니라, 기존에 학습 데이터에 적용한 Tokenizer 객체를 사용해야 한다는 것이다. 만약 새롭게 만들 경우 학습 데이터와 평가 데이터에 대한 각 단어의들의 인덱스가 달라져서 모델에 정상적으로 적용할 수 없기 때문이다. fit_on_texts, fit_on_sequences는 사전을 업데이트하는 행위인데 아래의 코드에서 fit_on_texts를 실행하지 않는 이유는 Tokenizer 객체를 새로 생성하지 않았기에 Train data의 사전을 갖고 만약에 Train data에 포함되어 있지 않은 단어가 Test data에 존재한다면 확률을 0으로 주어야 하기 때문이다.</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_data = pd.read_csv(DATA_IN_PATH + <span class="string">'testData.tsv'</span>, header=0, delimiter=<span class="string">'\t'</span>, quoting=3)</span><br><span class="line"></span><br><span class="line">test_data[<span class="string">'review'</span>] = test_data[<span class="string">'review'</span>].apply(lambda x: preprocessing(x, True))</span><br><span class="line">test_id = np.array(test_data[<span class="string">'id'</span>])</span><br><span class="line"></span><br><span class="line">text_sequences = tokenizer.texts_to_sequences(test_data[<span class="string">'review'</span>])</span><br><span class="line">test_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=<span class="string">'post'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_INPUT_DATA = <span class="string">'test_input.npy'</span></span><br><span class="line">TEST_CLEAN_DATA = <span class="string">'test_clean.csv'</span></span><br><span class="line">TEST_ID_DATA = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">np.save(open(DATA_IN_PATH + TEST_INPUT_DATA, <span class="string">'wb'</span>), test_inputs)</span><br><span class="line">np.save(open(DATA_IN_PATH + TEST_ID_DATA, <span class="string">'wb'</span>), test_id)</span><br><span class="line">test_data.to_csv(DATA_IN_PATH + TEST_CLEAN_DATA, index=False)</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/29/NLP_03/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Scrapy 웹 크롤링 04 - 실습</title>
      <link>https://heung-bae-lee.github.io/2020/01/28/Crawling_03/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/28/Crawling_03/</guid>
      <pubDate>Tue, 28 Jan 2020 05:55:17 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Scrapy-Practice&quot;&gt;&lt;a href=&quot;#Scrapy-Practice&quot; class=&quot;headerlink&quot; title=&quot;Scrapy Practice&quot;&gt;&lt;/a&gt;Scrapy Practice&lt;/h1&gt;&lt;h3 id=&quot;Daum-크롤링하기&quot;&gt;&lt;
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Scrapy-Practice"><a href="#Scrapy-Practice" class="headerlink" title="Scrapy Practice"></a>Scrapy Practice</h1><h3 id="Daum-크롤링하기"><a href="#Daum-크롤링하기" class="headerlink" title="Daum 크롤링하기"></a>Daum 크롤링하기</h3><ul><li><a href="https://news.daum.net/breakingnews/digital" target="_blank" rel="noopener">다음 디지털 뉴스</a> 페이지에서 <code>현재 URL</code>, <code>기사 타이틀에 걸려있는 href URL</code>, 기사 페이지로 이동한 후 <code>기사 제목</code>, <code>기사 내용</code>을 크롤링하는 것을 목표로 크롤러를 만들것</li></ul><h2 id="items-py"><a href="#items-py" class="headerlink" title="items.py"></a>items.py</h2><ul><li>먼저, 크롤링 대상을 items를 활용하기 위해 items.py에 Field를 생성한다. 위에서 언급했던 사항뿐만 아니라 SQLite에 저장할때, 수집된 시간을 로그로 남겨 놓기 위한 Field도 생성시켜 준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line">class PracticeItem(scrapy.Item):</span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 기사 제목</span></span><br><span class="line">    headline = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 기사 본문</span></span><br><span class="line">    contents = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 요청 리스트 페이지</span></span><br><span class="line">    parent_link = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 기사 페이지</span></span><br><span class="line">    article_link = scrapy.Field()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 수집된 시간</span></span><br><span class="line">    crawled_time = scrapy.Field()</span><br></pre></td></tr></table></figure><h2 id="setting-py"><a href="#setting-py" class="headerlink" title="setting.py"></a>setting.py</h2><h3 id="Middlware"><a href="#Middlware" class="headerlink" title="Middlware"></a>Middlware</h3><ul><li><p>개인이 만든 기능을 추가해서 사용가능하게 하는것, 즉, <code>Pipeline은 Item이 Export되어 파일에 저장되기 직전에 작업을 수행한다면, Middleware는 요청하기 직전, 응답 후에, 어떤함수가 실행전에, 이런 중간에서 작업을 수행하는 것</code>이라고 비교해볼 수 있다.</p></li><li><p>다른 크롤링할때와 마찬가지로 동일한 User-agent를 가지고 한다면, 서버에 지속적인 부하를 주게되어서 벤을 당한다거나 할 수 있으므로 <code>Fake User-agent</code>를 활용하여 크롤링을 할 것이다. Middlware의 장점은 커스터마이징할 수 있으므로 다른 사람들이 이미 개발해 놓은 것들이 많다. 특히 Github에서 검색한다면 star가 많은 것을 사용하는 것이 좋다는 것은 누구나 알고 있는 사실!!</p></li><li><p><a href="https://pypi.org/project/scrapy-fake-useragent/" target="_blank" rel="noopener">필자가 사용한 Middleware</a>의 사이트를 보면 scrapy 1.0이상과 1.0미만 버전에 따라 사용방법이 다른 것을 확인할 수 있다. 필자의 경우 1.8이므로 1.0이상의 방법을 사용할 것이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scrapy-fake-useragent</span><br></pre></td></tr></table></figure><ul><li>아래 주석 처리된 USER_AGENT 변수는 실제 자신의 user-agent를 사용해야하며, 주석처리한 이유는 추후에는 서버에 과부하주어 벤당하는 것을 방지하기 위해 fake-agent를 사용할 것이기 때문이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">BOT_NAME = <span class="string">'practice'</span></span><br><span class="line"></span><br><span class="line">SPIDER_MODULES = [<span class="string">'practice.spiders'</span>]</span><br><span class="line">NEWSPIDER_MODULE = <span class="string">'practice.spiders'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># User-agent 설정(개발자도구에서 Network창에서 찾아서 자신의 정보를 복사</span></span><br><span class="line"><span class="comment">#USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = False</span><br><span class="line"></span><br><span class="line"><span class="comment"># 다운로드 간격</span></span><br><span class="line">DOWNLOAD_DELAY = 2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 쿠키사용</span></span><br><span class="line">COOKIES_ENABLED = True</span><br><span class="line"></span><br><span class="line"><span class="comment"># Referer 삽입</span></span><br><span class="line"><span class="comment"># daum은 보안이 엄격하기에 referer속성을 주어야 한다.</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;<span class="string">'Referer'</span> : <span class="string">'https://news.daum.net/breakingnews/digital?page=2'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 재시도 횟수</span></span><br><span class="line">RETRY_ENABLED = True</span><br><span class="line">RETRY_TIME = 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 한글 쓰기(출력 인코딩)</span></span><br><span class="line">FEED_EXPORT_ENCODING = <span class="string">'utf-8'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># User-agent 미들웨어 사용</span></span><br><span class="line">DOWNLOADER_MIDDLEWARES = &#123;</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware'</span>: None,</span><br><span class="line">    <span class="string">'scrapy.downloadermiddlewares.retry.RetryMiddleware'</span>: None,</span><br><span class="line">    <span class="string">'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware'</span>: 400,</span><br><span class="line">    <span class="string">'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware'</span>: 401,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 파이프 라인 활성화</span></span><br><span class="line"><span class="comment"># 숫자가 작을수록 우선순위 상위</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">    <span class="string">'practice.pipelines.PracticePipeline'</span>: 300,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 캐시 사용</span></span><br><span class="line"><span class="comment">#HTTPCACHE_ENABLED = True</span></span><br><span class="line"><span class="comment">#HTTPCACHE_EXPIRATION_SECS = 0</span></span><br><span class="line"><span class="comment">#HTTPCACHE_DIR = 'httpcache'</span></span><br><span class="line"><span class="comment">#HTTPCACHE_IGNORE_HTTP_CODES = []</span></span><br><span class="line"><span class="comment">#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></span><br></pre></td></tr></table></figure><h2 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h2><ul><li>크롤링으로 수집된 시간의 로그를 남기기 위해 datetime 라이브러리를, DB에 저장하기 위해 sqlite3, 마지막으로 예외적인 처리를 위해 DropItem 라이브러리를 사용할 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">import datetime</span><br><span class="line">import sqlite3</span><br><span class="line">from scrapy.exceptions import DropItem</span><br><span class="line"></span><br><span class="line">class PracticePipeline(object):</span><br><span class="line">    <span class="comment"># 초기화 메소드</span></span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="comment"># DB 설정(자동 커밋)</span></span><br><span class="line">        <span class="comment"># isolation_level=None =&gt; Auto Commit</span></span><br><span class="line">        self.conn = sqlite3.connect(<span class="string">'저장할 위치에 대한 path/저장할 파일명.db'</span>, isolation_level=None)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># DB 연결</span></span><br><span class="line">        self.c = self.conn.cursor()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 최초 1회 실행</span></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        spider.logger.info(<span class="string">'NewsSpider Pipeline Started.'</span>)</span><br><span class="line">        self.c.execute(<span class="string">"CREATE TABLE IF NOT EXISTS NEWS_DATA(id INTEGER PRIMARY KEY AUTOINCREMENT, headline text, contents text, parent_link text, article_link text, crawled_time text)"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Item 건수 별 실행</span></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        <span class="keyword">if</span> not item.get(<span class="string">'contents'</span>) is None:</span><br><span class="line">            <span class="comment"># 삽입 시간</span></span><br><span class="line">            crawled_time = datetime.datetime.now().strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 크롤링 시간 필드 추가</span></span><br><span class="line">            item[<span class="string">'crawled_time'</span>] = crawled_time</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 데이터 -&gt; DB 삽입</span></span><br><span class="line">            <span class="comment"># tuple(item[k] for k in item.keys()) 로 대신해도 된다.</span></span><br><span class="line">            self.c.execute(<span class="string">'INSERT INTO NEWS_DATA(headline, contents, parent_link, article_link, crawled_time) VALUES(?, ?, ?, ?, ?);'</span>, (item.get(<span class="string">'headline'</span>), item.get(<span class="string">'contents'</span>), item.get(<span class="string">'parent_link'</span>), item.get(<span class="string">'article_link'</span>), item.get(<span class="string">'crawled_time'</span>))) <span class="comment"># tuple(item[k] for k in item.keys())</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 로그</span></span><br><span class="line">            spider.logger.info(<span class="string">'Item to DB inserted.'</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 결과 리턴</span></span><br><span class="line">            <span class="built_in">return</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            raise DropItem(<span class="string">'Dropped Item. Because This Contents is Empty.'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 마지막 1회 실행</span></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line">        spider.logger.info(<span class="string">'NewsSpider Pipeline Stopped.'</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># commit(auto commit으로 설정했지만 혹시 모르니)</span></span><br><span class="line">        self.conn.commit()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 연결 해제</span></span><br><span class="line">        self.conn.close()</span><br></pre></td></tr></table></figure><h2 id="Spider-py"><a href="#Spider-py" class="headerlink" title="Spider.py"></a>Spider.py</h2><ul><li>먼저 도메인과 시작하는 URL을 정하고나서 페이지의 규칙을 찾아본다. 규칙을 정해주면 LinkExtractor로 반복되는 URL을 보내줄 수 있다. 단, 1자리수 이외의 2자리수부터의 반복은 Rule함수에 follow=True로 주어야한다. 또한 <code>변수명 rules로 Rule객체를 받아야 사용가능함을 기억</code>하자. parent page에서 해당 기사들에 대한 url을 parse_child 함수로 넘겨주는데, 이때 그냥 넘겨주지 않고 <code>parent page에서 얻은 정보 또한 meta parameter를 통해 같이 넘겨 줄수 있다</code>.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from scrapy.linkextractors import LinkExtractor</span><br><span class="line">from scrapy.spiders import CrawlSpider, Rule</span><br><span class="line">import sys</span><br><span class="line">sys.path.insert(0, <span class="string">'items.py가 있는 절대 path'</span>)</span><br><span class="line">from items import PracticeItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class NewcralSpider(CrawlSpider):</span><br><span class="line">    name = <span class="string">'newcral'</span></span><br><span class="line">    allowed_domains = [<span class="string">'news.daum.net'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://news.daum.net/breakingnews/digital'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 링크 크롤링 규칙(정규표현식 사용 추천)</span></span><br><span class="line">    <span class="comment"># page=\d$ : 1자리 수</span></span><br><span class="line">    <span class="comment"># page=\d+ : 연속, follow=True</span></span><br><span class="line">    rules = [</span><br><span class="line">        Rule(LinkExtractor(allow=r<span class="string">'/breakingnews/digital\?page=\d+'</span>), callback=<span class="string">'parse_parent'</span>, follow=True),</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    def parse_parent(self, response):</span><br><span class="line">        <span class="comment"># 부모 URL 로깅</span></span><br><span class="line">        self.logger.info(<span class="string">'Parent Response URL : %s'</span> % response.url)</span><br><span class="line">        <span class="keyword">for</span> url <span class="keyword">in</span> response.css(<span class="string">'ul.list_news2.list_allnews &gt; li &gt; div'</span>):</span><br><span class="line">            <span class="comment"># URL 신문 기사 URL</span></span><br><span class="line">            article_link = url.css(<span class="string">'strong &gt; a::attr(href)'</span>).extract_first().strip()</span><br><span class="line">            yield scrapy.Request(article_link, self.parse_child, meta=&#123;<span class="string">'parent_url'</span>: response.url&#125;)</span><br><span class="line"></span><br><span class="line">                def parse_child(self, response):</span><br><span class="line">                    <span class="comment"># 부모, 자식 수신 정보 로깅</span></span><br><span class="line">                    self.logger.info(<span class="string">'----------------------------------------'</span>)</span><br><span class="line">                    self.logger.info(<span class="string">'Response From Parent URL : %s'</span> % response.meta[<span class="string">'parent_url'</span>])</span><br><span class="line">                    self.logger.info(<span class="string">'Child Response URL : %s'</span> % response.url)</span><br><span class="line">                    self.logger.info(<span class="string">'Child Response Status ; %s'</span> % reponse.status)</span><br><span class="line">                    self.logger.info(<span class="string">'----------------------------------------'</span>)</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 요청 리스트 페이지</span></span><br><span class="line">                    parent_link = response.meta[<span class="string">'parent_url'</span>]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 기사 페이지</span></span><br><span class="line">                    article_link = response.url</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 헤드라인</span></span><br><span class="line">                    headline = response.css(<span class="string">'h3.tit_view::text'</span>).extract_first().strip()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 본문</span></span><br><span class="line">                    c_list = response.css(<span class="string">'div.article_view &gt; section &gt; p::text'</span>).extract()</span><br><span class="line">                    contents = <span class="string">''</span>.join(c_list).strip()</span><br><span class="line"></span><br><span class="line">                    yield PracticeItem(headline=headline, contents=contents, article_link=article_link, parent_link=parent_link)</span><br></pre></td></tr></table></figure><h3 id="도움이-되는-학습"><a href="#도움이-되는-학습" class="headerlink" title="도움이 되는 학습"></a>도움이 되는 학습</h3><ul><li><p>비동기(asyncio), 병렬프로그래밍, 스레드, 멀티 프로세싱등 routine 개념을 학습해야 네트워크상의 블록 또는 논블럭 io로 인해 지연시간이 발생되는데, 제어권을 넘겨주면서 좀 더 성능을 올릴 수 있다.</p></li><li><p><code>scrapy twisted</code>는 예를 들어, 위의 실습에서와 같이 데이터를 크롤링한 후 pipeline을 통해 DB에 insert 할 때 이 작업이 다 끝나지 않으면, 다음 데이터에 대한 작업으로 넘어 갈 수 없어 시간적으로 성능이 떨어질수 있는데, 이런 경우 비동기식으로 처리를 해줌으로써 성능을 올릴수 있게끔 하는 framework이다.</p></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/28/Crawling_03/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Scrapy 웹 크롤링 03 - Exports, Settings, pipeline</title>
      <link>https://heung-bae-lee.github.io/2020/01/24/Crawling_02/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/24/Crawling_02/</guid>
      <pubDate>Thu, 23 Jan 2020 16:56:11 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Exports&quot;&gt;&lt;a href=&quot;#Exports&quot; class=&quot;headerlink&quot; title=&quot;Exports&quot;&gt;&lt;/a&gt;Exports&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;우리가 실행후 크롤링한 데이터를 저장하는 path를 실행할때마다 지정하거나
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Exports"><a href="#Exports" class="headerlink" title="Exports"></a>Exports</h2><ul><li><p>우리가 실행후 크롤링한 데이터를 저장하는 path를 실행할때마다 지정하거나 실행했는데, 일종의 template같이 미리 만들어 놓을 수 있는 기능이 <code>Exports</code>이다.</p></li><li><p>Exports 참조 사이트 : <a href="https://docs.scrapy.org/en/latest/topics/feed-exports.html" target="_blank" rel="noopener">https://docs.scrapy.org/en/latest/topics/feed-exports.html</a></p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 아래 2가지 방법은 동일한 방법이다.</span></span><br><span class="line">scrapy runspider using_items.py -o test.json -t json</span><br><span class="line"></span><br><span class="line">scrapy runspider using_items.py --output test.json --output-format json</span><br></pre></td></tr></table></figure><ul><li><p>위에서와 같이 크롤링을 할 경우에 명령어를 통해 결과의 형식과 파일 이름을 지정해주는 것과 다르게 <code>Settings.py에서 미리 지정하여 사용할 수 있다.</code> 커맨드라인에서도 가능하지만, 모든 테스트를 다 거친 후 확정적으로 사용할 것이라면, settings.py에서 변수설정을 하는 것이 더 좋다.</p></li><li><p>우리가 크롤링할 사이트 <a href="https://globalvoices.org/" target="_blank" rel="noopener">https://globalvoices.org/</a> 사이트의 기사들의 제목만을 크롤링 할 것이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">import scrapy</span><br><span class="line"></span><br><span class="line"><span class="comment"># Scrapy 환경설정</span></span><br><span class="line"><span class="comment"># 중요</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 실행방법</span></span><br><span class="line"><span class="comment"># 1.커맨드 라인 실행 -&gt; scrapy crawl 크롤러명 -s(--set) &lt;NAME&gt;=&lt;VALUE&gt;</span></span><br><span class="line"></span><br><span class="line">class ScrapyWithSettingsSpider(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'scrapy_with_settings'</span></span><br><span class="line">    allowed_domains = [<span class="string">'globalvoices.org'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://globalvoices.org/'</span>]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        <span class="comment"># 아래 3가지는 동일한 결과를 보여주는 코드</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># response.css('#main &gt; div.post-archive-container &gt; div#post-archive div.dategroup div.post-excerpt-container &gt; h3 &gt; a::text').getall()</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># response.xpath('//*[@id="post-archive"]//div[@class="dategroup"]//div[@class="post-summary-content"]//div[@class="post-excerpt-container"]/h3/a/text()').getall()</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># xpath + css 혼합</span></span><br><span class="line">        <span class="keyword">for</span> i, v <span class="keyword">in</span> enumerate(response.xpath(<span class="string">'//div[@class="post-summary-content"]'</span>).css(<span class="string">'div.post-excerpt-container &gt; h3 &gt; a::text'</span>).extract(),1):</span><br><span class="line">            <span class="comment"># 인덱스 번호, 헤드라인</span></span><br><span class="line">            yield dict(num=i, headline=v)</span><br></pre></td></tr></table></figure><p><img src="/image/spider_result_01.png" alt="최종 spider"></p><h3 id="settings-py에서-export하는-변수-설정"><a href="#settings-py에서-export하는-변수-설정" class="headerlink" title="settings.py에서 export하는 변수 설정"></a>settings.py에서 export하는 변수 설정</h3><ul><li>settings.py에 아래와 같이 필요한 변수를 추가로 설정하면된다.</li><li><a href="https://docs.scrapy.org/en/latest/topics/feed-exports.html#settings" target="_blank" rel="noopener">저장소, 저장 형식 관련 레퍼런스</a></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 출력(Exports)설정</span></span><br><span class="line"><span class="comment"># 파일이름 및 경로</span></span><br><span class="line"><span class="comment"># 만약 다른 특정 위치를 지정하고 싶다면 가능하다.</span></span><br><span class="line">FEED_URI = <span class="string">'result.json'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 파일 형식</span></span><br><span class="line">FEED_FORMAT = <span class="string">'json'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 출력 인코딩</span></span><br><span class="line">FEED_EXPORT_ENCODING = <span class="string">'utf-8'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 기본 들여쓰기</span></span><br><span class="line">FEED_EXPORT_INDENT = 2</span><br></pre></td></tr></table></figure><ul><li>또한, 동일한 자원을 반복해서 크롤링할 경우 서버에 과부하를 주는 것을 막기 위해 cache를 사용할 수도 있다. 이 또한, setting.py에서 변수를 설정할 수 있다.</li></ul><p><img src="/image/settings_py_variable_01.png" alt="setting.py에서 설정가능한 변수들에 대한 설명 - 01"></p><p><img src="/image/settings_py_variable_02.png" alt="setting.py에서 설정가능한 변수들에 대한 설명 - 02"></p><h2 id="pipeline"><a href="#pipeline" class="headerlink" title="pipeline"></a>pipeline</h2><ul><li><a href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html" target="_blank" rel="noopener">참고</a></li><li><p>pipeline은 item들이 최종적으로 나오는 파일을 만들기 전에 약간의 처리를 해주는 작업이라고 생각하면된다. 물론 spider에서도 가능하지만 장기적으로 코드 관리적인 면을 봤을 때 너무 좋지 않은 방식이다. 예를 들어, 크롤러를 만들었던 사이트의 구조가 바뀌었다면 한 python script에 모든 코드를 작성한다면 변경된 사이트의 구조에 맞춰 코드를 변경하려면 코드를 해석하는데 오랜시간을 투자해야 할 것이다.</p><ul><li>Item pipeline의 전형적인 예시<ul><li>HTML data 제거</li><li>정확하지 않은 데이터(또는 동일 데이터)가 수집되었다면 출력 전 pipeline단계에서 validation을 할 수 있다.</li><li>중복 체크</li><li>데이터베이스에 저장</li></ul></li></ul></li></ul><p><img src="/image/scrapy_item_pipeline.png" alt="scrapy item pipeline"></p><ul><li>pipeline 사용을 위한 새로운 크롤링 사이트 : <a href="https://www.alexa.com/topsites" target="_blank" rel="noopener">https://www.alexa.com/topsites</a></li><li>자신의 사이트 방문 50위 사이트를 알 수 있는 웹사이트이다.</li><li>사이트 순위, 사이트 명, 하루에 방문하는 평균 시간, 하루에 방문하는 평균 페이지뷰수를 크롤링할 것이다. items를 사용할 것이며, 모든 정보를 크롤링한후 pipeline을 통해 40위권안의 순위에 해당하는 데이터만 저장하는 방식으로 코드를 작성할 것이다.</li></ul><p><img src="/image/item_change.png" alt="새로운 크롤링을 위한 item.py 변경"></p><ul><li>setting.py에서 아래 그림과 같이 pipeline을 사용하기 위해 주석을 풀어주어서 사용할 것이다. 만약 아래에서와 다르게 <code>여러개의 pipeline을 사용한다면 숫자가 낮을 수록 우선 순위를 갖는다는 점에 유의</code>하자.</li></ul><p><img src="/image/setting_py_change_for_pipelines.png" alt="settings.py에서 pipeline을 사용하기위한 변경사항"></p><ul><li>spider를 다음과 같이 구성하였다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line">import scrapy</span><br><span class="line">import sys</span><br><span class="line"><span class="comment"># items.py에 대한 path 추가</span></span><br><span class="line">sys.path.insert(0, <span class="string">'../project/chat_bot_project/section01_2/section01_2'</span>)</span><br><span class="line">from items import SiteRankItems</span><br><span class="line"></span><br><span class="line">class Pipeline01Spider(scrapy.Spider):</span><br><span class="line">    name = <span class="string">'pipeline_01'</span></span><br><span class="line">    allowed_domains = [<span class="string">'alexa.com/topsites'</span>]</span><br><span class="line">    start_urls = [<span class="string">'https://www.alexa.com/topsites'</span>]</span><br><span class="line"></span><br><span class="line">    def parse(self, response):</span><br><span class="line">        <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        :param :response</span></span><br><span class="line"><span class="string">        : return : SiteRankItems</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> response.css(<span class="string">'div.listings.table &gt; div.tr.site-listing'</span>):</span><br><span class="line">            <span class="comment"># 아이템 객체 생성</span></span><br><span class="line">            item = SiteRankItems()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 순위</span></span><br><span class="line">            item[<span class="string">'rank_num'</span>] = p.xpath(<span class="string">'./div[1]/text()'</span>).get()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 사이트명</span></span><br><span class="line">            item[<span class="string">'site_name'</span>] = p.xpath(<span class="string">'./div[2]/p/a/text()'</span>).get()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 평균 접속 시간</span></span><br><span class="line">            item[<span class="string">'daily_time_site'</span>] = p.xpath(<span class="string">'./div[3]/p/text()'</span>).get()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 평균 본 횟수</span></span><br><span class="line">            item[<span class="string">'daily_page_view'</span>] = p.xpath(<span class="string">'./div[4]/p/text()'</span>).get()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            yield item</span><br><span class="line">~</span><br></pre></td></tr></table></figure><ul><li>위의 코드를 통해 크롤링한 데이터를 이제 pipeline을 통해 처리해보자. 간단히 csv파일과 엑셀파일을 만들어 볼 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">class Section012Pipeline(object):</span><br><span class="line">    <span class="comment"># 초기화 method</span></span><br><span class="line">    <span class="comment"># init method도 class가 초기화될 때 최초로 실행되므로 open_spider와 동일하게 사용가능</span></span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="comment"># 엑셀 처리 선언</span></span><br><span class="line">        self.workbook = xlsxwriter.Workbook(<span class="string">"../chat_bot_project/section01_2/section01_2/spiders/result_excel.xlsx"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># CSV 처리 선언 (a, w 옵션 변경)</span></span><br><span class="line">        self.file_opener = open(<span class="string">"../chat_bot_project/section01_2/section01_2/spiders/result_csv.csv"</span>, <span class="string">"w"</span>)</span><br><span class="line">        self.csv_writer = csv.DictWriter(self.file_opener, fieldnames=[<span class="string">'rank_num'</span>,<span class="string">'site_name'</span>,<span class="string">'daily_time_site'</span>,<span class="string">'daily_page_view'</span>,<span class="string">'is_pass'</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 워크시트</span></span><br><span class="line">        self.worksheet = self.workbook.add_worksheet()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 삽입 수</span></span><br><span class="line">        self.rowcount = 1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 최초 1회 실행</span></span><br><span class="line">    def open_spider(self, spider):</span><br><span class="line">        spider.logger.info(<span class="string">"TestSpider Pipelines Started."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 데이터를 크롤링할때 매번실행</span></span><br><span class="line">    def process_item(self, item, spider):</span><br><span class="line">        <span class="comment"># 현재 item은 spider에서 item을 활용해서 작성했으므로 dictionary로 되어있다.</span></span><br><span class="line">        <span class="comment"># rank_num이 40위 안에 있는 사이트들만 저장하기 위한 코드</span></span><br><span class="line">        <span class="keyword">if</span> int(item.get(<span class="string">'rank_num'</span>)) &lt; 41 :</span><br><span class="line">            item[<span class="string">'is_pass'</span>] = True</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 엑셀 저장</span></span><br><span class="line">            <span class="comment"># item['rank_num']처럼 접근가능하지만 데이터가 없다면 에러가 발생하므로 아래에서 처럼 get method를 사용하는 것이 좋다.</span></span><br><span class="line">            self.worksheet.write(<span class="string">'A%s'</span> % self.rowcount, item.get(<span class="string">'rank_num'</span> ))</span><br><span class="line">            self.worksheet.write(<span class="string">'B%s'</span> % self.rowcount, item.get(<span class="string">'site_name'</span> ))</span><br><span class="line">            self.worksheet.write(<span class="string">'C%s'</span> % self.rowcount, item.get(<span class="string">'daily_tiem_site'</span> ))</span><br><span class="line">            self.worksheet.write(<span class="string">'D%s'</span> % self.rowcount, item.get(<span class="string">'daily_page_view'</span> ))</span><br><span class="line">            self.worksheet.write(<span class="string">'E%s'</span> % self.rowcount, item.get(<span class="string">'is_pass'</span> ))</span><br><span class="line">            self.rowcount += 1</span><br><span class="line"></span><br><span class="line">            <span class="comment"># csv 저장</span></span><br><span class="line">            self.csv_writer.writerow(item)</span><br><span class="line"></span><br><span class="line">            <span class="built_in">return</span> item</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            raise DropItem(f<span class="string">'Dropped Item. Because This Site Rank is &#123;item.get("rank_num")&#125;'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 마지막 1회 실행</span></span><br><span class="line">    def close_spider(self, spider):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 엑셀 파일 닫기</span></span><br><span class="line">        self.workbook.close()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># CSV 파일 닫기</span></span><br><span class="line">        self.file_opener.close()</span><br><span class="line"></span><br><span class="line">        spider.logger.info(<span class="string">"TestSpider Pipelines Finished"</span>)</span><br></pre></td></tr></table></figure><p><img src="/image/result_excel.png" alt="결과파일"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/24/Crawling_02/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Attention mechanism을 사용한 Seq2seq 구현</title>
      <link>https://heung-bae-lee.github.io/2020/01/22/deep_learning_11/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/22/deep_learning_11/</guid>
      <pubDate>Tue, 21 Jan 2020 20:15:09 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Vallina-Seq2seq&quot;&gt;&lt;a href=&quot;#Vallina-Seq2seq&quot; class=&quot;headerlink&quot; title=&quot;Vallina Seq2seq&quot;&gt;&lt;/a&gt;Vallina Seq2seq&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;tf.functi
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Vallina-Seq2seq"><a href="#Vallina-Seq2seq" class="headerlink" title="Vallina Seq2seq"></a>Vallina Seq2seq</h1><ul><li><p>tf.function을 사용하기 위해 tensorflow 2.0.0-beta1버전을 설치한다.</p></li><li><p>한글 텍스트의 형태소분석을 위해 konlpy에서 Okt(Original Korean tag, Twitter에서 공개한 오픈소스 라이브러리)를 사용하기 위해 설치해준다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">!pip install tensorflow==2.0.0-beta1</span><br><span class="line">!pip install konlpy</span><br></pre></td></tr></table></figure><ul><li>필요한 라이브러리 import</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import random</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from konlpy.tag import Okt</span><br></pre></td></tr></table></figure><ul><li>tensorflow 버전이 맞는지 확인</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br></pre></td></tr></table></figure><h3 id="하이퍼-파라미터-설정"><a href="#하이퍼-파라미터-설정" class="headerlink" title="하이퍼 파라미터 설정"></a>하이퍼 파라미터 설정</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">EPOCHS = 200</span><br><span class="line"><span class="comment"># 가장 많이 사용된 2000개를 사용하기 위해</span></span><br><span class="line">NUM_WORDS = 2000</span><br></pre></td></tr></table></figure><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">class Encoder(tf.keras.Model):</span><br><span class="line">  def __init__(self):</span><br><span class="line">    super(Encoder, self).__init__()</span><br><span class="line">    <span class="comment"># 2000개의 단어들을 64크기의 vector로 Embedding해줌.</span></span><br><span class="line">    self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)</span><br><span class="line">    <span class="comment"># return_state는 return하는 Output에 최근의 state를 더해주느냐에 대한 옵션</span></span><br><span class="line">    <span class="comment"># 즉, Hidden state와 Cell state를 출력해주기 위한 옵션이라고 볼 수 있다.</span></span><br><span class="line">    <span class="comment"># default는 False이므로 주의하자!</span></span><br><span class="line">    self.lstm = tf.keras.layers.LSTM(512, return_state=True)</span><br><span class="line"></span><br><span class="line">  def call(self, x, training=False, mask=None):</span><br><span class="line">    x = self.emb(x)</span><br><span class="line">    _, h, c = self.lstm(x)</span><br><span class="line">    <span class="built_in">return</span> h, c</span><br></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class Decoder(tf.keras.Model):</span><br><span class="line">  def __init__(self):</span><br><span class="line">    super(Decoder, self).__init__()</span><br><span class="line">    self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)</span><br><span class="line">    <span class="comment"># return_sequence는 return 할 Output을 full sequence 또는 Sequence의 마지막에서 출력할지를 결정하는 옵션</span></span><br><span class="line">    <span class="comment"># False는 마지막에만 출력, True는 모든 곳에서의 출력</span></span><br><span class="line">    self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)</span><br><span class="line">    self.dense = tf.keras.layers.Dense(NUM_WORDS, activation=<span class="string">'softmax'</span>)</span><br><span class="line"></span><br><span class="line">  def call(self, inputs, training=False, mask=None):</span><br><span class="line">    x, h, c = inputs</span><br><span class="line">    x = self.emb(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initial_state는 셀의 첫 번째 호출로 전달 될 초기 상태 텐서 목록을 의미</span></span><br><span class="line">    <span class="comment"># 이전의 Encoder에서 만들어진 Hidden state와 Cell state를 입력으로 받아야 하므로</span></span><br><span class="line">    x, h, c = self.lstm(x, initial_state=[h, c])</span><br><span class="line">    <span class="built_in">return</span> self.dense(x), h, c</span><br></pre></td></tr></table></figure><h3 id="Seq2seq"><a href="#Seq2seq" class="headerlink" title="Seq2seq"></a>Seq2seq</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">class Seq2seq(tf.keras.Model):</span><br><span class="line">  def __init__(self, sos, eos):</span><br><span class="line">    super(Seq2seq, self).__init__()</span><br><span class="line">    self.enc = Encoder()</span><br><span class="line">    self.dec = Decoder()</span><br><span class="line">    self.sos = sos</span><br><span class="line">    self.eos = eos</span><br><span class="line"></span><br><span class="line">  def call(self, inputs, training=False, mask=None):</span><br><span class="line">    <span class="keyword">if</span> training is True:</span><br><span class="line">      <span class="comment"># 학습을 하기 위해서는 우리가 입력과 출력 두가지를 다 알고 있어야 한다.</span></span><br><span class="line">      <span class="comment"># 출력이 필요한 이유는 Decoder단의 입력으로 shited_ouput을 넣어주게 되어있기 때문이다.</span></span><br><span class="line">      x, y = inputs</span><br><span class="line"></span><br><span class="line">      <span class="comment"># LSTM으로 구현되었기 때문에 Hidden State와 Cell State를 출력으로 내준다.</span></span><br><span class="line">      h, c = self.enc(x)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Hidden state와 cell state, shifted output을 초기값으로 입력 받고</span></span><br><span class="line">      <span class="comment"># 출력으로 나오는 y는 Decoder의 결과이기 때문에 전체 문장이 될 것이다.</span></span><br><span class="line">      y, _, _ = self.dec((y, h, c))</span><br><span class="line">      <span class="built_in">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      x = inputs</span><br><span class="line">      h, c = self.enc(x)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Decoder 단에 제일 먼저 sos를 넣어주게끔 tensor화시키고</span></span><br><span class="line">      y = tf.convert_to_tensor(self.sos)</span><br><span class="line">      <span class="comment"># shape을 맞춰주기 위한 작업이다.</span></span><br><span class="line">      y = tf.reshape(y, (1, 1))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 최대 64길이 까지 출력으로 받을 것이다.</span></span><br><span class="line">      seq = tf.TensorArray(tf.int32, 64)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># tf.keras.Model에 의해서 call 함수는 auto graph모델로 변환이 되게 되는데,</span></span><br><span class="line">      <span class="comment"># 이때, tf.range를 사용해 for문이나 while문을 작성시 내부적으로 tf 함수로 되어있다면</span></span><br><span class="line">      <span class="comment"># 그 for문과 while문이 굉장히 효율적으로 된다.</span></span><br><span class="line">      <span class="keyword">for</span> idx <span class="keyword">in</span> tf.range(64):</span><br><span class="line">        y, h, c = self.dec([y, h, c])</span><br><span class="line">        <span class="comment"># 아래 두가지 작업은 test data를 예측하므로 처음 예측한값을 다시 다음 step의 입력으로 넣어주어야하기에 해야하는 작업이다.</span></span><br><span class="line">        <span class="comment"># 위의 출력으로 나온 y는 softmax를 지나서 나온 값이므로</span></span><br><span class="line">        <span class="comment"># 가장 높은 값의 index값을 tf.int32로 형변환해주고</span></span><br><span class="line">        <span class="comment"># 위에서 만들어 놓았던 TensorArray에 idx에 y를 추가해준다.</span></span><br><span class="line">        y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)</span><br><span class="line">        <span class="comment"># 위의 값을 그대로 넣어주게 되면 Dimension이 하나밖에 없어서</span></span><br><span class="line">        <span class="comment"># 실제로 네트워크를 사용할 때 Batch를 고려해서 사용해야 하기 때문에 (1,1)으로 설정해 준다.</span></span><br><span class="line">        y = tf.reshape(y, (1, 1))</span><br><span class="line">        seq = seq.write(idx, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> y == self.eos:</span><br><span class="line">          <span class="built_in">break</span></span><br><span class="line">      <span class="comment"># stack은 그동안 TensorArray로 받은 값을 쌓아주는 작업을 한다.    </span></span><br><span class="line">      <span class="built_in">return</span> tf.reshape(seq.stack(), (1, 64))</span><br></pre></td></tr></table></figure><h3 id="학습-테스트-루프-정의"><a href="#학습-테스트-루프-정의" class="headerlink" title="학습, 테스트 루프 정의"></a>학습, 테스트 루프 정의</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement training loop</span></span><br><span class="line">@tf.function</span><br><span class="line">def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):</span><br><span class="line">  <span class="comment"># output_labels는 실제 output과 비교하기 위함</span></span><br><span class="line">  <span class="comment"># shifted_labels는 Decoder부분에 입력을 넣기 위함</span></span><br><span class="line">  output_labels = labels[:, 1:]</span><br><span class="line">  shifted_labels = labels[:, :-1]</span><br><span class="line">  with tf.GradientTape() as tape:</span><br><span class="line">    predictions = model([inputs, shifted_labels], training=True)</span><br><span class="line">    loss = loss_object(output_labels, predictions)</span><br><span class="line">  gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line"></span><br><span class="line">  optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line">  train_loss(loss)</span><br><span class="line">  train_accuracy(output_labels, predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Implement algorithm test</span></span><br><span class="line">@tf.function</span><br><span class="line">def test_step(model, inputs):</span><br><span class="line">  <span class="built_in">return</span> model(inputs, training=False)</span><br></pre></td></tr></table></figure><h3 id="데이터셋-준비"><a href="#데이터셋-준비" class="headerlink" title="데이터셋 준비"></a>데이터셋 준비</h3><ul><li><a href="http://www.aihub.or.kr" target="_blank" rel="noopener">http://www.aihub.or.kr</a>에서 text데이터 중 AI chatbot 데이터를 사용할 것이다. 이 데이터를 다운받아 필자는 google storage 서비스를 이용해서 기존의 생성해놓았던 버킷을 통해 데이터를 업로드 한 후, 받아와서 사용할 것이다. 이 방법은 google storage에서 파일을 받아 사용하는 gsutil 방식이며 빠르다는 점이 장점이지만 현재 세션이 종료되거나 새로시작할 경우 다시 실행 시켜주어야 하는 방식이다. 또한 필자처럼 google colab이 아닌 자신의 로컬PC로 실행할 경우 아래 단계는 건너 뛰어도 상관없다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from google.colab import auth</span><br><span class="line">auth.authenticate_user()</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line">!gsutil cp gs://kaggle_key/chatbot_data.csv chatbot_data.csv</span><br></pre></td></tr></table></figure><ul><li><p>chatbot_data.csv 파일이 현재 path에 존재하는지 확인</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%ls</span><br></pre></td></tr></table></figure></li><li><p>chatbot_data.csv파일을 pandas DataFrame으로 읽어 어떤 데이터들이 존재하고 추후에 x(Question)와 y(Answer)로 나눠주려면 패턴을 찾아야 하기 때문에 모든 데이터를 볼 것이다. 전체 데이터는 999개이기 떄문에 출력되어지는 row의 수를 1000개로 맞춰준다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.options.display.max_rows = 1000</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chatbot_data = pd.read_csv(<span class="string">'chatbot_data.csv'</span>,header=0)</span><br><span class="line">chatbot_data</span><br></pre></td></tr></table></figure><p><img src="/image/chatbot_data.png" alt="chatbot_data"></p><ul><li>위에서 pandas로 불러들인 QA(Question &amp; Answer) data를 보면 Question과 Answer로 이루어져있다. 즉, 순차적인 데이터인 것이다. 또한 대화의 끝이 나누어져 있지 않아 입력으로 넣어주려면 Data를 Question과 Answer 쌍으로 가공해주어야 할 것이다. 맨처음 줄부터 Question 그다음은 Answer 이순으로 되어있다는 것을 확인할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">dataset_file = <span class="string">'chatbot_data.csv'</span></span><br><span class="line">okt = Okt()</span><br><span class="line"></span><br><span class="line">with open(dataset_file, <span class="string">'r'</span>) as file:</span><br><span class="line">  lines = file.readlines()</span><br><span class="line">  <span class="comment"># okt 라이브러리를 통해 형태소 분석을 한줄씩 진행하였고</span></span><br><span class="line">  <span class="comment"># 나누어진 형태소들을 하나의 sequence로 묶어주기위해</span></span><br><span class="line">  <span class="comment"># 구분자는 공백을 사용해서 join해주었다.</span></span><br><span class="line">  <span class="comment"># 구분자를 space로 한 이유는 나중에 사용할 tokenizer에서 space를 기준으로 단어를 구분하기 때문이다.</span></span><br><span class="line">  seq = [<span class="string">" "</span>.join(okt.morphs(line)) <span class="keyword">for</span> line <span class="keyword">in</span> lines]</span><br><span class="line"></span><br><span class="line">questions = seq[::2]</span><br><span class="line"><span class="comment"># tap은 Decoder단에서 Shifted Output을 입력으로 받을때 시작점을 알려주기 위한 SOS로 tap(\t)을 사용</span></span><br><span class="line">answers = [<span class="string">'\t'</span> + lines <span class="keyword">for</span> lines <span class="keyword">in</span> seq[1::2]]</span><br><span class="line"></span><br><span class="line">num_sample = len(questions)</span><br><span class="line"></span><br><span class="line">perm = list(range(num_sample))</span><br><span class="line">random.seed(0)</span><br><span class="line">random.shuffle(perm)</span><br><span class="line"></span><br><span class="line">train_q = list()</span><br><span class="line">train_a = list()</span><br><span class="line">test_q = list()</span><br><span class="line">test_a = list()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, qna <span class="keyword">in</span> enumerate(zip(questions, answers)):</span><br><span class="line">  q, a = qna</span><br><span class="line">  <span class="keyword">if</span> perm[idx] &gt; num_sample//5:</span><br><span class="line">    train_q.append(q)</span><br><span class="line">    train_a.append(a)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    test_q.append(q)</span><br><span class="line">    test_a.append(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># filters의 default에는 \t,\n도 제거하기 때문에 이 둘을 제외하고 나머지 문장기호들만 제거하게끔 변경해주었다.</span></span><br><span class="line">tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,</span><br><span class="line">                                                  filters=<span class="string">'!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`&#123;|&#125;~'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 시퀀스 목록을 기반으로 내부 어휘를 업데이트한다.</span></span><br><span class="line">tokenizer.fit_on_texts(train_q + train_a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 위에서 업데이트한 어휘를 기반으로 실수형태의 벡터 형태로 나타내 준다.</span></span><br><span class="line"><span class="comment"># 출력을 통해 나타나는 실수는 count의 수를 나타내는 것은 아니다!</span></span><br><span class="line">train_q_seq = tokenizer.texts_to_sequences(train_q)</span><br><span class="line">train_a_seq = tokenizer.texts_to_sequences(train_a)</span><br><span class="line"></span><br><span class="line">test_q_seq = tokenizer.texts_to_sequences(test_q)</span><br><span class="line">test_a_seq = tokenizer.texts_to_sequences(test_a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y값에는 maxlen=65인 이유는 앞에 SOS와 뒤에 EOS가 붙어 있는 상황이므로 학습시에는 앞에 하나를 떼고</span></span><br><span class="line"><span class="comment"># 학습하므로 실제로는 64길이만 사용하는 것과 동일하게 된다.</span></span><br><span class="line">x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq,</span><br><span class="line">                                                        value=0,</span><br><span class="line">                                                        padding=<span class="string">'pre'</span>,</span><br><span class="line">                                                        maxlen=64)</span><br><span class="line"></span><br><span class="line">y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq,</span><br><span class="line">                                                        value=0,</span><br><span class="line">                                                        padding=<span class="string">'post'</span>,</span><br><span class="line">                                                        maxlen=65)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq,</span><br><span class="line">                                                        value=0,</span><br><span class="line">                                                        padding=<span class="string">'pre'</span>,</span><br><span class="line">                                                        maxlen=64)</span><br><span class="line"></span><br><span class="line">y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq,</span><br><span class="line">                                                        value=0,</span><br><span class="line">                                                        padding=<span class="string">'post'</span>,</span><br><span class="line">                                                        maxlen=65)</span><br><span class="line"></span><br><span class="line"><span class="comment"># prefetch(1024)는 GPU에 미리 1024개의 데이터를 미리 fetch하는 기능!</span></span><br><span class="line"><span class="comment"># 근데 batch size도 아니고 왜 1024개??</span></span><br><span class="line">train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)</span><br><span class="line">test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024)</span><br></pre></td></tr></table></figure><h3 id="학습-환경-정의"><a href="#학습-환경-정의" class="headerlink" title="학습 환경 정의"></a>학습 환경 정의</h3><h4 id="모델-생성-손실-함수-최적화-알고리즘-평가지표-정의"><a href="#모델-생성-손실-함수-최적화-알고리즘-평가지표-정의" class="headerlink" title="모델 생성, 손실 함수, 최적화 알고리즘, 평가지표 정의"></a>모델 생성, 손실 함수, 최적화 알고리즘, 평가지표 정의</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 생성</span></span><br><span class="line">model = Seq2seq(sos=tokenizer.word_index[<span class="string">'\t'</span>],</span><br><span class="line">                eos=tokenizer.word_index[<span class="string">'\n'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 손실함수 및 최적화 기법 정의</span></span><br><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">optimizer = tf.keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 성능 지표 정의</span></span><br><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br></pre></td></tr></table></figure><h3 id="학습-루프-동작"><a href="#학습-루프-동작" class="headerlink" title="학습 루프 동작"></a>학습 루프 동작</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCHS):</span><br><span class="line">  <span class="keyword">for</span> seqs, labels <span class="keyword">in</span> train_ds:</span><br><span class="line">    train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)</span><br><span class="line"></span><br><span class="line">  template=<span class="string">'Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy:&#123;&#125;'</span></span><br><span class="line">  <span class="built_in">print</span>(template.format(epoch + 1,</span><br><span class="line">                        train_loss.result(),</span><br><span class="line">                        train_accuracy.result() * 100))</span><br></pre></td></tr></table></figure><p><img src="/image/seq2seq_metric_performence.png" alt="train data 성능"></p><h3 id="테스트-루프"><a href="#테스트-루프" class="headerlink" title="테스트 루프"></a>테스트 루프</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> test_seq, test_labels <span class="keyword">in</span> test_ds:</span><br><span class="line">  prediction = test_step(model, test_seq)</span><br><span class="line">  test_text = tokenizer.sequences_to_texts(test_seq.numpy())</span><br><span class="line">  <span class="comment"># ground_truth</span></span><br><span class="line">  gt_text = tokenizer.sequences_to_texts(test_labels.numpy())</span><br><span class="line">  <span class="comment"># prediction</span></span><br><span class="line">  texts = tokenizer.sequences_to_texts(prediction.numpy())</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">'_'</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">'q: '</span>, test_text)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">'a: '</span>, gt_text)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">'p: '</span>, texts)</span><br></pre></td></tr></table></figure><ul><li>예측된 값들을 보면 train data에 과적합된 것을 충분히 알 수 있을 것이다.</li></ul><p><img src="/image/seq2seq_prediction_result.png" alt="test data를 통한 예측 결과"></p><h3 id="이제-여기서-Attention-mechanism을-적용시켜보자"><a href="#이제-여기서-Attention-mechanism을-적용시켜보자" class="headerlink" title="이제 여기서 Attention mechanism을 적용시켜보자."></a>이제 여기서 Attention mechanism을 적용시켜보자.</h3><h4 id="Encoder-Decoder-Seq2seq-부분을-수정하면된다"><a href="#Encoder-Decoder-Seq2seq-부분을-수정하면된다" class="headerlink" title="Encoder, Decoder, Seq2seq 부분을 수정하면된다."></a>Encoder, Decoder, Seq2seq 부분을 수정하면된다.</h4><h3 id="Encoder-1"><a href="#Encoder-1" class="headerlink" title="Encoder"></a>Encoder</h3><ul><li><p>이전과 다르게 <code>LSTM 구조에서 return_sequences=True를 넣어 전체 Hidden State를 출력하게 해주었다. 이를 Key-Value로 사용할 것이다.</code></p></li><li><p>Embedding 결과의 Dimension : (32(batch_szie), 64(sequence의 길이), 64(Embedding Feature의 수))</p></li><li><p>LSTM의 결과의 Dimension : (32(batch_szie), 64(sequence의 길이), 512(LSTM unit의 갯수))</p><ul><li>H : 32(batch size) <em> 64(sequence의 길이) </em> 512(LSTM unit수)</li><li>h(s0) : 32(batch size) * 512 (LSTM unit수)</li><li>c(c0) : 32(batch size) * 512 (LSTM unit수)<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">class Encoder(tf.keras.Model):</span><br><span class="line">  def __init__(self):</span><br><span class="line">    super(Encoder, self).__init__()</span><br><span class="line">    <span class="comment"># 2000개의 단어들을 64크기의 vector로 Embedding해줌.</span></span><br><span class="line">    self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)</span><br><span class="line">    <span class="comment"># return_state는 return하는 Output에 최근의 state를 더해주느냐에 대한 옵션</span></span><br><span class="line">    <span class="comment"># 즉, Hidden state와 Cell state를 출력해주기 위한 옵션이라고 볼 수 있다.</span></span><br><span class="line">    <span class="comment"># default는 False이므로 주의하자!</span></span><br><span class="line">    <span class="comment"># return_sequence=True로하는 이유는 Attention mechanism을 사용할 때 우리가 key와 value는</span></span><br><span class="line">    <span class="comment"># Encoder에서 나오는 Hidden state 부분을 사용했어야 했다. 그러므로 모든 Hidden State를 사용하기 위해 바꿔준다.</span></span><br><span class="line">    self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)</span><br><span class="line"></span><br><span class="line">  def call(self, x, training=False, mask=None):</span><br><span class="line">    x = self.emb(x)</span><br><span class="line">    H, h, c = self.lstm(x)</span><br><span class="line">    <span class="built_in">return</span> H, h, c</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="Decoder-1"><a href="#Decoder-1" class="headerlink" title="Decoder"></a>Decoder</h3><ul><li><p>LSTM 다음에 Attention 구조를 넣어주고, Encoder의 출력 중 모든 sequence의 Hidden State를 모아놓은 H와 s0, c0, shifted Output을 받아서 Attention value를 구하기 위한 코드를 수정시킨다.</p></li><li><p>Dimension :</p><ul><li>x : shifted_labels로 맨마지막을 제외한 나머지데이터들 =&gt; 32(batch szie) * 64(sequence의 길이)</li><li>s0 : 이전 step의 hidden state =&gt; 32(batch size) * 512(LSTM의 Unit 갯수)</li><li>c0 : 이전 step의 cell state =&gt; 32(batch size) * 512(LSTM의 Unit 갯수)</li><li>H : Encoder단의 모든 Hidden state를 모은 것 =&gt; 32(batch size) <em> 64(sequence의 길이) </em> 512(LSTM의 Feature의 갯수)</li></ul></li><li><p>embedding 결과 =&gt; 32(batch size) <em> 64(sequence의 길이) </em> 64(Embedding Feature의 수)</p></li><li><p>LSTM의 결과의 Dimension : (32(batch_szie), 64(sequence의 길이), 512(LSTM unit의 갯수))</p><ul><li>S : 32(batch size) <em> 64(sequence의 길이) </em> 512(LSTM unit수)</li><li>h : 32(batch size) * 512 (LSTM unit수)</li><li>c : 32(batch size) * 512 (LSTM unit수)</li></ul></li><li><p>S_의 Dimension: 32(batch size) <em> 64(sequence의 길이) </em> 512(LSTM unit 수)</p></li><li>A의 Dimension: 32(batch size) <em> 64(sequence의 길이) </em> 512(LSTM unit 수)</li><li>y의 Dimension: 32(batch size) <em> 64(sequence의 길이) </em> 1024</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">class Decoder(tf.keras.Model):</span><br><span class="line">  def __init__(self):</span><br><span class="line">    super(Decoder, self).__init__()</span><br><span class="line">    self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)</span><br><span class="line">    <span class="comment"># return_sequence는 return 할 Output을 full sequence 또는 Sequence의 마지막에서 출력할지를 결정하는 옵션</span></span><br><span class="line">    <span class="comment"># False는 마지막에만 출력, True는 모든 곳에서의 출력</span></span><br><span class="line">    self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)</span><br><span class="line">    <span class="comment"># LSTM 출력에다가 Attention value를 dense에 넘겨주는 것이 Attention mechanism이므로</span></span><br><span class="line">    self.att = tf.keras.layers.Attention()</span><br><span class="line">    self.dense = tf.keras.layers.Dense(NUM_WORDS, activation=<span class="string">'softmax'</span>)</span><br><span class="line"></span><br><span class="line">  def call(self, inputs, training=False, mask=None):</span><br><span class="line">    <span class="comment"># x : shifted output, s0 : Decoder단의 처음들어오는 Hidden state</span></span><br><span class="line">    <span class="comment"># c0 : Decoder단의 처음들어오는 cell state H: Encoder단의 Hidden state(Key와 value로 사용)</span></span><br><span class="line">    x, s0, c0, H = inputs</span><br><span class="line">    x = self.emb(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initial_state는 셀의 첫 번째 호출로 전달 될 초기 상태 텐서 목록을 의미</span></span><br><span class="line">    <span class="comment"># 이전의 Encoder에서 만들어진 Hidden state와 Cell state를 입력으로 받아야 하므로</span></span><br><span class="line">    <span class="comment"># S : Hidden state를 전부다 모아놓은 것이 될 것이다.(Query로 사용)</span></span><br><span class="line">    S, h, c = self.lstm(x, initial_state=[s0, c0])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Query로 사용할 때는 하나 앞선 시점을 사용해줘야 하므로</span></span><br><span class="line">    <span class="comment"># s0가 제일 앞에 입력으로 들어가는데 현재 Encoder 부분에서의 출력이 batch 크기에 따라서 length가 현재 1이기 때문에 2차원형태로 들어오게 된다.</span></span><br><span class="line">    <span class="comment"># 그러므로 이제 3차원 형태로 확장해 주기 위해서 newaxis를 넣어준다.</span></span><br><span class="line">    <span class="comment"># 또한 decoder의 S(Hidden state) 중에 마지막은 예측할 다음이 없으므로 배제해준다.</span></span><br><span class="line">    S_ = tf.concat([s0[:, tf.newaxis, :], S[:, :-1, :]], axis=1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Attention 적용</span></span><br><span class="line">    <span class="comment"># 아래 []안에는 원래 Query, Key와 value 순으로 입력해야하는데 아래처럼 두가지만 입력한다면</span></span><br><span class="line">    <span class="comment"># 마지막 것을 Key와 value로 사용한다.</span></span><br><span class="line">    A = self.att([S_, H])</span><br><span class="line"></span><br><span class="line">    y = tf.concat([S, A], axis=-1)</span><br><span class="line">    <span class="built_in">return</span> self.dense(y), h, c</span><br></pre></td></tr></table></figure><h3 id="Seq2seq-1"><a href="#Seq2seq-1" class="headerlink" title="Seq2seq"></a>Seq2seq</h3><ul><li>이전의 코드에서 encoder의 출력에 전체 Hidden State를 모아놓은 것과 decoder의 입력으로 이값을 받는 코드를 추가해주었다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">class Seq2seq(tf.keras.Model):</span><br><span class="line">  def __init__(self, sos, eos):</span><br><span class="line">    super(Seq2seq, self).__init__()</span><br><span class="line">    self.enc = Encoder()</span><br><span class="line">    self.dec = Decoder()</span><br><span class="line">    self.sos = sos</span><br><span class="line">    self.eos = eos</span><br><span class="line"></span><br><span class="line">  def call(self, inputs, training=False, mask=None):</span><br><span class="line">    <span class="keyword">if</span> training is True:</span><br><span class="line">      <span class="comment"># 학습을 하기 위해서는 우리가 입력과 출력 두가지를 다 알고 있어야 한다.</span></span><br><span class="line">      <span class="comment"># 출력이 필요한 이유는 Decoder단의 입력으로 shited_ouput을 넣어주게 되어있기 때문이다.</span></span><br><span class="line">      x, y = inputs</span><br><span class="line"></span><br><span class="line">      <span class="comment"># LSTM으로 구현되었기 때문에 Hidden State와 Cell State를 출력으로 내준다.</span></span><br><span class="line">      H, h, c = self.enc(x)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Hidden state와 cell state, shifted output을 초기값으로 입력 받고</span></span><br><span class="line">      <span class="comment"># 출력으로 나오는 y는 Decoder의 결과이기 때문에 전체 문장이 될 것이다.</span></span><br><span class="line">      y, _, _ = self.dec((y, h, c, H))</span><br><span class="line">      <span class="built_in">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      x = inputs</span><br><span class="line">      H, h, c = self.enc(x)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Decoder 단에 제일 먼저 sos를 넣어주게끔 tensor화시키고</span></span><br><span class="line">      y = tf.convert_to_tensor(self.sos)</span><br><span class="line">      <span class="comment"># shape을 맞춰주기 위한 작업이다.</span></span><br><span class="line">      y = tf.reshape(y, (1, 1))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 최대 64길이 까지 출력으로 받을 것이다.</span></span><br><span class="line">      seq = tf.TensorArray(tf.int32, 64)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># tf.keras.Model에 의해서 call 함수는 auto graph모델로 변환이 되게 되는데,</span></span><br><span class="line">      <span class="comment"># 이때, tf.range를 사용해 for문이나 while문을 작성시 내부적으로 tf 함수로 되어있다면</span></span><br><span class="line">      <span class="comment"># 그 for문과 while문이 굉장히 효율적으로 된다.</span></span><br><span class="line">      <span class="keyword">for</span> idx <span class="keyword">in</span> tf.range(64):</span><br><span class="line">        y, h, c = self.dec([y, h, c, H])</span><br><span class="line">        <span class="comment"># 아래 두가지 작업은 test data를 예측하므로 처음 예측한값을 다시 다음 step의 입력으로 넣어주어야하기에 해야하는 작업이다.</span></span><br><span class="line">        <span class="comment"># 위의 출력으로 나온 y는 softmax를 지나서 나온 값이므로</span></span><br><span class="line">        <span class="comment"># 가장 높은 값의 index값을 tf.int32로 형변환해주고</span></span><br><span class="line">        <span class="comment"># 위에서 만들어 놓았던 TensorArray에 idx에 y를 추가해준다.</span></span><br><span class="line">        y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)</span><br><span class="line">        <span class="comment"># 위의 값을 그대로 넣어주게 되면 Dimension이 하나밖에 없어서</span></span><br><span class="line">        <span class="comment"># 실제로 네트워크를 사용할 때 Batch를 고려해서 사용해야 하기 때문에 (1,1)으로 설정해 준다.</span></span><br><span class="line">        y = tf.reshape(y, (1, 1))</span><br><span class="line">        seq = seq.write(idx, y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> y == self.eos:</span><br><span class="line">          <span class="built_in">break</span></span><br><span class="line">      <span class="comment"># stack은 그동안 TensorArray로 받은 값을 쌓아주는 작업을 한다.    </span></span><br><span class="line">      <span class="built_in">return</span> tf.reshape(seq.stack(), (1, 64))</span><br></pre></td></tr></table></figure><p><img src="/image/seq2seq_with_attention_metric_performence.png" alt="Attention mechanism을 사용한 Seq2seq의 train data 성능"></p><p><img src="/image/seq2seq_with_attention_prediction.png" alt="Attention mechanism을 사용한 Seq2seq의 test 결과"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/22/deep_learning_11/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Attention 기법</title>
      <link>https://heung-bae-lee.github.io/2020/01/21/deep_learning_10/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/21/deep_learning_10/</guid>
      <pubDate>Tue, 21 Jan 2020 08:10:39 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Attention-기법&quot;&gt;&lt;a href=&quot;#Attention-기법&quot; class=&quot;headerlink&quot; title=&quot;Attention 기법&quot;&gt;&lt;/a&gt;Attention 기법&lt;/h1&gt;&lt;h3 id=&quot;Sequence-to-sequence&quot;&gt;&lt;a 
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Attention-기법"><a href="#Attention-기법" class="headerlink" title="Attention 기법"></a>Attention 기법</h1><h3 id="Sequence-to-sequence"><a href="#Sequence-to-sequence" class="headerlink" title="Sequence-to-sequence"></a>Sequence-to-sequence</h3><ul><li><p>우선, Attention 기법이 가장 먼저 적용되었던 모델인 Sequence-to-sequence 모델을 살펴보면서 간략하게 conception적인 것을 살펴보겠다.</p></li><li><p>아래 그림의 왼쪽 부분은 <code>Encoder</code> 구조로 되어 있어, Input에 번역하고자 하는 문장을 단어 하나씩 받는 형태로 되어있다. 마지막 입력 단어에는 EOS(End-Of-Sequence)라는 특별한 단어(Token)를 받도록 되어있다. 마지막 EOS까지 받은 뒤의 Output을 Context라고 한다. 그렇게 나온 Context를 <code>Decoder</code> 구조에서 넘겨 받으며, 동시에 SOS(Start-Of-Sequence)라는 Token을 처음 Input에서 입력해주면 넘겨 받은 Context로 부터 첫번째 단어를 생성한다. 생성된 단어를 다음 입력으로 넣어주고 또 다음 단어를 생성하고 이런 작업을 EOS Token이 Output으로 나올때까지 반복해준다.</p></li><li><p>Encoder는 결국 Context를 만들기 위해서 RNN으로 동작하는 구조이고, Decoder는 한단어씩 출력하는데, 그 출력된 Output(단어)을 오른쪽으로 Shift해서 입력으로 받은 뒤 새로운 단어를 만들어주는 구조이다.</p></li></ul><p><img src="/image/Sequence_to_sequence.png" alt="Sequence-to-sequence"></p><p><img src="/image/Seq2seq_model_structures.png" alt="Seq2seq 간단한 모델 구조도"></p><h3 id="영어-문장의-데이터화"><a href="#영어-문장의-데이터화" class="headerlink" title="영어 문장의 데이터화"></a>영어 문장의 데이터화</h3><p><img src="/image/English_data.png" alt="영어 문장의 데이터화"></p><h3 id="한글-문장의-데이터화"><a href="#한글-문장의-데이터화" class="headerlink" title="한글 문장의 데이터화"></a>한글 문장의 데이터화</h3><p><img src="/image/Korean_data.png" alt="한들 문장의 데이터화"></p><ul><li>Seq2seq 모델같은 경우는 <code>입출력간의 step이 너무 멀리 떨어져 있으면 Gradient Vanishing이 일어나 잘 학습되지 않는다.</code> 예를 들어 아래와 같은 그림에서 처음 입력인 $x_{0}$가 $y_{1}$에 영향을 준다면(영어에서 한글로 번역할시 어순이 반대로 되어있는 경우를 예시로 생각하면 이해하기 쉽다.), BPTT로 펼쳐놓고 봤을때, 기본적으로 Encoder단과 Decoder단이 분리가 되어 있기 때문에, Ecoder단의 앞쪽과 Decoder 뒷쪽과은 거리가 멀다. 그렇다면 아무리 LSTM을 쓰고 GRU를 사용한다고 하더라도 한계가 있다.</li></ul><p><img src="/image/Gradient_vanishing_in_RNN.png" alt="RNN에서의  Gradient Vanishing"></p><ul><li>위에서 언급했던 것과 같이 그렇다면 Gradient Vanishing 문제를 해결할 수 있는 방법이 없을까? 그에 대한 답을 아래의 그림을 통해 설명하겠다. 이전에는 Encoder 부분에서 Context 하나(Feature vector 하나)만 넘겨줘서 그 Context를 통해 Decoder가 출력을 내주었어야 했는데, 직관적으로 볼때 Input들을 통해 만든 마지막 Hidden state인 Feature vector가 Encoder단의 모든 Input들을 많은 부분 커버할 수 없으며, Decoder 부분의 RNN을 통해 지나면서 우리가 원하는 방향대로 넘어갈것이라는 보장 또한 없다. 그래서 이제는 <code>Encoder Hidden state를 모아서 Decoder로 각각 전달시켜 줌으로써 각 출력에 필요한 Context를 새로이 뽑을 수 있는 구조로 변경</code>해주어 기울기 소실 문제를 해결할 수 있을 것이다. 그렇다해서 모든 Encoder단의 모든 Hidden state를 다 concate해서 하나의 긴 Feature vector로 넘겨준다면 좋은 성능을 기대하긴 어렵울 것이다. 왜냐하면 무조건 Feature vector의 데이터량을 늘려 준다면, 그만큼 많은 데이터셋을 통해 데이터셋이 cover를 해주어야 하기 때문이다. (자세히 말하면 학습데이터량은 고정적으로 동일한데, Input vector인 Feature vector의 차원만 늘려주게 된다면 Underfitting 문제가 발생하면서 Sparsity 문제가 발생해서 학습이 제대로 동작하지 않기 때문이다.) 이런 구조를 어떻게 효율적으로 구성할 수 있을까에 대한 답이 <code>Attention 메커니즘</code>이다.</li></ul><p><img src="/image/No_more_Gradient_Vanishing.png" alt="Gradient Vanishing을 해결할 수 있는 방법"></p><p><img src="/image/develop_the_context_info.png" alt="Context를 개선하려면?"></p><ul><li>Attention 메커니즘에서는 예를 들어 Encoder 부분과 Decoder 부분의 Layer들이 아래 그림에서 처럼 색깔별로 연관되어 있다고 했을때, 각 연관되어 있는 부분을 알게 해주는 것이 Attention Mechanism의 기본 아이디어이다.</li></ul><p><img src="/image/Attention.png" alt="Attention 메커니즘"></p><h1 id="Attention-신경망"><a href="#Attention-신경망" class="headerlink" title="Attention 신경망"></a>Attention 신경망</h1><p><img src="/image/Query_key_value.png" alt="Query, Key-Value"></p><ul><li><p><code>Dictionary형태로 이루어져 있다는 점은 순서는 상관 없다는 의미임을 명심하자!</code></p></li><li><p>질의-응답이 이루어지는 메커니즘을 살펴보면, query를 날려주게 되면, 우선 key들을 나열하여 동일한 것을 찾아낸다. 그 후에는 key-value쌍에서 동일한 부분의 value를 출력해준다.</p></li></ul><p><img src="/image/Querying.png" alt="Querying"></p><ul><li>Attention mechanism은 key-value 쌍이 있고, query를 날려서 query와 key를 유사한지 비교를 해준뒤, <code>유사도를 고려한 Value들을 섞어서 Aggregation(합성)을 해준것이 Attention value</code>이다.</li></ul><p><img src="/image/Attention_mechanism.png" alt="Attention mechanism"></p><p><img src="/image/Seq2seq.png" alt="Seq2seq에 Attention mechanism 적용은?"></p><ul><li><code>대부분의 Attention network에서는 key와 value를 같은 값을 사용</code>한다. <code>Seq2seq에서는 Encoder의 Hidden Layer들을 key와 value로 사용</code>한다. 직관적으로 생각을 해보면, Decoder에서 어떤것을 찾고자 한다면, 찾고자 하는 것에 대한 정보는 Encoder에서 찾을 수 있을 것이다. 그렇기에 Key-value가 Encoder의 Hidden Layer가 되는 것이다.</li></ul><p><img src="/image/Seq2seq_key_Value.png" alt="Seq2seq의 Key-Value"></p><ul><li><code>또한, Seq2seq에서는 Decoder의 Hidden Layer들을 Query로 사용</code>한다. 주의할 점은, <code>Encoder와 달리 하나 앞선 time-step의 Hidden Layer를 사용한다는 점</code>이다. 왜냐하면 현재의 출력을 내기 위해서는 현재의 출력이 사용될 수는 없기 때문에 즉, 미래(예측)를 가지고 현재 출력을 만들어낼 수는 없기 때문에 하나 앞선 Hidden Layer를 query로 사용하는 것이다.  </li></ul><p><img src="/image/Seq2seq_Querying.png" alt="Seq2seq의 Query"></p><ul><li>Key와 Value는 서로 동일한 Encoder의 Hidden Layer들이 사용되며, Query는 Decoder에 있는 각각의 Hidden Layer들이 될 것이며, i-번째 time step에 대한 Query를 날려서 Encoder에 있는 모든 Key와 유사도를 비교해서 최종적으로는 유사도를 고려한 Aggregation(종합)한 Attention Value를 출력해 준다.</li></ul><p><img src="/image/Seq2seq_Attention_mechanism.png" alt="Seq2seq에 Attention mechanism을 적용"></p><ul><li>이렇게 Attention Value($a_{0}$)를 입력으로 받아 $s_{0}$ Hidden Layer에서 LSTM 구조를 거쳐 $s_{1}$ Hidden Layer가 나올 것이다. 이 새롭게 얻어진 $s_{1}$ Hidden Layer에다가 $s_{0}$를 통해 얻어진 Attention Value($a_{0}$) Concatenate를 해서 출력을 내준다.($v_{1}$) 이전에는 Decoder에서 그대로 Hidden Layer가 나오던 것이 이제는 Encoder의 Hidden Layer들을 비교해서 만들어낸 Attention Value를 같이 출력으로 냄으로써, Encoder 부분의 value들을 잘 가져올 수 있도록 해주었다.</li></ul><p><img src="/image/Seq2seq_Application.png" alt="Seq2seq - Application"></p><h1 id="Attention-is-all-you-need-Transformer-모델"><a href="#Attention-is-all-you-need-Transformer-모델" class="headerlink" title="Attention is all you need - Transformer 모델"></a>Attention is all you need - Transformer 모델</h1><p><img src="/image/Attention_is_all_you_need.png" alt="Attention is all you need"></p><ul><li>RNN 같은 경우는 입력을 순서대로 넣어주기 때문에 입력된 단어의 위치를 따로 표시하지 않아도 되지만, <code>Transformer 구조</code> 같은 경우에는 병렬적으로 계산을 하기 때문에, 현재 계산하고 있는 단어가 어느 위치에 있는 단어인지를 표현을 해주어야 해서 <code>positional encoding</code>을 사용한다.</li></ul><p><img src="/image/Charateristics_of_Network.png" alt="네트워크의 특성"></p><ul><li>우선, Transformer와 Seq2seq 모델을 비교하자면, Seq2seq 모델은 Encoder와 Decoder가 있고 그 사이에 Context가 전달되는 구조를 가지고 있다. Transformer 모델의 경우는 Input쪽(왼쪽의 빨간색 박스)과 Output쪽(왼쪽의 파란색 박스)로 구성되며, Input쪽에서는 Input embedding이 들어가서 Encoding이 일어나고 Context가 전달이 되어 Output쪽의 Decoder부분에서 Decoding이 되서 출력이 나오게 된다. 전체적으로 구조는 비슷해보이지만, <code>Seq2seq 모델은 RNN로 구성되어 있어서 순차적으로 이루어지게 되어있고, Transformer 모델은 병렬적으로 계산되므로 Input쪽이 동시에 계산되고 Output쪽이 동시에 계산되는 형태로 학습이되는 점이 차이점이다.</code></li></ul><p><img src="/image/Transformer_vs_Seq2seq.png" alt="Transformer vs Seq2seq"></p><ul><li>Input을 먼저 보면, 아래의 노란색 Matrix 형태로 되어 있으며, 일반적으로는 입력 단어의 가짓수와 출력 단어의 가짓수는 동일할 것이다. 만약 기계번역처럼 2개의 언어가 다르다면, 다를 것이다! Output은 원래 Seq2seq의 구조에서 보았듯이 shift 시킨 입력을 넣어주었었던 것과 같이 SOS를 넣어주고 EOS를 빼준 형태로 Outputs에 넣어준다.</li></ul><p><img src="/image/Inputs_and_Outputs.png" alt="Inputs &amp; Output"></p><ul><li>One-hot encoding으로 되어있던 것들을 Embedding해서 각각의 Embedding에 넣어준다.</li></ul><p><img src="/image/Word_Embedding_in.png" alt="Word Embedding"></p><ul><li>Positional Encoding은 <code>시간적으로 위치가 따르때마다 고유의 코드를 생성해서 Input Embedding에 더해주는 형태로 구성</code>되어있다. 이렇게 해줌으로써, <code>전체 Sequence의 길이 중 상대적 위치에 따라서 고유의 벡터를 생성하여 Embedding된 벡터에 더해주게 된다.</code> 예를들면, 보통 Embedding된 벡터들은 0를 기준으로 분포가되어있는데 Input Embedding에 sin과 cosine을 조합해서 만들어진 feature 벡터를 더해주는 것이라고 보면된다. 위에서 말했듯, Embedding들은 0을 기준으로 분포하므로 여기에 sin과 cosine을 조합해 만든 Feature 벡터를 더해준다해도 크게 손상이 가지 않기 때문에 걱정하지 않아도된다.</li></ul><p><img src="/image/Related_in_Positional_Encoding.png" alt="Embedding의 분포 및 Positional Encoding의 원리"></p><p><img src="/image/Positional_Encoding.png" alt="Positional Encoding"></p><ul><li>Dot-Product는 우리가 알고있는 내적과 동일하다. 그리고 <code>Mask를 이용해서 Illegal connection의 Attention을 금지</code>한다는 의미는 self-attention에 대한 이야기인데,  일반적인 Attention 구조는 Decoder쪽에 Hidden Layer를 통해 Output을 내려면 Encoder 쪽에 Hidden Layer 전체와 비교해서 산출을해야하므로 이런 경우는 괜찮지만, <code>Self-attention</code>에서는 Decoder를 똑같은 Decoder 자기 자신과 Attention을 할 수가 있는데 여기서 Decoder 부분의 해당 Hidden Layer를 산출하려면 순차적으로 출력이 나온다고 했을때 해당 부분의 Decoder보다 이후 시점은 아직 결과가 산출되지 않았기 때문에 그보다 앞선 시점의 Decoder부분에서의 Hidden Layer들만을 사용할 수 있다는 이야기이다. 여기서 비교할 때 사용할 수 없는 Hidden Layer들을 <code>Illegal connection</code>이라고 한다. 이런 Illegal connection은 Mask를 통해 -inf로 보내버리면 Softmax에서 값이 0이되는 것을 이용하여 attention이 안되도록 구현하고 있다.</li></ul><p><img src="/image/Scaled_Dot_Product_Attention.png" alt="Scaled Dot-Product Attention"></p><p><img src="/image/Principal_of_self_attention.png" alt="Self Attention의 원리"></p><ul><li><p>Multi-Head Attention은 쉽게 말해 <code>Scaled Dot-Product Attention을 h개를 모아서 병렬적으로 연산을 할 수 있게끔하는 것</code>이다. h개를 사용함으로써 <code>같은 입력에 대해서 더 풍부한 출력이 나타날 수 있다.</code> 또한, 여기서 처음 Linear 연산(Matrix Mult)을 하는 것은 Query, Key, Value 각각 중 특정 차원만을 보겠다는 이야기이며, 차원을 줄여주어 병렬에 유리한 구조를 만드는 역할도 있다. 그러므로 이 연산을 한 후에 h가지로 병렬처리함으로써 풍부한 출력을 얻을 수 있는 것을 이해할 수 있다.</p></li><li><p>제일 아래 단계의 Linear연산을 통해<code>Q,K,V의 차원을 감소(h개로 나눠짐)시킨다는 것이 중요</code>하다. 또한 아래 수식에서 가중치 $W_{V,i}, W_{K,i}, W_{Q,i}$의 각각의 Dimension보다 더 작은 값으로 모델의 Dimension($d_{model}$)을 해준다. 이는 value, key, query의 차원을 모델에 사용하는 차원으로 차원을 변환시켜주는 의미이기도 하다.</p></li></ul><p><img src="/image/Multi_Head_Attention.png" alt="Multi-Head Attention"></p><ul><li>Mask는 RNN의 Decoder단을 생각해보았을때, context가 앞에서 뒤로 넘어가면서 이미 구한 것들만 참조를 할 수 있는데, <code>Transformer구조에서는 병렬적으로 계산을 하기 때문에 self-attention을 할 경우에는 시간적으로 앞에서 일어난 것들에 대해서만 영향을 받게해주어야 RNN과 동일한 구조가 되기 때문에 Mask를 이용해서 예측하고자 하는 시점을 포함한 미래값들을 가려준다.</code></li></ul><p><img src="/image/Masked_Multi_Head_Attention.png" alt="Masked Multi-Head Attention"></p><ul><li><p>Multi-Head Attention이 Transformer에 어떻게 적용되어 있는지 살펴보자. <code>Self-Attention은 Decoder와 동일한 Decoder를 참조하므로 Key와 Query와 Value는 모두 같은 것</code>이다. Encoding 같은 경우에는 causual system일 필요가 딱히 없기 때문에 Mask 없이 Key, Value, Query가 그대로 사용될 수 있지만, Decoder 부분같은 경우에는 현재 Query하려고 하는 것이 Key와 value가 Query보다 더 앞서서 나올수 없기 때문에 Mask를 활용한 Masked Multi-Head Attention을 사용한다. 이렇게 Encoder단의 Self-Attention을 통해서 Attention이 강조되어 있는 Feature들을 추출을 해주고 Decoder단에서는 Output Embedding(or 이전의 출력값)이 들어왔을 때 이것을 통해 Masked Multi-Head Attention을 통해 Feature 추출을 해주고, 그 다음에 붉은 색 박스 부분에서는 이 Decoder를 통해 추출된 Feature가 Query로 들어가고, 나머지 Key, Value는 Encoder를 통해 만들어진 출력을 가지고 입력을 받게된다. 결국에는 이런 구조는 <code>Seq2seq 모델의 Attention과 동일한 구조가 되게 될 것이다.</code></p></li><li><p>실제로는 Multi-Head Attention이 병렬적으로 계산됨으로써 self-attention이 RNN을 대체해서 들어간다고 볼 수 있다.</p></li></ul><p><img src="/image/Multi_Head_Attention_in_action.png" alt="Transformer에 적용된 Multi-Head Attention"></p><ul><li>Position-wise Feed-Forward는 특별한 것은 아니고 앞서서 말했던 것과 같이 아래 초록색 박스는 가로가 문장의 길이, 세로가 One-hot vector를 크기로 갖는 행렬인데 <code>병렬적을 처리되는 input 단어 하나마다 동일한 구조의 activation이 ReLu인 FC Layer층을 공유해서 사용하여 출력을 내보낸다는 의미</code>이다. 이를 통해 병렬적으로 계산하지만 기존의 FeedForward propagation을 구현할 수 있다.</li></ul><p><img src="/image/Position_wise_Feed_Forward.png" alt="Position-wise Feed-Forward"></p><ul><li>Feed-Forward가 일어난 다음이나 Self-Attention이 일어난 다음에는 이전의 것(Skip connection)을 가져와서 더 해준 뒤 Layer Normalization을 수행해서 사용하고 있다. Layer Normalization은 Batch의 영향을 받지 않는 Normalization이라고 생각하면된다.</li></ul><p><img src="/image/Add_Norm.png" alt="Add &amp; Norm"></p><ul><li>마지막 Feed-Forward에 의해서 마지막 Feature 출력이 나오게 되면 Linear 연산(Matrix Mult)을 사용해서 출력 단어 종류의 수에 맞추게 One-hot vector로 만들어준다. 그런 다음 Softmax를 이용해서 어떤 단어인지 classification을 한다.</li></ul><p><img src="/image/Output_softmax.png" alt="Output Softmax"></p><p><img src="/image/Attention_is_really_all_you_need.png" alt="Attention is really all you need"></p><h1 id="Attention-신경망의-수식적-이해"><a href="#Attention-신경망의-수식적-이해" class="headerlink" title="Attention 신경망의 수식적 이해"></a>Attention 신경망의 수식적 이해</h1><ul><li><p>Attention mechanism은 Key와 Query를 비교하는 Comparison을 통해 그에 따른 유사도를 가중치처럼 사용하여 Key에 맞는 value들의 조합으로 Aggregation(가중합)을 통하여 Attention value를 만들어 주는 구조가 Attention mechanism이었다. 그러므로 결국 Query와 비슷하면 비슷할 수록 높은 가중치를 주어 출력을 주는 것이다.</p></li><li><p>compare 함수로는 Dot-Product가 많이 쓰이며, 저기서 k와 q가 각각 벡터의 norm이 1이라면 결국 코사인 유사도를 구하는 것과 동일해 질 것이다. 그러나 길이가 1이 아닐 경우를 생각해서 <code>Dot-product이후에 softmax를 사용하여 전체의 합을 1로 만들어 주게끔하여 각각의 가중치들을 하나의 확률로 사용할 수 있게끔 변환해 주어 사용</code>한다.</p></li></ul><p><img src="/image/attention_mechanism_01.png" alt="Attention mechanism"></p><ul><li>Seq2seq 모델은 Encoder구조를 통해 Feature들을 만들게 되고 최종적으로는 출력으로 Context를 생성하여 이 Context 하나에 의지해서 Decoder는 SOS(Start Of Sequence)를 시작으로 출력으로는 단어를 하나씩 내어주는 모델이다.</li></ul><p><img src="/image/seq2seq_01.png" alt="Seq2seq"></p><ul><li>Key-value쌍은 기존의 Context만을 보며 출력을 내주었던 것과 다르게 Decoder부분의 Hidden Layer에 대한 출력을 낼때, Encoder 부분에 중간중간 부분을 알게하기 위해서 사용되어진다.</li></ul><p><img src="/image/seq_key_value.png" alt="Seq2seq - Key-Value"></p><ul><li>Query는 Decoder의 Hidden Layer들을 사용하는데, <code>해당 출력을 해야하는 RNN구조의 하나 이전의 time-step의 Hidden Layer를 Query로 사용한다는 점을 기억</code>하자! 아래 그림에서는 $s_{0}, s_{1}, s_{2}$가 해당한다.</li></ul><p><img src="/image/seq2seq_query.png" alt="Seq2seq - Query"></p><ul><li>i-th query가 들어오게되면 각각 Key와 비교를 하게되고, 앞에서 말한것과 같이 내적한 뒤 Softmax를 해줘 가중치로 만든뒤에 각각에 해당하는 Value와 곱해 가중합을 한 것을 Attention value로 산출한다.</li></ul><p><img src="/image/seq2seq_attention_mechanism_01.png" alt="Seq2seq - Attention mechanism"></p><ul><li>아래 그림에서 출력과 RNN 구조사이에 실제로는 FC Layer가 하나 존재해서 출력을 One-hot vector로 만들어준다.</li></ul><p><img src="/image/seq2seq_application_01.png" alt="Seq2seq -  Application"></p><p><img src="/image/Attention_paractice.png" alt="Attention의 구현"></p><ul><li>$X\in R^{BXLXN}$에서 B:Batch size, L:문장의 길이, N:One-hot vetor나 embedding Feature의 길이를 의미하며, 여기서 Decode 쪽으로 Context를 넘길때는 LSTM이라면 Hidden State와 Cell State 둘다 넘겨주어야 하기에 Batch_size X Hidden state의 Feature 갯수인 M을 사이즈로 갖는 tensor를 넘겨줄 것이다.</li></ul><p><img src="/image/Encoder_inputs_and_outputs.png" alt="Encoder 입출력"></p><p><img src="/image/Decoder_inputs_and_outputs.png" alt="Decoder 입출력(학습 단계)"></p><ul><li>Encoder의 Hidden State인 H가 Attention mechanism에 Key와 Value로 입력이 되고, Query에는 Decoder의 한 step 앞선 Hidden State를 사용하게 된다.</li></ul><p><img src="/image/Output_w_attention.png" alt="Output w/ Attention(학습 단계)"></p><p><img src="/image/attention_is_all_you_need_01.png" alt="Attention is all you need"></p><ul><li><code>만약에 Input의 언어와 Output의 언어가 다르다면, 단어의 가짓수나 길이가 달라질 수있다는 점을 주의</code>하자!</li></ul><p><img src="/image/INPUTS_AND_OUTPUTS_01.png" alt="Input &amp; Output"></p><ul><li>sin법칙과 cosine법칙에 의해 각각 분리해서 쓸수 있는데 결국 덧셈과 뺄셈으로 이 Positional Encoding이 달라지기 때문에 FC Layer에서 학습하는데 용이하게 될 것이다.</li></ul><p><img src="/image/POSITIONAL_ENCODING_01.png" alt="Positional Encoding"></p><ul><li>전체적인 구조는 Attention mechanism을 적용한 Seq2seq 모델과 유사하지만 <code>Scale이 되는 부분과 Mask를 사용하는 부분이 다르다.</code> 또한, <code>가장 중요한 점은 아래 수식 중 Query와 key, value 부분을 모아서 하나의 metrics로 만들어 줌으로써 우리가 처음 배웠던 shallow NN과 같이 병렬적으로 처리할 수 있게끔 해주었다는 것이 가장 큰 Transformer 모델의 요소</code>일 것이다. <code>scale 처리를 해줌으로써 내적의 값이 너무 커져서 saturation되서 Softmax값이 차이가 많이나는 것을 방지</code>할 수 있다.</li></ul><p><img src="/image/SCALED_DOT_PRODUCT_ATTENTION_01.png" alt="Scaled Dot-Product Attention"></p><ul><li>제일 아래 단계의 Linear연산을 통해<code>Q,K,V의 차원을 감소(h개로 나눠짐)시킨다는 것이 중요</code>하다. 또한 아래 수식에서 가중치 $W_{V,i}, W_{K,i}, W_{Q,i}$의 각각의 Dimension보다 더 작은 값으로 모델의 Dimension($d_{model}$)을 해준다. 이는 value, key, query의 차원을 모델에 사용하는 차원으로 차원을 변환시켜주는 의미이기도 하다.</li></ul><p><img src="/image/MULTI_HEAD_ATTENTION_01.png" alt="Multi-Head Attention"></p><p><img src="/image/TRANSFORMER_MODEL_REVIEW.png" alt="Transformer Model Review"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/21/deep_learning_10/#disqus_thread</comments>
    </item>
    
    <item>
      <title>순환신경망(Vanilla RNN 및 LSTM 구현)</title>
      <link>https://heung-bae-lee.github.io/2020/01/21/deep_learning_09/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/21/deep_learning_09/</guid>
      <pubDate>Mon, 20 Jan 2020 20:39:33 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;순환-신경망-구현-및-학습&quot;&gt;&lt;a href=&quot;#순환-신경망-구현-및-학습&quot; class=&quot;headerlink&quot; title=&quot;순환 신경망 구현 및 학습&quot;&gt;&lt;/a&gt;순환 신경망 구현 및 학습&lt;/h1&gt;&lt;h2 id=&quot;Vanilla-RNN&quot;&gt;&lt;a h
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="순환-신경망-구현-및-학습"><a href="#순환-신경망-구현-및-학습" class="headerlink" title="순환 신경망 구현 및 학습"></a>순환 신경망 구현 및 학습</h1><h2 id="Vanilla-RNN"><a href="#Vanilla-RNN" class="headerlink" title="Vanilla RNN"></a>Vanilla RNN</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install tensorflow==2.0.0-beta1</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br></pre></td></tr></table></figure><h3 id="tensorflow-version-확인"><a href="#tensorflow-version-확인" class="headerlink" title="tensorflow version 확인"></a>tensorflow version 확인</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br></pre></td></tr></table></figure><h3 id="하이퍼-파라미터-설정"><a href="#하이퍼-파라미터-설정" class="headerlink" title="하이퍼 파라미터 설정"></a>하이퍼 파라미터 설정</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">EPOCHS = 10</span><br><span class="line"></span><br><span class="line"><span class="comment"># 우리가 분석할 때 10000개의 단어만 사용하겠다는 의미로 설정하였다.</span></span><br><span class="line">NUM_WORDS = 10000</span><br></pre></td></tr></table></figure><h3 id="모델정의"><a href="#모델정의" class="headerlink" title="모델정의"></a>모델정의</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class MyModel(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyModel, self).__init__()</span><br><span class="line">        <span class="comment"># input_dim, output_dim</span></span><br><span class="line">        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 16)</span><br><span class="line">        self.rnn = tf.keras.layers.SimpleRNN(32)</span><br><span class="line">        self.dense = tf.keras.layers.Dense(2, activation=<span class="string">'softmax'</span>)</span><br><span class="line"></span><br><span class="line">    def call(self, x, training=None, mask=None):</span><br><span class="line">        x = self.emb(x)</span><br><span class="line">        x = self.rnn(x)</span><br><span class="line">        <span class="built_in">return</span> self.dense(x)</span><br></pre></td></tr></table></figure><h3 id="학습-테스트-루프-정의"><a href="#학습-테스트-루프-정의" class="headerlink" title="학습, 테스트 루프 정의"></a>학습, 테스트 루프 정의</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement training loop</span></span><br><span class="line">@tf.function</span><br><span class="line">def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        predictions = model(inputs, training=True)</span><br><span class="line">        loss = loss_object(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line"></span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(labels, predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Implement algorithm test</span></span><br><span class="line">@tf.function</span><br><span class="line">def test_step(model, images, labels, loss_object, test_loss, test_accuracy):</span><br><span class="line">    predictions = model(images, training=False)</span><br><span class="line"></span><br><span class="line">    t_loss = loss_object(labels, predictions)</span><br><span class="line">    test_loss(t_loss)</span><br><span class="line">    test_accuracy(labels, predictions)</span><br></pre></td></tr></table></figure><h3 id="데이터셋-준비"><a href="#데이터셋-준비" class="headerlink" title="데이터셋 준비"></a>데이터셋 준비</h3><p>IMDB</p><ul><li><p>review를 보고 긍정인지 부정인지를 예측하는 문제이며, y(target value)는 binary value(0 or 1)를 가지지만 x_data(feature)에서 각각의 review의 길이가 다르므로 입력에서 출력이 나오는 기준을 맞추기 위해 zero-padding을 해주는 작업을 실행할 것이다. 아래의 ‘pad_sequence’함수에서 maxlen=32는 최대 길이를 32글자로 맞추겠다는 의미이다.</p></li><li><p>maxlen=32로 함으로써 원래 본 데이터의 <code>맨 뒤부분에서 시작해서 32번째 데이터 까지를 잘라서 사용하는 것이며, 이 부분에 데이터가 없을 시 0으로 padding 처리를 해주는 함수</code>이다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">imdb = tf.keras.datasets.imdb</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=NUM_WORDS)</span><br><span class="line"></span><br><span class="line"><span class="comment"># padding='post' 뒤쪽으로 padding해준다.</span></span><br><span class="line">x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train,</span><br><span class="line">                                                       value=0,</span><br><span class="line">                                                       padding=<span class="string">'pre'</span>,</span><br><span class="line">                                                       maxlen=32)</span><br><span class="line"></span><br><span class="line">x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,</span><br><span class="line">                                                       value=0,</span><br><span class="line">                                                       padding=<span class="string">'pre'</span>,</span><br><span class="line">                                                       maxlen=32)</span><br><span class="line"></span><br><span class="line">train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)</span><br><span class="line">test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)</span><br></pre></td></tr></table></figure><p><img src="/image/list_and_not.png" alt="numpy에서 list로 묶은 것과 아닌 것의 차이"></p><p><img src="/image/IMDB_preprocessing.png" alt="IMDB 전처리"></p><h3 id="학습-환경-정의"><a href="#학습-환경-정의" class="headerlink" title="학습 환경 정의"></a>학습 환경 정의</h3><h4 id="모델-생성-손실함수-최적화-알고리즘-평가지표-정의"><a href="#모델-생성-손실함수-최적화-알고리즘-평가지표-정의" class="headerlink" title="모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"></a>모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 생성</span></span><br><span class="line">model = MyModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 손실함수 및 최적화 기법 정의</span></span><br><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">optimizer = tf.keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 성능 지표 정의</span></span><br><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">"train_loss"</span>)</span><br><span class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">"train_accuracy"</span>)</span><br><span class="line"></span><br><span class="line">test_loss = tf.keras.metrics.Mean(name=<span class="string">"test_loss"</span>)</span><br><span class="line">test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">"test_accuracy"</span>)</span><br></pre></td></tr></table></figure><h3 id="학습-루프-동작"><a href="#학습-루프-동작" class="headerlink" title="학습 루프 동작"></a>학습 루프 동작</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCHS):</span><br><span class="line">  <span class="keyword">for</span> seqs, labels, <span class="keyword">in</span> train_ds:</span><br><span class="line">    train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> test_seqs, test_labels <span class="keyword">in</span> test_ds:</span><br><span class="line">    test_step(model, test_seqs, test_labels, loss_object, test_loss, test_accuracy)</span><br><span class="line"></span><br><span class="line">  template = <span class="string">"Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;"</span></span><br><span class="line">  <span class="built_in">print</span>(template.format(epoch + 1,</span><br><span class="line">                        train_loss.result(),</span><br><span class="line">                        train_accuracy.result() * 100,</span><br><span class="line">                        test_loss.result(),</span><br><span class="line">                        test_accuracy.result() * 100))</span><br></pre></td></tr></table></figure><p><img src="/image/vanilla_RNN_result.png" alt="IMDB에 대한 vanilla RNN의 성능"></p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><ul><li>keras는 고수준 API이므로 이미 내부에 구현이 되어있어 다음과 같이 변경해주는 것만으로 LSTM을 구현 할 수 있다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class MyModel(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(MyModel, self).__init__()</span><br><span class="line">        <span class="comment"># input_dim, output_dim</span></span><br><span class="line">        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 16)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># tf.keras.layers.GRU(32)도 가능</span></span><br><span class="line">        <span class="comment"># 참고로 RNN은 층을 쌓을수록 성능이 안좋아질 가능성이 높다는 점을 주의하자!</span></span><br><span class="line">        self.rnn = tf.keras.layers.LSTM(32)</span><br><span class="line">        self.dense = tf.keras.layers.Dense(2, activation=<span class="string">'softmax'</span>)</span><br><span class="line"></span><br><span class="line">    def call(self, x, training=None, mask=None):</span><br><span class="line">        x = self.emb(x)</span><br><span class="line">        x = self.rnn(x)</span><br><span class="line">        <span class="built_in">return</span> self.dense(x)</span><br></pre></td></tr></table></figure><p><img src="/image/LSTM_result.png" alt="IMDB에 대한 LSTM의 성능"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/21/deep_learning_09/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP 전처리</title>
      <link>https://heung-bae-lee.github.io/2020/01/19/NLP_02/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/19/NLP_02/</guid>
      <pubDate>Sun, 19 Jan 2020 06:29:26 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;형태소&quot;&gt;&lt;a href=&quot;#형태소&quot; class=&quot;headerlink&quot; title=&quot;형태소&quot;&gt;&lt;/a&gt;형태소&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/image/Morphs.png&quot; alt=&quot;형태소란?&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/image/
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="형태소"><a href="#형태소" class="headerlink" title="형태소"></a>형태소</h1><p><img src="/image/Morphs.png" alt="형태소란?"></p><p><img src="/image/Morphs_analysis.png" alt="형태소 분석이란?"></p><h1 id="Tokenizing-라이브러리"><a href="#Tokenizing-라이브러리" class="headerlink" title="Tokenizing 라이브러리"></a>Tokenizing 라이브러리</h1><h3 id="영어-Tokenizing-라이브러리"><a href="#영어-Tokenizing-라이브러리" class="headerlink" title="영어 Tokenizing 라이브러리"></a>영어 Tokenizing 라이브러리</h3><ul><li><p>1) NLTK</p><ul><li>파이썬에서 영어 텍스트 전처리 작업을 하는데 많 쓰이는 라이브러리로, 이 라이브러리는 50여 개가 넘는 말뭉치 리소를 활용해 영어 텍스트를 분석할 수 있게 제공 한다. 직관적으로 함수를 쉽게 사용할 수 있게 구성돼 있어 빠르게 텍스트 전처리를 할 수 있다. 또한 <code>단어 단위 토크나이징</code>과 <code>문장 단위 토크나이징</code>을 하는 모듈이 <code>따로</code> 있으며 ‘a’, ‘the’ 같은 관사나 ‘is’와 같이 자주 의미는 별로 없지만 자주 등장하는 단어인 불용어들을 모아 불용어 사전을 구성하고 있어 <code>따로 불용어를 정의할 필요없이 바로 사용가능</code>하다.</li></ul></li><li><p>2) <a href="https:spacy.io/usage/training" target="_blank" rel="noopener">Spacy</a></p><ul><li>NLTK와 같은 오픈소스 라이브러리이다. 주로 <code>상업용 목적</code>으로 만들어졌다는 점이 NLTK와 다르며, <code>영어를 포함한 8개 언어에 대한 자연어 전처리 모듈을 제공</code>하고, 빠른 속도로 전처리할 수 있다. <code>원하는 언어에 대한 전처리를 한 번에 해결할 수 있다는 장점</code>이 있으며, <code>특히 딥러닝 언어 모델의 개발도 지원하고 있어 매력적</code>이다. NLTK와 다르게 <code>단어 단위, 문장 단위 토크나이징을 한가지 모듈을 통해 처리</code>한다.</li></ul></li><li><p><code>이러한 영어 토크나이징 도구는 한국어에 적용할 수 없다!!</code></p></li></ul><h3 id="한글-토크나이징-라이브러리"><a href="#한글-토크나이징-라이브러리" class="headerlink" title="한글 토크나이징 라이브러리"></a>한글 토크나이징 라이브러리</h3><ul><li>자연어 처리에서 각 언어마다 모두 특징이 다르기 때문에 천편일률적으로 동일한 방법을 적용하기는 어렵다. 한국어 자연어 처리에 많이 사용되는 파이썬 라이브러리 KoNLPy를 소개하겠다.</li></ul><p><img src="/image/Konlpy.png" alt="KoNLPy"></p><ul><li><p>1) <code>KoNLPy</code>(지도학습 기법으로 학습)</p><ul><li><p>한글 자연어 처리를 쉽고 간결하게 처리할 수 있도록 만들어진 오픈소스 라이브러리이다. 또한 국내에 이미 만드어져 사용되고 있는 여러 형태소 분석기를 사용할 수 있게 허용한다. 형태소 분석으로 형태소 단위의 토크나이징을 가능하게 할뿐만 아니라 구문 분석을 가능하게 해서 언어 분석을 하는 데 유용한 도구다.</p></li><li><p>한글 텍스트의 경우에는 형태소 단위 토크나이징이 필요할 때가 있다. KoNLPy에서는 여러 형태소 분석기를 제공하며, 각 형태소 분석기별로 분석한 결과가 다르므로 자신의 분석 데이터에 맞는 형태소 분석기를 선택해서 사용할 것을 권한다. <code>Mecab의 경우 원도우에서는 사용할 수 없으니 참고</code>해서 사용하자.</p><ul><li>Hannanum (한나눔)</li><li>Kkma (꼬꼬마)</li><li>Komoran (코모란)</li><li>Mecab (메케브)</li><li>Okt(Twitter)</li></ul></li></ul></li><li><p>KAIST에서 개발된 한나눔은 다음과 같은 메소드를 제공한다.</p></li></ul><p><img src="/image/Hannanum_tokenizer.png" alt="한나눔 형태소 분석기"></p><ul><li>서울대학교에서 개발된 한나눔은 다음과 같은 메소드를 제공한다.</li></ul><p><img src="/image/kkma_tokenizer.png" alt="꼬꼬마 형태소 분석기"></p><ul><li>코모란은 다음과 같은 메소드를 제공한다.</li></ul><p><img src="/image/Komoran_tokenizer.png" alt="코모란 형태소 분석기"></p><ul><li>mecab은 은전한닢이란 의미를 지니고 있으며(TMI인듯), 빠르고 성능이 우수한 것으로 알려져 있다. 다음과 같은 메소드를 제공한다.</li></ul><p><img src="/image/Mecab_tokenizer.png" alt="Mecab 형태소 분석기"></p><ul><li>Okt는 구 Twitter로 불리며, Twitter에서 만들었다. 다음과 같은 메소들르 제공한다.</li></ul><p><img src="/image/Okt_tokenizer.png" alt="Okt 형태소 분석기"></p><p>macOS에서 설치<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># JPype1은 파이썬에서 자바 클래스를 사용할 수 있도록 만들어주는 라이브러리이다.</span></span><br><span class="line"><span class="comment"># 만약 window라면 https://www.lfd.uci.edu/~gohlke/pythonlibs/#jpype에서 맞는 사양을 설치</span></span><br><span class="line"><span class="comment"># 64bit-python3.6 버전이라면 JPype1-0.63-cp36-cp36m-win_amd64.whl 을 설치하면된다.</span></span><br><span class="line"><span class="comment"># pip install JPype1-0.63-cp36-cp36m-win_amd64.whl</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MacOs에선</span></span><br><span class="line">conda install -c conda-forge jpype1</span><br><span class="line"></span><br><span class="line">pip install konlpy</span><br></pre></td></tr></table></figure></p><ul><li><p>형태소 분석기 사용법</p><ul><li><p>각각의 형태소 분석기는 클래스생성만 다르고 나머지는 동일한 함수를 사용하므로 아래 예시에서는 형태소 분석기 중 제일 속도가 빠르다고 알려져 있는 Mecab을 사용할 것이다.</p></li><li><p>pos를 통해 얻는 품사의 태깅의 의미를 알고 싶다면 <a href="https://docs.google.com/spreadsheets/d/1OGAjUvalBuX-oZvZ_-9tEfYD2gQe7hTGsgUpiiBSXI8/edit#gid=0" target="_blank" rel="noopener">클릭</a></p></li><li><p>tokenizer.morphs()</p><ul><li><code>텍스트를 형태소 단위로 나눈다.</code> 옵션으로는 norm과 stem이있다. 각각 True 혹은 False 값을 받으며, <code>norm</code>은 normalize의 약자로서 문장을 <code>정규화하는 역힐</code>을 하고, <code>stem</code>은 각 단어에서 <code>어간을 추출하는 기능</code>(예시: 해야지 -&gt; 하다)이다. 각각 True로 설정하면 각 기능이 적용 된다. 둘 다 default는 False이다.</li></ul></li><li><p>tokenizer.nouns()</p><ul><li>텍스트에서 <code>명사만 뽑아</code>낸다.</li></ul></li><li><p>tokenizer.phrases()</p><ul><li>텍스트에서 <code>어절을 뽑아</code>낸다.</li></ul></li><li><p>tokenizer.pos()</p><ul><li>위의 세 함수는 추출기인 반면에, pos 함수는 태깅함수이다. <code>각 품사를 태깅하는 역할</code>을 한다. <code>norm, stem 옵션이 존재하며 join=True로 하게 되면 (형태소, 품사)의 형태에서 형태소/품사 형태로 붙여서 리스트화한다.</code></li></ul></li><li><p><code>어떤 형태소 분석기를 사용할지는 자신이 가진 데이터로 실험 삼아 형태소 분석을 해보고 속도나 품질을 비교해서 고르는 것이 좋다.</code> 자신의 분석에서 사전에 추가해야할 단어들이 있다면 사용자 사전에 추가해 주면 된다.</p></li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from konlpy.tag import Mecab</span><br><span class="line">tokenizer = Mecab()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 형태소 단위로 나누기</span></span><br><span class="line">tokenizer.morphs(<span class="string">"아버지가방에들어가신다."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line">[<span class="string">"아버지"</span>, <span class="string">"가"</span>, <span class="string">"방"</span>, <span class="string">"에"</span>, <span class="string">"들어가"</span>, <span class="string">"신다"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 품사 태그</span></span><br><span class="line">tokenizer.pos(<span class="string">'아버지가방에들어가신다.'</span>)</span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line"><span class="comment">##### 결과</span></span><br><span class="line">``` bash</span><br><span class="line">[(<span class="string">'아버지'</span>, <span class="string">'NNG'</span>), (<span class="string">'가'</span>, <span class="string">'JKS'</span>), (<span class="string">'방'</span>, <span class="string">'NNG'</span>), (<span class="string">'에'</span>, <span class="string">'JKB'</span>), (<span class="string">'들어가'</span>, <span class="string">'VV'</span>), (<span class="string">'신다'</span>, <span class="string">'EP+EC'</span>)]</span><br></pre></td></tr></table></figure><ul><li><p>Mecab에 사용자 사전 추가하기</p><ul><li><p>Komoran은 추가할 내용의 txt를 만들어 객체 생성시 userdic 파라미터에 만든 path를 입력해 주면되고, Hannanum은 konlpy/java/data/kE/dic_user.txt에 존재하며 여기에 새로운 단어를 형식에 맞춰 추가해주면된다. Kkma는 ~/anaconda3/lib/python3.6/site-packages/konlpy/java/kkma-2.0.jar 압축파일 내의 .dic 형식의 파일들이 dictionary 파일이므로 압축을 푼뒤 해당 품사에 맞는 파일에 단어를 추가해주면된다. 그리고나서 다시 .jar로 압축을 해주고 원본은 만약을 위해 다른 곳에 보관하며 사용한다.</p></li><li><p>형태소 분석기를 사용하다 보면 가장 신경 써야 하는 점이 <code>중요 token들을 어떻게 처리해야 할지다.</code> 예를들면 우리가 ‘천리마전자’라는 기업의 데이터 분석 팀에 속해 있고 천리마저나에 관한 Corpus를 분석하거나 이로부터 임베딩을 만들어야 한다고 가정해보자. 이 경우 ‘천리마전자’라는 token은 섬세하게 처리해야한다. 만약 ‘천리마전자 텔레비전 정말 좋네요’라는 가상의 리뷰를 분석한다면 천리마 전자 보다 천리마전자로 분석됐을 때 임베딩 품질이 더 좋을 것이다. 이럴 경우 <code>사용자 사전에 추가하여 하나의 토큰으로 분석될 수 있도록 강제해야한다.</code></p></li></ul></li></ul><ul><li>2) <code>Khaiii</code> 사용법 (지도학습기법으로 학습)<ul><li><a href="http://tech.kakao.com/2018/12/13/khaiii" target="_blank" rel="noopener">reference</a></li><li>Khaiii(Kakao Hangul Analyzer iii)는 kakao가 2018년 말 공개한 오픈소스 한국어 형태소 분석기다. 국립국어원이 구축한 세종 코퍼스를 이용해 CNN 모델을 적용해 학습했다. Khaiii의 아키텍처는 입력 문장을 문자 단위로 읽어 들인 뒤 convolution filter가 이 문자들을 슬라이딩해 가면서 정보를 추출한다. 출력 노드에서는 이렇게 모은 정보들을 종합해 형태소의 경계와 품사 태그를 예측한다. 카카오 측 설명에 따르면 모델을 <code>C++로 구현해 GPU 없이도 형태소 분석이 가능하며 실행 속도 역시 빠르다고 한다.</code></li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from khaiii import KhaiiiApi</span><br><span class="line">tokenizer = KhaiiiApi()</span><br><span class="line"></span><br><span class="line">data = tokenizer.analyze(<span class="string">'아버지가방에들어가신다'</span>)</span><br><span class="line">tokens = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> data:</span><br><span class="line">    token.extend([str(m).split(<span class="string">"/"</span>)[0] <span class="keyword">for</span> m <span class="keyword">in</span> word.morphs])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line">[<span class="string">'아버지'</span>, <span class="string">'가'</span>, <span class="string">'방에'</span>, <span class="string">'들'</span>, <span class="string">'어'</span>, <span class="string">'가'</span>, <span class="string">'시'</span>, <span class="string">'ㄴ다'</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 품사 정보 확인 tagging</span></span><br><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> data:</span><br><span class="line">    token.extend([str(m) <span class="keyword">for</span> m <span class="keyword">in</span> word.morphs])</span><br></pre></td></tr></table></figure><h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'아버지/NNG'</span>, <span class="string">'가/JKS'</span>, <span class="string">'방에/NNG'</span>, <span class="string">'들/VV'</span>, <span class="string">'어/EC'</span>, <span class="string">'가/VV'</span>, <span class="string">'시/EP'</span>, <span class="string">'ㄴ다/EC'</span>]</span><br></pre></td></tr></table></figure><ul><li><p>3) <code>soynlp</code> (비지도학습으로 학습)</p><ul><li><p>형태소 분석, 품사 판별 등을 지원하는 파이썬 기반 한국어 자연어 처리 패키지다. 데이터 패턴을 스스로 학습하는 <code>비지도 학습 접근법을 지향하기 때문에 하나의 문장 혹은 문서에서보다는 어느 정도 규모가 있으면서 동질적인 문서 집합(homogeneous documents)에서 잘 작동</code>한다.</p></li><li><p>soynlp 패키지에 포함된 형태소 분석기는 데이터의 통계량을 확인해 만든 단어 점수 표로 작동한다. 단어 점수는 크게 <code>응집확률(Cohesion Probability) 과 브랜칭 엔트로피(Branching Entropy)</code>를 활용한다. <code>구체적으로는 주어진 문자령이 유기적으로 연결돼 함께 자주 나타나고(응집 확률이 높을 때), 그 단어 앞뒤로 다양한 조사, 어미 혹은 다른 단어가 등장하는 경우(브랜칭 엔트로피가 높을 때) 해당 문자열을 형태소로 취급</code>한다.</p></li><li><p>예를 들어, 주어진 Corpus에서 ‘꿀잼’이라는 단어가 연결돼 자주 나타났다면 ‘꿀잼’을 형태소라고 본다(응집 확률이 높음). 한편 ‘꿀잼’ 앞에 ‘영화’, ‘정말’, ‘너무’ 등 문자열이, 뒤에 ‘ㅋㅋ’, ‘ㅎㅎ’, ‘!!’ 등 패턴이 다양하게 나타났다면 이 역시 ‘꿀잼’을 형태소로 취급한다.(브랜칭 엔트로피가 높음)</p></li><li><p><code>Cohesion score + L-Tokenizer</code></p><ul><li><p>Cohesion score는 한국어의 단어 추출을 위하여 character n-gram 을 이용한다. 새로운 개념을 설명하기 위해 새로운 단어가 만들어지기 때문에 모든 단어를 포함하는 사전은 존재할 수 없다. 학습데이터를 이용하는 supervised algorithms 은 가르쳐주지 않은 단어를 인식하기가 어렵다. 실질적으로는 사전에 등록되지 않은 단어는 형태소 분석을 할 수 없다는 것이다. <code>통계 기반 단어 추출 기법은 ‘우리가 분석하려는 데이터에서 최대한 단어를 인식’하여 학습데이터를 기반으로 하는 supervised approach 를 보완하기 위한 방법이다.</code> 단어의 경계에 가까워질수록 P(xy|x)의 값이 커지고, 단어의 경계를 넘어서면 P(xy|x)의 값이 줄어든다. 이 현상을 이용하여 L part 에서 단어를 추출할 수 있는 character n-gram 기반 score 를 정의한다.</p></li><li><p>필자가 이해하기로는 간단하게 말하면 corpus에서 사용되는 단어 중 의미를 갖는 단위를 나누는 기준으로써 Cohesion score가 높은 것을 사용한다는 것이다.</p></li><li><p><a href="https://lovit.github.io/nlp/2018/04/09/cohesion_ltokenizer/" target="_blank" rel="noopener">참조</a></p></li></ul></li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from soynlp.word import WordExtractor</span><br><span class="line"></span><br><span class="line">sentence = [데이터]</span><br><span class="line">word_extractor = WordExtractor(min_frequency=100,</span><br><span class="line">                               min_cohesion_forward=0.05,</span><br><span class="line">                               min_right_branching_entropy=0.0)</span><br><span class="line"></span><br><span class="line">word_extractor.train(sentence)</span><br><span class="line"><span class="comment"># model 저장</span></span><br><span class="line">word_extractor.save(model_fname_and_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 위에서 저장한 모델 load</span></span><br><span class="line">import math</span><br><span class="line">from soynlp.tokenizer import LTokenizer</span><br><span class="line"></span><br><span class="line">model_fname = <span class="string">'위에서 저장했던 model path'</span></span><br><span class="line"></span><br><span class="line">word_extractor = WordExtractor(min_frequency=100,</span><br><span class="line">                               min_cohesion_forward=0.05,</span><br><span class="line">                               min_right_branching_entropy=0.0)</span><br><span class="line"></span><br><span class="line">word_extractor.load(model_fname)</span><br><span class="line">scores = word_extractor.word_scores()</span><br><span class="line">scores = &#123;key ; (scores[key].cohesion_forward * math.exp(scores[key].min_right_branching_entropy)) <span class="keyword">for</span> key <span class="keyword">in</span> scores.keys()&#125;</span><br><span class="line">tokenizer = LTokenizer(scores=scores)</span><br><span class="line">tokens = tokenizer.tokenize(<span class="string">'애비는 종이었다.'</span>)</span><br></pre></td></tr></table></figure><ul><li><p>4) <code>구글 센텐스피스(sentencepiece)</code></p><ul><li><p>구글에서 공개한 비지도 학습기반 형태소 분석 패키지이며, 1994년 제안된 <code>바이트 페어 인코딩(BPE : Byte Pair Encoding)기법 등을 지원</code>하며 pip 설치를 통해 파이썬 콘솔에서도 사용할 수 있다.</p></li><li><p>BPE의 기본 원리</p><ul><li>Corpus에서 가장 많이 등장한 문자열을 병합해 문자열을 압축하는 것</li><li><p>예시</p><ul><li>aaabdaaabac</li><li>위의 문자열에서는 aa가 가장 많이 나타났다. 이를 Z로 치환하면 원래 문자열을 다음과 같이 압축할 수 있다.</li><li>ZabdZabac</li><li>이번에는 ab가 가장 많이 나타났으므로 Y로 치환하겠다.</li><li>ZYdZYac</li></ul></li><li><p>자연어 처리에서 BPE가 처음 쓰인 것은 기계 번역 분야다. BPE를 활용해 토크나이즈하는 메커니즘의 핵심은 <code>원하는 어휘 집합 크기가 될 때까지 반복적으로 고빈도 문자열들을 병합해 어휘 집합에 추가한다.</code> 이것이 BPE학습이다. 학습이 끝난 이후의 예측과정은 문장 내 각 어절(띄어쓰기로 문장을 나눈 것)에 어휘 집합에 있는 subword가 포함돼 있을 경우 해당 subword를 어절에서 분리한다.(최장 일치 기준) 이후 어절의 나머지에서 어휘 집합에 있는 subword를 다시 찾고, 또 분리한다. 어절 끝까지 찾았는데 어휘 집합에 없으면 미등록 단어(Unknown word)로 취급한다.</p></li></ul></li><li><p>BERT 모델은 BPE로 학습한 어휘 집합을 쓴다. BPE는 문자열 기반의 비지도 학습 기법이기 때문에 데이터만 확보할 수 있다면 어떤 언어에든 적용이 가능하다. 물론 BERT 모델에 사용할 수 있는 어휘 집합으로 쓸 수 있게 하기 위해서는 언더바(_) 문자를 ‘##’로 바꾸고 [PAD], [UNK], [CLS], [MASK], [SEP] 등 스페셜 토큰을 추가한다.</p></li></ul></li><li><p>구글이 공개한 BERT 모델 코드에서 BPE로 학습한 어휘 집합으로 토큰을 분리하는 클래스를 실행</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import sentencepiece as spm</span><br><span class="line">train =<span class="string">""</span><span class="string">"--input=input_file_path \</span></span><br><span class="line"><span class="string">          --model_prefix=sentence \</span></span><br><span class="line"><span class="string">          --vocab_size=32000 \</span></span><br><span class="line"><span class="string">          --model_type=bpe --character_coverage=0.9995"</span><span class="string">""</span></span><br><span class="line"></span><br><span class="line">spm.SentencePieceTrainer.Train(train)</span><br><span class="line"></span><br><span class="line">from bert.tokenization import FullTokenizer</span><br><span class="line"></span><br><span class="line">vocab_fname = <span class="string">"vocabulary_file_path.vocab"</span></span><br><span class="line">tokenizer = FullTokenizer(vocab_file=vocab_fname, do_lower_case=False)</span><br><span class="line"></span><br><span class="line">tokenizer.tokenize(<span class="string">"집에좀 가자"</span>)</span><br></pre></td></tr></table></figure><h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'집에'</span>, <span class="string">'##좀'</span>, <span class="string">'가자'</span>]</span><br></pre></td></tr></table></figure><h4 id="soynlp-형태소-분석이나-BPE-방식의-토크나이즈-기법은-띄어쓰기에-따라-분석-결과가-크게-달라지므로-이들-모델을-학습하기-전-띄어쓰기-교정을-먼저-적용하면-그-분석-품질이-개선될-수-있다"><a href="#soynlp-형태소-분석이나-BPE-방식의-토크나이즈-기법은-띄어쓰기에-따라-분석-결과가-크게-달라지므로-이들-모델을-학습하기-전-띄어쓰기-교정을-먼저-적용하면-그-분석-품질이-개선될-수-있다" class="headerlink" title="soynlp 형태소 분석이나 BPE 방식의 토크나이즈 기법은 띄어쓰기에 따라 분석 결과가 크게 달라지므로 이들 모델을 학습하기 전 띄어쓰기 교정을 먼저 적용하면 그 분석 품질이 개선될 수 있다."></a>soynlp 형태소 분석이나 BPE 방식의 토크나이즈 기법은 띄어쓰기에 따라 분석 결과가 크게 달라지므로 이들 모델을 학습하기 전 띄어쓰기 교정을 먼저 적용하면 그 분석 품질이 개선될 수 있다.</h4><h3 id="띄어쓰기-교정"><a href="#띄어쓰기-교정" class="headerlink" title="띄어쓰기 교정"></a>띄어쓰기 교정</h3><ul><li>soynlp에서는 띄어쓰기 교정 모듈도 제공한다. <code>Corpus에서 띄어쓰기 패턴을 학습한 뒤 해당 패턴대로 교정을 수행</code>한다. 예를 들어, 학습 데이터에서 ‘하자고’라는 문자 앞뒤로 다수의 공백이 발견됐다면 예측단계에서 ‘하자고’가 출현한다면 앞뒤를 띄어서 교정하는 방식이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from soyspacing.countbase importCountSpace</span><br><span class="line"></span><br><span class="line"><span class="comment"># corpus가 띄어쓰기가 이미 올바로 되어있어야 품질이 높아질 것이다.</span></span><br><span class="line">corpus_fname = <span class="string">'corpus_path'</span></span><br><span class="line">model_fname = <span class="string">'저장할때 사용할 모델 path'</span></span><br><span class="line"></span><br><span class="line">model = CountSpace()</span><br><span class="line">model.train(corpus_fname)</span><br><span class="line">model.save(model_fname, json_format=False)</span><br><span class="line"></span><br><span class="line">model.load_model(model_fname, json_format=False)</span><br><span class="line">model.correct(<span class="string">"어릴때보고 지금다시봐도 재밌어요"</span>)</span><br></pre></td></tr></table></figure><h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[어릴때 보고 지금 다시봐도 재밌어요]</span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/19/NLP_02/#disqus_thread</comments>
    </item>
    
    <item>
      <title>임베딩이란?</title>
      <link>https://heung-bae-lee.github.io/2020/01/16/NLP_01/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/16/NLP_01/</guid>
      <pubDate>Thu, 16 Jan 2020 08:29:53 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;컴퓨터가-바라보는-문자&quot;&gt;&lt;a href=&quot;#컴퓨터가-바라보는-문자&quot; class=&quot;headerlink&quot; title=&quot;컴퓨터가 바라보는 문자&quot;&gt;&lt;/a&gt;컴퓨터가 바라보는 문자&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/image/encoding_with
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="컴퓨터가-바라보는-문자"><a href="#컴퓨터가-바라보는-문자" class="headerlink" title="컴퓨터가 바라보는 문자"></a>컴퓨터가 바라보는 문자</h2><p><img src="/image/encoding_with_asciii.png" alt="컴퓨터가 문자를 해석하는 방법"></p><ul><li>아래와 같이 문자는 컴퓨터가 해석할 때 그냥 기호일 뿐이다. 이렇게 encoding된 상태로 보게 되면 아래와 같은 문제점이 발생할 수 있다.</li></ul><p><img src="/image/what_is_different_love_like.png" alt="컴퓨터가 보는 단어"></p><ul><li>이 글자가 어떤 글자인지를 표시할 수 있고 그에 따른 특성을 갖게 하려면 우선 계산할 수 있게 숫자로 만들어 주어야 할 것이다. 그러한 방법 중 가장 단순한 방법이 One-hot encoding을 통한 것이다. 허나, 이러한 Sparse matrix를 통한 계산은 너무 비효율 적이다. 그렇다면 어떻게 dense하게 표현할 수 있을지를 고민하는 것이 바로 Embedding이라는 개념의 본질일 것이다.</li></ul><p><img src="/image/why_is_sparse_matirx_one_hot_encoding.png" alt="One-hot encoding"></p><p><img src="/image/Dense_representation.png" alt="Dense representation"></p><h1 id="임베딩-Embedding-이란"><a href="#임베딩-Embedding-이란" class="headerlink" title="임베딩(Embedding)이란?"></a>임베딩(Embedding)이란?</h1><ul><li>자연어 처리(Natural Language Processing)분야에서 임베딩(Embedding)은 <code>사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자형태인 vector로 바꾼 결과 혹은 그 일련의 과정 전체를 의미</code>한다. 가장 간단한 형태의 임베딩은 단어의 빈도를 그대로 벡터로 사용하는 것이다. <code>단어-문서 행렬(Term-Document Matrix)</code>는 row는 단어 column은 문서에 대응한다.</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">구분</th><th style="text-align:center">메밀꽃 필 무렵</th><th style="text-align:center">운수 좋은 날</th><th style="text-align:center">사랑 손님과 어머니</th><th style="text-align:center">삼포 가는 길</th></tr></thead><tbody><tr><td style="text-align:center">기차</td><td style="text-align:center">0</td><td style="text-align:center">2</td><td style="text-align:center">10</td><td style="text-align:center">7</td></tr><tr><td style="text-align:center">막걸리</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr><tr><td style="text-align:center">선술집</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">0</td></tr></tbody></table></div><ul><li>위의 표에서 운수좋은 날이라는 문서의 임베딩은 [2, 1, 1]이다. 막걸리라는 단어의 임베딩은 [0, 1, 0, 0]이다. 또한 사랑 손님과 어머니, 삼포 가는 길이 사용하는 단어 목록이 상대적으로 많이 겹치고 있는 것을 알 수 있다. 위의 Matrix를 바탕으로 우리는 사랑 손님과 어머니는 삼포 가는 길과 기차라는 소재를 공유한다는 점에서 비슷한 작품일 것이라는 추정을 해볼 수 있다. 또 막걸리라는 단어와 선술집이라는 단어가 운수 좋은 날이라는 작품에만 등장하는 것을 알 수 있다. <code>막걸리-선술집 간 의미 차이가 막걸리 기차 보다 작을 것이라고 추정해 볼 수 있다.</code></li></ul><h2 id="임베딩의-역할"><a href="#임베딩의-역할" class="headerlink" title="임베딩의 역할"></a>임베딩의 역할</h2><ul><li><p>1) <code>단어/문장 간 관련도 계산</code></p><ul><li>단어-문서 행렬은 가장 단순한 형태의 임베딩이다. 현업에서는 이보다 복잡한 형태의 임베딩을 사용한다. 대표적인 임베딩 기법은 <code>Word2Vec</code>을 뽑을 수 있을 것이다. 이렇듯 컴퓨터가 계산하기 쉽도록 <code>단어를 전체 단어들간의 관계에 맞춰 해당 단어의 특성을 갖는 벡터로 바꾸면 단어들 사이의 유사도를 계산하는 일이 가능</code>해진다. 자연어일 때 불가능했던 유사도를 계산할 수코사인 유사도 계산이 임베딩 덕분에 가능하다는 것이다. 또한 임베딩을 수행하면 <code>벡터 공간을 기하학적으로 나타낸 시각화 역시 가능</code>하다.</li></ul></li><li><p>2) <code>의미적/문법적 정보 함축</code></p><ul><li>임베딩은 벡터인 만큼 사칙 연산이 가능하다. <code>단어 벡터 간 덧셈/뺄셈을 통해 단어들 사이의 의미적, 문법적 관계를 도출해낼 수 있다.</code> 예를들면, 아들 - 딸 + 소녀 = 소년이 성립하면 성공적인 임베딩이라고 볼 수 있다. <code>아들 - 딸 사이의 관계와 소년 - 소녀 사이의 의미 차이가 임베딩에 함축돼 있으면 품질이 좋은 임베딩이라 말할 수 있다는 이야기</code>이다. 이렇게 단어 임베딩을 평가하는 방법을 <code>단어 유추 평가(word analogy test)</code>라고 부른다.</li></ul></li><li><p>3) <code>전이학습(Transfer learning)</code></p><ul><li><code>품질 좋은 임베딩은 모형의 성능과 모형의 수렴속도가 빨라지는데 이런 품질 좋은 임베딩을 다른 딥러닝 모델의 입력값으로 사용하는 것을 transfer learning</code>이라 한다. 예를 들면, 대규모 Corpus를 활용해 임베딩을 미리 만들어 놓는다. 임베딩에는 의미적, 문법적 정보 등이 녹아 있다. 이 임베딩을 입력값으로 쓰는 전이 학습 모델은 문서 분류라는 업무를 빠르게 잘 할 수 있게 되는 것이다.</li></ul></li></ul><h2 id="임베딩-기법의-역사와-종류"><a href="#임베딩-기법의-역사와-종류" class="headerlink" title="임베딩 기법의 역사와 종류"></a>임베딩 기법의 역사와 종류</h2><ul><li><p>통계 기반 -&gt; 뉴럴 네트워크 기반</p><ul><li><p>통계 기반 기법</p><ul><li><code>잠재 의미 분석(Latent Semantic Analysis)</code> : 단어 사용 빈도 등 <code>Corpus의 통계량 정보가 들어 있는 행렬에 특이값 분해등 수학적 기법을 적용해 행렬에 속한 벡터들의 차원을 축소하는 방법</code>이다. 차원을 축소하는 이유는 예를 들어 Term-Document matrix 같은 경우는 row가 더 큰 sparse matrix일 확률이 높기 때문에 쓸데 없이 계산량과 메모리자원을 낭비하는 것을 예방하기 위해서이다. 여기서 차원 축소를 통해 얻은 행렬을 기존의 행렬과 비교했을 때 단어를 기준으로 했다면 단어 수준 임베딩, 문서를 기준으로 했다면 문서 임베딩이된다. <code>잠재 의미 분석 수행 대상 행렬은 여러 종류가 될 수 있으며, Term-Document Matrix, TF-IDF Matrix, Word-Context Matrix, PMI Matrix등</code>이 있다.</li></ul></li><li><p>Neural Network 기반 기법</p><ul><li>Neural Probabilistic Language Model이 발표된 이후 부터 Neural Network기반의 임베딩 기법들이 주목 받고 있다. <code>Neural Network는 구조가 유연하고 표현력이 풍부하기 때문에 자연어의 무한한 문맥을 상당 부분 학습할 수 있다.</code></li></ul></li></ul></li><li><p>단어 수준 -&gt; 문장 수준</p><ul><li><p><code>단어 수준 임베딩 기법</code> : 각각의 벡터에 해당 <code>단어의 문맥적 의미를 함축</code>하지만, 단어의 형태가 동일하다면 동일단어로 인식하고, 모든 문맥 정보를 해당 단어 벡터 투영하므로 <code>동음이의어를 분간하기 어렵다는 단점</code>이 있다.</p><ul><li>ex) NPLM, Word2Vec, GloVe, FastText, Swivel 등</li></ul></li><li><p><code>문장 수준 임베딩 기법</code> : 2018년 초에 ELMo(Embedding from Language Models)가 발표된 이후 주목 받기 시작했다. <code>개별 단어가 아닌 단어 Sequence 전체의 문맥적 의미를 함축 하기 때문에 단어 임베딩 기법보다 Transfer learning 효과가 좋은 것으로 알려져 있다.</code> 또한, 단어 수준 임베딩의 단점인 <code>동음이의어도 문장수준 임베딩 기법을 사용하면 분리해서 이해할 수 있다.</code></p><ul><li>ex) BERT(Bidirectional Encoder Representations from Transformer), GPT(Generation Pre-Training) 등</li></ul></li></ul></li><li><p>Rule based -&gt; End to End -&gt; Pre-training/fine tuning</p><ul><li><p>1990년대에는 자연어 처리 모델 대부분은 우리가 딥러닝과 달리 머신러닝처럼 사람이 Feature를 직접 뽑았다. 그렇기에 Feature를 추출할 때 언어학적인 지식을 활용해야 했다. 허나. 2000년대 중반 이후 NLP 분야에서도 딥러닝 모델이 주목받기 시작하여 Feature를 직접 뽑지 않아도 되었다. 데이터를 넣어주면 사람의 개입없이 모델 스스로 처음부터 끝까지 이해하는 End-to-End Model 기법을 사용하였다. 대표적으로는 기계번역에 널리 사용됐던 Sequence-to-Sequence 모델이 있다. <code>2018년 ELMo 모델이 제안된 이후 NLP 모델은 pre-training과 fine tuning 방식으로 발전하고 있다.</code></p></li><li><p>우선 대규모 Corpus로 임베딩을 만든다.(Pre-train) 이 임베딩에는 Corpus의 의미적, 문법적 맥락이 포함돼 있다. 이후 임베딩을 입력으로 하는 새로운 딥러닝 모델을 만드로 우리가 풀고 싶은 구체적 문제에 맞는 소규모 데이터에 맞게 임베딩을 포함한 모델 전체를 업데이트한다.(fine tuning) <code>ELMo, GPT, BERT등</code>이 이 방식에 해당된다.  </p></li></ul></li><li><p>우리가 풀고 싶은 자연어 처리의 구체적 문제들(예시 : 품사 판별(Part-Of-Speech tagging), 개체명 인식(Named Entity Recognition), 의미역 분석(Semantic Role Labeling))을 <code>다운 스트림 태스크(DownStream task)</code>라고 한다. 다운스트림에 앞서 해결해야 할 과제라는 뜻의 <code>업스트림 테스크(UpStream task)</code>는 단어/문장 임베딩을 Pre-train하는 작업이 해당된다.</p></li></ul><h2 id="임베딩의-종류와-성능"><a href="#임베딩의-종류와-성능" class="headerlink" title="임베딩의 종류와 성능"></a>임베딩의 종류와 성능</h2><h3 id="1-행렬-분해"><a href="#1-행렬-분해" class="headerlink" title="1) 행렬 분해"></a>1) 행렬 분해</h3><ul><li><code>Corpus 정보가 들어 있는 원래 행렬을 Decomposition을 통해 임베딩</code>하는 기법이다. <code>Decomposition 이후엔 둘 중 하나의 행렬만 사용하거나 둘을 sum하거나 concatenate하는 방식으로 임베딩을 한다.</code><ul><li>ex) GloVe, Swivel 등</li></ul></li></ul><h3 id="2-예측-기반"><a href="#2-예측-기반" class="headerlink" title="2) 예측 기반"></a>2) 예측 기반</h3><ul><li><code>어떤 단어 주변에 특정 단어가 나타날지 예측하거나, 이전 단어들이 주어졌을 때 다음 단어가 무엇일지 예측하거나, 문장 내 일부 단어를 지우고 해당 단어가 무엇일지 맞추는 과정에서 학습하는 방법</code><ul><li>Neural Network기반 방법들이 속한다. ex) Word2Vec, FastText, BERT, ELMo, GPT 등</li></ul></li></ul><h3 id="3-토픽-기반"><a href="#3-토픽-기반" class="headerlink" title="3) 토픽 기반"></a>3) 토픽 기반</h3><ul><li><code>주어진 문서에 잠재된 주제를 추론하는 방식으로 임베딩을 수행</code>하는 기법이며, 대표적으로 <code>잠재 디리클레 할당(LDA)</code>가 있다. LDA 같은 모델은 <code>학습이 완료되면 각 문서가 어떤 주제 분포를 갖는지 확률 벡터 형태로 반환하기 때문에 임베딩 기법의 일종으로 이해</code>할 수 있다.</li></ul><h2 id="NLP-용어-정리"><a href="#NLP-용어-정리" class="headerlink" title="NLP 용어 정리"></a>NLP 용어 정리</h2><h3 id="Corpus-말뭉치"><a href="#Corpus-말뭉치" class="headerlink" title="Corpus(말뭉치)"></a>Corpus(말뭉치)</h3><ul><li>임베딩 학습이라는 특정한 목적을 가지고 수집한 표본이다.</li></ul><p><img src="/image/what_is_Corpus.png" alt="Corpus"></p><h3 id="Collection-컬렉션"><a href="#Collection-컬렉션" class="headerlink" title="Collection(컬렉션)"></a>Collection(컬렉션)</h3><ul><li>Corpus에 속한 각가의 집합을 칭한다.<ul><li>예를 들어, 한국어 위키백과와 네이버 영화 리뷰를 말뭉치로 쓴다면 이들 각각이 컬렉션이 된다.</li></ul></li></ul><h3 id="Sentence-문장"><a href="#Sentence-문장" class="headerlink" title="Sentence(문장)"></a>Sentence(문장)</h3><ul><li>생각이나 감정을 말과 글로 표현할 때 완결된 내용을 나타내는 최소의 독립적인 형식 단위를 가리킨다. 실무에서는 <code>주로 문장을 마침표(.)나 느낌표(!), 물음표(?)와 같은 기호로 구분된 문자열을 문장으로 취급</code>한다.</li></ul><h3 id="Document-문서"><a href="#Document-문서" class="headerlink" title="Document(문서)"></a>Document(문서)</h3><ul><li>생각이나 감정, 정보를 공유하는 문장 집합을 의미한다. 문서는 단락(Paragraph)의 집합으로 표현될 수 있다. <code>별도의 기준이 없다면 줄바꿈(\n) 문자로 구분된 문자열을 문서로 취급한다.</code></li></ul><h3 id="Token-토큰"><a href="#Token-토큰" class="headerlink" title="Token(토큰)"></a>Token(토큰)</h3><ul><li>문장은 여러개의 토큰으로 구성된다. 토큰은 단어(Word), 형태소(Morpheme), 서브워드(subword)라고도 한다. 문장을 토큰 시퀀스로 분석하는 과정을 토크나이즈(tokenize)라고 한다.</li></ul><h3 id="Vocabulary-어휘집합"><a href="#Vocabulary-어휘집합" class="headerlink" title="Vocabulary(어휘집합)"></a>Vocabulary(어휘집합)</h3><ul><li>Corpus에 있는 모든 Document를 Sentence로 나누고 여기에 Tokenize를 실행한 후 중복을 제거한 Token들의 집합이다. Vocabulary에 없는 token은 <code>미등록 단어(Unknown word)</code>라고 한다.</li></ul><h1 id="벡터가-어떻게-의미를-가지게-되는가"><a href="#벡터가-어떻게-의미를-가지게-되는가" class="headerlink" title="벡터가 어떻게 의미를 가지게 되는가"></a>벡터가 어떻게 의미를 가지게 되는가</h1><ul><li><p>자연어의 의미를 임베딩에 녹여내는 방법은 <code>자연어의 통계적 패턴 정보를 통째로 임베딩에 넣는 것</code>이다. 자연어의 의미(문법적 의미, 단어의 의미등)는 그 언어를 사용하는 사람들의 일상 언어에 정보가 들어있기 때문이다. <code>임베딩을 만들 때 사용하는 통계 정보는 크게 3가지가 있다.</code></p><ul><li>1) 문장에 어떤 단어가 많이 쓰였는지 -&gt; <code>bag of words(백오브워즈) 가정</code></li><li>2) 단어가 어떤 순서로 등장하는지 -&gt; <code>Language model(언어 모델) 가정</code></li><li>3) 문장에 어떤 단어가 같이 나타났는지 -&gt; <code>distribution hypothesis(분포가정)</code></li></ul></li></ul><h3 id="1-BOW-Bag-Of-Words-가정"><a href="#1-BOW-Bag-Of-Words-가정" class="headerlink" title="1) BOW(Bag-Of-Words) 가정"></a>1) <code>BOW(Bag-Of-Words) 가정</code></h3><ul><li>문서의 저자가 생각한 주제가 문서에서의 단어 사용에 녹아있다는 생각으로부터 <code>단어의 순서 정보는 무시하고 어떤 단어가 많이 쓰였는지 정보를 중시</code>한다. 경우에 따라서는 빈도 역시 단순화해 등장 여부(등장 시 1, 아니면 0)만을 사용하기도 한다. 간단한 아이디어지만 <code>정보 검색(information Retrieval)분야에서 여전히 많이 쓰이고 있다.</code> 사용자 질의에 가장 적절한 문서를 보여줄 때 질의를 BOW 임베딩으로 변환하고 질의와 검색 대상 문서 임베딩 간 코사인 유사도를 구해 가장 높은 문서를 사용자에게 노출 한다.<ul><li>대표 통계량 : TF-IDF<a href="https://heung-bae-lee.github.io/2019/12/14/Recommendation_System_00/">개념을 모른다면 클릭</a></li><li>대표 모델 : Deep Averaging Network<ul><li>단어의 순서를 고려하지 않고 단어의 임베딩을 평균을 취해 만든다. 간단한 구조임에도 성능이 좋아서 현업에서도 자주 쓰인다.</li></ul></li></ul></li></ul><h3 id="2-Language-model-가정"><a href="#2-Language-model-가정" class="headerlink" title="2) Language model 가정"></a>2) <code>Language model 가정</code></h3><pre><code>- `시퀀스에 확률을 부여하여 단어 시퀀스를 명시적(순서를 고려)으로 학습하는 모델`</code></pre><h4 id="2-1-통계-기반-언어-모델"><a href="#2-1-통계-기반-언어-모델" class="headerlink" title="2-1) 통계 기반 언어 모델"></a>2-1) 통계 기반 언어 모델</h4><ul><li>단어가 n개 주어진 상황이라면 Language model은 n개 단어가 동시에 나타날 확률을 반환한다. 통계 기반의 언어 모델은 <code>말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습</code>한다. 잘 학습된 언어 모델이 있다면 주어진 단어 시퀀스 다음 단어로 확률이 높은 자연스러운 단어를 선택할 것이다. 구체적인 방법은 <code>한 상태의 확률은 그 직전 상태에만 의존한다는 Markov assumption에 기반하여 n-gram을 통해 확률을 계산</code>할 수 있다. <code>허나 데이터에 한 번도 등장하지 않는 n-gram이 존재할 때 예측 단계에서는 확률값을 0으로 취하는 문제가 있다.</code></li></ul><script type="math/tex; mode=display">P(w_{n}|w_{n-1} =  \frac{w_{n-1}}{w_{n}})</script><ul><li>위의 문제점들을 해결하기 위해 Back-off, Smoothing 등의 방식이 제안됐다.</li><li>1) <code>Back-off</code><ul><li><code>n-gram 등장 빈도가 0인 단어들이 있을 수 있으므로 n-gram 등장빈도를 n보다 작은 범위의 단어 시퀀스 빈도로 근사하는 방식</code>이다. n을 크게 하면 할수록 등장하지 않는 케이스가 많아질 가능성이 높기 때문이다. $\alpha, \beta$는 실제 빈도와의 차이를 보정해주는 parameter이다.</li></ul></li></ul><script type="math/tex; mode=display">Freq(내 마음 속에 영원히 기억될 최고의 명작이다) \approx \alpha Freq(영원히 기억될 최고의 명작이다) + \beta</script><ul><li>2) (Add-k) Smoothing<ul><li><code>등장 빈도 표에 모두 k 만큼 더하는 기법</code>이다. 만약 k=1로 설정한다면 특별히 <code>라플라스  스무딩(laplace smoothing)</code>이라고 한다. <code>스무딩을 시행하면 높은 빈도를 가진 문자열 등장 확률을 일부 깎고 전혀 등장하지 않는 케이스들에는 약간의 확률을 부여하게 된다.</code></li></ul></li></ul><h4 id="2-2-뉴럴-네트워크-기반-언어-모델"><a href="#2-2-뉴럴-네트워크-기반-언어-모델" class="headerlink" title="2-2) 뉴럴 네트워크 기반 언어 모델"></a>2-2) 뉴럴 네트워크 기반 언어 모델</h4><ul><li>Neural Network는 입력과 출력 사이의 관계를 유연하게 포착해낼 수 있고, 그 자체로 확률 모델로 기능할 수 있다. <code>주어진 단어 시퀀스를 가지고 다음 단어를 예측하는 과정에서 학습</code>된다. <code>학습이 완료되면 이들 모델의 중간 혹은 말단 계산 결과물을 단어나 문자의 임베딩으로 활용</code>한다. <code>Language model 기반 기법은 순차적으로 입력받아 다음 단어를 맞춰야 하기 때문에 일방향(uni-directional)이지만 Masked language model은 문장 전체를 다 보고 중간에 있는 단어를 예측하기 때문에 양방향(bi-directional)학습이 가능하다.</code> 그로인해 Masked Language model 기반의 방법들(예:BERT)은 기존 Language model 기법들 대비 임베딩 품질이 좋다.<ul><li>대표 모델 : <code>ELMo, GPT</code> 등</li></ul></li></ul><h3 id="3-Distribution-hypothesis"><a href="#3-Distribution-hypothesis" class="headerlink" title="3) Distribution hypothesis"></a>3) Distribution hypothesis</h3><ul><li>자연어 처리에서 분포란 특정 범위, 즉 Window(해당 단어를 중심으로 범위에 포함시킬 앞뒤 단어 수, 예를 들어 윈도우가 2라면 타깃 단어 앞뒤로 2개의 문맥단어의 빈도를 계산) 내에 동시에 등장하는 이웃 단어 또는 문맥(context)의 집합을 가리킨다. <code>어떤 단어 쌍이 비슷한 문맥 환경에서 자주 등장한다면 그 의미 또한 유사할 것이라는 것이 Distribution hypothesis의 전제</code>이다. <code>형태소의 경계를 정하거나 품사를 나누는 것과 같은 다양한 언어학적 문제는 말뭉치의 분포 정보와 깊은 관계를 갖고 있다.</code> 이 덕분에 <code>임베딩에 분포 정보를 함축하게 되면 해당 벡터에 해당 단어의 의미를 내제시킬 수 있는 것</code>이다.<ul><li>대표 통계량 : PMI(Pointwise Mutual Information :  점별 상호 정보량)<ul><li><code>두 단어의 등장이 독립일 때 대비해 얼마나 자주 같이 등장하는지를 수치화한 것</code></li></ul></li></ul></li></ul><script type="math/tex; mode=display">PMI(A, B) = log\frac{P(A,B)}{P(A)P(B)}</script><ul><li>Term-context matrix는 특정 단어 기준으로 Window에 존재하는 단어들을 count하는 방식으로 만들어지는데, 여기에서 PMI 수식을 적용시키면된다. 이렇게 구축한 <code>PMI 행렬의 행 벡터 자체를 해당 단어의 임베딩으로 사용할 수도 있다.</code></li></ul><ul><li>대표 모델 : <code>Word2Vec</code></li></ul><p><img src="/image/word_smillarity_in_word2vec.png" alt="word2vec"></p><ul><li><code>CBOW 모델</code><ul><li>문맥 단어들을 가지고 타깃 단어 하나를 맞추는 과정에서 학습된다.<ul><li>1) 각 주변 단어들을 one-hot 벡터로 만들어 입력값으로 사용 (입력층 벡터)</li><li>2) 가중치 행렬을 각 one-hot 벡터에 곱해서 n-차원 벡터를 만든다. (N-차원 은닉층)</li><li>3) 만들어진 n-차원 벡터를 모두 더한 후 개수로 나눠 평균 n-차원 벡터를 만든다. (출력층 벡터)</li><li>4) n-차원 벡터에 다시 가중치 행렬을 곱해서 one-hot 벡터와 같은 차원의 벡터로 만든다.</li><li>5) 만들어진 벡터를 실제 예측하려고 하는 단어의 one-hot 벡터와 비교해서 학습한다.</li></ul></li></ul></li></ul><p><img src="/image/step_by_step_CBOW.png" alt="CBOW 모델 01"></p><p><img src="/image/step_by_step_CBOW_01.png" alt="CBOW 모델 02"></p><p><img src="/image/Projection_layer_in_CBOW.png" alt="CBOW 모델의 Projection Layer"></p><p><img src="/image/Output_layer_in_CBOW.png" alt="CBOW 모델의 Output Layer"></p><ul><li><p><code>Skip-gram 모델</code></p><ul><li><p>타깃 단어를 가지고 문맥 단어가 무엇일지 예측하는 과정에서 학습된다.</p><ul><li>1) 하나의 단어를 one-hot 벡터로 만들어서 입력값으로 사용한다.(입력층 벡터)</li><li>2) 가중치 행렬을 one-hot 벡터에 곱해서 n-차원 벡터를 만든다.(N-차원 은닉층)</li><li>3) n-차원 벡터에 다시 가중치 행렬을 곱해서 one-hot 벡터와 같은 차원의 벡터로 만든다.(출력층 벡터)</li><li>4) 만들어진 벡터를 실제 예측하려는 주변 단어들 각각의 one-hot 벡터와 비교해서 학습한다.</li></ul></li><li><p>두 모델의 확실한 차이점은 <code>CBOW에서는 입력값으로 여러 개의 단어를 사용하고, 학습을 위해 하나의 단어와 비교하지만, Skip-gram에서는 입력값이 하나의 단어를 사용하고, 학습을 위해 주변의 여러 단어와 비교</code>한다.</p></li><li><p><code>위의 학습 과정을 모두 끝낸 후 가중치 행렬의 각 행을 단어 벡터로 사용한다. 카운트 기반 방법(Bag of Words 가정 방법들)로 만든 단어 벡터보다 단어 간의 유사도를 잘 측정하며, 단어들의 복잡한 특징까지도 잘 잡아낸다는 장점</code>이 있다. <code>보통 CBOW보다 Skip-gram의 성능이 더 좋아 자주 사용된다. 하지만 무조건적으로 좋은 것은 아니다!</code></p></li></ul></li></ul><p><img src="/image/skip_gram_model_in_word2vec.png" alt="Skip-gram 모델"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/16/NLP_01/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Regression(03) - 회귀진단</title>
      <link>https://heung-bae-lee.github.io/2020/01/15/machine_learning_04/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/15/machine_learning_04/</guid>
      <pubDate>Wed, 15 Jan 2020 09:24:28 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;교호작용&quot;&gt;&lt;a href=&quot;#교호작용&quot; class=&quot;headerlink&quot; title=&quot;교호작용&quot;&gt;&lt;/a&gt;교호작용&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;성별, 결혼여부, 혹은 소속 정치단체 등과 같은 &lt;code&gt;질적(qualitative) 또는 범주형(
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="교호작용"><a href="#교호작용" class="headerlink" title="교호작용"></a>교호작용</h2><ul><li>성별, 결혼여부, 혹은 소속 정치단체 등과 같은 <code>질적(qualitative) 또는 범주형(categorical)요인들이 회귀분석에서 종속(반응)변수의 변화를 설명하는 데 매우 유용한 독립(설명) 변수 역할을 할 때</code>가 있다. 이런 질적 독립(설명)변수로 이용할 경우 이들은 <code>지시변수(Indicator variable) 또는 가변수(dummy variable)</code>의 형식으로 표현해야한다. 가변수는 다양한 용도를 가지고 있으며, 회귀관계에 영향을 주는 질적 요인을 고려할 때마다 항상 사용할 수 있다.</li></ul><p><img src="/image/interaction_term.png" alt="교호작용"></p><p>-<code>여러 범주를 표현하기 위하여 가변수를 사용할 경우 필요한 가변수의 개수는 일반적으로 가능한 범주의 수보다 하나 작게 잡으면 된다.</code>왜냐하면, 가변수를 종합하면 교육수준에 관한 3개의 범주를 나타낼 수 있기 때문이다. 게다가, 범주를 모두 다 지시변수로 사용하면 범주화된 변수들끼리 완벽한 선형관계가 성립되어 극단적인 다중공선성을 보일수 있다. 선형대수 측면에서도 각 Column vector들끼리 서로 linearly independent 해야 해를 갖을 수 있으므로 위의 방법으로 만드는 것이 옳은 방법이다. 여기서 지시변수 또는 가변수에 의하여 표현되지 않는 범주는 <code>기저범주(base category) 또는 대조 그룹(control group)이라고 불리는데, 지시변수의 회귀계수가 대조 그룹에 대한 상대적인 값으로 해석되기 때문이다.</code> 아래 표에서 만일, 최종학력이 대학원인 사람과 대학교인 사람의 평균적인 차이가 궁금할 경우는 $B_{2}-B-{1}=2,000$로 구할 수 있다. 또한 아래 <code>해석은 다른 변수들을 고정시켰을 경우에 해당</code>한다.</p><script type="math/tex; mode=display">수입 = ( \beta_{0} + 대학교 + 대학원 )</script><p><img src="/image/interaction_term_translation.png" alt="교호작용 해석"></p><h2 id="변수-선택법"><a href="#변수-선택법" class="headerlink" title="변수 선택법"></a>변수 선택법</h2><p><img src="/image/variable_selection.png" alt="변수 선택법"></p><p><img src="/image/FeedForward_selection.png" alt="점진적 선택법"></p><p><img src="/image/Backward_Elimination.png" alt="후진적 제거법"></p><p><img src="/image/stepwise_selection.png" alt="stepwise 방법"></p><h2 id="회귀-분석의-진단-모형-위반의-검출"><a href="#회귀-분석의-진단-모형-위반의-검출" class="headerlink" title="회귀 분석의 진단 : 모형 위반의 검출"></a>회귀 분석의 진단 : 모형 위반의 검출</h2><ul><li>주어진 데이터에 모형을 적합함에 있어서, 한 개 또는 몇 개의 관측 개체들에 의하여 적합이 과도하게 결정되는 것으 바람직 하지 않다. 앞서 말한 가설검정등은 <code>표준적인 회귀의 가정들이 만족될 때만 유의미</code>하다. <code>이들 가정이 위반된다면, 이전에 언급된 표준적인 결과들은 유효하지 않으며 결과의 응용이 심각한 오류를 야기할 수도 있다.</code> 모형위반을 검토하기 위해 엄격한 수치적 규칙들을 적용하는 것 대신에 주로 그래프적인 방법들을 소개할 것이다.</li></ul><h3 id="회귀분석의-표준적인-가정들"><a href="#회귀분석의-표준적인-가정들" class="headerlink" title="회귀분석의 표준적인 가정들"></a>회귀분석의 표준적인 가정들</h3><ul><li>1) <code>선형성 가정</code> : 종속(반응)변수 Y와 독립(설명)변수 X들을 관계시키는 모형이 회귀계수 $\beta$들에 대하여 선형임을 가정한다. <code>만약, 선형성 가정이 만족되지 않는다면 종종 데이터에 대한 변환을 통해 선형성을 달성할 수 있다.</code> 단순회귀에서는 이 가정을 Y와 X의 산점도를 통해 쉽게 확인 할 수 있으나, 다중회귀에서는 고차원성 때문에 산점도를 통해 확인이 어렵다.</li></ul><script type="math/tex; mode=display">Y = \beta_{0} + \beta_{1}X_{1} + \cdots + \beta_{p}X_{p} + \epsilon</script><ul><li><p>2) <code>잔차에 대한가정</code> : $\epsilon_{i}  \sim^{i.i.d} N(0, \sigma^{2})$ 이 가정을 통해 아래의 가정들을 만족해야한다.</p><ul><li>1) <code>잔차의 정규성</code> : 잔차 $\epsilon_{i}$는 정규분포를 따른다. 독립(설명)변수들의 값이 반복되어 있지 않다면 쉽게 위반되지 않는다.    </li><li>2) <code>잔차의 등분산성</code> : 동일한 상수분산 $\sigma^{2}$을 가져야 한다. 이 가정이 만족하지 않을 때 <code>이분산성을 띈다</code>는 문제가 있다고 한다.</li><li>3) <code>잔차의 독립성</code> : 잔차들이 서로 독립이므로 그들의 공분산은 모두 0이다. 이 가정이 만족되지 않으면 <code>자기상관의 문제</code>가 있다고 한다.</li></ul></li><li><p>3) 독립(설명)변수들에 대한 가정(<code>1,2는 실제로 평가 불가하므로 3이 중요!!</code>) :</p><ul><li>1) 독립(설명)변수들은 확률변수가 아니다. 만약 실험 설계에 의해서 얻어진 데이터 값들에 의한 것이 아닌 비실험 또는 관측의 상황에서는 이것이 만족되지 않을 거이라는 것은 명확하며, 이에 대한 해석도 수정되어야한다. <code>독립(설명)변수들이 확률변수이면 모든 추론은 관측된 데이터에 의존하여 조건부적</code>이다.</li><li>2) 값 $x_{1j}, x_{2j}, \cdots ,x_{nj}$는 오차 없이 측정된 것으로 가정된다. 허나 <code>이 가정은 만족되기 쉽지 않다</code>. 측정에서의 오차는 잔차의 분산, 다중상관계수, 회귀계수의 개별 추정치들에 영향을 줄 것이다. <code>추정된 회귀계수로부터 측정오차의 영향을 제거하는 것은 거의 기대하기 힘들다.</code>그러므로 변수들이 오차를 가지고 있어서 회귀계수의 추정에 문제가 있더라도 회귀방정식이 예측을 위해 여전히 사용될 수 있다. 그러나 독립(설명)변수에 존재하는 오차는 예측의 정확도를 감소 시킬 것이다.</li><li>3) 독립(설명)변수는 선형종속이 아닌 것으로 가정된다. 즉, 위해서 언급했었던 <code>linearly independent</code>해야 한다는 의미이며 이 가정으로 인해 정규방정식의 해의 유일성을 보장받을 수 있다. 이 가정이 위반 되는 것이 <code>공선성(collinearity)</code>의 문제이다.</li></ul></li><li><p>4) 관측개체에 대한 가정 : 모든 관측개체들은 동일하게 신뢰할 만하며, 회귀의 결과를 결정하고 결론을 도출함에 있어서 거의 동등한 역할을 한다.</p></li><li><p><code>최소제곱버의 특징 중 하나는 기본 가정에 대한 사소한 또는 작은 위반이 분석으로부터 도출된 추론이나 결론을 무효화할 만큼 큰 영향을 주지는 않는다는 것</code>이다. 그러나 모형의 가정에 대한 큰 위반은 결론을 심각하게 왜곡 시키므로 <code>결론적으로, 그래프를 통해서 잔차의 구조와 데이터의 패턴을 조사하는 것은 매우 중요</code>하다.</p></li></ul><p><img src="/image/diagonosis_of_linear_regression.png" alt="회귀분석의 진단"></p><h3 id="다양한-유형의-잔차들"><a href="#다양한-유형의-잔차들" class="headerlink" title="다양한 유형의 잔차들"></a>다양한 유형의 잔차들</h3><ul><li>회귀분석에 있어서 모형이 가지는 가능한 결함을 찾아내는 데 가장 간단하고 효과적인 방법은 잔차플롯을 살펴보는 것이다. <code>더욱이, 분석이 요약통계량에만 근거할 경우 간과할지도 모를 데이터의 중요한 구조와 정보들을 잔차분석을 통해 발견할 수도 있다.</code></li></ul><script type="math/tex; mode=display">\hat{Y} = X\hat{\beta} = PY</script><script type="math/tex; mode=display">P = X(X^{T}X)X{T}</script><ul><li>즉, Y를 $\hat{Y}$로 만들기 위한 linear transform matrix를 모자(hat) 또는 사영(Projection) matrix P라고 한다. 여기서 $i=j일 때, p_{ii}=p_{ij}=p_{ji}=p_{jj}$는 사영행렬(P)의 i번째 대각원소이다. <code>이것은 i번째 관측개체에 대한 지레값(Leverage value)으로 불린다.</code> 아래의 식에서 볼 수 있듯이 $\hat{y}_{i}$은 Y의 모든 관측값들의 가중합이며, $p_{ii}$는 i번째 적합값 $\hat{y}_{i}$을 결정함에 있어서 $y_{i}$에 부여되는 가중치(지레)이기 때문이다.</li></ul><script type="math/tex; mode=display">\hat{y}_{i} = p_{i1}y_{1} + p_{i2}y_{2} + \cdots + p_{in}y_{n}, i=1,2,...,n</script><ul><li>또한, 잔차($e_{i}$)의 분산은 그의 표준편차로 나누어 표준화하여 다음과 같이 평균 0과 표준편차 1을 가지는  <code>표준화 잔차(standardized residual)</code>을 얻을 수 있다. 자세하게는 $\sigma$를 어떤 것을 사용하냐에 따라 내적 표준화잔차와 외적 표준화잔차로 나뉘어지지만, 결국 <code>표본크기가 충분히 클때(30이상) 이 잔차들은 근사적으로 표준정규분포를 따른다. 또한 잔차들을 엄밀하게는 서로 독립이 아니지만, 표본크기가 크면 독립성의 문제는 무시 될 수 있다.</code> 따라서, <code>잔차플롯을 작성함에 있어서 두가지 형태의 잔차 중 어느 것을 사용하는가는 별로 문제가 되지 않는다.</code></li></ul><script type="math/tex; mode=display">z_{i} = \frac{e_{i}}{\sigma \sqrt{1-p_{ii}}}</script><h2 id="그래프적-방법들"><a href="#그래프적-방법들" class="headerlink" title="그래프적 방법들"></a>그래프적 방법들</h2><ul><li>그래프적 방법들은 데이터 분석에서 중요한 역할을 하며, 특히 데이터에 선형모형을 적합할 때 더욱 중요하다. <code>분석이 수치적 결과에만 의존한다면 잘못된 결론에 도달할 수 있음을 볼 수 있다.</code>그 대표적인 예로는 Anscombe의 데이터를 들 수 있다.</li></ul><p><img src="/image/Anscombe_data.png" alt="Anscombe data"></p><ul><li><code>특정 그래프를 탐색하기에 앞서, 어떤 가정이 만족될 때 그 그래프가 어떻게 나타나야 하는지를 알아야 한다. 그러고 나서 그 그래프가 기대와 일치하는지를 살펴보아야 한다.</code> 이렇게 함으로써 가정의 올바름 또는 그릇됨을 확인할 수 있을 것이다.</li></ul><h3 id="1-모형을-적합하기-이전의-그래프"><a href="#1-모형을-적합하기-이전의-그래프" class="headerlink" title="1) 모형을 적합하기 이전의 그래프"></a>1) 모형을 적합하기 이전의 그래프</h3><ul><li><p><code>종속(반응)변수와 독립(설명)변수 사이의 관계를 나타내는 모형의 형태는 이론적 배경 또는 검정될 가설에 근거해야 한다.</code></p></li><li><p>1) 일차원 그래프 : 개별 변수의 분포를 개략적으로 살펴보기 위해 그린다. 이를 통해 <code>어떤 변수가 매우 치우쳐져 있다면 변환이 수행되어야 한다. 비대칭의 정도가 심한 변수에 대하여 로그 변환이 추천된다.</code> 일변량 그래프는 원래의 변수를 이용해야 할지 아니면 변환된 변수를 가지고 분석을 수행해야 하지에 대하여 정보를 제공한다. 또한 일변량 그래프는 <code>변수에 있는 특이값의 존재 유무를 제시</code>한다. 특이값은 그것이 입력오류 등에 의한 것인지(측정후 잘못 기입된 경우와 같은)를 알아보기 위해 조사되어야한다. 또한 특이값은 이후의 분석에서 문제를 발생시킬 수도 있기 때문에 분석을 수행할 때 주의깊게 다루어져야 한다.</p><ul><li>ex) histogram, stem-and-leaf display, dot plot, box plot</li></ul></li><li><p>2) 이차원 그래프 : 변수의 수가 많은 경우 해당 차원과 같은 차원에서 변수들을 볼 수 없으므로, <code>각 변수들의 쌍에 대한 관계를 탐색하고 일반적인 패턴을 파악하기 위해</code> 산점도를 통해 살펴볼 수 있다. <code>산점도행렬</code>을 살펴볼때 주의할 점은 상관계수는 오직 선형관계만을 측정하며 robust하지 않으므로 <code>쌍별 상관계수는 대응되는 산점도와 연관하여 해석</code>해야 한다는 점이다. 단순회귀에서는 Y대 X의 산점도가 선형의 형태를 보일 것으로 기대되나, <code>다중회귀에서는 Y대 각 독립(설명)변수의 산점도가 선형의 형태를 보일 수도 있고 그렇지 않을 수도 있다. 즉, 선형의 형태가 보이지 않는다고 해서 주어진 선형모형이 옳지 않다는 것을 의미하지 않늗다.</code> 또한, <code>각각의 독립(설명)변수들끼리 선형패턴을 보이지 않아야 한다. 산점도에 선형관계가 보이지 않는다는 것이 전체 독립(설명)변수들의 집합이 선형적으로 독립이라는 것을 의미하지는 않기 때문에 주의가 필요하다. 선형 관계는 두개 이상의 변수들을 포함하고 있을 수 있다. scatter plot을 통해서는 그런 다변량 관계를 검출하는 것이 쉽지 않다.</code> 그러한 다중공선성 문제는 앞서 다룬 방법과 같이 해결하려고 해보아야 한다.</p></li><li><p>3) 회전도표</p></li><li>4) 동적그래프</li></ul><h3 id="2-모형을-적합한-이후의-그래프"><a href="#2-모형을-적합한-이후의-그래프" class="headerlink" title="2) 모형을 적합한 이후의 그래프"></a>2) 모형을 적합한 이후의 그래프</h3><ul><li>앞에서 소개된 그래프들은 데이터 검토와 모형설정 단계에서 유용하다. 데이터에 모형을 적합한 이후의 그래프들은 <code>가정들을 검토하고 주어진 모형의 적합도를 평가하는 데 도움을 준다.</code></li></ul><p><img src="/image/diagonosis_of_linear_regression_01.png" alt="잔차의 정규성"></p><p><img src="/image/diagonosis_of_linear_regression_02.png" alt="회귀진단을 하기위한 그래프들"></p><h4 id="1-선형성과-정규성-가정을-검토하기-위한-그래프"><a href="#1-선형성과-정규성-가정을-검토하기-위한-그래프" class="headerlink" title="1) 선형성과 정규성 가정을 검토하기 위한 그래프"></a>1) 선형성과 정규성 가정을 검토하기 위한 그래프</h4><h5 id="표준화잔차의-정규확률-plot-Q-Q-plot"><a href="#표준화잔차의-정규확률-plot-Q-Q-plot" class="headerlink" title="표준화잔차의 정규확률 plot (Q-Q plot)"></a>표준화잔차의 정규확률 plot (Q-Q plot)</h5><ul><li>표준화 잔차의 분위수와 표준정규분포의 분위수의 scatter plot이라고 보면된다. 만약 잔차가 정규성을 띈다면 대각선과 최대한 비슷하게 그려져야한다.</li></ul><p><img src="/image/Q_Q_plot_tail.png" alt="Q-Q plot의 tail"></p><p><img src="/image/Q_Q_plot_tail_01.png" alt="Q-Q plot의 나올수 있는 tail의 형태"></p><h5 id="Standardized-Residual-vs-Predictor-독립변수-산점도"><a href="#Standardized-Residual-vs-Predictor-독립변수-산점도" class="headerlink" title="(Standardized) Residual vs Predictor(독립변수) 산점도"></a>(Standardized) Residual vs Predictor(독립변수) 산점도</h5><ul><li>표준적인 가정 하에서 표준화잔차는 각 독립(설명)변수들과 상관되어 있지 않다. 이 가정이 만족된다면 이 플롯은 <code>랜덤하게 흩어진 점들이 나타나야 한다.</code> 이 plot에서 특정한 패턴이 발견된다면 어떤 가정들이 위반되었음을 의미한다. 아래 그림에서 <code>(a)는 선형성 가정이 만족되지 않았을 때 나타나느 플롯 중 하나이며, 이 경우에는 Y 또는 특정 예측 변수에 대한 변환이 선형석을 위하여 필요</code>할 수 있다. 그림 <code>(b)는 이분산성을 의미하며 분산의 안정화를 위하여 데이터 변환이 필요할 것</code>이다.</li></ul><p><img src="/image/standadized_Residual_vs_predictor.png" alt="표준화잔차 vs 설명변수"></p><h5 id="Standardized-Residual-vs-fitted-value-plot"><a href="#Standardized-Residual-vs-fitted-value-plot" class="headerlink" title="(Standardized) Residual vs fitted-value plot"></a>(Standardized) Residual vs fitted-value plot</h5><ul><li>표준적인 가정 하에서 표준화잔차는 적합값과도 상관되어 있지 않다. 따라서 이 가정이 만족된다면 이 plot은 <code>랜덤하게 흩어진 점들을 나타내야 한다.</code></li></ul><p><img src="/image/Residual_vs_fitted_value.png" alt="표준화잔차와 적합값 plot"></p><h5 id="표준화잔차의-인덱스-plot"><a href="#표준화잔차의-인덱스-plot" class="headerlink" title="표준화잔차의 인덱스 plot"></a>표준화잔차의 인덱스 plot</h5><ul><li><code>표준화잔차 vs 관측개체 번호의 plot</code>이다. 아래와 같이 해석할 수 있으며, 만일 <code>관측개체의 취해진 순서가 중요한 의미를 가진다면, (예컨데, 개체가 시간 또는 공간 상의 순서에 따라 취해졌을 때), 연속적인 순서에 의한 잔차 plot은 오차의 독립성 가정을 검토하기 위해 사용될 수 있다.</code> 독립성 가정 하에서 점들은 0 주위의 수평 띠(밴드) 안에서 랜덤하게 흩어져 있어야 한다.<br><img src="/image/Index_plot_of_Residuals.png" alt="표준화잔차의 인덱스 plot - 01"></li></ul><p><img src="/image/Index_plot_of_Residuals_01.png" alt="표준화잔차의 인덱스 plot - 02"></p><h4 id="2-특이값과-영향력-있는-개체를-검출하기-위한-그래프"><a href="#2-특이값과-영향력-있는-개체를-검출하기-위한-그래프" class="headerlink" title="2) 특이값과 영향력 있는 개체를 검출하기 위한 그래프"></a>2) 특이값과 영향력 있는 개체를 검출하기 위한 그래프</h4><h5 id="지레점-영햘력-특이값"><a href="#지레점-영햘력-특이값" class="headerlink" title="지레점, 영햘력, 특이값"></a>지레점, 영햘력, 특이값</h5><ul><li><p>주어진 데이터에 모형을 적합함에 있어서 <code>한두 개의 관측값들에 의해 적합이 과도하게 결정되면 분석이 제대로 이루어지지 않은 것이므로 이런 관측값들은 보통 잔차가 0에 가깝거나 0이기 때문에 특이값이 아니나 영향력있는 개체이다. 이런 상황에서는 잔차를 살펴보는 것은 거의 도움이 되지 않는다.</code></p></li><li><p>어떤 점이 제외되었을 때 혼자서 또는 다른 점들과 결합하여 적합모형(추정된 회귀계수, 적합값, t-통계량 등)에 큰 변화를 준다면 그 점을 <code>영향력 있는 점</code>이라고 한다. <code>일반적으로 어떤 점을 제외하면 약간이라도 적합에 변화가 있을 것이다. 여기에서의 관심은 그 점이 과도한 영향력이 있는가이다.</code><br>따라서, 영향력이 있는 관측개체가 데이터에 존재한다면 그것을 파악하는 것이 중요하다. <code>영향력 있는 개체는 일반적으로 종속(반응)변수 Y 또는 독립(설명)변수 X 공간에 대하여 특이값이다.</code></p><ul><li><p><code>반응(종속) 변수에 대한 특이값</code> : 잔차 plot을 통해 파악될 수 있으며, 잔차 plot은 존재하는 총체적인 모형위반들을 나타낼 것이며, 잔차 plot의 탐색은 분석에서 주요 도구 중 하나이다.</p></li><li><p><code>독립(설명) 변수에 대한 특이값</code> : 앞에서 설명한 지레값($p_{ii}$)는 X-공간에서 특이성을 측정하는 데 이용될 수 있다. <code>큰 지레값을 가지는 관측개체는 X-공간에서 특이값이기 때문</code>이다. 반응변수에 대한 특이값(큰 표준화잔차를 가진 점)과 구별하기 위하여 <code>높은 지레점(high leverage point)</code>라고 한다. 위의 반응 변수에 대한 특이값은 잔차 plot을 통해 충분히 살펴 볼 수 있지만, <code>독립(설명)변수에 대한 특이값은 잔차 plot으로는 찾아보기 힘들다.</code> 그 이유는 아래 잔차와 지레값의 관계에 대한 식을 살펴보면 <code>높은 지레값을 갖는 점들은 잔차가 낮기 때문</code>이다. 그러므로, <code>잔차 plot을 살펴보는 것만으로는 충분하지 않으므로 종속변수와 독립변수의 산점도에 회귀식을 그려보거나 지레값의 index plot을 그려 살펴 봐야 한다.</code> 통상적으로 사용되는 $p_{ii}$에 대한 임계값은 $2(p+1)/n=0.2$이다.</p></li></ul></li></ul><script type="math/tex; mode=display">p_{ii} + \frac{e^2_{i}}{SSE} \leq 1</script><h5 id="영향력의-측도"><a href="#영향력의-측도" class="headerlink" title="영향력의 측도"></a>영향력의 측도</h5><ul><li><code>Cook&#39;s distance</code> : <code>전체 데이터로부터 얻은 회귀계수들과 i번째 개체를 제거하고 얻은 회귀계수(또는 적합값)들의 차이를 측정</code>한다. <code>C값에 대한 index plot을 그려 C값들이 비슷한 값을 가지지 않다면 돋보이는 C값들을 갖는 데이터들을 제외하고 모형을 적합에 보는 등의 방법을 검토해 봐야할 것이다</code></li></ul><script type="math/tex; mode=display">C_{i} = \frac{sum^{n}_{j=1} (\hat{y_{j}} - \hat{y_{j}}_{i})^2}{\hat{\sigma^}^{2} (p+1)} , i=1,2, \cdots ,n</script><ul><li><p>이외의 Welsch &amp; Kuh의 측도(DFITS)와 Hadi의 영향력 측도가 있으나, Cook’s distance를 통해 충분히 검사가능하므로 생략하도록 한다. 다만, Welsh &amp; Kuh와 Cook’s distance는 잔차와 지레값에 대한 승법적(곱하는)함수인 반면에 Haid의 측도는 가법적(종속변수와 독립변수 각각에 대한 영향력의 수치를 더하는)함수이다.</p></li><li><p><code>특이값은 언제나 조심스럽게 조사되어야 되며 실무에서 분석시 함부로 제거해서는 안된다. 그 데이터 자체도 의미가 있을 수 있기 때문(예를 들면, 데이터가 모집단으로 부터 추출되지 않았다든가 또는 모형이 선형이 아니라는 것을 의미할 수 있기 때문)</code>이다. <code>지레대 효과는 높으나 영향력이 작은 경우는 큰 문제를 일으키지는 않는다. 그러나 높은 지레값을 가지며 영향력이 큰 점들은 예측변수들의 공간에서 보통의 것들에 비해 멀리 떨어져 있으며 적합에 유의적인 영향을 끼치기 때문에 잘 검토할 필요가 있다.</code></p></li><li><p>즉, <code>P값들의 index plot과 Cook&#39;s distance의 index plot과 종속변수와 독립변수 plot을 종합해서 비교해 보면서 각각의 지렛값이 높은 데이터와 영향력이 있는 값을 찾아야 할 것이다.</code></p></li><li><p>criterion에는 cooks 와 DFITS를 사용할 수 있다. 아래 그래프의 해석은 몇가지 주의해야할 관측치들이 있는데, contractor와 reporter는 낮은 Leverage를 갖지만 큰 잔차를 갖는것을 볼 수 있다. RR.engineer는 작은 잔차와 낮은 Leverage를 갖는다. Conductor와 minister는 둘다 모두 높은 Leverage와 높은 잔차를 갖으므로 영향력있는 관측치이다.</p></li></ul><p><img src="/image/influence_plot.png" alt="influence plot"></p><ul><li><code>특이값(잔차가 큰 관측치)과 영향력있는 관측개체(high leverage high residual)를 식별하는 데 유용한 다른 접근방법은 로버스트 회귀(robust regression)</code>이다. <code>높은 지레값을 가지는 관측개체에 상대적으로 낮은 가중치를 주고 회귀직선을 적합</code>시킨다. 다음에 더 자세한 설명을 할 것이다.</li></ul><h4 id="3-변수들의-효과에-대한-진단플롯"><a href="#3-변수들의-효과에-대한-진단플롯" class="headerlink" title="3) 변수들의 효과에 대한 진단플롯"></a>3) 변수들의 효과에 대한 진단플롯</h4><ul><li><code>회귀방정식의 어떤 변수를 보유해야 할 것인지 아니면 제거해야 할 것이지를 각각의 t-검정에 대한 보조도구로 사용</code>될 수 있다.</li></ul><h5 id="첨가변수-plot-또는-편회귀-plot-added-variable-plot-또는-partial-regression-plot"><a href="#첨가변수-plot-또는-편회귀-plot-added-variable-plot-또는-partial-regression-plot" class="headerlink" title="첨가변수 plot 또는 편회귀 plot(added-variable plot 또는 partial regression plot)"></a>첨가변수 plot 또는 편회귀 plot(added-variable plot 또는 partial regression plot)</h5><ul><li><p>회귀 모형에 대한 특정 독립(설명)변수를 포함시킬 것인지의 여부를 검토할 때, 그 대상이 되는 예측 변수에 대한 회귀계수의 크기를 그래프를 통하여 표현한다. plot에 나타나는 점들의 <code>기울기는 곧 해당 독립(설명)변수에 대한 회귀계수</code>를 나타낸다. <code>따라서 이 plot에 나타난 점들이 뚜렷한 기울기를 보이지 않는다면 이는 그 변수가 모형에서 별로 유용하지 않음을 의미</code>한다. <code>X축이 해당 예측변수 그 자체가 아니므로 비선형성의 여부를 나타내주지는 않는 점을 주의해야 한다.</code> 또한 이 plot은 <code>그 계수의 크기를 결정하는 데 중요한 역할을 하는 데이터 점을 제시해 주기도 한다.</code> 첨가변수 plot은 Y-잔차$(X_{j}를 제외한 나머지 변수들로 설명되지 않은 Y의 부분) vs X_{j}-잔차(X_{j}를 종속변수로하여 나머지 변수들로 설명되지 않은 X_{j}의 부분)$을 그리는 plot이다. <code>이 두개의 잔차들을 최소 제곱벙으로 적합시켰을 때, 적합된 회귀직선의 기울기는 Xj를 포함한 모든 독립(설명)변수 얻은 회귀계수</code>($\hat{\beta}_{j}$)와 같다.</p></li><li><p>개별로 그리는 것은 index로 식별을 할 수 있지만 아래에 여러가지를 한 꺼번에 그리는 방법은 인덱스를 볼 수 없으므로 처음에는 여러개를 다 같이 그린 후에 자세히 살펴봐야할 변수에 대해서만 개별로 그리는 방법을 사용하는 것이 좋을 것이다.</p></li></ul><p><img src="/image/partial_regression_plot.png" alt="partial regression plot을 그리는 방법"></p><p><img src="many_partial_regression_plot.png" alt="partial regression plot을 모형의 모든 변수에 관해 그리는 법"></p><h5 id="성분잔차-plot-component-plus-residual-plot"><a href="#성분잔차-plot-component-plus-residual-plot" class="headerlink" title="성분잔차 plot(component plus residual plot)"></a>성분잔차 plot(component plus residual plot)</h5><ul><li><p>회귀 분석에서 가장 오래된 그래프적 기법 중의 하나이다. $(e + \hat{\beta}_{j} X_{j} VS X_{j})$에 대한 산점도이다. $\hat{\beta}_{j} X_{j}$은 j번째 독립(설명)변수가 적합값에 기여하는 공헌도(성분)임을 주목하자. 이 plot에서 <code>기울기는 해당 독립변수에 대한 추정 회귀계수를 의미</code>하며, <code>해당 예측변수의 기울기를 보여 줄 뿐 아니라 종속변수와 해당 독립변수사이의 비선형성의 존재도 알려줌으로써 필요할 경우 독립변수에 관한 구체적인 선형변환의 내용까지도 제시</code>한다는 것이다.</p></li><li><p>이 또한, added-variable plot처럼 여러개를 그려본 뒤 필요한 변수에 대해서만 살펴보는 것을 추천.</p></li></ul><p><img src="/image/ccpr_plot" alt="CCPR plot을 그리는 방법"></p><p><img src="/image/ccpr_grid.png" alt="CCPR plot을 모형의 모든 변수에 관해 그리는 법"></p><h5 id="component-plus-residual-plot-vs-added-variable-plot"><a href="#component-plus-residual-plot-vs-added-variable-plot" class="headerlink" title="component plus residual plot vs added-variable plot"></a>component plus residual plot vs added-variable plot</h5><ul><li>두 그래프 모두 회귀계수에 대한 추정치를 기울기로 보여주지만, added-variable plot은 어떤 데이터가 회귀계수를 추정하는데 많은 영향을 주었는지를 알 수 있게 도와준다. 반면에, component plus residual plot은 added-variable plot보다 특정 독립변수를 회귀모형에 도입해야 하느냐 하는 문제에 대한 답이나 그 독립변수가 가지는 비선형성의 여부를 탐색하는 데 더 민감한 것으로 알려져 있다.</li></ul><h4 id="추가적인-예측변수의-효과"><a href="#추가적인-예측변수의-효과" class="headerlink" title="추가적인 예측변수의 효과"></a>추가적인 예측변수의 효과</h4><ul><li><p>회귀식에 새로운 변수를 도입하는 것의 효과에 대하여 다음의 두가지 질문을 고려해야할 것이다. (a) 새로운 변수의 회귀계수가 유의한가? (b)새로운 변수를 도입함으로써 회귀식에 이미 포함되어 있는 변수들의 회귀계수를 유의하게 변화시키는가? 이 두가지 질문에 대한 답으로 크게 4가지 유형이 있을 수 있다.</p><ul><li><p>1) 새로운 변수가 유의하지 않은 회귀계수를 가지며, 다른 회귀계수들은 이전의 값에 비해 거의 변화가 없다. 어떤 다른 외부적인 조건(예컨대, 이론 또는 주제에 대한 고려)에 의하여 필요성이 있지 않다면, 새로운 변수는 회귀식에 포함되지 않아야 한다.</p></li><li><p>2) 새로운 변수가 유의한 회귀계수를 가지며, 이전에 도입된 다른 변수들의 회귀계수에 큰 변화가 있다. 이 경우 새로운 변수가 유지되어야 하며, 그러나 공선성에 대한 탐색이 필요하다. 공선성의 증거가 없다면, 그 변수는 방정식에 포함되어야 하며 다른 추가적인 변수의 도입에 대한 탐색이 수행되어야 한다.</p></li><li><p>3) 새로운 변수가 유의한 회귀계수를 가지며, 다른 회귀계수들은 이전의 값에 비하여 큰 변화가 없다. <code>이것은 이상적인 상황이며 새로운 변수가 이전에 도입된 변수들과 상관되어 있지 않을 때 발생</code>한다. 이 경우 새로운 변수는 방정식에 포함되어야한다.</p></li><li><p>4) 새로운 변수가 유의하지 않은 회귀계수를 가지며, 이전에 도입된 다른 회귀계수에 큰 변화가 있다. 이것은 <code>명백한 공선성의 증거</code>이며, 회귀식에 새로운 변수를 포함시킬 것인지 아니면 제 제외시킬 것인지를 결정하기 전에 수정작업이 취해져야 한다.</p></li></ul></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/15/machine_learning_04/#disqus_thread</comments>
    </item>
    
    <item>
      <title>NLP란?</title>
      <link>https://heung-bae-lee.github.io/2020/01/15/NLP_07/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/15/NLP_07/</guid>
      <pubDate>Tue, 14 Jan 2020 19:01:52 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;자연어란&quot;&gt;&lt;a href=&quot;#자연어란&quot; class=&quot;headerlink&quot; title=&quot;자연어란?&quot;&gt;&lt;/a&gt;자연어란?&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/image/what_is_Natural_Language.png&quot; alt=&quot;자연어&quot;&gt;&lt;/p
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="자연어란"><a href="#자연어란" class="headerlink" title="자연어란?"></a>자연어란?</h2><p><img src="/image/what_is_Natural_Language.png" alt="자연어"></p><h2 id="NLP란"><a href="#NLP란" class="headerlink" title="NLP란?"></a>NLP란?</h2><p><img src="/image/What_is_Natural_language_processing.png" alt="NLP"></p><h2 id="NLP의-어려움"><a href="#NLP의-어려움" class="headerlink" title="NLP의 어려움"></a>NLP의 어려움</h2><ul><li><p>우리가 실생활에서 사용하는 언어는 복잡성, 애매함, 그리고 의존성을 지니고 있기 때문이다.</p><ul><li><p><code>복잡성</code>이란 예를 들어 필자가 좋아하는 게임인 배틀그라운드로 예를 들어 보겠다. 몇 주전 PUBG에서는 배틀그라운드의 신맵인 카라킨에 대해 반응을 보기 위해 각 배틀그라운드 커뮤니티 사이트에 대한 댓글을 분석한다고 가정해보자. 그렇다면, 우선 게임 용어가 어떤 것들을 지칭하는 지 사전지식이 필요할 것이다. 이렇듯 복잡하게 연관되어 있는 Token간의 관계를 복잡성이라고 한다.</p></li><li><p><code>애매함</code>은 다중모드라는 것이 있기 때문에 발생하는 것이다. 예를 들어, 우리가 실생활에서 사용하는 어구 중 ‘너 참 잘한다.’라는 문장은 여러가지 상황에서 사용되며, 상황에 따라 다른 의미를 갖는다. ‘데이터 전처리를 이렇게 잘해놨어? 와.. 너 참 잘한다.’와 ‘응? 이거 뭐야? 이거 왜 최소 글자수를 8자로했어? 전체 Corpus에 단어 평균 길이는 11인데??? 몰랐다고?? 참 잘한다~!’ 앞의 두 문장은 ‘참 잘한다’의 의미가 문맥적으로 다르다는 것이다.</p></li><li><p><code>의존성</code>은 특정 질문에 대한 답변을 할때 정보가 부족해서 의존적인 부분들이 생기는 문제를 의미한다. 한가지 예로, ‘젤리 먹고싶은데 젤리 하나만 사다줄래’의 질문에 대한 답을 할때, 위의 질문만 들었을 땐 도대체 어떤 젤리를 사야할지 모를것이다. 이런 상황에 다시 ‘어떤 젤리 먹고싶은데’라는 질문을 통한 <code>상호작용</code>으로 해결할 수 있을 것이다.</p></li></ul></li></ul><p><img src="/image/why_is_so_difficult_NLP.png" alt="NLP의 어려움"></p><h2 id="NLP분야에서의-Machine-learning-vs-Deep-learning"><a href="#NLP분야에서의-Machine-learning-vs-Deep-learning" class="headerlink" title="NLP분야에서의 Machine learning vs Deep learning"></a>NLP분야에서의 Machine learning vs Deep learning</h2><ul><li>반복해서 얘기하지만 Feature의 추출을 사람이 직접하는 Machine learning같은 경우는 언어학에 대한 지식을 깊이 알고 있어야 가능할 것이다. 그에 반해, 상대적으로 deep learning은 feature를 만들어 줄 수 있는 구조를 만들면 그에따라 Feature를 알아서 생성해주므로 상대적으로 언어학에 대한 깊이 있는 지식이 없어도 분석이 가능하다.</li></ul><p><img src="/image/NLP_deep_learning.png" alt="NLP 분야에서의 Machine learning과 deep learning의 차이점"></p><h2 id="NLP의-Applicatio의-종류"><a href="#NLP의-Applicatio의-종류" class="headerlink" title="NLP의 Applicatio의 종류"></a>NLP의 Applicatio의 종류</h2><ul><li>네이버의 파파고, 구글의 구글 번역기 같은 번역 서비스를 예로 들수 있다.</li></ul><p><img src="/image/Machine_translation.png" alt="NLP 분야 - 기계 번역"></p><p><img src="/image/sentiment_analysis.png" alt="NLP 분야 - 감성 분석"></p><ul><li>여러 회사에서 특히 은행이나 카드사, 쇼핑몰등에서 많이 보았을 법한 챗봇 서비스도 NLP의 응용분야이다. 스캐터랩의 핑퐁이나, 심심이같은 챗봇 서비스들도 있다.</li></ul><p><img src="/image/chatbots.png" alt="NLP 분야 - 챗봇"></p><ul><li>예를 들면, 구글 애드 센스 같은 서비스가 있다.</li></ul><p><img src="/image/Contextual_Advertising.png" alt="NLP 분야 - 문맥 광고">        </p><p><img src="/image/Not_NLP.png" alt="NLP가 아닌 응용분야"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/15/NLP_07/#disqus_thread</comments>
    </item>
    
    <item>
      <title>순환 신경망(RNN) - 순차 데이터의 이해</title>
      <link>https://heung-bae-lee.github.io/2020/01/12/deep_learning_08/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/12/deep_learning_08/</guid>
      <pubDate>Sun, 12 Jan 2020 06:25:39 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;순차-데이터의-이해&quot;&gt;&lt;a href=&quot;#순차-데이터의-이해&quot; class=&quot;headerlink&quot; title=&quot;순차 데이터의 이해&quot;&gt;&lt;/a&gt;순차 데이터의 이해&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;우리가 &lt;code&gt;순환 신경망을 사용하는 이유&lt;/code&gt;
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="순차-데이터의-이해"><a href="#순차-데이터의-이해" class="headerlink" title="순차 데이터의 이해"></a>순차 데이터의 이해</h1><ul><li>우리가 <code>순환 신경망을 사용하는 이유</code>는 <code>입력을 순환 신경망으로 받거나 출력을 순환 신경망으로 내기 위해서</code>이다.</li></ul><p><img src="/image/Sequential_Data.png" alt="순차 데이터(Sequential Data)"></p><ul><li>일정한 시간차을 갖는 Time Series라면, x축이 특정 시간을 의미하는 Temporal Sequence와는 다르게 하나하나의 Step으로 간주한다.</li></ul><p><img src="/image/Resampling.png" alt="Resampling"></p><ul><li>일반적으로는 위에서 보는 것과 같이 Temporal Sequence를 보간하여 Time Series로 변환해 준 뒤에 사용한다.</li></ul><p><img src="/image/DNN_and_Sequential_Data.png" alt="심층 신경망과 순차 데이터"></p><p><img src="/image/many_one.png" alt="다중 입력, 단일 출력"></p><ul><li>개인 비서 서비스는 예를 들어, siri나 구글의 okay google 같은 서비스이다.</li></ul><p><img src="/image/many_to_many.png" alt="다중 입력, 다중 출력"></p><p><img src="/image/one_to_may.png" alt="단일 입력, 다중 출력"></p><h1 id="기본-적인-순환-신경망-Vanilla-RNN"><a href="#기본-적인-순환-신경망-Vanilla-RNN" class="headerlink" title="기본 적인 순환 신경망(Vanilla RNN)"></a>기본 적인 순환 신경망(Vanilla RNN)</h1><p><img src="/image/processing_with_Sequence_Data.png" alt="순차 데이터의 처리"></p><ul><li>앞서 말한 순차데이터를 입력받아 원하는 출력을 하려면, <code>기억시스템</code>이 전제되어야 한다. <code>CNN이나 Deep Neural Network, shallow NN은 Memoryless System이다.</code></li></ul><p><img src="/image/Memory_System.png" alt="기억 시스템(Memory System)"></p><p><img src="/image/shallow_Neural_Network_related_in_RNN.png" alt="얕은 신경망(Shallow Neural Network)"></p><ul><li>다음 그림에서 볼 수 있듯이, RNN은 이전의 Network 구조와는 다르게 <code>입력층에 n-1번째 step의 hidden layer를 n번째 데이터와 concatenation을 하여 사용</code>하는 것이다. 이렇게 함으로써 <code>이전의 모든 입력에 영향을 받는다.</code></li></ul><p><img src="/image/vanilla_RNN.png" alt="기본적인 순환 신경망(Vanilla Recurrent Network)"></p><ul><li>shallow Neural Network를 Deep Neural Network로 만들어 주었듯이, 동일하게 하여 Multi-Layer RNN을 만들 수 있다. 왼쪽의 노드들만 본다면 <code>다음 Layer들의 이전 step의 hidden Layer를 가져온 것을 확인</code> 할 수 있다. <code>하지만, 이런 구조는 Vanilla RNN과 다르게 Hidden Layer의 길이도 2배이상으로 늘어나기 때문에, 복잡도가 높아지게 되며, 현실적으로 학습이 잘 되지 않아 권장되지 않는다.</code> 그 이유는 간단하게만 말하자면, 일반적인 Neural Network는 depth 방향으로만 gradient가 잘 학습되면 되지만, 이 구조는 input까지 Gradient의 영향을 주도록 해야하기 때문이다.</li></ul><p><img src="/image/Multi_Layer_RNN.png" alt="다중 계층 순환 신경망(Multi-Layer RNN)"></p><h1 id="심화-순환-신경망"><a href="#심화-순환-신경망" class="headerlink" title="심화 순환 신경망"></a>심화 순환 신경망</h1><ul><li>그렇다면 <code>&#39;Vanilla RNN이 왜 잘 쓰이지 않는가?&#39;</code>에 대한 가장 큰 이유를 앞서 언급했던 것과 같이 Gradient가 Input까지 타고 가서 학습을 잘 못하기 때문이라고 언급했다. 즉, <code>Gradient Vanishing 문제</code>라는 것이다.</li></ul><p><img src="/image/Gradient_Vanishing_problem_with_vanilla_RNN.png" alt="기울기 소실 문제"></p><p><img src="/image/LSTM.png" alt="LSTM(Long Short-Term Memory)"></p><p><img src="/image/Cell_State_and_Hidden_State.png" alt="Cell State, Hidden State"></p><p><img src="/image/Forget_Gate.png" alt="Forget Gate"></p><ul><li>검정색 선이 Input Gate이다. 빨간색 선은 Vanilla RNN에서의 Hidden State와 동일하다. 입력이 들어오고 이전 Hidden State를 받아서 같이 tanh activation function을 FC(Fully connected) Layer를 통과시켜 출력을 내주면 RNN의 Hidden Layer이기 때문이다.</li></ul><p><img src="/image/Input_Gate.png" alt="Input Gate"></p><ul><li>해당 Layer에서 필요한 정보만을 출력층으로 내주고, 필요하지 않은 정보는 계속 기억하게끔 다음 time step으로 넘겨주어서 이전에 어떤 출력을 내주었었나를 Cell State와는 별개로 또 넘겨주어 기억하게끔 한다. 아래 그림에서 출력을 내보내기 위해 Cell State에서 tanh를 거쳐 주는데 이 작업은 <code>다른 activation function들이 존재하는 노드들과 달리 FC layer로 이루어져 있지 않고 그냥 activation function만 거치게 된다.</code> 그 이유는 <code>Cell State가 Forget gate를 지나면서는 0~1사이의 값이 곱해지므로 크게 문제가 없지만 Input Gate를 지나면서 Feature가 추가적으로 더해질때 tanh를 지나면 -1~1사이의 값이 되므로 범위 -2~2로 늘어나게 되어 추후에 Gradient Explode가 일어날 수 있어 예방차원에서 tanh을 사용</code>하는 것이기 때문이다.</li></ul><p><img src="/image/Output_Gate.png" alt="Output Gate"></p><p><img src="/image/GRU.png" alt="GRU(Gated Recurrent Unit)"></p><p><img src="/image/GRU_structure.png" alt="GRU의 구조"></p><ul><li>아래 그림에서 $1 - Forget Gate$를 Input Gate로 사용하는 것은 Forget Gate에서 잊어버린 만큼만 Input Gate를 통해 채워 주는 의미로 해석할 수 있다.</li></ul><p><img src="/image/Forget_gate_and_Input_gate.png" alt="Forget Gate &amp; Input Gate"></p><ul><li>Input Gate를 통해 새로운 Feature가 추가되기에 앞서서 이전 Hidden State 정보를 얼마나 잊게 하느냐의 의미인데, 예를 들어 앞의 문장이 .을 통해 마쳐졌다면, 그 뒤의 문장은 다른 문장 구조를 띄게 되므로 0에 가깝게 하여 Reset을 시켜줄 것이다.</li></ul><p><img src="/image/Reset_gate.png" alt="Reset Gate"></p><h1 id="시간펼침-역전파-학습법-BPTT-Back-Propagation-Through-Time"><a href="#시간펼침-역전파-학습법-BPTT-Back-Propagation-Through-Time" class="headerlink" title="시간펼침 역전파 학습법(BPTT: Back Propagation Through Time)"></a>시간펼침 역전파 학습법(BPTT: Back Propagation Through Time)</h1><ul><li><code>순환신경망은 기존의 기본적인 역전파 학습법으로는 학습할 수 없다. 그렇다면, 어떻게 해야할까?</code></li></ul><p><img src="/image/Sequence_data_instructure.png" alt="순차 데이터 셋의 구조"></p><p><img src="/image/many_to_one_inference.png" alt="다중 입력, 단일 출력의 학습"></p><ul><li>물론, <code>모델이 학습할때 언제 입력이 끝날지 모르기에 마지막 입력 같은 경우는 EOS(End Of Sequence)라는 특별한 미리정해준 하나의 토큰을 날려주는 경우가 많다.</code></li></ul><p><img src="/image/RNN_inference.png" alt="순환 신경망의 순방향 추론"></p><ul><li>아래의 그림은 Input의 시점에 따라 펼쳐져있다는 것을 이해하기 쉽게 펼쳐 놓은 것인데, 여기서 주의할 점은 <code>아래의 RNN안의 Hyper parameter들은 모두 동일하다는 것이다. 즉 아래의 그림은 재귀적형태를 시간의 흐름상으로 나열해 놓은 것이라고 생각하면 된다.</code></li></ul><p><img src="/image/Back_Propagation_Through_Time.png" alt="시간 펼침 역전파(Back Propagation Trough Time)"></p><ul><li>위에서 언급했던 것과 같이 출력(또는 입력)의 길이가 정해져있지 않은 RNN의 경우, 아래는 마지막 출력에 EOS 출력을 내게끔 학습시켜야 모든 출력이 나왔다는 것을 알 수가 있다. 또한, Back propagation도 마찬가지로 각각의 출력에 대한 Loss값 부터 시작해서 Input지점까지 해주면 된다. 단일 입력, 다중 출력의 실제 모델에서는 입력이 없는 다른 층에서는 0을 입력하거나 미리 정해놓은 입력을 넣어주어 학습을 시키는 것이 일반적이다.</li></ul><p><img src="/image/one_to_many_inference.png" alt="단일 입력, 다중 출력"></p><ul><li><code>다중 입력에 대해서 다중 출력</code>이 나오려면 2가지 상황이 있을 수 있다. 하나는 아래 그림에서와 같이 <code>입력에 대해서 출력이 나오고 입력이 끝나면 출력도 끝나는 것이 있을 수 있다.</code><br>이런 경우는 대표적으로, <code>동영상의 프레임 분류</code>가 있다. 예를 들면, CF의 한 프레임이 입력으로 들어와 각 장면이 어떤 장면인지 서술하는 식으로의 분류를 들 수 있을 것이다.</li></ul><p><img src="/image/many_to_many_inference.png" alt="다중 입력, 다중 출력 - 01"></p><ul><li>또 다른 한 상황은 <code>모든 입력을 받고 그 다음에 출력이 나오는 경우</code>가 있다. 이 경우도 마찬가지로 입력의 길이가 언제 끝날지 모르므로 마지막 입력에 EOS를 날려 주어야 한다.</li></ul><p><img src="/image/many_to_many_inference_01.png" alt="다중 입력, 다중 출력 - 02"></p><h1 id="심화-순환-신경망의-수식적-이해"><a href="#심화-순환-신경망의-수식적-이해" class="headerlink" title="심화 순환 신경망의 수식적 이해"></a>심화 순환 신경망의 수식적 이해</h1><ul><li>Vanilla RNN의 수식은 이전에 간단히 다루었다. 이제 LSTM과 GRU도 수식으로 접근해 보자.</li></ul><p><img src="/image/LSTM_equation.png" alt="LSTM 수식"></p><ul><li><code>특징이 여러차원으로 되어있는데, 이 Forget gate 또한 여러 차원으로 되어있어 특징별로 기억할지 말지를 결정</code>할 수 있다.</li></ul><p><img src="/image/Forget_gate_equation.png" alt="Forget gate"></p><p><img src="/image/Input_gate_equation.png" alt="Input gate"></p><p><img src="/image/LSTM_Cell_State_equation.png" alt="Cell state"></p><p><img src="/image/LSTM_Output_gate_equation.png" alt="Output gate"></p><p><img src="/image/LSTM_Hidden_state_equation.png" alt="Hidden state"></p><p><img src="/image/GRU_equation.png" alt="GRU"></p><ul><li><code>Reset gate</code>는 Hidden state에서 바로 잊는 Forget gate와는 다르게 <code>현재 Feature를 뽑을 때 얼만큼 잊어줄 것인가를 결정하는 부분</code>이다. <code>큰 맥락에서는 기억하고 있어야 하지만, 현재 Feature를 뽑을 때는 방해가 될 수 있는 정보를 잊게하는 역할</code>이다.</li></ul><p><img src="/image/GRU_Reset_gate_equation.png" alt="Reset gate"></p><ul><li>예를 들어 아래와 같은 상황일때, 마지막 박 아무개의 답을 추론하고자 한다면, 먼저 “나는 사과가 좋다.”, “너는 과일을 싫어한다.”라는 문장 2개는 분리가 된 문장이지만 “나는 사과가 좋다” 내에서는 ‘나’하고 ‘사과’는 잘 기억이 되어야 하지만 “너는 과일을 싫어한다”라는 문장은 다른 문장이므로 기억이 안되어야 할 것이다. 하지만, “나는 어떤 과일이 먹고 싶을까?”에 답을 하려면, 최근에 문장인 “너는 과일을 싫어한다”에서는 추론할 때 필요한 정보가 없기 때문에 그 이전 문장인 “나는 사과가 좋다.”는 context를 계속해서 가지고 있어야한다. 이 정보가 Hidden state를 타고 움직여야 하는 정보이고, 여기서 “나는 사과가 좋다.”와 “너는 과일을 싫어한다.”라는 문장을 구분하여 단계적으로 활용하지 않기 위한 작업이 Reset gate를 통한 작업이다.</li></ul><p><img src="/image/GRU_Forget_gate_equation_example.png" alt="Reset gate 예시"></p><ul><li>Reset gate와 다르게 Hidden state에 직접적으로 곱해져서 이전 time step Hidden State에서 기억을 잊어버리게 하는 역할을 한다. 잊어버린 부분만큼을 다시 새로운 정보로 보충하기 위해 1에서 뺀 만큼을 새로운 입력의 결과에 곱해준다.  </li></ul><p><img src="/image/GRU_Forget_gate_equation.png" alt="Forget gate"></p><ul><li>이전 time step의 Hidden state가 들어왔을 때 reset gate를 통해 제어가 된 것을 가지고 현재 Feature들을 뽑아주게 되고, Forget gate에서 의해서 제어가 된 만큼 넘어오고 Forget gate에 의해서 상보적인 만큼 다시 새로 뽑은 Feature를 입력을 받아서 다음 출력으로 나가게 된다. 그렇기에 값이 -1~1로 bound되어있어 LSTM과 다르게 tanh함수가 필요하지 않다.</li></ul><p><img src="/image/GRU_Hidden_state_equation.png" alt="Hidden state"></p><h1 id="순차-신경망에서-Tensor의-이해"><a href="#순차-신경망에서-Tensor의-이해" class="headerlink" title="순차 신경망에서 Tensor의 이해"></a>순차 신경망에서 Tensor의 이해</h1><p><img src="/image/Sequence_data_for_Tensor.png" alt="순차 데이터셋"></p><p><img src="/image/FC_Layer_Tensor.png" alt="전결합 계층의 입출력 텐서"></p><ul><li>데이터가 Feature같은 경우에는 항상 꽉차게 되는데, <code>순차데이터 같은 경우에는 길이가 L보다 짧을 수 있다. 그런 경우에는 앞을 0으로 채워준다.(pre-padding)</code></li></ul><p><img src="/image/RNN_Input_Tensor.png" alt="RNN의 입력 텐서 - 01"></p><p><img src="/image/RNN_Input_Tensor_why.png" alt="RNN의 입력 텐서 - 02"></p><p><img src="/image/RNN_Output_Tensor.png" alt="RNN의 출력 텐서 - 01"></p><ul><li>출력이 나오는 시점은 고정되므로 일관되게 앞쪽으로 정렬된 출력이 나올 수 있게 하기 위해서 뒷부분을 0으로 채운다.</li></ul><p><img src="/image/RNN_Output_Tensor_why.png" alt="RNN의 출력 텐서 - 02"></p><h1 id="순환-신경망의-학습법"><a href="#순환-신경망의-학습법" class="headerlink" title="순환 신경망의 학습법"></a>순환 신경망의 학습법</h1><p><img src="/image/BPTT_with_backpropagation.png" alt="BPTT"></p><p><img src="/image/BPTT_data_input.png" alt="BPTT 데이터 입력"></p><ul><li>시간에 대해서 펼쳐있고, 추가적으로 Batch로도 펼쳐 주어야 하는데, 즉, 아래와 같은 구조가 Batch size만큼 더 있어주어야 한다는 의미이다. 그래서 <code>시간적으로 펼칠 때 역전파를 위한 추가적인 메모리가 필요</code>하다. 일반적인 CNN이나 DNN은 시간적으로 펼치는 것이 없기 때문에 Batch에 대해서 크게 엄격하지 않다. 하지만 <code>RNN은 아래와 같이 시간적으로 펼치기 때문에 Batch size를 늘리는데 엄격하다.</code></li></ul><p><img src="/image/BPTT_batch_training.png" alt="BPTT의 배치 학습법"></p><ul><li>순차 데이터의 길이 L이 매우 클 경우, 시간 펼침이 늘어나면서 필요 메모리가 L배 증가한다. 그 이유는 길이가 1개 씩 늘어날 때마다 펼침을 하나씩 더 해야하기 때문이다. <code>이 때 B(Batch)를 한번에 계산하므로, 얕은 신경망에 비해 훨씬 큰 메모리가 필요.</code></li></ul><p><img src="/image/problem_with_BPTT.png" alt="BPTT의 문제점"></p><p><img src="/image/many_to_many_solving_problem_with_Truncated_BPTT.png" alt="다중 입력, 다중 출력"></p><p><img src="/image/Truncated_BPTT_data_input.png" alt="Truncated BPTT 데이터 입력"></p><ul><li><p><code>길이 L의 입력을 길이 T로 쪼개어 순서대로 학습</code>한다.<br><img src="/image/Truncated_BPTT.png" alt="Truncated BPTT"></p></li><li><p>즉, <code>Time step이 T 이상 떨어진 입-출력 관계는 학습되지 않는다.</code> Hidden state와 Cell state를 통해 Forward propagation에서는 잘 추론 할 수 있도록 넘겨 주지만, Back propagation에서는 그 관계를 서로 넘겨주지 못한다. 만약 전부다 연결시킨 관계를 학습시켜야 한다면 Truncated BPTT가 아닌 길이가 L인 모든 데이터를 학습시켜야 한다. 그러므로 <code>Truncated BPTT를 사용할 시 반드시 영향을 주는 데이터 사이의 관계를 침해하지 않게 T로 적절하게 나누어졌는지, 우리가 학습하고자 하는 것이 어느 정도의 시간차이까지 우리가 연관성을 봐야 하는지를 염두해 두고 학습을 시켜야 한다.</code></p></li><li><p><code>만약 연관성이 있는 데이터의 주기(데이터간의 시점 차이)가 크고 Gradient가 끊기지 않고 연결되어 업데이트가 이루어져야 한다면, 최대한 Batch Size를 극단적 낮추고, 최대한 Memory가 큰 GPU를 사용해서 최대한 긴 길이를 학습해 주는 방법을 사용해야 할 것이다.</code><br><img src="/image/Back_Propagation_of_Truncated_BPTT.png" alt="Truncated BPTT의 역전파 흐름"></p></li></ul>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/12/deep_learning_08/#disqus_thread</comments>
    </item>
    
    <item>
      <title>DenseNet 구현 및 학습</title>
      <link>https://heung-bae-lee.github.io/2020/01/12/deep_learning_07/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/12/deep_learning_07/</guid>
      <pubDate>Sun, 12 Jan 2020 06:23:36 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;DenseNetwork-구현-및-학습&quot;&gt;&lt;a href=&quot;#DenseNetwork-구현-및-학습&quot; class=&quot;headerlink&quot; title=&quot;DenseNetwork 구현 및 학습&quot;&gt;&lt;/a&gt;DenseNetwork 구현 및 학습&lt;/h1&gt;&lt;
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="DenseNetwork-구현-및-학습"><a href="#DenseNetwork-구현-및-학습" class="headerlink" title="DenseNetwork 구현 및 학습"></a>DenseNetwork 구현 및 학습</h1><ul><li>필자는 구글 colab을 통해 학습시켰으며, @tf.function을 사용하기위해 tensorflow 2.0 버젼을 시용하였다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install tensorflow==2.0.0-beta1</span><br></pre></td></tr></table></figure><ul><li>사용할 라이브러리 import</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br></pre></td></tr></table></figure><ul><li>위에서 설치한 tensorflow의 버전이 2.0인지 확인</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br></pre></td></tr></table></figure><ul><li>Hyper parameter를 설정</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EPOCHS = 10</span><br></pre></td></tr></table></figure><h3 id="DenseUnit-구현"><a href="#DenseUnit-구현" class="headerlink" title="DenseUnit 구현"></a>DenseUnit 구현</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">class DenseUnit(tf.keras.Model):</span><br><span class="line">    def __init__(self, filter_out, kernel_size):</span><br><span class="line">        super(DenseUnit, self).__init__()</span><br><span class="line">        <span class="comment"># batch normalization -&gt; ReLu -&gt; Conv Layer</span></span><br><span class="line">        <span class="comment"># 여기서 ReLu 같은 경우는 변수가 없는 Layer이므로 여기서 굳이 initialize 해주지 않는다. (call쪽에서 사용하면 되므로)</span></span><br><span class="line">        <span class="comment"># Pre-activation 구조는 똑같이 가져가되, concatenate 구조로 만들어주어야함에 주의하자!!</span></span><br><span class="line"></span><br><span class="line">        self.bn = tf.keras.layers.BatchNormalization()</span><br><span class="line">        self.conv = tf.keras.layers.Conv2D(filter_out, kernel_size, padding=<span class="string">'same'</span>)</span><br><span class="line">        self.concat = tf.keras.layers.Concatenate()</span><br><span class="line"></span><br><span class="line">    def call(self, x, training=False, mask=None): <span class="comment"># x : (Batch 갯수, Height, width, Channel_in)</span></span><br><span class="line">        <span class="comment"># training 꼭 잊어버리지 말자!!</span></span><br><span class="line">        h = self.bn(x, training=training)</span><br><span class="line">        h = tf.nn.relu(h)</span><br><span class="line">        h = self.conv(h) <span class="comment"># h : (Batch, height, width, filter_output) zero-padding을 했으므로 filter만 바뀜</span></span><br><span class="line">        <span class="built_in">return</span> self.concat([x, h]) <span class="comment"># (Batch, height, width, (channel_in + filter_output))</span></span><br></pre></td></tr></table></figure><h3 id="DenseLayer-구현"><a href="#DenseLayer-구현" class="headerlink" title="DenseLayer 구현"></a>DenseLayer 구현</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class DenseLayer(tf.keras.Model):</span><br><span class="line">    def __init__(self, num_unit, growth_rate, kernel_size):</span><br><span class="line">        super(DenseLayer, self).__init__()</span><br><span class="line">        self.sequence = list()</span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> range(num_unit):</span><br><span class="line">            self.sequence.append(DenseUnit(growth_rate, kernel_size))</span><br><span class="line"></span><br><span class="line">    def call(self, x, training=False, mask=None):</span><br><span class="line">        <span class="keyword">for</span> unit <span class="keyword">in</span> self.sequence:</span><br><span class="line">            x = unit(x, training=training)</span><br><span class="line">        <span class="built_in">return</span> x</span><br></pre></td></tr></table></figure><h3 id="Transition-Layer-구현"><a href="#Transition-Layer-구현" class="headerlink" title="Transition Layer 구현"></a>Transition Layer 구현</h3><ul><li>Maxpooling을 해줄 때 필요함.</li><li>예를 들어, DenseLayer를 사용을 하게 되면, Growth_rate=32, num_unit=8인 경우에는, 32X8 만큼 channel의 수가 급격하게 증가하기 때문에 너무 커질 수 있다. 이럴 경우 이 Transition Layer를 통해 Pooling 전에 channel의 수를 조절해주기 위해 사용되는 것이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class TransitionLayer(tf.keras.Model):</span><br><span class="line">    def __init__(self, filters, kernel_size):</span><br><span class="line">        super(TransitionLayer, self).__init__()</span><br><span class="line">        <span class="comment"># transition을 할 경우에는 이렇게 convolution을 해서 단순히 filter 개수를 변경만 해준뒤</span></span><br><span class="line">        <span class="comment"># 그 다음에 Maxpooling을 해주는 식으로 구현이 된다.</span></span><br><span class="line">        self.conv = tf.keras.layers.Conv2D(filters, kernel_size, padding=<span class="string">'same'</span>)</span><br><span class="line">        self.pool = tf.keras.layers.MaxPool2D()</span><br><span class="line"></span><br><span class="line">    def call(self, x, training=False, mask=None):</span><br><span class="line">        <span class="comment"># 여기서는 Batch normalization이 없기 때문에 training을 안써줘도 된다.</span></span><br><span class="line">        x = self.conv(x)</span><br><span class="line">        <span class="built_in">return</span> self.pool(x)</span><br></pre></td></tr></table></figure><h3 id="모델-정의"><a href="#모델-정의" class="headerlink" title="모델 정의"></a>모델 정의</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">class DenseNet(tf.keras.Model):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(DenseNet, self).__init__()</span><br><span class="line">        self.conv1 = tf.keras.layers.Conv2D(8, (3, 3), padding=<span class="string">'same'</span>, activation=<span class="string">'relu'</span>) <span class="comment"># 28x28x8</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># num_unit=2, growth_rate=4, kernel_size=(3,3)</span></span><br><span class="line">        <span class="comment"># num_unit은 ResNet과 동일</span></span><br><span class="line">        self.dl1 = DenseLayer(2, 4, (3, 3)) <span class="comment"># 28x28x(8+2*4)</span></span><br><span class="line">        self.tr1 = TransitionLayer(16, (3, 3)) <span class="comment"># 14x14x16</span></span><br><span class="line"></span><br><span class="line">        self.dl2 = DenseLayer(2, 8, (3, 3)) <span class="comment"># 14x14x(16 + 2*8)</span></span><br><span class="line">        self.tr2 = TransitionLayer(32, (3, 3)) <span class="comment"># 7x7x32</span></span><br><span class="line"></span><br><span class="line">        self.dl3 = DenseLayer(2, 16, (3, 3)) <span class="comment"># 7x7x(32+2*16)</span></span><br><span class="line"></span><br><span class="line">        self.flatten = tf.keras.layers.Flatten()</span><br><span class="line">        self.dense1 = tf.keras.layers.Dense(128, activation=<span class="string">'relu'</span>)</span><br><span class="line">        self.dense2 = tf.keras.layers.Dense(10, activation=<span class="string">'softmax'</span>)       </span><br><span class="line"></span><br><span class="line">    def call(self, x, training=False, mask=None):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line"></span><br><span class="line">        x = self.dl1(x, training=training)</span><br><span class="line">        x = self.tr1(x)</span><br><span class="line"></span><br><span class="line">        x = self.dl2(x, training=training)</span><br><span class="line">        x = self.tr2(x)</span><br><span class="line"></span><br><span class="line">        x = self.dl3(x, training=training)</span><br><span class="line"></span><br><span class="line">        x = self.flatten(x)</span><br><span class="line">        x = self.dense1(x)</span><br><span class="line">        <span class="built_in">return</span> self.dense2(x)</span><br></pre></td></tr></table></figure><h3 id="학습-데스트-루프-정의"><a href="#학습-데스트-루프-정의" class="headerlink" title="학습, 데스트 루프 정의"></a>학습, 데스트 루프 정의</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement training loop</span></span><br><span class="line">@tf.function</span><br><span class="line">def train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy):</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        predictions = model(images, training=True)</span><br><span class="line">        loss = loss_object(labels, predictions)</span><br><span class="line">    gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line"></span><br><span class="line">    optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line">    train_loss(loss)</span><br><span class="line">    train_accuracy(labels, predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Implement algorithm test</span></span><br><span class="line">@tf.function</span><br><span class="line">def test_step(model, images, labels, loss_object, test_loss, test_accuracy):</span><br><span class="line">    predictions = model(images, training=False)</span><br><span class="line"></span><br><span class="line">    t_loss = loss_object(labels, predictions)</span><br><span class="line">    test_loss(t_loss)</span><br><span class="line">    test_accuracy(labels, predictions)</span><br></pre></td></tr></table></figure><h3 id="데이터셋-준비"><a href="#데이터셋-준비" class="headerlink" title="데이터셋 준비"></a>데이터셋 준비</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train, x_test = x_train / 255.0, x_test / 255.0</span><br><span class="line"></span><br><span class="line">x_train = x_train[..., tf.newaxis].astype(np.float32)</span><br><span class="line">x_test = x_test[..., tf.newaxis].astype(np.float32)</span><br><span class="line"></span><br><span class="line">train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)</span><br><span class="line">test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)</span><br></pre></td></tr></table></figure><h3 id="학습-환경-정의"><a href="#학습-환경-정의" class="headerlink" title="학습 환경 정의"></a>학습 환경 정의</h3><h4 id="모델-생성-손실함수-최적화-알고리즘-평가지표-정의"><a href="#모델-생성-손실함수-최적화-알고리즘-평가지표-정의" class="headerlink" title="모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"></a>모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 생성</span></span><br><span class="line">model = DenseNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 손실함수 정의 및 최적화 기법 정의</span></span><br><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">optimizer = tf.keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 평가지표 정의</span></span><br><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">test_loss = tf.keras.metrics.Mean(name=<span class="string">'test_loss'</span>)</span><br><span class="line">test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">'test_accuracy'</span>)</span><br></pre></td></tr></table></figure><h3 id="학습-루프-동작"><a href="#학습-루프-동작" class="headerlink" title="학습 루프 동작"></a>학습 루프 동작</h3><ul><li>어떤 것은 DenseNet이 성능이 더 좋게 나오고 어떤 것은 ResNet이 더 좋게 나오기도 한다. 또한 parameter에 따라 다르다. <code>Resnet은 좀 더 안정적이게 수렴하지만 Densenet은 좀 fluctuate하다는 점을 유의</code>하자.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCHS):</span><br><span class="line">  <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_ds:</span><br><span class="line">    train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> test_images, test_labels <span class="keyword">in</span> test_ds:</span><br><span class="line">    test_step(model, test_images, test_labels, loss_object, test_loss, test_accuracy)</span><br><span class="line"></span><br><span class="line">  template = <span class="string">"Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;"</span></span><br><span class="line">  <span class="built_in">print</span>(template.format(epoch+1,</span><br><span class="line">                        train_loss.result(),</span><br><span class="line">                        train_accuracy.result() * 100,</span><br><span class="line">                        test_loss.result(),</span><br><span class="line">                        test_accuracy.result() * 100))</span><br><span class="line"></span><br><span class="line"><span class="comment">#train_loss.reset_states()</span></span><br><span class="line"><span class="comment">#train_accuracy.reset_states()</span></span><br><span class="line"><span class="comment">#test_loss.reset_states()</span></span><br><span class="line"><span class="comment">#test_accuray.reset_states()</span></span><br></pre></td></tr></table></figure><p><img src="/image/DenseNet_result.png" alt="mnist 데이터에대한 DenseNet 성능"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/12/deep_learning_07/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Residual Network 구현 및 학습</title>
      <link>https://heung-bae-lee.github.io/2020/01/12/deep_learning_06/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/12/deep_learning_06/</guid>
      <pubDate>Sun, 12 Jan 2020 06:17:16 GMT</pubDate>
      <description>
      
        
        
          &lt;h1 id=&quot;Residual-Network&quot;&gt;&lt;a href=&quot;#Residual-Network&quot; class=&quot;headerlink&quot; title=&quot;Residual Network&quot;&gt;&lt;/a&gt;Residual Network&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;GoogLeNe
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h1 id="Residual-Network"><a href="#Residual-Network" class="headerlink" title="Residual Network"></a>Residual Network</h1><ul><li>GoogLeNet 이후에 나온 모델로 Residual 구조를 Skip connection 구조를 갖으며, pre-activation을 갖는 Residual unit을 먼저 만든 후에서 Resnet Unit을 연결하야 만들 ResnetLayer를 만들어 Residual Layer를 구현할 것이다. 그 후 전체적인 ResNet Model를 생성하여 앞서 구현해본 VGG-16과 성능을 비교하기 위해 동일한 mnist 데이터를 통해 학습과 검증을 해볼 것이다.</li></ul><h1 id="DenseNetwork-구현-및-학습"><a href="#DenseNetwork-구현-및-학습" class="headerlink" title="DenseNetwork 구현 및 학습"></a>DenseNetwork 구현 및 학습</h1><ul><li>필자는 구글 colab을 통해 학습시켰으며, @tf.function을 사용하기위해 tensorflow 2.0 버젼을 시용하였다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install tensorflow==2.0.0-beta1</span><br></pre></td></tr></table></figure><ul><li>사용할 라이브러리 import</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br></pre></td></tr></table></figure><ul><li>위에서 설치한 tensorflow의 버전이 2.0인지 확인</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(tf.__version__)</span><br></pre></td></tr></table></figure><ul><li>Hyper parameter를 설정</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EPOCHS = 10</span><br></pre></td></tr></table></figure><h3 id="Residual-Unit-구현"><a href="#Residual-Unit-구현" class="headerlink" title="Residual Unit 구현"></a>Residual Unit 구현</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">class ResidualUnit(tf.keras.Model):</span><br><span class="line">  def __init__(self, filter_in, filter_out, kernel_size):</span><br><span class="line">    super(ResidualUnit, self).__init__()</span><br><span class="line">    <span class="comment"># batch normalization -&gt; ReLu -&gt; Conv Layer</span></span><br><span class="line">    <span class="comment"># 여기서 ReLu 같은 경우는 변수가 없는 Layer이므로 여기서 굳이 initialize 해주지 않는다. (call쪽에서 사용하면 되므로)</span></span><br><span class="line"></span><br><span class="line">    self.bn1 = tf.keras.layers.BatchNormalization()</span><br><span class="line">    self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding=<span class="string">"same"</span>)</span><br><span class="line"></span><br><span class="line">    self.bn2 = tf.keras.layers.BatchNormalization()</span><br><span class="line">    self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding=<span class="string">"same"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># identity를 어떻게 할지 정의</span></span><br><span class="line">    <span class="comment"># 원래 Residual Unit을 하려면 위의 순서로 진행한 뒤, 바로 X를 더해서 내보내면 되는데,</span></span><br><span class="line">    <span class="comment"># 이 X와 위의 과정을 통해 얻은 Feature map과 차원이 동일해야 더하기 연산이 가능할 것이므로</span></span><br><span class="line">    <span class="comment"># 즉, 위에서 filter_in과 filter_out이 같아야 한다는 의미이다.</span></span><br><span class="line">    <span class="comment"># 하지만, 다를 수 있으므로 아래와 같은 작업을 거친다.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> filter_in == filter_out:</span><br><span class="line">      self.identity = lambda x: x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.identity = tf.keras.layers.Conv2D(filter_out, (1,1), padding=<span class="string">"same"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 아래에서 batch normalization은 train할때와 inference할 때 사용하는 것이 달라지므로 옵션을 줄것이다.</span></span><br><span class="line">  def call(self, x, training=False, mask=None):</span><br><span class="line">    h = self.bn1(x, training=training)</span><br><span class="line">    h = tf.nn.relu(h)</span><br><span class="line">    h = self.conv1(h)</span><br><span class="line"></span><br><span class="line">    h = self.bn2(h, training=training)</span><br><span class="line">    h = tf.nn.relu(h)</span><br><span class="line">    h = self.conv2(h)</span><br><span class="line">    <span class="built_in">return</span> self.identity(x) + h</span><br></pre></td></tr></table></figure><h3 id="Residual-Layer-구현"><a href="#Residual-Layer-구현" class="headerlink" title="Residual Layer 구현"></a>Residual Layer 구현</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class ResnetLayer(tf.keras.Model):</span><br><span class="line">  <span class="comment"># 아래 arg 중 filter_in : 처음 입력되는 filter 개수를 의미</span></span><br><span class="line">  <span class="comment"># Resnet Layer는 Residual unit이 여러개가 있게끔해주는것이므로</span></span><br><span class="line">  <span class="comment"># filters : [32, 32, 32, 32]는 32에서 32로 Residual unit이 연결되는 형태</span></span><br><span class="line">  def __init__(self, filter_in, filters, kernel_size):</span><br><span class="line">    super(ResnetLayer, self).__init__()</span><br><span class="line">    self.sequnce = list()</span><br><span class="line">    <span class="comment"># [16] + [32, 32, 32]</span></span><br><span class="line">    <span class="comment"># 아래는 list의 length가 더 작은 것을 기준으로 zip이 되어서 돌아가기 때문에</span></span><br><span class="line">    <span class="comment"># 앞의 list의 마지막 element 32는 무시된다.</span></span><br><span class="line">    <span class="comment"># zip([16, 32, 32, 32], [32, 32, 32])</span></span><br><span class="line">    <span class="keyword">for</span> f_in, f_out <span class="keyword">in</span> zip([filter_in] + list(filters), filters):</span><br><span class="line">      self.sequnce.append(ResidualUnit(f_in, f_out, kernel_size))</span><br><span class="line"></span><br><span class="line">  def call(self, x, training=False, mask=None):</span><br><span class="line">    <span class="keyword">for</span> unit <span class="keyword">in</span> self.sequnce:</span><br><span class="line">      <span class="comment"># 위의 batch normalization에서 training이 쓰였기에 여기서 넘겨 주어야 한다.</span></span><br><span class="line">      x = unit(x, training=training)</span><br><span class="line">    <span class="built_in">return</span> x</span><br></pre></td></tr></table></figure><h3 id="모델-정의"><a href="#모델-정의" class="headerlink" title="모델 정의"></a>모델 정의</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">class ResNet(tf.keras.Model):</span><br><span class="line">  def __init__(self):</span><br><span class="line">    super(ResNet, self).__init__()</span><br><span class="line">    self.conv1 = tf.keras.layers.Conv2D(8, (3,3), padding=<span class="string">"same"</span>, activation=<span class="string">"relu"</span>) <span class="comment"># 28X28X8</span></span><br><span class="line"></span><br><span class="line">    self.res1 = ResnetLayer(8, (16, 16), (3, 3)) <span class="comment"># 28X28X16</span></span><br><span class="line">    self.pool1 = tf.keras.layers.MaxPool2D((2,2)) <span class="comment"># 14X14X16</span></span><br><span class="line"></span><br><span class="line">    self.res2 = ResnetLayer(16, (32, 32), (3, 3)) <span class="comment"># 14X14X32</span></span><br><span class="line">    self.pool2 = tf.keras.layers.MaxPool2D((2,2)) <span class="comment"># 7X7X32</span></span><br><span class="line"></span><br><span class="line">    self.res3 = ResnetLayer(32, (64, 64), (3, 3)) <span class="comment"># 7X7X64</span></span><br><span class="line"></span><br><span class="line">    self.flatten = tf.keras.layers.Flatten()</span><br><span class="line">    self.dense1 = tf.keras.layers.Dense(128, activation=<span class="string">"relu"</span>)</span><br><span class="line">    self.dense2 = tf.keras.layers.Dense(10, activation=<span class="string">"softmax"</span>)</span><br><span class="line"></span><br><span class="line">  def call(self, x, training=False, mask=None):</span><br><span class="line">    x = self.conv1(x)</span><br><span class="line"></span><br><span class="line">    x = self.res1(x, training=training)</span><br><span class="line">    x = self.pool1(x)</span><br><span class="line">    x = self.res2(x, training=training)</span><br><span class="line">    x = self.pool2(x)</span><br><span class="line">    x = self.res3(x, training=training)</span><br><span class="line"></span><br><span class="line">    x = self.flatten(x)</span><br><span class="line">    x = self.dense1(x)</span><br><span class="line">    <span class="built_in">return</span> self.dense2(x)</span><br></pre></td></tr></table></figure><h3 id="학습-테스트-루프-정의"><a href="#학습-테스트-루프-정의" class="headerlink" title="학습,테스트 루프 정의"></a>학습,테스트 루프 정의</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Implement training loop</span></span><br><span class="line">@tf.function</span><br><span class="line">def train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy):</span><br><span class="line">  with tf.GradientTape() as tape:</span><br><span class="line">    <span class="comment"># training=True 꼭 넣어주기!!</span></span><br><span class="line">    predictions = model(images, training=True)</span><br><span class="line">    loss = loss_object(labels, predictions)</span><br><span class="line">  gradients = tape.gradient(loss, model.trainable_variables)</span><br><span class="line"></span><br><span class="line">  optimizer.apply_gradients(zip(gradients, model.trainable_variables))</span><br><span class="line">  train_loss(loss)</span><br><span class="line">  train_accuracy(labels, predictions)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Implement algorithm test</span></span><br><span class="line">@tf.function</span><br><span class="line">def test_step(model, images, labels, loss_object, test_loss, test_accuracy):</span><br><span class="line">  <span class="comment"># training=False 꼭 넣어주기!!</span></span><br><span class="line">  predictions = model(images, training=False)</span><br><span class="line"></span><br><span class="line">  t_loss = loss_object(labels, predictions)</span><br><span class="line">  test_loss(t_loss)</span><br><span class="line">  test_accuracy(labels, predictions)</span><br></pre></td></tr></table></figure><h3 id="데이터셋-준비"><a href="#데이터셋-준비" class="headerlink" title="데이터셋 준비"></a>데이터셋 준비</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line"></span><br><span class="line">(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><br><span class="line">x_train, x_test = x_train / 255.0, x_test / 255.0</span><br><span class="line"></span><br><span class="line">x_train = x_train[..., tf.newaxis].astype(np.float32)</span><br><span class="line">x_test = x_test[..., tf.newaxis].astype(np.float32)</span><br><span class="line"></span><br><span class="line">train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)</span><br><span class="line">test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)</span><br></pre></td></tr></table></figure><h3 id="학습-환경-정의"><a href="#학습-환경-정의" class="headerlink" title="학습 환경 정의"></a>학습 환경 정의</h3><h4 id="모델-생성-손실-함수-최적화-알고리즘-평가지표-정의"><a href="#모델-생성-손실-함수-최적화-알고리즘-평가지표-정의" class="headerlink" title="모델 생성, 손실 함수, 최적화 알고리즘, 평가지표 정의"></a>모델 생성, 손실 함수, 최적화 알고리즘, 평가지표 정의</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 모델 생성</span></span><br><span class="line">model = ResNet()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 손실함수 정의 및 최적화 기법 정의</span></span><br><span class="line">loss_object = tf.keras.losses.SparseCategoricalCrossentropy()</span><br><span class="line">optimizer = tf.keras.optimizers.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 평가지표 정의</span></span><br><span class="line">train_loss = tf.keras.metrics.Mean(name=<span class="string">'train_loss'</span>)</span><br><span class="line">train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">'train_accuracy'</span>)</span><br><span class="line"></span><br><span class="line">test_loss = tf.keras.metrics.Mean(name=<span class="string">'test_loss'</span>)</span><br><span class="line">test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=<span class="string">'test_accuracy'</span>)</span><br></pre></td></tr></table></figure><h3 id="학습-루프-동작"><a href="#학습-루프-동작" class="headerlink" title="학습 루프 동작"></a>학습 루프 동작</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(EPOCHS):</span><br><span class="line">  <span class="keyword">for</span> images, labels <span class="keyword">in</span> train_ds:</span><br><span class="line">    train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> test_images, test_labels <span class="keyword">in</span> test_ds:</span><br><span class="line">    test_step(model, test_images, test_labels, loss_object, test_loss, test_accuracy)</span><br><span class="line"></span><br><span class="line">  template = <span class="string">"Epoch &#123;&#125;, Loss: &#123;&#125;, Accuracy: &#123;&#125;, Test Loss: &#123;&#125;, Test Accuracy: &#123;&#125;"</span></span><br><span class="line">  <span class="built_in">print</span>(template.format(epoch+1,</span><br><span class="line">                        train_loss.result(),</span><br><span class="line">                        train_accuracy.result() * 100,</span><br><span class="line">                        test_loss.result(),</span><br><span class="line">                        test_accuracy.result() * 100))</span><br></pre></td></tr></table></figure><p><img src="/image/Resnet_result.png" alt="ResNet 성능 결과"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/12/deep_learning_06/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Scrapy 웹 크롤링 02 - Spider, Scrapy selectors, Items</title>
      <link>https://heung-bae-lee.github.io/2020/01/11/Crawling_01/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/11/Crawling_01/</guid>
      <pubDate>Fri, 10 Jan 2020 18:39:47 GMT</pubDate>
      <description>
      
        
        
          &lt;h3 id=&quot;Spider&quot;&gt;&lt;a href=&quot;#Spider&quot; class=&quot;headerlink&quot; title=&quot;Spider&quot;&gt;&lt;/a&gt;Spider&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Spider의 종류 (참고로, 아래 3가지 종류의 Spider는 잘 사용되지 않는다.)
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h3 id="Spider"><a href="#Spider" class="headerlink" title="Spider"></a>Spider</h3><ul><li>Spider의 종류 (참고로, 아래 3가지 종류의 Spider는 잘 사용되지 않는다.)<ul><li>CrawlSpider</li><li>XMLFeedSpider</li><li>CSVFeedSpider</li><li>SitemapSpider</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 여러사이트를 크롤링하기 위한 spider를 생성</span></span><br><span class="line">scrapy genspider many_site hub.scraping.com</span><br></pre></td></tr></table></figure><p><img src="/image/add_many_site_py.png" alt="여러사이트를 크롤링하기 위한 새로운 spider 생성"></p><ul><li>spider 폴더의 many_site.py파일에서 코드를 작성하기에 앞서 settings.py에서 naver와 daum은 robots.txt에서 크롤링을 불허하기에 다음과 같은 수정작업을 해주어야 크롤링이 가능하다.</li></ul><p><img src="/image/modify_settings_py_03.png" alt="daum과 naver를 크롤링하기 위한 setting.py 수정"></p><ul><li>아래의 코드처럼 여러 도메인을 크롤링할 수 있는 방법은 크게 3가지 정도가 있다.</li></ul><p><img src="/image/many_site_py_in.png" alt="여러 사이트 크롤링 방법"></p><ul><li>다음과 같이 1번째 방법을 사용해서 로깅 및 분기처리로 여러 사이트를 크롤링할 수 있게끔 코드를 수정해 주었다.<br><img src="/image/many_site_py_in_last.png" alt="여러 사이트 크롤링 방법 1"></li></ul><h3 id="Selector"><a href="#Selector" class="headerlink" title="Selector"></a>Selector</h3><h4 id="xpath-selector-도움-사이트"><a href="#xpath-selector-도움-사이트" class="headerlink" title="xpath selector 도움 사이트"></a>xpath selector 도움 사이트</h4><ul><li><a href="https://docs.scrapy.org/en/latest/topics/selectors.html#working-with-xpath" target="_blank" rel="noopener">https://docs.scrapy.org/en/latest/topics/selectors.html#working-with-xpath</a></li><li><a href="www.nextree.co.kr/p6278">www.nextree.co.kr/p6278</a></li></ul><h4 id="css-selector-도움-사이트"><a href="#css-selector-도움-사이트" class="headerlink" title="css selector 도움 사이트"></a>css selector 도움 사이트</h4><ul><li><a href="https://docs.scrapy.org/en/latest/topics/selectors.html#extension-to-css-selectors" target="_blank" rel="noopener">https://docs.scrapy.org/en/latest/topics/selectors.html#extension-to-css-selectors</a></li></ul><h4 id="crawling시-활용-tip"><a href="#crawling시-활용-tip" class="headerlink" title="crawling시 활용 tip"></a>crawling시 활용 tip</h4><ul><li>타겟 데이터는 크롬 개발자 도구 사용</li><li>선택자 연습 팁 : scrapy shell 에서 테스트(효율성)</li><li><p>scrapy shell 도메인</p></li><li><p>중요(완전 동치는 아니다!)</p><ul><li>get() == extract_first()</li><li>getall() == extract()</li></ul></li></ul><h4 id="CSS-선택자"><a href="#CSS-선택자" class="headerlink" title="CSS 선택자"></a>CSS 선택자</h4><ul><li>div#chan div : (자손) chan이라는 class속성값으로 갖는 div tag의 아래에 존재하는 모든 div</li><li>div#chan &gt; div : (자식) chan이라는 class속성값으로 갖는 div tag의 직계자식 div들</li><li>::text -&gt; 노드의 텍스트만 추출</li><li>::attr(name) -&gt; 노드 속성값 추출            </li><li><p>get(default=’’) : get으로 추출할 때 해당사항이 없다면 공백으로 출력</p></li><li><p>예시)</p><ul><li>response.css(‘title::text’).get() : title tag의 텍스트만 추출</li><li>response.css(‘div &gt; a::attr(href)’).getall() : div tag의 자식 a tag의 href속성값 전부 추출</li></ul></li></ul><h4 id="Xpath-선택자"><a href="#Xpath-선택자" class="headerlink" title="Xpath 선택자"></a>Xpath 선택자</h4><ul><li>nodename : 이름이 nodename 선택</li><li>text() -&gt; 노드 텍스트만 추출</li><li>/ : 루트부터 시작</li><li>// : 현재 node 부터 문서상의 모든 노드 조회</li><li>. : 현재 node</li><li>.. : 현재 node의 부모노드</li><li>@ : 속성 선택자</li><li>예시)<ul><li>response.xpath(‘/div’) : 루트노드부터 모든 div tag 선택</li><li>response.xpath(‘//div[@id=”id”]/a/text()’).get() : div tag 중 id가 ‘id’인 자식 a tag의 텍스트 하나만 추출</li></ul></li></ul><h4 id="혼합-사용-가능"><a href="#혼합-사용-가능" class="headerlink" title="혼합 사용 가능!!"></a>혼합 사용 가능!!</h4><ul><li>response.css(‘img’).xpath(‘@src’).getall()</li></ul><h4 id="실습-w3school-웹에-관한-정보들이-있는-사이트"><a href="#실습-w3school-웹에-관한-정보들이-있는-사이트" class="headerlink" title="실습) w3school(웹에 관한 정보들이 있는 사이트)"></a>실습) w3school(웹에 관한 정보들이 있는 사이트)</h4><ul><li>실습 목표 : <code>nav 메뉴 이름 크롤링 실습</code></li><li>과정 : shell 실행 -&gt; 선택자 확인 -&gt; 코딩 -&gt; 데이터 저장(프로그램 테스트)</li></ul><p><img src="/image/css_selector_practice.png" alt="실습을 위한 spider 코드"></p><h3 id="Items"><a href="#Items" class="headerlink" title="Items"></a>Items</h3><ul><li>구조적으로 데이터를 크롤링할 수 있게 해주는 역할을 한다. 예를 들면 내가 크롤링할 데이터를 정확하게 구분(신문기사의 이름, 본문, 이미지 이렇게 구조적으로 구분)하게 구조적으로 규칙을 정하고 그 규칙들을 Items라는 파일안에 작성하여 나중에 Items를 return하면 명확하게 구분된 우리가 원하는 여러가지 형식으로 저장할 수 있다. <code>spider는 직접 크롤링을 하는 역할, Items는 크롤링 될 타겟 데이터를 명확히 해주는 역할이라고 생각하면 될 것이다.</code></li></ul><h4 id="Scrapy-Item"><a href="#Scrapy-Item" class="headerlink" title="Scrapy Item"></a>Scrapy Item</h4><h5 id="장점"><a href="#장점" class="headerlink" title="장점"></a>장점</h5><ul><li>1) 수집 데이터를 일관성있게 관리 가능</li><li>2) 데이터를 사전형(Dict)로 관리, 오타 방지</li><li><p>3) 추후 가공 및 DB 저장 용이</p></li><li><p>Items를 사용한 scrapy는 새로운 사이트를 크롤링할 것이므로 새로운 spider를 만들어준다.</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider using_items itnews.com</span><br></pre></td></tr></table></figure><p><img src="/image/add_using_items_spider.png" alt="using_items spider 생성"></p><ul><li>items.py 파일에서 우리의 타켓 데이터를 정의해준다.</li></ul><p><img src="/image/make_rule_items.png" alt="items.py 파일에 데이터 정의"></p><ul><li><code>items.py를 활용하기 위해 import를 할 경우 다음과 같이 절대경로를 사용한 path 추가 방법을 사용</code>해야한다. 다음과 같이 items의 ItArticle class를 활용하여 spider를 좀 더 깔끔하게 작성할 수 있다.</li></ul><p><img src="/image/using_items_spider_in.png" alt="using_items.py spider 작성"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/11/Crawling_01/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Scrapy 웹 크롤링 01 - 환경설정 및 기초</title>
      <link>https://heung-bae-lee.github.io/2020/01/09/Crawling_00/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/09/Crawling_00/</guid>
      <pubDate>Thu, 09 Jan 2020 12:08:12 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;Scrapy-VS-Beautiful-Soup&quot;&gt;&lt;a href=&quot;#Scrapy-VS-Beautiful-Soup&quot; class=&quot;headerlink&quot; title=&quot;Scrapy VS Beautiful Soup&quot;&gt;&lt;/a&gt;Scrapy VS Beau
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="Scrapy-VS-Beautiful-Soup"><a href="#Scrapy-VS-Beautiful-Soup" class="headerlink" title="Scrapy VS Beautiful Soup"></a>Scrapy VS Beautiful Soup</h2><h3 id="Beautiful-Soup"><a href="#Beautiful-Soup" class="headerlink" title="Beautiful Soup"></a>Beautiful Soup</h3><ul><li>Beautiful Soup는 웹 상의 정보를 빠르게 크롤링 하기위한 도구이며, <code>정적인 정보를 가져 올 수 있다. 즉, 해당 API(URL)에 요청했을때 바로 가져올수 있는 정보들만 가져올 수 있다. 시간이 좀 더 걸린 후에 나오는 정보들은 가져올 수 없다는 것이다.</code> 진입 장벽이 매우 낮고 간결해서, 입문 개발자에게 안성맞춤이다. 그리고 이 라이브러리는 스스로 크롤링을 하는 것이 아니라 <code>urlib2</code> 또는 <code>requests</code> 모듈을 통해 HTML 소스를 가져와야 한다.</li></ul><h3 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h3><ul><li>Scrapy는 Python으로 작성된 Framework이며, spider(bot)을 작성해서 크롤링을 한다. Scrapy에서는 직접 <code>Beautiful Soup</code> 이나 <code>lxml</code>을 사용할 수 있다. 하지만 Beautiful Soup에서는 지원하지 않는 <code>Xpath</code>를 사용할 수 있다. 또한, Xpath를 사용함으롴써 복잡한 HTML소스를 쉽게 크롤링 할 수 있게 해준다. 또한 Xpath를 통한 crawling이 가능한 모듈로는 selenium도 존재한다. selenium도 Scrapy와 연동해서 가능하다.</li></ul><h3 id="Anaconda-env"><a href="#Anaconda-env" class="headerlink" title="Anaconda env"></a>Anaconda env</h3><ul><li>먼저 사전에 anaconda를 통해 가상환경을을 만들어준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># env 생성</span></span><br><span class="line">conda create -n env_name python=3.5</span><br><span class="line"></span><br><span class="line"><span class="comment"># env 리스트 보기</span></span><br><span class="line">conda env list</span><br><span class="line"></span><br><span class="line"><span class="comment"># env 활성화</span></span><br><span class="line">conda activate env_name</span><br><span class="line"></span><br><span class="line"><span class="comment"># env 비활성화</span></span><br><span class="line">conda deavtivate</span><br><span class="line"></span><br><span class="line"><span class="comment"># env 삭제</span></span><br><span class="line">conda env remove -n env_name</span><br></pre></td></tr></table></figure><h3 id="Scrapy-환경설정"><a href="#Scrapy-환경설정" class="headerlink" title="Scrapy 환경설정"></a>Scrapy 환경설정</h3><ul><li>먼저 가상환경을 활성화시켜준 후에, spider bot을 만들 폴더의 상위 폴더에서 다음의 명령어를 실행시켜준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conda activate env_name</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"></span><br><span class="line">scrapy startproject project_name</span><br></pre></td></tr></table></figure><ul><li>다음과 같이 설정한 project명을 갖는 폴더가 만들어지며, 필자는 section01_2라고 명명했다.</li></ul><p><img src="/image/scrapy_startproject.png" alt="create project folder"></p><ul><li>위의 단계까지 실행했다면, 다음과 같은 출력이 보일 것이다. 빨간줄 아래에 나와있는 예시 명령어를 따라서 실행시키면 spider bot을 만들수 있다.</li></ul><p><img src="/image/scrapy_startproject_then.png" alt="scrapy startproject"></p><ul><li>또한, <code>모든 앞으로의 모든 명령어는 scrapy.cfg라는 파일이 존재하는 directory path에서 해야한다.</code></li></ul><p><img src="/image/scrapy_commend_site.png" alt="scrapy 명령어 실행하는 path"></p><h3 id="genspider"><a href="#genspider" class="headerlink" title="genspider"></a>genspider</h3><ul><li>scrapy에서 가장 중요한 spider class를 만들어준다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># spider를 만들기 위해 명령어를 실행하려면 scrapy.cfg파일의 경로로 이동해야하기 때문에</span></span><br><span class="line"><span class="built_in">cd</span> section01_2</span><br><span class="line"></span><br><span class="line"><span class="comment"># scrapy.cfg파일이 존재하는지 다시 한번 확인</span></span><br><span class="line">ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># https://blog.scrapinghub.com/은 crawling 테스트를 위한 사이트로 유명하다.</span></span><br><span class="line"><span class="comment"># https://blog.scrapinghub.com/이라는 사이트를 크롤링할 testspider라는 이름으로 spider 을 만들어라는 명령어</span></span><br><span class="line">scrapy genspider testspider blog.scrapinghub.com</span><br></pre></td></tr></table></figure><ul><li>다음과 같은 출력결과를 볼 수 있으며, section01_2에 spiders라는 폴더의 testspider라고 만들어졌다는 것을 의미한다.<br><img src="/image/genspider.png" alt="genspider"></li></ul><p><img src="/image/genspider_then.png" alt="genspider 실행 후 파일 변경"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 만들어진 spider 파일 확일을 위해 이동</span></span><br><span class="line"><span class="built_in">cd</span> section01_2/spiders</span><br><span class="line"></span><br><span class="line"><span class="comment"># 위에서 만들어 놓았던 testspider라는 spider가 있는지 확인</span></span><br><span class="line">ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># testspider.py 확인</span></span><br><span class="line">vim testspider.py</span><br></pre></td></tr></table></figure><p><img src="/image/testspider.png" alt="만들어진 spider 파일 확인"></p><ul><li><p>먼저 straturl과 우리가 크롤링하려는 URL endpoint가 https인지 확인한 후 고쳐준다.(여기서 필자는 vim으로 수정하였기에 pep8에 의거하여 space 4번으로 indent를 사용하였다. <code>space와 tap을 번갈아가며 사용하면 python interpreter가 다르게 인식하므로 에러를 발생시킨다!</code>)</p></li><li><p>앞으로의 실습에 헷갈림을 방지하기 위해서 name을 test1으로 변동해주었고, allowed_domains과 start_urls를 보면 설정해 놓은 대로 들어가 있는 것을 알 수 있다. 여기서 <code>scrapy는 allowed_domains과 start_urls가 리스트 구조로 되어있는데 다른 URL과 도메인들을 추가하면 해당 사이트들을 돌아가며 크롤링을 할 수 있는 병렬처리가 가능하다는 것이 가장 큰 장점</code>이다. 추후에 설명하겠지만, 눈치 빠르신 분들은 아래 <code>parse</code>함수에서 response를 parameter로 받는 함수이므로 이 함수에 크롤링하고 싶은 부분에 대한 코드를 만들면 크롤링이 가능하다는 것을 알 것이다!! 혹시 response에서 어떤 명령어가 사용가능한지 보고 싶다면</p></li></ul><p><img src="/image/testspider_in.png" alt="크롤링 상태 확인을 하기 위해 parse에 print문 추가"></p><h3 id="runspider-vs-crawl"><a href="#runspider-vs-crawl" class="headerlink" title="runspider vs crawl"></a>runspider vs crawl</h3><ul><li>runspider와 crawl의 차이점은 <code>runspider는 spiders폴더에서 실행할 수 있고, crawl은 scrapy.cfg파일이 존재하는 폴더에서 실행하여햐 한다는 점이 차이점이다!!</code> runspider 명령어를 통해 spider bot을 실행시키는 것은 단위 테스트라고 소위 불리는 방식을 할 때 유용하고 crawl은 우리가 원하는 구조를 다 만들어 놓은 후 테스트를 할 때나 실제로 크롤링을 할 경우 사용하는 것이 유용하다.  </li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># runspider는 spiders 폴더에서 실행하여야한다.</span></span><br><span class="line">scrapy runspider testspider.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># crawl은 scrapy.cfg파일이 존재하는 path에서 실행시켜주어야한다.</span></span><br><span class="line">scrapy crawl test1 --nolog</span><br></pre></td></tr></table></figure><p><img src="/image/dir.png" alt="spider 실행후 결과 중 parse함수에서 reponse뒤에 사용할 수 있는 명령어의 종류"></p><h3 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a>settings.py</h3><p><img src="/image/settings_py.png" alt="settings.py의 위치"></p><ul><li>spider의 속성에 관련된 parameter들이 있는 파일이라고 생각하면 된다. 예를 들면, 아래의 그림에서 볼 수 있듯이 <code>SPIDER MODULES</code>는 현재 SPIDER의 위치를 의미하고, <code>NEWSPIDER MODULE</code>은 Spider를 새로 생성시 어느 위치에 추가되는지를 의미한다. <code>ROBOTSTXT_OBEY</code>는 robots.txt의 규칙에 의거하여 crawling을 하겠다는 의미이며, <code>DOWNLOAD_DELAY</code>는 몇초간격으로 서버에 요청을 할지에 대한 수치이다. 필자는 1로 정했는데 여기서는 1초마다라는 의미이다. <code>만약에 0.2라고 하게 되면 0.2초마다 서버에 요청하게 되어 서버에 부하를 일으키게 되면 심할경우 영구 van을 당할 수도 있기에 간격을 1초이상으로 하는 것을 권장한다.</code></li></ul><p><img src="/image/settings_py_in.png" alt="settings.py"></p><h3 id="실습-blog-scrapinghub-com에서-기사-제목들만-크롤링-하기"><a href="#실습-blog-scrapinghub-com에서-기사-제목들만-크롤링-하기" class="headerlink" title="실습)blog.scrapinghub.com에서 기사 제목들만 크롤링 하기!"></a>실습)blog.scrapinghub.com에서 기사 제목들만 크롤링 하기!</h3><ul><li><p>위의 실습주제로 실습을 진행하기 위해서는 앞서 만들어본 spider 파일에서 parse함수를 수정해야할 것이다. 그에 앞서 크롤링할 blog.scrapinghub.com의 제목에 해당하는 css path를 보면 전체 html의 body 부분에서 div element 중 class의 이름이 post-header인 부분에만 존재하는 것을 개발자 도구를 통해 알아내었다. <code>다른 부분에 동일한 element나 class명을 가질 수도 있으므로 find를 해보아야한다!</code></p></li><li><p>다음과 같이 제목을 크롤링하기 위해 testspider.py을 수정해 주었다.</p></li></ul><p><img src="/image/modify_spider_00.png" alt="크롤링을 위한 testspider.py 수정"></p><ul><li>참고로 결과를 파일로 저장할때 scrapy가 지원하는 파일 형식은 <code>json, jsonlines, jl, csv, xml, marshal, pickle</code>이다. <code>결과를 저장할때 동일한 파일명과 확장자명을 가진 파일이 이미 존재한다면 그 파일에 데이터를 추가해주므로 주의</code>하자!</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 필자는 spider 폴더에서 실행함.</span></span><br><span class="line">scrapy runspider testspider.py -o result.csv -t csv</span><br><span class="line"></span><br><span class="line"><span class="comment"># result 파일인 result.csv가 만들어진 것을 확인할 수 있다.</span></span><br><span class="line">ls</span><br></pre></td></tr></table></figure><h3 id="Requests를-사용하여-페이지-순회하며-크롤링"><a href="#Requests를-사용하여-페이지-순회하며-크롤링" class="headerlink" title="Requests를 사용하여 페이지 순회하며 크롤링"></a>Requests를 사용하여 페이지 순회하며 크롤링</h3><ul><li><p>우리가 예를 들어 어떤 페이지내에서 여러 항목에 대해 해당 url로 이동 후 크롤링하고 다시 그 전 페이지로 돌아가서 다음 항목의 url로 이동 후 크롤링하는 이런 순회를 거쳐야하는 작업은 request나 selenium으로 하게되면 iterable한 코드를 통해 가능하게 되며, 그렇지 않다면, 동일한 구성을 지닌 페이지들이라면 함수를 여러번 실행하는 등의 multiprocessing을 통해 병렬처리를 따로 해주어여하는 불편함이 있다. 허나, scrapy는 코드 몇 줄로 가능하다.</p></li><li><p>먼저 앞의 spider말고 새로운 spider를 만들어 사용할 것이다. spider를 만든 설정은 위에서 만든 것과 동일하다. spider 이름만 pagerutine이라고 명명했을 뿐이다.</p></li></ul><p><img src="/image/new_spider.png" alt="새로운 spider 만들기"></p><ul><li><p>이전에 blog.scrapinghub.com의 페이지에 해당 10개의 기사들에 대한 path가 “div.post-header h2 &gt; a” 이며, a tag의 href 속성 값들을 통해 각각의 기사에 해당 페이지로 이동이 가능할 수 있다는 사실을 확인 할 수 있다.   </p></li><li><p>첫번째 <code>parse 함수</code>에서 미리 추출한 url을 request하여 얻은 response를 다른 함수로 전달해주기 위해 Request 명령어를 사용하였으며, urljoin을 사용한 이유는 절대주소가 아닌 상대주소로 되어있는 경우 위의 start_urls에 설정해 놓은 주소를 앞에 붙여 절대 주소로 바꿔주는 기능이다. 물론 절대주소인 경우는 이런 작업을 생략한다.</p></li></ul><p><img src="/image/pagerutine_py_in.png" alt="pagerutine.py 파일"></p><h3 id="Scrapy-Shell-사용법"><a href="#Scrapy-Shell-사용법" class="headerlink" title="Scrapy Shell 사용법"></a>Scrapy Shell 사용법</h3><ul><li>쉽게 말해 이전에 크롤링을 할때 Spider 폴더에서 파일을 수정하고 테스트해보는 방식으로는 작업의 효율성이 떨어지므로 그 전에 css나 xpath selector를 테스트해볼 수 있는 것이 Shell mode이다.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shell 모드 접속</span></span><br><span class="line">scrapy shell</span><br><span class="line"></span><br><span class="line"><span class="comment">################ shell 모드 접속했다고 가정 ###############</span></span><br><span class="line"><span class="comment"># url 설정(request하는 대상을 바꾸는 역할)</span></span><br><span class="line">fetch(<span class="string">'url'</span>)</span><br><span class="line"></span><br><span class="line">quit</span><br><span class="line"></span><br><span class="line"><span class="comment">################# 다른 방법 ############################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 위의 단계를 한번에 하는 방법</span></span><br><span class="line">scrapy shell https://blog.scrapinghub.com</span><br><span class="line"></span><br><span class="line"><span class="comment"># response data가 무엇이 있는지 확인 할 수 있다. 소스페이지를 현재 내 컴퓨저에 가져와서 보여주는 방식</span></span><br><span class="line">view(response)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 예시</span></span><br><span class="line">fetch(https://daum.net)</span><br><span class="line">view(response)</span><br><span class="line"></span><br><span class="line"><span class="comment"># response가 사용할수있는 method 확인</span></span><br><span class="line">dir(response)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 현재 reponse의 url 확인</span></span><br><span class="line">response.url</span><br><span class="line"></span><br><span class="line"><span class="comment"># 현재 response의 body정보 확인</span></span><br><span class="line">response.body</span><br><span class="line"></span><br><span class="line"><span class="comment"># 현재 reponse의 status 확인</span></span><br><span class="line">response.status</span><br><span class="line"></span><br><span class="line"><span class="comment"># robot.txt에 크롤링이 허용되지 않았으면 shell script가 실행되지 않는다.</span></span><br><span class="line"><span class="comment"># settings 파일에서의 설정 파라미터들을 동적으로 설정하며 실행 가능!</span></span><br><span class="line">scrapy shell https://daum.net --<span class="built_in">set</span>=<span class="string">"ROBOTSTXT_OBEY=False"</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/09/Crawling_00/#disqus_thread</comments>
    </item>
    
    <item>
      <title>모형 성능 평가 지표</title>
      <link>https://heung-bae-lee.github.io/2020/01/09/machine_learning_03/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/09/machine_learning_03/</guid>
      <pubDate>Thu, 09 Jan 2020 06:12:42 GMT</pubDate>
      <description>
      
        
        
          &lt;h3 id=&quot;회귀-regression-평가-지표&quot;&gt;&lt;a href=&quot;#회귀-regression-평가-지표&quot; class=&quot;headerlink&quot; title=&quot;회귀(regression) 평가 지표&quot;&gt;&lt;/a&gt;회귀(regression) 평가 지표&lt;/h3&gt;&lt;ul
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h3 id="회귀-regression-평가-지표"><a href="#회귀-regression-평가-지표" class="headerlink" title="회귀(regression) 평가 지표"></a>회귀(regression) 평가 지표</h3><ul><li>회귀의 평가를 위한 지표는 실제 값과 회귀 예측값의 차이 값을 기반으로 한 지표가 중심이다. 실제값과 예측값의 차이를 그냥 더하면 잔차의 합은 0이므로 지표로 쓸 수 없다. 이 때문에 잔차의 절대값 평균이나 제곱, 또는 제곱한 뒤 다시 루트를 씌운 평균값을 성능 지표로 사용한다.</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">평가 지표</th><th>수식</th></tr></thead><tbody><tr><td style="text-align:center">MAE(Mean Absolute Error)</td><td>$MAE =  \frac{1}{n} \sum_{i=1}^{n} \lvert Y_{i} - \hat{Y_{i}} \rvert$</td></tr><tr><td style="text-align:center">MSE(Mean Squared Error)</td><td>$MSE =  \frac{1}{n} \sum_{i=1}^{n} (Y_{i} - \hat{Y_{i}})$</td></tr><tr><td style="text-align:center">RMSE(Root Mean Squared Error)</td><td>$RMSE = $\sqrt{\frac{1}{n} \sum_{i=1}^{n} (Y_{i} - \hat{Y_{i}})^{2}}$</td></tr><tr><td style="text-align:center">$R^{2}$</td><td>$R^{2} = \frac{예측값의 Variance}{실제값 Variance} = \frac{SSR}{SST}$</td></tr></tbody></table></div><p><img src="/image/MSE.png" alt="MSE"></p><p><img src="/image/MAPE.png" alt="MAPE"></p><p><img src="/image/R_squared.png" alt="결정계수"></p><p><img src="/image/adj_R_squared.png" alt="수정된 결정계수"></p><ul><li>이전에도 언급했던 것처럼 변수가 추가된다면 당연히 SSR의 수치가 높아지기 때문에 $R^{2}$값은 올라갈 수 밖에 없다. 그러므로 변수 수에 영향을 받지 않고 서로 비교할 수 있게끔 만들어 준 것이 수정된 결정계수이다.</li></ul><p><img src="/image/AIC.png" alt="AIC"></p><p><img src="/image/BIC.png" alt="BIC"></p><h3 id="분류-classification-성능-지표"><a href="#분류-classification-성능-지표" class="headerlink" title="분류(classification) 성능 지표"></a>분류(classification) 성능 지표</h3><p><img src="/image/confusion_matrix_00.png" alt="Accuarcy"></p><p><img src="/image/confusion_matrix_01.png" alt="Recall, Precision, Specificity"></p><ul><li><p>특히 imbalanced data에서 모형의 성능을 정확도 하나만을 가지고 성능을 평가한다면, 예를 들어, 100개중 90개는 세모고 10개는 네모라고 할 때 100개 모두 세모라고 예측해버리게 되면 정확도는 90%이므로 좋은 성능 지표라고 할 수 없다. 그러므로 <code>imbalanced data에서의 성능 지표는 정확도(accuarcy) 보다는 정밀도(precision), 재현율(Recall)를 더 선호한다.</code></p></li><li><p>정밀도(precision)와 재현율(recall) 지표 중에 분류 모델의 업무 특성에 따라서 특정 평가 지표가 더 중요한 지표로 간주 될 수 있다. <code>재현율(recall)이 중요 지표인 경우는 실제 Positive 양성 데이터를 Negative로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우(FN이 Critical한 경우)</code>이다. 예를 들어 <code>암 판단 모형</code>은 재현율(recall)이 훨씬 중요한 지표이다. 왜냐하면 실제 Positive인 암 환자를 Positive 양성이 아닌 Negative 음성으로 잘못 판단했을 경우 오류의 대가가 생명을 앗아갈 정도로 심각하기 때문이다. 반면에 실제 Negative인 건강한 환자를 암 환자인 Positive로 예측한 경우면 다시 한번 재검사를 하는 수준의 비용이 소모될 것이다. 또 다른 예로는, 금융 사기 적발 모델을 들 수 있다. 물론 고객에게 금융 사기 혐의를 잘못 씌우면 문제가 될 수 있기에 정밀도(Precision)도 중요 평가 지표지만, <code>업무적인 특성을 고려하면 재현율(Recall)이 상대적으로 더 중요한 지표</code>입니다. <code>보통은 재현율(Recall)이 정밀도(Precision)보다 상대적으로 중요한 업무가 많지만, 정밀도가 더 중요한 지표인 경우도 있다.</code> 예를 들어, 스팸메일 여부를 판단하는 모형의 경우 실제 Positive인 스팸 메일을 Negative인 일반 메일로 분류하더라도 사용자가 불편함을 느끼는 정도이지만, 실제 Negative인 일반 메일을 Positive인 스팸메일로 분류할 경우에는 메일을 아예 받지 못하게 돼 업무에 차질이 생긴다. 정밀도(Precision)이 상대적으로 더 중요한 지표인 경우는 <code>실제 Negative 음성인 데이터 예측을 Positive 양성으로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우(FP가 Critical한 경우)</code>이다.</p></li><li><p>재현율(Recall)과 정밀도(Precision) 모두 TP를 높이는 데 동일하게 초점을 맞추지만, 재현율(Recall)은 FN를 낮추는데, 정밀도(Precision)는 FP를 낮추는데 초점을 맞춘다. <code>이 같은 특성 때문에 재현율(Recall)과 정밀도(Precision)은 서로 보완적인 지표로 분류의 성능을 평가하는데 적용</code>된다. <code>가장 좋은 성능 평가는 재현율(Recall)과 정밀도(Precision) 모두 높은 수치를 얻는 것</code>이다. 반면에 둘 중 어느 한 평가 지표만 매우 높고, 다른 수치는 매우 낮은 결과를 나타내는 경우에는 바람직하지 않다.</p></li></ul><h3 id="정밀도-Precision-재현율-Recall-Trade-off"><a href="#정밀도-Precision-재현율-Recall-Trade-off" class="headerlink" title="정밀도(Precision)/ 재현율(Recall) Trade-off"></a>정밀도(Precision)/ 재현율(Recall) Trade-off</h3><ul><li><p>분류하려는 업무의 특성상 정밀도(Precision) 또는 재현율(Recall)이 특별히 강조돼야 할 경우 분류의 결정 임계값(Threshold)을 조정해 정밀도(Precision) 또는 재현율(Recall)의 수치를 높일 수 있다. 하지만 정밀도(Precision)와 재현율(Recall)은 상호 보완적인 평가 지표이기 때문에 어느 한쪽을 강제로 높이면 다른 하나의 수치는 떨어지기 쉽다. 이를 <code>정밀도(Precision)/재현율(Recall)의 Trade-off</code>라고 부른다.</p></li><li><p>scikit-learn에서 각각의 분류모델들은 predict_proba의 결과를 <code>Threshold(보통은 0.5)보다 같거나 작으면 0값으로, 크면 1값으로 변환해 반환하는 Binarizer 클래스를 사용</code>하여 predict의 결과를 계산하여 반환해 준다. 만약 <code>임계값을 낮추면 재현율(Recall)값이 올라가고 정밀도(Precision)가 떨어질 것</code>이다. 그 이유는 임계값은 Positive 예측값을 결정하는 확률의 기준이 되는데 임계값을 0.5에서 0.4로 낮추면 그만큼 Positive 예측을 더 너그럽게 하기 떄문에 True로 예측하는 값이 많아지게 된다. <code>Positive 예측을 많이 하다보니 실제 양성을 음성으로 예측하는 횟수가 상대적으로 줄어들기 때문</code>이다.</p></li></ul><h3 id="정밀도-Precision-과-재현율-Recall-의-맹점"><a href="#정밀도-Precision-과-재현율-Recall-의-맹점" class="headerlink" title="정밀도(Precision)과 재현율(Recall)의 맹점"></a>정밀도(Precision)과 재현율(Recall)의 맹점</h3><ul><li>Positive 예측의 임계값을 변경함에 따라 정밀도(Precision)와 재현율(Recall)의 수치가 변경된다. <code>임계값의 이러한 변경은 업무 환경에 맞게 두 개의 수치를 상호 보완할 수 있는 수준에서 적용돼야 한다.</code> 그렇지 않고 단 하나의 성능 지표 수치를 높이기 위한 수단으로 사용돼서는 안된다. 각각의 지표를 극단적으로 높일 수는 있고, 정밀도(Precision) 또는 재현율(Recall) 중 하나에 상대적인 중요도를 부여해 각 예측 상황에 맞는 분류 알고리즘을 튜닝할 수 있지만, 그렇다고 <code>정밀도(Precision)/재현율(Recall) 중 하나에 상대적인 중요도를 부여해 각 예측 상황에 맞는 분류 알고리즘을 튜닝할 수 있지만, 그렇다고 정밀도(Precision)/재현율(Recall) 하나만 강조하는 상황이 돼서는 안된다.</code></li></ul><ul><li><code>F1-score</code>는 <code>정밀도(Precision)와 재현율(Recall)을 결합한 지표로 어느 한쪽으로 치우치지 않는 수치를 나타낼 때 상대적으로 높은 값을 가진다.</code> 여기서 또 한가지 주의할 점은 정밀도(Precision)와 재현율(Recall)의 조화평균값이라 해서 무조건 F1-score가 높은 것이 좋은 모형은 아니라는 점이다. 정밀도(Precision)과 재현율(Recall) 그리고 F1-score 모두 구한 후 비교하여 적합한 모형을 선정하는 것이 중요하다!</li></ul><script type="math/tex; mode=display">F1 = \frac{2}{\frac{1}{Recall} + \frac{1}{Precision}}</script><p><img src="/image/G_maen_and_F1_score.png" alt="G-maen, F1-score"></p><ul><li><p><code>ROC Curve</code>와 이에 기반한 <code>AUC score</code>는 <code>이진 분류의 예측 성능 측정에서 중요하게 사용되는 지표</code>이다. ROC Curve(Receiver Operation Characteristic Curve)는 일반적으로 의학분야에서 많이 사용되지만, 머신 러닝의 이진 분류 모델의 예측 성능을 판단하는 중요한 지표이다. <code>ROC Curve</code>는 <code>FRR(False Positive Rate)이 변할 때 TPR(True Positive Rate)이 어떻게 변하는지를 나타내는 곡선</code>이다. FPR을 X축으로 하고 FPR을 0부터 1까지 변경하면서, TPR을 Y축으로 잡아 FPR에 변화에 따른 TPR의 변화가 곡선 형태로 나타난다. 분류결정 임계값은 Positive 에측값을 결정하는 값이므로 FPR을 0으로 만들려면 1로 지정하면 된다. TPR은 재현율(Recall)과 동일하며, 민감도라고도 불린다. <code>가운데 직선은 ROC Curve의 최저값(AUC는 0.5)이다. ROC 곡선이 가운데 직선에 가까울수록 성능이 떨어지는 것이며, 멀어질수록 성능이 뛰어나다는 것이다.</code></p></li><li><p><code>일반적으로 ROC Curve 자체는 FPR과 TPR의 변화값을 보는 데 이용하며 분류의 성능 지표로 사용되는 것은 ROC Curve 면적에 기반한 AUC 값으로 결정</code>한다. AUC(Area Under Curve)값은 ROC Curve 밑의 면적을 구한 것으로서 일반적으로 1에 가까울수록 좋은 수치이다. AUC 수치가 커지려면 FPR이 작음 상태에서 얼마나 큰 TPR을 얻을 수 있느냐가 관건이다. 가운데 직선을 랜덤 수준의(동전 던지기 수준) 이진 분류 AUC 값으로 0.5이다. 따라서 보통의 분류는 0.5이상의 AUC값을 가지낟.</p></li></ul><script type="math/tex; mode=display">TPR(민감도) = \frac{TP}{TP+FN}</script><script type="math/tex; mode=display">TNR(특이성) = \frac{TN}{FP+TN})</script><script type="math/tex; mode=display">FPR = 1 - TNR = \frac{FP}{FP+TN}</script><p><img src="/image/ROC_and_AUC.png" alt="ROC, AUC"></p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/09/machine_learning_03/#disqus_thread</comments>
    </item>
    
    <item>
      <title>Regression(02) - 다중선형회귀 및 다중공선성</title>
      <link>https://heung-bae-lee.github.io/2020/01/08/machine_learning_02/</link>
      <guid>https://heung-bae-lee.github.io/2020/01/08/machine_learning_02/</guid>
      <pubDate>Wed, 08 Jan 2020 14:22:36 GMT</pubDate>
      <description>
      
        
        
          &lt;h2 id=&quot;다중-선형-회귀&quot;&gt;&lt;a href=&quot;#다중-선형-회귀&quot; class=&quot;headerlink&quot; title=&quot;다중 선형 회귀&quot;&gt;&lt;/a&gt;다중 선형 회귀&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/image/Multiple_linear_regression.pn
        
      
      </description>
      
      
      <content:encoded><![CDATA[<h2 id="다중-선형-회귀"><a href="#다중-선형-회귀" class="headerlink" title="다중 선형 회귀"></a>다중 선형 회귀</h2><p><img src="/image/Multiple_linear_regression.png" alt="다중 선형 회귀"></p><p><img src="/image/Multiple_linear_regression_01.png" alt="다중 선형 회귀의 계수 추정"></p><p><img src="/image/Multiple_linear_regression_01.png" alt="다중 선형 회귀의 계수의 해석"></p><ul><li><p><code>다중회귀방정식에서 회귀계수에 대한 해석은 자주 혼동되는 것 중 하나이다. 단순회귀방정식은 직선을 표현하지만 다중회귀방정식은 평면(독립(설명)변수가 두개인 경우) 혹은 초평면(독립(설명)변수가 두개보다 많은 경우)을 표현</code>한다. 위의 예에서 회귀계수의 해석은 다른 변수들이 고정되어 있을때 TV가 1단위 증가할 때 매출액은 0.046단위 증가한다고 해석할 수 있다.</p></li><li><p>회귀 계수 $\beta_{j}$는 $X_{j}$를 제외한 나머지 모든 예측 변수들을 상수로 고정시킨 상태에서 $X_{j}$의 한 단위 증가에 따른 Y의 증분으로 해석될 수 있다. 변화의 크기는 다른 예측 변수들이 어떤 값으로 고정되어 있는지에 의존하지 않는다. 또 다른 해석은 $\beta_{j}$가 다른 독립(설명)변수들에 의하여 종속(반응)변수 Y가 조정된 후에 Y에 대한 $X_{j}$의 공헌도를 의미한다. 이는 예를 들어 $Y = \beta{0} + \beta_{1}X_{1} + \beta_{2}X_{2} + \epsilon $일 때, $[Y~X_{1}로 얻은 잔차] ~ [X_{2}~X_{1}]$에서의 계수와 동일하다.</p></li></ul><h5 id="Y와-X-2-각각으로부터-X-1-의-선형효과를-제거한-후-Y에-미치는-X-2-의-효과를-나타내기-때문이다"><a href="#Y와-X-2-각각으로부터-X-1-의-선형효과를-제거한-후-Y에-미치는-X-2-의-효과를-나타내기-때문이다" class="headerlink" title="Y와 $X_{2}$ 각각으로부터 $X_{1}$의 선형효과를 제거한 후 Y에 미치는 $X_{2}$의 효과를 나타내기 때문이다."></a>Y와 $X_{2}$ 각각으로부터 $X_{1}$의 선형효과를 제거한 후 Y에 미치는 $X_{2}$의 효과를 나타내기 때문이다.</h5><h3 id="다중-선형-회귀-계수-검정"><a href="#다중-선형-회귀-계수-검정" class="headerlink" title="다중 선형 회귀 계수 검정"></a>다중 선형 회귀 계수 검정</h3><p><img src="/image/Multiple_linear_regression_coefficient_check.png" alt="다중 선형 회귀 계수 검정 - 01"></p><ul><li>단순 선형 회귀와 동일하게 각각의 회귀계수가 통계적으로 유의미한지를 검정하는 것은 동일하다. 하지만, 다중 선형 회귀는 다음과 같이 전체 회귀계수가 의미있는지에 대한 검정도 하게 된다. <code>여기서 회귀계수가 0에 가깝고 standard error를 더하고 뺀 범위내에 0이 포함된다면 그 변수 또한 유의미하더라도 제거해야할 것이다.</code> 또한 여기서 <code>잔차의 정규성이라는 가정을 만족할 때 t 통계량의 절대값이 크거나 대응되는 p-value가 더 작다면 독립(설명)변수와 종속(반응)변수 사이의 선형관계가 더 강함을 의미</code>한다.</li></ul><p><img src="/image/Multiple_linear_regression_coefficient_check_01.png" alt="다중 선형 회귀 계수 검정 - 02"></p><ul><li><p>위에서 언급하고 있는 개별적인 회귀계수 $\beta$에 대한 검정 이외에, 여러 가지 다른 형태의 가설들이 선형모형의 분석과 관련하여 고려될 수 있다. 통상적으로 고려될 수 있는 가설들은 다음과 같다.</p><ul><li><p>1) 독립 변수의 모든 회귀 계수들이 0이다.</p></li><li><p>2) 독립 변수의 회귀 계수들 중 일부분이 0이다.</p></li><li><p>3) 회귀계수들 중 일부분이 서로 같은 값을 가진다.</p></li><li><p>4) 회귀모수들이 특정한 제약조건을 만족한다.</p></li></ul></li><li><p>이런 가설들은 하나의 통합된 접근방법을 통해 동일한 방식으로 검정될 수 있다. 먼저, 모든 독립변수를 포함한 모형을 완전모형(FM)이라고 하자. 그리고 귀무가설에 가정된 내용들을 완전모형에 대입해서 얻은 모형을 축소모형(RM)이라고 하자. 완전모형의 변수들이 상대적으로 축소모형에 비해 많으므로 SSR값이 커져 잔차제곱합(SSE)을 감소시킬 것이므로 $SSE(RM) \geq SSE(FM)$이 된다. 따라서 차이 $SSE(RM) - SSE(FM)$은 축소모형을 적합함으로써 증가하는 잔차제곱합(SSE)을 의미한다. <code>만약 이 차이가 크다면 축소모형은 적절하지 않다.</code></p></li></ul><script type="math/tex; mode=display">F = \frac{[SSE(RM) - SSE(FM) ]/(p + 1 - k)}{SSE(FM)/(n-p-1)}</script><ul><li><p>위의 식을 통해 다 위에서 언급했던 가설들을 모두 검정할 수 있다.</p><ul><li><p>가설 1)독립 변수의 모든 회귀 계수들이 0이다.의 귀무가설은 $H_{0}:\beta_{1} = \beta_{2} = \cdots = \beta_{p} = 0$이며 대립가설은 $H_{1}: 최소한 하나의 계수는 0이 아니다. $이다. 이 가설은 축소모형의 변수는 1개이므로 해당 모형을 fitting한 후에 나오는 분산분석표에서의 F 통계량 값을 보고 검정 할 수 있다.</p></li><li><p>가설 2)독립 변수의 회귀 계수들 중 일부분이 0이다.의 귀무가설은 $H_{0}:\beta_{1} =  \beta_{3} = \beta_{5} = 0 $ 이고, 대립가설은 $H_{1}:\beta_{1}, \beta_{3}, \beta_{5} 중 최소한 하나는 0이 아니다. $ 이다. 이 가설은 위에서 F 통계량을 구하는 방식에 변형을 주어 생각해보면 $R^{2}$값을 통해 구할 수 있음을 알 수 있다.</p></li></ul></li></ul><script type="math/tex; mode=display">F = \frac{({R_{p}}^{2}-{R_{q}}^{2})/(p-q)}{(1 - {R_{p}}^{2})/(n-p-q)}, df=(p-q,n-p-1)</script><p><img src="/image/Multiple_linear_regression_coefficient_check_02.png" alt="다중 선형 회귀 계수 검정 - 03"></p><ul><li>여기서 주목할 점은 <code>SST는 정해져 있어 고정되어 있는데, SSR은 변수를 추가할수록 점점 더 커지므로 귀무가설을 기각하기 더 쉬워진다는 것이며, 우리가 추후에 말할</code>$R^{2}$<code>값도 변수를 추가할수록 높아지므로 이값으로 모형의 성능을 평가할때 무조건 이값이 높다고 좋은 모형이라고 생각하지 않아야 한다.</code> 또한 수정결정계수(adjusted R-squared) ${R_{adj}}^{2}$도 적합도를 평가하기 위해 사용될 수 있다. ${R_{adj}}^{2}$은 모형안에 있는 독립(설명)변수들의 수가 다르다는 것을 조정하므로 F값과 같이 서로 다른 모형들(포함된 독립변수가 다르거나 갯수가 다른)을 비교하기 위해 사용된다. <code>이 값은 결정계수 값과 다르게 Y의 전체 변이 중에서 독립변수들에 의하여 설명되는 비율로 해석 될 수 없다!</code></li></ul><script type="math/tex; mode=display">{R_{adj}}^{2} = 1 - \frac{SSE/(n-p-1)}{SST/(n-1)}</script><h3 id="원점을-통과하는-회귀선"><a href="#원점을-통과하는-회귀선" class="headerlink" title="원점을 통과하는 회귀선"></a>원점을 통과하는 회귀선</h3><ul><li><p>일반적으로 고려되는 단순선형회귀모형은 $ Y = \beta_{0} + \beta_{1}X + \epsilon $과 같이 절편항을 가지고 있다. 그러나 원점을 통과하는 다음과 같은 모형 $ Y = \beta_{1}X + \epsilon $ 에 데이터를 적합시킬 필요가 있을 때도 있다.</p></li><li><p>이 모형은 절편항이 없는 모형으로 불린다. <code>문제의 성격이나 외적 상황에 의해 회귀선이 원점을 지나야만하는 경우가 있다.</code> 예를 들어, 시간(X)의 함수로서 여행 거리(Y)는 상수항을 가지지 않아야 한다. 이때는 <code>SSE의 자유도가 확률 변수인 절편항이 하나 빠지므로 N-p(전체 확률변수가 설명변수p개 절편항 1개 이었던 N-p-1에서)로 바뀌게 된다.</code> 또한 이때는 우리가 알고 있는 SST=SSR+SSE라는 공식이 더 이상 성립되지 않는다. 그러므로 $R^{2}$와 같은 절편항을 갖는 모형에 대한 몇몇 성능 평가 지표들은 절편항이 없는 모형에 대해서는 더 이상 적절하지 않다. <code>절편항이 없는 모형에 대한 적절한 항등식은 y의 평균을 0으로 대체함으로써 얻어진다.</code></p></li></ul><script type="math/tex; mode=display">\sum_{i = 1}^{n} y_{i}^{2} = \sum_{i = 1}^{n} \hat{y_{i}^{2}} + \sum_{i = 1}^{n} e_{i}^{2}</script><ul><li>그러므로 $R^{2}$ 또한 재정의 된다.</li></ul><script type="math/tex; mode=display">R^{2} = \frac{\sum \hat{ y_{i}^{2} }}{\sum {y_{i}^{2}} = 1 - \frac{\sum e_{i}^{2}}{\sum y_{i}^{2}}</script><ul><li><p><code>절편항을 가진 모형의 경우</code> $R^{2}$가 Y를 그의 평균으로 조정한 후에 Y의 전체 변동성 중에서 독립(설명)변수 X에 의하여 설명되는 비율로 해석될 수 있다. <code>절편항이 없는 모형의 경우</code>에는 Y에 대한 조정이 없다.</p></li><li><p><code>이 처럼 절편항이 없는 모형은 풀고자하는 문제와 관련된 이론 혹은 물리적 상황에 부합되는 경우에만 사용되어야만 한다!</code> 그러나 몇몇 응용에서는 어떤 모형을 사용해야 할지가 분명하지 않을 수 있다. 이러한 경우</p><ul><li>1) 관측값과 예측값의 가까운 정도를 측정하는 것이 잔차제곱평균이므로 두 모형에 의해 산출되는 잔차평균제곱(SSE를 각각의 모형에 대한자유도로 나눈 값)을 비교하여 평가한다.</li><li>2) 데이터 모형을 적합하고 절편항의 유의성을 검정하여(t통계량을 바탕으로) 검정이 유의하다면 절편항을 가진 모형을 사용하고 그렇지 않으면 절편항이 없는 모형을 사용한다. <code>그러나, 일반적으로 회귀모형에서는 상수항이 통계적으로 유의하지 않더라도, 강한 이론적 근거가 존재하지 않는다면, 상수항은 모형에 포함되어야 한다. 특히 분석에 사용되는 데이터가 원점을 포함하지 않는 경우 더욱 강조되는데 그 이유는 상수항이 종속(반응)변수의 기본적인 수준(평균)을을 나타내기 때문</code>이다.</li></ul></li><li><p>참고로 필자는 ANCOVA 분석 즉, 회귀식에서 설명변수들 중 질적인 변수(혹은 더미변수)가 포함되어 있어 그런 질적인 변수와 더미변수가 절편항(상수항)을 대신해줄 것이라고 착각하여 상수항을 생성하지 않고, 모형을 적합시켰던 경험이 있다. 프로젝트였는데 멘토분께서 왜 절편항을 포함하지 않았냐고 물어보보셨는데 위와 같은 답변을 했었는데 잘못된 접근법이라고 조언을 해주셨었다. 그 당시에는 이해가 가지 않았지만 이제는 나의 접근법이 말이 안된다는 것부터 깨달았다. 왜냐하면 SGD 방법으로 확률변수인 절편항과 계수항들을 업데이트해 나가는 방식으로 회귀모형을 짜는데 필자는 이미 상수항 취급을 하는 질적변수나 더미변수 자체를 절편항이라고 생각했으니 말 자체가 안되는 것이다.</p></li></ul><h3 id="모형에서-중심화-centering-와-척도화-scaling"><a href="#모형에서-중심화-centering-와-척도화-scaling" class="headerlink" title="모형에서 중심화(centering)와 척도화(scaling)"></a>모형에서 중심화(centering)와 척도화(scaling)</h3><ul><li><p>회귀분석에서는 <code>회귀계수의 크기가 변수의 측정 단위에 영향을 받게 되므로 중심화와 척도화를 해야한다.</code>예를 들어 달러 단위로 측정된 소득의 회귀계수가 5.123이라면, 소득이 1,000달러 단위로 측정 되었을때는 5123으로 바뀌게 된다. <code>절편항(상수항)이 있는 모형을 다룰 때는 변수에 대한 중심화와 척도화가 필요하지만, 절편이 없는 모형을 다룰 때는 변수의 척도화만 필요하다.</code> 또한 이는 다른 선형성을 가정하는 모델(RBF kernel을 사용하는 SVM, logistic regression)에선 피처를 정규성을 띄게 해주어야하는 모형 뿐만아니라 피처 scaling을 하여 과적합을 방지하는 방법이므로 알고있어야한다.</p></li><li><p><code>중심화(centering)</code> 변수는 <code>각 관측값에서 모든 관측값의 평균을 빼는 것</code>으로 얻어진다. 중심화된 변수 <code>척도화</code> 또한 가능하다. <code>두 가지 형태의 척도화(scaling)가 통상적으로 가능한데, 단위 길이 척도화(unit length scaling or normalization)</code>와 <code>표준화(Standardization)</code>이다. <code>단위길이 척도화는 피처 벡터의 길이로 나누어주거나 min-max scaling 같은 것을 의미</code>한다. <code>표준화는 말 그대로 편차를 표준편차로 나누어 표준정규분포를 띄게끔해주는 작업</code>을 의미한다.</p></li></ul><h3 id="다중-공선성-Multi-collinearity"><a href="#다중-공선성-Multi-collinearity" class="headerlink" title="다중 공선성(Multi-collinearity)"></a>다중 공선성(Multi-collinearity)</h3><p><img src="/image/multicollinearity_00.png" alt="다중 공선성 - 01"></p><p><img src="/image/multicollinearity_01.png" alt="다중 공선성 - 02"></p><p><img src="/image/multicollinearity_02.png" alt="다중 공선성 - 03"></p><ul><li><p>Ordinary Least Squares(OLS) 즉 최소 제곱법 기반의 회귀 계수 계산은 독립 변수(입력 피처)의 독립성에 많은 영향을 받는다. <code>피처간의 상관관계가 매우 높은 경우 분산이 매우 커져서 오류에 매우 민감해지며 선형대수의 관점에서 보면, 모든 컬럼들이 linearly independent해야 최소한 하나 이상의 해가 존재하기 때문</code>이다. 만약 위의 말이 이해가 가지 않는다면, 필자가 추전하는 선형대수학 강의를 듣는 것을 권한다. 다음 페이지를 가면 찾을 수 있다. <a href="https://heung-bae-lee.github.io/2019/12/14/linear_algebra_00/">선형대수학 강의 추천</a></p></li><li><p>위와 같은 다중 공선성 문제가 있을 경우, <code>일반적으로 상관관계가 높은 독립 변수(입력 피처)가 많은 경우 독립적인 중요한 독립 변수(입력 변수)만을 남기고 제거하거나 규제를 적용</code>한다. 또한 <code>매우 많은 피처가 다중 공선성 문제를 가지고 있다면 PCA를 통해 차원 축소를 수행하는 것도 고려해 볼 수 있다.</code></p></li></ul><h3 id="다중-공선성-검사하는-방법들"><a href="#다중-공선성-검사하는-방법들" class="headerlink" title="다중 공선성 검사하는 방법들"></a>다중 공선성 검사하는 방법들</h3><p><img src="/image/vif.png" alt="VIF"></p><ul><li>위에서 VIF가 10이상인 경우 다중공선성이 있는 변수라고 판단할 수 있다고 했는데, 그렇다면 다중공선성이 있다고 판단되는 변수를 무조건적으로 제거해야 하나라는 의문이 들 수 있을 것이다. 그에 대한 답은 <code>무조건적으로 제거하면 안된다라는 것</code>이다. <code>VIF가 높더라도 통계적으로 유의미한 변수(p-value가 유의수준 보다 낮은 변수)라면 제거하지 않는 것이 적절하다. 추가적으로 다른 변수들 중에 VIF가 높고, 유의미하지 않은 변수가 있다면 그 변수들을 제거해 본 뒤 VIF를 계산해 보아야 할 것이다. 이 과정을 거쳐서도 아마도 VIF는 높은 수치이겠지만, 제거를 해서는 안된다.</code></li></ul><p><img src="/image/correlation_matrix.png" alt="상관 행렬"></p><ul><li><p>다중 공선성을 검사하는 방법에는 VIF외에도 상관계수 행렬을 구해서 위와 같이 산점도와 같이 그려서 보아야한다. 상관계수는 공분산을 각각의 표준편차로 나누어준 수치인데, 공분산은 예를 들어 두 변수 X와 Y가 있다면, Y와 X 사이의 선형 관계에 대한 방향을 나타낸다. <code>Cov(X, Y)는 측정단위의 변화에 영향을 받기 때문에 우리에게 관계의 강도가 얼마나 되는 지를 알려주지는 않고 방향만을 알려준다.</code> 이러한 문제를 해결하기 위해 <code>표준편차로 나누어 Standardization을 해주어 단위에 대한 영향을 없애준 것</code>이 <code>상관계수</code>이다. 참고로 여기서 <code>Corr(X, Y)=0 가 반드시 Y와 X 사이에 관계가 없음을 의미하는 것이 아님을 주의하자! 상관계수는 오직 선형 관계를 측정하기에 선형적으로 관계가 없음을 의미한다. 즉, X와 Y가 비선형적으로 관련되어 있을 때에도 Corr(X, Y)가 0이 될 수 있다.</code><br>또한 <code>상관계수도 평균과 분산과 마찬가지로 극단값에 민감하다. 그러므로 이러한 요약 통계량에만 의존하는 분석으로는 전체적인 패턴을 보는데에 있어서 차이를 발견할 수 없게 할 것이다. 따라서 필자는 개인적으로 데이터 EDA 과정에서 독립(설명)변수들과 반응 변수 사이의 관계를 꼭 산점도로 그려 확인한 뒤, 상관계수와 의미가 일치하는지 확인해보는 작업이 필수라고 여긴다.</code></p></li><li><p>통계를 공부하는 Beginner들이 많이들 오해할 만한 사실은 <code>Corr(X, Y)는</code> 한 변수의 값이 주어졌을 때 다른 변수의 값을 예측하기 위해서 사용할 수 없다. <code>단지 대응(pairwise)관계만 측정</code>한다. <code>예측을 하고 관계를 설명하기 위해서 우리는 회귀분석을 하는 것!!!!</code></p></li></ul><p><img src="/image/multicollinearity_03.png" alt="다중 공선성을 완전히 해소하는 방법은 없다!">  </p>]]></content:encoded>
      
      <comments>https://heung-bae-lee.github.io/2020/01/08/machine_learning_02/#disqus_thread</comments>
    </item>
    
  </channel>
</rss>
