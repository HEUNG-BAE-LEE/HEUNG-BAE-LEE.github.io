<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>Imbalanced Data | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="Imbalanced Data 실제로 도메인에서 적용될 때 클래스가 Imbalance한 데이터들이 많을 것이다. 아래와 같이 불균형인 데이터를 그냥 학습시키면 다수의 클래스를 갖는 데이터를 많이 학습하게 되므로 소수 클래스에 대해서는 잘 분류해내지 못한다.   데이터 클래스 비율이 너무 차이가 나면(highly-Imbalanced data) 단순히 우세한 클">
<meta property="og:type" content="article">
<meta property="og:title" content="Imbalanced Data">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/06/06/machine_learning_20/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="Imbalanced Data 실제로 도메인에서 적용될 때 클래스가 Imbalance한 데이터들이 많을 것이다. 아래와 같이 불균형인 데이터를 그냥 학습시키면 다수의 클래스를 갖는 데이터를 많이 학습하게 되므로 소수 클래스에 대해서는 잘 분류해내지 못한다.   데이터 클래스 비율이 너무 차이가 나면(highly-Imbalanced data) 단순히 우세한 클">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/what_is_imbalanced_data_problem.png">
<meta property="og:updated_time" content="2020-06-06T17:42:02.192Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Imbalanced Data">
<meta name="twitter:description" content="Imbalanced Data 실제로 도메인에서 적용될 때 클래스가 Imbalance한 데이터들이 많을 것이다. 아래와 같이 불균형인 데이터를 그냥 학습시키면 다수의 클래스를 갖는 데이터를 많이 학습하게 되므로 소수 클래스에 대해서는 잘 분류해내지 못한다.   데이터 클래스 비율이 너무 차이가 나면(highly-Imbalanced data) 단순히 우세한 클">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/what_is_imbalanced_data_problem.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/06/06/machine_learning_20/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">
    <link rel="stylesheet" type="text/css" href>
    <link rel="stylesheet" href="https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css">	
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothiccoding.css">
    <link rel="stylesheet" href="/css/style.css">
   
    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/machine-learning/">machine learning</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-machine_learning_20" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        Imbalanced Data
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/06/06/machine_learning_20/" class="article-date">
            <time datetime="2020-06-05T16:52:20.000Z" itemprop="datePublished">2020-06-06</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h2 id="Imbalanced-Data"><a href="#Imbalanced-Data" class="headerlink" title="Imbalanced Data"></a>Imbalanced Data</h2><ul>
<li>실제로 도메인에서 적용될 때 클래스가 Imbalance한 데이터들이 많을 것이다. 아래와 같이 불균형인 데이터를 그냥 학습시키면 다수의 클래스를 갖는 데이터를 많이 학습하게 되므로 소수 클래스에 대해서는 잘 분류해내지 못한다.</li>
</ul>
<ul>
<li>데이터 클래스 비율이 너무 차이가 나면(highly-Imbalanced data) 단순히 우세한 클래스를 택하는 모형의 정확도가 높아지므로 모형의 성능판별이 어려워진다. 즉, 정확도(Accuracy)가 높아도 데이터 개수가 적은 클래스의 재현율(recall-rate)이 급격히 작아지는 현상이 발생할 수 있다. 이렇게 <code>각 클래스에 속한 데이터의 개수의 차이에 의해 발생하는 문제</code>들을 <code>비대칭 데이터 문제</code>(Imbalanced data problem)이라고 한다.</li>
</ul>
<p><img src="/image/what_is_imbalanced_data_problem.png" alt="데이터 불균형 문제 - 01"></p>
<ul>
<li>아래 코드와 그림은 SVM을 사용하여 각각 다변량(아래는 이변량) 정규분포를 갖는 비대칭 데이터와 대칭 데이터를 분류한 결과를 비교하는 코드이다. 우선 label마다 확실하게 구분되어질 수 있도록 서로 다른 평균을 갖는 이변량 정규분포에 샘플링하여 사용한다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import *</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.metrics import recall_score</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def classification_result(n0, n1, title=<span class="string">""</span>):</span><br><span class="line">    rv1 = sp.stats.multivariate_normal([-1, 0], [[1, 0], [0, 1]])</span><br><span class="line">    rv2 = sp.stats.multivariate_normal([+1, 0], [[1, 0], [0, 1]])</span><br><span class="line">    X0 = rv1.rvs(n0, random_state=0)</span><br><span class="line">    X1 = rv2.rvs(n1, random_state=0)</span><br><span class="line">    X = np.vstack([X0, X1])</span><br><span class="line">    y = np.hstack([np.zeros(n0), np.ones(n1)])</span><br><span class="line"></span><br><span class="line">    x1min = -4; x1max = 4</span><br><span class="line">    x2min = -2; x2max = 2</span><br><span class="line">    xx1 = np.linspace(x1min, x1max, 1000)</span><br><span class="line">    xx2 = np.linspace(x2min, x2max, 1000)</span><br><span class="line">    X1, X2 = np.meshgrid(xx1, xx2)</span><br><span class="line"></span><br><span class="line">    plt.contour(X1, X2, rv1.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=<span class="string">"dashed"</span>)</span><br><span class="line">    plt.contour(X1, X2, rv2.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=<span class="string">"dashed"</span>)</span><br><span class="line"></span><br><span class="line">    model = SVC(kernel=<span class="string">"linear"</span>, C=1e4, random_state=0).fit(X, y)</span><br><span class="line">    Y = np.reshape(model.predict(np.array([X1.ravel(), X2.ravel()]).T), X1.shape)</span><br><span class="line">    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker=<span class="string">'x'</span>, label=<span class="string">"0 클래스"</span>)</span><br><span class="line">    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker=<span class="string">'o'</span>, label=<span class="string">"1 클래스"</span>)</span><br><span class="line">    plt.contour(X1, X2, Y, colors=<span class="string">'k'</span>, levels=[0.5])</span><br><span class="line">    y_pred = model.predict(X)</span><br><span class="line">    plt.xlim(-4, 4)</span><br><span class="line">    plt.ylim(-3, 3)</span><br><span class="line">    plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> model, X, y, y_pred</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))    </span><br><span class="line">plt.subplot(121)</span><br><span class="line">model1, X1, y1, y_pred1 = classification_result(200, 200, <span class="string">"대칭 데이터 (5:5)"</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model2, X2, y2, y_pred2 = classification_result(200, 20, <span class="string">"비대칭 데이터 (9:1)"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.savefig(<span class="string">'Imbalanced_data_svc_example'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/Imbalanced_data_svc_example.png" alt="SVC를 이용한 대칭 데이터와 비대칭 데이터의 분류 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(classification_report(y1, y_pred1))</span><br><span class="line"><span class="built_in">print</span>(classification_report(y2, y_pred2))</span><br></pre></td></tr></table></figure>
<h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.86      0.83      0.84       200</span><br><span class="line">         1.0       0.84      0.86      0.85       200</span><br><span class="line"></span><br><span class="line">    accuracy                           0.85       400</span><br><span class="line">   macro avg       0.85      0.85      0.85       400</span><br><span class="line">weighted avg       0.85      0.85      0.85       400</span><br><span class="line"></span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.96      0.98      0.97       200</span><br><span class="line">         1.0       0.75      0.60      0.67        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.95       220</span><br><span class="line">   macro avg       0.86      0.79      0.82       220</span><br><span class="line">weighted avg       0.94      0.95      0.94       220</span><br></pre></td></tr></table></figure>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import roc_curve, confusion_matrix</span><br><span class="line"></span><br><span class="line">fpr1, tpr1, thresholds1 = roc_curve(y1, model1.decision_function(X1))</span><br><span class="line">fpr2, tpr2, thresholds2 = roc_curve(y2, model2.decision_function(X2))</span><br><span class="line"></span><br><span class="line">c1 = confusion_matrix(y1, y_pred1, labels=[1, 0])</span><br><span class="line">c2 = confusion_matrix(y2, y_pred2, labels=[1, 0])</span><br><span class="line">r1 = c1[0, 0] / (c1[0, 0] + c1[0, 1])</span><br><span class="line">r2 = c2[0, 0] / (c2[0, 0] + c2[0, 1])</span><br><span class="line">f1 = c1[1, 0] / (c1[1, 0] + c1[1, 1])</span><br><span class="line">f2 = c2[1, 0] / (c2[1, 0] + c2[1, 1])</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12, 8))</span><br><span class="line">plt.plot(fpr1, tpr1, <span class="string">':'</span>, label=<span class="string">"대칭"</span>)</span><br><span class="line">plt.plot(fpr2, tpr2, <span class="string">'-'</span>, label=<span class="string">"비대칭"</span>)</span><br><span class="line">plt.plot([f1], [r1], <span class="string">'ro'</span>)</span><br><span class="line">plt.plot([f2], [r2], <span class="string">'ro'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'Fall-Out'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.title(<span class="string">'ROC 커브'</span>)</span><br><span class="line"><span class="comment"># plt.savefig("roc_curve_diiferent_result_between_balanced_and_Imbalanced")</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>주황색 포인트가 현재 각 모델들의 성능을 의미한다. 대칭 데이터보다 비대칭 데이터를 사용하였을 경우 훨씬 좋지 않다.</li>
</ul>
<p><img src="/image/roc_curve_diiferent_result_between_balanced_and_Imbalanced.png" alt="SVC의 결과의 위치 및 ROC 커브"></p>
<ul>
<li>모형이 학습을 하면서 위에서 언급한 것 과 같이 소수의 데이터를 잘 학습하지 못하여 소수의 데이터를 잘 분류해 내지 못하는 모습을 아래의 그림에서도 확인할 수 있다. 빨간색 원형의 부분에도 소수 클래스의 데이터라 존재하지만 주변 다수 클래스가 많이 분포되어있어 분류해 내지 못했다.</li>
</ul>
<p><img src="/image/data_Imbalanced_examples_01.png" alt="데이터 불균형 문제의 예시 - 01"></p>
<ul>
<li>아래 그림은 oversampling을 통해 소수 클래스의 데이터의 비중을 늘려 주어 이전보다 학습을 많이 할 수 있도록 하는 방법을 택했을 경우의 학습 결과를 보여준다. 확실히, 이전에 decision boundary가 생성되어야 할 부분에 생성되어 진 것을 확인 할 수 있다. 또한, <code>이러한 방법은 모형내에 weight 파라미터가 존재할 경우 해당 소수 클래스에 더 많은 가중치를 줌으로써 동일한 결과를 도출해 낼 수 도 있을 것</code>이다.</li>
</ul>
<p><img src="/image/data_Imbalanced_examples_02.png" alt="데이터 불균형 문제의 예시 - 02"></p>
<ul>
<li><code>소수 클래스의 데이터를 충분히 학습하지 못하는 문제뿐만 아니라 다수 클래스의 데이터에 대한 예측 확률 값은 높을 수록 1로 예측하기 때문에 0에 가까이 수렴할 수 밖에 없다. 그러므로 본래의 threshold인 0.5를 낮춰 설정하여야 좀 더 예측의 성능을 높일 수 있을 것</code>이다. <code>threshold는 validation set을 통해 설정</code>해야 할 것이다.</li>
</ul>
<p><img src="/image/what_is_imbalanced_data_problem_02.png" alt="데이터 불균형 문제 - 02"></p>
<ul>
<li>또한, 성능지표를 설정하는 것에도 문제가 있다. 기존의 Accuracy만을 생각한다면 다수의 클래스를 잘 예측해내기만 한다면 성능이 높을 수 밖에 없다. 그러므로 <code>소수클래스의 비중이 적은 점을 고려하면서 성능을 비교할 수 있는 지표를 설정</code>해야 할 것이다. 그런 측면에서의 성능지표는 아래와 같이 2 가지를 많이 사용한다.</li>
</ul>
<p><img src="/image/considered_imbalanced_data_problem_metrics_01.png" alt="불균형 문제를 갖는 데이터에 대한 성능지표 - 01"></p>
<ul>
<li><code>실제 양성이라고 판단한 것 중 양성이라고 예측한 비율을 의미</code>하는 <code>recall(재현율)</code>과 <code>양성이라고 예측한 것 중 실제 양성인 비율을 의미</code>하는 <code>precision(정밀도)</code>의 조화평균이 F1-Score라고 할 수 있다. 두 지표를 모두 고려하는 지표인 것이다. 두 지표는 서로 트레이드 오프 관계를 갖고 있다. 3가지 성능 지표(recall, precision, f1-score) 모두를 구하여 비교해보는 것이 좋다.</li>
</ul>
<p><img src="/image/considered_imbalanced_data_problem_metrics_02.png" alt="불균형 문제를 갖는 데이터에 대한 성능지표 - 02"></p>
<ul>
<li>threshold가 변동되야 하므로 ROC curve를 그려보거나 AUC를 통한 지표를 설정하는 것도 좋은 방법이다.</li>
</ul>
<p><img src="/image/considered_imbalanced_data_problem_metrics_03.png" alt="불균형 문제를 갖는 데이터에 대한 성능지표 - 03"></p>
<h3 id="해결방법"><a href="#해결방법" class="headerlink" title="해결방법"></a>해결방법</h3><ul>
<li>위에서 잠깐 언급했듯이 Imbalanced data를 해결하기 위한 방법은 크게 2 가지로 소개 할 수 있다. 첫번째 방법은 리샘플링 방법으로 소수의 데이터를 부풀리는 <code>Over sampling</code> 과 다수의 데이터에서 일부만 사용하는 <code>Under sampling</code>, 그리고 마지막으로 두 가지 방법을 섞어 사용하는 <code>Hybrid sampling</code>이 있다. 두 번째 방법은 모형 자체의 학습하는 가중치를 소수 클래스에 더 주어 학습시키는 방법이다.</li>
</ul>
<p><img src="/image/how_to_overcome_Imbalanced_data_problem_with_resampling_01.png" alt="불균형 문제를 해결하기 위한 리샘플링 방법들"></p>
<ul>
<li>리샘플링 시 유의할 점은 <code>먼저 데이터세트의 클래스별 비율을 유지한 채 train, validation, test 세트로 나누어야 한다는 점</code>이다. 그리고 나서 <code>학습시킬 데이터에 대해서만 resampling 방법을 적용</code>시킨다는 점이다. 직관적으로 생각해 보면 위의 단계는 당연하지만, 초보자의 입장에선 헷갈릴수 있는 부분이기 때문에 언급하고 넘어가겠다.</li>
</ul>
<p><img src="/image/how_to_overcome_Imbalanced_data_problem_with_resampling_01.png" alt="불균형 문제를 해결하기 위한 리샘플링 방법들"></p>
<p><img src="/image/oversampling_example.png" alt="오버 샘플링을 통한 불균형 문제 해결 예시"></p>
<h4 id="Imbalanced-learn-패키지"><a href="#Imbalanced-learn-패키지" class="headerlink" title="Imbalanced-learn 패키지"></a>Imbalanced-learn 패키지</h4><ul>
<li>Imbalanced data 문제를 해결하기 위한 다양한 샘플링 방법을 구현한 Python 패키지</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U imbalanced-learn</span><br></pre></td></tr></table></figure>
<h3 id="Under-sampling"><a href="#Under-sampling" class="headerlink" title="Under sampling"></a>Under sampling</h3><ul>
<li><code>RandomUnderSampler</code> : random under-sampling method</li>
<li><code>TomekLinks</code> : Tomek’s link method</li>
<li><code>CondensedNearestNeighbour</code> : condensed nearest neighbor method</li>
<li><code>OneSidedSelection</code> : under-sampling based on one-sided selection method</li>
<li><code>EditedNearestNeighbours</code> : edited nearest neighbor method</li>
<li><code>NeighbourhoodCLeaningRule</code> : neighborhood cleaning rule</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">from imblearn.under_sampling import *</span><br><span class="line"></span><br><span class="line">n0 = 200; n1 = 20</span><br><span class="line">rv1 = sp.stats.multivariate_normal([-1, 0], [[1, 0], [0, 1]])</span><br><span class="line">rv2 = sp.stats.multivariate_normal([+1, 0], [[1, 0], [0, 1]])</span><br><span class="line">X0 = rv1.rvs(n0, random_state=0)</span><br><span class="line">X1 = rv2.rvs(n1, random_state=0)</span><br><span class="line">X_imb = np.vstack([X0, X1])</span><br><span class="line">y_imb = np.hstack([np.zeros(n0), np.ones(n1)])</span><br><span class="line"></span><br><span class="line">x1min = -4; x1max = 4</span><br><span class="line">x2min = -2; x2max = 2</span><br><span class="line">xx1 = np.linspace(x1min, x1max, 1000)</span><br><span class="line">xx2 = np.linspace(x2min, x2max, 1000)</span><br><span class="line">X1, X2 = np.meshgrid(xx1, xx2)</span><br><span class="line"></span><br><span class="line">def classification_result2(X, y, title=<span class="string">""</span>):</span><br><span class="line">    plt.contour(X1, X2, rv1.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=<span class="string">"dashed"</span>)</span><br><span class="line">    plt.contour(X1, X2, rv2.pdf(np.dstack([X1, X2])), levels=[0.05], linestyles=<span class="string">"dashed"</span>)</span><br><span class="line">    model = SVC(kernel=<span class="string">"linear"</span>, C=1e4, random_state=0).fit(X, y)</span><br><span class="line">    Y = np.reshape(model.predict(np.array([X1.ravel(), X2.ravel()]).T), X1.shape)</span><br><span class="line">    plt.scatter(X[y == 0, 0], X[y == 0, 1], marker=<span class="string">'x'</span>, label=<span class="string">"0 클래스"</span>)</span><br><span class="line">    plt.scatter(X[y == 1, 0], X[y == 1, 1], marker=<span class="string">'o'</span>, label=<span class="string">"1 클래스"</span>)</span><br><span class="line">    plt.contour(X1, X2, Y, colors=<span class="string">'k'</span>, levels=[0.5])</span><br><span class="line">    y_pred = model.predict(X)</span><br><span class="line">    plt.xlim(-4, 4)</span><br><span class="line">    plt.ylim(-3, 3)</span><br><span class="line">    plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    <span class="built_in">return</span> model</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="Random-Under-Sampler"><a href="#Random-Under-Sampler" class="headerlink" title="Random Under-Sampler"></a>Random Under-Sampler</h4><ul>
<li>무작위로 데이터를 없애는 단순 샘플링</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = RandomUnderSampler(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br></pre></td></tr></table></figure>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = RandomUnderSampler(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'RandomUnderSampler로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('RandomUnderSampler_result_resampling')</span></span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/RandomUnderSampler_result_resampling.png" alt="RandomUnderSampler로 리샘플링한 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.92      0.95       200</span><br><span class="line">         1.0       0.51      0.90      0.65        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.91       220</span><br><span class="line">   macro avg       0.75      0.91      0.80       220</span><br><span class="line">weighted avg       0.95      0.91      0.92       220</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="Tomek’s-link-method"><a href="#Tomek’s-link-method" class="headerlink" title="Tomek’s link method"></a>Tomek’s link method</h4><ul>
<li>토멕링크(Tomek’s link)란 서로 다른 클래스에 속하는 한 쌍의 데이터 $ (x_{+}, x_{-}) $ 로 서로에게 더 가까운 다른 데이터 존재하지 않는 상태이다. 클래스가 다른 두 데이터가 아주 가까이 붙어있으면 토멕링크가 된다. 토멕링크 방법은 이러한 토멕링크를 찾은 다음 그 중에서 다수 클래스에 속하는 데이터를 제외하는 방법으로 <code>경계선을 다수 클래스쪽으로 밀어붙이는 효과</code>가 있다.</li>
</ul>
<ul>
<li><code>소수쪽 경계선을 늘리자라는 방법이다.(단 없애지는 것들이 많은 수가 있지는 않다.) 반복가능!</code></li>
</ul>
<p><img src="/image/what_is_tomeklinks_under_sampling.png" alt="Tomek&#39;s link 방법"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = TomekLinks(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'Tomeks link method로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('TomekLinks_result_resampling')</span></span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/TomekLinks_result_resampling.png" alt="Tomek&#39;s link method로 리샘플링 한 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.97      0.97      0.97       200</span><br><span class="line">         1.0       0.70      0.70      0.70        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.95       220</span><br><span class="line">   macro avg       0.83      0.83      0.83       220</span><br><span class="line">weighted avg       0.95      0.95      0.95       220</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="Condensed-Nearest-Neighbor"><a href="#Condensed-Nearest-Neighbor" class="headerlink" title="Condensed Nearest Neighbor"></a>Condensed Nearest Neighbor</h4><ul>
<li>CNN(Condensed Nearest Neighbor) 방법은 1-NN 모형으로 분류되지 않는 데이터만 남기는 방법이다.<ul>
<li>원래는 clustering에서 사용하는 방법이지만 응용해서 사용한다.</li>
<li>랜덤하게 하나를 고른 후, 그 다음에는 1-NN방법을 사용한다. 그 다음 선택된 것과 만약 같은 클래스(다수 클래스)이면 안 뽑고 다른 클래스(소수 클래스)이면 뽑음.</li>
</ul>
</li>
</ul>
<ul>
<li><p>선택된 데이터 집합을 $ S $ 라고 하자.</p>
<ul>
<li>1) 소수 클래스 데이터를 모두 $ S $ 에 포함시킨다.</li>
<li>2) 다수 데이터 중에서 하나를 골라서 가장 가까운 데이터가 다수 클래스이면 포함시키지 않고 아니면 $ S $ 에 포함시킨다.</li>
<li>3) 더 이상 선택되는 데이터가 없을 때 까지 2를 반복한다.</li>
</ul>
</li>
</ul>
<ul>
<li><p>이 방법을 사용하면 기존에 선택된 데이터와 가까이 있으면서 같은 클래스인 데이터는 선택되지 않기 때문에 다수 데이터의 경우 선택되는 비율이 적어진다.</p>
<ul>
<li><p>허나, <code>가장 가까운 데이터가 소수 클래스인 경우 집합에 포함되겠지만 그만큼 아래 그림처럼 소수 클래스 집단과 거리가 가깝다면 두 클래스를 분류하는데 도움을 주진 못할 것</code>이다.</p>
</li>
<li><p>CNN 자체는 <code>경계선을 살리는 역할</code>을 하기 때문에 경계선 보다 멀거나 적은 클래스 주변에 없으면 제거된다. 그러므로 <code>홀로 이걸 사용한다기 보다는 다른 사용에 중간과정에 사용하기도한다.</code></p>
</li>
</ul>
</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = CondensedNearestNeighbour(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'CondensedNearestNeighbour로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('CondensedNearestNeighbour_result_resampling')</span></span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/CondensedNearestNeighbour_result_resampling.png" alt="CondensedNearestNeighbour로 리샘플링 한 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.96      0.98      0.97       200</span><br><span class="line">         1.0       0.75      0.60      0.67        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.95       220</span><br><span class="line">   macro avg       0.86      0.79      0.82       220</span><br><span class="line">weighted avg       0.94      0.95      0.94       220</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="One-Sided-Selection"><a href="#One-Sided-Selection" class="headerlink" title="One Sided Selection"></a>One Sided Selection</h4><ul>
<li>One Sided Selection은 토맥링크 방법과 Condensed Nearest Neighbour 방법을 섞은 것이다. 토맥링크 중 다수 클래스를 제외하고 나머지 데이터 중에서도 서로 붙어있는 다수 클래스 데이터는 1-NN 방법으로 제외한다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = OneSidedSelection(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'OneSidedSelection로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('OneSidedSelection_result_resampling')</span></span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/OneSidedSelection_result_resampling.png" alt="OneSidedSelection으로 리샘플링 한 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.97      0.97      0.97       200</span><br><span class="line">         1.0       0.70      0.70      0.70        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.95       220</span><br><span class="line">   macro avg       0.83      0.83      0.83       220</span><br><span class="line">weighted avg       0.95      0.95      0.95       220</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="Edited-Nearest-Neighbours"><a href="#Edited-Nearest-Neighbours" class="headerlink" title="Edited Nearest Neighbours"></a>Edited Nearest Neighbours</h4><ul>
<li>ENN(Edited Nearest Neighbours) 방법은 다수 클래스 데이터 중 가장 가까운 k(<code>n_neighbors</code>)개의 데이터가 모두(<code>kind_sel=&#39;all&#39;</code>) 또는 다수(<code>kind_sel=&#39;mode&#39;</code>) 다수 클래스가 아니면 삭제하는 방법이다. 소수 클래스 주변의 다수 클래스 데이터는 사라진다.</li>
</ul>
<ul>
<li>그러므로 <code>가까운 k개 중 소수 클래스를 지닌 데이터들은 모두 제거되어 소수 클래스와 다수 클래스 간의 구분이 상대적으로 명확</code>해지게 된다.</li>
</ul>
<ul>
<li>CNN과 비슷하지만, 모든 데이터에 대해서 주변에 제일 가까운 k개를 지정해줘서 주변에 다수 데이터가 많거나 또는 모두 다수 데이터가 아니면 그 데이터를 삭제하여 <code>경계선에 있는 애들 중 다수클래스가 사라져서 위에 말한 것과 동일한 효과</code>를 준다.</li>
</ul>
<ul>
<li><code>knn방법은 데이터간의 모든 거리를 구하기 때문에 데이터 갯수가 많으면 사용하기 힘들다.</code></li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = EditedNearestNeighbours(kind_sel=<span class="string">"all"</span>, n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'EditedNearestNeighbours로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('EditedNearestNeighbours_result_resampling')</span></span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/EditedNearestNeighbours_result_resampling.png" alt="EditedNearestNeighbours으로 리샘플링 한 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.94      0.96       200</span><br><span class="line">         1.0       0.58      0.90      0.71        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.93       220</span><br><span class="line">   macro avg       0.79      0.92      0.83       220</span><br><span class="line">weighted avg       0.95      0.93      0.94       220</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="Neighbourhood-Cleaning-Rule"><a href="#Neighbourhood-Cleaning-Rule" class="headerlink" title="Neighbourhood Cleaning Rule"></a>Neighbourhood Cleaning Rule</h4><ul>
<li>CNN(Condensed Nearest Neighbour) 방법과 ENN(Edited Nearest Neighbours) 방법을 섞은 것이다.</li>
</ul>
<ul>
<li>가장 가까운 데이터가 소수데이터가 아니거나, 가장 가까운 k개가 다수 클래스이거나 다수가 다수 클래스인 데이터를 제거하는 방법이므로 경계선을 너무 명확히 해주는 것을 방지한 효과를 줄 수 있다고 생각한다. ENN을 통해 소수 클래스 데이터 주변을 너무 많이 제거하는 것을 방지한다고 보면 될 것 같다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = NeighbourhoodCleaningRule(kind_sel=<span class="string">"all"</span>, n_neighbors=5, random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'NeighbourhoodCleaningRule로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('NeighbourhoodCleaningRule_result_resampling')</span></span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/NeighbourhoodCleaningRule_result_resampling.png" alt="NeighbourhoodCleaningRule으로 리샘플링 한 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.93      0.96       200</span><br><span class="line">         1.0       0.56      0.95      0.70        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.93       220</span><br><span class="line">   macro avg       0.78      0.94      0.83       220</span><br><span class="line">weighted avg       0.96      0.93      0.94       220</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Over-sampling"><a href="#Over-sampling" class="headerlink" title="Over sampling"></a>Over sampling</h3><ul>
<li><code>RandomOverSampler</code> : random sampler</li>
<li><code>ADASYN</code> : Adaptive Synthetic Sampling Approach for Imbalanced Learning</li>
<li><code>SMOTE</code> : Synthetic Minority Over-sampling Technique</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from imblearn.over_sampling import *</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="RandomOverSampler"><a href="#RandomOverSampler" class="headerlink" title="RandomOverSampler"></a>RandomOverSampler</h4><ul>
<li>Random Over Sampling은 소수 클래스의 데이터를 반복해서 넣는 것(replacement)이다. <code>가중치를 증가시키는 것과 비슷</code>하다.</li>
</ul>
<ul>
<li><code>오른쪽과 왼쪽이 변화가 없는 것 처럼 보이지만 오른쪽은 똑같은 데이터를 복제하는 것이기 때문에 숫자를 늘려 경계선을 밀어버린다.</code></li>
</ul>
<p><img src="/image/what_is_Random_over_sampling_conception.png" alt="Random Over Sampling이란"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = RandomOverSampler(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'RandomOverSampler로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('RandomOverSampler_result_resampling')</span></span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/RandomOverSampler_result_resampling.png" alt="RandomOverSampler로 리샘플링 한 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.91      0.95       200</span><br><span class="line">         1.0       0.51      0.95      0.67        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.91       220</span><br><span class="line">   macro avg       0.75      0.93      0.81       220</span><br><span class="line">weighted avg       0.95      0.91      0.92       220</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="ADASYN"><a href="#ADASYN" class="headerlink" title="ADASYN"></a>ADASYN</h4><ul>
<li><code>소수 데이터를 랜덤하게 두 포인트를 고른 후 직선으로 이어 그 선 사이의 랜덤한 위치에 데이터를 새로 생성</code>한다.</li>
</ul>
<ul>
<li>ADASYN(Adaptive Synthetic Sampling) 방법은 <code>소수 클래스 데이터와 그 데이터에서 가장 가까운 k개의 소수 클래스 데이터 중 무작위로 선택된 데이터 사이의 직선상에 가상의 소수 클래스 데이터를 만드는 방법</code>이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = ADASYN(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'ADASYN로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('ADASYN_result_resampling')</span></span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/ADASYN_result_resampling.png" alt="ADASYN으로 리샘플링 한 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.90      0.94       200</span><br><span class="line">         1.0       0.47      0.95      0.63        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.90       220</span><br><span class="line">   macro avg       0.73      0.92      0.79       220</span><br><span class="line">weighted avg       0.95      0.90      0.91       220</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="SMOTE"><a href="#SMOTE" class="headerlink" title="SMOTE"></a>SMOTE</h4><ul>
<li>SMOTE(Synthetic Minority Over-sampling Technique) 방법도 <code>ADASYN 방법처럼 데이터를 생성하지만 생성된 데이터를 무조건 소수 클래스라고 하지 않고 분류 모형에 따라 분류</code>한다. 따라서 <code>순수하게 소수 클래스만 sampling을 하지는 않는다.</code></li>
</ul>
<p><img src="/image/SMOTE_view.png" alt="SMOTE 방법"></p>
<ul>
<li>또한, 실행할 때 마다 랜덤하게 데이터가 생성되므로 결과는 매번 다르다.</li>
</ul>
<p><img src="/image/what_is_SMOTE_sampling_conception.png" alt="SMOTE 방법의 작동 원리"></p>
<ul>
<li><code>oversampling하는 대상이 전체 소수 클래스의 데이터이므로 noise로 판단되어 질 수 있는 밀집되어 나타나지 않는 소수 클래스의 데이터에 대해서도 모두 샘플링하므로 주의</code>하자.</li>
</ul>
<p><img src="/image/SMOTE_disadvantages_plot.png" alt="SMOTE를 할 경우 주의할 점"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = SMOTE(random_state=4).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'SMOTE로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('SMOTE_result_resampling')</span></span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/SMOTE_result_resampling.png" alt="SMOTE로 리샘플링 한 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">               precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.91      0.95       200</span><br><span class="line">         1.0       0.50      0.90      0.64        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.91       220</span><br><span class="line">   macro avg       0.74      0.91      0.80       220</span><br><span class="line">weighted avg       0.94      0.91      0.92       220</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="BLSMOTE"><a href="#BLSMOTE" class="headerlink" title="BLSMOTE"></a>BLSMOTE</h4><ul>
<li><code>Borderline에 있는 데이터는 class Imbalanced problem에 큰 영향을 미친다고 판단하여 해당 dataset에만 SMOTE를 적용하는 방법</code>이다. 애초에 Decision boundary에 영향을 미칠수 있는 데이터를 리샘플링하겠다는 의도이다.</li>
</ul>
<p><img src="/image/What_is_BLSMOTE_conception.png" alt="BLSMOTE의 개념 및 작동원리"></p>
<ul>
<li>noise같이 판단되어지는 데이터에 대해서도 oversampling을 하는 SMOTE의 단점을 보완하여 noise라는 카테고리로 분류해 새로 리샘플링하는 데이터에 소수 클래스 데이터로 인해 과적합되지 않도록 해준다.</li>
</ul>
<p><img src="/image/overcoming_SMOTE_with_BLSMOTE.png" alt="BLSMOTE의 장점"></p>
<h4 id="DBSMOTE-DBSCAN-SMOTE"><a href="#DBSMOTE-DBSCAN-SMOTE" class="headerlink" title="DBSMOTE(DBSCAN SMOTE)"></a>DBSMOTE(DBSCAN SMOTE)</h4><ul>
<li>DBSCAN cluster 생성 후, cluster 내에서 SMOTE를 적용하는 방법이다.</li>
</ul>
<p><img src="/image/what_is_DBSMOTE_conception.png" alt="DBSMOTE 개념 및 작동원리"></p>
<ul>
<li>DBSCAN Cluster 를 진행하기에 군집의 중심과 이어지는 경향이 있으며, 이 또한 <code>BLSMOTE와 비슷하게 원래 DBSCAN의 장점 중 하나인 noise를 제거하여 샘플링해준다.</code></li>
</ul>
<p><img src="/image/DBSMOTE_result_plot.png" alt="DBSMOTE의 결과"></p>
<ul>
<li>데이터 마다 편차가 크므로, 실험적으로 해본뒤 해당 데이터에 잘 맞는 샘플링 방법을 사용해야 한다.</li>
</ul>
<p><img src="/image/Oversampling_disadvantages.png" alt="Oversampling의 단점"></p>
<h3 id="복합-샘플링"><a href="#복합-샘플링" class="headerlink" title="복합 샘플링"></a>복합 샘플링</h3><ul>
<li><code>SMOTEENN</code> : SMOTE + ENN</li>
<li><code>SMOTETomek</code> : SMOTE + Tomek</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">from imblearn.combine import *</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="SMOTE-ENN"><a href="#SMOTE-ENN" class="headerlink" title="SMOTE + ENN"></a>SMOTE + ENN</h4><ul>
<li>SMOTE+ENN 방법은 SMOTE(Synthetic Minority Over-sampling Technique) 방법과 ENN(Edited Nearest Neighbours) 방법을 섞은 것이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = SMOTEENN(random_state=0).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'SMOTE +  ENN으로 리샘플링한 결과'</span>)</span><br><span class="line"><span class="comment"># plt.savefig('SMOTE_and_ENN_result_resampling')</span></span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/SMOTE_and_ENN_result_resampling.png" alt="SMOTE + ENN으로 리샘플링 한 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.92      0.96       200</span><br><span class="line">         1.0       0.54      0.95      0.69        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.92       220</span><br><span class="line">   macro avg       0.77      0.94      0.82       220</span><br><span class="line">weighted avg       0.95      0.92      0.93       220</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="SMOTE-Tomek"><a href="#SMOTE-Tomek" class="headerlink" title="SMOTE+Tomek"></a>SMOTE+Tomek</h4><ul>
<li>SMOTE+Tomek 방법은 SMOTE(Synthetic Minority Over-sampling Technique) 방법과 토멕링크 방법을 섞은 것이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_samp, y_samp = SMOTETomek(random_state=4).fit_sample(X_imb, y_imb)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">classification_result2(X_imb, y_imb)</span><br><span class="line">plt.title(<span class="string">'원본 데이터'</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">model_samp = classification_result2(X_samp, y_samp)</span><br><span class="line">plt.title(<span class="string">'SMOTE + Tomek으로 리샘플링한 결과'</span>)</span><br><span class="line">plt.savefig(<span class="string">'SMOTETomek_result_resampling'</span>)</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/SMOTETomek_result_resampling.png" alt="SMOTE + Tomek으로 리샘플링 한 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_imb, model_samp.predict(X_imb)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">         0.0       0.99      0.92      0.95       200</span><br><span class="line">         1.0       0.51      0.90      0.65        20</span><br><span class="line"></span><br><span class="line">    accuracy                           0.91       220</span><br><span class="line">   macro avg       0.75      0.91      0.80       220</span><br><span class="line">weighted avg       0.95      0.91      0.92       220</span><br></pre></td></tr></table></figure>
<hr>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "Imbalanced Data",
        "image": "https://heung-bae-lee.github.io/image/what_is_imbalanced_data_problem.png",
        "keywords": "",
        "genre": "machine learning",
        "datePublished": "2020-06-06",
        "dateCreated": "2020-06-06",
        "dateModified": "2020-06-07",
        "url": "https://heung-bae-lee.github.io/2020/06/06/machine_learning_20/",
        "description": "Imbalanced Data
실제로 도메인에서 적용될 때 클래스가 Imbalance한 데이터들이 많을 것이다. 아래와 같이 불균형인 데이터를 그냥 학습시키면 다수의 클래스를 갖는 데이터를 많이 학습하게 되므로 소수 클래스에 대해서는 잘 분류해내지 못한다.


데이터 클래스 비율이 너무 차이가 나면(highly-Imbalanced data) 단순히 우세한 클"
        "wordCount": 5510
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
    
        <a href="/2020/06/04/machine_learning_19/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">Clustering - Hierarchical, DBSCAN, Affinity Propagation</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/06/machine_learning_20/" class="title">Imbalanced Data</a></p>
                            <p class="item-date"><time datetime="2020-06-05T16:52:20.000Z" itemprop="datePublished">2020-06-06</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/04/machine_learning_19/" class="title">Clustering - Hierarchical, DBSCAN, Affinity Propagation</a></p>
                            <p class="item-date"><time datetime="2020-06-04T13:46:15.000Z" itemprop="datePublished">2020-06-04</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/30/machine_learning_18/" class="title">Clustering - K-means, K-medoid</a></p>
                            <p class="item-date"><time datetime="2020-05-29T16:01:30.000Z" itemprop="datePublished">2020-05-30</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/29/machine_learning_17/" class="title">Clustering</a></p>
                            <p class="item-date"><time datetime="2020-05-28T18:17:21.000Z" itemprop="datePublished">2020-05-29</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/27/machine_learning_16/" class="title">Ensemble Learning - Ensemble의 Ensemble</a></p>
                            <p class="item-date"><time datetime="2020-05-26T17:04:18.000Z" itemprop="datePublished">2020-05-27</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">21</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/06/06/machine_learning_20/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>
