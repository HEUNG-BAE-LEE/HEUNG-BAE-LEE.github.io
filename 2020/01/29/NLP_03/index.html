<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>NLP 실습 텍스트 분류 -01 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="영어 텍스트 분류 한국어는 띄어쓰기를 기준으로 모든 단어를 처리할 수 없으므로 상대적으로 전처리하기 쉬운 영어 텍스트를 가지고 먼저 감각을 키워보겠다.  데이터 이름 : Bag of Words Meets Bags of Popcorn 데이터 용도 : 텍스트 분류 학습을 목적으로 사용 데이터 권한 : MIT 데이터 출처 : https://www.kaggle.c">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP 실습 텍스트 분류 -01">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/01/29/NLP_03/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="영어 텍스트 분류 한국어는 띄어쓰기를 기준으로 모든 단어를 처리할 수 없으므로 상대적으로 전처리하기 쉬운 영어 텍스트를 가지고 먼저 감각을 키워보겠다.  데이터 이름 : Bag of Words Meets Bags of Popcorn 데이터 용도 : 텍스트 분류 학습을 목적으로 사용 데이터 권한 : MIT 데이터 출처 : https://www.kaggle.c">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/log_histogram_of_length_of_review.png">
<meta property="og:updated_time" content="2020-02-01T09:00:50.736Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP 실습 텍스트 분류 -01">
<meta name="twitter:description" content="영어 텍스트 분류 한국어는 띄어쓰기를 기준으로 모든 단어를 처리할 수 없으므로 상대적으로 전처리하기 쉬운 영어 텍스트를 가지고 먼저 감각을 키워보겠다.  데이터 이름 : Bag of Words Meets Bags of Popcorn 데이터 용도 : 텍스트 분류 학습을 목적으로 사용 데이터 권한 : MIT 데이터 출처 : https://www.kaggle.c">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/log_histogram_of_length_of_review.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/01/29/nlp_03/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/NLP/">NLP</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-NLP_03" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        NLP 실습 텍스트 분류 -01
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/01/29/NLP_03/" class="article-date">
            <time datetime="2020-01-29T14:40:09.000Z" itemprop="datePublished">2020-01-29</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h2 id="영어-텍스트-분류"><a href="#영어-텍스트-분류" class="headerlink" title="영어 텍스트 분류"></a>영어 텍스트 분류</h2><ul>
<li><p>한국어는 띄어쓰기를 기준으로 모든 단어를 처리할 수 없으므로 상대적으로 전처리하기 쉬운 영어 텍스트를 가지고 먼저 감각을 키워보겠다.</p>
<ul>
<li>데이터 이름 : Bag of Words Meets Bags of Popcorn</li>
<li>데이터 용도 : 텍스트 분류 학습을 목적으로 사용</li>
<li>데이터 권한 : MIT</li>
<li>데이터 출처 : <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/data" target="_blank" rel="noopener">https://www.kaggle.com/c/word2vec-nlp-tutorial/data</a></li>
</ul>
</li>
</ul>
<h2 id="문제-소개"><a href="#문제-소개" class="headerlink" title="문제 소개"></a>문제 소개</h2><p>영어 텍스트 분류 문제 중 캐글의 대회인 워드팝콘 문제를 활용할 것이다. 이 문제를 해결하면서 텍스트 분류 기술을 알아볼것이다.</p>
<h3 id="워드-팝콘"><a href="#워드-팝콘" class="headerlink" title="워드 팝콘"></a>워드 팝콘</h3><ul>
<li>워드 팝콘은 인터넷 영화 데이터베이스(IMDB)에서 나온 영화 평점 데이터를 활용한 캐글 문제다. 영화 평점 데이터이므로 각 데이터는 영화 리뷰 텍스트와 평점에 따른 감정 값(긍정 혹은 부정)으로 구성돼 있다. 이 데이터는 보통 감성 분석(sentiment analysis) 문제에서 자주 활용된다.</li>
</ul>
<h3 id="목표"><a href="#목표" class="headerlink" title="목표"></a>목표</h3><ul>
<li>1) 데이터를 불러오고 정제되지 않은 데이터를 활용하기 쉽게 전처리하는 과정</li>
<li>2) 데이터 분석 과정<ul>
<li>데이터를 분석하여 어떻게 문제를 풀어가야 할지 접근하는 과정</li>
</ul>
</li>
<li>3) 실제 문제를 해결하기 위해 알고리즘을 모델링하는 과정  </li>
</ul>
<p>캐글 API를 colab에서 사용하기 위한 인증 및 google storage에 업로드 되어있는 인증키 파일 현재 colab pwd로 복사해온 후 설정완료하기</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from google.colab import auth</span><br><span class="line">import warnings</span><br><span class="line">%matplotlib inline</span><br><span class="line">%config InlineBackend.figure_format = <span class="string">'retina'</span></span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line">auth.authenticate_user()</span><br><span class="line"></span><br><span class="line">!gsutil cp gs://kaggle_key/kaggle.json kaggle.json</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">!mkdir -p ~/.kaggle</span><br><span class="line">!mv ./kaggle.json ~/.kaggle/</span><br><span class="line">!chmod 600 ~/.kaggle/kaggle.json</span><br><span class="line">!pip install kaggle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 캐글 competition 목록확인</span></span><br><span class="line"><span class="comment">#!kaggle competitions list</span></span><br></pre></td></tr></table></figure>
<ul>
<li>목표 competition의 데이터 다운로드</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파일 확인</span></span><br><span class="line">!kaggle competitions files -c word2vec-nlp-tutorial</span><br><span class="line"></span><br><span class="line"><span class="comment"># 파일 다운로드</span></span><br><span class="line">!kaggle competitions download -c word2vec-nlp-tutorial</span><br></pre></td></tr></table></figure>
<h2 id="데이터-분석-및-전처리"><a href="#데이터-분석-및-전처리" class="headerlink" title="데이터 분석 및 전처리"></a>데이터 분석 및 전처리</h2><ul>
<li><p>모델을 학습시키기 전에 데이터를 전처리하는 과정을 거쳐야 한다. 전처리는 데이터를 모델에 적용하기에 적합하도록 데이터를 정제하는 과정이다. 그전에 데이터를 불러오고 분석하는 과정을 선행할 것이다. EDA과정을 거친 후 분석 결과를 바탕으로 전처리 작업을 할 것이다.</p>
</li>
<li><p>참고로 데이터를 불러오는데 403 error가 출력된다면, 우선적으로 대회의 rule을 check했는지 확인해 보아야 한다.</p>
<ul>
<li>sampleSubmission.csv 파일을 제외한 나머지 파일이 zip으로 압축돼 있기 때문에 압축을 푸는 과정부터 시작한다. 압축을 풀기 위해 zipfile이라는 내장 라이브러리를 사용할 것이다.</li>
</ul>
</li>
</ul>
<pre><code>- 압축을 풀기 위해 경로와 압축을 풀 파일명을 리스트로 선언한 후 반복문을 사용해 압축을 풀 것이다.
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import zipfile</span><br><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">file_list = [<span class="string">'labeledTrainData.tsv.zip'</span>, <span class="string">'testData.tsv.zip'</span>, <span class="string">'unlabeledTrainData.tsv.zip'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> file_list:</span><br><span class="line">  <span class="comment"># 압축풀기 대상 설정 및 모드 설정</span></span><br><span class="line">  zipRef = zipfile.ZipFile(DATA_IN_PATH + file, <span class="string">'r'</span>)</span><br><span class="line">  <span class="comment"># 압축 풀기 및 저장 경로 설정</span></span><br><span class="line">  zipRef.extractall(DATA_IN_PATH)</span><br><span class="line">  <span class="comment"># 호출 종료</span></span><br><span class="line">  zipRef.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import os</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line"></span><br><span class="line"><span class="comment"># 그래프를 바로 그리도록 함</span></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>
<ul>
<li>현재 사용할 데이터는 tap(\t)으로 구분돼 있으므로 delimeter=’\t’로 설정해주었고, 각 데이터에 각 항목명(Header)이 포함돼 있기 때문에 header인자에 0을 설정한다. R에서는 header=Ture로 하는 역할과 같다고 보면된다. 그리고 쌍따옴표를 무시하기 위해 quoting=3을 설정해 주었다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_data = pd.read_csv(DATA_IN_PATH+<span class="string">"labeledTrainData.tsv"</span>, header=0, delimiter=<span class="string">'\t'</span>, quoting=3)</span><br><span class="line"></span><br><span class="line">train_data.head()</span><br></pre></td></tr></table></figure>
<h3 id="데이터-분석-진행-순서"><a href="#데이터-분석-진행-순서" class="headerlink" title="데이터 분석 진행 순서"></a>데이터 분석 진행 순서</h3><ul>
<li>1) 데이터 크기</li>
<li>2) 데이터의 개수</li>
<li>3) 각 리뷰의 문자 길이 분포</li>
<li>4) 많이 사용된 단어</li>
<li>5) 긍정, 부정 데이터의 분포</li>
<li>6) 각 리뷰의 단어 개수 분포</li>
<li>7) 특수문자 및 대문자, 소문자 비율</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 데이터 크기</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"파일 크기 : "</span>)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(DATA_IN_PATH):</span><br><span class="line">  <span class="keyword">if</span> <span class="string">'tsv'</span> <span class="keyword">in</span> file and <span class="string">'zip'</span> not <span class="keyword">in</span> file:</span><br><span class="line">    <span class="built_in">print</span>(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + <span class="string">'MB'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습 데이터의 개수</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'전체 학습 데이터의 개수: &#123;&#125;'</span>.format(len(train_data)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># 전체 학습 데이터의 개수: 25000</span></span><br></pre></td></tr></table></figure>
<ul>
<li>각 review의 길이를 분석</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_length = train_data[<span class="string">'review'</span>].apply(len)</span><br><span class="line">train_length.head()</span><br></pre></td></tr></table></figure>
<ul>
<li>각 리뷰의 문자 길이가 대부분 6,000 이하이고 대부분 2,000이하에 분포돼 있음을 알 수 있다. 그리고 일부 데이터의 경우 이상치로 10,000 이상의 값을 가지고 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12, 5))</span><br><span class="line"></span><br><span class="line">plt.hist(train_length, bins=200, alpha=0.5, color=<span class="string">'r'</span>, label=<span class="string">'word'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># y축의 범위를 log단위로 바꿔주고 non-positive에 대해서는 아주작은 양수로 클리핑한다.</span></span><br><span class="line">plt.yscale(<span class="string">'log'</span>, nonposy=<span class="string">'clip'</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">'Log-Histogram of length of review'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Length of review'</span>)</span><br><span class="line"></span><br><span class="line">plt.ylabel(<span class="string">'Number of review'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/log_histogram_of_length_of_review.png" alt="각 review의 길이에 대한 히스토그램"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 기초 통계량 확인</span></span><br><span class="line">train_length.describe()</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12, 5))</span><br><span class="line"></span><br><span class="line">plt.boxplot(train_length, labels=[<span class="string">'train data review length'</span>], showmeans=True)</span><br></pre></td></tr></table></figure>
<p><img src="/image/train_data_review_length.png" alt="학습 데이터의 길이에 대한 boxplot"></p>
<ul>
<li><p>wordcloud를 통해 시각적으로 빈도수를 확인하기 위해 설치한다.</p>
</li>
<li><p>워드 클라우드를 보면 가장 많이 사용된 단어는 br이라는 것을 확인할 수 있다. HTML 태그인 br 해당 데이터가 높은 빈도수를 보이는 것으로 미루어보아 정제되지 않은 인터넷 상의 리뷰 형태로 작성돼 있음을 알 수 있다. 이후 전처리 작업에서 이 태그들을 모두 제거하겠다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install wordcloud</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from wordcloud import WordCloud</span><br><span class="line">cloud = WordCloud(width=800, height=600).generate(<span class="string">' '</span>.join(train_data[<span class="string">'review'</span>]))</span><br><span class="line">plt.figure(figsize=(20, 15))</span><br><span class="line">plt.imshow(cloud)</span><br><span class="line">plt.axis(<span class="string">'off'</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/IMDB_wordcloud.png" alt="학습데이터에 대한 wordcloud"></p>
<ul>
<li><p>이제 각 라벨의 분포를 확인해 본다. 해당 데이터의 경우 긍정과 부정이라는 두 가지 라벨만 가지고 있다. 분포의 경우 또 다른 시각화 도구인 seaborn을 사용해 시각화하겠다.</p>
</li>
<li><p>label의 분포 그래프를 보면 거의 동일한 개수로 분포돼 있음을 확인 할 수 있다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fig, axe = plt.subplots(ncols=1)</span><br><span class="line">fig.set_size_inches(6, 3)</span><br><span class="line">sns.countplot(train_data[<span class="string">'sentiment'</span>])</span><br></pre></td></tr></table></figure>
<p><img src="/image/IMDB_count_plot.png" alt="긍정 부정에 대한 count plot"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">"긍정 리뷰 개수: &#123;&#125;"</span>.format(train_data[<span class="string">'sentiment'</span>].value_counts()[1]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"부정 리뷰 개수: &#123;&#125;"</span>.format(train_data[<span class="string">'sentiment'</span>].value_counts()[0]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># 긍정 리뷰 개수: 12500</span></span><br><span class="line"><span class="comment"># 부정 리뷰 개수: 12500</span></span><br></pre></td></tr></table></figure>
<ul>
<li>각 리뷰를 단어 기준으로 나눠서 각 리뷰당 단어의 개수를 확인해 본다. 단어는 띄어쓰기 기준으로 하나의 단어라 생각하고 개수를 계산한다. 우선 각 단어의 길이를 가지는 변수를 하나 설정하자.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_word_counts = train_data[<span class="string">'review'</span>].apply(lambda x: len(x.split(<span class="string">' '</span>)))</span><br></pre></td></tr></table></figure>
<h4 id="대부분의-단어가-1000개-미만의-단어를-가지고-있고-대부분-200개-정도의-단어를-가지고-있음을-확인할-수-있다"><a href="#대부분의-단어가-1000개-미만의-단어를-가지고-있고-대부분-200개-정도의-단어를-가지고-있음을-확인할-수-있다" class="headerlink" title="대부분의 단어가 1000개 미만의 단어를 가지고 있고, 대부분 200개 정도의 단어를 가지고 있음을 확인할 수 있다."></a>대부분의 단어가 1000개 미만의 단어를 가지고 있고, 대부분 200개 정도의 단어를 가지고 있음을 확인할 수 있다.</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(15,10))</span><br><span class="line">plt.hist(train_word_counts, bins=50, facecolor=<span class="string">'r'</span>, label=<span class="string">'train'</span>)</span><br><span class="line">plt.title(<span class="string">'Log-Histogram of word count in review'</span>, fontsize=15)</span><br><span class="line">plt.yscale(<span class="string">'log'</span>, nonposy=<span class="string">'clip'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">'Number of words'</span>, fontsize=15)</span><br><span class="line">plt.ylabel(<span class="string">'Number of reviews'</span>, fontsize=15)</span><br></pre></td></tr></table></figure>
<p><img src="/image/word_count_histogram.png" alt="공백을 기준으로 분리한 각 review가 갖는 단어 수에 대한 histogram"></p>
<h4 id="review의-75-가-300개-이하의-단어를-가지고-있음을-확인-할-수-있다"><a href="#review의-75-가-300개-이하의-단어를-가지고-있음을-확인-할-수-있다" class="headerlink" title="review의 75%가 300개 이하의 단어를 가지고 있음을 확인 할 수 있다."></a>review의 75%가 300개 이하의 단어를 가지고 있음을 확인 할 수 있다.</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_word_counts.describe()</span><br></pre></td></tr></table></figure>
<ul>
<li><p>마지막으로 각 review에 대해 구두점과 대소문자 비율 값을 확인한다.</p>
</li>
<li><p>대부분 마침표를 포함하고 있고, 대문자도 대부분 사용하고 있다. 따라서 전처리 과정에서 대문자의 경우 모두 소문자로 바꾸고 특수 문자의 경우 제거한다. <code>이 과정은 학습에 방해가 되는 요소들을 제거하기 위함</code>이다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 물음표가 구두점으로 사용되는 비율</span></span><br><span class="line">qmarks = np.mean(train_data[<span class="string">'review'</span>].apply(lambda x : <span class="string">'?'</span> <span class="keyword">in</span> x))</span><br><span class="line"><span class="comment"># 마침표가 구두점으로 사용되는 비율</span></span><br><span class="line">fullstop = np.mean(train_data[<span class="string">'review'</span>].apply(lambda x : <span class="string">'.'</span> <span class="keyword">in</span> x))</span><br><span class="line"><span class="comment"># 첫 번째 대문자의 비율</span></span><br><span class="line">capital_first = np.mean(train_data[<span class="string">'review'</span>].apply(lambda x : x[0].isupper()))</span><br><span class="line"><span class="comment"># 대문자 비율</span></span><br><span class="line">capitals = np.mean(train_data[<span class="string">'review'</span>].apply(lambda x : max([y.isupper() <span class="keyword">for</span> y <span class="keyword">in</span> x])))</span><br><span class="line"><span class="comment"># 숫자 비율</span></span><br><span class="line">numbers = np.mean(train_data[<span class="string">'review'</span>].apply(lambda x : max([y.isdigit() <span class="keyword">for</span> y <span class="keyword">in</span> x])))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'물음표가 있는 질문: &#123;:.2f&#125;%'</span>.format(qmarks * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'마침표가 있는 질문: &#123;:.2f&#125;%'</span>.format(fullstop * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'첫 글자가 대문자인 질문: &#123;:.2f&#125;%'</span>.format(capital_first * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'대문자가 있는 질문: &#123;:.2f&#125;%'</span>.format(capitals * 100))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'숫자가 있는 질문: &#123;:.2f&#125;%'</span>.format(numbers * 100))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># 물음표가 있는 질문: 29.55%</span></span><br><span class="line"><span class="comment"># 마침표가 있는 질문: 99.69%</span></span><br><span class="line"><span class="comment"># 첫 글자가 대문자인 질문: 0.00%</span></span><br><span class="line"><span class="comment"># 대문자가 있는 질문: 99.59%</span></span><br><span class="line"><span class="comment"># 숫자가 있는 질문: 56.66%</span></span><br></pre></td></tr></table></figure>
<h3 id="데이터-전처리"><a href="#데이터-전처리" class="headerlink" title="데이터 전처리"></a>데이터 전처리</h3><ul>
<li><p>데이터를 모델에 적용할 수 있도록 데이터 전처리를 진행한다. 먼저 데이터 전처리 과정에서 사용할 라이브러리들을 불러올 것이다.</p>
</li>
<li><p>우선 전처리를 위해 nltk의 stopword를 이용하기 위해 nltk에서 다운로드를 받아야한다. 필자는 all를 선택하여서 모든 파일을 download를 받았지만, stopword만 받아도 상관없다.</p>
<ul>
<li>re, BeautifulSoup : 데이터 정제</li>
<li>stopwords : 불용어 제거</li>
<li>pad_sequence, Tokenizer : 데이터 전처리</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import json</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">from nltk.corpus import stopwords</span><br><span class="line">from tensorflow.python.keras.preprocessing.sequence import pad_sequences</span><br><span class="line">from tensorflow.python.keras.preprocessing.text import Tokenizer</span><br><span class="line">import nltk</span><br><span class="line">nltk.download()</span><br></pre></td></tr></table></figure>
<p><img src="/image/nltk_download.png" alt="nltk 다운로드"></p>
<ul>
<li><p>리뷰 데이터를 보면 문장 사이에 <br>과 같은 HTML 태그와 ‘\’, ‘…’ 같은 특수문자가 포함된 것을 확인할 수 있다. <code>문장부호 및 특수문자는 일반적으로 문자의 의미에 크게 영향을 미치지 않기 때문에 최적화된 학습을 위해 제거하자</code></p>
</li>
<li><p>BeautifulSoup의 get_text함수를 이용하면 HTML 태그를 제거한 나머지 텍스트를 얻을 수 있고 re.sub을 이용해 특수문자를 제거한다.</p>
</li>
<li><p>stopwords(불용어)를 삭제할 것이다. <code>불용어</code>란 <code>문장에서 자주 출현하나 전체적인 의미에 큰 영향을 주지 않는 단어</code>를 말한다. 예를들어, 영어에서는 조사, 관사 등과 같은 어휘가 있다. <code>데이터에 따라 불용어를 제거하는 것은 장단점이 있다.</code> 경우에 따라 불용어가 포함된 데이터를 모델링하는 데 있어 노이즈를 줄 수 있는 요인이 될 수 있어 불용어를 제거하는 것이 좋을 수 있다. 그렇지만 데이터가 많고 문장 구문에 대한 전체적인 패턴을 모델링하고자 한다면 이는 역효과를 줄 수도 있다. 지금 시행하고자 하는 분석은 감성 분석을 하고 있으므로 불용어가 감정 판단에 영향을 주지 않는다고 가정하고 불용어를 제거한다.</p>
</li>
<li><p>불용어를 제거하려면 따로 정의한 불용어 사전을 이용해야 한다. <code>사용자가 직접 정의할 수도 있지만 고려해야 하는 경우가 너무 많아서 보통 라이브러리에서 일반적으로 정의해놓은 불용어 사전을 이용</code>한다. NLTK의 불용어 사전을 이용할 것이며, <code>NLTK에서 제공하는 불용어 사전은 전부 소문자 단어로 구성돼 있기 때문에 불용어를 제거하기 위해서는 모든 단어를 소문자로 바꿔야한다.</code></p>
</li>
<li><p>review_text를 lower함수를 사용해 모두 소문자로 바꿔주었고, 이후 split 함수를 사용해 공백을 기준으로 reivew_text를 단어 리스트로 바꾼 후 불용어에 해당하지 않는 단어만 다시 모아서 리스트로 만들었다.</p>
</li>
<li><p>결과를 보면 단어 리스트가 하나의 문자열로 바뀐 것을 확인할 수 있다.</p>
</li>
<li><p>데이터를 한번에 처리하기 위해 위의 과정을 하나의 함수로 작성한다음에 apply로 적용시킨다. 함수의 경우 불용어 제거는 인자값으로 받아서 선택할 수 있게 하였다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def preprocessing(review, remove_stopwords=False):</span><br><span class="line">  <span class="comment"># 불용어 제거는 옵션으로 선택</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1. HTML 태그 제거</span></span><br><span class="line">  review_text = BeautifulSoup(review, <span class="string">'html5lib'</span>).get_text()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2. 영어가 아닌 특수문자를 공백(" ")으로 대체</span></span><br><span class="line">  review_text = re.sub(<span class="string">"[^a-zA-Z]"</span>, <span class="string">" "</span>, review_text)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3. 대문자를 소문자로 바꾸고 공백 단위로 텍스트를 나눠서 리스트로 만든다.</span></span><br><span class="line">  words = review_text.lower().split()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> remove_stopwords:</span><br><span class="line">    <span class="comment"># 4. 불용어 제거</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 영어 불용어 불러오기</span></span><br><span class="line">    stops = <span class="built_in">set</span>(stopwords.words(<span class="string">'english'</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 불용어가 아닌 단어로 이뤄진 새로운 리스트 생성</span></span><br><span class="line">    words = [w <span class="keyword">for</span> w <span class="keyword">in</span> words <span class="keyword">if</span> not w <span class="keyword">in</span> stops]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5. 단어 리스트를 공백을 넣어서 하나의 글로 합친다.</span></span><br><span class="line">    clean_review = <span class="string">' '</span>.join(words)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># 불용어를 제거하지 않을 때</span></span><br><span class="line">    clean_review = <span class="string">' '</span>.join(words)</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> clean_review</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="string">'clean_review'</span>]=train_data[<span class="string">'review'</span>].apply(lambda x : preprocessing(review=x, remove_stopwords=True))</span><br><span class="line"></span><br><span class="line">train_data[<span class="string">'clean_review'</span>][0]</span><br></pre></td></tr></table></figure>
<p><img src="/image/preprocessing_after_and_before.png" alt="전처리 전후 비교"></p>
<ul>
<li>우선 전처리한 데이터에서 각 단어를 인덱스로 벡터화해야 한다. 그리고 모델에 따라 입력값의 길이가 동일해야 하기 때문에 일정 길이로 자르고 부족한 부분은 특정값으로 채우는 패딩 과정을 진행해야 한다. <code>하지만 모델에 따라 각 review가 단어들의 인덱스로 구성된 벡터가 아닌 텍스트로 구성돼야 하는 경우도 있다.</code> 따라서 지금까지 전처리한 데이터를 pandas의 DataFrame으로 만들어 두고 이후에 전처리 과정이 모두 끝난 후 전처리한 데이터를 저장할 때 함께 저장하게 한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from tensorflow.python.keras.preprocessing.sequence import pad_sequences</span></span><br><span class="line"><span class="comment"># from tensorflow.python.keras.preprocessing.text import Tokenizer</span></span><br><span class="line"></span><br><span class="line">tokenizer = Tokenizer()</span><br><span class="line">tokenizer.fit_on_texts(train_data[<span class="string">'clean_review'</span>])</span><br><span class="line">text_sequences = tokenizer.texts_to_sequences(train_data[<span class="string">'clean_review'</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>위와 같이 사전에 등록되어진 인덱스로 각 review의 값들이 변경되었음을 확인할 수 있었다. 단어 사전은 앞서 정의한 tokenizer 객체에 word_index 값을 뽑으면 dictionary 형태로 구성되어 있음을 확인 할 수 있다. 또한 <code>단어 사전 뿐만 아니라 전체 단어 개수도 이후 모델에서 사용되기 때문에 저장해 둔다.</code></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">word_vocab = tokenizer.word_index</span><br><span class="line"><span class="built_in">print</span>(word_vocab)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"전체 단어 개수:"</span>, len(word_vocab))</span><br></pre></td></tr></table></figure>
<p><img src="/image/word_vocabulary.png" alt="단어 사전"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data_configs = &#123;&#125;</span><br><span class="line"></span><br><span class="line">data_configs[<span class="string">'vocab'</span>] = word_vocab</span><br><span class="line"></span><br><span class="line">data_configs[<span class="string">'vocab_size'</span>] = len(word_vocab) + 1</span><br></pre></td></tr></table></figure>
<ul>
<li><p>현재 각 데이터는 서로 길이가 다른데 이 길이를 하나로 통일해야 이후 모델에 바로 적용할 수 있기 때문에 특정 길이를 최대 길이로 정하고 더 긴 데이터의 경우 뒷부분을 자르고 짧은 데이터의 경우에는 0 값으로 패딩하는 작업을 진행한다.</p>
</li>
<li><p>패딩 처리에는 앞서 불러온 pad_sequences 함수를 사용한다. 이 함수를 사용할 때는 인자로 패딩을 적용할 데이터, 최대 길이값, 0 값을 데이터 앞에 넣을지 뒤에 넣을 지 여부를 설정한다. 또한, <code>제일 마지막 단어부터 단어를 카운트한다는 것에 유의하자.</code> 여기서 최대 길이를 174로 설정했는데, 이는 앞서 <code>데이터 분석 과정에서 단어 개수의 통계를 계산했을 때 나왔던 중앙값(median)이다. 보통 평균이 아닌 중앙값(median)을 사용하는 경우가 많은데, 평균은 이상치에 민감하기 때문이다.</code></p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 문장 최대 길이</span></span><br><span class="line">MAX_SEQUENCE_LENGTH = 174</span><br><span class="line"></span><br><span class="line"><span class="comment"># padding을 뒷부분에 한다.</span></span><br><span class="line">train_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=<span class="string">'post'</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Shape of train data: '</span>, train_inputs.shape)</span><br></pre></td></tr></table></figure>
<ul>
<li>마지막으로 학습 시 label 값을 넘파이 배열로 저장한다. 그 이유는 이후 전처리한 데이터를 저장할 때 넘파이 형태로 저장하기 때문이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_labels = np.array(train_data[<span class="string">'sentiment'</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Shape of label tensor: '</span>, train_labels.shape)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>이제 전처리한 데이터를 이후 모델링 과정에서 사용하기 위해 저장할 것이다. 여기서는 다음과 같은 총 4개의 데이터를 저장할 것이다. 텍스트 데이터의 경우 CSV 파일로 저장하고, 벡터화한 데이터와 정답 라벨의 경우 넘파이 파일로 저장한다. 마지막 데이터 정보의 경우 dictionary 형태이기 때문에 Json 파일로 저장한다.</p>
<ul>
<li>정제된 텍스트 데이터</li>
<li>벡터화한 데이터</li>
<li>정답 라벨</li>
<li>데이터 정보</li>
</ul>
</li>
<li><p>우선 경로와 파일명을 설정하고 os 라이브러리를 통해 폴더가 없는 경우 폴더를 생성한다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line">TRAIN_INPUT_DATA = <span class="string">'train_input.npy'</span></span><br><span class="line">TRAIN_LABEL_DATA = <span class="string">'train_label.npy'</span></span><br><span class="line">TRAIN_CLEAN_DATA = <span class="string">'train_clean.csv'</span></span><br><span class="line">DATA_CONFIGS = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line"><span class="comment"># 저장하는 디렉터리가 존재하지 않으면 생성</span></span><br><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_IN_PATH)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 전처리된 데이터를 numpy 형태로 저장</span></span><br><span class="line">np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, <span class="string">'wb'</span>), train_inputs)</span><br><span class="line">np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, <span class="string">'wb'</span>), train_labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 정제된 텍스트를 CSV 형태로 저장</span></span><br><span class="line">train_data.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index=False)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 데이터 사전을 JSON 형태로 저장</span></span><br><span class="line">json.dump(data_configs, open(DATA_IN_PATH + DATA_CONFIGS, <span class="string">'w'</span>), ensure_ascii=False)</span><br></pre></td></tr></table></figure>
<ul>
<li>지금까지 학습 데이터에 대해서만 전처리를 했으므로 테스트 데이터에 대해서도 위와 동일한 과정을 진행하면 된다. 다른 점은 평가 데이터의 경우 라벨 값이 없기 때문에 라벨은 따로 저장하지 않아도 되며, <code>데이터 저옵인 단어 사전과 단어 개수에 대한 정보도 학습 데이터의 것을 사용하므로 저장하지 않아도 된다.</code> 추가로 <code>테스트 데이터에 대해 저장해야 하는 값이 있는데 각 review 데이터에 대해 review에 대한 &#39;id&#39;값을 저장</code>해야 한다.</li>
</ul>
<hr>
<h5 id="평가-데이터를-전처리-할-때-한-가지-중요한-점은-Tokenizer를-통해-인덱스-벡터로-만들-때-Tokenizing-객체로-새롭게-만드는-것이-아니라-기존에-학습-데이터에-적용한-Tokenizer-객체를-사용해야-한다는-것이다-만약-새롭게-만들-경우-학습-데이터와-평가-데이터에-대한-각-단어의들의-인덱스가-달라져서-모델에-정상적으로-적용할-수-없기-때문이다-fit-on-texts-fit-on-sequences는-사전을-업데이트하는-행위인데-아래의-코드에서-fit-on-texts를-실행하지-않는-이유는-Tokenizer-객체를-새로-생성하지-않았기에-Train-data의-사전을-갖고-만약에-Train-data에-포함되어-있지-않은-단어가-Test-data에-존재한다면-확률을-0으로-주어야-하기-때문이다"><a href="#평가-데이터를-전처리-할-때-한-가지-중요한-점은-Tokenizer를-통해-인덱스-벡터로-만들-때-Tokenizing-객체로-새롭게-만드는-것이-아니라-기존에-학습-데이터에-적용한-Tokenizer-객체를-사용해야-한다는-것이다-만약-새롭게-만들-경우-학습-데이터와-평가-데이터에-대한-각-단어의들의-인덱스가-달라져서-모델에-정상적으로-적용할-수-없기-때문이다-fit-on-texts-fit-on-sequences는-사전을-업데이트하는-행위인데-아래의-코드에서-fit-on-texts를-실행하지-않는-이유는-Tokenizer-객체를-새로-생성하지-않았기에-Train-data의-사전을-갖고-만약에-Train-data에-포함되어-있지-않은-단어가-Test-data에-존재한다면-확률을-0으로-주어야-하기-때문이다" class="headerlink" title="평가 데이터를 전처리 할 때 한 가지 중요한 점은 Tokenizer를 통해 인덱스 벡터로 만들 때 Tokenizing 객체로 새롭게 만드는 것이 아니라, 기존에 학습 데이터에 적용한 Tokenizer 객체를 사용해야 한다는 것이다. 만약 새롭게 만들 경우 학습 데이터와 평가 데이터에 대한 각 단어의들의 인덱스가 달라져서 모델에 정상적으로 적용할 수 없기 때문이다. fit_on_texts, fit_on_sequences는 사전을 업데이트하는 행위인데 아래의 코드에서 fit_on_texts를 실행하지 않는 이유는 Tokenizer 객체를 새로 생성하지 않았기에 Train data의 사전을 갖고 만약에 Train data에 포함되어 있지 않은 단어가 Test data에 존재한다면 확률을 0으로 주어야 하기 때문이다."></a>평가 데이터를 전처리 할 때 한 가지 중요한 점은 Tokenizer를 통해 인덱스 벡터로 만들 때 Tokenizing 객체로 새롭게 만드는 것이 아니라, 기존에 학습 데이터에 적용한 Tokenizer 객체를 사용해야 한다는 것이다. 만약 새롭게 만들 경우 학습 데이터와 평가 데이터에 대한 각 단어의들의 인덱스가 달라져서 모델에 정상적으로 적용할 수 없기 때문이다. fit_on_texts, fit_on_sequences는 사전을 업데이트하는 행위인데 아래의 코드에서 fit_on_texts를 실행하지 않는 이유는 Tokenizer 객체를 새로 생성하지 않았기에 Train data의 사전을 갖고 만약에 Train data에 포함되어 있지 않은 단어가 Test data에 존재한다면 확률을 0으로 주어야 하기 때문이다.</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">test_data = pd.read_csv(DATA_IN_PATH + <span class="string">'testData.tsv'</span>, header=0, delimiter=<span class="string">'\t'</span>, quoting=3)</span><br><span class="line"></span><br><span class="line">test_data[<span class="string">'review'</span>] = test_data[<span class="string">'review'</span>].apply(lambda x: preprocessing(x, True))</span><br><span class="line">test_id = np.array(test_data[<span class="string">'id'</span>])</span><br><span class="line"></span><br><span class="line">text_sequences = tokenizer.texts_to_sequences(test_data[<span class="string">'review'</span>])</span><br><span class="line">test_inputs = pad_sequences(text_sequences, maxlen=MAX_SEQUENCE_LENGTH, padding=<span class="string">'post'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_INPUT_DATA = <span class="string">'test_input.npy'</span></span><br><span class="line">TEST_CLEAN_DATA = <span class="string">'test_clean.csv'</span></span><br><span class="line">TEST_ID_DATA = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">np.save(open(DATA_IN_PATH + TEST_INPUT_DATA, <span class="string">'wb'</span>), test_inputs)</span><br><span class="line">np.save(open(DATA_IN_PATH + TEST_ID_DATA, <span class="string">'wb'</span>), test_id)</span><br><span class="line">test_data.to_csv(DATA_IN_PATH + TEST_CLEAN_DATA, index=False)</span><br></pre></td></tr></table></figure>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "NLP 실습 텍스트 분류 -01",
        "image": "https://heung-bae-lee.github.io/image/log_histogram_of_length_of_review.png",
        "keywords": "",
        "genre": "NLP",
        "datePublished": "2020-01-29",
        "dateCreated": "2020-01-29",
        "dateModified": "2020-02-01",
        "url": "https://heung-bae-lee.github.io/2020/01/29/NLP_03/",
        "description": "영어 텍스트 분류
한국어는 띄어쓰기를 기준으로 모든 단어를 처리할 수 없으므로 상대적으로 전처리하기 쉬운 영어 텍스트를 가지고 먼저 감각을 키워보겠다.

데이터 이름 : Bag of Words Meets Bags of Popcorn
데이터 용도 : 텍스트 분류 학습을 목적으로 사용
데이터 권한 : MIT
데이터 출처 : https://www.kaggle.c"
        "wordCount": 2736
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2020/01/30/NLP_04/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            NLP 실습 텍스트 분류(TF-IDF, CountVectorizer, Word2Vec) -02
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2020/01/28/Crawling_03/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">Scrapy 웹 크롤링 04 - 실습</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></p>
                            <p class="item-title"><a href="/2020/03/19/data_structure_01/" class="title">내가 정리하는 자료구조 00 (Node, List, Queue)</a></p>
                            <p class="item-date"><time datetime="2020-03-19T09:46:30.000Z" itemprop="datePublished">2020-03-19</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/data-engineering/">data engineering</a></p>
                            <p class="item-title"><a href="/2020/03/03/data_engineering_10/" class="title">data engineering (데이터 모델링 및 챗봇 만들기)</a></p>
                            <p class="item-date"><time datetime="2020-03-03T14:29:20.000Z" itemprop="datePublished">2020-03-03</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/data-engineering/">data engineering</a></p>
                            <p class="item-title"><a href="/2020/03/01/data_engineering_09/" class="title">data engineering (데이터 파이프라인 자동화)</a></p>
                            <p class="item-date"><time datetime="2020-03-01T04:49:06.000Z" itemprop="datePublished">2020-03-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/data-engineering/">data engineering</a></p>
                            <p class="item-title"><a href="/2020/02/24/data_engineering_08/" class="title">data engineering (Presto란?)</a></p>
                            <p class="item-date"><time datetime="2020-02-24T13:11:09.000Z" itemprop="datePublished">2020-02-24</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/data-engineering/">data engineering</a></p>
                            <p class="item-title"><a href="/2020/02/22/data_engineering_07/" class="title">data engineering (데이터 웨어하우스 vs 데이터 레이크)</a></p>
                            <p class="item-date"><time datetime="2020-02-21T16:14:31.000Z" itemprop="datePublished">2020-02-22</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">5</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/01/29/NLP_03/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
