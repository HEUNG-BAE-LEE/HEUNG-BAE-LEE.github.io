<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>로지스틱 회귀분석 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="로지스틱 회귀분석   로지스틱(Logistic) 회귀분석은 회귀분석이라는 명칭과 달리 회귀분석 문제와 분류문제 모두에 사용할 수 있다. 로지스틱 회귀분석 모형에서는 종속변수가 이항분포를 따르고 그 모수 $ \theta $ 가 독립변수 $ x $ 에 의존한다고 가정한다.  p(y \mid x) = \text{Bin} (y; \theta (x), N) 위 식에">
<meta property="og:type" content="article">
<meta property="og:title" content="로지스틱 회귀분석">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/04/03/machine_learning_05/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="로지스틱 회귀분석   로지스틱(Logistic) 회귀분석은 회귀분석이라는 명칭과 달리 회귀분석 문제와 분류문제 모두에 사용할 수 있다. 로지스틱 회귀분석 모형에서는 종속변수가 이항분포를 따르고 그 모수 $ \theta $ 가 독립변수 $ x $ 에 의존한다고 가정한다.  p(y \mid x) = \text{Bin} (y; \theta (x), N) 위 식에">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/logistic_regression.png">
<meta property="og:updated_time" content="2020-06-09T07:49:23.930Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="로지스틱 회귀분석">
<meta name="twitter:description" content="로지스틱 회귀분석   로지스틱(Logistic) 회귀분석은 회귀분석이라는 명칭과 달리 회귀분석 문제와 분류문제 모두에 사용할 수 있다. 로지스틱 회귀분석 모형에서는 종속변수가 이항분포를 따르고 그 모수 $ \theta $ 가 독립변수 $ x $ 에 의존한다고 가정한다.  p(y \mid x) = \text{Bin} (y; \theta (x), N) 위 식에">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/logistic_regression.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/04/03/machine_learning_05/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">
    <link rel="stylesheet" type="text/css" href>
    <link rel="stylesheet" href="https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css">	
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothiccoding.css">
    <link rel="stylesheet" href="/css/style.css">
   
    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/machine-learning/">machine learning</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-machine_learning_05" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        로지스틱 회귀분석
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/04/03/machine_learning_05/" class="article-date">
            <time datetime="2020-04-03T04:58:10.000Z" itemprop="datePublished">2020-04-03</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h1 id="로지스틱-회귀분석"><a href="#로지스틱-회귀분석" class="headerlink" title="로지스틱 회귀분석"></a>로지스틱 회귀분석</h1><p><img src="/image/logistic_regression.png" alt="로지스틱 회귀"></p>
<p><img src="/image/logistic_regression_02.png" alt="로지스틱 함수를 사용하는 이유"></p>
<ul>
<li>로지스틱(Logistic) 회귀분석은 회귀분석이라는 명칭과 달리 회귀분석 문제와 분류문제 모두에 사용할 수 있다. 로지스틱 회귀분석 모형에서는 종속변수가 이항분포를 따르고 그 모수 $ \theta $ 가 독립변수 $ x $ 에 의존한다고 가정한다.</li>
</ul>
<script type="math/tex; mode=display">p(y \mid x) = \text{Bin} (y; \theta (x), N)</script><ul>
<li>위 식에서 보듯이 <code>로지스틱 함수는 y의 값이 특정한 구간내의 값 (0 ~ N)만 가질 수 있기 때문에 종속변수가 이러한 특성을 가진 경우에 회귀분석 방법으로 사용</code>할 수 있다.</li>
</ul>
<ul>
<li>또는 이항 분포의 특별한 경우 $ (N=1) $ 로 $ y $ 가 베르누이 확률분포인 경우도 있을 수 있다. 여기에서는 베르누이 확률분포를 따르는 로지스틱 회귀분석만 고려하기로 한다.</li>
</ul>
<script type="math/tex; mode=display">p(y \mid x) = \text{Bern} (y; \theta (x) )</script><ul>
<li>종속변수 $ y $ 가  $ 0 $ 또는 $ 1 $ 인 분류 예측 문제를 풀 때는 $ x $ 값을 이용하여 $ \theta ( x ) $ 를 예측한 후 다음 기준에 따라 $ \hat{y} $ 값을 출력한다.</li>
</ul>
<script type="math/tex; mode=display">\hat{y} = \begin{cases} 1 & \text{ if } \theta (x) \geq 0.5 \\ 0 & \text{ if } \theta (x) < 0.5 \end{cases}</script><ul>
<li>회귀분석을 할 때는 $ \hat{y} $ 으로 $ y = 1 $ 이 될 확률값 $ \theta ( x ) $ 을 직접 사용한다.</li>
</ul>
<script type="math/tex; mode=display">\hat{y} = \theta (x)</script><h3 id="시그모이드-함수"><a href="#시그모이드-함수" class="headerlink" title="시그모이드 함수"></a>시그모이드 함수</h3><ul>
<li>로지스틱 회귀모형에서는 베르누이 확률분포의 모수 $ \theta $ 가 $ x $ 의 함수라고 가정한다. $ \theta (x) $ 는 $ x $ 에 대한 함수를 0부터 1 사이의 값만 나올 수 있도록 시그모이드 함수(sigmoid function)라는 함수를 사용하여 변형한 것을 사용한다.</li>
</ul>
<ul>
<li>시그모이드 함수는 종속변수의 모든 실수 값에 대해 유한한 구간 $ ( a, b ) $ 사이의 한정된 값을 가지고</li>
</ul>
<script type="math/tex; mode=display">a < f(x) < b</script><ul>
<li><code>항상 양의 기울기를 가지는 단조증가</code>하는</li>
</ul>
<script type="math/tex; mode=display">a > b \; \rightarrow \; f(a) > f(b)</script><ul>
<li><p>함수의 집합을 말한다. 실제로는 다음과 같은 함수들이 주로 사용된다.</p>
</li>
<li><p>로지스틱(Logistic)함수</p>
</li>
</ul>
<script type="math/tex; mode=display">\text{logitstic}(z) = \sigma(z) = \dfrac{1}{1+\exp{(-z)}}</script><ul>
<li>하이퍼볼릭 탄젠트(Hyperbolic tangent)함수</li>
</ul>
<script type="math/tex; mode=display">\tanh(z) = \frac{\sinh z}{\cosh z} = \frac{(e^z - e^{-z})/2}{(e^z + e^{-z})/2} = 2 \sigma(2z) - 1</script><ul>
<li>오차(Error)함수</li>
</ul>
<script type="math/tex; mode=display">\text{erf}(z) = \frac{2}{\sqrt\pi}\int_0^z e^{-t^2}\,dt</script><ul>
<li><code>하이퍼볼릭탄젠트 함수는 로지스틱함수를 위아래 방향으로 2배 늘리고 좌우 방향으로 1/2로 축소한 것과 같다.</code></li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">xx = np.linspace(-5, 5, 1000)</span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.plot(xx, 1/(1+np.exp(-xx)), <span class="string">'r-'</span>, label=<span class="string">"로지스틱함수"</span>)</span><br><span class="line">plt.plot(xx, sp.special.erf(0.5*np.sqrt(np.pi)*xx), <span class="string">'g:'</span>, label=<span class="string">"오차함수"</span>)</span><br><span class="line">plt.plot(xx, np.tanh(xx), <span class="string">'b--'</span>, label=<span class="string">"하이퍼볼릭탄젠트함수"</span>)</span><br><span class="line">plt.ylim([-1.1, 1.1])</span><br><span class="line">plt.legend(loc=2)</span><br><span class="line">plt.xlabel(<span class="string">"x"</span>)</span><br><span class="line">plt.savefig(<span class="string">'sigmoid_functions_ploting'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/sigmoid_functions_ploting.png" alt="시그모이드 함수들"></p>
<h3 id="로지스틱-함수"><a href="#로지스틱-함수" class="headerlink" title="로지스틱 함수"></a>로지스틱 함수</h3><ul>
<li>로지스틱함수는 <code>음의 무한대부터 양의 무한대까지의 실수값을 0부터 1사이의 실수값으로 1대 1 대응시키는 시그모이드함수</code>이다. 보통 시그모이드함수라고 하면 로지스틱함수를 의미한다.</li>
</ul>
<p><img src="/image/logistic_regression_03.png" alt="로지스틱 회귀 모형식이 만들어진 과정"></p>
<ul>
<li>위의 추정식에서 가장 오른쪽에 있는 로그 오즈비(log odds ratio)를 먼저 설명하자면, 로그 안에 들어간 오즈비(odds ratio)는 베르누이 시도에서 1이 나올 확률 $ \theta (x) $와 0이 나올 확률 $ 1 - \theta (x) $의 비율(ratio)을 의미한다.</li>
</ul>
<script type="math/tex; mode=display">odds ratio = \frac{ \theta (x) }{1 - \theta (x)}</script><p>0부터 1사이의 값만 가지는 확률값인 $ \theta (x) $ 를 오즈비로 변환하면 0부터 양의 무한대까지의 값을 가질 수 있다. 오즈비를 로그변환한 것이 위에서 언급한 Logit function이다. 이로써 로지트 함수의 값은 로그변환에 의해 음의 무한대 $(- \infty)$부터 양의 무한대$(\infty)$까지의 값을 가질 수 있다.</p>
<script type="math/tex; mode=display">y = logit(odds ratio) = log ( \frac{ \theta (x) }{ 1 - \theta (x) } )</script><ul>
<li>로지스틱함수(Logistic function)은 로지트 함수의 역함수이다. 즉 <code>음의 무한대부터 양의 무한대 까지의 값을 가지는 입력변수를 0부터 1사이의 값을 가지는 출력변수로 변환한 것</code>이다.</li>
</ul>
<script type="math/tex; mode=display">logistic(z) = \theta (z) = \frac{1}{1 + exp(-z)}</script><h3 id="선형-판별-함수"><a href="#선형-판별-함수" class="headerlink" title="선형 판별 함수"></a>선형 판별 함수</h3><ul>
<li>또한, <code>로지스틱 회귀분석에서는 판별함수 수식으로 선형함수를 사용하므로 판별 경계면 또한 선형이 됨을 유의</code>해야한다.</li>
</ul>
<ul>
<li><p>로지스틱함수 $ \sigma(z) $ 를 사용하는 경우에는 $ z $ 과 $ \theta $ 값은 다음과 같은 관계가 있다.</p>
<ul>
<li>$ z = 0 $ 일 때 $ \theta = 0.5 $</li>
<li>$ z &gt; 0 $ 일 때 $ \theta &gt; 0.5; \rightarrow \hat{y} = 1 $</li>
<li>$ z &lt; 0 $ 일 때 $ \theta &lt; 0.5; \rightarrow \hat{y} = 0 $</li>
</ul>
</li>
</ul>
<ul>
<li>즉, $ z $ 가 분류 모형의 판별 함수(decision function)의 역할을 한다. 로지스틱 회귀분석에서는 <code>판별함수 수식으로 선형함수를 사용</code>한다. 따라서 판별 경계면도 선형이 된다.</li>
</ul>
<script type="math/tex; mode=display">z = w^{T} x</script><p><img src="/image/Logistic_function_01.png" alt="로지스틱 함수"></p>
<ul>
<li>위의 그림에서 알 수 있듯이, X의 범위가 [$-\infty$, $+\infty$]인 경우에 Y의 범위를 [0,1]로 만들어주는 함수이다.</li>
</ul>
<p><img src="/image/logistic_regression_example.png" alt="로지스틱 회귀 예제"></p>
<ul>
<li>위의 그림에서 좌측그림의 적합시킨 회귀선(파란선)을 보면 예측할 확률값이 500미만이면 음수를 갖을 수 있게 되는 것을 확인할 수 있다. 이는 <code>확률값을 예측하는 모델을 만든다는 가정 자체에 위반하는 것이므로 이전에 학습했었던 회귀모형들과는 다른 방법의 회귀모형을 만들어 적합시켜야 할 것임이 분명</code>해졌다.</li>
</ul>
<ul>
<li>일반적으로 decision boundary(threshold)를 0.5로 하지만 class가 imbalanced 한 경우는 조절하여보면서 적합시킬 수 있다.</li>
</ul>
<p><img src="/image/logistic_regression_coefficient_estimation_01.png" alt="로지스틱 회귀계수 추정방법 소개"></p>
<ul>
<li>기존의 일반 선형 회귀이듯, 다중 선형 회귀는 최소제곱법을 통해 해를 추정해 낼 수 있지만, <code>로지스틱 회귀</code>는 비선형 방정식이 되어 동일하게 최소제곱법으로 회귀계수를 추정해내기 힘들다. 그러므로, 위의 그림처럼 <code>MLE를 통해 회귀계수들을 추정</code>한다.</li>
</ul>
<p><img src="/image/logistic_regression_coefficient_estimation_02.png" alt="로지스틱 회귀계수 MLE를 사용한 추정"></p>
<blockquote>
<p>그레디언트 벡터가 영벡터가 되는 모수의 값이 로그가능도를 최대화하는 값이다. 하지만 그레디언트 벡터 수식이 $ w $ 에 대한 비선형 함수이므로 선형모형과 같이 간단하게 그레디언트가 0이 되는 모수 $ w $ 값에 대한 수식을 구할 수 없으며 수치적인 최적화 방법(numerical optimization)을 통해 반복적으로 최적 모수 $ w $ 의 값을 구해야 한다.</p>
</blockquote>
<h3 id="수치적-최적화"><a href="#수치적-최적화" class="headerlink" title="수치적 최적화"></a>수치적 최적화</h3><ul>
<li>위의 MLE 방식을 통해 parameter의 값을 업데이트하는데, 아래와 같은 방식으로 수치적 최적화를 진행한다. 위의 로그 가능도함수 $log(l(\beta_{0}, \beta_{1}))$을 편의상 LL이라하자. 로그가능도함수 LL을 최대화하는 것은 다음 목적함수를 최소화하는 것과 같다.</li>
</ul>
<script type="math/tex; mode=display">J = - LL</script><ul>
<li>SGD(Steepest Gradient Descent)방식을 사용(Stochastic Gradient Descent아님!!)하여 아래와 같은 그레디언 벡터를 구할 수 있다.</li>
</ul>
<script type="math/tex; mode=display">g_{k} = \frac{d}{dw} (-LL)</script><ul>
<li>위에서 구한 그레디언트 방향으로 step size $(n_{k})$만큼 이동한다.</li>
</ul>
<script type="math/tex; mode=display">\begin{eqnarray} w_{k+1} &=& w_{k} - \eta_k g_k \\ &=& w_{k} + \eta_k \sum_{i=1}^N \big( y_i  - \mu(x_i; w_k) \big) x_i\\ \end{eqnarray}</script><p><img src="/image/multi_logistic_regression_example_01.png" alt="다중 로지스틱 회귀 예제 - 01"></p>
<ul>
<li>위의 그림에서 요약된 다중 로지스틱 회귀분석의 결과를 살펴보면, 맨 마지막 설명과 같이 해석할 수 있지만, 좀 더 자연스러운 해석하고 싶을 경우 아래 그림과 같이 Logit 보다 exp를 취해줘 Odds로 변환하여 해석하는 것을 권한다.</li>
</ul>
<p><img src="/image/multi_logistic_regression_example_02.png" alt="다중 로지스틱 회귀 예제 - 02"></p>
<h3 id="StatsModels-패키지의-로지스틱-회귀"><a href="#StatsModels-패키지의-로지스틱-회귀" class="headerlink" title="StatsModels 패키지의 로지스틱 회귀"></a>StatsModels 패키지의 로지스틱 회귀</h3><ul>
<li>다음과 같은 1차원 독립변수를 가지는 분류문제를 풀어보자.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_classification</span><br><span class="line"></span><br><span class="line">X0, y = make_classification(n_features=1, n_redundant=0, n_informative=1,</span><br><span class="line">                            n_clusters_per_class=1, random_state=4)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.scatter(X0, y, c=y, s=100, edgecolor=<span class="string">"k"</span>, linewidth=2)</span><br><span class="line">sns.distplot(X0[y == 0, :], label=<span class="string">"y = 0"</span>, hist=False)</span><br><span class="line">sns.distplot(X0[y == 1, :], label=<span class="string">"y = 1"</span>, hist=False)</span><br><span class="line">plt.ylim(-0.2, 1.2)</span><br><span class="line"><span class="comment"># plt.savefig('logistic_regression_example_datas')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/logistic_regression_example_datas.png" alt="로지스틱 회귀 예시 데이터"></p>
<ul>
<li>StatsModels 패키지는 베르누이 분포를 따르는 로지스틱 회귀 모형 <code>Logit</code>를 제공한다. 사용방법은 <code>OLS</code> 클래스 사용법과 동일하다. 종속변수와 독립변수 데이터를 넣어 모형을 만들고 <code>fit</code> 메서드로 학습을 시킨다. <code>fit</code> 메서드의 <code>disp=0</code> 인수는 최적화 과정에서 문자열 메세지를 나타내지 않는 역할을 한다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = sm.add_constant(X0)</span><br><span class="line">logit_mod = sm.Logit(y, X)</span><br><span class="line">logit_res = logit_mod.fit(disp=0)</span><br><span class="line"><span class="built_in">print</span>(logit_res.summary())</span><br></pre></td></tr></table></figure>
<h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">                           Logit Regression Results                           </span><br><span class="line">==============================================================================</span><br><span class="line">Dep. Variable:                      y   No. Observations:                  100</span><br><span class="line">Model:                          Logit   Df Residuals:                       98</span><br><span class="line">Method:                           MLE   Df Model:                            1</span><br><span class="line">Date:                Mon, 08 Jun 2020   Pseudo R-squ.:                  0.7679</span><br><span class="line">Time:                        00:12:48   Log-Likelihood:                -16.084</span><br><span class="line">converged:                       True   LL-Null:                       -69.295</span><br><span class="line">Covariance Type:            nonrobust   LLR p-value:                 5.963e-25</span><br><span class="line">==============================================================================</span><br><span class="line">coef    std err          z      P&gt;|z|      [0.025      0.975]</span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">const          0.2515      0.477      0.527      0.598      -0.683       1.186</span><br><span class="line">x1             4.2382      0.902      4.699      0.000       2.470       6.006</span><br><span class="line">==============================================================================</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>결과 객체에서 <code>summary</code>메서드를 사용하여 리포트를 출력할 수 있다. 결과 리포트에서 판별함수의 수식이 다음과 같다는 것을 알 수 있다.</li>
</ul>
<script type="math/tex; mode=display">\theta (x) = \sigma(4.2382x + 0.2515)</script><ul>
<li>따라서 $ z $ 값의 부로를 나누는 기준값은 $ 4.2382x + 0.2515 = 0.5 $ 가 되는 $ x $ 값 즉, $ (0.5-0.2515)/4.2382 $ 이다. <code>predict</code> 메서드를 사용하면 $ \theta (x) $ 값을 출력한다.</li>
</ul>
<ul>
<li>유의확률을 감안하면 상수항의 값은 0과 마찬가지이므로 $ \theta (x) $ 가 다음과 같다고 볼 수도 있다.</li>
</ul>
<script type="math/tex; mode=display">\mu(x) = \sigma(4.2382x)</script><ul>
<li>이렇게 생각하면 $ z $ 값의 부호를 나누는 기준값은 실질적으로는 $ 0.5/4.2382 = 0.118 $ 이다. 실제 데이터가 네모 포인트로 되어있고, 원형의 데이터가 예측 데이터이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">xx = np.linspace(-3, 3, 100)</span><br><span class="line">mu = logit_res.predict(sm.add_constant(xx))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12, 8))</span><br><span class="line">plt.plot(xx, mu, lw=3)</span><br><span class="line">plt.scatter(X0, y, c=y, s=100, edgecolor=<span class="string">"k"</span>, lw=2)</span><br><span class="line">plt.scatter(X0, logit_res.predict(X), label=r<span class="string">"$\hat&#123;y&#125;$"</span>, marker=<span class="string">'s'</span>, c=y,</span><br><span class="line">            s=100, edgecolor=<span class="string">"k"</span>, lw=1)</span><br><span class="line">plt.vlines(0.118, ymin=0, ymax=1, lw=1, linestyles=<span class="string">'--'</span>, colors=<span class="string">"red"</span>)</span><br><span class="line">plt.xlim(-3, 3)</span><br><span class="line">plt.xlabel(<span class="string">"x"</span>)</span><br><span class="line">plt.ylabel(r<span class="string">"$\mu$"</span>)</span><br><span class="line">plt.title(r<span class="string">"$\hat&#123;y&#125; = \mu(x)$"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"><span class="comment"># plt.savefig('predicted_value_y_hat_logistic_regression_example')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/predicted_value_y_hat_logistic_regression_example.png" alt="실제 데이터와 예측결과 비교"></p>
<h3 id="판별함수"><a href="#판별함수" class="headerlink" title="판별함수"></a>판별함수</h3><ul>
<li><code>Logit</code> 모형의 결과 객체에는 <code>fittedvalues</code>라는 속성으로 판별함수 $ z = w^{T}x $ 값이 들어가 있다. 이 값을 이용하여 분류문제를 풀 수도 있다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.scatter(X0, y, c=y, s=100, edgecolor=<span class="string">"k"</span>, lw=2, label=<span class="string">"데이터"</span>)</span><br><span class="line">plt.plot(X0, logit_res.fittedvalues, label=<span class="string">"판별함수값"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"><span class="comment"># plt.savefig('decision_function_with_logit_fitted_value')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/logistic_fuction_using_logit_fitted_value.png" alt="판별함수 값을 통해 분류문제 풀기"></p>
<ul>
<li>위의 $ z $ 값을 통해 직접 시그모이드 함수(로지스틱함수)를 취하여 그려보면 다음과 같이 위에서 로지스틱함수를 그려 예측한 그림과 동일해 지는 것을 확인할 수 있다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.scatter(X0, y, c=y, s=100, edgecolor=<span class="string">"k"</span>, lw=2, label=<span class="string">"데이터"</span>)</span><br><span class="line">plt.scatter(X0, 1/(1+np.exp(-logit_res.fittedvalues)), label=<span class="string">"로지스틱함수값"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"><span class="comment"># plt.savefig('logistic_fuction_using_logit_fitted_value')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/decision_function_with_logit_fitted_value.png" alt="직접 로짓값을 로지스틱 함수에 적용한 그림"></p>
<h3 id="로지스틱-회귀-성능-측정"><a href="#로지스틱-회귀-성능-측정" class="headerlink" title="로지스틱 회귀 성능 측정"></a>로지스틱 회귀 성능 측정</h3><ul>
<li>로지스틱 회귀 성능은 맥파든 의사결정계수(McFadden pseudo R square) 값으로 측정한다.</li>
</ul>
<script type="math/tex; mode=display">R^2_{\text{pseudo}} = 1 - \dfrac{G^2}{G^2_0}</script><ul>
<li>$ G^{2} $ 는 이탈도(deviance)라고 하는 양으로 다음과 같이 정의된다. 여기에서 $ \hat{y} $ 은 $ y=1 $ 일 확률 $ \theta $ 를 뜻한다.</li>
</ul>
<script type="math/tex; mode=display">G^2 = 2\sum_{i=1}^N \left( y_i\log\dfrac{y_i}{\hat{y}_i} + (1-y_i)\log\dfrac{1-y_i}{1-\hat{y}_i} \right)</script><ul>
<li><code>이탈도는 모형이 100% 정확한 경우에는 0이 되고 모형의 성능이 나빠질수록 값이 커진다.</code> 또한, 이탈도는 로그가능도에 음수를 취한 값과 같다.</li>
</ul>
<script type="math/tex; mode=display">G^2 = -LL</script><ul>
<li>$ G^{2} $ 는 현재 이탈도이고 $ G_{0}^{2} $ 는 귀무모형(null model)으로 측정한 이탈도이다.</li>
</ul>
<ul>
<li>귀무모형이란 모든 $ x $ 가 $ y $ 를 예측하는데 전혀 영향을 미치지 않는 모형을 말한다. 즉, 무조건부 확률 $ p(y) $ 에 따라 $ x $ 에 상관없이 동일하게 $ y $ 를 예측하는 모형을 말한다. 결국 우리가 만들 수 있는 가장 성능이 나쁜 모형이 된다.</li>
</ul>
<script type="math/tex; mode=display">\mu_{\text{null}} = \dfrac{\text{number of $Y=1$ data}}{\text{number of all data}}</script><ul>
<li><code>따라서 맥파든 의사결정계수는 가장 성능이 좋을 때는 1이 되고 가장 성능이 나쁠 때는 0이된다.</code></li>
</ul>
<ul>
<li>Scikit-learn 패키지의 metric 서브패키지에는 로그 손실을 계산하는 <code>log_loss</code> 함수가 있다. 위 예제에서 최적 모형의 로그 손실은 약 16.08로 계산된다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import log_loss</span><br><span class="line"></span><br><span class="line">y_hat = logit_res.predict(X)</span><br><span class="line">log_loss(y, y_hat, normalize=False)</span><br></pre></td></tr></table></figure>
<h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">16.084355200413036</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>귀무 모형의 모수값을 구하면 0.51이고 이 값으로 로그 손실을 계산하면 약 69이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mu_null = np.sum(y) / len(y)</span><br><span class="line">mu_null</span><br></pre></td></tr></table></figure>
<h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.51</span><br></pre></td></tr></table></figure>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y_null = np.ones_like(y) * mu_null</span><br><span class="line">log_loss(y, y_null, normalize=False)</span><br></pre></td></tr></table></figure>
<h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">69.29471672244784</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>두 값을 이용하여 맥파든 의사 결정계수 값을 계산할 수 있다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 - (log_loss(y, y_hat) / log_loss(y, y_null))</span><br></pre></td></tr></table></figure>
<h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.7678848264170398</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="Scikit-Learn-패키지의-로지스틱-회귀"><a href="#Scikit-Learn-패키지의-로지스틱-회귀" class="headerlink" title="Scikit-Learn 패키지의 로지스틱 회귀"></a>Scikit-Learn 패키지의 로지스틱 회귀</h3><ul>
<li>Scikit-Learn 패키지는 로지스틱 회귀 모형 <code>LogisticRegression</code> 를 제공한다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line"></span><br><span class="line">model_sk = LogisticRegression().fit(X0, y)</span><br><span class="line"></span><br><span class="line">xx = np.linspace(-3, 3, 100)</span><br><span class="line">mu = 1.0/(1 + np.exp(-model_sk.coef_[0][0]*xx - model_sk.intercept_[0]))</span><br><span class="line">plt.plot(xx, mu)</span><br><span class="line">plt.scatter(X0, y, c=y, s=100, edgecolor=<span class="string">"k"</span>, lw=2)</span><br><span class="line">plt.scatter(X0, model_sk.predict(X0), label=r<span class="string">"$\hat&#123;y&#125;$"</span>, marker=<span class="string">'s'</span>, c=y,</span><br><span class="line">            s=100, edgecolor=<span class="string">"k"</span>, lw=1, alpha=0.5)</span><br><span class="line">plt.xlim(-3, 3)</span><br><span class="line">plt.xlabel(<span class="string">"x"</span>)</span><br><span class="line">plt.ylabel(r<span class="string">"$\mu$"</span>)</span><br><span class="line">plt.title(r<span class="string">"$\hat&#123;y&#125;$ = sign $\mu(x)$"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/logistic_regression_example_with_scikit_learn.png" alt="scikit-learn의 logistic regression"></p>
<blockquote>
<p>연습문제</p>
<ol>
<li>붓꽃 분류문제에서 클래스가 세토사와 베르시칼라 데이터만 사용하고 (setosa=0, versicolor=1) 독립변수로는 꽃받침 길이(Sepal Length)와 상수항만 사용하여 StatsModels 패키지의 로지스틱 회귀모형으로 결과를 예측하고 보고서를 출력한다. 이 보고서에서 어떤 값이 세토사와 베르시칼라를 구분하는 기준값(threshold)으로 사용되고 있는가?</li>
<li>위 결과를 분류결과표(confusion matrix)와 분류결과보고서(classification report)로 나타내라.</li>
<li>이 모형에 대해 ROC커브를 그리고 AUC를 구한다. 이 때 Scikit-Learn의 LogisticRegression을 사용하지 않고 위에서 StatsModels로 구한 모형을 사용한다.</li>
</ol>
</blockquote>
<ul>
<li>1번에 대한 문제 풀이는 아래와 같다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line"></span><br><span class="line">iris=load_iris()</span><br><span class="line">X=pd.DataFrame(iris.data[iris.target &lt;= 1], columns=iris.feature_names)</span><br><span class="line">X=X.loc[:, <span class="string">'sepal length (cm)'</span>]</span><br><span class="line">y=pd.Series(iris.target[iris.target &lt;= 1], name=<span class="string">"target"</span>)</span><br><span class="line"></span><br><span class="line">import statsmodels.api as sm</span><br><span class="line"></span><br><span class="line">X = sm.add_constant(X0)</span><br><span class="line">logit_mod = sm.Logit(y, X)</span><br><span class="line">logit_res = logit_mod.fit(disp=0)</span><br><span class="line"><span class="built_in">print</span>(logit_res.summary())</span><br></pre></td></tr></table></figure>
<h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">                           Logit Regression Results                           </span><br><span class="line">==============================================================================</span><br><span class="line">Dep. Variable:                 target   No. Observations:                  100</span><br><span class="line">Model:                          Logit   Df Residuals:                       98</span><br><span class="line">Method:                           MLE   Df Model:                            1</span><br><span class="line">Date:                Mon, 08 Jun 2020   Pseudo R-squ.:                0.003818</span><br><span class="line">Time:                        01:50:59   Log-Likelihood:                -69.050</span><br><span class="line">converged:                       True   LL-Null:                       -69.315</span><br><span class="line">Covariance Type:            nonrobust   LLR p-value:                    0.4669</span><br><span class="line">==============================================================================</span><br><span class="line">coef    std err          z      P&gt;|z|      [0.025      0.975]</span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">const         -0.0025      0.201     -0.012      0.990      -0.396       0.391</span><br><span class="line">x1             0.1287      0.177      0.726      0.468      -0.219       0.476</span><br><span class="line">==============================================================================</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>Setosa와 versicolor의 구분하는 기준값은 $ 1/1+exp(z)=0.5 $ 인 지점을 찾아야하므로 $ z= -0.0025+0.1287 x $ 이므로 $ x = 0.019425019425019424 $ 에 대한 수직선이 판별함수 경계선이 될 것이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import classification_report, confusion_matrix</span><br><span class="line"></span><br><span class="line">pred_y=np.array([1 <span class="keyword">if</span> x &gt;= 0.5 <span class="keyword">else</span> 0 <span class="keyword">for</span> x <span class="keyword">in</span> list(logit_res.predict(X))])</span><br><span class="line">y=np.array(y)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(confusion_matrix(y, pred_y))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(classification_report(y, pred_y))</span><br></pre></td></tr></table></figure>
<h6 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[[27 23]</span><br><span class="line"> [23 27]]</span><br><span class="line"></span><br><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.54      0.54      0.54        50</span><br><span class="line">           1       0.54      0.54      0.54        50</span><br><span class="line"></span><br><span class="line">    accuracy                           0.54       100</span><br><span class="line">   macro avg       0.54      0.54      0.54       100</span><br><span class="line">weighted avg       0.54      0.54      0.54       100</span><br></pre></td></tr></table></figure>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line"><span class="built_in">print</span>(roc_auc_score(y,logit_res.predict(X)))</span><br></pre></td></tr></table></figure>
<h6 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">0.5327999999999999</span><br></pre></td></tr></table></figure>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import roc_curve</span><br><span class="line">fpr, tpr, thresholds = roc_curve(y, logit_res.predict(X))</span><br><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">plt.plot(fpr, tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/one_variable_fpr_tpr_roc_curve.png" alt="Roc curve"></p>
<h4 id="로지스틱-회귀를-사용한-이진-분류의-예"><a href="#로지스틱-회귀를-사용한-이진-분류의-예" class="headerlink" title="로지스틱 회귀를 사용한 이진 분류의 예"></a>로지스틱 회귀를 사용한 이진 분류의 예</h4><ul>
<li><p>다음 데이터는 미국 의대생의 입학관련 데이터이다. 데이터의 의미는 다음과 같다.</p>
</li>
<li><p><code>Acceptance</code> : 0이면 불합격, 1이면 합격</p>
</li>
<li><code>BCPM</code> : Bio/Chem/Physics/Math 과목의 학점 평균</li>
<li><code>GPA</code> : 전체과목 학점 평균</li>
<li><code>VR</code> : MCAT Verbal reasoning 과목 점수</li>
<li><code>PS</code> : MCAT Physical sciences 과목 점수</li>
<li><code>WS</code> : MCAT Writing sample 과목 점수</li>
<li><code>BS</code> : MCAT Biological science 과목 점수</li>
<li><code>MCAT</code> : MCAT 총점</li>
<li><code>Apps</code> : 의대 지원 횟수</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_med = sm.datasets.get_rdataset(<span class="string">"MedGPA"</span>, package=<span class="string">"Stat2Data"</span>)</span><br><span class="line">df_med = data_med.data</span><br><span class="line">df_med.tail()</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>일단 학점(GPA)과 합격여부의 관계를 살펴보자.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12,8))</span><br><span class="line">sns.stripplot(x=<span class="string">"GPA"</span>, y=<span class="string">"Acceptance"</span>, data=df_med,</span><br><span class="line">              jitter=True, orient=<span class="string">'h'</span>, order=[1, 0])</span><br><span class="line">plt.grid(True)</span><br><span class="line"><span class="comment"># plt.savefig('MCAT_data_target')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/MCAT_data_target.png" alt="합격여부에 따른 학점의 분포"></p>
<ul>
<li>로지스틱 회귀분석을 실시한다. <code>MCAT = VR + PS + WS + BS</code> 이므로 <code>MCAT</code>은 독립 변수에서 제외해야 한다. 데이터 행렬이 <code>선형독립이어야 하기 때문</code>이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_med = sm.Logit.from_formula(<span class="string">"Acceptance ~ Sex + BCPM + GPA + VR + PS + WS + BS + Apps"</span>, df_med)</span><br><span class="line">result_med = model_med.fit()</span><br><span class="line"><span class="built_in">print</span>(result_med.summary())</span><br></pre></td></tr></table></figure>
<h6 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.</span><br><span class="line">         Current <span class="keyword">function</span> value: 0.280736</span><br><span class="line">         Iterations 9</span><br><span class="line">                           Logit Regression Results                           </span><br><span class="line">==============================================================================</span><br><span class="line">Dep. Variable:             Acceptance   No. Observations:                   54</span><br><span class="line">Model:                          Logit   Df Residuals:                       45</span><br><span class="line">Method:                           MLE   Df Model:                            8</span><br><span class="line">Date:                Mon, 08 Jun 2020   Pseudo R-squ.:                  0.5913</span><br><span class="line">Time:                        02:56:41   Log-Likelihood:                -15.160</span><br><span class="line">converged:                       True   LL-Null:                       -37.096</span><br><span class="line">Covariance Type:            nonrobust   LLR p-value:                 6.014e-07</span><br><span class="line">==============================================================================</span><br><span class="line">                 coef    std err          z      P&gt;|z|      [0.025      0.975]</span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">Intercept    -46.6414     15.600     -2.990      0.003     -77.216     -16.067</span><br><span class="line">Sex[T.M]      -2.2835      1.429     -1.597      0.110      -5.085       0.518</span><br><span class="line">BCPM          -6.1633      6.963     -0.885      0.376     -19.811       7.484</span><br><span class="line">GPA           12.3973      8.611      1.440      0.150      -4.479      29.274</span><br><span class="line">VR             0.0790      0.311      0.254      0.799      -0.530       0.688</span><br><span class="line">PS             1.1673      0.539      2.164      0.030       0.110       2.225</span><br><span class="line">WS            -0.7784      0.396     -1.968      0.049      -1.554      -0.003</span><br><span class="line">BS             1.9184      0.682      2.814      0.005       0.582       3.255</span><br><span class="line">Apps           0.0512      0.147      0.348      0.728      -0.237       0.340</span><br><span class="line">==============================================================================</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>예측 결과와 실제 결과를 비교하면 다음과 같다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df_med[<span class="string">"Prediction"</span>] = result_med.predict(df_med)</span><br><span class="line">plt.figure(figsize=(12, 8))</span><br><span class="line">sns.boxplot(x=<span class="string">"Acceptance"</span>, y=<span class="string">"Prediction"</span>, data=df_med)</span><br><span class="line"><span class="comment"># plt.savefig('box_plot_predict_value')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/box_plot_predict_value.png" alt="예측확률값과 실제값의 비교"></p>
<ul>
<li>위 분석 결과를 토대로 유의하지 않은 변수들을 제외하고 PS와 BS 점수만을 이용하여 다시 회귀분석하면 다음과 같다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_med = sm.Logit.from_formula(<span class="string">"Acceptance ~  PS + BS"</span>, df_med)</span><br><span class="line">result_med = model_med.fit()</span><br><span class="line"><span class="built_in">print</span>(result_med.summary())</span><br></pre></td></tr></table></figure>
<h6 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.</span><br><span class="line">         Current <span class="keyword">function</span> value: 0.460609</span><br><span class="line">         Iterations 7</span><br><span class="line">                           Logit Regression Results                           </span><br><span class="line">==============================================================================</span><br><span class="line">Dep. Variable:             Acceptance   No. Observations:                   55</span><br><span class="line">Model:                          Logit   Df Residuals:                       52</span><br><span class="line">Method:                           MLE   Df Model:                            2</span><br><span class="line">Date:                Mon, 08 Jun 2020   Pseudo R-squ.:                  0.3315</span><br><span class="line">Time:                        03:04:00   Log-Likelihood:                -25.333</span><br><span class="line">converged:                       True   LL-Null:                       -37.896</span><br><span class="line">Covariance Type:            nonrobust   LLR p-value:                 3.503e-06</span><br><span class="line">==============================================================================</span><br><span class="line">                 coef    std err          z      P&gt;|z|      [0.025      0.975]</span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">Intercept    -15.5427      4.684     -3.318      0.001     -24.723      -6.362</span><br><span class="line">PS             0.4798      0.316      1.518      0.129      -0.140       1.099</span><br><span class="line">BS             1.1464      0.387      2.959      0.003       0.387       1.906</span><br><span class="line">==============================================================================</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>위 결과를 바탕으로 다음 점수가 $ 15.5427 + 0.5 $ 보다 크면 합격이라고 예측할 수 있다.</li>
</ul>
<script type="math/tex; mode=display">0.4798 \text{PS} + 1.1464 \text{BS}</script><h3 id="로지스틱-회귀를-사용한-회귀분석"><a href="#로지스틱-회귀를-사용한-회귀분석" class="headerlink" title="로지스틱 회귀를 사용한 회귀분석"></a>로지스틱 회귀를 사용한 회귀분석</h3><ul>
<li>로지스틱 회귀는 분류문제뿐만 아니라 종속변수 $ y $ 가 $ 0 $ 부터 $ 1 $ 까지 막혀있는 회귀분석 문제에도 사용할 수 있다. 이 때는 다음처럼 $ \theta $ 값을 종속변수 $ y $ 의 예측값으로 사용한다.</li>
</ul>
<script type="math/tex; mode=display">\hat{y} = \mu(x)</script><ul>
<li><code>만약 실제 $ y $ 의 범위가 0 부터 1이 아니면 스케일링을 통해 바꿔야 한다.</code></li>
</ul>
<blockquote>
<p>예제<br>다음 데이터는 1974년도에 “여성은 가정을 보살피고 국가를 운영하는 일은 남자에게 맡겨두어야 한다”라는 주장에 대한 찬성, 반대 입장을 조사한 결과이다. 각 열은 다음을 뜻한다.</p>
</blockquote>
<ul>
<li><code>education</code> : 교육 기간</li>
<li><code>sex</code> :  성별</li>
<li><code>agree</code> : 찬성 인원</li>
<li><code>disagree</code> : 반대 인원</li>
<li><code>ratio</code> : 찬성 비율</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data_wrole = sm.datasets.get_rdataset(<span class="string">"womensrole"</span>, package=<span class="string">"HSAUR"</span>)</span><br><span class="line">df_wrole = data_wrole.data</span><br><span class="line">df_wrole[<span class="string">"ratio"</span>] = df_wrole.agree / (df_wrole.agree + df_wrole.disagree)</span><br><span class="line">df_wrole.tail()</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>교육을 많이 받는 사람일수록 찬성 비율이 감소하는 것을 볼 수 있다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12, 8))</span><br><span class="line">sns.scatterplot(x=<span class="string">"education"</span>, y=<span class="string">"ratio"</span>, style=<span class="string">"sex"</span>, data=df_wrole)</span><br><span class="line">plt.grid(True)</span><br><span class="line">plt.savefig(<span class="string">'education_ratio_up'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/education_ratio_up.png" alt="교육과 찬성비율과의 관계"></p>
<ul>
<li>분석 결과는 다음과 같다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_wrole = sm.Logit.from_formula(<span class="string">"ratio ~ education + sex"</span>, df_wrole)</span><br><span class="line">result_wrole = model_wrole.fit()</span><br><span class="line"><span class="built_in">print</span>(result_wrole.summary())</span><br></pre></td></tr></table></figure>
<h6 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.</span><br><span class="line">         Current <span class="keyword">function</span> value: 0.448292</span><br><span class="line">         Iterations 6</span><br><span class="line">                           Logit Regression Results                           </span><br><span class="line">==============================================================================</span><br><span class="line">Dep. Variable:                  ratio   No. Observations:                   41</span><br><span class="line">Model:                          Logit   Df Residuals:                       38</span><br><span class="line">Method:                           MLE   Df Model:                            2</span><br><span class="line">Date:                Mon, 08 Jun 2020   Pseudo R-squ.:                  0.3435</span><br><span class="line">Time:                        03:13:11   Log-Likelihood:                -18.380</span><br><span class="line">converged:                       True   LL-Null:                       -27.997</span><br><span class="line">Covariance Type:            nonrobust   LLR p-value:                 6.660e-05</span><br><span class="line">===============================================================================</span><br><span class="line">                  coef    std err          z      P&gt;|z|      [0.025      0.975]</span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">Intercept       2.0442      0.889      2.299      0.022       0.302       3.787</span><br><span class="line">sex[T.Male]    -0.1968      0.736     -0.267      0.789      -1.640       1.247</span><br><span class="line">education      -0.2127      0.071     -2.987      0.003      -0.352      -0.073</span><br><span class="line">===============================================================================</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>성별은 유의하지 않다는 것을 알게되었으므로 성별을 제외하고 다시 모형을 구한다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_wrole2 = sm.Logit.from_formula(<span class="string">"ratio ~ education"</span>, df_wrole)</span><br><span class="line">result_wrole2 = model_wrole2.fit()</span><br><span class="line"><span class="built_in">print</span>(result_wrole2.summary())</span><br></pre></td></tr></table></figure>
<h6 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.</span><br><span class="line">         Current <span class="keyword">function</span> value: 0.449186</span><br><span class="line">         Iterations 6</span><br><span class="line">                           Logit Regression Results                           </span><br><span class="line">==============================================================================</span><br><span class="line">Dep. Variable:                  ratio   No. Observations:                   41</span><br><span class="line">Model:                          Logit   Df Residuals:                       39</span><br><span class="line">Method:                           MLE   Df Model:                            1</span><br><span class="line">Date:                Mon, 08 Jun 2020   Pseudo R-squ.:                  0.3422</span><br><span class="line">Time:                        03:14:51   Log-Likelihood:                -18.417</span><br><span class="line">converged:                       True   LL-Null:                       -27.997</span><br><span class="line">Covariance Type:            nonrobust   LLR p-value:                 1.202e-05</span><br><span class="line">==============================================================================</span><br><span class="line">                 coef    std err          z      P&gt;|z|      [0.025      0.975]</span><br><span class="line">------------------------------------------------------------------------------</span><br><span class="line">Intercept      1.9345      0.781      2.478      0.013       0.405       3.464</span><br><span class="line">education     -0.2117      0.071     -2.983      0.003      -0.351      -0.073</span><br><span class="line">==============================================================================</span><br></pre></td></tr></table></figure>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(12, 8))</span><br><span class="line">sns.scatterplot(x=<span class="string">"education"</span>, y=<span class="string">"ratio"</span>, data=df_wrole)</span><br><span class="line">xx = np.linspace(0, 20, 100)</span><br><span class="line">df_wrole_p = pd.DataFrame(&#123;<span class="string">"education"</span>: xx&#125;)</span><br><span class="line">plt.plot(xx, result_wrole2.predict(df_wrole_p), <span class="string">"r-"</span>, lw=4, label=<span class="string">"예측"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/logistic_regression_using_ratio.png" alt="예측 결과"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h6 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h6><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<hr>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "로지스틱 회귀분석",
        "image": "https://heung-bae-lee.github.io/image/logistic_regression.png",
        "keywords": "",
        "genre": "machine learning",
        "datePublished": "2020-04-03",
        "dateCreated": "2020-04-03",
        "dateModified": "2020-06-09",
        "url": "https://heung-bae-lee.github.io/2020/04/03/machine_learning_05/",
        "description": "로지스틱 회귀분석


로지스틱(Logistic) 회귀분석은 회귀분석이라는 명칭과 달리 회귀분석 문제와 분류문제 모두에 사용할 수 있다. 로지스틱 회귀분석 모형에서는 종속변수가 이항분포를 따르고 그 모수 $ \theta $ 가 독립변수 $ x $ 에 의존한다고 가정한다.

p(y \mid x) = \text{Bin} (y; \theta (x), N)
위 식에"
        "wordCount": 6580
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2020/04/03/machine_learning_06/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            PCA를 이해하기 위한 기본적 선형대수
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2020/03/20/Python_00/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">Python - 00 (Python의 장점 및 자료형)</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/06/09/linear_algebra_05/" class="title">Linear Transformation &amp; onto, ono-to-one의 개념</a></p>
                            <p class="item-date"><time datetime="2020-06-09T05:23:12.000Z" itemprop="datePublished">2020-06-09</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/06/08/linear_algebra_04/" class="title">Linear Independence, Span, and Subspace</a></p>
                            <p class="item-date"><time datetime="2020-06-08T06:52:22.000Z" itemprop="datePublished">2020-06-08</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/06/machine_learning_20/" class="title">Imbalanced Data</a></p>
                            <p class="item-date"><time datetime="2020-06-05T16:52:20.000Z" itemprop="datePublished">2020-06-06</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/04/machine_learning_19/" class="title">Clustering - Hierarchical, DBSCAN, Affinity Propagation</a></p>
                            <p class="item-date"><time datetime="2020-06-04T13:46:15.000Z" itemprop="datePublished">2020-06-04</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/30/machine_learning_18/" class="title">Clustering - K-means, K-medoid</a></p>
                            <p class="item-date"><time datetime="2020-05-29T16:01:30.000Z" itemprop="datePublished">2020-05-30</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">21</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/04/03/machine_learning_05/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
