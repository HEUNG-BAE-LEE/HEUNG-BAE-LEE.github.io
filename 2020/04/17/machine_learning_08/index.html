<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>K-Nearest Neighbors(KNN) | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="K-Nearest Neighbors(KNN) k의 개수만큼 주변에 있는 sample들의 정보를 이용해서 새로운 관측치의 종속변수를 예측하는 방법이다. 아래 그림에서 기존의 파란색 네모와 빨간색 세모라는 2가지 클래스를 갖는 데이터 집합이 있다고 할 때, 여기서 새로운 관측치인 녹색에 대해 어떻게 예측할지를 생각해 보자.  k=3인 경우(실선): 빨간색 세모">
<meta property="og:type" content="article">
<meta property="og:title" content="K-Nearest Neighbors(KNN)">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/04/17/machine_learning_08/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="K-Nearest Neighbors(KNN) k의 개수만큼 주변에 있는 sample들의 정보를 이용해서 새로운 관측치의 종속변수를 예측하는 방법이다. 아래 그림에서 기존의 파란색 네모와 빨간색 세모라는 2가지 클래스를 갖는 데이터 집합이 있다고 할 때, 여기서 새로운 관측치인 녹색에 대해 어떻게 예측할지를 생각해 보자.  k=3인 경우(실선): 빨간색 세모">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/KNN_algorithm_concept.png">
<meta property="og:updated_time" content="2020-04-22T09:05:32.382Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="K-Nearest Neighbors(KNN)">
<meta name="twitter:description" content="K-Nearest Neighbors(KNN) k의 개수만큼 주변에 있는 sample들의 정보를 이용해서 새로운 관측치의 종속변수를 예측하는 방법이다. 아래 그림에서 기존의 파란색 네모와 빨간색 세모라는 2가지 클래스를 갖는 데이터 집합이 있다고 할 때, 여기서 새로운 관측치인 녹색에 대해 어떻게 예측할지를 생각해 보자.  k=3인 경우(실선): 빨간색 세모">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/KNN_algorithm_concept.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/04/17/machine_learning_08/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">
    <link rel="stylesheet" type="text/css" href>
    <link rel="stylesheet" href="https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css">	
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothiccoding.css">
    <link rel="stylesheet" href="/css/style.css">
   
    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/machine-learning/">machine learning</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-machine_learning_08" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        K-Nearest Neighbors(KNN)
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/04/17/machine_learning_08/" class="article-date">
            <time datetime="2020-04-17T09:11:42.000Z" itemprop="datePublished">2020-04-17</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h1 id="K-Nearest-Neighbors-KNN"><a href="#K-Nearest-Neighbors-KNN" class="headerlink" title="K-Nearest Neighbors(KNN)"></a>K-Nearest Neighbors(KNN)</h1><ul>
<li><p><code>k의 개수만큼 주변에 있는 sample들의 정보를 이용해서 새로운 관측치의 종속변수를 예측하는 방법</code>이다. 아래 그림에서 기존의 파란색 네모와 빨간색 세모라는 2가지 클래스를 갖는 데이터 집합이 있다고 할 때, 여기서 새로운 관측치인 녹색에 대해 어떻게 예측할지를 생각해 보자.</p>
<ul>
<li>k=3인 경우(실선): 빨간색 세모로 분류될 가능성이 높다.</li>
<li>k=5인 경우(점선): 파란색 네모로 분류될 가능성이 높다.<ul>
<li>허나, k=5인 경우 빨간색 세모와의 거리가 더 가깝기 때문에 그만큼의 weight를 주어서 갯수를 떠나서 빨간색으로 분류될 가능성도 알고리즘에 따라 존재한다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/image/KNN_algorithm_concept.png" alt="KNN 알고리즘"></p>
<h2 id="K-Nearest-neighborhood"><a href="#K-Nearest-neighborhood" class="headerlink" title="K-Nearest neighborhood"></a>K-Nearest neighborhood</h2><p><img src="/image/what_kinds_of_case_KNN_with_some_k.png" alt="K에 따른 KNN의 결과"></p>
<p><img src="/image/KNN_algorithm_how_to_decide_k.png" alt="파라미터 K의 상황 및 voting 방법"></p>
<p><img src="/image/how_to_decide_target_variable_in_KNN.png" alt="종속변수에 따른 KNN 알고리즘의 결과값"></p>
<ul>
<li>그렇다면 voting 방식외에도 관측치들간의 distance에 따른 가중치를 주는데 여기서 거리를 구하는 방법은 아래 그림과 같은 방법들이 존재한다. 물론, 이 밖에도 데이터의 거리를 구하는 방법은 더 많이 존재한다. 대표적인 유클리디안 거리와 맨하탄 거리, 그리고 범주형 변수에 사용되는 Hamming distance를 간단히 보여 줄 것이다. 유클리디안 거리는 우리가 잘 알고있는 피타고라스 정리에 의해 두 지점 사이의 최단 거리를 구하는 공식으로 계산되며, 맨하탄 거리는 맨하탄 같이 블록별로 되어있는 곳에서의 거리를 계산할 경우 사용된다. 마지막으로 Hamming distance를 계산하는 방식은 Indicator함수안의 조건이 참인 경우만을 1로 값을 산출하여 합한 값이다. 즉, 쉽게 말하면 해당 이진값들의 자리에서 다른 곳이 존재하는 개수를 의미한다.</li>
</ul>
<p><img src="/image/how_to_calculate_distance_for_each_type_of_variables.png" alt="독립변수에 따른 각 관측치들간 거리 계산 방법"></p>
<ul>
<li>먼저 독립 변수와 새로이 예측하려는 관측값과의 거리를 따져 가장 가까운 데이터부터 순서대로 정렬해 놓는다. 통계적으로 순서 통계량이라고 할 수 있다. 그에 따른 순서로 독립변수와 종속 변수의 쌍으로 정렬한다. 여기서의 거리는 위에서 언급했던 방법들을 사용한다.</li>
</ul>
<p><img src="/image/KNN_order_statistics_and_distance_cal.png" alt="KNN 알고리즘의 순서"></p>
<ul>
<li>종속변수가 범주형이라면 아래와 같이 소프트 보팅방법으로 확률을 구해 확률값이 가장 큰 클래스를 예측값으로 채택한다. 물론, 범주형 변수도 연속형 변수처럼 거리에 의한 가중치를 사용할 수 있다.</li>
</ul>
<p><img src="/image/categorical_variable_method_in_KNN.png" alt="종속변수가 범주형인 경우의 KNN"></p>
<ul>
<li>종속변수가 연속형인 경우는 평균을 구하는데, 거리에 따른 가중치를 두어 가중평균을 구하는 것이 일반적이다. 가중치는 거리에 반비례하게 하여 거리가 짧을수록 큰 가중치를 갖게 한다.</li>
</ul>
<p><img src="/image/numerical_variable_method_in_KNN.png" alt="종속변수가 연속형인 경우의 KNN"></p>
<h3 id="Cross-validation"><a href="#Cross-validation" class="headerlink" title="Cross validation"></a>Cross validation</h3><ul>
<li><p>교차검증(Cross validation)은 기본적으로 과적합에 대한 방지를 위해서 실행하며, 또 다른 이유로는 sample loss에 관한 측면이 존재한다.</p>
</li>
<li><p>과적합(Overfitting)문제는 Training set에만 최적화 되어있어 새로운 데이터가 들어왔을 때 기존의 Training set에 이질적으로 처리하기 때문이다. 이는 모델의 복잡도와도 밀접한 연관이 있다. 모델의 복잡도가 높을수록 Training set은 잘 맞추지만, Test set에 대한 성능은 낮을 가능성이 높다. 그렇다고 너무 간단하게 만들어버려도 Training set과 Test set 모두에 대해 대한 성능이 좋지 않을 것이다.</p>
</li>
<li><p>Training error는 error를 과소추정하는 성향이 있다는 말은 아래 그래프를 살펴보면서 설명하겠다. KNN 모델의 경우 K가 작을수록 모델의 복잡도는 높을 것이다. 가장 복잡한 k=1인 경우 Training error는 거의 0에 수렴한다. 허나, Test error는 상당히 높다. 즉, 과적합이 발생되었다는 이야기이다. 여기서 알 수 있는 것은 <code>Training error로 모델을 선택한다면 너무 복잡한 모델을 선택하여 과적합을 발생시켜 Test error가 커지게 될 시킬 수 있다</code>는 점이다.</p>
</li>
</ul>
<p><img src="/image/overfitting_problem_with_KNN.png" alt="과적합 문제"></p>
<ul>
<li>또한, Test error를 구하기 위해 처음에 Train set, Test set으로 나누어야 하기에 Test set으로 나눈만큼의 데이터 소실로 인해 Test error는 증가하게 된다. 이렇게 Error를 과소추정하지 않기 위해서는 교차검증을 해야 할 것이다.</li>
</ul>
<p><img src="/image/How_to_overcome_overfitting.png" alt="과적합 문제 보완 방안"></p>
<ul>
<li>이러한 과적합을 방지하고자 CV를 실행하는데 그 중 K-hold Cross validation은 다음과 같은 방법으로 진행한다.</li>
</ul>
<p><img src="/image/K_hold_crosvalidation_with_KNN_01.png" alt="K-hold 교차검증 - 01"></p>
<ul>
<li>Loss function은 예로 들어놓은 것인데, 해당 문제의 성능지표를 의미한다.</li>
</ul>
<p><img src="/image/K_hold_crosvalidation_with_KNN_02.png" alt="K-hold 교차검증 - 02"></p>
<h2 id="KNN의-심화적-이해"><a href="#KNN의-심화적-이해" class="headerlink" title="KNN의 심화적 이해"></a>KNN의 심화적 이해</h2><ul>
<li>데이터에 따라 적절한 K가 다른데 아래 그림에서와 같이 너무 K를 작게 설정하면, Train set은 굉장히 잘 설명하지만 Test set에 대해선 성능이 안좋게 되는 과적합이 발생될 수 있다. 그 다음으로는 K=1로 설정했을 경우 이상치와의 거리가 가장 가까운 데이터에 영향을 줄 것이다. 아래의 예시 그래프 처럼 영역이 불연속적이어서 영역을 나누는 의미가 없어 보이는 경우가 발생 될 수 있다. 반대로 K가 너무 크다면 미세한 경계에 분류가 아쉬울 것이다.</li>
</ul>
<p><img src="/image/what_kinds_of_case_KNN_with_some_k.png" alt="모수 K의 결정"></p>
<ul>
<li>데이터에 따라 적절한 K가 다르므로 Test error를 작게하는 k를 선택해야 할 것이다. Cross-validation을 이용하여 보통 모형의 모수들을 조절하게 된다. 그 중 검증 데이터에 대한 성능이 좋은 모수를 채택하여 최종적으로 test 데이터를 예측하는 것이다.</li>
</ul>
<p><img src="/image/K_decision_on_KNN.png" alt="K의 결정"></p>
<ul>
<li>지난 번에 언급했던 차원의 저주와 KNN 알고리즘도 밀접한 관련이 있다. 차원의 저주는 차원이 늘어남에 따라 우리가 설명하고 싶은 공간 대비 설명할 수 있는 공간이 줄어드는 문제인데, 아래 그림에서와 같이 가로축 변수만을 사용한다면 충분히 두 클래스를 나눌 수 있는 기준을 설정할 수 있지만, 오히려 세로축에 의해서는 클래스를 구분하는데 아무런 영향을 주지 않는데 세로축 변수도 고려하게 되면서 다른 예측을 하게 된다.</li>
</ul>
<p><img src="/image/demasion_of_curse_on_knn.png" alt="차원의 저주"></p>
<h2 id="k-Nearest-Neighborhood-Algorithm-실습"><a href="#k-Nearest-Neighborhood-Algorithm-실습" class="headerlink" title="k-Nearest Neighborhood Algorithm 실습"></a>k-Nearest Neighborhood Algorithm 실습</h2><h3 id="1-데이터-모듈-불러오기-및-kNN-피팅-방법"><a href="#1-데이터-모듈-불러오기-및-kNN-피팅-방법" class="headerlink" title="1. 데이터, 모듈 불러오기 및 kNN 피팅 방법"></a>1. 데이터, 모듈 불러오기 및 kNN 피팅 방법</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import modin.pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib.colors import ListedColormap</span><br><span class="line">from sklearn import neighbors, datasets</span><br><span class="line">from sklearn.metrics import confusion_matrix</span><br></pre></td></tr></table></figure>
<ul>
<li>iris 데이터를 사용할 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line">X = iris.data[:, :2]</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure>
<ul>
<li>모델 구축<ul>
<li>neighbors를 5로 설정</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">clf = neighbors.KNeighborsClassifier(n_neighbors=5)</span><br><span class="line">clf.fit(X,y)</span><br><span class="line">y_pred=clf.predict(X)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">confusion_matrix(y,y_pred)</span><br></pre></td></tr></table></figure>
<h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[49,  1,  0],</span><br><span class="line">       [ 0, 38, 12],</span><br><span class="line">       [ 0, 12, 38]])</span><br></pre></td></tr></table></figure>
<h3 id="2-Cross-validation을-활용한-최적의-k찾기"><a href="#2-Cross-validation을-활용한-최적의-k찾기" class="headerlink" title="2.Cross-validation을 활용한 최적의 k찾기"></a>2.Cross-validation을 활용한 최적의 k찾기</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line"></span><br><span class="line">k_range = np.arange(1,100)</span><br><span class="line">k_scores = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_range:</span><br><span class="line">    knn=neighbors.KNeighborsClassifier(k)</span><br><span class="line">    scores= cross_val_score(knn, X, y, cv=10, scoring=<span class="string">"accuracy"</span>)</span><br><span class="line">    k_scores.append(scores.mean())</span><br><span class="line"></span><br><span class="line">plt.plot(k_range, k_scores)</span><br><span class="line">plt.xlabel(<span class="string">'Value of K for KNN'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Cross-validated accuracy'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li>이렇게 적절한 모수를 찾는 방법은 위에서와 같이 직접 grid search를 하는 방법을 주로 사용한다.</li>
</ul>
<p><img src="/image/decision_parameter_through_cross_validation.png" alt="모수 K의 결정"></p>
<h3 id="2-Weight를-준-kNN"><a href="#2-Weight를-준-kNN" class="headerlink" title="2.Weight를 준 kNN"></a>2.Weight를 준 kNN</h3><ul>
<li>거리를 가중치로 사용한 경우가 좀 더 decision boundary가 매끄럽게 연결되어 있다. 좀 더 보편적인 모델인 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">n_neighbors = 40</span><br><span class="line"></span><br><span class="line">h = .02  <span class="comment"># step size in the mesh</span></span><br><span class="line"></span><br><span class="line">cmap_light = ListedColormap([<span class="string">'#FFAAAA'</span>, <span class="string">'#AAFFAA'</span>, <span class="string">'#AAAAFF'</span>])</span><br><span class="line">cmap_bold = ListedColormap([<span class="string">'#FF0000'</span>, <span class="string">'#00FF00'</span>, <span class="string">'#0000FF'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> weights <span class="keyword">in</span> [<span class="string">'uniform'</span>, <span class="string">'distance'</span>]:</span><br><span class="line">    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot the decision boundary. For that, we will assign a color to each</span></span><br><span class="line">    <span class="comment"># point in the mesh [x_min, x_max]x[y_min, y_max].</span></span><br><span class="line">    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),</span><br><span class="line">                         np.arange(y_min, y_max, h))</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Put the result into a color plot</span></span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    plt.figure()</span><br><span class="line">    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Plot also the training points</span></span><br><span class="line">    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,</span><br><span class="line">                edgecolor=<span class="string">'k'</span>, s=20)</span><br><span class="line">    plt.xlim(xx.min(), xx.max())</span><br><span class="line">    plt.ylim(yy.min(), yy.max())</span><br><span class="line">    plt.title(<span class="string">"3-Class classification (k = %i, weights = '%s')"</span></span><br><span class="line">              % (n_neighbors, weights))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/mesh_color_groud_plot_with_KNN.png" alt="거리를 가중치로 사용한 경우와 비교"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(0)</span><br><span class="line">X = np.sort(5 * np.random.rand(40, 1), axis=0)</span><br><span class="line">T = np.linspace(0, 5, 500)[:, np.newaxis]</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line">y[::5] += 1 * (0.5 - np.random.rand(8))</span><br><span class="line"></span><br><span class="line">knn = neighbors.KNeighborsRegressor(n_neighbors)</span><br><span class="line">y_ = knn.fit(X, y).predict(T)</span><br></pre></td></tr></table></figure>
<ul>
<li>이웃을 하나만 사용할 때는 훈련 세트의 각 데이터 포인트가 예측에 주는 영향이 커서 예측값이 훈련 데이터 포인트를 모두 지나간다. 이는 매우 불안정한 예측을 만들어 낸다. 이웃을 많이 사용하면 훈련 데이터에는 잘 안 맞을 수 있지만 더 안정된 예측을 얻게 된다. 또한 Uniform하게 모두 동일한 가중치를 사용해서 계산하는 것보다 때론 거리를 통해 가중치를 주는 편이 아래 그림에서와 같이 좀 더 예측하는 데 효과가 있을수도 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">n_neighbors = 5</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, weights <span class="keyword">in</span> enumerate([<span class="string">'uniform'</span>, <span class="string">'distance'</span>]):</span><br><span class="line">    knn = neighbors.KNeighborsRegressor(n_neighbors, weights=weights)</span><br><span class="line">    y_ = knn.fit(X, y).predict(T)</span><br><span class="line"></span><br><span class="line">    plt.subplot(2, 1, i + 1)</span><br><span class="line">    plt.scatter(X, y, c=<span class="string">'k'</span>, label=<span class="string">'data'</span>)</span><br><span class="line">    plt.plot(T, y_, c=<span class="string">'g'</span>, label=<span class="string">'prediction'</span>)</span><br><span class="line">    plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(<span class="string">"KNeighborsRegressor (k = %i, weights = '%s')"</span> % (n_neighbors,</span><br><span class="line">                                                                weights))</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/regression_of_KNN.png" alt="KNN알고리즘을 통한 회귀"></p>
<h3 id="장단점과-매개변수"><a href="#장단점과-매개변수" class="headerlink" title="장단점과 매개변수"></a>장단점과 매개변수</h3><ul>
<li>일반적으로 KNeighbors 분류기에 중요한 매개변수는 두 가지이다. 데이터 포인트 사이의 거리를 재는 방법과 이웃의 수이다. 실제로 <code>이웃의 수는 3개나 5개 정도로 적을 때 잘 작동</code>하지만, 이 매개변수는 잘 조정해야 합니다.</li>
</ul>
<ul>
<li>k-NN의 장점은 이해하기 매우 쉬운 모델이라는 점이다. 그리고 많이 조정하지 않아도 자주 좋은 성능을 발휘한다. 더 복잡한 알고리즘을 적용해보기 전에 시도해볼 수 있는 좋은 시작점이다. 보통 최근접 이웃 모델은 매우 빠르게 만들 수 있지만, 훈련 세트가 매우 크면 (특성의 수나 샘플의 수가 클 경우) 예측이 느려진다. k-NN 알고리즘을 사용할 땐 <code>데이터를 전처리하는 과정이 중요</code>하다. 그리고 <code>(수백 개 이상의) 많은 특성을 가진 데이터셋에는 잘 동작하지 않으며, 특성 값 대부분이 0인 (즉 희소한) 데이터셋과는 특히 잘 작동하지 않는다.</code></li>
</ul>
<ul>
<li>k-최근접 이웃 알고리즘이 <code>이해하긴 쉽지만, 예측이 느리고 많은 특성을 처리하는 능력이 부족해 현업에서는 잘 쓰지 않는다.</code></li>
</ul>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "K-Nearest Neighbors(KNN)",
        "image": "https://heung-bae-lee.github.io/image/KNN_algorithm_concept.png",
        "keywords": "",
        "genre": "machine learning",
        "datePublished": "2020-04-17",
        "dateCreated": "2020-04-17",
        "dateModified": "2020-04-22",
        "url": "https://heung-bae-lee.github.io/2020/04/17/machine_learning_08/",
        "description": "K-Nearest Neighbors(KNN)
k의 개수만큼 주변에 있는 sample들의 정보를 이용해서 새로운 관측치의 종속변수를 예측하는 방법이다. 아래 그림에서 기존의 파란색 네모와 빨간색 세모라는 2가지 클래스를 갖는 데이터 집합이 있다고 할 때, 여기서 새로운 관측치인 녹색에 대해 어떻게 예측할지를 생각해 보자.

k=3인 경우(실선): 빨간색 세모"
        "wordCount": 1665
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2020/04/19/machine_learning_09/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            LDA, QDA
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2020/04/14/machine_learning_07/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">나이브 베이즈 분류모형</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/06/machine_learning_20/" class="title">Imbalanced Data</a></p>
                            <p class="item-date"><time datetime="2020-06-05T16:52:20.000Z" itemprop="datePublished">2020-06-06</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/04/machine_learning_19/" class="title">Clustering - Hierarchical, DBSCAN, Affinity Propagation</a></p>
                            <p class="item-date"><time datetime="2020-06-04T13:46:15.000Z" itemprop="datePublished">2020-06-04</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/30/machine_learning_18/" class="title">Clustering - K-means, K-medoid</a></p>
                            <p class="item-date"><time datetime="2020-05-29T16:01:30.000Z" itemprop="datePublished">2020-05-30</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/29/machine_learning_17/" class="title">Clustering</a></p>
                            <p class="item-date"><time datetime="2020-05-28T18:17:21.000Z" itemprop="datePublished">2020-05-29</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/27/machine_learning_16/" class="title">Ensemble Learning - Ensemble의 Ensemble</a></p>
                            <p class="item-date"><time datetime="2020-05-26T17:04:18.000Z" itemprop="datePublished">2020-05-27</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">21</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/04/17/machine_learning_08/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
</body>
</html>
