<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>Support Vector Machine(SVM) - 01 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="Support Vector Machine(SVM) 데이터의 분포가 정규분포를 띈다고 보이지 않는다면 이전에 말했던 LDA나 QDA를 사용하기 힘들다. 이런 경우 클래스간 분류를 하려고 할 때 사용될 수 있는 방법 중 하나가 SVM이다. 아래 그림과 같이 클래스 집단을 분류할 수 있는 벡터를 기준으로 최대한의 마진을 주어 분류하는 방식이다.    아래 그림과">
<meta property="og:type" content="article">
<meta property="og:title" content="Support Vector Machine(SVM) - 01">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/04/22/machine_learning_11/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="Support Vector Machine(SVM) 데이터의 분포가 정규분포를 띈다고 보이지 않는다면 이전에 말했던 LDA나 QDA를 사용하기 힘들다. 이런 경우 클래스간 분류를 하려고 할 때 사용될 수 있는 방법 중 하나가 SVM이다. 아래 그림과 같이 클래스 집단을 분류할 수 있는 벡터를 기준으로 최대한의 마진을 주어 분류하는 방식이다.    아래 그림과">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/background_of_svm_concept_01.png">
<meta property="og:updated_time" content="2020-05-26T18:26:26.961Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Support Vector Machine(SVM) - 01">
<meta name="twitter:description" content="Support Vector Machine(SVM) 데이터의 분포가 정규분포를 띈다고 보이지 않는다면 이전에 말했던 LDA나 QDA를 사용하기 힘들다. 이런 경우 클래스간 분류를 하려고 할 때 사용될 수 있는 방법 중 하나가 SVM이다. 아래 그림과 같이 클래스 집단을 분류할 수 있는 벡터를 기준으로 최대한의 마진을 주어 분류하는 방식이다.    아래 그림과">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/background_of_svm_concept_01.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/04/22/machine_learning_11/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">
    <link rel="stylesheet" type="text/css" href>
    <link rel="stylesheet" href="https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css">	
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothiccoding.css">
    <link rel="stylesheet" href="/css/style.css">
   
    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/machine-learning/">machine learning</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-machine_learning_11" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        Support Vector Machine(SVM) - 01
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/04/22/machine_learning_11/" class="article-date">
            <time datetime="2020-04-22T09:04:12.000Z" itemprop="datePublished">2020-04-22</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h1 id="Support-Vector-Machine-SVM"><a href="#Support-Vector-Machine-SVM" class="headerlink" title="Support Vector Machine(SVM)"></a>Support Vector Machine(SVM)</h1><ul>
<li>데이터의 분포가 정규분포를 띈다고 보이지 않는다면 이전에 말했던 LDA나 QDA를 사용하기 힘들다. 이런 경우 클래스간 분류를 하려고 할 때 사용될 수 있는 방법 중 하나가 SVM이다. 아래 그림과 같이 클래스 집단을 분류할 수 있는 벡터를 기준으로 최대한의 마진을 주어 분류하는 방식이다.</li>
</ul>
<p><img src="/image/background_of_svm_concept_01.png" alt="Support Vector Machine의 배경 - 01"></p>
<ul>
<li>아래 그림과 같이 두 클래스 집단간의 데이터 분포가 혼용되어있다면 어떤 방식으로 접근해야 할까? 두 클래스 집단간의 거리를 최대화하면서 혼용되어있는 데이터들에 대한 error를 적당히 허용하는 선에서 decision boundary를 결정해야 할 것이다.</li>
</ul>
<p><img src="/image/background_of_svm_concept_02.png" alt="Support Vector Machine의 배경 - 02"></p>
<ul>
<li>SVM은 regression 문제도 사용하지만 보통은 범주형 변수에 대한 classification 문제에 많이 사용한다. Support vector regression(SVR)은 최대한 많은 데이터를 margin 안에 포함하고자 하는 것이다. 이에 대해 margin 바깥에 존해하는 데이터에 대해 error를 줄어서 그 error를 최소화하는 방향으로 회귀를 진행한다. 일반적인 Regression은 해당 데이터를 설명할 수 있는 선에 대해 error를 계산하는 방식이라면, SVR은 margin 바깥에 존재하는 데이터들에 대해서만 error를 계산하는 방식이다.</li>
</ul>
<p><img src="/image/background_of_svm_concept_03.png" alt="Support Vector Machine의 배경 - 03"></p>
<p><img src="/image/background_of_svm_concept_04.png" alt="Support Vector Machine의 배경 - 04"></p>
<ul>
<li>초평면에 부등호를 도입하면 다음과 같이 영역으로 데이터를 구분지을 수 있게 된다.</li>
</ul>
<p><img src="/image/how_to_make_decision_boundary_with_svm_01.png" alt="Support Vector Machine의 Decision boundary - 01"></p>
<p><img src="/image/how_to_make_decision_boundary_with_svm_02.png" alt="Support Vector Machine의 Decision boundary - 02"></p>
<p><img src="/image/how_to_make_decision_boundary_with_svm_03.png" alt="Support Vector Machine의 Decision boundary - 03"></p>
<h2 id="나그랑주-승수-Lagrange-multiplier"><a href="#나그랑주-승수-Lagrange-multiplier" class="headerlink" title="나그랑주 승수(Lagrange multiplier)"></a>나그랑주 승수(Lagrange multiplier)</h2><ul>
<li>최적화 문제(예를 들어서 극대값이나 극소값)를 푸는데 특정조건하에서 문제를 풀 수 있도록 하는 방법이다. 아래 그림에서 보면, 아무런 제한이 없었다면 x와 y의 값에 따라 $-\infty$에서 $\infty$로 움직일 수 있을 것이다. 허나, $g(x,y)=c$라는 함수 범위 내에서만 움직일 수 있다고 제한을 주면 해당 제한영역하에서의 최적화를 풀어야할 것이다.</li>
</ul>
<p><img src="/image/Lagrange_multiplier_01.png" alt="Lagrange multiplier - 01"></p>
<p><img src="/image/Lagrange_multiplier_02.png" alt="Lagrange multiplier - 02"></p>
<p><img src="/image/Lagrange_multiplier_03.png" alt="Lagrange multiplier - 03"></p>
<ul>
<li>위에서 언급하는 최적화 문제를 수학적으로 살펴보려면, 아래와 같이 최적화문제에 대한 설명이 필요하다.</li>
</ul>
<h3 id="제한조건이-있는-최적화-문제"><a href="#제한조건이-있는-최적화-문제" class="headerlink" title="제한조건이 있는 최적화 문제"></a>제한조건이 있는 최적화 문제</h3><ul>
<li>제한조건(constraint)을 가지는 최적화 문제를 풀어본다. 제한 조건은 연립방적식 또는 연립부등식이다. <code>연립방정식 제한조건이 있는 경우에는 라그랑주 승수법을 사용하여 새로운 최적화 문제를 풀어야</code> 한다. <code>연립부등식 제한조건의 경우에는 KKT조건이라는 것을 만족하도록 하는 복잡한 과정을 거쳐야 한다.</code></li>
</ul>
<h4 id="등식-제한조건이-있는-최적화-문제"><a href="#등식-제한조건이-있는-최적화-문제" class="headerlink" title="등식 제한조건이 있는 최적화 문제"></a>등식 제한조건이 있는 최적화 문제</h4><ul>
<li>현실의 최적화 문제에서는 여러가지 제한조건이 있는 최적화(constrained optimization) 문제가 많다. 가장 간단한 경우는 다음과 같이 연립방정식 제한조건이 있는 경우다. 등식(equality)제한 조건이라고도 한다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} x^{\ast} = \text{arg} \min_x f(x) \end{align}</script><script type="math/tex; mode=display">\begin{align} x \in \mathbf{R}^N \end{align}</script><script type="math/tex; mode=display">\begin{align} g_j(x) = 0 \;\; (j=1, \ldots, M) \end{align}</script><ul>
<li>첫 번째 식만 보면 단순히 목적함수 $f(x)$를 가장 작게 하는 N차원 벡터 x값을 찾는 문제다. 하지만 마지막 식에 있는 M개의 등식 제한 조건이 있으면 M개 연립 방정식을 동시에 모두 만족시키면서 목적함수 $f(x)$를 가장 작게하는 $x$값을 찾아야 한다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \begin{aligned} g_1(x) &= 0 \\ g_2(x) &= 0 \\ &\vdots \\ g_M(x) &= 0 \\ \end{aligned} \end{align}</script><h5 id="예제"><a href="#예제" class="headerlink" title="예제"></a>예제</h5><blockquote>
<p>목적 함수 $f$와 등식 제한조건 $g$가 다음과 같은 경우를 생각하자. 이 문제는 다음 그림 처럼 $g(x_{1}, x_{2}) = 0$으로 정의되는 직선상에서 가장 $f(x_{1},x_{2})$값이 작아지는 점 $(x_1^{\ast}, x_2^{\ast})$을 찾는 문제가 된다.</p>
</blockquote>
<script type="math/tex; mode=display">\begin{align} f(x_1, x_2) = x_1^2 + x_2^2 \end{align}</script><script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = x_1 + x_2 - 1 = 0 \end{align}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 목적함수 f(x) = x1^2 + x2^2</span></span><br><span class="line">def f1(x1, x2):</span><br><span class="line">    <span class="built_in">return</span> x1 ** 2 + x2 ** 2</span><br><span class="line"></span><br><span class="line">x1 = np.linspace(-5, 5, 100)</span><br><span class="line">x2 = np.linspace(-3, 3, 100)</span><br><span class="line">X1, X2 = np.meshgrid(x1, x2)</span><br><span class="line">Y = f1(X1, X2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 등식 제한조건 방정식 g(x) = x1 + x2 - 1 = 0</span></span><br><span class="line">x2_g = 1 - x1</span><br><span class="line"></span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>, levels=[0.5, 2, 8, 32])</span><br><span class="line">plt.plot(x1, x2_g, <span class="string">'g-'</span>)</span><br><span class="line"></span><br><span class="line">plt.plot([0], [0], <span class="string">'rP'</span>)</span><br><span class="line">plt.plot([0.5], [0.5], <span class="string">'ro'</span>, ms=10)</span><br><span class="line"></span><br><span class="line">plt.xlim(-5, 5)</span><br><span class="line">plt.ylim(-3, 3)</span><br><span class="line">plt.xticks(np.linspace(-4, 4, 9))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"등식 제한조건이 있는 최적화 문제"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Lagrange_multiplier_optimization_problem.png" alt="최적화 문제 예시"></p>
<h4 id="라그랑주-승수법"><a href="#라그랑주-승수법" class="headerlink" title="라그랑주 승수법"></a>라그랑주 승수법</h4><ul>
<li>이렇게 등식 제한조건이 있는 최적화 문제는 라그랑주 승수법(Lagrange multiplier)을 사용하여 최적화할 수 있다. 라그랑주 승수 방법에서는 목적함수를 원래의 목적함수 $f(x)$를 사용하지 않는다. 대신 제한조건 등식에 $\lambda$라는 새로운 변수를 곱해서 더한 함수를 목적함수로 간주하여 최적화한다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \begin{aligned} h(x, \lambda) &= h(x_1, x_2, \ldots , x_N, \lambda_1, \ldots , \lambda_M) \\ &= f(x) + \sum_{j=1}^M \lambda_j g_j(x) \end{aligned} \end{align}</script><ul>
<li>이때 제한조건 등식 하나마다 새로운 $\lambda_{i}$를 추가해주어야 한다. 따라서 만약 제한조건이 $M$개이면 $\lambda_{1}, \cdots, \lambda_{M}$개의 변수가 새로 생긴 것과 같다. 이렇게 확장된 목적함수 $h$는 입력변수가 더 늘어났기 때문에 그레디언트 벡터를 영벡터로 만드는 최적화 필요 조건이 다음처럼 $N\;+\;M$개가 된다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= \dfrac{\partial f}{\partial x_1} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_1} = 0 \\ \dfrac{\partial h}{\partial x_2} &= \dfrac{\partial f}{\partial x_2} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_2} = 0 \\ & \vdots  \\ \dfrac{\partial h}{\partial x_N} &= \dfrac{\partial f}{\partial x_N} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_N} = 0 \\ \dfrac{\partial h}{\partial \lambda_1} &= g_1 = 0 \\ & \vdots  \\ \dfrac{\partial h}{\partial \lambda_M} &= g_M = 0 \end{aligned} \end{align}</script><ul>
<li>이 $N\;+\;M$개의 연립 방정식을 풀면 $N\;+\;M$개의 미지수를 구할 수 있다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} x_1, x_2, \ldots, x_N, , \lambda_1, \ldots , \lambda_M \end{align}</script><ul>
<li>구한 결과에서 찾는 최소값 $x$를 구할 수 있다. 라그랑주 승수값은 필요없다.</li>
</ul>
<blockquote>
<p>예제) 위에서 제시한 예제를 라그랑주 승수법으로 풀어보자. 새로운 목적함수는 다음과 같다.</p>
</blockquote>
<script type="math/tex; mode=display">\begin{align} h(x_1, x_2, \lambda) = f(x_1, x_2) + \lambda g(x_1, x_2) = x_1^2 + x_2^2 + \lambda ( x_1 + x_2 - 1 )  \end{align}</script><ul>
<li>라그랑주 승수법을 적용하여 그레디언트 벡터가 영벡터인 위치를 구한다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= 2{x_1} + \lambda = 0 \\ \dfrac{\partial h}{\partial x_2} &= 2{x_2} + \lambda = 0 \\ \dfrac{\partial h}{\partial \lambda}  &= x_1 + x_2 - 1 = 0 \end{aligned} \end{align}</script><ul>
<li>방정식의 해는 다음과 같다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} x_1 = x_2 = \dfrac{1}{2}, \;\;\; \lambda = -1 \end{align}</script><blockquote>
<p>연습문제 제한조건이 $x_{1}+x_{2}\; = \; 1$일 때 목적 함수 $f(x) = - log x_{1} - log x_{2} x_{1},x_{2} &gt; 0$ 을 최소화하는 x_{1}, x_{2}값을 라그랑주 승수법으로 계산하라.</p>
</blockquote>
<ul>
<li>위의 문제에서 목저함수는 아래와 같다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} h(x_1, x_2, \lambda) = f(x_1, x_2) + \lambda g(x_1, x_2) = - log x_1 - log x_2 + \lambda ( x_1 + x_2 - 1 ) \end{align}</script><ul>
<li>라그랑주 승수법은 적용하여 그레디언트 벡터가 영벡터인 위치를 구한다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= \lambda x_1 -1 = 0 \\ \dfrac{\partial h}{\partial x_2} &= \lambda x_2 -1 = 0 \\ \dfrac{\partial h}{\partial \lambda} &= x_1 + x_2 - 1 = 0 \end{aligned} \end{align}</script><ul>
<li>위 방정식을 풀면 해는 다음과 같다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} x_1 = x_2 = \dfrac{1}{2}, \;\;\; \lambda = 2 \end{align}</script><h5 id="scipy를-사용하여-등식-제한조건이-있는-최적화-문제-계산하기"><a href="#scipy를-사용하여-등식-제한조건이-있는-최적화-문제-계산하기" class="headerlink" title="scipy를 사용하여 등식 제한조건이 있는 최적화 문제 계산하기"></a>scipy를 사용하여 등식 제한조건이 있는 최적화 문제 계산하기</h5><ul>
<li>scipy의 optimize 서브패키지는 제한조건이 있는 최적화 문제를 푸는 <code>fmin_slsqp()</code>명령을 제공한다. 목적함수와 초기값, 그리고 제한조건 함수의 리스트를 인수로 받는다. 목적함수는 배열인 인수를 받도록 구현되어야 하고 제한조건 함수의 경우에는 항상 <code>eqcons</code>인수를 명시해야 한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fmin_slsqp(func_objective, x0, eqcons=[func_constraint1, func_constraint2])</span><br></pre></td></tr></table></figure>
<ul>
<li>위에서의 두 문제를 scipy를 통해서 풀어보겠다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def f1array(x):</span><br><span class="line">    <span class="built_in">return</span> x[0] ** 2 + x[1] ** 2</span><br><span class="line"></span><br><span class="line">def eq_constraint(x):</span><br><span class="line">    <span class="built_in">return</span> x[0] + x[1] - 1</span><br><span class="line"></span><br><span class="line">scipy.optimize.fmin_slsqp(f1array, np.array([1, 1]), eqcons=[eq_constraint])</span><br></pre></td></tr></table></figure>
<h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.    (Exit mode 0)</span><br><span class="line">            Current <span class="keyword">function</span> value: 0.5000000000000002</span><br><span class="line">            Iterations: 2</span><br><span class="line">            Function evaluations: 8</span><br><span class="line">            Gradient evaluations: 2</span><br><span class="line">array([0.5, 0.5])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def f1array(x):</span><br><span class="line">    <span class="built_in">return</span> -np.log(x[0]) -np.log(x[1])</span><br><span class="line"></span><br><span class="line">def eq_constraint(x):</span><br><span class="line">    <span class="built_in">return</span> x[0] + x[1] - 1</span><br><span class="line"></span><br><span class="line">scipy.optimize.fmin_slsqp(f1array, np.array([1, 1]), eqcons=[eq_constraint])</span><br></pre></td></tr></table></figure>
<h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.    (Exit mode 0)</span><br><span class="line">            Current <span class="keyword">function</span> value: 1.3862943611198901</span><br><span class="line">            Iterations: 2</span><br><span class="line">            Function evaluations: 8</span><br><span class="line">            Gradient evaluations: 2</span><br><span class="line">array([0.5, 0.5])</span><br></pre></td></tr></table></figure>
<h3 id="라그랑주-승수의-의미"><a href="#라그랑주-승수의-의미" class="headerlink" title="라그랑주 승수의 의미"></a>라그랑주 승수의 의미</h3><ul>
<li>만약 최적화 문제에서 등식 제한조건 $g_{i}가 있는가 없는가에 따라 해의 값이 달라진다면 이 등식 제한조건에 대응하는 라그랑주 승수 $\lambda_{i}$는 0이 아닌 값이어야 한다. $\lambda_{i} = 0$일 때만 원래의 문제와 제한조건이 있는 문제의 최적화 조건이 같아지므로 최적화 해의 위치도 같게 나오기 때문이다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \lambda_i \neq 0 \end{align}</script><h4 id="예제-1"><a href="#예제-1" class="headerlink" title="예제"></a>예제</h4><ul>
<li>목적함수가 아래와 같은 최소화 문제의 답은 $x_{1} = x_{2} = 0$이다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} f(x) = x_1^2 + x_2^2 \end{align}</script><ul>
<li>여기에 다음 제한 조건이 있다고 하자.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = x_1 + x_2 = 0 \end{align}</script><ul>
<li>라그랑주 승수법에서 새로운 목적함수는 아래와 같다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} h(x_1, x_2, \lambda) = f(x_1, x_2) + \lambda g(x_1, x_2) = x_1^2 + x_2^2 + \lambda ( x_1 + x_2) \end{align}</script><ul>
<li>이에 따른 최적화 조건은 다음과 같다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \begin{aligned} \dfrac{\partial h}{\partial x_1} &= 2{x_1} + \lambda = 0 \\ \dfrac{\partial h}{\partial x_2} &= 2{x_2} + \lambda = 0 \\ \dfrac{\partial h}{\partial \lambda} &= x_1 + x_2 = 0 \end{aligned} \end{align}</script><ul>
<li>이에 대한 해는 $x_{1} = x_{2} = \lambda = 0$으로 <code>제한조건이 있으나 없으나 해는 동일하며, 라그랑주 승수는 0이 된다.</code> 즉, 제한조건이 의미가 없는 경우는 라그랑주 승수가 0이된다는 의미이다.</li>
</ul>
<h3 id="부등식-제한조건이-있는-최적화-문제"><a href="#부등식-제한조건이-있는-최적화-문제" class="headerlink" title="부등식 제한조건이 있는 최적화 문제"></a>부등식 제한조건이 있는 최적화 문제</h3><ul>
<li>이번에는 다음과 같이 부등식(inequality) 제한조건이 있는 최적화 문제를 생각하자.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} x^{\ast} = \text{arg} \min_x f(x) \end{align}</script><script type="math/tex; mode=display">\begin{align} x \in \mathbf{R}^N \end{align}</script><script type="math/tex; mode=display">\begin{align} g_j(x) \leq 0 \;\; (j=1, \ldots, M) \end{align}</script><ul>
<li>만약 부등식이 $g_j(x) \geq 0$과 같다면 양변에 $-1$을 곱하여 부등호의 방향을 바꾼다. 이렇게 부등식 제한조건이 있는 최적화 문제도 라그랑주 승수 방법과 목적함수를 다음처럼 바꾸어 푼다. 이렇게 부등식 제한 조건이 있는 최적화 문제도 라그랑주 승수 방법과 목적함수를 다음처럼 바꾸어 푼다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} h(x, \lambda) = f(x) + \sum_{j=1}^M \lambda_j g_j(x) \end{align}</script><h5 id="다만-이-경우-최적화-해의-필요조건은-방정식-제한조건이-있는-최적화-문제와-다르게-KKT-Karush-Kuhn-Tucker-조건이라고-하며-다음처럼-3개의-조건으로-이루어진다"><a href="#다만-이-경우-최적화-해의-필요조건은-방정식-제한조건이-있는-최적화-문제와-다르게-KKT-Karush-Kuhn-Tucker-조건이라고-하며-다음처럼-3개의-조건으로-이루어진다" class="headerlink" title="다만, 이 경우 최적화 해의 필요조건은 방정식 제한조건이 있는 최적화 문제와 다르게 KKT(Karush-Kuhn-Tucker)조건이라고 하며 다음처럼 3개의 조건으로 이루어진다."></a>다만, 이 경우 최적화 해의 필요조건은 방정식 제한조건이 있는 최적화 문제와 다르게 <code>KKT(Karush-Kuhn-Tucker)조건</code>이라고 하며 다음처럼 3개의 조건으로 이루어진다.</h5><ul>
<li>1) 모든 독립변수 $x_{1}, x_{2}, \ldots, \x_{N}$에 대한 미분값이 0이다.<ul>
<li>첫 번째 조건은 방정식 제한조건의 경우와 같다. 다만 변수 $x$들에 대한 미분값만 0이어야 한다. 라그랑주 승수 $\lambda$에 대한 미분은 0이 아니어도 된다.</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \dfrac{\partial h(x, \lambda)}{\partial x_i} = 0 \end{align}</script><ul>
<li>2) 모든 라그랑주 승수 $\lambda_{1}, \lambda_{2}, \ldots, \lambda_{M}$과 제한조건 부등식($\lambda$에 대한 미분값)의 곱이 0이다.<ul>
<li>두 번째 조건을 보면 확장된 목적함수를 나그랑주 승수로 미분한 값은 변수 $x$들에 대한 미분값과는 달리 반드시 0이 될 필요는 없다는 것을 알 수 있다. 이렇게 하려면 두 경우가 가능한데 등식 제한조건의 경우처럼 라그랑주 승수 $\lambda$에 대한 미분값이 0이어도 되고 아니면 라그랑주 승수 $\lambda$값 자체가 0이 되어도 된다.</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \lambda_j \cdot \dfrac{\partial h(x, \lambda)}{\partial \lambda_j} = \lambda_j \cdot g_j = 0  \end{align}</script><ul>
<li>3) 라그랑주 승수는 음수가 아니어야 한다.<ul>
<li>마지막 조건은 KKT 조건이 실제로 부등식 제한조건이 있는 최적화 문제와 같은 문제임을 보장하는 조건이다.</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \lambda_j \geq 0 \end{align}</script><h4 id="예제-2"><a href="#예제-2" class="headerlink" title="예제"></a>예제</h4><ul>
<li>부등식 제한조건을 가지는 최적화의 예를 풀어보자. 목적함수는 아래와 같다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} f(x_1, x_2) = x_1^2 + x_2^2 \end{align}</script><ul>
<li>이 예제에서 두 가지 제한 조건을 고려해 볼 텐데 하나는 다음 그림 중 왼쪽 그림처럼 부등식 제한조건이 아래와 같은 경우이다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = x_1 + x_2 - 1 \leq 0 \end{align}</script><ul>
<li>다른 하나의 제한조건은 아래와 같고 이에 대한 그림은 오른쪽에 해당한다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} g(x_1, x_2) = -x_1 - x_2 + 1 \leq 0 \end{align}</script><ul>
<li>아래 그림에서 제한조건을 만족하는 영역을 어둡게 표시했다. 최적점의 위치는 점으로 표시했다. 첫 번째 제한조건의 경우에는 부등식 제한조건이 있기는 하지만 원래의 최적화 문제의 해가 부등식 제한조건이 제시하는 영역 안에 있기 때문에 최적점의 위치가 달라지지 않는다. 두 번째 제한조건의 경우에는 원래의 최적화 문제의 해가 부등식 제한조건이 제시하는 영역 바깥에 있기 때문에 최적점의 위치가 달라졌다. 하지만 최적점의 위치가 영역의 경계선(boundary line)에 있다는 점에 주의하라.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(13, 7))</span><br><span class="line">ax1 = plt.subplot(121)</span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>, levels=[0.5, 2, 8])</span><br><span class="line">plt.plot(x1, x2_g, <span class="string">'g-'</span>)</span><br><span class="line">ax1.fill_between(x1, -20, x2_g, alpha=0.5)</span><br><span class="line">plt.plot([0], [0], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.xlim(-3, 3)</span><br><span class="line">plt.ylim(-5, 5)</span><br><span class="line">plt.xticks(np.linspace(-4, 4, 9))</span><br><span class="line">plt.yticks(np.linspace(-5, 5, 11))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"최적해가 부등식과 관계없는 경우"</span>)</span><br><span class="line">ax2 = plt.subplot(122)</span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>, levels=[0.5, 2, 8])</span><br><span class="line">plt.plot(x1, x2_g, <span class="string">'g-'</span>)</span><br><span class="line">ax2.fill_between(x1, 20, x2_g, alpha=0.5)</span><br><span class="line">plt.plot([0.5], [0.5], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.xlabel(<span class="string">"x_1"</span>)</span><br><span class="line">plt.xlim(-3, 3)</span><br><span class="line">plt.ylim(-5, 5)</span><br><span class="line">plt.xticks(np.linspace(-4, 4, 9))</span><br><span class="line">plt.yticks(np.linspace(-5, 5, 11))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"최적해가 부등식에 의해 결정되는 경우"</span>)</span><br><span class="line">plt.suptitle(<span class="string">"부등식 제한조건이 있는 최적화 문제"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/inequality_having_constraint_optimization_problem.png" alt="부등식 제한조건이 있는 최적화 문제"></p>
<ul>
<li><p>그림에서 보듯이 부등식 제한조건이 있는 최적화 문제를 풀면 그 제한조건은 다음 두 가지 경우의 하나가 되어 버린다.</p>
<ul>
<li>최적화 결과에 전혀 영향을 주지 않는 <code>쓸모없는</code> 제한조건</li>
<li>최적화 결과에 영향을 주는 <code>등식(equality)</code>인 제한조건</li>
</ul>
</li>
<li><p>어느 경우이든 부등식 제한조건 문제로 시작했지만 결과는 제한조건이 없거나 등식 제한조건 문제를 푸는 것과 같아진다. KKT조건 중 두 번째 조건이 뜻하는 바는 다음과 같다. 다음 식에서 $x^{\ast}, \lambda^{\ast}$는 KKT 조건을 풀어서 구한 최적해의 값이다.</p>
</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \lambda^{\ast} = 0 \;\; \text{or} \;\;  g(x^{\ast}) = 0 \end{align}</script><ul>
<li>만약 $g_{i} = 0$이면 이 조건은 부등식 제한조건이 아닌 등식 제한조건이 된다. 그리고 등식 제한조건에서 말한 바와 같이 (이 제한조건이 있으나 없으나 해가 바뀌지 않는 특수한 경우를 제외하면) 라그랑주 승수는 0이 아닌값을 가진다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} g_i = 0 \;\; \rightarrow \;\; \lambda_i \neq 0 \; (\lambda_i > 0) \end{align}</script><ul>
<li>반대로 $g_i \neq 0 \; (g_i &lt; 0)$이면 해가 $g_{i}$가 표현하는 곡선으로부터 떨어져 있기 때문에 부등식 제한조건이 아무런 의미가 없어진다. 즉, 제한조건이 있을 때와 없을 때의 해가 같다. 따라서 목적함수 $h(x,\lambda)$는 $\lambda_{i}g_{i}(g_{i} \neq 0)$항이 있으나 없으나 상관없이 같은 해를 가진다. 따라서 $\lambda_{i} = 0$이 된다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} g_i \neq 0 \;\; \rightarrow \;\; \lambda_i = 0  \end{align}</script><ul>
<li>따라서 <code>부등식 제한조건이 있는 최적화 문제는 각 제한조건에 대해 위의 두 가지 경우를 가정하여 각각 풀어보면서 최적의 답을 찾는다</code>.</li>
</ul>
<blockquote>
<p>예제) 다음은 복수의 부등식 제한조건이 있는 또다른 2차원 최적화 문제의 예이다.</p>
</blockquote>
<script type="math/tex; mode=display">\begin{align} \text{arg} \min_x \; (x_1-4)^2 + (x_2-2)^2 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_1(x) = x_1 + x_2 - 1\leq 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_2(x) = -x_1 + x_2 - 1\leq 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_3(x) = -x_1 - x_2 - 1\leq 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} g_4(x) = x_1 - x_2 - 1\leq 0 \end{align}</script><ul>
<li>위의 4가지 제한조건은 다음과 같은 하나의 부등식으로 나타낼 수도 있다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} g(x) = \left\vert\, x_1 \right\vert + \left\vert\, x_2 \right\vert - 1 = \sum_{i=1}^{2} \left\vert\, x_i \right\vert - 1 \leq 0 \end{align}</script><ul>
<li><p>아래 예제에서 최적해가 $x_{1}\;=\;1,  x_{2}\; = \;0$이라는 사실을 이용하여 라그랑주 승수 $\lambda_{1}, \lambda_{2}, \lambda_{3}, \lambda_{4}$ 중 어느 값이 0이 되는지 말해보자.</p>
<ul>
<li>이에 대한 답은 $\lambda_{2} = \lambda_{3} = 0$이 될 것이다. 해를 찾는데 아무런 영향을 미치지 않기 때문이다.</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">def f2plt(x1, x2):</span><br><span class="line">    <span class="built_in">return</span> np.sqrt((x1 - 4) ** 2 + (x2 - 2) ** 2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x1 = np.linspace(-2, 5, 100)</span><br><span class="line">x2 = np.linspace(-1.5, 3, 100)</span><br><span class="line">X1, X2 = np.meshgrid(x1, x2)</span><br><span class="line">Y = f2plt(X1, X2)</span><br><span class="line"></span><br><span class="line">plt.contour(X1, X2, Y, colors=<span class="string">"gray"</span>,</span><br><span class="line">            levels=np.arange(0.5, 5, 0.5) * np.sqrt(2))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 제한 조건의 상수</span></span><br><span class="line">k = 1</span><br><span class="line">ax = plt.gca()</span><br><span class="line">x12 = np.linspace(-k, 0, 10)</span><br><span class="line">x13 = np.linspace(0, k, 10)</span><br><span class="line">ax.fill_between(x12, x12 + k, -k - x12, color=<span class="string">'g'</span>, alpha=0.5)</span><br><span class="line">ax.fill_between(x13, x13 - k, k - x13, color=<span class="string">'g'</span>, alpha=0.5)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 최적점 위치</span></span><br><span class="line">x1_sol = 1</span><br><span class="line">x2_sol = 0</span><br><span class="line">plt.plot(x1_sol, x2_sol, <span class="string">'ro'</span>, ms=20)</span><br><span class="line"></span><br><span class="line">plt.xlim(-2, 5)</span><br><span class="line">plt.ylim(-1.5, 3)</span><br><span class="line">plt.xticks(np.linspace(-2, 5, 8))</span><br><span class="line">plt.yticks(np.linspace(-1, 3, 5))</span><br><span class="line">plt.xlabel(<span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.title(<span class="string">"$|x_1| + |x_2| \leq &#123;&#125;$ 제한조건을 가지는 최적화 문제"</span>.format(k))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/inequality_optimization_problem_plot.png" alt="부등식 최적화 문제"></p>
<h4 id="Scipy를-사용하여-부등식-제한조건이-있는-최적화-문제-계산하기"><a href="#Scipy를-사용하여-부등식-제한조건이-있는-최적화-문제-계산하기" class="headerlink" title="Scipy를 사용하여 부등식 제한조건이 있는 최적화 문제 계산하기"></a>Scipy를 사용하여 부등식 제한조건이 있는 최적화 문제 계산하기</h4><ul>
<li><code>fmin_slsqp()</code>명령은 이렇게 부등식 제한조건이 있는 경우에도 사용할 수 있다. 제한조건 인수의 이름이 <code>ieqcons</code>로 달라졌다. 단, <code>ieqcons</code> 인수에 들어가는 부등호는 우리가 지금까지 사용한 방식과 달리 0 또는 양수이어야 한다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} g \geq 0 \end{align}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fmin_slsqp(func_objective, x0, ieqcons=[func_constraint1, func_constraint2])</span><br></pre></td></tr></table></figure>
<ul>
<li>이렇듯, <code>fmin_slsqp()</code> 명령은 등식 제한조건과 부등식 제한조건을 동시에 사용할 수 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def f2(x):</span><br><span class="line">    <span class="built_in">return</span> np.sqrt((x[0] - 4) ** 2 + (x[1] - 2) ** 2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 제한 조건 상수</span></span><br><span class="line">k = 1</span><br><span class="line">def ieq_constraint(x):</span><br><span class="line">    <span class="built_in">return</span> np.atleast_1d(k - np.sum(np.abs(x)))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sp.optimize.fmin_slsqp(f2, np.array([0, 0]), ieqcons=[ieq_constraint])</span><br></pre></td></tr></table></figure>
<h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Optimization terminated successfully.    (Exit mode 0)</span><br><span class="line">            Current <span class="keyword">function</span> value: 3.6055512804550336</span><br><span class="line">            Iterations: 11</span><br><span class="line">            Function evaluations: 77</span><br><span class="line">            Gradient evaluations: 11</span><br><span class="line"></span><br><span class="line">array([9.99999982e-01, 1.79954011e-08])</span><br></pre></td></tr></table></figure>
<h1 id="서포트-벡터-머신"><a href="#서포트-벡터-머신" class="headerlink" title="서포트 벡터 머신"></a>서포트 벡터 머신</h1><ul>
<li>변수가 1개있다면, $x^T$는 N차원을 갖는 헹벡터라면, $\beta$도 같은 차원을 갖는 열벡터로서 내적을 통해 그에 따른 벡터의 영역이 초평면(hyperplane)을 이루게 될 것이다.</li>
</ul>
<p><img src="/image/SVM_conception_hyperplan_01.png" alt="SVM 정의"></p>
<ul>
<li><code>퍼셉트론</code>은 가장 단순하고 빠른 판별 함수 기반 분류 모형이지만 <code>판별 경계선(decision hyperplane)이 유니크하게 존재하지 않는다</code>는 특징이 있다. 서포트 벡터 머신(SVM: Support vector machine)은 퍼셉트론 기반의 모형에 가장 안정적인 판별 경계선을 찾기 위한 제한 조건을 추가한 모형이라고 볼 수 있다.</li>
</ul>
<p><img src="/image/svm_decision_boundary_hyperplane.png" alt="SVM의 decision boundary"></p>
<h3 id="서포트와-마진"><a href="#서포트와-마진" class="headerlink" title="서포트와 마진"></a>서포트와 마진</h3><ul>
<li>다음과 같이 $N$개의 학습용 데이터가 있다고 하자.</li>
</ul>
<script type="math/tex; mode=display">(x_{1}, y_{1}), (x_{2}, y_{2}), \ldots, (x_{i}, y_{i}), \ldots,(x_{N}, y_{N})</script><ul>
<li>판별함수 모형에서 $y$는 $+1,\; -1$ 두 개의 값을 가진다.</li>
</ul>
<script type="math/tex; mode=display">y = \begin{cases} +1 \\ -1 \end{cases}</script><ul>
<li>$x$ 데이터 중에서 $y$값이 $+1$인 데이터를 $x_{+}$, $y$값이 $-1$인 데이터를 $x_{-}$라고 하자. 판별함수 모형에서 직선인 판별 함수 $f(x)$는 다음과 같은 수식으로 나타낼 수 있다.</li>
</ul>
<script type="math/tex; mode=display">f(x) = w^Tx-w_0</script><ul>
<li>혹시라도 판별함수의 직선의 방정식이 어떻게 나온건지 이해가 안가시는 분들에게 설명을 드리고자 잠깐 선형대수의 직선의 방정식에 관한 설명을 하도록 하겠다.</li>
</ul>
<h3 id="직선의-방정식"><a href="#직선의-방정식" class="headerlink" title="직선의 방정식"></a>직선의 방정식</h3><ul>
<li><p>어떤 벡터 $w$가 있을 때</p>
<ul>
<li>원점에서 출발한 벡터 $w$가 가리키는 점을 지나면서</li>
<li>벡터 $w$에 수직인</li>
</ul>
</li>
<li><p>직선의 방정식을 구해보자.</p>
</li>
</ul>
<ul>
<li>위 두 조건을 만족하는 직선상의 임의의 점을 가리키는 벡터를 $x$라고 하면, 벡터 $x$가 가리키는 점과 벡터 $w$가 가리키는 점을 이은 벡터 $x - w$는 조건에 따라 벡터 $w$와 직교해야 한다. 따라서 다음 식이 성립한다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} w^T(x - w) = 0 \end{align}</script><ul>
<li>정리하면 다음과 같아진다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} w^T(x - w) = w^Tx - w^Tw = w^Tx - \| w \|^2 \end{align}</script><script type="math/tex; mode=display">\begin{align} w^Tx - \| w \|^2 = 0 \end{align}</script><ul>
<li>이 직선과 원점 사이의 거리는 벡터 $w$의 norm $|w|$이다.</li>
</ul>
<blockquote>
<p>연습문제) 만약 $v$가 원점을 지나는 직선의 방향을 나타내는 단위벡터라고 하자. 이때 그 직선 위에 있지 않는 어떤 점 $x$와 그 직선과의 거리의 제곱이 다음과 같음을 증명하라.</p>
</blockquote>
<script type="math/tex; mode=display">\begin{align} \| x \|^2 - (x^Tv)^2 \end{align}</script><ul>
<li><p>$ x \; - \; v \perp v$이기 때문에</p>
</li>
<li><p>$a^{\Vert b} = | x | cos \theta = \frac{| v | | x | cos \theta}{| v |} = \frac{x^{T} v}{| v |}$</p>
</li>
<li><p>벡터 $v$는 단위벡터이므로 $a^{\Vert b} = x^{T}v$가 된다. 여기서 피타고라스 정리를 사용하면 우리가 증명해야 하는 식을 구할 수 있다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">v = np.array([2, 1]) / np.sqrt(5)</span><br><span class="line">x = np.array([1, 3])</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=v, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.plot([-2, 8], [-1, 4], <span class="string">'b--'</span>, lw=2)</span><br><span class="line">plt.plot([1, 2], [3, 1], <span class="string">'g:'</span>, lw=2)</span><br><span class="line">plt.plot(x[0], x[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.text(0.1, 0.5, <span class="string">"<span class="variable">$v</span>$"</span>)</span><br><span class="line">plt.text(0.6, 3.2, <span class="string">"<span class="variable">$x</span>$"</span>)</span><br><span class="line">plt.xticks(np.arange(-3, 15))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-3, 7)</span><br><span class="line">plt.ylim(-1, 5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/line_with_distance_dot.png" alt="벡터 v 의 스칼라배한 직선위에 존재하지 않는 점과의 거리"></p>
<ul>
<li>예를 들어 아래와 같을 때</li>
</ul>
<script type="math/tex; mode=display">\begin{align} w = \begin{bmatrix}1 \\ 2\end{bmatrix} \tag{3.1.49} \end{align}</script><script type="math/tex; mode=display">\begin{align} \| w \|^2 = 5 \end{align}</script><script type="math/tex; mode=display">\begin{align} \begin{bmatrix}1 & 2\end{bmatrix} \begin{bmatrix}x_1 \\ x_2 \end{bmatrix} - 5 = x_1 + 2x_2 - 5 = 0 \end{align}</script><script type="math/tex; mode=display">\begin{align} x_1 + 2x_2 = 5 \end{align}</script><ul>
<li>이 방정식은 벡터 $w$가 가리키는 점 (1,2)를 지나면서 벡터 $w$에 수직인 직선을 뜻한다. 이 직선과 원점 사이의 거리는 $ |w|=\sqrt{5} $이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([1, 2])</span><br><span class="line">x1 = np.array([3, 1])</span><br><span class="line">x2 = np.array([-1, 3])</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=w, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x1, xytext=(0, 0), arrowprops=green)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x2, xytext=(0, 0), arrowprops=green)</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.plot(w[0], w[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot(x1[0], x1[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot(x2[0], x2[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot([-3, 5], [4, 0], <span class="string">'r-'</span>, lw=5)</span><br><span class="line">plt.text(-0.2, 1.5, <span class="string">"벡터 <span class="variable">$w</span>$"</span>)</span><br><span class="line">plt.text(1.55, 0.25, <span class="string">"<span class="variable">$x_1</span>$"</span>)</span><br><span class="line">plt.text(-0.9, 1.40, <span class="string">"<span class="variable">$x_2</span>$"</span>)</span><br><span class="line">plt.text(1.8, 1.8, <span class="string">"<span class="variable">$x_1</span> - w$"</span>)</span><br><span class="line">plt.text(-0.2, 2.8, <span class="string">"<span class="variable">$x_2</span> - w$"</span>)</span><br><span class="line">plt.text(3.6, 0.8, <span class="string">"직선 <span class="variable">$x</span>$"</span>)</span><br><span class="line">plt.xticks(np.arange(-2, 5))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-2, 5)</span><br><span class="line">plt.ylim(-0.6, 3.6)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/starting_zero_point_orthogonal_vector_distance.png" alt="원점에서 출발하는 벡터가 가리키는 점을 지나는 수직인 직선과의 거리"></p>
<ul>
<li>이번에는 벡터 $w$가 가리키는 점을 지나야 한다는 조건을 없애고 단순히<ul>
<li>벡터 $w$에 수직인</li>
</ul>
</li>
</ul>
<ul>
<li>직선 $x$의 방정식을 구하면 이때는 직선이 $w$가 아니라 $w$와 방향이 같고 길이가 다른 벡터 $w’=cw$을 지날 것이다. c는 양의 실수이다. 위에서 했던 방법으로 다시 직선의 방정식을 다음과 같다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} w'^Tx - \| w' \|^2 =  cw^Tx - c^2 \| w \|^2 = 0  \end{align}</script><script type="math/tex; mode=display">\begin{align} w^Tx - c \| w \|^2 = 0 \end{align}</script><ul>
<li>여기에서 $c | w |^2$는 임의의 수가 될 수 있으므로 단순히 벡터 $w$에 수직인 직선의 방정식은 다음과 같이 나타낼 수 있다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} w^Tx - w_0 = 0 \end{align}</script><ul>
<li>이 직선과 원점 사이의 거리는 다음과 같다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} c \| w \| = \dfrac{w_0}{\|w\|}  \end{align}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([1, 2])</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=w, xytext=(0, 0), arrowprops=gray)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=0.5 * w, xytext=(0, 0), arrowprops=black)</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.plot(0.5 * w[0], 0.5 * w[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot([-2, 5], [2.25, -1.25], <span class="string">'r-'</span>, lw=5)</span><br><span class="line">plt.text(-0.7, 0.8, <span class="string">"벡터 <span class="variable">$cw</span>$"</span>)</span><br><span class="line">plt.text(-0.1, 1.6, <span class="string">"벡터 <span class="variable">$w</span>$"</span>)</span><br><span class="line">plt.text(1, 1, <span class="string">"직선 <span class="variable">$x</span>$"</span>)</span><br><span class="line">plt.xticks(np.arange(-2, 5))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-2, 5)</span><br><span class="line">plt.ylim(-0.6, 3.6)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/orthogonal_vector_with_w.png" alt="벡터 w에 수직인 직선"></p>
<ul>
<li>예를 들어 $c=0.5$이면 벡터 $w=[1, 2]^T$에 수직이고 원점으로부터의 거리가 $\frac{\sqrt{5}}{2}$인 직선이 된다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} x_1 + 2x_2 - 2.5 = 0 \end{align}</script><h4 id="직선과-점의-거리"><a href="#직선과-점의-거리" class="headerlink" title="직선과 점의 거리"></a>직선과 점의 거리</h4><ul>
<li>이번에는 직선 $w^Tx - |w|^2 = 0$과 이 직선 위에 있지 않은 점 $x’$ 사이의 거리를 구해볼 것이다. 벡터 $w$에 대한 벡터 $x’$의 투영성분 $x’^{\Vert w}$의 길이는 다음과 같다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \|x'^{\Vert w}\| = \dfrac{w^Tx'}{\|w\|} \end{align}</script><ul>
<li>직선과 점 $x’$ 사이의 거리는 이 길이에서 원점에서 직선까지의 거리 $|w|$를 뺀 값의 절대값이다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \left|  \|x'^{\Vert w}\| - \|w\| \right| = \left| \dfrac{w^Tx'}{\|w\|} - \|w\| \right| = \dfrac{\left|w^Tx' - \|w\|^2 \right|}{\|w\|} \end{align}</script><ul>
<li>직선의 방정식이 $w^Tx - w_0 = 0$이면 직선과 점의 거리는 다음과 같다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} \dfrac{\left|w^Tx' - w_0 \right|}{\|w\|} \end{align}</script><ul>
<li><code>이 공식은 아래 내용중 SVM의 판별함수의 직선과 서포트벡터간의 거리를 계산하는데에서 사용</code>된다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">w = np.array([1, 2])</span><br><span class="line">x1 = np.array([4, 3])</span><br><span class="line">x2 = np.array([1, 2]) * 2</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x1, xytext=(0, 0), arrowprops=gray)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=x2, xytext=(0, 0), arrowprops=gray)</span><br><span class="line">plt.annotate(<span class="string">''</span>, xy=w, xytext=(0, 0), arrowprops=red)</span><br><span class="line">plt.plot(0, 0, <span class="string">'kP'</span>, ms=10)</span><br><span class="line">plt.plot(w[0], w[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot(x1[0], x1[1], <span class="string">'ro'</span>, ms=10)</span><br><span class="line">plt.plot([-3, 7], [4, -1], <span class="string">'r-'</span>, lw=5)</span><br><span class="line">plt.plot([2, 4], [4, 3], <span class="string">'k:'</span>, lw=2)</span><br><span class="line">plt.plot([3, 4], [1, 3], <span class="string">'k:'</span>, lw=2)</span><br><span class="line">plt.text(0.1, 0.9, <span class="string">"<span class="variable">$w</span>$"</span>)</span><br><span class="line">plt.text(4.2, 3.1, <span class="string">"<span class="variable">$x</span>'$"</span>)</span><br><span class="line">plt.text(1.5, 2.4, <span class="string">"<span class="variable">$x</span>'^&#123;\Vert w&#125;$"</span>)</span><br><span class="line">plt.xticks(np.arange(-3, 15))</span><br><span class="line">plt.yticks(np.arange(-1, 5))</span><br><span class="line">plt.xlim(-3, 7)</span><br><span class="line">plt.ylim(-1, 5)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/distance_between_dots_and_line.png" alt="점과 직선사이의 거리"></p>
<ul>
<li>다시 SVM 판별함수를 살펴보면 정의에 따라 $y$값이 $+1$인 그 데이터 $x_{+}$에 대한 판별함수 값은 양수가 된다.</li>
</ul>
<script type="math/tex; mode=display">f(x_+) = w^Tx_+ - w_0 > 0</script><ul>
<li>반대로 y값이 -1인 그 데이터 $x_{-}$에 대한 판별함수 값은 음수가 된다.</li>
</ul>
<script type="math/tex; mode=display">f(x_-) = w^Tx_- - w_0 < 0</script><ul>
<li>$y$ 값이  $+1$인 데이터 중에서 판별 함수의 값이 가장 작은 데이터를 $x^{+}$라고 하고 $y$값이 $-1$인 데이터 중에서 판별함수의 값이 가장 큰 데이터를 $x^{-}$라고 하자. 이 데이터들은 각각의 클래스에 속한 데이터 중에서 가장 경계선에 가까이 붙어있는 최전방(most front)의 데이터들이다. 이러한 데이터를 서포트(support) 혹은 서포트 벡터(support vector)라고 한다. 물론 이 서포트에 대해서도 부호 조건은 만족되어야 한다.</li>
</ul>
<script type="math/tex; mode=display">f(x^+) = w^Tx^+ - w_0 > 0</script><script type="math/tex; mode=display">f(x^-) = w^Tx^- - w_0 < 0</script><ul>
<li>서포트에 대한 판별 함수의 값 $f(x^{+}), f(x^{-})$값은 부호 조건만 지키면 어떤 값이 되어도 괜찮다, 따라서 다음과 같은 조건을 만족하도록 판별 함수를 구한다.</li>
</ul>
<script type="math/tex; mode=display">f(x^+) = w^T x^{+} - w_0 = +1</script><script type="math/tex; mode=display">f(x^-) = w^T x^{-} - w_0 = -1</script><p><img src="/image/decision_function_value_of_SVM.png" alt="Support vector의 판별함수 값"></p>
<ul>
<li>이렇게 되면 모든 Support vector $(x_{+}, x_{-})$  데이터들에 대한 판별함수의 값의 절대값이 1보다 커지므로 다음 부등식이 성립한다.</li>
</ul>
<script type="math/tex; mode=display">w^Tx_+ - w_o \geq 1</script><script type="math/tex; mode=display">w^Tx_- - w_o \leq -1</script><ul>
<li>판별 경계선 $w^{T}x -  w_{0} = 0$과 점 $x^{+}, x^{-}$ 사이의 거리는 다음과 같이 계산할 수 있다.</li>
</ul>
<script type="math/tex; mode=display">\dfrac{w^T x^{+} - w_0}{\| w \|} = \dfrac{1}{\| w \|}</script><script type="math/tex; mode=display">-\dfrac{w^T x^{-} - w_0}{\| w \|} = \dfrac{1}{\| w \|}</script><ul>
<li>이 거리의 합을 마진(margin)이라고 하며 마진값이 클 수록 더 경계선이 안정적이라고 볼 수 있다. 그런데 위에서 정한 스케일링에 의해 마진은 다음과 같이 정리된다.</li>
</ul>
<script type="math/tex; mode=display">\dfrac{w^T x^{+} - w_0}{\| w \|}  -\dfrac{w^T x^{-} - w_0}{\| w \|} = \dfrac{2}{\| w \|}</script><ul>
<li>마진 값이 최대가 되는 경우는 $| w |$ 즉, $| w |^{2}$가 최소가 되는 경우와 같다. 다음과 같은 목적함수를 최소화하면 된다.</li>
</ul>
<script type="math/tex; mode=display">L = \dfrac{1}{2} ||w||^2 = \dfrac{1}{2} w^T w</script><ul>
<li>또한 모든 표본 데이터에 대해 분류는 제대로 되어야 하므로 모든 데이터 $x_{i}, y_{i} (i = 1,\ldots, N)$에 대해 다음 조건을 만족해야 한다. 위에서 스케일링을 사용하여 모든 데이터에 대해 $f(x_i) = w^Tx_i - w_o$가 $1$보다 크거나 $-1$보다 작게 만들었다는 점을 이용한다.</li>
</ul>
<script type="math/tex; mode=display">y_i \cdot f(x_i) = y_i \cdot( w^Tx_i - w_o) \geq 1 \;\;\; ( i = 1, \ldots, N )</script><script type="math/tex; mode=display">y_i \cdot ( w^Tx_i - w_o) - 1 \geq 0 \;\;\; ( i = 1, \ldots, N )</script><ul>
<li><code>라그랑주 승수법을 사용하면 최소화 목적함수를 다음과 같이 고치면 된다.</code> 즉, 위의 조건을 만족하는 w의 최소화 문제를 푸는 것과 같게 된다. $a_{i}$은 각각의 부등식에 대한 라그랑주 승수이다. 이 최적화 문제를 풀어 $w, w_{0}, a$를 구하면 판별함수를 얻을 수 있다.</li>
</ul>
<script type="math/tex; mode=display">L = \dfrac{1}{2} w^T w - \sum_{i=1}^N a_i \{ y_i \cdot ( w^Tx_i - w_o) - 1 \}</script><ul>
<li>KKT(Karush-Kuhn-Tucker) 조건에 따르면 부등식 제한 조건이 있는 경우에는 등식 제한조건을 가지는 라그랑주 승수 방법과 비슷하지만 $i$번째 부등식이 있으나 없으나 답이 같은 경우에는 해당 라그랑주 승수의 값이 $a_{i}=0$이 된다. 이 경우는 판별함수의 값 $w^Tx_i - w_o$이 $-1$보다 작거나 1보다 큰 경우이다.<br>즉, <code>마진안에 포함되지 않고 바깥에 있는 데이터들 같은 경우는 해당 조건식이 해를 찾는데 영향을 주지 않아 등식이 있는 최적화 문제를 푸는 것과 같다는 의미</code>이다.</li>
</ul>
<script type="math/tex; mode=display">y_i(w^Tx_i - w_o) - 1  > 0</script><ul>
<li>학습 데이터 중에서 최전방 데이터인 서포트 벡터가 아닌 모든 데이터들에 대해서는 이 조건이 만족되므로 서포트 벡터가 아닌 데이터는 라그랑지 승수가 $0$이라는 것을 알 수 있다.</li>
</ul>
<script type="math/tex; mode=display">a_i = 0 \;\; \text{if} \;\; x_i \notin \{ x^{+}, x^{-} \}</script><h3 id="듀얼-형식"><a href="#듀얼-형식" class="headerlink" title="듀얼 형식"></a>듀얼 형식</h3><ul>
<li>최적화 조건은 목적함수 $L$을 $w, w_{0}$로 미분한 값이 0이 되어야 하는 것이다.</li>
</ul>
<script type="math/tex; mode=display">\dfrac{\partial L}{\partial w} = 0</script><script type="math/tex; mode=display">\dfrac{\partial L}{\partial w_0} = 0</script><ul>
<li>이 식을 풀어서 정리하면 다음과 같아진다.</li>
</ul>
<script type="math/tex; mode=display">\begin{eqnarray} \dfrac{\partial L}{\partial w} &=& \dfrac{\partial}{\partial w} \left( \dfrac{1}{2} w^T w \right) - \dfrac{\partial}{\partial w} \sum_{i=1}^N \left( a_i y_i w^Tx_i - a_i y_i w_o - a_i \right) \\ &=& w - \sum_{i=1}^N  a_i y_i x_i \\ &=& 0 \end{eqnarray}</script><script type="math/tex; mode=display">\begin{eqnarray} \dfrac{\partial L}{\partial w_0} &=& \dfrac{\partial}{\partial w_0} \left( \dfrac{1}{2} w^T w \right) - \dfrac{\partial}{\partial w_0} \sum_{i=1}^N \left( a_i y_i w^Tx_i - a_i y_i w_o - a_i \right) \\
&=& \sum_{i=1}^N  a_i y_i \\ &=& 0 \end{eqnarray}</script><ul>
<li>정리해보면, 다음과 같다.</li>
</ul>
<script type="math/tex; mode=display">w = \sum_{i=1}^N a_i y_i x_i</script><script type="math/tex; mode=display">0 = \sum_{i=1}^N a_i y_i</script><ul>
<li>이 두 수식을 원래의 목적함수에 대입하여 $w, w_{0}$을 없애면 다음과 같다.</li>
</ul>
<script type="math/tex; mode=display">\begin{eqnarray} L &=& \dfrac{1}{2} w^T w - \sum_{i=1}^N a_i \{ y_i \cdot ( w^Tx_i - w_o) - 1 \}  \\ &=& \dfrac{1}{2} \left( \sum_{i=1}^N a_i y_i x_i \right)^T \left( \sum_{j=1}^N a_j y_j x_j \right) - \sum_{i=1}^N a_i \left\{ y_i \cdot \left( \left( \sum_{j=1}^N a_j y_j x_j \right)^Tx_i - w_o \right) - 1 \right\}  \\ &=& \dfrac{1}{2} \sum_{i=1}^N \sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j - \sum_{i=1}^N \sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j + w_0 \sum_{i=1}^N a_i y_i + \sum_{i=1}^N a_i   \\ &=& \sum_{i=1}^N a_i - \dfrac{1}{2}\sum_{i=1}^N\sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j \end{eqnarray}</script><script type="math/tex; mode=display">L = \sum_{i=1}^N a_i - \dfrac{1}{2}\sum_{i=1}^N\sum_{j=1}^N a_i a_j y_i y_j x_i^T x_j</script><ul>
<li>이 떄 $a$는 다음 조건을 만족한다.</li>
</ul>
<script type="math/tex; mode=display">\sum_{i=1}^N a_i y_i = 0</script><script type="math/tex; mode=display">a_i \geq 0 \;\;\;  ( i = 1, \ldots, N )</script><ul>
<li>이 문제는 $w$를 구하는 문제가 아니라 $a$만을 구하는 문제로 바뀌었으므로 듀얼형식(dual form)이라고 한다. 듀얼형식으로 바꾸면 수치적으로 박스(Box)제한 조건이 있는 이차프로그래밍(QP: Quadratic programming)문제가 되므로 원래의 문제보다는 효율적으로 풀 수 있다.</li>
</ul>
<p><a href="https://datascienceschool.net/view-notebook/0fca28c71c13460fb7168ee2adb9a8be/" target="_blank" rel="noopener">선형계획법 문제와 이차계획법 문제</a></p>
<ul>
<li>듀얼 형식 문제를 풀어 함수 $L$을 최소화하는 $a$를 구하면 예측 모형을 다음과 같이 쓸 수 있다.</li>
</ul>
<script type="math/tex; mode=display">f(x) = w^T x - w_0 = \sum_{i=1}^N a_i y_i x_i^T x - w_0</script><ul>
<li>$w_{0}$는 아래와 같이 구한다.</li>
</ul>
<script type="math/tex; mode=display">w_0 = w^T x^{+} - 1 또는 w_0 = w^T x^{-} + 1 또는 w_0 = \dfrac{1}{2} w^T (x^+ + x^{-})</script><ul>
<li>라그랑주 승수 값이 0 즉, $a_{i} = 0$이면 해당 데이터는 예측 모형, 즉 $w$ 계산에 아무런 기여를 하지 않으므로 위의 식은 실제로는 다음과 같다.</li>
</ul>
<script type="math/tex; mode=display">f(x) = a^+ x^T x^+ - a^- x^T x^- - w_0</script><ul>
<li>여기에서 $x^{T}x^{+}$는 $x$와 $x^{+}$ 사이의 코사인 유사도, $x^{T}x^{-}$는 $x$와 $x^{-}$ 사이의 코사인 유사도이므로 결국 두 <code>서포트 벡터와의 유사도를 측정해서 값이 큰쪽으로 판별</code>하게 된다.</li>
</ul>
<h3 id="Scikit-Learn의-서포트-벡터-머신"><a href="#Scikit-Learn의-서포트-벡터-머신" class="headerlink" title="Scikit-Learn의 서포트 벡터 머신"></a>Scikit-Learn의 서포트 벡터 머신</h3><ul>
<li>Scikit-Learn의 <code>svm</code> 서브패키지는 서포트 벡터 머신 모형인 <code>SVC</code>(Support Vector Classifier) 클래스를 제공한다. 이와 동시에 SVR(Support Vector Regressor)도 제공을 하지만 SVR은 추후에 설명하고 먼저, SVC에 대해 다루어 볼 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">X, y = make_blobs(n_samples=50, centers=2, cluster_std=0.5, random_state=4)</span><br><span class="line">y = 2 * y - 1</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y == -1, 0], X[y == -1, 1], marker=<span class="string">'o'</span>, label=<span class="string">"-1 클래스"</span>)</span><br><span class="line">plt.scatter(X[y == +1, 0], X[y == +1, 1], marker=<span class="string">'x'</span>, label=<span class="string">"+1 클래스"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">"학습용 데이터"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Support_Vector_Machine_train_data_set.png" alt="Support Vector Machine train data set"></p>
<ul>
<li><code>SVC</code> 클래스는 커널(Kernel)을 선택하는 인수 <code>kernel</code>과 슬랙변수 가중치(slack variable weight)를 선택하는 인수 <code>C</code>를 받는데 지금까지 공부한 서포트 벡터 머신을 사용하려면 인수를 다음처럼 넣어준다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.svm import SVC</span><br><span class="line">model = SVC(kernel=<span class="string">'linear'</span>, C=1e10).fit(X, y)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>SVC</code>를 사용하여 모형을 구하면 다음과 같은 속성값을 가진다.<ul>
<li><code>n_support</code> : 각 클래스의 서포트 벡터의 개수</li>
<li><code>support</code> : 각 클래스의 서포트 벡터의 인덱스</li>
<li><code>support_vectors_</code> : 각 클래스의 서포트의 $x$값.$(x^{T}, x^{-})$</li>
<li><code>coef</code> : $w$벡터</li>
<li><code>intercept</code> : $- w_{0}$</li>
<li><code>dual_coef</code> : 각 원소가 $a_{i} \dot y_{i}$로 이루어진 벡터</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.n_support_</span><br></pre></td></tr></table></figure>
<h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1, 1], dtype=int32)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.support_</span><br></pre></td></tr></table></figure>
<h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([42,  1], dtype=int32)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.support_vectors_</span><br></pre></td></tr></table></figure>
<h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[9.03715314, 1.71813465],</span><br><span class="line">       [9.17124955, 3.52485535]])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y[model.support_]</span><br></pre></td></tr></table></figure>
<h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-1,  1])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">xmin = X[:, 0].min()</span><br><span class="line">xmax = X[:, 0].max()</span><br><span class="line">ymin = X[:, 1].min()</span><br><span class="line">ymax = X[:, 1].max()</span><br><span class="line">xx = np.linspace(xmin, xmax, 10)</span><br><span class="line">yy = np.linspace(ymin, ymax, 10)</span><br><span class="line">X1, X2 = np.meshgrid(xx, yy)</span><br><span class="line"></span><br><span class="line">Z = np.empty(X1.shape)</span><br><span class="line"><span class="keyword">for</span> (i, j), val <span class="keyword">in</span> np.ndenumerate(X1):</span><br><span class="line">    x1 = val</span><br><span class="line">    x2 = X2[i, j]</span><br><span class="line">    p = model.decision_function([[x1, x2]])</span><br><span class="line">    Z[i, j] = p[0]</span><br><span class="line">levels = [-1, 0, 1]</span><br><span class="line">linestyles = [<span class="string">'dashed'</span>, <span class="string">'solid'</span>, <span class="string">'dashed'</span>]</span><br><span class="line">plt.scatter(X[y == -1, 0], X[y == -1, 1], marker=<span class="string">'o'</span>, label=<span class="string">"-1 클래스"</span>)</span><br><span class="line">plt.scatter(X[y == +1, 0], X[y == +1, 1], marker=<span class="string">'x'</span>, label=<span class="string">"+1 클래스"</span>)</span><br><span class="line">plt.contour(X1, X2, Z, levels, colors=<span class="string">'k'</span>, linestyles=linestyles)</span><br><span class="line">plt.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=300, alpha=0.3)</span><br><span class="line"></span><br><span class="line">x_new = [10, 2]</span><br><span class="line">plt.scatter(x_new[0], x_new[1], marker=<span class="string">'^'</span>, s=100)</span><br><span class="line">plt.text(x_new[0] + 0.03, x_new[1] + 0.08, <span class="string">"테스트 데이터"</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">"SVM 예측 결과"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/prediction_of_test_data_svm.png" alt="SVM 모델 test data 예측 결과"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = [10, 2]</span><br><span class="line">model.decision_function([x_new])</span><br></pre></td></tr></table></figure>
<h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-0.61101582])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.coef_.dot(x_new) + model.intercept_</span><br></pre></td></tr></table></figure>
<h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-0.61101582])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dual_coef_ = a_i * y_i</span></span><br><span class="line">model.dual_coef_</span><br></pre></td></tr></table></figure>
<h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[-0.60934379,  0.60934379]])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.dual_coef_[0][0] * model.support_vectors_[0].dot(x_new) + \</span><br><span class="line">    model.dual_coef_[0][1] * model.support_vectors_[1].dot(x_new) + \</span><br><span class="line">    model.intercept_</span><br></pre></td></tr></table></figure>
<h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([-0.61101582])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>iris 문제를 서포트 벡터 머신으로 풀어보자. 다음과 같은 데이터만 사용한 이진 분류 문제로 바꾸어 풀어본다. 위의 예제와 마찬가지로 커널 인수 <code>kernel</code>과 슬랙변수 가중치 인수 <code>C</code>는 각각 <code>linear</code>, <code>1e10</code>으로 한다.</p>
</blockquote>
<pre><code>- 특징 변수를 꽃받침의 길이와 폭만 사용한다.
- 붓꽆 종을 Setosa와 Versicolour만 대상으로 한다.
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix, classification_report</span><br><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X_data = iris.data[(iris.target == 0) | (iris.target == 1), :2]</span><br><span class="line">y = iris.target[(iris.target == 0) | (iris.target == 1)]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.3)</span><br><span class="line">svm = SVC(kernel=<span class="string">"linear"</span>, C=1e10)</span><br><span class="line">svm.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>
<h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,</span><br><span class="line">    decision_function_shape=<span class="string">'ovr'</span>, degree=3, gamma=<span class="string">'auto_deprecated'</span>,</span><br><span class="line">    kernel=<span class="string">'linear'</span>, max_iter=-1, probability=False, random_state=None,</span><br><span class="line">    shrinking=True, tol=0.001, verbose=False)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred_y = svm.predict(X_test)</span><br><span class="line">confusion_matrix(pred_y, y_test)</span><br></pre></td></tr></table></figure>
<h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[14,  0],</span><br><span class="line">       [ 0, 16]])</span><br></pre></td></tr></table></figure>
<ul>
<li><code>위의 조건에서 kernel=&quot;linear&quot;로 유지한채 C값만 [0.01, 0.1, 1, 10, 100]으로 변화를 주며 결과를 살펴보니 C값이 높아지면 Slack 변수로 줄 수 있는 값이 줄어들어 서포트 벡터의 수가 줄어든다. 반대로 C값을 낮추어 줄수록 Slack 변수가 갖는 값이 크게 되어 서포트 벡터는 많아지며 마진이 줄어든다.</code></li>
</ul>
<h3 id="슬랙변수"><a href="#슬랙변수" class="headerlink" title="슬랙변수"></a>슬랙변수</h3><ul>
<li><p>만약 데이터가 직선인 판별 경계선으로 나누어지지 않는 즉, 선형분이(linear seperable)가 불가능한 경우에는 다음과 같이 슬랙변수(slack variable)를 사용하여 개별적인 오차를 허용할 수 있다.</p>
</li>
<li><p>원래 판별 함수의 값은 클래스 $x^{T}$ 영역의 샘플 $x_{+}$에 대해선 첫번째 수식과 같고, 클래스 -1 영역의 샘플 $x_{-}$에 대해서는 두 번째 수식과 같아야한다.</p>
</li>
</ul>
<script type="math/tex; mode=display">w^Tx_+ - w_0 \geq 1</script><script type="math/tex; mode=display">w^Tx_- - w_0 \leq -1</script><ul>
<li>양수인 슬랙변수 $\xi \geq 0$를 사용하면 이 조건을 다음과 같이 완화할 수 있다.</li>
</ul>
<script type="math/tex; mode=display">w^Tx_+ - w_0 \geq +1-\xi_i</script><script type="math/tex; mode=display">w^Tx_- - w_0 \leq -1+\xi_i</script><ul>
<li>모든 슬랙변수는 0보다 같거나 크다.</li>
</ul>
<script type="math/tex; mode=display">\xi_i \geq 0 \;\;\; (i=1, \ldots, N)</script><ul>
<li>위의 부등식 조건을 모두 고려한 최적화 목적함수는 다음과 같아진다. 아래 식에서 $C \sum_{i=1}^N \xi_i$ 항은 슬랙변수의 합이 너무 커지지 않도록 제한하는 역할을 한다.</li>
</ul>
<script type="math/tex; mode=display">L = \dfrac{1}{2} ||w||^2 - \sum_{i=1}^N a_i (y_i \cdot ( w^Tx_i - w_o) - 1 + \xi_i ) - \sum_{i=1}^N \mu_i \xi_i  + C \sum_{i=1}^N \xi_i</script><p><img src="/image/Slack_variable_C_difference_each_value.png" alt="슬랙변수의 C값에 따른 오차 허용의 차이"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(0)</span><br><span class="line">X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]</span><br><span class="line">Y = [-1] * 20 + [1] * 20</span><br><span class="line"></span><br><span class="line">plotnum = 1</span><br><span class="line"><span class="keyword">for</span> name, penalty <span class="keyword">in</span> ((<span class="string">'C=10'</span>, 10), (<span class="string">'C=0.1'</span>, 0.1)):</span><br><span class="line">    clf = SVC(kernel=<span class="string">'linear'</span>, C=penalty).fit(X, Y)</span><br><span class="line">    xx = np.linspace(-5, 5)</span><br><span class="line"></span><br><span class="line">    x_jin = -5</span><br><span class="line">    x_jax = 5</span><br><span class="line">    y_jin = -9</span><br><span class="line">    y_jax = 9</span><br><span class="line">    XX, YY = np.mgrid[x_jin:x_jax:200j, y_jin:y_jax:200j]</span><br><span class="line"></span><br><span class="line">    levels = [-1, 0, 1]</span><br><span class="line">    linestyles = [<span class="string">'dashed'</span>, <span class="string">'solid'</span>, <span class="string">'dashed'</span>]</span><br><span class="line">    Z = clf.decision_function(np.c_[XX.ravel(), YY.ravel()])</span><br><span class="line">    Z = Z.reshape(XX.shape)</span><br><span class="line"></span><br><span class="line">    plt.subplot(1, 2, plotnum)</span><br><span class="line">    plt.contour(XX, YY, Z, levels, colors=<span class="string">'k'</span>, linestyles=linestyles)</span><br><span class="line">    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=120, linewidth=4)</span><br><span class="line">    plt.scatter(X[:, 0], X[:, 1], c=Y, s=60, linewidth=1, cmap=plt.cm.Paired)</span><br><span class="line">    plt.xlim(x_jin, x_jax)</span><br><span class="line">    plt.ylim(y_jin, y_jax)</span><br><span class="line">    plt.title(name)</span><br><span class="line"></span><br><span class="line">    plotnum += 1</span><br><span class="line"></span><br><span class="line">plt.suptitle(<span class="string">"슬랙변수 가중치 C의 영향"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/influence_of_weighted_value_C_with_slack_variables.png" alt="슬랙변수 가중치 C의 영향"></p>
<ul>
<li>다시 한번 정리하자면, 아래 그림에서 살펴보면 직선의 방정식 $ x^{T} \beta + \beta_{0} $와 각 서포트 벡터 $ x(x^{+}, x^{-}) $와의 거리가 $ \frac{1}{\beta}$ 이므로 결굴 마진을 크게 하는 것은 $ \beta $를 작게 하는 것과 동일한 의미이다. 그러한 측면에서도 위에서 자세히 언급했듯이 Cost function(목적함수, 비용함수)가 아래와 같이 나온다는 것을 알 수 있다. 여기서 동시에 error를 최소화하고 싶으므로 라그랑주 승수 $ C $를 크게가져가면서 error를 허용하는 slack변수를 최소화하는 동시에 $ \beta $ 의 값도 최소화하는 optimization 문제를 풀 수 있다.</li>
</ul>
<p><img src="/image/calculate_svm.png" alt="SVM 계산 - 01"></p>
<ul>
<li>위에서의 조건하에 최적화를 하는 것이므로 라그랑주 승수를 도입하여 부등식이 있는 최적화 문제를 풀게 된다.</li>
</ul>
<p><img src="/image/calculate_svm_01.png" alt="SVM 계산 - 02"></p>
<p><img src="/image/calculate_svm_02.png" alt="SVM 계산 - 03"></p>
<ul>
<li>KKT조건을 만족함으로서, <code>global minimum을 보장</code>받을 수 있다.</li>
</ul>
<p><img src="/image/calculate_svm_03.png" alt="SVM 계산 - 04"></p>
<ul>
<li>즉, KKT의 2번째 조건에 의해서 서포트벡터인 경우는 조건식이 의미가 있기 때문에 라그랑지 승수 $\alpha_{i} \neq 0$이 된다는 의미이다.</li>
</ul>
<p><img src="/image/calculate_svm_04.png" alt="SVM 계산 - 05"></p>
<blockquote>
<p>얼굴 이미지 인식</p>
</blockquote>
<ul>
<li>총 40명이 각각 10장의 조금씩 다른 표정이나 모습으로 찍은 이미지 데이터이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_olivetti_faces</span><br><span class="line">faces = fetch_olivetti_faces()</span><br><span class="line"></span><br><span class="line">N = 2</span><br><span class="line">M = 5</span><br><span class="line">np.random.seed(0)</span><br><span class="line">fig = plt.figure(figsize=(9, 5))</span><br><span class="line">plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05)</span><br><span class="line">klist = np.random.choice(range(len(faces.data)), N * M)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(M):</span><br><span class="line">        k = klist[i * M + j]</span><br><span class="line">        ax = fig.add_subplot(N, M, i * M + j + 1)</span><br><span class="line">        ax.imshow(faces.images[k], cmap=plt.cm.bone)</span><br><span class="line">        ax.grid(False)</span><br><span class="line">        ax.xaxis.set_ticks([])</span><br><span class="line">        ax.yaxis.set_ticks([])</span><br><span class="line">        plt.title(faces.target[k])</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/fetch_olivetti_faces_01.png" alt="랜덤하게 뽑은 이미지"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(faces.data, faces.target, test_size=0.4, random_state=0)</span><br><span class="line"></span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">svc = SVC(kernel=<span class="string">'linear'</span>).fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">N = 2</span><br><span class="line">M = 5</span><br><span class="line">np.random.seed(4)</span><br><span class="line">fig = plt.figure(figsize=(9, 5))</span><br><span class="line">plt.subplots_adjust(top=1, bottom=0, hspace=0, wspace=0.05)</span><br><span class="line">klist = np.random.choice(range(len(y_test)), N * M)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(M):</span><br><span class="line">        k = klist[i * M + j]</span><br><span class="line">        ax = fig.add_subplot(N, M, i * M + j + 1)</span><br><span class="line">        ax.imshow(X_test[k:(k + 1), :].reshape(64, 64), cmap=plt.cm.bone)</span><br><span class="line">        ax.grid(False)</span><br><span class="line">        ax.xaxis.set_ticks([])</span><br><span class="line">        ax.yaxis.set_ticks([])</span><br><span class="line">        plt.title(<span class="string">"%d =&gt; %d"</span> %</span><br><span class="line">                  (y_test[k], svc.predict(X_test[k:(k + 1), :])[0]))</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/fetch_olivetti_faces_02.png" alt="랜덤하게 뽑은 이미지의 예측"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import classification_report, accuracy_score</span><br><span class="line"></span><br><span class="line">y_pred_train = svc.predict(X_train)</span><br><span class="line">y_pred_test = svc.predict(X_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_train, y_pred_train))</span><br></pre></td></tr></table></figure>
<h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       1.00      1.00      1.00         4</span><br><span class="line">           1       1.00      1.00      1.00         5</span><br><span class="line">           2       1.00      1.00      1.00         6</span><br><span class="line">           3       1.00      1.00      1.00         8</span><br><span class="line">           4       1.00      1.00      1.00         8</span><br><span class="line">           5       1.00      1.00      1.00         5</span><br><span class="line">           6       1.00      1.00      1.00         4</span><br><span class="line">           7       1.00      1.00      1.00         7</span><br><span class="line">           8       1.00      1.00      1.00         8</span><br><span class="line">           9       1.00      1.00      1.00         7</span><br><span class="line">          10       1.00      1.00      1.00         4</span><br><span class="line">          11       1.00      1.00      1.00         6</span><br><span class="line">          12       1.00      1.00      1.00         6</span><br><span class="line">          13       1.00      1.00      1.00         6</span><br><span class="line">          14       1.00      1.00      1.00         4</span><br><span class="line">          15       1.00      1.00      1.00         4</span><br><span class="line">          16       1.00      1.00      1.00         8</span><br><span class="line">          17       1.00      1.00      1.00         4</span><br><span class="line">          18       1.00      1.00      1.00         9</span><br><span class="line">          19       1.00      1.00      1.00         4</span><br><span class="line">          20       1.00      1.00      1.00         9</span><br><span class="line">          21       1.00      1.00      1.00         6</span><br><span class="line">          22       1.00      1.00      1.00         7</span><br><span class="line">          23       1.00      1.00      1.00         5</span><br><span class="line">          24       1.00      1.00      1.00         6</span><br><span class="line">          25       1.00      1.00      1.00         5</span><br><span class="line">          26       1.00      1.00      1.00         5</span><br><span class="line">          27       1.00      1.00      1.00         8</span><br><span class="line">          28       1.00      1.00      1.00         6</span><br><span class="line">          29       1.00      1.00      1.00         4</span><br><span class="line">          30       1.00      1.00      1.00         6</span><br><span class="line">          31       1.00      1.00      1.00         5</span><br><span class="line">          32       1.00      1.00      1.00         6</span><br><span class="line">          33       1.00      1.00      1.00         7</span><br><span class="line">          34       1.00      1.00      1.00         4</span><br><span class="line">          35       1.00      1.00      1.00         7</span><br><span class="line">          36       1.00      1.00      1.00         6</span><br><span class="line">          37       1.00      1.00      1.00         6</span><br><span class="line">          38       1.00      1.00      1.00         9</span><br><span class="line">          39       1.00      1.00      1.00         6</span><br><span class="line"></span><br><span class="line">    accuracy                           1.00       240</span><br><span class="line">   macro avg       1.00      1.00      1.00       240</span><br><span class="line">weighted avg       1.00      1.00      1.00       240</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred_test))</span><br></pre></td></tr></table></figure>
<h5 id="결과-14"><a href="#결과-14" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.86      1.00      0.92         6</span><br><span class="line">           1       1.00      1.00      1.00         5</span><br><span class="line">           2       1.00      1.00      1.00         4</span><br><span class="line">           3       0.50      1.00      0.67         2</span><br><span class="line">           4       1.00      0.50      0.67         2</span><br><span class="line">           5       1.00      1.00      1.00         5</span><br><span class="line">           6       0.83      0.83      0.83         6</span><br><span class="line">           7       1.00      0.67      0.80         3</span><br><span class="line">           8       0.67      1.00      0.80         2</span><br><span class="line">           9       1.00      1.00      1.00         3</span><br><span class="line">          10       1.00      1.00      1.00         6</span><br><span class="line">          11       1.00      1.00      1.00         4</span><br><span class="line">          12       0.67      1.00      0.80         4</span><br><span class="line">          13       1.00      1.00      1.00         4</span><br><span class="line">          14       1.00      1.00      1.00         6</span><br><span class="line">          15       1.00      0.33      0.50         6</span><br><span class="line">          16       0.67      1.00      0.80         2</span><br><span class="line">          17       1.00      1.00      1.00         6</span><br><span class="line">          18       1.00      1.00      1.00         1</span><br><span class="line">          19       1.00      1.00      1.00         6</span><br><span class="line">          20       1.00      1.00      1.00         1</span><br><span class="line">          21       1.00      0.75      0.86         4</span><br><span class="line">          22       1.00      1.00      1.00         3</span><br><span class="line">          23       0.71      1.00      0.83         5</span><br><span class="line">          24       1.00      1.00      1.00         4</span><br><span class="line">          25       1.00      1.00      1.00         5</span><br><span class="line">          26       1.00      1.00      1.00         5</span><br><span class="line">          27       1.00      1.00      1.00         2</span><br><span class="line">          28       1.00      1.00      1.00         4</span><br><span class="line">          29       1.00      1.00      1.00         6</span><br><span class="line">          30       1.00      1.00      1.00         4</span><br><span class="line">          31       1.00      1.00      1.00         5</span><br><span class="line">          32       1.00      1.00      1.00         4</span><br><span class="line">          33       1.00      1.00      1.00         3</span><br><span class="line">          34       1.00      0.83      0.91         6</span><br><span class="line">          35       1.00      0.67      0.80         3</span><br><span class="line">          36       1.00      1.00      1.00         4</span><br><span class="line">          37       1.00      1.00      1.00         4</span><br><span class="line">          38       0.50      1.00      0.67         1</span><br><span class="line">          39       0.67      0.50      0.57         4</span><br><span class="line"></span><br><span class="line">    accuracy                           0.93       160</span><br><span class="line">   macro avg       0.93      0.93      0.91       160</span><br><span class="line">weighted avg       0.95      0.93      0.92       160</span><br></pre></td></tr></table></figure>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "Support Vector Machine(SVM) - 01",
        "image": "https://heung-bae-lee.github.io/image/background_of_svm_concept_01.png",
        "keywords": "",
        "genre": "machine learning",
        "datePublished": "2020-04-22",
        "dateCreated": "2020-04-22",
        "dateModified": "2020-05-27",
        "url": "https://heung-bae-lee.github.io/2020/04/22/machine_learning_11/",
        "description": "Support Vector Machine(SVM)
데이터의 분포가 정규분포를 띈다고 보이지 않는다면 이전에 말했던 LDA나 QDA를 사용하기 힘들다. 이런 경우 클래스간 분류를 하려고 할 때 사용될 수 있는 방법 중 하나가 SVM이다. 아래 그림과 같이 클래스 집단을 분류할 수 있는 벡터를 기준으로 최대한의 마진을 주어 분류하는 방식이다.



아래 그림과"
        "wordCount": 10103
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2020/04/25/machine_learning_12/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            Support Vector Machine(SVM) - 02
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2020/04/21/machine_learning_10/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">퍼셉트론</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/27/machine_learning_16/" class="title">Ensemble Learning - Ensemble의 Ensemble</a></p>
                            <p class="item-date"><time datetime="2020-05-26T17:04:18.000Z" itemprop="datePublished">2020-05-27</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/27/machine_learning_15/" class="title">Ensemble Learning - Boosting, Stacking</a></p>
                            <p class="item-date"><time datetime="2020-05-26T17:00:49.000Z" itemprop="datePublished">2020-05-27</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"></p>
                            <p class="item-title"><a href="/2020/05/17/data_structure_07/" class="title">내가 정리하는 자료구조 06 - 힙(heap)</a></p>
                            <p class="item-date"><time datetime="2020-05-17T14:39:31.000Z" itemprop="datePublished">2020-05-17</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/05/14/linear_algebra_03/" class="title">Linear combination, vector equation, Four views of matrix multiplication</a></p>
                            <p class="item-date"><time datetime="2020-05-14T06:36:03.000Z" itemprop="datePublished">2020-05-14</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/05/13/linear_algebra_02/" class="title">선형 시스템(Linear system)</a></p>
                            <p class="item-date"><time datetime="2020-05-13T07:50:33.000Z" itemprop="datePublished">2020-05-13</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">17</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/04/22/machine_learning_11/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
