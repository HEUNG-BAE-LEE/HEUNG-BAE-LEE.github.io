<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>나이브 베이즈 분류모형 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="분류모형  현실적인 문제로 바꾸어 말하면 어떤 표본에 대한 데이터가 주어졌을 때 그 표본이 어떤 카테고리 혹은 클래스에 속하는지를 알아내는 문제이기도 하다.    흔히들 많이 사용하는 모형들의 분류모형 종류를 아래의 표로 기재해 놓았다. 이전에 기재했던 로지스틱 회귀 분석도 실질적으론 분류문제에 많이 사용된다.      모형 방법론     나이브 베이지안">
<meta property="og:type" content="article">
<meta property="og:title" content="나이브 베이즈 분류모형">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="분류모형  현실적인 문제로 바꾸어 말하면 어떤 표본에 대한 데이터가 주어졌을 때 그 표본이 어떤 카테고리 혹은 클래스에 속하는지를 알아내는 문제이기도 하다.    흔히들 많이 사용하는 모형들의 분류모형 종류를 아래의 표로 기재해 놓았다. 이전에 기재했던 로지스틱 회귀 분석도 실질적으론 분류문제에 많이 사용된다.      모형 방법론     나이브 베이지안">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/classification_problem.png">
<meta property="og:updated_time" content="2020-04-17T12:47:37.106Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="나이브 베이즈 분류모형">
<meta name="twitter:description" content="분류모형  현실적인 문제로 바꾸어 말하면 어떤 표본에 대한 데이터가 주어졌을 때 그 표본이 어떤 카테고리 혹은 클래스에 속하는지를 알아내는 문제이기도 하다.    흔히들 많이 사용하는 모형들의 분류모형 종류를 아래의 표로 기재해 놓았다. 이전에 기재했던 로지스틱 회귀 분석도 실질적으론 분류문제에 많이 사용된다.      모형 방법론     나이브 베이지안">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/classification_problem.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/machine-learning/">machine learning</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-machine_learning_07" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        나이브 베이즈 분류모형
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/04/14/machine_learning_07/" class="article-date">
            <time datetime="2020-04-14T05:59:44.000Z" itemprop="datePublished">2020-04-14</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h1 id="분류모형"><a href="#분류모형" class="headerlink" title="분류모형"></a>분류모형</h1><p><img src="/image/classification_problem.png" alt="분류 문제란?"></p>
<ul>
<li>현실적인 문제로 바꾸어 말하면 어떤 표본에 대한 데이터가 주어졌을 때 그 표본이 어떤 카테고리 혹은 클래스에 속하는지를 알아내는 문제이기도 하다.</li>
</ul>
<p><img src="/image/what_kinds_of_clssification_model.png" alt="분류모형의 종류"></p>
<ul>
<li>흔히들 많이 사용하는 모형들의 분류모형 종류를 아래의 표로 기재해 놓았다. 이전에 기재했던 로지스틱 회귀 분석도 실질적으론 분류문제에 많이 사용된다.</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>모형</th>
<th>방법론</th>
</tr>
</thead>
<tbody>
<tr>
<td>나이브 베이지안</td>
<td>확률적 생성모형</td>
</tr>
<tr>
<td>LDA/QDA</td>
<td>확률적 생성모형</td>
</tr>
<tr>
<td>로지스틱회귀</td>
<td>확률적 판별모형</td>
</tr>
<tr>
<td>의사결정나무</td>
<td>확률적 판별모형</td>
</tr>
<tr>
<td>퍼셉트론</td>
<td>판별함수 모형</td>
</tr>
<tr>
<td>서포트벡터머신</td>
<td>판별함수 모형</td>
</tr>
<tr>
<td>인공신경망</td>
<td>판별함수 모형</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/image/classification_graph.png" alt="분류모형의 종류 도식화"></p>
<h1 id="나이브-베이즈-분류-모형"><a href="#나이브-베이즈-분류-모형" class="headerlink" title="나이브 베이즈 분류 모형"></a>나이브 베이즈 분류 모형</h1><ul>
<li>아래 그림과 같은 질문에 대한 확률을 계산하려면, 아래 결합확률을 계산해야 할 것이다. 독립변수가 2개 즉, Feature의 차원이 2개인 지금은 크게 어렵진 않겠지만, 현실에서는 Feature의 개수가 훨씬 더 많을 것이다. 차원이 커진다면 이에따라 해당 조건부 함수의 가능도함수를 추정하는데 어려움을 겪을 것이다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} P( X = weather state, Humidity | Y = playing Tennis) \\ = \frac{P( X_{1} = weather state, Humidity \cap Y = playing Tennis)}{P(Y = playing Tennis)} \end{align}</script><p><img src="/image/naive_bayesian_model_assumption.png" alt="나이브 베이지안 모형의 가정"></p>
<ul>
<li>위의 수식에서 분자부분의 확률인 결합확률(Joint probability)을 구하는 것은 쉬운 문제가 아니다. 결합되어있는 상황을 나누어서 확률을 계산할 순 없을까라는 가정에서 나이브 베이지안 모형은 출발한다. 그렇다면, 어떻게 나눌수 있을까? 통계를 조금이라도 아시는 분들은 독립이라는 개념을 알고계실 것이다. 예를 들어 A: 코로나가 집단 발병한다, B: 2주간 재택근무를 시행한다. 이 두가지 사건은 서로 발현될 확률에 영향을 미치기 때문에 두 사건은 종속(dependent) 관계라고 할 수 있다. 허나, A: 비가온다, B: 대출심사에서 승인을 받는다 라는 두 사건은 서로 둘 중 어떤 사건에도 발현될 확률에 영향을 주지 않는다. 이러한, 경우에 우리는 두 사건이 확률적으로 독립이라고 정의한다. 두 사건 A,B가 독립일 필요충분조건은 아래와 같다.</li>
</ul>
<script type="math/tex; mode=display">P(A \cap B) = P(A)P(B)</script><ul>
<li>이렇게 두 사건이 독립이라면 좀 더 추정하기 쉬울 것이다. 허나, 위의 상황에선 조건부이기 때문에 <code>조건부 독립</code>의 가정이 맞을 것이다. 조건부 독립(conditional independence)은 일반적인 독립과 달리 조건이 되는 별개의 확률변수 C가 존재해야한다. 아래 수식이 성립되면 조건부 독립이라고 한다.</li>
</ul>
<script type="math/tex; mode=display">A \text{⫫} B \;\vert\; C \Longrightarrow P(A, B | C) = P(A|C)P(B|C)</script><ul>
<li>조건부독립과 비교하여 일반적인 독립은 무조건부독립이라고 한다. 무조건부 독립은 다음과 같이 표기하기도 한다.</li>
</ul>
<script type="math/tex; mode=display">A \text{⫫} B \;\vert\; \emptyset</script><ul>
<li>A, B가 C에 대해 조건부 독립이면 다음도 만족한다.</li>
</ul>
<script type="math/tex; mode=display">P(A \;\vert\; B, C) = P(A \vert C)</script><script type="math/tex; mode=display">P(B \;\vert\; A, C) = P(B \vert C)</script><script type="math/tex; mode=display">\begin{align} P(A|B,C) &= \frac{P(A,B,C)}{P(B,C)} \\ &= \frac{P(A,B|C)P(C)}{P(B|C)P(C)} \\ &= \frac{P(A|C)P(B|C)}{P(B|C)} \\ &= P(A|C) \end{align}</script><ul>
<li><code>주의할 점은 조건부 독립과 무조건부 독립은 관계가 없다는 점</code>이다. 즉, 두 확률변수가 독립이라고 항상 조건부 독립이 되는 것도 아니고 조건부 독립이라고 꼭 독립이 되는 것도 아니다.</li>
</ul>
<script type="math/tex; mode=display">P(A,B) = P(A)P(B) \;\; \nRightarrow \;\; P(A,B|C) = P(A|C)P(B|C)</script><script type="math/tex; mode=display">P(A,B|C) = P(A|C)P(B|C) \;\; \nRightarrow \;\; P(A,B) = P(A)P(B)</script><h4 id="베이즈-정리-Bayes’-rule"><a href="#베이즈-정리-Bayes’-rule" class="headerlink" title="베이즈 정리(Bayes’ rule)"></a>베이즈 정리(Bayes’ rule)</h4><ul>
<li>베이즈 정리는 사건 B가 발생함으로써 사건 A의 확률이 어떻게 변화하는지를 표현한 정리이다. 따라서 <code>베이즈 정리는 새로운 정보가 기존의 추론에 어떻게 영향을 미치는지를 나타낸다.</code></li>
</ul>
<script type="math/tex; mode=display">P(A|B) = \frac{P(B|A) P(A)}{P(B)}</script><ul>
<li>$P(A|B)$ : 사후확률(Posterior). 사건 B가 발생한 후 갱신된 사건 A의 확률</li>
<li>$P(A)$ : 사전확률(Prior). 사건 B가 발생하기 전에 가지고 있던 사건 A의 확률</li>
<li>$P(B|A)$ : 가능도(likelihood). 사건 A가 발생한 경우 사건 B의 확률</li>
<li>$P(B)$ : 정규화 상수(normalizing constant) 또는 증거(evidence). 확률의 크기 조정역할을 함.</li>
</ul>
<blockquote>
<p>이렇게 독립변수(Feature)들간에 서로 조건부독립임을 가정하여 조건을 나이브하게 하였고, 베이즈정리를 사용하여 MLE를 통해 가장 큰 확률값을 갖는 모수를 추정해내는 모형이 나이브 베이지안 모형이다.</p>
</blockquote>
<script type="math/tex; mode=display">\begin{align} P(y = k \mid x) &= \dfrac{ P(x_1, \ldots, x_D \mid y = k) P(y = k) }{P(x)} \\ &= \dfrac{ \left( \prod_{d=1}^D P(x_{d} \mid y = k) \right) P(y = k) }{P(x)} \end{align}</script><ul>
<li>제일 처음 예시로 들었던 날씨의 상태와 습도를 독립변수로 하고, 종속변수를 테니스를 치는지에 대한 여부에 대한 문제를 푼다고 가정해보자. 먼저 테니스를 치는 경우에 대한 확률을 계산할 경우 테니스를 치는 사건에 대한 확률을 높게 해주는 두가지 경우의 수를 아래와 같이 생각해 볼 수 있다. 아래 두 그림을 수식으로 정리해보면 다음과 같다는 사실을 쉽게 생각해 볼 수 있다. <code>수식의 마지막에서 분자부분에서 likelihood와 Prior 확률값을 살펴보면 그림에서 언급한 두 가지 경우의 사건들의 확률 값의 곱으로 되어있음</code>을 볼 수 있다. 즉, <code>사전확률에 새롭게 추가되는 likelihood가 추론의 어떤 영향을 미치는지를 나타내는 베이즈 정리의 의미를 생각해 볼 수 있다</code>.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} P(weather state, Humidity | playing Tennis) \\ &= \frac{P(weather state)}{P(playing Tennis)} \frac{P(Humidity)}{P(playing Tennis)} \\ &= \frac{P(playing Tennis|weather state)P(weather state)}{P(playing Tennis)} \frac{P(playing Tennis|Humidity)P(Humidity)}{P(playing Tennis)} \end{align}</script><p><img src="/image/naive_bayesian_model_example_01.png" alt="나이브 베이지안 모델의 모수 추정 - 01"></p>
<p><img src="/image/naive_bayesian_model_example_02.png" alt="나이브 베이지안 모델의 모수 추정 - 02"></p>
<ul>
<li>아래 그림과 같이 데이터가 주어졌을 때의 베이즈 정리를 이용해 계산하면 다음과 같다.</li>
</ul>
<p><img src="/image/Naive_bayesian_probability.png" alt="베이즈 정리 확률 예시"></p>
<ul>
<li>예시로 계산해 본 것과 같이 아래의 그림처럼 독립변수들이 조건부 독립이라는 조건을 통해 곱으로 표현 될 수 있다. 예측값은 마지막 수식에서 보는 것과 같이 해당 가능도함수의 확률값을 최대로 해주는 class를 선택하여 예측값으로 출력해준다.</li>
</ul>
<p><a href="https://datascienceschool.net/view-notebook/864a2cc43df44531be32e3fa48769501/" target="_blank" rel="noopener">참조 : MLE 개념</a></p>
<p><img src="/image/Naive_bayesian_model_estimaion_01.png" alt="나이브 베이지안 모형 모수 추정식"></p>
<ul>
<li>이제 하나씩 앞에서 언급했던 순서대로 계산하여 최종적으로 출력을 내는 단계까지 그림을 통해 살펴볼 것이다.</li>
</ul>
<p><img src="/image/Naive_bayesian_model_estimaion_02.png" alt="나이브 베이지안 모형 계산 - 01"></p>
<p><img src="/image/Naive_bayesian_model_estimaion_03.png" alt="나이브 베이지안 모형 계산 - 02"></p>
<p><img src="/image/Naive_bayesian_model_estimaion_04.png" alt="나이브 베이지안 모형 계산 - 03"></p>
<p><img src="/image/Naive_bayesian_model_estimaion_05.png" alt="나이브 베이지안 모형 계산 - 04"></p>
<p><img src="/image/Naive_bayesian_model_estimaion_06.png" alt="나이브 베이지안 모형 계산 - 05"></p>
<h3 id="나이브-베이지안-모형의-종류"><a href="#나이브-베이지안-모형의-종류" class="headerlink" title="나이브 베이지안 모형의 종류"></a>나이브 베이지안 모형의 종류</h3><p><img src="/image/Naive_bayesian_classifier_type.png" alt="나이브 베이지안 모형의 종류"></p>
<h4 id="나이브-가정"><a href="#나이브-가정" class="headerlink" title="나이브 가정"></a>나이브 가정</h4><ul>
<li>독립변수 $x$가 $D$차원이라고 가정하자.</li>
</ul>
<script type="math/tex; mode=display">x = (x_1, \ldots, x_D)</script><ul>
<li>가능도 함수는 $x_{1}, \ldots, x_{D}$의 결합확률이 된다.</li>
</ul>
<script type="math/tex; mode=display">P(x \mid y = k) = P(x_1, \ldots, x_D \mid y = k)</script><ul>
<li>원리상으로는 $y=k$인 데이터만 모아서 이 가능도함수의 모양을 추정할 수 있다. 하지만 차원 $D$가 커지면 가능도함수의 추정이 현실적으로 어려워진다. 따라서 나이브베이즈 분류모형(Naive Bayes classification model)에서는 모든 차원의 개별 독립변수가 서로 조건부 독립(conditional independent)이라는 가정을 사용한다. 이러한 가정을 나이브 가정(naive assumption)이라고 한다.</li>
</ul>
<ul>
<li>나이브 가정으로 사용하면 벡터 $x$의 결합확률분포함수는 개별 스칼라 원소 $x_{d}$의 확률분포함수의 곱이 된다.</li>
</ul>
<script type="math/tex; mode=display">P(x_1, \ldots, x_D \mid y = k) = \prod_{d=1}^D P(x_d \mid y = k)</script><ul>
<li>스칼라 원소 $x_{d}$의 확률분포함수는 결합확률분포함수보다 추정하기 훨씬 쉽다. 가능도함수를 추정한 후에는 베이즈 정리를 사용하며 조건부확률을 계산할 수 있다.</li>
</ul>
<script type="math/tex; mode=display">\begin{align} P(y = k \mid x) &= \dfrac{ P(x_1, \ldots, x_D \mid y = k) P(y = k) }{P(x)} \\ &= \dfrac{ \left(\prod_{d=1}^D P(x_{d} \mid y = k) \right) P(y = k) }{P(x)} \end{align}</script><h4 id="가우시안-분포-정규분포-가능도-모형"><a href="#가우시안-분포-정규분포-가능도-모형" class="headerlink" title="가우시안 분포(정규분포) 가능도 모형"></a>가우시안 분포(정규분포) 가능도 모형</h4><ul>
<li>$x$벡터의 원소가 모두 실수이고 클래스마다 특정한 값 주변에서 발생한다고 하면 가능도 분포로 정규분포를 사용한다. 각 독립변수 $x_{d}$마다, 그리고 클래스 $k$마다 정규 분포의 기대값 $\mu_{d,k}$, 표준편차 $\sigma^{2}_{d,k}$가 달라진다. QDA모형과는 달리 모든 독립변수들이 서로 조건부독립이라고 가정한다.</li>
</ul>
<script type="math/tex; mode=display">P(x_d \mid y = k) = \dfrac{1}{\sqrt{2\pi\sigma_{d,k}^2}} \exp \left(-\dfrac{(x_d-\mu_{d,k})^2}{2\sigma_{d,k}^2}\right)</script><p><img src="/image/gaussian_dist_likelihood_model.png" alt="정규분포 가능도 추정 방법"></p>
<h4 id="베르누이-분포-가능도-모형"><a href="#베르누이-분포-가능도-모형" class="headerlink" title="베르누이 분포 가능도 모형"></a>베르누이 분포 가능도 모형</h4><ul>
<li><p>베이누이분포 가능도 모형에서는 각각의 $x = (x_1,\ldots, x_D)$의 각 원소 $x_{d}$가 0 또는 1이라는 값만을 가질 수 있다. 독립변수는 $D$개의 독립적인 베르누이 확률변수, 동전으로 구성된 동전 세트로 표현할 수 있다. 이 동전들의 모수 $\mu_{d}$는 동전 $d$마다 다르다.</p>
</li>
<li><p>그런데 class $y=k (k = 1,\ldots, K)$마다도 $x_{d}$가 1이 될 확률이 다르다. 즉, 동전의 모수 $\mu_{d,k}$는 동전 $d$마다 다르고 class $k$마다도 다르다. 즉, 전체 $ D \times K $의 조합의 동전이 존재하며 같은 class에 속하는 D개의 동전이 하나의 동전 세트를 구성하고 이러한 동전 세트가 $K$개 있다고 생각할 수 있다.</p>
</li>
</ul>
<script type="math/tex; mode=display">P(x_d \mid y = k) = \mu_{d,k}^{x_d} (1-\mu_{d,k})^{(1-x_d)}</script><script type="math/tex; mode=display">P(x_1, \ldots, x_D \mid y = k)
= \prod_{d=1}^D \mu_{d,k}^{x_d} (1-\mu_{d,k})^{(1-x_d)}</script><ul>
<li>이러한 동전 세트마다 확률 특성이 다르므로 베르누이분포 가능도 모형을 기반으로 하는 나이브베이즈 모형은 동전 세트를 $N$번 던진 결과로부터 $1, \ldots, K$ 중 어느 동전 세트를 던졌는지를 찾아내는 모형이라고 할 수 있다.</li>
</ul>
<h4 id="다항분포-Multinomial-Distribution-가능도-모형"><a href="#다항분포-Multinomial-Distribution-가능도-모형" class="headerlink" title="다항분포(Multinomial Distribution) 가능도 모형"></a>다항분포(Multinomial Distribution) 가능도 모형</h4><ul>
<li>다항분포 모형에서는 $x$ 벡터가 다항분포의 표본이라고 가정한다. 즉, $D$개의 면을 가지는 주사위를 $\sum_{d=1}^D x_d$ 번 던져서 나온 결과로 본다. 예를 들어 $x$가 다음과 같다면, $x=(1, 4, 0, 5)$ 4면체 주사위를 10번 던져서 1인 면이 1번, 2인 면이 4번, 4인 면이 5번 나온 결과로 해석한다. 각 class 마다 주사위가 다르다고 가정하므로 $K$개의 class를 구분하는 문제에서는 $D$개의 면을 가진 주사위가 $K$개 있다고 본다.</li>
</ul>
<script type="math/tex; mode=display">P(x_1, \ldots, x_D \mid y = k) \;\; \propto \;\; \prod_{d=1}^D \mu_{d,k}^{x_{d,k}}</script><script type="math/tex; mode=display">\sum_{d=1}^{D} \mu_{d,k} = 1</script><ul>
<li>따라서 다항분포 가능도 모형을 기반으로 하는 나이브 베이즈 모형은 주사위를 던진 결과로부터 $1, \ldots, K$ 중 어느 주사위를 던졌는지를 찾아내는 모형이라고 할 수 있다.</li>
</ul>
<h2 id="Naive-Bayes-실습"><a href="#Naive-Bayes-실습" class="headerlink" title="Naive Bayes 실습"></a>Naive Bayes 실습</h2><h3 id="1-Gaussian-Naive-Bayes"><a href="#1-Gaussian-Naive-Bayes" class="headerlink" title="1. Gaussian Naive Bayes"></a>1. Gaussian Naive Bayes</h3><ul>
<li>데이터, 모듈 불러오기<ul>
<li>iris(붓꽃)데이터를 가지고 실습을 진행할 것이다.</li>
<li>붓꽃의 종류는 3가지(‘setosa’, ‘versicolor’, ‘virginica’)이며, 각 feature는 sepal length (cm),    sepal width (cm), petal length (cm), petal width (cm) 로 이루어져있다. 모든 피처가 실수의 값을 갖기 때문에 가우시안 나이브 베이즈 모형을 사용할 것이다.</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">df_X=pd.DataFrame(iris.data, columns=iris.feature_names)</span><br><span class="line">df_Y=pd.DataFrame(iris.target, columns=[<span class="string">"target"</span>])</span><br></pre></td></tr></table></figure>
<ul>
<li>예측의 성능은 항상 테스트 데이터를 통해 측정해야 하므로 분리해준다. 지금은 연습삼아 해보는 것이므로 필자는 따로 validation set을 구축하지 않겠다. 허나 실제로 데이터를 분석하고 그에 따른 성능을 측정하려면 꼭 validation set을 따로 두어 베이스라인 모델의 parameter들을 튜닝을 통해 모델의 성능을 높이도록 한 뒤 최종적으로 test set을 예측하여 성능을 검증해야 할 것이다.  </li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, test_X, train_Y, test_Y = train_test_split(df_X, df_Y, train_size=0.8, test_size=0.2, random_state=123)</span><br></pre></td></tr></table></figure>
<ul>
<li>모델 피팅</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GNB = GaussianNB()</span><br><span class="line">fitted = GNB.fit(train_X, train_Y)</span><br><span class="line">y_pred = fitted.predict(test_X)</span><br><span class="line">y_pred</span><br></pre></td></tr></table></figure>
<ul>
<li>확률 구하기<ul>
<li>먼저 예측한 클래스와 해당 예측 데이터의 클래스별 확률을 살펴 볼 것이다.</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.predict(test_X)[:1]</span><br></pre></td></tr></table></figure>
<h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.predict_proba(test_X)[:1]</span><br></pre></td></tr></table></figure>
<h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[7.24143720e-126, 9.23061979e-001, 7.69380215e-002]])</span><br></pre></td></tr></table></figure>
<ul>
<li>위의 확률 직접 구해보기</li>
</ul>
<p>1) 위의 확률 값이 나오게 된 중간과정을 살펴보자. 우선 추정된 독립변수의 모수와 정규 분포의 확률밀도 함수를 사용하여 가능도를 구할 수 있다.</p>
<script type="math/tex; mode=display">p(x_1, x_2, x_3, x_4 | y) \propto p(x_1) p(x_2) p(x_3) p(x_4)</script><pre><code>- 위에서 학습했을 때도 말했던 것 처럼, (class개수 * 변수의 개수)개의 조합의 모수를 갖고 있으므로 아래와 같이 12개의 모수를 갖는다. 아래에서는 각 class별로 정규분포의 모수인 평균과 분산을 보여준다.
</code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">predict_data = np.array(test_X.iloc[0])</span><br><span class="line">predict_data</span><br></pre></td></tr></table></figure>
<h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([6.3, 2.5, 4.9, 1.5])</span><br></pre></td></tr></table></figure>
<ul>
<li>추정한 모델의 클래스별 모수(평균과 분산)을 다음과 같이 알 수 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.theta_[0], fitted.sigma_[0]</span><br></pre></td></tr></table></figure>
<h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([5.01621622, 3.43243243, 1.46756757, 0.25945946]),</span><br><span class="line"> array([0.10568298, 0.14975895, 0.02705625, 0.01214025]))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.theta_[1], fitted.sigma_[1]</span><br></pre></td></tr></table></figure>
<h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([5.95      , 2.78409091, 4.24090909, 1.32272727]),</span><br><span class="line"> array([0.27068182, 0.10042872, 0.22741736, 0.04221075]))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.theta_[2], fitted.sigma_[2]</span><br></pre></td></tr></table></figure>
<h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(array([6.58717949, 2.95897436, 5.57948718, 2.02820513]),</span><br><span class="line"> array([0.39752795, 0.11011177, 0.29188692, 0.0774096 ]))</span><br></pre></td></tr></table></figure>
<ul>
<li>위의 모수들을 통해 class별 가능도를 구하면 아래와 같을 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">likelihood = [</span><br><span class="line">(sp.stats.norm(fitted.theta_[0][0], np.sqrt(fitted.sigma_[0][0])).pdf(predict_data[0]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[0][1], np.sqrt(fitted.sigma_[0][1])).pdf(predict_data[1]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[0][2], np.sqrt(fitted.sigma_[0][2])).pdf(predict_data[2]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[0][3], np.sqrt(fitted.sigma_[0][3])).pdf(predict_data[3])),\</span><br><span class="line">(sp.stats.norm(fitted.theta_[1][0], np.sqrt(fitted.sigma_[1][0])).pdf(predict_data[0]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[1][1], np.sqrt(fitted.sigma_[1][1])).pdf(predict_data[1]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[1][2], np.sqrt(fitted.sigma_[1][2])).pdf(predict_data[2]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[1][3], np.sqrt(fitted.sigma_[1][3])).pdf(predict_data[3])),\</span><br><span class="line">(sp.stats.norm(fitted.theta_[2][0], np.sqrt(fitted.sigma_[0][0])).pdf(predict_data[0]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[2][1], np.sqrt(fitted.sigma_[0][1])).pdf(predict_data[1]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[2][2], np.sqrt(fitted.sigma_[0][2])).pdf(predict_data[2]) * \</span><br><span class="line">sp.stats.norm(fitted.theta_[2][3], np.sqrt(fitted.sigma_[0][3])).pdf(predict_data[3]))    </span><br><span class="line">]</span><br><span class="line">likelihood</span><br></pre></td></tr></table></figure>
<h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[2.0700298536453225e-126, 0.2218869448618605, 7.497361843154609e-09]</span><br></pre></td></tr></table></figure>
<ul>
<li>여기에 사전확률을 곱하면 사후 확률에 비례하는 값이 나온다.</li>
</ul>
<script type="math/tex; mode=display">p(y|x_1, x_2) \propto p(x_1, x_2|y) p(y)</script><blockquote>
<p>아직 정규화 상수 $p(x)$로 나누어주지 않았으므로 두 값의 합이 1이 아니다. 즉, 확률이라고 부를 수는 없다. 하지만 크기를 비교하면 이 데이터는 $y=1$일 확률이 가장 높다는 것을 알 수 있다.</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.class_prior_</span><br></pre></td></tr></table></figure>
<h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.30833333, 0.36666667, 0.325     ])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">posterior = likelihood * fitted.class_prior_</span><br><span class="line">posterior</span><br></pre></td></tr></table></figure>
<h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([6.38259205e-127, 8.13585464e-002, 2.43664260e-009])</span><br></pre></td></tr></table></figure>
<ul>
<li>이 값을 정규화하면 predict_proba 메서드로 구한 것과 같은 값이 나온다. 물론 완벽히 일치하진 않지만 그에 근사하는 값을 추정값으로 계산해내서 얻을 수 있다. 이는 계산시 소수점을 어느정도까지 사용하였는지에 따라 다르기 때문에 이 정도의 오차는 문제가 없다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">posterior / np.sum(posterior, axis=0)</span><br></pre></td></tr></table></figure>
<h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([7.84501707e-126, 9.99999970e-001, 2.99494353e-008])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.predict_proba(test_X)[:1]</span><br></pre></td></tr></table></figure>
<h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[7.24143720e-126, 9.23061979e-001, 7.69380215e-002]])</span><br></pre></td></tr></table></figure>
<ul>
<li>Confusion matrix 구하기</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line">confusion_matrix(y_pred, test_Y)</span><br></pre></td></tr></table></figure>
<h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[13,  0,  0],</span><br><span class="line">       [ 0,  6,  1],</span><br><span class="line">       [ 0,  0, 10]])</span><br></pre></td></tr></table></figure>
<ul>
<li>Prior 설정하기<ul>
<li>이번에는 class가 발생되는 사전확률을 미리 알고 있었던 경우라고 가정하고 문제를 풀어볼 것이다.</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GNB2 = GaussianNB(priors=[0.01, 0.01, 0.98])</span><br><span class="line">set_prior_fitted_01 = GNB2.fit(train_X, train_Y)</span><br><span class="line">set_prior_pred_01 = set_prior_fitted_01.predict(test_X)</span><br><span class="line">confusion_matrix(set_prior_pred_01, test_Y)</span><br></pre></td></tr></table></figure>
<h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[13,  0,  0],</span><br><span class="line">       [ 0,  4,  0],</span><br><span class="line">       [ 0,  2, 11]])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GNB3 = GaussianNB(priors=[0.01, 0.98, 0.01])</span><br><span class="line">set_prior_fitted_02 = GNB3.fit(train_X, train_Y)</span><br><span class="line">set_prior_pred_02 = set_prior_fitted_02.predict(test_X)</span><br><span class="line">confusion_matrix(set_prior_pred_02, test_Y)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array([[13,  0,  0],</span><br><span class="line">       [ 0,  6,  4],</span><br><span class="line">       [ 0,  0,  7]])</span><br></pre></td></tr></table></figure>
<h3 id="2-Bernoulli-naive-bayes"><a href="#2-Bernoulli-naive-bayes" class="headerlink" title="2. Bernoulli naive bayes"></a>2. Bernoulli naive bayes</h3><ul>
<li>e-mail과 같은 문서 내에 특정한 단어가 포함되어 있는지의 여부는 베르누이 확률변수로 모형화할 수 있다. 이렇게 <code>독립변수가 0 또는 1의 값을 가지면 베르누이 나이브베이즈 모형을 사용</code>한다.</li>
</ul>
<ul>
<li><p>python의 sklearn의 베르누이분포 나이브베이즈 모형 클래스 <code>BernoulliNB</code>는 가능도 추정과 관련하여 다음 속성을 가진다.</p>
<ul>
<li><code>feature_count_</code> : 각 class k에 대해 d번째 동전이 앞면이 나온 횟수 $N_{d,k}$</li>
<li><code>feature_log_prob_</code> : 베르누이분포 모수의 로그값</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\log \mu_k = (\log \mu_{1,k}, \ldots, \log \mu_{D, k}) = \left( \log \dfrac{N_{1,k}}{N_k}, \ldots, \log \dfrac{N_{D,k}}{N_k} \right)</script><ul>
<li>여기에서 $N_{k}$은 클래스 k에 대해 동전을 던진 총 횟수이다. <code>표본 데이터의 수가 적은 경우에는 모수에 대해 다음처럼 스무딩(smoothing)을 할 수도 있다.</code></li>
</ul>
<h4 id="스무딩-Smoothing"><a href="#스무딩-Smoothing" class="headerlink" title="스무딩(Smoothing)"></a>스무딩(Smoothing)</h4><ul>
<li>표본 데이터의 수가 적은 경우에는 베르누이 모수가 0 또는 1이라는 극단적인 모수 추정값이 나올 수도 있다. 하지만 현실적으로는 실제 모수값이 이런 극단적인 값이 나올 가능성이 적다. <code>따라서 베르누이 모수가 0.5인 가장 일반적인 경우를 가정하여 0이 나오는 경우와 1이 나오는 경우, 두 개의 가상 표본 데이터를 추가</code>한다. 그러면 0이나 1과 같은 극단적인 추정값이 0.5에 가까운 다음과 같은 값으로 변한다. 이를 <code>라플라스 스무딩(Laplace smoothing)</code> 또는 <code>애드원(Add-One) 스무딩</code>이라고 한다.</li>
</ul>
<script type="math/tex; mode=display">\hat{\mu}_{d,k} = \frac{ N_{d,k} + \alpha}{N_k + 2 \alpha}</script><ul>
<li><p>가중치 $\alpha$를 사용하여 스무딩의 정도를 조절할 수도 있다. 가중치 $\alpha$는 정수가 아니라도 괜찮다. 가중치가 1인 경우는 무정보 사전확률을 사용한 베이즈 모수추정의 결과와 같다.</p>
</li>
<li><p>아래의 데이터는 4개의 key word를 사용하여 정상 메일 4개와 spam 메일 6개를 BOW 인코딩한 행렬이다. 예를 들어 첫번째 메일은 정상 메일이고 1번, 4번 key word는 포함하지 않지만 2번,3번 key word를 포함한다고 볼 수 있다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.naive_bayes import BernoulliNB</span><br><span class="line">X = np.array([</span><br><span class="line">    [0, 1, 1, 0],</span><br><span class="line">    [1, 1, 1, 1],</span><br><span class="line">    [1, 1, 1, 0],</span><br><span class="line">    [0, 1, 0, 0],</span><br><span class="line">    [0, 0, 0, 1],</span><br><span class="line">    [0, 1, 1, 0],</span><br><span class="line">    [0, 1, 1, 1],</span><br><span class="line">    [1, 0, 1, 0],</span><br><span class="line">    [1, 0, 1, 1],</span><br><span class="line">    [0, 1, 1, 0]])</span><br><span class="line">y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])</span><br><span class="line"></span><br><span class="line">BNB = BernoulliNB()</span><br><span class="line">fitted = BNB.fit(X, y)</span><br></pre></td></tr></table></figure>
<ul>
<li>y 클래스의 종류와 각 클래스에 속하는 표본의 수, 그리고 그 값으로부터 구한 사전 확률의 값은 다음과 같다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.classes_</span><br></pre></td></tr></table></figure>
<h5 id="결과-13"><a href="#결과-13" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0, 1])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.class_count_</span><br></pre></td></tr></table></figure>
<h5 id="결과-14"><a href="#결과-14" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([4., 6.])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.exp(fitted.class_log_prior_)</span><br></pre></td></tr></table></figure>
<h5 id="결과-15"><a href="#결과-15" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.4, 0.6])</span><br></pre></td></tr></table></figure>
<ul>
<li>각 클래스 k 별로, 그리고 각 독립변수 d 별로, 각각 다른 베르누이 확률변수라고 가정하여 모두 8개의 베르누이 확률변수의 모수를 구하면 다음과 같다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 행은 클래스의 개수, 열은 변수의 개수를 의미</span></span><br><span class="line">count = fitted.feature_count_</span><br><span class="line">count</span><br></pre></td></tr></table></figure>
<h5 id="결과-16"><a href="#결과-16" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[2., 4., 3., 1.],</span><br><span class="line">       [2., 3., 5., 3.]])</span><br></pre></td></tr></table></figure>
<ul>
<li>위의 각 변수의 클래스별로 몇번 나온지에 대한 행렬에 각 클래스의 전체 개수로 나누어 주어야 해당 변수들의 모수인 p값을 알 수 있으므로 class_count의 배열의 모양을 변형시켜주어야 한다. 현재는 1차원의 벡터(2,)이므로 2차원의 (2,1)의 모양을 갖도록 해주어야 나누어 줄 수 있기 때문이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count / fitted.class_count_[:,np.newaxis]</span><br></pre></td></tr></table></figure>
<h5 id="결과-17"><a href="#결과-17" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.5       , 1.        , 0.75      , 0.25      ],</span><br><span class="line">       [0.33333333, 0.5       , 0.83333333, 0.5       ]])</span><br></pre></td></tr></table></figure>
<ul>
<li>위에서는 필자는 Numpy의 brodcasting 연산을 사용하여 구한것인데, 혹시 count 행렬의 모양과 동일하게 만들어 확실하게 연산하고 싶다면, 아래와 같이 실행하면 동일한 결과를 얻을 수 있는 것을 볼 수 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count / np.repeat(fitted.class_count_[:,np.newaxis], 4, axis=1)</span><br></pre></td></tr></table></figure>
<h5 id="결과-18"><a href="#결과-18" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.5       , 1.        , 0.75      , 0.25      ],</span><br><span class="line">       [0.33333333, 0.5       , 0.83333333, 0.5       ]])</span><br></pre></td></tr></table></figure>
<ul>
<li>그런데 이 값은 모형 내에서 구한 값과 다르다. 모형 내에서 스무딩(smoothing)이 이루어지기 때문이다. 스무딩은 동전의 각 면 즉, 0과 1이 나오는 가상의 데이터를 추가함으로서 추정한 모수의 값이 좀 더 0.5에 가까워지도록 하는 방법이다. 이 때 사용한 스무딩 가중치 값은 다음처럼 확인할 수 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.alpha</span><br></pre></td></tr></table></figure>
<h5 id="결과-19"><a href="#결과-19" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta = np.exp(fitted.feature_log_prob_)</span><br><span class="line">theta</span><br></pre></td></tr></table></figure>
<h5 id="결과-20"><a href="#결과-20" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.5       , 0.83333333, 0.66666667, 0.33333333],</span><br><span class="line">       [0.375     , 0.5       , 0.75      , 0.5       ]])</span><br></pre></td></tr></table></figure>
<ul>
<li>이에 모형이 완성되었으니 테스트 데이터를 사용하여 예측을 해 본다. 예를 들어 1번, 2번 키워드를 포함한 메일이 정상 메일인지 스팸 메일인지 알아보자.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = np.array([0, 1, 1, 1])</span><br><span class="line">fitted.predict_proba([x_new])</span><br></pre></td></tr></table></figure>
<h5 id="결과-21"><a href="#결과-21" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.34501348, 0.65498652]])</span><br></pre></td></tr></table></figure>
<ul>
<li>위 결과에서 정상 메일일 가능성이 약 3배임을 알 수 있다. 이 값은 다음처럼 구할 수도 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># np.prod(axis=1)로 해준 이유는 아래 각 변수별로 독립이므로 가능도를 구하려면 곱을 해주어야 하기 때문</span></span><br><span class="line">p = ((theta ** x_new) * (1 - theta) ** (1 - x_new)).prod(axis=1) * np.exp(fitted.class_log_prior_)</span><br><span class="line">p / p.sum()</span><br></pre></td></tr></table></figure>
<h5 id="결과-22"><a href="#결과-22" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.34501348, 0.65498652])</span><br></pre></td></tr></table></figure>
<ul>
<li>반대로 3번, 4번 keyword가 포함된 메일은 스팸일 가능성이 약 90%이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = np.array([0, 0, 1, 1])</span><br><span class="line">fitted.predict_proba([x_new])</span><br></pre></td></tr></table></figure>
<h5 id="결과-23"><a href="#결과-23" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.09530901, 0.90469099]])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = ((theta ** x_new) * (1 - theta) ** (1 - x_new)).prod(axis=1) * np.exp(fitted.class_log_prior_)</span><br><span class="line">p / p.sum()</span><br></pre></td></tr></table></figure>
<h5 id="결과-24"><a href="#결과-24" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.09530901, 0.90469099]])</span><br></pre></td></tr></table></figure>
<ul>
<li>MNIST 숫자 분류문제에서 sklearn.preprocessing.Binarizer로 x값을 0, 1로 바꾼다(값이 8 이상이면 1, 8 미만이면 0). 즉 흰색과 검은색 픽셀로만 구성된 이미지로 만든다(다음 코드 참조)</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">digits = load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line">X = Binarizer(7).fit_transform(X)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>이 이미지에 대해 베르누이 나이브베이즈 모형을 적용하자. 분류 결과를 분류보고서 형식으로 나타내라.</p>
<ul>
<li><p>(1) BernoulliNB 클래스의 binarize 인수를 사용하여 같은 문제를 풀어본다.</p>
</li>
<li><p>(2) 계산된 모형의 모수 벡터 값을 각 클래스별로 8x8 이미지의 형태로 나타내라. 이 이미지는 무엇을 뜻하는가?</p>
</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">digits.images[0]</span><br></pre></td></tr></table></figure>
<h5 id="결과-25"><a href="#결과-25" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],</span><br><span class="line">       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],</span><br><span class="line">       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],</span><br><span class="line">       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],</span><br><span class="line">       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],</span><br><span class="line">       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],</span><br><span class="line">       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],</span><br><span class="line">       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape</span><br></pre></td></tr></table></figure>
<h5 id="결과-26"><a href="#결과-26" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(1797, 64)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BNB =  BernoulliNB()</span><br><span class="line">fitted = BNB.fit(X, y)</span><br><span class="line">theta = np.exp(fitted.feature_log_prob_)</span><br><span class="line">theta = theta.reshape((10, 8, 8))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">fig, axes = plt.subplots(2, 5, figsize=(12, 3),</span><br><span class="line">                         subplot_kw=&#123;<span class="string">'xticks'</span>: [], <span class="string">'yticks'</span>: []&#125;)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(5):</span><br><span class="line">    axes[0][i].set_title(<span class="string">"class &#123;&#125;"</span>.format(i))</span><br><span class="line">    axes[0][i].imshow(theta[i], interpolation=<span class="string">'nearest'</span>, cmap=plt.cm.Blues)</span><br><span class="line">    axes[1][i].set_title(<span class="string">"class &#123;&#125;"</span>.format(i+5))</span><br><span class="line">    axes[1][i].imshow(theta[i+5], interpolation=<span class="string">'nearest'</span>, cmap=plt.cm.Blues)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/parameter_vector_of_MNIST_visualization.png" alt="MNIST 베르누이 나이브 베이즈 모형 모수벡터 시각화"></p>
<ul>
<li>위의 이미지에서는 모수값이 높은 변수가 진한 파란색을 띄게 된다. sklearn.preprocessing.Binarizer를 통해 x값을 값이 8 이상이면 1, 8 미만이면 0으로 바꾸어주었으므로 <code>8 미만인 데이터보다는 8이상인 데이터가 각 클래스를 구분하는데 좀 더 영향을 주는 공간을 알 수 있게 해준다.</code></li>
</ul>
<h3 id="3-Multinomial-naive-bayes"><a href="#3-Multinomial-naive-bayes" class="headerlink" title="3. Multinomial naive bayes"></a>3. Multinomial naive bayes</h3><ul>
<li>다항분포 나이브베이즈 모형 클래스 <code>MultinomialNB</code>는 가능도 추정과 관련하여 다음 속성을 가진다.<ul>
<li><code>feature_count_</code> : 각 클래스 $k$에서 $d$번째 면이 나온 횟수 $N_{d,k}$</li>
<li><code>feature_log_prob_</code> : 다항분포의 모수의 로그</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">\log \mu_k = (\log \mu_{1,k}, \ldots, \log \mu_{D, k}) = \left( \log \dfrac{N_{1,k}}{N_k}, \ldots, \log \dfrac{N_{D,k}}{N_k} \right)</script><ul>
<li>여기에서 $N_{k}$은 클래스 $k$에 대해 주사위를 던진 총 회수를 뜻한다.</li>
</ul>
<ul>
<li>스무딩공식은 아래와 같다.</li>
</ul>
<script type="math/tex; mode=display">\hat{\mu}_{d,k} = \frac{ N_{d,k} + \alpha}{N_k + D \alpha} , \; (D=변수의 개수)</script><ul>
<li>이번에도 스팸 메일 필터링을 예로 들어보다. 다만 BOW 인코딩을 할 때, 각 키워드가 출현한 빈도를 직접 입력 변수로 사용한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([</span><br><span class="line">    [3, 4, 1, 2],</span><br><span class="line">    [3, 5, 1, 1],</span><br><span class="line">    [3, 3, 0, 4],</span><br><span class="line">    [3, 4, 1, 2],</span><br><span class="line">    [1, 2, 1, 4],</span><br><span class="line">    [0, 0, 5, 3],</span><br><span class="line">    [1, 2, 4, 1],</span><br><span class="line">    [1, 1, 4, 2],</span><br><span class="line">    [0, 1, 2, 5],</span><br><span class="line">    [2, 1, 2, 3]])</span><br><span class="line">y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">MNB = MultinomialNB()</span><br><span class="line">fitted = MNB.fit(X, y)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.classes_</span><br></pre></td></tr></table></figure>
<h5 id="결과-27"><a href="#결과-27" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0, 1])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.class_count_</span><br></pre></td></tr></table></figure>
<h5 id="결과-28"><a href="#결과-28" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([4., 6.])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.exp(fitted.class_log_prior_)</span><br></pre></td></tr></table></figure>
<h5 id="결과-29"><a href="#결과-29" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.4, 0.6])</span><br></pre></td></tr></table></figure>
<ul>
<li>다음으로 각 클래스에 대한 가능도 확률분포를 구한다. 다항분포 모형을 사용하므로 각 클래스를 4개의 면을 가진 주사위로 생각할 수 있다. 그리고 각 면이 나올 확률은 각 면이 나온 횟수를 주사위를 던진 전체 횟수로 나누면 된다. 우선 각 클래스 별로 각각의 면이 나온 횟수는 다음과 같다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">count = fitted.feature_count_</span><br><span class="line">count</span><br></pre></td></tr></table></figure>
<h5 id="결과-30"><a href="#결과-30" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[12., 16.,  3.,  9.],</span><br><span class="line">       [ 5.,  7., 18., 18.]])</span><br></pre></td></tr></table></figure>
<ul>
<li>이 데이터에서 클래스 $Y=0$인 주사위를 던진 횟수는 첫번째 행의 값의 합인 40이므로 클래스 $Y=0$인 주사위를 던져 1이라는 면이 나올 확률은 다음처럼 계산할 수 있다.</li>
</ul>
<script type="math/tex; mode=display">\mu_{1,Y=0} = \dfrac{12}{40} = 0.3</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">count / np.repeat(count.sum(axis=1)[:,np.newaxis], 4, axis=1)</span><br></pre></td></tr></table></figure>
<h5 id="결과-31"><a href="#결과-31" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.3       , 0.4       , 0.075     , 0.225     ],</span><br><span class="line">       [0.10416667, 0.14583333, 0.375     , 0.375     ]])</span><br></pre></td></tr></table></figure>
<ul>
<li>실제로는 극단적인 추정을 피하기 위해 이 값을 가중치 1인 스무딩을 한 추정값을 사용한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fitted.alpha</span><br></pre></td></tr></table></figure>
<h5 id="결과-32"><a href="#결과-32" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1.0</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(count + fitted.alpha) / \</span><br><span class="line">(np.repeat(count.sum(axis=1)[:,np.newaxis], 4, axis=1) + fitted.alpha * X.shape[1])</span><br></pre></td></tr></table></figure>
<h5 id="결과-33"><a href="#결과-33" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.29545455, 0.38636364, 0.09090909, 0.22727273],</span><br><span class="line">       [0.11538462, 0.15384615, 0.36538462, 0.36538462]])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta = np.exp(fitted.feature_log_prob_)</span><br><span class="line">theta</span><br></pre></td></tr></table></figure>
<h5 id="결과-34"><a href="#결과-34" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[0.29545455, 0.38636364, 0.09090909, 0.22727273],</span><br><span class="line">       [0.11538462, 0.15384615, 0.36538462, 0.36538462]])</span><br></pre></td></tr></table></figure>
<ul>
<li>이제 이 값을 사용하여 예측을 해 보자. 만약 어떤 메일에 1번부터 4번까지의 키워드가 각각 10번씩 나왔다면 다음처럼 확률을 구할 수 있다. 구해진 확률로부터 이 메일이 스팸임을 알 수 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_new = np.array([10, 10, 10, 10])</span><br><span class="line">fitted.predict_proba([x_new])</span><br></pre></td></tr></table></figure>
<h5 id="결과-35"><a href="#결과-35" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([[0.38848858, 0.61151142]])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p = (theta ** x_new).prod(axis=1) * np.exp(fitted.class_log_prior_)</span><br><span class="line">p / p.sum(axis=0)</span><br></pre></td></tr></table></figure>
<h5 id="결과-36"><a href="#결과-36" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0.38848858, 0.61151142])</span><br></pre></td></tr></table></figure>
<ul>
<li>MNIST 숫자 분류문제를 다항분포 나이브베이즈 모형을 사용하여 풀고 이진화(Binarizing)를 하여 베르누이 나이브베이즈 모형을 적용했을 경우와 성능을 비교하라.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line">from sklearn.metrics import classification_report</span><br><span class="line">digits = load_digits()</span><br><span class="line">X = digits.data</span><br><span class="line">y = digits.target</span><br><span class="line">train_X, test_X, train_Y, test_Y = train_test_split(X, y, test_size=0.3, random_state=123)</span><br><span class="line">from sklearn.preprocessing import Binarizer</span><br><span class="line">binary_train_X = Binarizer(7).fit_transform(train_X)</span><br><span class="line">binary_test_X = Binarizer(7).fit_transform(test_X)</span><br><span class="line">BNB = BernoulliNB().fit(binary_train_X, train_Y)</span><br><span class="line">bnb_pred = BNB.predict(binary_test_X)</span><br><span class="line">MNB = MultinomialNB().fit(train_X, train_Y)</span><br><span class="line">mnb_pred = MNB.predict(test_X)</span><br></pre></td></tr></table></figure>
<h5 id="이진화-한-베르누이-나이브베이즈-모형-성능"><a href="#이진화-한-베르누이-나이브베이즈-모형-성능" class="headerlink" title="이진화 한 베르누이 나이브베이즈 모형 성능"></a>이진화 한 베르누이 나이브베이즈 모형 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(bnb_pred, test_Y))</span><br></pre></td></tr></table></figure>
<h5 id="결과-37"><a href="#결과-37" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">            precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.98      0.98      0.98        59</span><br><span class="line">           1       0.82      0.84      0.83        55</span><br><span class="line">           2       0.87      0.90      0.88        51</span><br><span class="line">           3       0.80      0.93      0.86        40</span><br><span class="line">           4       0.95      0.97      0.96        60</span><br><span class="line">           5       0.84      0.94      0.89        51</span><br><span class="line">           6       0.96      1.00      0.98        55</span><br><span class="line">           7       1.00      0.89      0.94        56</span><br><span class="line">           8       0.85      0.80      0.83        51</span><br><span class="line">           9       0.87      0.74      0.80        62</span><br><span class="line"></span><br><span class="line">    accuracy                           0.90       540</span><br><span class="line">   macro avg       0.90      0.90      0.90       540</span><br><span class="line">weighted avg       0.90      0.90      0.90       540</span><br></pre></td></tr></table></figure>
<h5 id="다항-분포-나이브-베이즈-모형-성능"><a href="#다항-분포-나이브-베이즈-모형-성능" class="headerlink" title="다항 분포 나이브 베이즈 모형 성능"></a>다항 분포 나이브 베이즈 모형 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(mnb_pred, test_Y))</span><br></pre></td></tr></table></figure>
<h5 id="결과-38"><a href="#결과-38" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           0       0.98      1.00      0.99        58</span><br><span class="line">           1       0.77      0.88      0.82        49</span><br><span class="line">           2       0.83      0.88      0.85        50</span><br><span class="line">           3       0.89      1.00      0.94        41</span><br><span class="line">           4       0.97      0.92      0.94        64</span><br><span class="line">           5       0.74      1.00      0.85        42</span><br><span class="line">           6       0.98      0.98      0.98        57</span><br><span class="line">           7       1.00      0.89      0.94        56</span><br><span class="line">           8       0.81      0.74      0.77        53</span><br><span class="line">           9       0.89      0.67      0.76        70</span><br><span class="line"></span><br><span class="line">    accuracy                           0.89       540</span><br><span class="line">   macro avg       0.89      0.90      0.89       540</span><br><span class="line">weighted avg       0.89      0.89      0.89       540</span><br></pre></td></tr></table></figure>
<ul>
<li><p>텍스트 분석에서 TF-IDF 인코딩을 하면 단어의 빈도수가 정수가 아닌 실수값이 된다. 이런 경우에도 다항분포 모형을 적용할 수 있는가?</p>
<ul>
<li>정수가 아니더라도 해당 적용 가능하다! <code>다항분포의 모수를 추정할 때 해당 관측치의 변수가 갖는 값의 합으로 나누어주어 모수값을 구했는데, TF-idf 행렬은 row가 문서를 의미하고 전체 문서에서의 토큰들이 열을 이루게 되므로 해당 문서에서 어떠한 단어가 몇번 나온것인지에 대해 다항분포를 통해 계산할 수 있기 때문</code>이다.</li>
</ul>
</li>
<li><p><code>MultinomialNB를 사용할 경우 범주형으로 정수이면 사용하는 것이라고 생각하지말고 위의 예시 처럼 데이터 당 각 피처가 유기적으로 하나의 사건에서 파생되어 이루어질 수 있는지에 대해서 먼저 생각해보자. 통계적인 분포를 다항분포로 생각할 수 있는지를 확인해보자는 이야기</code>이다.</p>
</li>
<li><p>아래의 뉴스그룹 분류 문제를 통해 검증해보자.</p>
</li>
</ul>
<h4 id="뉴스그룹-분류"><a href="#뉴스그룹-분류" class="headerlink" title="뉴스그룹 분류"></a>뉴스그룹 분류</h4><ul>
<li>다음은 뉴스그룹 데이터에 대해 나이브베이즈 분류모형을 적용한 결과이다.<ul>
<li>문서는18846건</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_20newsgroups</span><br><span class="line"></span><br><span class="line">news = fetch_20newsgroups(subset=<span class="string">"all"</span>)</span><br><span class="line">X = news.data</span><br><span class="line">y = news.target</span><br><span class="line"></span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer, CountVectorizer</span><br><span class="line">from sklearn.naive_bayes import MultinomialNB</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">model1 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, CountVectorizer()),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br><span class="line">model2 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, TfidfVectorizer()),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br><span class="line">model3 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, TfidfVectorizer(stop_words=<span class="string">"english"</span>)),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br><span class="line">model4 = Pipeline([</span><br><span class="line">    (<span class="string">'vect'</span>, TfidfVectorizer(stop_words=<span class="string">"english"</span>,</span><br><span class="line">                             token_pattern=r<span class="string">"\b[a-z0-9_\-\.]+[a-z][a-z0-9_\-\.]+\b"</span>)),</span><br><span class="line">    (<span class="string">'model'</span>, MultinomialNB()),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">from sklearn.model_selection import cross_val_score, KFold</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i, model <span class="keyword">in</span> enumerate([model1, model2, model3, model4]):</span><br><span class="line">    scores = cross_val_score(model, X, y, cv=5)</span><br><span class="line">    <span class="built_in">print</span>((<span class="string">"Model&#123;0:d&#125;: Mean score: &#123;1:.3f&#125;"</span>).format(i + 1, np.mean(scores)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-39"><a href="#결과-39" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Model1: Mean score: 0.855</span><br><span class="line">Model2: Mean score: 0.856</span><br><span class="line">Model3: Mean score: 0.883</span><br><span class="line">Model4: Mean score: 0.888</span><br><span class="line">CPU <span class="built_in">times</span>: user 1min 35s, sys: 4.54 s, total: 1min 40s</span><br><span class="line">Wall time: 1min 53s</span><br></pre></td></tr></table></figure>
<ul>
<li>(1) 만약 독립변수로 실수 변수, 0 또는 1 값을 가지는 변수, 자연수 값을 가지는 변수가 섞여있다면 사이킷런에서 제공하는 나이브베이즈 클래스를 사용하여 풀 수 있는가?</li>
</ul>
<pre><code>- 위에서 likelihood를 직접 계산했던 것과 같이 `likelihood만을 각각 계산하여 각 변수들은 독립이라는 가정을 전제로하기 때문에 서로 곱한뒤에 베이즈정리식에 따라 최종적으로 확률값을 구해 클래스를 구분`할 수 있다.
</code></pre><ul>
<li>(2) 사이킷런에서 제공하는 분류문제 예제 중 숲의 수종을 예측하는 covtype 분류문제는 연속확률분포 특징과 베르누이확률분포 특징이 섞여있다. 이 문제를 사이킷런에서 제공하는 나이브베이즈 클래스를 사용하여 풀어라.</li>
</ul>
<h3 id="대표-수종-데이터-covtype"><a href="#대표-수종-데이터-covtype" class="headerlink" title="대표 수종 데이터(covtype)"></a>대표 수종 데이터(covtype)</h3><ul>
<li>대표 수종 데이터는 미국 삼림을 30×30m 영역으로 나누어 각 영역의 특징으로부터 대표적인 나무의 종류(species of tree)을 예측하기위한 데이터이다. 수종은 7종류이지만 특징 데이터가 54종류, 표본 데이터의 갯수가 581,012개에 달하는 대규모 데이터이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_covtype</span><br><span class="line">covtype = fetch_covtype()</span><br><span class="line"><span class="comment"># print(covtype.DESCR)</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(covtype.data,</span><br><span class="line">                  columns=[<span class="string">"x&#123;:02d&#125;"</span>.format(i + 1) <span class="keyword">for</span> i <span class="keyword">in</span> range(covtype.data.shape[1])],</span><br><span class="line">                  dtype=int)</span><br><span class="line">sy = pd.Series(covtype.target, dtype=<span class="string">"category"</span>)</span><br><span class="line">df[<span class="string">'covtype'</span>] = sy</span><br></pre></td></tr></table></figure>
<ul>
<li>각 특징 데이터가 가지는 값의 종류를 보면 1번부터 10번 특징은 실수값이고 11번부터 54번 특징은 이진 카테고리값이라는 것을 알 수 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.iloc[:, 10:54] = df.iloc[:, 10:54].astype(<span class="string">'category'</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>다음 플롯은 카테고리값에 따라 “x14” 특징의 값이 어떻게 변하는지 나타낸 것이다. “x14” 특징이 0인가 1인가를 사용하면 1, 5, 7번 클래스와 4번 클래스는 완벽하게 분류할 수 있다는 것을 알 수 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(10,12))</span><br><span class="line">df_count = df.pivot_table(index=<span class="string">"covtype"</span>, columns=<span class="string">"x14"</span>, aggfunc=<span class="string">"size"</span>)</span><br><span class="line">sns.heatmap(df_count, cmap=sns.light_palette(<span class="string">"gray"</span>, as_cmap=True), annot=True, fmt=<span class="string">"0"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/categorical_variable_of_tree_binary.png" alt="x14 피처로 분류할 수 있는 클래스"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Gaussian_df_X = df.iloc[:, :10]</span><br><span class="line">Bern_df_X = df.iloc[:, 10:-1]</span><br><span class="line">df_Y = df.iloc[:, -1]</span><br><span class="line"></span><br><span class="line">GNB = GaussianNB()</span><br><span class="line">fitted_GNB = GNB.fit(Gaussian_df_X, df_Y)</span><br><span class="line"></span><br><span class="line">theta = fitted_GNB.theta_</span><br><span class="line">sigma = fitted_GNB.sigma_</span><br></pre></td></tr></table></figure>
<ul>
<li>가능도를 계산하기 위한 함수를 작성하였다.<ul>
<li>아래 함수는 반복문을 통해 실행하는 방식인데 데이터 수가 많다면 너무 비효율적이다. 그러므로, 데이터(관측치)의 수가 적은 경우에만 이용하는 것을 권장한다.</li>
<li>그래서 또 직접적으로 for문을 돌리지 않고 사용할 수 있는 방식의 함수를 다시 구현하였다. for문으로 반복문을 작성한것과 비교했을 때는 직관적으로 어떻게 가능도를 구하는 지 알 수 있다.</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def Gaussian_likelihood_cal(predict_data, theta, sigma, class_count, feature_count):</span><br><span class="line">    likelihood = []</span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> np.arange(class_count):</span><br><span class="line">        prod = 1</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> np.arange(feature_count):</span><br><span class="line">            prod = prod * sp.stats.norm(theta[c][f], np.sqrt(sigma[c][f])).pdf(predict_data[f])</span><br><span class="line">        likelihood.append(prod)</span><br><span class="line">    <span class="built_in">return</span> likelihood</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">def Gaussian_likelihood_cal(predict_data, theta, sigma):</span><br><span class="line">    likelihood = [(sp.stats.norm(theta[0][0], np.sqrt(sigma[0][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][1], np.sqrt(sigma[0][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][2], np.sqrt(sigma[0][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][3], np.sqrt(sigma[0][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][4], np.sqrt(sigma[0][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][5], np.sqrt(sigma[0][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][6], np.sqrt(sigma[0][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][7], np.sqrt(sigma[0][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][8], np.sqrt(sigma[0][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[0][9], np.sqrt(sigma[0][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[1][0], np.sqrt(sigma[1][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][1], np.sqrt(sigma[1][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][2], np.sqrt(sigma[1][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][3], np.sqrt(sigma[1][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][4], np.sqrt(sigma[1][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][5], np.sqrt(sigma[1][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][6], np.sqrt(sigma[1][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][7], np.sqrt(sigma[1][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][8], np.sqrt(sigma[1][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[1][9], np.sqrt(sigma[1][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[2][0], np.sqrt(sigma[2][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][1], np.sqrt(sigma[2][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][2], np.sqrt(sigma[2][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][3], np.sqrt(sigma[2][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][4], np.sqrt(sigma[2][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][5], np.sqrt(sigma[2][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][6], np.sqrt(sigma[2][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][7], np.sqrt(sigma[2][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][8], np.sqrt(sigma[2][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[2][9], np.sqrt(sigma[2][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[3][0], np.sqrt(sigma[3][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][1], np.sqrt(sigma[3][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][2], np.sqrt(sigma[3][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][3], np.sqrt(sigma[3][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][4], np.sqrt(sigma[3][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][5], np.sqrt(sigma[3][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][6], np.sqrt(sigma[3][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][7], np.sqrt(sigma[3][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][8], np.sqrt(sigma[3][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[3][9], np.sqrt(sigma[3][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[4][0], np.sqrt(sigma[4][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][1], np.sqrt(sigma[4][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][2], np.sqrt(sigma[4][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][3], np.sqrt(sigma[4][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][4], np.sqrt(sigma[4][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][5], np.sqrt(sigma[4][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][6], np.sqrt(sigma[4][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][7], np.sqrt(sigma[4][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][8], np.sqrt(sigma[4][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[4][9], np.sqrt(sigma[4][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[5][0], np.sqrt(sigma[5][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][1], np.sqrt(sigma[5][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][2], np.sqrt(sigma[5][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][3], np.sqrt(sigma[5][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][4], np.sqrt(sigma[5][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][5], np.sqrt(sigma[5][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][6], np.sqrt(sigma[5][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][7], np.sqrt(sigma[5][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][8], np.sqrt(sigma[5][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[5][9], np.sqrt(sigma[5][9])).pdf(predict_data[9])),\</span><br><span class="line">                  (sp.stats.norm(theta[6][0], np.sqrt(sigma[6][0])).pdf(predict_data[0]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][1], np.sqrt(sigma[6][1])).pdf(predict_data[1]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][2], np.sqrt(sigma[6][2])).pdf(predict_data[2]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][3], np.sqrt(sigma[6][3])).pdf(predict_data[3]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][4], np.sqrt(sigma[6][4])).pdf(predict_data[4]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][5], np.sqrt(sigma[6][5])).pdf(predict_data[5]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][6], np.sqrt(sigma[6][6])).pdf(predict_data[6]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][7], np.sqrt(sigma[6][7])).pdf(predict_data[7]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][8], np.sqrt(sigma[6][8])).pdf(predict_data[8]) *\</span><br><span class="line">                   sp.stats.norm(theta[6][9], np.sqrt(sigma[6][9])).pdf(predict_data[9]))]</span><br><span class="line">    <span class="built_in">return</span> likelihood</span><br></pre></td></tr></table></figure>
<ul>
<li>위의 함수를 사용하여 10개의 실수 변수들에 대한 모수를 계산하여 가능도를 구하는 반복문을 작성하였다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">total_num = np.array(Gaussian_df_X).shape[0]</span><br><span class="line">gaussian_likelihood_matrix = []</span><br><span class="line">percentage = 0</span><br><span class="line"><span class="keyword">for</span> num, predict_data <span class="keyword">in</span> enumerate(np.array(Gaussian_df_X)):</span><br><span class="line">    <span class="keyword">if</span> (percentage != int(num / total_num * 100)) and (int(num / total_num * 100) <span class="keyword">in</span> list(np.arange(10,101,10))):</span><br><span class="line">        percentage = int(num / total_num * 100)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"완성도 &#123;&#125; %"</span>.format(percentage))</span><br><span class="line">    likelihood = Gaussian_likelihood_cal(predict_data, theta, sigma)</span><br><span class="line">    gaussian_likelihood_matrix.append(likelihood)</span><br></pre></td></tr></table></figure>
<ul>
<li>위에서 가우시안 나이브 베이즈 모형의 가능도를 구했으므로 이젠 베르누이 나이브 베이즈 모형의 가능도를 구할 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BNB = BernoulliNB()</span><br><span class="line">fitted_BNB = BNB.fit(Bern_df_X, df_Y)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">theta = np.exp(fitted_BNB.feature_log_prob_)</span><br><span class="line">theta.shape</span><br></pre></td></tr></table></figure>
<ul>
<li>상대적으로 가우시안 나이브 베이즈 모형의 가능도를 계산하는 것보단 단순 연산으로 이루어져 있어 속도가 훨씬 빠르다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">bern_likelihood_matrix = []</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> np.array(Bern_df_X):</span><br><span class="line">    bern_likelihood_matrix.append(list(((theta ** data) * (1 - theta) ** (1 - data)).prod(axis=1)))</span><br></pre></td></tr></table></figure>
<h5 id="결과-40"><a href="#결과-40" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CPU <span class="built_in">times</span>: user 49.2 s, sys: 1.28 s, total: 50.5 s</span><br><span class="line">Wall time: 50.4 s</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">likelihood = np.array(gaussian_likelihood_matrix) * np.array(bern_likelihood_matrix)</span><br><span class="line">posterior = likelihood * np.exp(BNB.class_log_prior_)</span><br><span class="line">prob = posterior / np.repeat(posterior.sum(axis=1)[:, np.newaxis], 7, axis=1)</span><br><span class="line">result = np.argmax(prob, axis=1)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result_bern = fitted_BNB.predict(Bern_df_X)</span><br><span class="line">result_gaussian = fitted_GNB.predict(Gaussian_df_X)</span><br></pre></td></tr></table></figure>
<h5 id="가우시안-나이브-베이즈-모형의-성능"><a href="#가우시안-나이브-베이즈-모형의-성능" class="headerlink" title="가우시안 나이브 베이즈 모형의 성능"></a>가우시안 나이브 베이즈 모형의 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(result_gaussian, df_Y))</span><br></pre></td></tr></table></figure>
<h5 id="결과-41"><a href="#결과-41" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           1       0.67      0.63      0.65    225973</span><br><span class="line">           2       0.66      0.73      0.69    255934</span><br><span class="line">           3       0.65      0.50      0.56     46785</span><br><span class="line">           4       0.47      0.41      0.44      3099</span><br><span class="line">           5       0.22      0.18      0.20     11425</span><br><span class="line">           6       0.31      0.33      0.32     16424</span><br><span class="line">           7       0.28      0.27      0.28     21372</span><br><span class="line"></span><br><span class="line">    accuracy                           0.63    581012</span><br><span class="line">   macro avg       0.47      0.44      0.45    581012</span><br><span class="line">weighted avg       0.63      0.63      0.63    581012</span><br></pre></td></tr></table></figure>
<h5 id="베르누이-나이브-베이즈-모형의-성능"><a href="#베르누이-나이브-베이즈-모형의-성능" class="headerlink" title="베르누이 나이브 베이즈 모형의 성능"></a>베르누이 나이브 베이즈 모형의 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(result_bern, df_Y))</span><br></pre></td></tr></table></figure>
<h5 id="결과-42"><a href="#결과-42" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           1       0.67      0.63      0.65    225973</span><br><span class="line">           2       0.66      0.73      0.69    255934</span><br><span class="line">           3       0.65      0.50      0.56     46785</span><br><span class="line">           4       0.47      0.41      0.44      3099</span><br><span class="line">           5       0.22      0.18      0.20     11425</span><br><span class="line">           6       0.31      0.33      0.32     16424</span><br><span class="line">           7       0.28      0.27      0.28     21372</span><br><span class="line"></span><br><span class="line">    accuracy                           0.63    581012</span><br><span class="line">   macro avg       0.47      0.44      0.45    581012</span><br><span class="line">weighted avg       0.63      0.63      0.63    581012</span><br></pre></td></tr></table></figure>
<h5 id="가우시안-나이브-베이즈-모형과-베르누이-나이브-베이즈-모형의-가능도를-곱해-확률을-계산한-모형의-성능"><a href="#가우시안-나이브-베이즈-모형과-베르누이-나이브-베이즈-모형의-가능도를-곱해-확률을-계산한-모형의-성능" class="headerlink" title="가우시안 나이브 베이즈 모형과 베르누이 나이브 베이즈 모형의 가능도를 곱해 확률을 계산한 모형의 성능"></a>가우시안 나이브 베이즈 모형과 베르누이 나이브 베이즈 모형의 가능도를 곱해 확률을 계산한 모형의 성능</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(result, df_Y))</span><br></pre></td></tr></table></figure>
<h5 id="결과-43"><a href="#결과-43" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">              precision    recall  f1-score   support</span><br><span class="line"></span><br><span class="line">           1       0.03      0.25      0.06     26481</span><br><span class="line">           2       0.93      0.48      0.63    554531</span><br><span class="line">           3       0.00      0.00      0.00         0</span><br><span class="line">           4       0.00      0.00      0.00         0</span><br><span class="line">           5       0.00      0.00      0.00         0</span><br><span class="line">           6       0.00      0.00      0.00         0</span><br><span class="line">           7       0.00      0.00      0.00         0</span><br><span class="line"></span><br><span class="line">    accuracy                           0.47    581012</span><br><span class="line">   macro avg       0.14      0.10      0.10    581012</span><br><span class="line">weighted avg       0.89      0.47      0.60    581012</span><br></pre></td></tr></table></figure>
<ul>
<li>위의 성능 보면 3가지 모형 다 성능이 좋지 않다는 것을 확인 할 수 있다. 이는 적절한 피처의 선택이 이루어지지 않은 모형이기 때문일 것이며, 또한 아래 그림에서와 같이 클래스간의 비율차이가 극심하게 차이가 나는데, 특히 1,2 클래스가 대다수를 이루고 있기 때문에 1, 2클래스에 대한 학습이 많이 된 결과라고 해석 할 수 있을 것이다. 이는 마지막 두 나이브 베이즈 모형의 성능을 보아도 확인 할 수 있다. 마지막 모형의 성능은 다른 클래스로 예측한 데이터는 존재하지 않고 오로지 1과 2로 예측을 했다.</li>
</ul>
<p><img src="/image/class_dist_sovya.png" alt="클래스별 분포"></p>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "나이브 베이즈 분류모형",
        "image": "https://heung-bae-lee.github.io/image/classification_problem.png",
        "keywords": "",
        "genre": "machine learning",
        "datePublished": "2020-04-14",
        "dateCreated": "2020-04-14",
        "dateModified": "2020-04-17",
        "url": "https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/",
        "description": "분류모형

현실적인 문제로 바꾸어 말하면 어떤 표본에 대한 데이터가 주어졌을 때 그 표본이 어떤 카테고리 혹은 클래스에 속하는지를 알아내는 문제이기도 하다.



흔히들 많이 사용하는 모형들의 분류모형 종류를 아래의 표로 기재해 놓았다. 이전에 기재했던 로지스틱 회귀 분석도 실질적으론 분류문제에 많이 사용된다.





모형
방법론




나이브 베이지안
"
        "wordCount": 9433
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2020/04/17/machine_learning_08/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            K-Nearest Neighbors(KNN)
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2020/04/03/machine_learning_06/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">PCA를 이해하기 위한 기본적 선형대수</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/05/14/linear_algebra_03/" class="title">Linear combination, vector equation, Four views of matrix multiplication</a></p>
                            <p class="item-date"><time datetime="2020-05-14T06:36:03.000Z" itemprop="datePublished">2020-05-14</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/05/13/linear_algebra_02/" class="title">선형 시스템(Linear system)</a></p>
                            <p class="item-date"><time datetime="2020-05-13T07:50:33.000Z" itemprop="datePublished">2020-05-13</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/05/12/linear_algebra_01/" class="title">선형대수 요소(Elements in linear algebra)</a></p>
                            <p class="item-date"><time datetime="2020-05-12T13:41:59.000Z" itemprop="datePublished">2020-05-12</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/02/machine_learning_14/" class="title">Ensemble Learning - 01</a></p>
                            <p class="item-date"><time datetime="2020-05-02T12:00:10.000Z" itemprop="datePublished">2020-05-02</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></p>
                            <p class="item-title"><a href="/2020/05/02/data_structure_06/" class="title">내가 정리하는 자료구조 05 - 트리(Tree)</a></p>
                            <p class="item-date"><time datetime="2020-05-02T10:27:21.000Z" itemprop="datePublished">2020-05-02</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">15</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/04/14/machine_learning_07/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
