<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>NLP - 단어 수준 임베딩 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="단어 수준 임베딩 예측 기반 모델  NPLM Word2Vec FastText   행렬 분해 기반 모델  LSA GloVe Swivel   단어 임베딩을 문장 수준 임베딩으로 확장하는 방법  가중 임베딩(Weighted Embedding)    NPLM(Neural Probabilistic Language Model) NLP 분야에서 임베딩 개념을 널리 퍼뜨">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP - 단어 수준 임베딩">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/02/01/NLP_06/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="단어 수준 임베딩 예측 기반 모델  NPLM Word2Vec FastText   행렬 분해 기반 모델  LSA GloVe Swivel   단어 임베딩을 문장 수준 임베딩으로 확장하는 방법  가중 임베딩(Weighted Embedding)    NPLM(Neural Probabilistic Language Model) NLP 분야에서 임베딩 개념을 널리 퍼뜨">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/NPLM_principal.png">
<meta property="og:updated_time" content="2020-02-05T07:20:01.886Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP - 단어 수준 임베딩">
<meta name="twitter:description" content="단어 수준 임베딩 예측 기반 모델  NPLM Word2Vec FastText   행렬 분해 기반 모델  LSA GloVe Swivel   단어 임베딩을 문장 수준 임베딩으로 확장하는 방법  가중 임베딩(Weighted Embedding)    NPLM(Neural Probabilistic Language Model) NLP 분야에서 임베딩 개념을 널리 퍼뜨">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/NPLM_principal.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/02/01/nlp_06/">
   
    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script data-ad-client="ca-pub-4604833066889492" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/NLP/">NLP</a>
    </h1>
</div>

                        <div class="main-body-content">
			    <article id="post-NLP_06" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        NLP - 단어 수준 임베딩
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/02/01/NLP_06/" class="article-date">
            <time datetime="2020-02-01T11:47:03.000Z" itemprop="datePublished">2020-02-01</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h1 id="단어-수준-임베딩"><a href="#단어-수준-임베딩" class="headerlink" title="단어 수준 임베딩"></a>단어 수준 임베딩</h1><ul>
<li><p>예측 기반 모델</p>
<ul>
<li>NPLM</li>
<li>Word2Vec</li>
<li>FastText</li>
</ul>
</li>
<li><p>행렬 분해 기반 모델</p>
<ul>
<li>LSA</li>
<li>GloVe</li>
<li>Swivel</li>
</ul>
</li>
<li><p>단어 임베딩을 문장 수준 임베딩으로 확장하는 방법</p>
<ul>
<li>가중 임베딩(Weighted Embedding)</li>
</ul>
</li>
</ul>
<h2 id="NPLM-Neural-Probabilistic-Language-Model"><a href="#NPLM-Neural-Probabilistic-Language-Model" class="headerlink" title="NPLM(Neural Probabilistic Language Model)"></a>NPLM(Neural Probabilistic Language Model)</h2><ul>
<li><p>NLP 분야에서 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델로서 임베딩 역사에서 차지하는 역할이 작지 않다.</p>
</li>
<li><p>‘단어가 어떤 순서로 쓰였는가’라는 가정을 통해 만들어진 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 만들어졌다는데 의의가 있으며 <code>NPLM 자체가 단어 임베딩 역할</code>을 수행할 수 있다. 다음과 같은 한계점들이 기존에는 존재했다.</p>
<ul>
<li><p>1) 학습 데이터에 존재하지 않는 데이터에 대해 나타날 확률을 0으로 부여 한다. 물론, 백오프(back-off)나 스무딩(smoothing)으로 완화시켜 줄 순 있지만 근본적인 해결방안은 아니였다.</p>
</li>
<li><p>2) 문장의 장기 의존성(long-term dependency)을 포착해내기 어렵다. 다시 말해서 n-gram모델의 n을 5 이상으로 길게 설정할 수 없다. <code>n이 커질수록 확률이 0이될 가능성이 높기 때문이다.</code></p>
</li>
<li><p>3) 단어/문장 간 유사도를 계산할 수 없다.</p>
</li>
</ul>
</li>
</ul>
<h3 id="NLPM의-학습"><a href="#NLPM의-학습" class="headerlink" title="NLPM의 학습"></a>NLPM의 학습</h3><ul>
<li>NLPM은 직전까지 등장한 n-1개(n: gram으로 묶을 수) 단어들로 다음 단어를 맞추는 <code>n-gram 언어 모델</code>이다.</li>
</ul>
<p><img src="/image/NPLM_principal.png" alt="NPLM의 학습 원리"></p>
<ul>
<li><p>NLPM 구조의 말단 출력</p>
<ul>
<li>$|V|$(corpus의 token vector, 어휘집합의 크기) 차원의 score 벡터 $y_{w_{t}}$에 softmax 함수를 적용한 $|V|$차원의 확률 벡터이다. <code>NPLM은 확률 벡터에서 가장 높은 요소의 인덱스에 해당하는 단어가 실제 정답 단어와 일치하도록 학습 한다.</code></li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">P(w_{t})|w_{t-1}, \cdots ,w_{t-n+1}) = \frac{exp(y_{w_{t}})}{\sum_{i} exp(y_{i}) }</script><script type="math/tex; mode=display">y_{w_{t}} \in R^{|V|} : w_{t}라는 단어에 해당하는 점수 벡터</script><ul>
<li><p>NLPM 구조의 입력</p>
<ul>
<li><p>문장 내 t번째 단어($w_{t}$)에 대응하는 단어 벡터 $x_{t}$를 만드는 과정은 다음과 같다. 먼저 $|V| \times m (m: x_{t}의 차원 수)$크기를 갖는 행렬 C에서 $w_{t}$에 해당하는 벡터를 참조하는 형태이다. <code>C 행렬의 원소값은 초기에 랜덤 설정</code>한다. 참조한다는 의미는 예를 들어 아래 그림에서 처럼 어휘 집합에 속한 단어가 5개이고 $w_{t}$가 4번째 인덱스를 의미한다고 하면 행렬 $C$와 $w_{t}$에 해당하는 one-hot 벡터를 내적한 것과 같다. 즉 $C$라는 행렬에서 $w_{t}$에 해당하는 행만 참조하는 것과 동일한 결과를 얻을 수 있다. 따라서 이 과정을 Look-up table이라는 용어로 많이 사용한다</p>
</li>
<li><p><code>문장 내 모든 단어들을 한 단어씩 훑으면서 Corpus 전체를 학습하게 된다면 NPLM 모델의 C 행렬에 각 단어의 문맥 정보를 내재할 수 있게 된다.</code></p>
</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">x_{t} = w_{t} \cdot C = C(w_{t}), C \in R^{|v| \times m}</script><p><img src="/image/NPLM_C_matrix.png" alt="NPLM 입력 벡터"></p>
<h3 id="모델-구조-및-의미정보"><a href="#모델-구조-및-의미정보" class="headerlink" title="모델 구조 및 의미정보"></a>모델 구조 및 의미정보</h3><p>이 때, walking을 맞추는 과정에서 발생한 손실(train loss)를 최소화하는 그래디언트(gradient)를 받아 각 단어에 해당하는 행들이 동일하게 업데이트 된다. 따라서 이 단어들의 벡터는 벡터 공간에서 같은 방향으로 조금씩 움직인다고 볼 수 있다. <code>결과적으로 임베딩 벡터 공간에서 해당 단어들 사이의 거리가 가깝다는 것은 의미가 유사하다라는 의미로 해석</code>할 수 있다.</p>
<p><img src="/image/NLPM_input.png" alt="NPLM의 input 벡터"></p>
<p><img src="/image/NPLM_structure.png" alt="NPLM의 구조"></p>
<h3 id="NPLM의-특징"><a href="#NPLM의-특징" class="headerlink" title="NPLM의 특징"></a>NPLM의 특징</h3><ul>
<li><code>NPLM은 그 자체로 언어 모델 역할을 수행</code>할 수 있다. 기존의 통계 기반 n-gram 모델은 학습 데이터에 한 번도 등장하지 않은 패턴에 대해서는 그 등장 확률을 0으로 부여하는 문제점을 NPLM은 <code>문장이 Corpus에 없어도 문맥이 비슷한 다른 문장을 참고해 확률을 부여</code>하는 방식으로 극복하는데 큰 의의가 있다. 하지만, 학습 파라미터가 너무 많아 계산이 복잡해지고, 자연스럽게 모델 과적합(overfitting)이 발생할 가능성이 높다는 한계를 가진다.</li>
</ul>
<p>이 한계를 극복하기 위해 다음 임베딩 방법론인 Word2Vec이 등장하였다.</p>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>Word2vec은 가장 널리 쓰이고 있는 단어 임베딩 모델이다. <code>Skip-gram</code>과 <code>CBOW</code>라는 모델이 제안되었고, 이 두 모델을 근간으로 하되 negative sampling등 학습 최적화 기법을 제안한 내용이 핵심이다.</p>
<ul>
<li><p>CBOW</p>
<ul>
<li><p><code>주변에 있는 context word들을 가지고 target word 하나를 맟추는 과정에서 학습</code>된다.</p>
</li>
<li><p>입,출력 데이터 쌍</p>
<ul>
<li>{context words, target word}</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/image/step_by_step_CBOW.png" alt="CBOW 모델 01"></p>
<p><img src="/image/step_by_step_CBOW_01.png" alt="CBOW 모델 02"></p>
<p><img src="/image/Projection_layer_in_CBOW.png" alt="CBOW 모델의 Projection Layer"></p>
<p><img src="/image/Output_layer_in_CBOW.png" alt="CBOW 모델의 Output Layer"></p>
<ul>
<li><p>Skip-gram</p>
<ul>
<li><p>처음 제안된 방식은 <code>target word 하나를 가지고 context word들이 무엇일지 예측하는 과정에서 학습</code>된다. 하지만, 이 방식은 정답 문맥 단어가 나타날 확률은 높이고 나머지 단어들 확률은 그에 맞게 낮춰야 한다. 그런데 어휘 집합에 속한 단어 수는 보통 수십만 개나되므로 이를 모두 계산하려면 비효율 적이다. 이런 점을 극복하기 위해 <code>negative sampling</code>이라는 <code>target word와 context word 쌍이 주어졌을 때 해당 쌍이 positive sample인지 negative sample인지 이진 분류하는 과정에서 학습하는 방식</code>을 제안했다. 이런다면 학습 step마다 1개의 positive sample과 나머지 k개(임의의 k:target 단어의 negative sampling 개수)만 계산하면 되므로 차원수가 2인 시그모이드를 k+1회만 계산하면된다. 이전의 매 step마다 어휘 집합 크기만큼의 차원을 갖는 softmax를 1회 계산하는 방법보다 <code>계산량이 훨씬 적다.</code> 또한 <code>Corpus에서 자주 등장하지 않는 희귀한 단어가 negative sample로 조금 더 잘 뽑힐 수 있도록 하고 자주 등장하는 단어는 학습에서 제외하는 subsampling이라는 기법을</code> 적용하였다. Skip-gram은 Corpus로 부터 엄청나게 많은 학습 데이터 쌍을 만들어 낼 수 있기 때문에 고빈도 단어의 경우 등장 횟수만큼 모두 학습시키는 것이 비효울적이라고 보았다. 이 또한, 학습량을 효과적으로 줄여 계산량을 감소시키는 전략이다.</p>
</li>
<li><p><code>작은 Corpus는 k=5~20, 큰 Corpus는 k=2~5로 하는 것이 성능이 좋다고 알려져 있다.</code></p>
</li>
<li><p><code>skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 CBOW보다 좋은 경향</code>이 있다.</p>
</li>
<li><p>입,출력 데이터 쌍</p>
<ul>
<li>{target word, target word 전전 단어}, {target word, target word 직전 단어}, {target word, target word 다음 단어}, {target word, target word 다다음 단어}</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/image/skip_gram_model_in_word2vec.png" alt="Skip-gram 모델"></p>
<ul>
<li><p>negative sample Prob</p>
<script type="math/tex; mode=display">P_{negative}(w_{i}) =  \frac{U(w_{i})^{3/4}}{\sum^{n}_{j=0}} U(w_{i]}^{3/4})</script><script type="math/tex; mode=display">U(w_{i]}) = \frac{해당 단어 빈도}{전체 단어 수} = 해당 단어의 Unigram Prob</script></li>
<li><p>subsampling Prob</p>
<script type="math/tex; mode=display">P_{subsampling}(w_{i}) = 1 - \sqrt(\frac{t}{f(w_{i})}) = w_{i}가 학습에서 제외될 확률</script><script type="math/tex; mode=display">f(w_{i]}) = w_{i]}'s frequency, t = 0.00001</script></li>
<li><p>t, c가 positive sample(=target word 주변에 context word가 존재)일 확률</p>
<ul>
<li>target word와 context가 실제 positive sample이라면 아래의 조건부 확률을 최대화해야 한다. 모델의 학습 parameter는 U와 V 행렬 두개 인데, 둘의 크기는 어휘 집합 크기$(|V|) \times 임베딩 차원 수(d)$로 동일하다. <code>U와 V는 각각 target word와 context word에 대응</code>한다.</li>
</ul>
</li>
</ul>
<p><img src="/image/Skip_gram_model_parameter.png" alt="Skip-gram 모델의 파라미터"></p>
<script type="math/tex; mode=display">P(+|t, c) = \frac{1}{1 + exp(-u_{t}v_{c})}</script><ul>
<li>위의 식을 최대화 하려면 분모를 줄여야한다. 분모를 줄이려면 $exp(-u_{t}v_{c})$를 줄여야 한다. 그러려면 두 벡터의 내적값이 커지게 해야한다. 이는 <code>코사인유사도와 비례</code>함을 알 수 있다. 결론적으로 <code>두 벡터간의 유사도를 높인다는 의미</code>이다.</li>
</ul>
<p><img src="/image/exponential_function.png" alt="Exponential 함수"></p>
<ul>
<li>잘 이해가 가지 않는다면 아래과 그림을 보자. A 가 B에 정확히 포개어져 있을 때(θ=0도) cos(θ)는 1이다. 녹색선의 길이가 단위원 반지름과 일치하기 때문이다. B는 고정한 채 A가 y축 상단으로 옮겨간다(θ가 0도에서 90도로 증가)고 할때 cos(θ)는 점점 감소하여 0이 되게 됩니다. 아래 그림의 경우 빨간색 직선이 x축과 만나는 점이 바로 cos(θ)를 의미한다.</li>
</ul>
<p><img src="/image/cosine_value.png" alt="cosine함수와 벡터간의 내적과의 관계"></p>
<ul>
<li>t, c가 negative sample(target word와 context word가 무관할때)일 확률<ul>
<li>만약 학습데이터가 negative sample에 해당한다면 아래의 조건부 확률을 최대화하여야 한다. 이 때는 분자를 최대화 해주어야 하므로, 두 벡터의 내적값을 줄여야 한다.</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">P(-|t, c) = 1 - P(+|t,c) = \frac{exp(-u_{t}v_{c})}{1 + exp(-u_{t}v_{c})}</script><ul>
<li><p>모델의 손실함수를 이제 알았으니 최대화를 하는 파라미터를 찾으려면 MLE를 구해야 할 것이다. 그렇다면 log likelihood function은 아래와 같을 것이다. 임의의 모수인 모델 파라미터인 $\seta$라고 가정 했을때, $\seta$를 한 번 업데이트할 때 1개 쌍의 positive sample과 k개의 negative sample이 학습된다는 의미이다. <code>Word2vec은 결국 두 단어벡터의 유사도 뿐만아니라 전체 Corpus 분포 정보를 단어 Embedding에 함축시키게 된다. 분포가 유사한 단어 쌍은 그 속성 또한 공유할 가능성이 높다.</code> 유사도 검사를 통해 비슷한 단어들을 출력 했을때, <code>그 단어들이 반드시 유의 관계를 보여준다기 보다는 동일한 속성을 갖는 관련성이 높은 단어를 출력한다는 의미로 이해해야한다.</code></p>
</li>
<li><p><code>모델 학습이 완료되면 U(target_word에 관한 행렬)만 d차원의 단어 임베딩으로 사용할 수도 있고, U+V.t 행렬을 임베딩으로 쓸 수도 있다. 혹은 concatenate([U, V.t])를 사용할 수도 있다.</code></p>
</li>
</ul>
<script type="math/tex; mode=display">L(\seta) = log P (+|t_{p},c_{p}) + \sum^{k}_{i=1} log P (-|t_{n_{i}},c_{n_{i}})</script><p><a href="https://radimrehurek.com/gensim/models/word2vec.html" target="_blank" rel="noopener">참고</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from gensim.models import word2vec</span></span><br><span class="line">from gensim.models import Word2Vec</span><br><span class="line">corpus = <span class="string">'원하는 텍스트를 단어를 기준으로 tokenize한 상태의 파일(가장 쉽게는 공백을 기준으로)'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model = word2vec.Word2Vec()</span></span><br><span class="line">model = Word2Vec(corpus,</span><br><span class="line">                 size=임베딩 특징 벡터 차원수,</span><br><span class="line">                 <span class="comment"># 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다.</span></span><br><span class="line">                 min_count=단어에 대한 최소 빈도 수,</span><br><span class="line">                 <span class="comment"># default negative=5, 보통 5~20을 많이 사용</span></span><br><span class="line">                 negative=negative sample을 뽑는 k,</span><br><span class="line">                 workers=학습시 사용하는 프로세스 개수,</span><br><span class="line">                 window=context window 크기,</span><br><span class="line">                 <span class="comment"># 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정한다.</span></span><br><span class="line">                 <span class="comment"># 유용한 범위는 (0, 1e-5)이며, 보통 0.001이 좋은 성능을 낸다고 한다.</span></span><br><span class="line">                 sample=다운 샘플링 비율,</span><br><span class="line">                 <span class="comment"># default sg=0 =&gt; CBOW, if sg=1 =&gt; skip-gram</span></span><br><span class="line">                 sg=1</span><br><span class="line">                 )</span><br><span class="line"></span><br><span class="line">model.save(<span class="string">"모델을 저장할 directory path"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 저장했던 모델을 불러와서 추가적으로 훈련시킬 수 있다.</span></span><br><span class="line">model = Word2Vec.load(<span class="string">"이미 존재하는 모델의 directory path"</span>)</span><br><span class="line">model.train([[<span class="string">"hello"</span>, <span class="string">"world"</span>]], total_examples=1, epochs=1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 훈련된 벡터를 KeyedVector로 분리하는 이유는 전체 모델 상태가 더 이상 필요하지 않을 경우(훈련을 계속할 필요가 없음)</span></span><br><span class="line"><span class="comment"># 모델이 폐기될 수 있기 때문에 프로세스 간에 RAM의 벡터를 빠르게 로드하고 공유할 수 있는 훨씬 작고 빠른 상태로 만드는 것이다.</span></span><br><span class="line">vector = model.wv[<span class="string">'computer'</span>]</span><br><span class="line"></span><br><span class="line">from gensim.models import KeyedVectors</span><br><span class="line"></span><br><span class="line">path = get_tmpfile(<span class="string">"wordvectors 파일명"</span>)</span><br><span class="line"></span><br><span class="line">model.wv.save(path)</span><br><span class="line">wv = KeyedVectors.load(<span class="string">"model.wv"</span>, mmap=<span class="string">'r'</span>)</span><br><span class="line">vector = wv[<span class="string">'computer'</span>]</span><br></pre></td></tr></table></figure></p>
<ul>
<li>학습이 완료된 임베딩 결과물을 활요하여 코사인 유사도가 가장 높은 단어들을 뽑아 임베딩을 평가해 볼 수도 있다. 이는 추후에 한번에 소개할 것이다.</li>
</ul>
<h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><ul>
<li><p>Facebook에서 개발해 공개한 <code>단어 임베딩 기법</code>이다. <code>각 단어를 문자단위 n-gram으로 표현한다. 이외의 점은 모두 Word2Vec과 같다.</code> 동일하게 negative sampling을 사용하며, 조금 다른 점은 <code>Fasttext는 target word(t), context word(c) 쌍을 학습할 때 target word(t)에 속한 문자 단위 n-gram 벡터(z)들을 모두 업데이트 한다는 점이다.</code></p>
</li>
<li><p>설치 방법은 gensim에서 FastText를 제공하고 있기에 pip를 통해 설치해주거나 이 방법이 안된다면, <a href="https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/07/06/fasttext/" target="_blank" rel="noopener">참조페이지</a>를 클릭해서 직접 C++방식으로 받아도 상관없다.</p>
</li>
</ul>
<h4 id="모델-기본-구조"><a href="#모델-기본-구조" class="headerlink" title="모델 기본 구조"></a>모델 기본 구조</h4><ul>
<li><p>예를 들어 시나브로라는 단어의 문자 단위 3-gram은 다음과 같이 n-gram 벡터의 합으로 표현한다. 아래 식에서 $G_{t}$는 target word t에 속한 문자 단위 n-gram집합을 의미한다.</p>
</li>
<li><p>Fasttext의 단어 벡터 표현(&lt;,&gt;는 단어의 경계를 나타내 주기 위해 모델이 사용하는 기호)</p>
<script type="math/tex; mode=display">u_{시나브로} = z_{<시나} + z_{시나브} + z_{나브로} + z_{브로>} + z_{시나브로}, u_{t}=\sum_{g \in G_{t}} z_{g}</script></li>
</ul>
<p><img src="/image/FastText_embedding_little.png" alt="FastText를 통한 임베딩"></p>
<ul>
<li><p><a href="https://wikidocs.net/21692" target="_blank" rel="noopener">n-gram 참조 및 NLP에 도움이 되는 사이트</a></p>
<ul>
<li>n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어진다. 그렇기 때문에 적절한 n을 선택해야 한다. <code>trade-off 문제로 인해 정확도를 높이려면 n은 최대 5를 넘게 잡아서는 안 된다고 권장되고 있다.</code></li>
</ul>
</li>
<li><p>손실함수 자체는 위의 식을 word2vec 손실함수 $u_{t}$에 대입해 주기만 하면된다.</p>
</li>
<li><p><code>FastText 모델의 강점은 조사나 어미가 발달한 한국어에 좋은 성능을 낼 수 있다는 점</code>이다. 용언(동사, 형용사)의 활용이나 그와 관계된 어미들이 벡터 공간상 가깝게 임베딩 되기 때문이다.(예를들면, ‘하였다’가 t이고, ‘공부’가 c라면 ‘공부’와 ‘했(다), 하(다), 하(였으며)’등에 해당하는 벡터도 비슷한 공간상에 있다는 의미이다.) <code>한글은 자소 단위(초성, 중성, 종성)로 분해할 수 있고, 이 자소 각각을 하나의 문자로 보고 FastText를 실행할 수 있다는 점도 강점</code>이다.</p>
</li>
<li><p><code>또한, 각 단어의 임베딩을 문자 단위 n-gram 벡터의 합으로 표현하기 때문에 오타나 미등록단어(unknown word)에도 robust하다. 그래서 미등록된 단어도 벡터를 뽑아낼수 있다.</code> 동일한 음절이나 단어를 가진 공간상의 벡터를 추출할 수 있기 때문이다. <code>다른 단어 임베딩 기법이 미등록 단어 벡터를 아예 추출할 수 없다는 사실을 감안하면 FastText는 경쟁력이 있다.</code></p>
</li>
</ul>
<p><a href="https://radimrehurek.com/gensim/models/fasttext.html" target="_blank" rel="noopener">Fasttext 참조</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import FastText</span><br><span class="line"></span><br><span class="line">corpus = <span class="string">'원하는 텍스트를 단어를 기준으로 tokenize한 상태의 파일(가장 쉽게는 공백을 기준으로)'</span></span><br><span class="line"></span><br><span class="line">model = Word2Vec(corpus,</span><br><span class="line">                 size=임베딩 특징 벡터 차원수,</span><br><span class="line">                 <span class="comment"># 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다.</span></span><br><span class="line">                 min_count=단어에 대한 최소 빈도 수,</span><br><span class="line">                 <span class="comment"># default negative=5, 보통 5~20을 많이 사용</span></span><br><span class="line">                 negative=negative sample을 뽑는 k,</span><br><span class="line">                 workers=학습시 사용하는 프로세스 개수,</span><br><span class="line">                 window=context window 크기,</span><br><span class="line">                 <span class="comment"># 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정한다.</span></span><br><span class="line">                 <span class="comment"># 유용한 범위는 (0, 1e-5)이며, 보통 0.001이 좋은 성능을 낸다고 한다.</span></span><br><span class="line">                 sample=다운 샘플링 비율,</span><br><span class="line">                 <span class="comment"># default sg=0 =&gt; CBOW, if sg=1 =&gt; skip-gram</span></span><br><span class="line">                 sg=1,</span><br><span class="line">                 <span class="comment"># default word_ngrams=1 =&gt; n-gram 사용, 0 =&gt; 미사용(word2vec과 동일)</span></span><br><span class="line">                 word_ngrams=1,</span><br><span class="line">                 <span class="comment"># n-gram 최소 단위</span></span><br><span class="line">                 min_n=3,</span><br><span class="line">                 <span class="comment"># n-gram 최대 단위 (최소단위보단 커야한다.)</span></span><br><span class="line">                 max_n=6,</span><br><span class="line">                 )</span><br></pre></td></tr></table></figure>
<h2 id="잠재-의미-분석-LSA-Latent-Semantic-Analysis"><a href="#잠재-의미-분석-LSA-Latent-Semantic-Analysis" class="headerlink" title="잠재 의미 분석(LSA, Latent Semantic Analysis)"></a>잠재 의미 분석(LSA, Latent Semantic Analysis)</h2><ul>
<li><p>word-document 행렬이나 TF-IDF 행렬, word-context 행렬 같은 <code>커다란 행렬에 차원 축소 방법의 일종인 특이값 분해를 수행해 데이터의 차원 수를 줄여 계산 효율성을 키우는 한편 행간에 숨어있는 잠재 의미를 추출해내는 방법론</code>이다.</p>
</li>
<li><p>예를 들면, word-documents 행렬이나 word-context 행렬 등에 SVD를 한 다음 그 결과로 도출되는 행벡터들을 단어 임베딩으로 사용할 수 있다. <code>잠재 의미 분석은 GloVe나 Swivel과 더불어 Matrix Factorization 기반의 기법으로 분류</code>된다.</p>
</li>
</ul>
<h4 id="PPMI-점별-상호-정보량-행렬"><a href="#PPMI-점별-상호-정보량-행렬" class="headerlink" title="PPMI(점별 상호 정보량) 행렬"></a>PPMI(점별 상호 정보량) 행렬</h4><ul>
<li><p>word-document 행렬, TF-IDF 행렬, word-context 행렬, PMI 행렬에 모두 LSA를 수행할 수 있다. 이 중 PMI 행렬을 보완하는 PPMI 행렬에 대해 소개하고자한다. <a href="https://heung-bae-lee.github.io/2020/01/16/NLP_01/">PMI 행렬과 위의 행렬들을 모른다면 클릭!</a></p>
</li>
<li><p>PPMI란 간단히 말해 우리가 가진 말뭉치의 크기가 충분히 크지 않다면, PMI식의 로그 안 분자가 분모보다 작을 때 음수가 되거나, 극단적으로 단어 A,B가 단 한번도 같이 등장하지 않는다면 $-inf$값을 갖게 된다. 이러한 이유로 <code>NLP 분야에서는 PMI 대신 PPMI(Positive Pointwise Mutual Information)를 지표로 사용한다.</code> PMI가 양수가 아닌 경우 그 값을 신뢰하기 힘들어 0으로 치환해 무시한다는 뜻이다.</p>
</li>
</ul>
<script type="math/tex; mode=display">PPMI(A, B) = max(PMI(A,B), 0)</script><ul>
<li><code>Shifted PMI(SPMI)는 Word2Vec과 깊은 연관이 있다는 논문이 발표되기도 했다.</code></li>
</ul>
<script type="math/tex; mode=display">SPMI(A, B) = PMI(A, B) - log k, k > 0</script><h4 id="행렬-분해로-이해하는-잠재-의미-분석"><a href="#행렬-분해로-이해하는-잠재-의미-분석" class="headerlink" title="행렬 분해로 이해하는 잠재 의미 분석"></a>행렬 분해로 이해하는 잠재 의미 분석</h4><ul>
<li>Eigenvalue Decomposition(고유값 분해)를 우선 알고 있다는 전제조건으로 SVD를 모르실수도 있는 분들을 위해 간략히 설명하자면, 고유값 분해는 행렬 A가 정방행렬일 경우만 가능한데, 만약 정방행렬이 아닌 행렬은 고유값 분해를 어떻게 해야 하는지에 대한 개념이라고 말할 수 있겠다. 혹시 고유값 분해도 잘 모르시겠다면 <a href="https://heung-bae-lee.github.io/2019/12/14/linear_algebra_00/">이곳</a>을 클릭해서 필자가 추천하는 강의들을 꼭 공부해 보시길 추천한다. 필자는 개인적으로 선형대수는 Computer Science(or 데이터 분석)를 하는데 기본적으로 어느 정도 알고 있어야 한다고 생각한다.</li>
</ul>
<p><a href="https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/" target="_blank" rel="noopener">참조</a></p>
<p><img src="/image/SVD_NLP.png" alt="특이값 분해 - SVD"></p>
<ul>
<li><p>예를 들어, 행렬 A의 m개의 word, n개 documents로 이루어져 shape이 $ m \times n $인 word-documents 행렬에 truncated SVD를 하여 LSA를 수행한다고 가정해본다. 그렇다면 <code>U는 단어 임베딩, V.t는 문서 임베딩에 대응</code>한다. 마찬가지로 <code>m개 단어, m개 단어로 이루어진 PMI 행렬에 LSA를 하면 d차원 크기의 단어 임베딩을 얻을 수 있다</code>.</p>
</li>
<li><p><code>각종 연구들에 따르면 LSA를 적용하면 단어와 문맥 간의 내재적인 의미를 효과적으로 보존할 수 있게 돼 결과적으로 문서 간 유사도 측정 등 모델의 성능 향상에 도움을 줄 수 있다고 한다. 또한 입력 데이터의 노이즈, sparsity(희소)를 줄일 수 있다.</code></p>
</li>
</ul>
<p><img src="/image/Truncated_SVD_NLP.png" alt="Truncated SVD"></p>
<h4 id="행렬-분해로-이해하는-Word2Vec"><a href="#행렬-분해로-이해하는-Word2Vec" class="headerlink" title="행렬 분해로 이해하는 Word2Vec"></a>행렬 분해로 이해하는 Word2Vec</h4><ul>
<li><code>negative sampling 기법으로 학습된 Word2Vec의 Skip-gram 모델(SGNS, Skip-Gram with Negative Sampling)은 Shifted PMI 행렬을 분해한 것과 같다는 것을 볼 수 있다.</code></li>
</ul>
<p><img src="/image/word2vec_whith_aspect_of_matrix_decomposition.png" alt="행렬 분해 관점에서 이해하는 word2vec"></p>
<ul>
<li>$A_{ij}$는 SPMI행렬의 i,j번째 원소이다. k는 Skip-gram 모델의 negative sample 수를 의미한다. 그러므로 k=1인 negative sample 수가 1개인 Skip-gram 모델은 PMI 행렬을 분해하는 것과 같다.</li>
</ul>
<script type="math/tex; mode=display">A^{SGNS}_{ij} = U_{i} \cdot V_{j} = PMI(i,j) - log k</script><ul>
<li>soynlp에서 제공하는 sent_to_word_contexts_matrix 함수를 활용하면 word-context 행렬을 구축할 수 있다. <code>dynamic_weight=True는 target word에서 멀어질수록 카운트하는 동시 등장 점수(co-occurrence score)를 조금씩 깎는다는 의미</code>이다. dynamic_weight=False라면 window 내에 포함된 context word들의 동시 등장 점수는 target word와의 거리와 관계 없이 모두 1로 계산한다. 예를 들어서 window=3이고 ‘도대체 언제쯤이면 데이터 사이언스 분야를 조금은 공부했다고 말할 수 있을까…’라는 문장의 target word가 ‘분야’라면, ‘를’과 ‘사이언스’의 동시 등장 점수는 1, ‘데이터’, ‘조금’은 0.66, ‘은’, ‘이면’은 0.33이 된다.    </li>
</ul>
<ul>
<li><p>word-context 행렬을 활용한 LSA</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import TruncatedSVD</span><br><span class="line">from soynlp.vectorizer import sent_to_word_contexts_matrix</span><br><span class="line"></span><br><span class="line">corpus_file</span><br><span class="line"></span><br><span class="line">corpus = [sent.replace(<span class="string">'\n'</span>, <span class="string">''</span>).strip() <span class="keyword">for</span> sent <span class="keyword">in</span> open(corpus_file, <span class="string">'r'</span>).readlines()]</span><br><span class="line"></span><br><span class="line">input_matrix, idx2vocab = sent_to_word_contexts_matrix(corpus,</span><br><span class="line">                                                       window=3,</span><br><span class="line">                                                       <span class="comment"># 최소 단어 빈도 수</span></span><br><span class="line">                                                       min_tf=10,</span><br><span class="line">                                                       dynamic_weight=True,</span><br><span class="line">                                                       verbose=True)</span><br><span class="line"></span><br><span class="line">cooc_svd = TruncatedSVD(n_components=100)</span><br><span class="line">cooc_vecs = cooc_svd.fit_transform(input_matrix)</span><br></pre></td></tr></table></figure>
</li>
<li><p>구축한 word-context 행렬에 soynlp에서 제공하는 pmi 함수를 적용한다. min_pmi 보다 낮은 PMI 값은 0으로 치환한다. 따라서 <code>min_pmi=0으로 설정하면 정확히 PPMI와 같다.</code> 또한, pmi matrix의 차원수는 어휘 수 x 어휘 수의 정방 행렬이다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from soynlp import pmi</span><br><span class="line">ppmi_matrix, _, _ = pmi(input_matrix, min_pmi=0)</span><br><span class="line">ppmi_svd = TruncatedSVD(n_components=100)</span><br><span class="line">ppmi_vecs = ppmi_svd.fit_transform(input_matrix)</span><br></pre></td></tr></table></figure>
<h4 id="GloVe-Global-Word-Vectors"><a href="#GloVe-Global-Word-Vectors" class="headerlink" title="GloVe(Global Word Vectors)"></a>GloVe(Global Word Vectors)</h4><ul>
<li>미국 스탠포트대학교연구팀에서 개발한 단어 임베딩 기법이다. 임베딩된 단어 벡터 간 유사도 측정을 수월하게 하면서도 Corpus 전체의 통계 정보를 좀 더 잘 반영하는 것을 지향하여 <code>Vanilla Word2Vec과 LSA 두 기법의 단점을 극복하고자 했다.</code> LSA(잠재 의미 분석)은 Corpus 전체의 통계량을 모두 활용할 수 있지만, 그 결과물로 단어 간 유사도를 측정하기는 어렵다. 반대로 Vanilla Word2Vec은 단어 벡터 사이의 유사도를 측정하는 데는 LSA보다 유리하지만 사용자가 지정한 window 내의 local context만 학습하기 때문에 Corpus 전체의 통계 정보는 반영되기 어렵다는 단점을 지닌다. <code>물론 GloVe 이후 발표된 Skip-gram 모델이 Corpus 전체의 Global한 통계량인 SPMI 행렬을 분해하는 것과 동치라는 점을 증명하기는 했다.</code></li>
</ul>
<p><img src="/image/GloVe_model.png" alt="그림으로 이해하는 GloVe"></p>
<ul>
<li><p>손실 함수</p>
<ul>
<li><p>임베딩된 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 증장 빈도의 로그 값이 되도록 정의했다.</p>
</li>
<li><p>단어 i,j 각각에 해당하는 벡터 $U_{i}$, $V_{j}$ 사이의 내적값과 두 단어 동시 등장 빈도 $A_{ij}$의 로그값 사이의 차이가 최소화될수록 학습 손실이 작아진다. bias항 2개와 f(A_{ij})는 임베딩 품질을 높이기 위해 고안된 장치이다.</p>
</li>
<li><p>Glove는 word-context 행렬 A를 만든 후에 학습이 끝나면 <code>U를 단어 임베딩으로 사용하거나 U+V.t, concatenate([U, V.t])를 임베딩으로 사용할 수 있다.</code></p>
</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">J = \sum^{|V|}_{i,j=1} f(A_{ij}) (U_{i} \cdot V_{j} + b_{i} + b+{j} - log A_{ij})^{2}</script><h2 id="Swivel"><a href="#Swivel" class="headerlink" title="Swivel"></a>Swivel</h2><ul>
<li><p>Google 연구팀이 발표한 행렬 분해 기반의 단어 임베딩 기법이다. PMI 행렬을 U와 V로 분해하고, 학습이 종료되면 <code>U를 단어 임베딩으로 쓸 수 있으며 U+V.t, concatenate([U, V.t])도 임베딩으로 사용할 수 있다.</code></p>
</li>
<li><p><code>PMI 행렬을 분해한다는 점에서 word-context 행렬을 분해하는 GloVe와 다르며, Swivel은 목적함수를 PMI의 단점을 보완할 수 있도록 설계했다.</code> 두 단어가 한번도 등장하지 않았을 경우 PMI가 -inf로 가능 현상을 보완하기 위해 이런경우의 손실함수를 따로 정의했다. 그 결과, <code>i,j가 각각 고빈도 단어인데 두 단어의 동시 등장빈도가 0이라면 두 단어는 정말로 등장하지 않는 의미상 무관계한 단어라고 가정하고, 단어 i,j가 저빈도 단어인데 두 단어의 동시 등장빈도가 0인 경우에는 두 단어는 의미상 관계가 일부 있을 수 있다고 가정한다.</code></p>
</li>
</ul>
<p><img src="/image/Swivel_model.png" alt="그림으로 이해하는 Swivel"></p>
<h2 id="단어-임베딩-평가-방법"><a href="#단어-임베딩-평가-방법" class="headerlink" title="단어 임베딩 평가 방법"></a>단어 임베딩 평가 방법</h2><ul>
<li><p>참고로 카카오브레인 박규병 님께서는 한국어, 일본어, 중국어 등 30개 언어의 단어 임베딩을 학습해 공개했다. 모델은 주로 해당 언어의 위키백과 등으로 학습됐으며 벡터 차원 수는 100, 3000차원 두 종류가 있다.</p>
<ul>
<li><a href="https://github.com/Kyubyong/wordvectors" target="_blank" rel="noopener">https://github.com/Kyubyong/wordvectors</a></li>
</ul>
</li>
</ul>
<h4 id="단어-유사도-평가-word-similarity-test"><a href="#단어-유사도-평가-word-similarity-test" class="headerlink" title="단어 유사도 평가(word similarity test)"></a>단어 유사도 평가(word similarity test)</h4><ul>
<li><p>일련의 단어 쌍을 미리 구성한 후에 사람이 평가한 점수와 단어 벡터 간 코사인 유사도 사이의 상관관계를 계산해 단어 임베딩의 품질을 평가하는 방법이다.</p>
</li>
<li><p><code>Word2Vec과 FastText 같은 예측 기반 임베딩 기법들이 GloVe, Swivel 등 행렬 분해 방법들에 비해 상관관계가 상대적으로 강한 것을 알 수 있다. 물론 무조건 예측기반이 좋다는 의미는 아니다. 데이터에 다르겠지만 보통은 저런 결과를 얻을 것이다.</code></p>
</li>
</ul>
<h4 id="단어-유추-평가-word-analogy-test"><a href="#단어-유추-평가-word-analogy-test" class="headerlink" title="단어 유추 평가(word analogy test)"></a>단어 유추 평가(word analogy test)</h4><ul>
<li>의미론적 유추에서 단어 벡터 간 계산을 통해 <code>갑 - 을  + 병 = 정</code>을 통해 평가하는 방법이다. <code>갑 - 을 + 병</code>에 해당하는 벡터에 대해 코사인 유사도가 가장 높은 벡터에 해당하는 단어가 실제 <code>정</code>인지를 확인한다.</li>
</ul>
<h4 id="단어-임베딩-시각화"><a href="#단어-임베딩-시각화" class="headerlink" title="단어 임베딩 시각화"></a>단어 임베딩 시각화</h4><ul>
<li>시각화 또한 단어 임베딩을 평가하는 방법이다. 다만 단어 임베딩은 보통 고차원 벡터이기 때문에 사람이 인식하는 2, 3차원으로 축소해 시각화를 하게 된다. <code>t-SNE(t-Stochastic Neighbor Embedding)은 고차원의 원공간에 존재하는 벡터 x의 이웃 간의 거리를 최대한 보존하는 저차원 벡터 y를 학습하는 방법론</code>이다. 원 공간의 데이터 확률 분포와 축소된 공간의 분포 사이의 차이를 최소화하는 방향으로 벡터 공간을 업데이트한다.</li>
</ul>
<h2 id="가중-임베딩"><a href="#가중-임베딩" class="headerlink" title="가중 임베딩"></a>가중 임베딩</h2>
        </div>
        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "NLP - 단어 수준 임베딩",
        "image": "https://heung-bae-lee.github.io/image/NPLM_principal.png",
        "keywords": "",
        "genre": "NLP",
        "datePublished": "2020-02-01",
        "dateCreated": "2020-02-01",
        "dateModified": "2020-02-05",
        "url": "https://heung-bae-lee.github.io/2020/02/01/NLP_06/",
        "description": "단어 수준 임베딩
예측 기반 모델

NPLM
Word2Vec
FastText


행렬 분해 기반 모델

LSA
GloVe
Swivel


단어 임베딩을 문장 수준 임베딩으로 확장하는 방법

가중 임베딩(Weighted Embedding)



NPLM(Neural Probabilistic Language Model)
NLP 분야에서 임베딩 개념을 널리 퍼뜨"
        "wordCount": 3751
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
    
        <a href="/2020/02/01/NLP_05/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/01/NLP_06/" class="title">NLP - 단어 수준 임베딩</a></p>
                            <p class="item-date"><time datetime="2020-02-01T11:47:03.000Z" itemprop="datePublished">2020-02-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/01/NLP_05/" class="title">NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03</a></p>
                            <p class="item-date"><time datetime="2020-02-01T07:57:48.000Z" itemprop="datePublished">2020-02-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/01/30/NLP_04/" class="title">NLP 실습 텍스트 분류(TF-IDF, CountVectorizer, Word2Vec) -02</a></p>
                            <p class="item-date"><time datetime="2020-01-29T15:13:48.000Z" itemprop="datePublished">2020-01-30</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/01/29/NLP_03/" class="title">NLP 실습 텍스트 분류 -01</a></p>
                            <p class="item-date"><time datetime="2020-01-29T14:40:09.000Z" itemprop="datePublished">2020-01-29</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/crawling/">crawling</a></p>
                            <p class="item-title"><a href="/2020/01/28/Crawling_03/" class="title">Scrapy 웹 크롤링 04 - 실습</a></p>
                            <p class="item-date"><time datetime="2020-01-28T05:55:17.000Z" itemprop="datePublished">2020-01-28</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">5</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/02/01/NLP_06/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
