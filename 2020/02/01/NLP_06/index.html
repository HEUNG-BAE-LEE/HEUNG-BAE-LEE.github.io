<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>NLP - 단어 수준 임베딩 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="단어 수준 임베딩 예측 기반 모델  NPLM Word2Vec FastText   행렬 분해 기반 모델  LSA GloVe Swivel   단어 임베딩을 문장 수준 임베딩으로 확장하는 방법  가중 임베딩(Weighted Embedding)    NPLM(Neural Probabilistic Language Model) NLP 분야에서 임베딩 개념을 널리 퍼뜨">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP - 단어 수준 임베딩">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/02/01/NLP_06/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="단어 수준 임베딩 예측 기반 모델  NPLM Word2Vec FastText   행렬 분해 기반 모델  LSA GloVe Swivel   단어 임베딩을 문장 수준 임베딩으로 확장하는 방법  가중 임베딩(Weighted Embedding)    NPLM(Neural Probabilistic Language Model) NLP 분야에서 임베딩 개념을 널리 퍼뜨">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/NPLM_principal.png">
<meta property="og:updated_time" content="2020-02-07T19:26:13.782Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP - 단어 수준 임베딩">
<meta name="twitter:description" content="단어 수준 임베딩 예측 기반 모델  NPLM Word2Vec FastText   행렬 분해 기반 모델  LSA GloVe Swivel   단어 임베딩을 문장 수준 임베딩으로 확장하는 방법  가중 임베딩(Weighted Embedding)    NPLM(Neural Probabilistic Language Model) NLP 분야에서 임베딩 개념을 널리 퍼뜨">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/NPLM_principal.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/02/01/nlp_06/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">
    <link rel="stylesheet" type="text/css" href>
    <link rel="stylesheet" href="https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css">	
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothiccoding.css">
    <link rel="stylesheet" href="/css/style.css">
   
    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/NLP/">NLP</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-NLP_06" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        NLP - 단어 수준 임베딩
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/02/01/NLP_06/" class="article-date">
            <time datetime="2020-02-01T11:47:03.000Z" itemprop="datePublished">2020-02-01</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h1 id="단어-수준-임베딩"><a href="#단어-수준-임베딩" class="headerlink" title="단어 수준 임베딩"></a>단어 수준 임베딩</h1><ul>
<li><p>예측 기반 모델</p>
<ul>
<li>NPLM</li>
<li>Word2Vec</li>
<li>FastText</li>
</ul>
</li>
<li><p>행렬 분해 기반 모델</p>
<ul>
<li>LSA</li>
<li>GloVe</li>
<li>Swivel</li>
</ul>
</li>
<li><p>단어 임베딩을 문장 수준 임베딩으로 확장하는 방법</p>
<ul>
<li>가중 임베딩(Weighted Embedding)</li>
</ul>
</li>
</ul>
<h2 id="NPLM-Neural-Probabilistic-Language-Model"><a href="#NPLM-Neural-Probabilistic-Language-Model" class="headerlink" title="NPLM(Neural Probabilistic Language Model)"></a>NPLM(Neural Probabilistic Language Model)</h2><ul>
<li><p>NLP 분야에서 임베딩 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델로서 임베딩 역사에서 차지하는 역할이 작지 않다.</p>
</li>
<li><p>‘단어가 어떤 순서로 쓰였는가’라는 가정을 통해 만들어진 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 만들어졌다는데 의의가 있으며 <code>NPLM 자체가 단어 임베딩 역할</code>을 수행할 수 있다. 다음과 같은 한계점들이 기존에는 존재했다.</p>
<ul>
<li><p>1) 학습 데이터에 존재하지 않는 데이터에 대해 나타날 확률을 0으로 부여 한다. 물론, 백오프(back-off)나 스무딩(smoothing)으로 완화시켜 줄 순 있지만 근본적인 해결방안은 아니였다.</p>
</li>
<li><p>2) 문장의 장기 의존성(long-term dependency)을 포착해내기 어렵다. 다시 말해서 n-gram모델의 n을 5 이상으로 길게 설정할 수 없다. <code>n이 커질수록 확률이 0이될 가능성이 높기 때문이다.</code></p>
</li>
<li><p>3) 단어/문장 간 유사도를 계산할 수 없다.</p>
</li>
</ul>
</li>
</ul>
<h3 id="NLPM의-학습"><a href="#NLPM의-학습" class="headerlink" title="NLPM의 학습"></a>NLPM의 학습</h3><ul>
<li>NLPM은 직전까지 등장한 n-1개(n: gram으로 묶을 수) 단어들로 다음 단어를 맞추는 <code>n-gram 언어 모델</code>이다.</li>
</ul>
<p><img src="/image/NPLM_principal.png" alt="NPLM의 학습 원리"></p>
<ul>
<li><p>NLPM 구조의 말단 출력</p>
<ul>
<li>$|V|$(corpus의 token vector, 어휘집합의 크기) 차원의 score 벡터 $y_{w_{t}}$에 softmax 함수를 적용한 $|V|$차원의 확률 벡터이다. <code>NPLM은 확률 벡터에서 가장 높은 요소의 인덱스에 해당하는 단어가 실제 정답 단어와 일치하도록 학습 한다.</code></li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">P(w_{t})|w_{t-1}, \cdots ,w_{t-n+1}) = \frac{exp(y_{w_{t}})}{\sum_{i} exp(y_{i}) }</script><script type="math/tex; mode=display">y_{w_{t}} \in R^{|V|} : w_{t}라는 단어에 해당하는 점수 벡터</script><ul>
<li><p>NLPM 구조의 입력</p>
<ul>
<li><p>문장 내 t번째 단어($w_{t}$)에 대응하는 단어 벡터 $x_{t}$를 만드는 과정은 다음과 같다. 먼저 $|V| \times m (m: x_{t}의 차원 수)$크기를 갖는 행렬 C에서 $w_{t}$에 해당하는 벡터를 참조하는 형태이다. <code>C 행렬의 원소값은 초기에 랜덤 설정</code>한다. 참조한다는 의미는 예를 들어 아래 그림에서 처럼 어휘 집합에 속한 단어가 5개이고 $w_{t}$가 4번째 인덱스를 의미한다고 하면 행렬 $C$와 $w_{t}$에 해당하는 one-hot 벡터를 내적한 것과 같다. 즉 $C$라는 행렬에서 $w_{t}$에 해당하는 행만 참조하는 것과 동일한 결과를 얻을 수 있다. 따라서 이 과정을 Look-up table이라는 용어로 많이 사용한다</p>
</li>
<li><p><code>문장 내 모든 단어들을 한 단어씩 훑으면서 Corpus 전체를 학습하게 된다면 NPLM 모델의 C 행렬에 각 단어의 문맥 정보를 내재할 수 있게 된다.</code></p>
</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">x_{t} = w_{t} \cdot C = C(w_{t}), C \in R^{|v| \times m}</script><p><img src="/image/NPLM_C_matrix.png" alt="NPLM 입력 벡터"></p>
<h3 id="모델-구조-및-의미정보"><a href="#모델-구조-및-의미정보" class="headerlink" title="모델 구조 및 의미정보"></a>모델 구조 및 의미정보</h3><p>이 때, walking을 맞추는 과정에서 발생한 손실(train loss)를 최소화하는 그래디언트(gradient)를 받아 각 단어에 해당하는 행들이 동일하게 업데이트 된다. 따라서 이 단어들의 벡터는 벡터 공간에서 같은 방향으로 조금씩 움직인다고 볼 수 있다. <code>결과적으로 임베딩 벡터 공간에서 해당 단어들 사이의 거리가 가깝다는 것은 의미가 유사하다라는 의미로 해석</code>할 수 있다.</p>
<p><img src="/image/NLPM_input.png" alt="NPLM의 input 벡터"></p>
<p><img src="/image/NPLM_structure.png" alt="NPLM의 구조"></p>
<h3 id="NPLM의-특징"><a href="#NPLM의-특징" class="headerlink" title="NPLM의 특징"></a>NPLM의 특징</h3><ul>
<li><code>NPLM은 그 자체로 언어 모델 역할을 수행</code>할 수 있다. 기존의 통계 기반 n-gram 모델은 학습 데이터에 한 번도 등장하지 않은 패턴에 대해서는 그 등장 확률을 0으로 부여하는 문제점을 NPLM은 <code>문장이 Corpus에 없어도 문맥이 비슷한 다른 문장을 참고해 확률을 부여</code>하는 방식으로 극복하는데 큰 의의가 있다. 하지만, 학습 파라미터가 너무 많아 계산이 복잡해지고, 자연스럽게 모델 과적합(overfitting)이 발생할 가능성이 높다는 한계를 가진다.</li>
</ul>
<p>이 한계를 극복하기 위해 다음 임베딩 방법론인 Word2Vec이 등장하였다.</p>
<h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>Word2vec은 가장 널리 쓰이고 있는 단어 임베딩 모델이다. <code>Skip-gram</code>과 <code>CBOW</code>라는 모델이 제안되었고, 이 두 모델을 근간으로 하되 negative sampling등 학습 최적화 기법을 제안한 내용이 핵심이다.</p>
<ul>
<li><p>CBOW</p>
<ul>
<li><p><code>주변에 있는 context word들을 가지고 target word 하나를 맟추는 과정에서 학습</code>된다.</p>
</li>
<li><p>입,출력 데이터 쌍</p>
<ul>
<li>{context words, target word}</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/image/step_by_step_CBOW.png" alt="CBOW 모델 01"></p>
<p><img src="/image/step_by_step_CBOW_01.png" alt="CBOW 모델 02"></p>
<p><img src="/image/Projection_layer_in_CBOW.png" alt="CBOW 모델의 Projection Layer"></p>
<p><img src="/image/Output_layer_in_CBOW.png" alt="CBOW 모델의 Output Layer"></p>
<ul>
<li><p>Skip-gram</p>
<ul>
<li><p>처음 제안된 방식은 <code>target word 하나를 가지고 context word들이 무엇일지 예측하는 과정에서 학습</code>된다. 하지만, 이 방식은 정답 문맥 단어가 나타날 확률은 높이고 나머지 단어들 확률은 그에 맞게 낮춰야 한다. 그런데 어휘 집합에 속한 단어 수는 보통 수십만 개나되므로 이를 모두 계산하려면 비효율 적이다. 이런 점을 극복하기 위해 <code>negative sampling</code>이라는 <code>target word와 context word 쌍이 주어졌을 때 해당 쌍이 positive sample인지 negative sample인지 이진 분류하는 과정에서 학습하는 방식</code>을 제안했다. 이런다면 학습 step마다 1개의 positive sample과 나머지 k개(임의의 k:target 단어의 negative sampling 개수)만 계산하면 되므로 차원수가 2인 시그모이드를 k+1회만 계산하면된다. 이전의 매 step마다 어휘 집합 크기만큼의 차원을 갖는 softmax를 1회 계산하는 방법보다 <code>계산량이 훨씬 적다.</code> 또한 <code>Corpus에서 자주 등장하지 않는 희귀한 단어가 negative sample로 조금 더 잘 뽑힐 수 있도록 하고 자주 등장하는 단어는 학습에서 제외하는 subsampling이라는 기법을</code> 적용하였다. Skip-gram은 Corpus로 부터 엄청나게 많은 학습 데이터 쌍을 만들어 낼 수 있기 때문에 고빈도 단어의 경우 등장 횟수만큼 모두 학습시키는 것이 비효울적이라고 보았다. 이 또한, 학습량을 효과적으로 줄여 계산량을 감소시키는 전략이다.</p>
</li>
<li><p><code>작은 Corpus는 k=5~20, 큰 Corpus는 k=2~5로 하는 것이 성능이 좋다고 알려져 있다.</code></p>
</li>
<li><p><code>skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있어 임베딩 품질이 CBOW보다 좋은 경향</code>이 있다.</p>
</li>
<li><p>입,출력 데이터 쌍</p>
<ul>
<li>{target word, target word 전전 단어}, {target word, target word 직전 단어}, {target word, target word 다음 단어}, {target word, target word 다다음 단어}</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/image/skip_gram_model_in_word2vec.png" alt="Skip-gram 모델"></p>
<ul>
<li><p>negative sample Prob</p>
<script type="math/tex; mode=display">P_{negative}(w_{i}) =  \frac{U(w_{i})^{3/4}}{\sum^{n}_{j=0}} U(w_{i]}^{3/4})</script><script type="math/tex; mode=display">U(w_{i]}) = \frac{해당 단어 빈도}{전체 단어 수} = 해당 단어의 Unigram Prob</script></li>
<li><p>subsampling Prob</p>
<script type="math/tex; mode=display">P_{subsampling}(w_{i}) = 1 - \sqrt(\frac{t}{f(w_{i})}) = w_{i}가 학습에서 제외될 확률</script><script type="math/tex; mode=display">f(w_{i]}) = w_{i]}'s frequency, t = 0.00001</script></li>
<li><p>t, c가 positive sample(=target word 주변에 context word가 존재)일 확률</p>
<ul>
<li>target word와 context가 실제 positive sample이라면 아래의 조건부 확률을 최대화해야 한다. 모델의 학습 parameter는 U와 V 행렬 두개 인데, 둘의 크기는 어휘 집합 크기$(|V|) \times 임베딩 차원 수(d)$로 동일하다. <code>U와 V는 각각 target word와 context word에 대응</code>한다.</li>
</ul>
</li>
</ul>
<p><img src="/image/Skip_gram_model_parameter.png" alt="Skip-gram 모델의 파라미터"></p>
<script type="math/tex; mode=display">P(+|t, c) = \frac{1}{1 + exp(-u_{t}v_{c})}</script><ul>
<li>위의 식을 최대화 하려면 분모를 줄여야한다. 분모를 줄이려면 $exp(-u_{t}v_{c})$를 줄여야 한다. 그러려면 두 벡터의 내적값이 커지게 해야한다. 이는 <code>코사인유사도와 비례</code>함을 알 수 있다. 결론적으로 <code>두 벡터간의 유사도를 높인다는 의미</code>이다.</li>
</ul>
<p><img src="/image/exponential_function.png" alt="Exponential 함수"></p>
<ul>
<li>잘 이해가 가지 않는다면 아래과 그림을 보자. A 가 B에 정확히 포개어져 있을 때(θ=0도) cos(θ)는 1이다. 녹색선의 길이가 단위원 반지름과 일치하기 때문이다. B는 고정한 채 A가 y축 상단으로 옮겨간다(θ가 0도에서 90도로 증가)고 할때 cos(θ)는 점점 감소하여 0이 되게 됩니다. 아래 그림의 경우 빨간색 직선이 x축과 만나는 점이 바로 cos(θ)를 의미한다.</li>
</ul>
<p><img src="/image/cosine_value.png" alt="cosine함수와 벡터간의 내적과의 관계"></p>
<ul>
<li>t, c가 negative sample(target word와 context word가 무관할때)일 확률<ul>
<li>만약 학습데이터가 negative sample에 해당한다면 아래의 조건부 확률을 최대화하여야 한다. 이 때는 분자를 최대화 해주어야 하므로, 두 벡터의 내적값을 줄여야 한다.</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">P(-|t, c) = 1 - P(+|t,c) = \frac{exp(-u_{t}v_{c})}{1 + exp(-u_{t}v_{c})}</script><ul>
<li><p>모델의 손실함수를 이제 알았으니 최대화를 하는 파라미터를 찾으려면 MLE를 구해야 할 것이다. 그렇다면 log likelihood function은 아래와 같을 것이다. 임의의 모수인 모델 파라미터인 $\theta$라고 가정 했을때, $\theta$를 한 번 업데이트할 때 1개 쌍의 positive sample과 k개의 negative sample이 학습된다는 의미이다. <code>Word2vec은 결국 두 단어벡터의 유사도 뿐만아니라 전체 Corpus 분포 정보를 단어 Embedding에 함축시키게 된다. 분포가 유사한 단어 쌍은 그 속성 또한 공유할 가능성이 높다.</code> 유사도 검사를 통해 비슷한 단어들을 출력 했을때, <code>그 단어들이 반드시 유의 관계를 보여준다기 보다는 동일한 속성을 갖는 관련성이 높은 단어를 출력한다는 의미로 이해해야한다.</code></p>
</li>
<li><p><code>모델 학습이 완료되면 U(target_word에 관한 행렬)만 d차원의 단어 임베딩으로 사용할 수도 있고, U+V.t 행렬을 임베딩으로 쓸 수도 있다. 혹은 concatenate([U, V.t])를 사용할 수도 있다.</code></p>
</li>
</ul>
<script type="math/tex; mode=display">L(\theta) = log P (+|t_{p},c_{p}) + \sum^{k}_{i=1} log P (-|t_{n_{i}},c_{n_{i}})</script><p><a href="https://radimrehurek.com/gensim/models/word2vec.html" target="_blank" rel="noopener">참고</a><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from gensim.models import word2vec</span></span><br><span class="line">from gensim.models import Word2Vec</span><br><span class="line">corpus = <span class="string">'원하는 텍스트를 단어를 기준으로 tokenize한 상태의 파일(가장 쉽게는 공백을 기준으로)'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># model = word2vec.Word2Vec()</span></span><br><span class="line">model = Word2Vec(corpus,</span><br><span class="line">                 size=임베딩 특징 벡터 차원수,</span><br><span class="line">                 <span class="comment"># 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다.</span></span><br><span class="line">                 min_count=단어에 대한 최소 빈도 수,</span><br><span class="line">                 <span class="comment"># default negative=5, 보통 5~20을 많이 사용</span></span><br><span class="line">                 negative=negative sample을 뽑는 k,</span><br><span class="line">                 workers=학습시 사용하는 프로세스 개수,</span><br><span class="line">                 window=context window 크기,</span><br><span class="line">                 <span class="comment"># 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정한다.</span></span><br><span class="line">                 <span class="comment"># 유용한 범위는 (0, 1e-5)이며, 보통 0.001이 좋은 성능을 낸다고 한다.</span></span><br><span class="line">                 sample=다운 샘플링 비율,</span><br><span class="line">                 <span class="comment"># default sg=0 =&gt; CBOW, if sg=1 =&gt; skip-gram</span></span><br><span class="line">                 sg=1</span><br><span class="line">                 )</span><br><span class="line"></span><br><span class="line">model.save(<span class="string">"모델을 저장할 directory path"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 저장했던 모델을 불러와서 추가적으로 훈련시킬 수 있다.</span></span><br><span class="line">model = Word2Vec.load(<span class="string">"이미 존재하는 모델의 directory path"</span>)</span><br><span class="line">model.train([[<span class="string">"hello"</span>, <span class="string">"world"</span>]], total_examples=1, epochs=1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 훈련된 벡터를 KeyedVector로 분리하는 이유는 전체 모델 상태가 더 이상 필요하지 않을 경우(훈련을 계속할 필요가 없음)</span></span><br><span class="line"><span class="comment"># 모델이 폐기될 수 있기 때문에 프로세스 간에 RAM의 벡터를 빠르게 로드하고 공유할 수 있는 훨씬 작고 빠른 상태로 만드는 것이다.</span></span><br><span class="line">vector = model.wv[<span class="string">'computer'</span>]</span><br><span class="line"></span><br><span class="line">from gensim.models import KeyedVectors</span><br><span class="line"></span><br><span class="line">path = get_tmpfile(<span class="string">"wordvectors 파일명"</span>)</span><br><span class="line"></span><br><span class="line">model.wv.save(path)</span><br><span class="line">wv = KeyedVectors.load(<span class="string">"model.wv"</span>, mmap=<span class="string">'r'</span>)</span><br><span class="line">vector = wv[<span class="string">'computer'</span>]</span><br></pre></td></tr></table></figure></p>
<ul>
<li>학습이 완료된 임베딩 결과물을 활요하여 코사인 유사도가 가장 높은 단어들을 뽑아 임베딩을 평가해 볼 수도 있다. 이는 추후에 한번에 소개할 것이다.</li>
</ul>
<h2 id="FastText"><a href="#FastText" class="headerlink" title="FastText"></a>FastText</h2><ul>
<li><p>Facebook에서 개발해 공개한 <code>단어 임베딩 기법</code>이다. <code>각 단어를 문자단위 n-gram으로 표현한다. 이외의 점은 모두 Word2Vec과 같다.</code> 동일하게 negative sampling을 사용하며, 조금 다른 점은 <code>Fasttext는 target word(t), context word(c) 쌍을 학습할 때 target word(t)에 속한 문자 단위 n-gram 벡터(z)들을 모두 업데이트 한다는 점이다.</code></p>
</li>
<li><p>설치 방법은 gensim에서 FastText를 제공하고 있기에 pip를 통해 설치해주거나 이 방법이 안된다면, <a href="https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/07/06/fasttext/" target="_blank" rel="noopener">참조페이지</a>를 클릭해서 직접 C++방식으로 받아도 상관없다.</p>
</li>
</ul>
<h4 id="모델-기본-구조"><a href="#모델-기본-구조" class="headerlink" title="모델 기본 구조"></a>모델 기본 구조</h4><ul>
<li><p>예를 들어 시나브로라는 단어의 문자 단위 3-gram은 다음과 같이 n-gram 벡터의 합으로 표현한다. 아래 식에서 $G_{t}$는 target word t에 속한 문자 단위 n-gram집합을 의미한다.</p>
</li>
<li><p>Fasttext의 단어 벡터 표현(&lt;,&gt;는 단어의 경계를 나타내 주기 위해 모델이 사용하는 기호)</p>
<script type="math/tex; mode=display">u_{시나브로} = z_{<시나} + z_{시나브} + z_{나브로} + z_{브로>} + z_{시나브로}, u_{t}=\sum_{g \in G_{t}} z_{g}</script></li>
</ul>
<p><img src="/image/FastText_embedding_little.png" alt="FastText를 통한 임베딩"></p>
<ul>
<li><p><a href="https://wikidocs.net/21692" target="_blank" rel="noopener">n-gram 참조 및 NLP에 도움이 되는 사이트</a></p>
<ul>
<li>n을 작게 선택하면 훈련 코퍼스에서 카운트는 잘 되겠지만 근사의 정확도는 현실의 확률분포와 멀어진다. 그렇기 때문에 적절한 n을 선택해야 한다. <code>trade-off 문제로 인해 정확도를 높이려면 n은 최대 5를 넘게 잡아서는 안 된다고 권장되고 있다.</code></li>
</ul>
</li>
<li><p>손실함수 자체는 위의 식을 word2vec 손실함수 $u_{t}$에 대입해 주기만 하면된다.</p>
</li>
<li><p><code>FastText 모델의 강점은 조사나 어미가 발달한 한국어에 좋은 성능을 낼 수 있다는 점</code>이다. 용언(동사, 형용사)의 활용이나 그와 관계된 어미들이 벡터 공간상 가깝게 임베딩 되기 때문이다.(예를들면, ‘하였다’가 t이고, ‘공부’가 c라면 ‘공부’와 ‘했(다), 하(다), 하(였으며)’등에 해당하는 벡터도 비슷한 공간상에 있다는 의미이다.) <code>한글은 자소 단위(초성, 중성, 종성)로 분해할 수 있고, 이 자소 각각을 하나의 문자로 보고 FastText를 실행할 수 있다는 점도 강점</code>이다.</p>
</li>
<li><p><code>또한, 각 단어의 임베딩을 문자 단위 n-gram 벡터의 합으로 표현하기 때문에 오타나 미등록단어(unknown word)에도 robust하다. 그래서 미등록된 단어도 벡터를 뽑아낼수 있다.</code> 동일한 음절이나 단어를 가진 공간상의 벡터를 추출할 수 있기 때문이다. <code>다른 단어 임베딩 기법이 미등록 단어 벡터를 아예 추출할 수 없다는 사실을 감안하면 FastText는 경쟁력이 있다.</code></p>
</li>
</ul>
<p><a href="https://radimrehurek.com/gensim/models/fasttext.html" target="_blank" rel="noopener">Fasttext 참조</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import FastText</span><br><span class="line"></span><br><span class="line">corpus = <span class="string">'원하는 텍스트를 단어를 기준으로 tokenize한 상태의 파일(가장 쉽게는 공백을 기준으로)'</span></span><br><span class="line"></span><br><span class="line">model = Word2Vec(corpus,</span><br><span class="line">                 size=임베딩 특징 벡터 차원수,</span><br><span class="line">                 <span class="comment"># 모델에 의미 있는 단어를 가지고 학습하기 위해 적은 빈도 수의 단어들은 학습하지 않는다.</span></span><br><span class="line">                 min_count=단어에 대한 최소 빈도 수,</span><br><span class="line">                 <span class="comment"># default negative=5, 보통 5~20을 많이 사용</span></span><br><span class="line">                 negative=negative sample을 뽑는 k,</span><br><span class="line">                 workers=학습시 사용하는 프로세스 개수,</span><br><span class="line">                 window=context window 크기,</span><br><span class="line">                 <span class="comment"># 학습을 수행할 때 빠른 학습을 위해 정답 단어 라벨에 대한 다운샘플링 비율을 지정한다.</span></span><br><span class="line">                 <span class="comment"># 유용한 범위는 (0, 1e-5)이며, 보통 0.001이 좋은 성능을 낸다고 한다.</span></span><br><span class="line">                 sample=다운 샘플링 비율,</span><br><span class="line">                 <span class="comment"># default sg=0 =&gt; CBOW, if sg=1 =&gt; skip-gram</span></span><br><span class="line">                 sg=1,</span><br><span class="line">                 <span class="comment"># default word_ngrams=1 =&gt; n-gram 사용, 0 =&gt; 미사용(word2vec과 동일)</span></span><br><span class="line">                 word_ngrams=1,</span><br><span class="line">                 <span class="comment"># n-gram 최소 단위</span></span><br><span class="line">                 min_n=3,</span><br><span class="line">                 <span class="comment"># n-gram 최대 단위 (최소단위보단 커야한다.)</span></span><br><span class="line">                 max_n=6,</span><br><span class="line">                 )</span><br></pre></td></tr></table></figure>
<h2 id="잠재-의미-분석-LSA-Latent-Semantic-Analysis"><a href="#잠재-의미-분석-LSA-Latent-Semantic-Analysis" class="headerlink" title="잠재 의미 분석(LSA, Latent Semantic Analysis)"></a>잠재 의미 분석(LSA, Latent Semantic Analysis)</h2><ul>
<li><p>word-document 행렬이나 TF-IDF 행렬, word-context 행렬 같은 <code>커다란 행렬에 차원 축소 방법의 일종인 특이값 분해를 수행해 데이터의 차원 수를 줄여 계산 효율성을 키우는 한편 행간에 숨어있는 잠재 의미를 추출해내는 방법론</code>이다.</p>
</li>
<li><p>예를 들면, word-documents 행렬이나 word-context 행렬 등에 SVD를 한 다음 그 결과로 도출되는 행벡터들을 단어 임베딩으로 사용할 수 있다. <code>잠재 의미 분석은 GloVe나 Swivel과 더불어 Matrix Factorization 기반의 기법으로 분류</code>된다.</p>
</li>
</ul>
<h4 id="PPMI-점별-상호-정보량-행렬"><a href="#PPMI-점별-상호-정보량-행렬" class="headerlink" title="PPMI(점별 상호 정보량) 행렬"></a>PPMI(점별 상호 정보량) 행렬</h4><ul>
<li><p>word-document 행렬, TF-IDF 행렬, word-context 행렬, PMI 행렬에 모두 LSA를 수행할 수 있다. 이 중 PMI 행렬을 보완하는 PPMI 행렬에 대해 소개하고자한다. <a href="https://heung-bae-lee.github.io/2020/01/16/NLP_01/">PMI 행렬과 위의 행렬들을 모른다면 클릭!</a></p>
</li>
<li><p>PPMI란 간단히 말해 우리가 가진 말뭉치의 크기가 충분히 크지 않다면, PMI식의 로그 안 분자가 분모보다 작을 때 음수가 되거나, 극단적으로 단어 A,B가 단 한번도 같이 등장하지 않는다면 $-inf$값을 갖게 된다. 이러한 이유로 <code>NLP 분야에서는 PMI 대신 PPMI(Positive Pointwise Mutual Information)를 지표로 사용한다.</code> PMI가 양수가 아닌 경우 그 값을 신뢰하기 힘들어 0으로 치환해 무시한다는 뜻이다.</p>
</li>
</ul>
<script type="math/tex; mode=display">PPMI(A, B) = max(PMI(A,B), 0)</script><ul>
<li><code>Shifted PMI(SPMI)는 Word2Vec과 깊은 연관이 있다는 논문이 발표되기도 했다.</code></li>
</ul>
<script type="math/tex; mode=display">SPMI(A, B) = PMI(A, B) - log k, k > 0</script><h4 id="행렬-분해로-이해하는-잠재-의미-분석"><a href="#행렬-분해로-이해하는-잠재-의미-분석" class="headerlink" title="행렬 분해로 이해하는 잠재 의미 분석"></a>행렬 분해로 이해하는 잠재 의미 분석</h4><ul>
<li>Eigenvalue Decomposition(고유값 분해)를 우선 알고 있다는 전제조건으로 SVD를 모르실수도 있는 분들을 위해 간략히 설명하자면, 고유값 분해는 행렬 A가 정방행렬일 경우만 가능한데, 만약 정방행렬이 아닌 행렬은 고유값 분해를 어떻게 해야 하는지에 대한 개념이라고 말할 수 있겠다. 혹시 고유값 분해도 잘 모르시겠다면 <a href="https://heung-bae-lee.github.io/2019/12/14/linear_algebra_00/">이곳</a>을 클릭해서 필자가 추천하는 강의들을 꼭 공부해 보시길 추천한다. 필자는 개인적으로 선형대수는 Computer Science(or 데이터 분석)를 하는데 기본적으로 어느 정도 알고 있어야 한다고 생각한다.</li>
</ul>
<p><a href="https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/04/06/pcasvdlsa/" target="_blank" rel="noopener">참조</a></p>
<p><img src="/image/SVD_NLP.png" alt="특이값 분해 - SVD"></p>
<ul>
<li><p>예를 들어, 행렬 A의 m개의 word, n개 documents로 이루어져 shape이 $ m \times n $인 word-documents 행렬에 truncated SVD를 하여 LSA를 수행한다고 가정해본다. 그렇다면 <code>U는 단어 임베딩, V.t는 문서 임베딩에 대응</code>한다. 마찬가지로 <code>m개 단어, m개 단어로 이루어진 PMI 행렬에 LSA를 하면 d차원 크기의 단어 임베딩을 얻을 수 있다</code>.</p>
</li>
<li><p><code>각종 연구들에 따르면 LSA를 적용하면 단어와 문맥 간의 내재적인 의미를 효과적으로 보존할 수 있게 돼 결과적으로 문서 간 유사도 측정 등 모델의 성능 향상에 도움을 줄 수 있다고 한다. 또한 입력 데이터의 노이즈, sparsity(희소)를 줄일 수 있다.</code></p>
</li>
</ul>
<p><img src="/image/Truncated_SVD_NLP.png" alt="Truncated SVD"></p>
<h4 id="행렬-분해로-이해하는-Word2Vec"><a href="#행렬-분해로-이해하는-Word2Vec" class="headerlink" title="행렬 분해로 이해하는 Word2Vec"></a>행렬 분해로 이해하는 Word2Vec</h4><ul>
<li><code>negative sampling 기법으로 학습된 Word2Vec의 Skip-gram 모델(SGNS, Skip-Gram with Negative Sampling)은 Shifted PMI 행렬을 분해한 것과 같다는 것을 볼 수 있다.</code></li>
</ul>
<p><img src="/image/word2vec_whith_aspect_of_matrix_decomposition.png" alt="행렬 분해 관점에서 이해하는 word2vec"></p>
<ul>
<li>$A_{ij}$는 SPMI행렬의 i,j번째 원소이다. k는 Skip-gram 모델의 negative sample 수를 의미한다. 그러므로 k=1인 negative sample 수가 1개인 Skip-gram 모델은 PMI 행렬을 분해하는 것과 같다.</li>
</ul>
<script type="math/tex; mode=display">A^{SGNS}_{ij} = U_{i} \cdot V_{j} = PMI(i,j) - log k</script><ul>
<li>soynlp에서 제공하는 sent_to_word_contexts_matrix 함수를 활용하면 word-context 행렬을 구축할 수 있다. <code>dynamic_weight=True는 target word에서 멀어질수록 카운트하는 동시 등장 점수(co-occurrence score)를 조금씩 깎는다는 의미</code>이다. dynamic_weight=False라면 window 내에 포함된 context word들의 동시 등장 점수는 target word와의 거리와 관계 없이 모두 1로 계산한다. 예를 들어서 window=3이고 ‘도대체 언제쯤이면 데이터 사이언스 분야를 조금은 공부했다고 말할 수 있을까…’라는 문장의 target word가 ‘분야’라면, ‘를’과 ‘사이언스’의 동시 등장 점수는 1, ‘데이터’, ‘조금’은 0.66, ‘은’, ‘이면’은 0.33이 된다.    </li>
</ul>
<ul>
<li><p>word-context 행렬을 활용한 LSA</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import TruncatedSVD</span><br><span class="line">from soynlp.vectorizer import sent_to_word_contexts_matrix</span><br><span class="line"></span><br><span class="line">corpus_file</span><br><span class="line"></span><br><span class="line">corpus = [sent.replace(<span class="string">'\n'</span>, <span class="string">''</span>).strip() <span class="keyword">for</span> sent <span class="keyword">in</span> open(corpus_file, <span class="string">'r'</span>).readlines()]</span><br><span class="line"></span><br><span class="line">input_matrix, idx2vocab = sent_to_word_contexts_matrix(corpus,</span><br><span class="line">                                                       window=3,</span><br><span class="line">                                                       <span class="comment"># 최소 단어 빈도 수</span></span><br><span class="line">                                                       min_tf=10,</span><br><span class="line">                                                       dynamic_weight=True,</span><br><span class="line">                                                       verbose=True)</span><br><span class="line"></span><br><span class="line">cooc_svd = TruncatedSVD(n_components=100)</span><br><span class="line">cooc_vecs = cooc_svd.fit_transform(input_matrix)</span><br></pre></td></tr></table></figure>
</li>
<li><p>구축한 word-context 행렬에 soynlp에서 제공하는 pmi 함수를 적용한다. min_pmi 보다 낮은 PMI 값은 0으로 치환한다. 따라서 <code>min_pmi=0으로 설정하면 정확히 PPMI와 같다.</code> 또한, pmi matrix의 차원수는 어휘 수 x 어휘 수의 정방 행렬이다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from soynlp import pmi</span><br><span class="line">ppmi_matrix, _, _ = pmi(input_matrix, min_pmi=0)</span><br><span class="line">ppmi_svd = TruncatedSVD(n_components=100)</span><br><span class="line">ppmi_vecs = ppmi_svd.fit_transform(input_matrix)</span><br></pre></td></tr></table></figure>
<h4 id="GloVe-Global-Word-Vectors"><a href="#GloVe-Global-Word-Vectors" class="headerlink" title="GloVe(Global Word Vectors)"></a>GloVe(Global Word Vectors)</h4><ul>
<li>미국 스탠포트대학교연구팀에서 개발한 단어 임베딩 기법이다. 임베딩된 단어 벡터 간 유사도 측정을 수월하게 하면서도 Corpus 전체의 통계 정보를 좀 더 잘 반영하는 것을 지향하여 <code>Vanilla Word2Vec과 LSA 두 기법의 단점을 극복하고자 했다.</code> LSA(잠재 의미 분석)은 Corpus 전체의 통계량을 모두 활용할 수 있지만, 그 결과물로 단어 간 유사도를 측정하기는 어렵다. 반대로 Vanilla Word2Vec은 단어 벡터 사이의 유사도를 측정하는 데는 LSA보다 유리하지만 사용자가 지정한 window 내의 local context만 학습하기 때문에 Corpus 전체의 통계 정보는 반영되기 어렵다는 단점을 지닌다. <code>물론 GloVe 이후 발표된 Skip-gram 모델이 Corpus 전체의 Global한 통계량인 SPMI 행렬을 분해하는 것과 동치라는 점을 증명하기는 했다.</code></li>
</ul>
<p><img src="/image/GloVe_model.png" alt="그림으로 이해하는 GloVe"></p>
<ul>
<li><p>손실 함수</p>
<ul>
<li><p>임베딩된 두 단어 벡터의 내적이 말뭉치 전체에서의 동시 증장 빈도의 로그 값이 되도록 정의했다.</p>
</li>
<li><p>단어 i,j 각각에 해당하는 벡터 $U_{i}$, $V_{j}$ 사이의 내적값과 두 단어 동시 등장 빈도 $A_{ij}$의 로그값 사이의 차이가 최소화될수록 학습 손실이 작아진다. bias항 2개와 f(A_{ij})는 임베딩 품질을 높이기 위해 고안된 장치이다.</p>
</li>
<li><p>Glove는 word-context 행렬 A를 만든 후에 학습이 끝나면 <code>U를 단어 임베딩으로 사용하거나 U+V.t, concatenate([U, V.t])를 임베딩으로 사용할 수 있다.</code></p>
</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">J = \sum^{|V|}_{i,j=1} f(A_{ij}) (U_{i} \cdot V_{j} + b_{i} + b+{j} - log A_{ij})^{2}</script><h2 id="Swivel"><a href="#Swivel" class="headerlink" title="Swivel"></a>Swivel</h2><ul>
<li><p>Google 연구팀이 발표한 행렬 분해 기반의 단어 임베딩 기법이다. PMI 행렬을 U와 V로 분해하고, 학습이 종료되면 <code>U를 단어 임베딩으로 쓸 수 있으며 U+V.t, concatenate([U, V.t])도 임베딩으로 사용할 수 있다.</code></p>
</li>
<li><p><code>PMI 행렬을 분해한다는 점에서 word-context 행렬을 분해하는 GloVe와 다르며, Swivel은 목적함수를 PMI의 단점을 보완할 수 있도록 설계했다.</code> 두 단어가 한번도 동시에 등장하지 않았을 경우 PMI가 -inf로 가능 현상을 보완하기 위해 이런경우의 손실함수를 따로 정의했다. 그 결과, <code>i,j가 각각 고빈도 단어인데 두 단어의 동시 등장빈도가 0이라면 두 단어는 정말로 등장하지 않는 의미상 무관계한 단어라고 가정하고, 단어 i,j가 저빈도 단어인데 두 단어의 동시 등장빈도가 0인 경우에는 두 단어는 의미상 관계가 일부 있을 수 있다고 가정한다.</code></p>
</li>
</ul>
<p><img src="/image/Swivel_model.png" alt="그림으로 이해하는 Swivel"></p>
<h2 id="단어-임베딩-평가-방법"><a href="#단어-임베딩-평가-방법" class="headerlink" title="단어 임베딩 평가 방법"></a>단어 임베딩 평가 방법</h2><ul>
<li><p>참고로 카카오브레인 박규병 님께서는 한국어, 일본어, 중국어 등 30개 언어의 단어 임베딩을 학습해 공개했다. 모델은 주로 해당 언어의 위키백과 등으로 학습됐으며 벡터 차원 수는 100, 3000차원 두 종류가 있다.</p>
<ul>
<li><a href="https://github.com/Kyubyong/wordvectors" target="_blank" rel="noopener">https://github.com/Kyubyong/wordvectors</a></li>
</ul>
</li>
</ul>
<h4 id="단어-유사도-평가-word-similarity-test"><a href="#단어-유사도-평가-word-similarity-test" class="headerlink" title="단어 유사도 평가(word similarity test)"></a>단어 유사도 평가(word similarity test)</h4><ul>
<li><p>일련의 단어 쌍을 미리 구성한 후에 사람이 평가한 점수와 단어 벡터 간 코사인 유사도 사이의 상관관계를 계산해 단어 임베딩의 품질을 평가하는 방법이다.</p>
</li>
<li><p><code>Word2Vec과 FastText 같은 예측 기반 임베딩 기법들이 GloVe, Swivel 등 행렬 분해 방법들에 비해 상관관계가 상대적으로 강한 것을 알 수 있다. 물론 무조건 예측기반이 좋다는 의미는 아니다. 데이터에 다르겠지만 보통은 저런 결과를 얻을 것이다.</code></p>
</li>
</ul>
<h4 id="단어-유추-평가-word-analogy-test"><a href="#단어-유추-평가-word-analogy-test" class="headerlink" title="단어 유추 평가(word analogy test)"></a>단어 유추 평가(word analogy test)</h4><ul>
<li>의미론적 유추에서 단어 벡터 간 계산을 통해 <code>갑 - 을  + 병 = 정</code>을 통해 평가하는 방법이다. <code>갑 - 을 + 병</code>에 해당하는 벡터에 대해 코사인 유사도가 가장 높은 벡터에 해당하는 단어가 실제 <code>정</code>인지를 확인한다.</li>
</ul>
<h4 id="단어-임베딩-시각화"><a href="#단어-임베딩-시각화" class="headerlink" title="단어 임베딩 시각화"></a>단어 임베딩 시각화</h4><ul>
<li>시각화 또한 단어 임베딩을 평가하는 방법이다. 다만 단어 임베딩은 보통 고차원 벡터이기 때문에 사람이 인식하는 2, 3차원으로 축소해 시각화를 하게 된다. <code>t-SNE(t-Stochastic Neighbor Embedding)은 고차원의 원공간에 존재하는 벡터 x의 이웃 간의 거리를 최대한 보존하는 저차원 벡터 y를 학습하는 방법론</code>이다. 원 공간의 데이터 확률 분포와 축소된 공간의 분포 사이의 차이를 최소화하는 방향으로 벡터 공간을 업데이트한다.</li>
</ul>
<h2 id="가중-임베딩"><a href="#가중-임베딩" class="headerlink" title="가중 임베딩"></a>가중 임베딩</h2><ul>
<li><code>단어 임베딩을 문장 수준 임베딩으로 확장하는 방법을 설명</code>하겠다. <code>아주 간단한 방법이지만 성능 효과가 좋아서 사용해볼만한 방법</code>이다. 미국 프린스턴 대학교 연구팀이 ICLR에 발표한 방법론이다.</li>
</ul>
<h4 id="모델-개요"><a href="#모델-개요" class="headerlink" title="모델 개요"></a>모델 개요</h4><ul>
<li><p>Arora et al.(2016)은 <code>문서 내 단어의 등장은 저자가 생각한 주제에 의존한다고 가정</code>했다. 이를 위해 <code>주제 벡터(discourse vector)</code>라는 개념을 도입했다. 주제 벡터 $c_{s}$가 주어졌을 때 어떤 단어 w가 나타날 확률을 아래와 같이 정의했다. $\tilde{c_{s}}$는 $c_{s}$로 부터 도출되는데 그 과정은 생략하고, 간단히 말하면 주제 벡터 c_{s}와 거의 비슷한 역할을 하는 임의의 어떤 벡터라고 보겠다. Z는 우변 두번째 항이 확률 값이 되도록 해주는 Normalize Factor이다.</p>
</li>
<li><p><code>우변의 첫째항은 단어 w가 주제와 상관없이 등장할 확률</code>이며, 한국어에서는 조사(은,는,이,가 등)가 P(w)가 높은 축에 속한다. <code>두 번째 항은 단어 w가 주제와 관련을 가질 확률을 의미</code>한다. 주제 벡터 $\tilde{c_{s}}$와 w에 해당하는 단어 벡터 $v_{w}$가 내적값이 클수록 그 값이 커진다. $\alpha$는 사용자가 지정하는 hyper parameter이다.</p>
</li>
<li><p>단어 등장 확률</p>
<script type="math/tex; mode=display">P(w|c_{s}) = \alpha P(w) + (1-\alpha) frac{ exp( \tilde{c_{s}} \cdot v_{w}) }{Z}</script></li>
<li><p>단어 sequence는 문장이다. <code>문장 등장 확률(단어들이 동시에 등장할 확률)은 문장에 속한 모든 단어들이 등장할 확률의 누적 곱으로 나타낼 수 있다. 그런데 확률을 누적해서 곱하면 너무 작아지는 underflow 문제가 발생하므로 로그를 취해 덧셈을 하는 것으로 대체한다.</code></p>
</li>
<li><p>문장 등장확률</p>
<script type="math/tex; mode=display">P(s|c_{s}) \propto \sum_{w \in s} log P(w|c_{s}) = \sum_{w \in s} f_{w}(\tilde{c_{s}})</script></li>
<li><p>단어 등장 확률의 Taylor Series approximation</p>
<script type="math/tex; mode=display">f_{w}(\tilde{c_{s}}) \approx f_{w}(0) + \triangledown f_{w}(0)^{T} \tilde{c_{s}}</script></li>
</ul>
<script type="math/tex; mode=display">= constant + frac{ (1-\alpha) / \alpha Z }{ P(w) + (1-\alpha) / \alpha Z } \tilde{c_{s}} \cdot v_{w}</script><ul>
<li><p>우리가 관찰하고 있는 단어 w가 등장할 확률을 최대화하는 주제벡터 $ c_{s} / \tilde{c_{s}} $를 찾는 것이 목표이다. w가 등장할 확률을 최대화하는 $ c_{s} / \tilde{c_{s}} $를 찾게 된다면 이 $ c_{s} / \tilde{c_{s}} $ 는 <code>해당 단어의 사용을 제일 잘 설명하는 주제 벡터가 될 것이다.</code></p>
</li>
<li><p>직관적으로 말하자면, 우리가 관찰하고 있는 문장이 등장할 확률을 최대화하는 주제 벡터 $ c_{s} / \tilde{c_{s}} $는 문장에 속한 단어들에 해당하는 단어 벡터에 가중치를 곱해 만든 새로운 벡터들의 합에 비례한다. <code>희귀한 단어라면 높은 가중치를 곱해 해당 단어 벡터의 크기를 키우고, 고빈도 단어라면 해당 벡터의 크기를 줄니다. 이는 정보성이 높은, 희귀한 단어에 가중치를 높게 주는 TF-IDF의 철학과도 맞닿아 있는 부분이다. 또한 문장 내 단어의 등장 순서를 고려하지 않는다는 점에서 Bag of Words 가정과도 연결된다.</code></p>
</li>
</ul>
<h4 id="모델-구현"><a href="#모델-구현" class="headerlink" title="모델 구현"></a>모델 구현</h4><ul>
<li><p>문장을 Token으로 나눈 뒤 해당 Token들에 대응하는 벡터들의 합으로 문장의 임베딩을 구한다. 예측은 테스트 문장이 들어오면 Token 벡터의 합으로 만들고, 이 벡터와 코사인 유사도가 가장 높은 학습 데이터 문장의 임베딩을 찾는다. 이후 해당 학습 데이터 문장에 달려 있는 레이블을 리턴하는 방식이다.</p>
</li>
<li><p>예를들어, ‘영화 정말 재밌다.’가 테스트 문장이고, 이 문장과 유사한 학습 데이터 임베딩이 ‘영화가 진짜 재미지네요.+긍정’이라면, 테스트 문장을 긍정이라고 예측한다는 것이다.</p>
</li>
<li><p>또한, 과연 어느정도의 효과가 있는지 비교하기위해 대조군으로 일반적인 합을 통한 임베딩 방식도 수행해볼것이다.</p>
</li>
</ul>
<h2 id="Weighted-Sum을-이용한-Documents-Classification-Model"><a href="#Weighted-Sum을-이용한-Documents-Classification-Model" class="headerlink" title="Weighted Sum을 이용한 Documents Classification Model"></a>Weighted Sum을 이용한 Documents Classification Model</h2><ul>
<li><p>참고로 해당 모델을 수행하려면 먼저 형태소 분석이 완료된 Corpus file과 Corpus를 통해 만들어진 Embedding File이 존재해야한다.</p>
</li>
<li><p>먼저 tokenizer를 선택해서 사용할 수 있도록  각 Tokenizer에 따른 객체를 생성해주는 함수를 만들어준다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from khaiii import KhaiiiApi</span><br><span class="line">from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_tokenizer(tokenizer_name):</span><br><span class="line">    <span class="keyword">if</span> tokenizer_name == <span class="string">"komoran"</span>:</span><br><span class="line">        tokenizer = Komoran()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"okt"</span>:</span><br><span class="line">        tokenizer = Okt()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"mecab"</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"hannanum"</span>:</span><br><span class="line">        tokenizer = Hannanum()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"kkma"</span>:</span><br><span class="line">        tokenizer = Kkma()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"khaiii"</span>:</span><br><span class="line">        tokenizer = KhaiiiApi()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="built_in">return</span> tokenizer</span><br></pre></td></tr></table></figure>
<ul>
<li>모델을 저장할 path가 존재하지 않는다면 directory를 만들어주는 함수를 만들어준다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line"></span><br><span class="line">def make_save_path(full_path):</span><br><span class="line">    <span class="keyword">if</span> full_path[:4] == <span class="string">"data"</span>:</span><br><span class="line">        full_path = os.path.join(os.path.abspath(<span class="string">"."</span>), full_path)</span><br><span class="line">    model_path = <span class="string">'/'</span>.join(full_path.split(<span class="string">"/"</span>)[:-1])</span><br><span class="line">    <span class="keyword">if</span> not os.path.exists(model_path):</span><br><span class="line">        os.makedirs(model_path)</span><br></pre></td></tr></table></figure>
<ul>
<li>아래 embedding_method의 default값은 fasttext이지만 실제로 필자가 실행시에는 word2vec을 사용할 것이다.</li>
<li><p>defaultdict은 말 그대로 처음에 값을 지정해주지 않으면 default값을 넣어준다는 의미이다.</p>
</li>
<li><p>비교 할 사항</p>
<ul>
<li>embedding method : <code>fasttext</code> vs <code>word2vec</code></li>
<li>sum method : <code>weighted sum</code> vs <code>sum</code></li>
<li>average or nor : <code>average</code> vs <code>not average</code></li>
</ul>
</li>
<li><p>이 글에서는 2번째 항목의 비교한 결과만을 보여 줄 것이다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br></pre></td><td class="code"><pre><span class="line">from collections import defaultdict</span><br><span class="line">from gensim.models import Word2Vec</span><br><span class="line"></span><br><span class="line">class CBoWModel(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, train_fname, embedding_fname, model_fname, embedding_corpus_fname,</span><br><span class="line">                 embedding_method=<span class="string">"fasttext"</span>, is_weighted=None, average=False, dim=100, tokenizer_name=<span class="string">"mecab"</span>):</span><br><span class="line">        <span class="comment"># configurations</span></span><br><span class="line">        make_save_path(model_fname)</span><br><span class="line">        self.dim = dim</span><br><span class="line">        <span class="comment"># 평균을 내줄것인지 아니면 합만을 사용할 것인지에 대한 옵션이다.</span></span><br><span class="line">        self.average = average</span><br><span class="line">        <span class="keyword">if</span> is_weighted:</span><br><span class="line">            model_full_fname = model_fname + <span class="string">"-weighted"</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            model_full_fname = model_fname + <span class="string">"-original"</span></span><br><span class="line">        self.tokenizer = get_tokenizer(tokenizer_name)</span><br><span class="line">        <span class="keyword">if</span> is_weighted:</span><br><span class="line">            <span class="comment"># ready for weighted embeddings</span></span><br><span class="line">            <span class="comment"># dictionary 형태로 이루어져있다. (embedding["word"]=embedding_vaector)</span></span><br><span class="line">            self.embeddings = self.load_or_construct_weighted_embedding(embedding_fname, embedding_method, embedding_corpus_fname)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"loading weighted embeddings, complete!"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># ready for original embeddings</span></span><br><span class="line">            words, vectors = self.load_word_embeddings(embedding_fname, embedding_method)</span><br><span class="line">            self.embeddings = defaultdict(list)</span><br><span class="line">            <span class="keyword">for</span> word, vector <span class="keyword">in</span> zip(words, vectors):</span><br><span class="line">                self.embeddings[word] = vector</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"loading original embeddings, complete!"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 모델이 존재하지 않는다면 새롭게 훈련시키고 존재한다면 load해 온다.</span></span><br><span class="line">        <span class="keyword">if</span> not os.path.exists(model_full_fname):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"train Continuous Bag of Words model"</span>)</span><br><span class="line">            self.model = self.train_model(train_fname, model_full_fname)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"load Continuous Bag of Words model"</span>)</span><br><span class="line">            self.model = self.load_model(model_full_fname)</span><br><span class="line"></span><br><span class="line">    def evaluate(self, test_data_fname, batch_size=3000, verbose=False):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"evaluation start!"</span>)</span><br><span class="line">        test_data = self.load_or_tokenize_corpus(test_data_fname)</span><br><span class="line">        data_size = len(test_data)</span><br><span class="line">        num_batches = int((data_size - 1) / batch_size) + 1</span><br><span class="line">        eval_score = 0</span><br><span class="line">        <span class="keyword">for</span> batch_num <span class="keyword">in</span> range(num_batches):</span><br><span class="line">            batch_sentences = []</span><br><span class="line">            batch_tokenized_sentences = []</span><br><span class="line">            batch_labels = []</span><br><span class="line">            start_index = batch_num * batch_size</span><br><span class="line">            end_index = min((batch_num + 1) * batch_size, data_size)</span><br><span class="line">            features = test_data[start_index:end_index]</span><br><span class="line">            <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">                sentence, tokens, label = feature</span><br><span class="line">                batch_sentences.append(sentence)</span><br><span class="line">                batch_tokenized_sentences.append(tokens)</span><br><span class="line">                batch_labels.append(label)</span><br><span class="line">            preds, curr_eval_score = self.predict_by_batch(batch_tokenized_sentences, batch_labels)</span><br><span class="line">            eval_score += curr_eval_score</span><br><span class="line">        <span class="keyword">if</span> verbose:</span><br><span class="line">            <span class="keyword">for</span> sentence, pred, label <span class="keyword">in</span> zip(batch_sentences, preds, batch_labels):</span><br><span class="line">                <span class="built_in">print</span>(sentence, <span class="string">", pred:"</span>, pred, <span class="string">", label:"</span>, label)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"number of correct:"</span>, str(eval_score), <span class="string">", total:"</span>, str(len(test_data)), <span class="string">", score:"</span>, str(eval_score / len(test_data)))</span><br><span class="line"></span><br><span class="line">    def predict(self, sentence):</span><br><span class="line">        <span class="comment"># 문장을 예측을 하기 위해서는 우선 형태소를 분석을 해야한다.</span></span><br><span class="line">        tokens = self.tokenizer.morphs(sentence)</span><br><span class="line">        <span class="comment"># 문장의 형태소들을 임베딩 벡터와 같은 크기의 영벡터를 만든후 계속해서 더해주는 방식으로 문장 임베딩 벡터를 생성한다.</span></span><br><span class="line">        <span class="comment"># 만약 average=True했다면,</span></span><br><span class="line">        sentence_vector = self.get_sentence_vector(tokens)</span><br><span class="line">        <span class="comment"># 모델의 문장 임베딩 벡터와 sentence 문장 벡터와의 내적으로 유사도를 측정한다.</span></span><br><span class="line">        scores = np.dot(self.model[<span class="string">"vectors"</span>], sentence_vector)</span><br><span class="line">        <span class="comment"># 제일높은 유사도를 지닌 라벨을 출력해준다.</span></span><br><span class="line">        pred = self.model[<span class="string">"labels"</span>][np.argmax(scores)]</span><br><span class="line">        <span class="built_in">return</span> pred</span><br><span class="line"></span><br><span class="line">    def predict_by_batch(self, tokenized_sentences, labels):</span><br><span class="line">        sentence_vectors, eval_score = [], 0</span><br><span class="line">        <span class="keyword">for</span> tokens <span class="keyword">in</span> tokenized_sentences:</span><br><span class="line">            sentence_vectors.append(self.get_sentence_vector(tokens))</span><br><span class="line">        scores = np.dot(self.model[<span class="string">"vectors"</span>], np.array(sentence_vectors).T)</span><br><span class="line">        preds = np.argmax(scores, axis=0)</span><br><span class="line">        <span class="keyword">for</span> pred, label <span class="keyword">in</span> zip(preds, labels):</span><br><span class="line">            <span class="keyword">if</span> self.model[<span class="string">"labels"</span>][pred] == label:</span><br><span class="line">                eval_score += 1</span><br><span class="line">        <span class="built_in">return</span> preds, eval_score</span><br><span class="line"></span><br><span class="line">    def get_sentence_vector(self, tokens):</span><br><span class="line">        vector = np.zeros(self.dim)</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> self.embeddings.keys():</span><br><span class="line">                vector += self.embeddings[token]</span><br><span class="line">        <span class="keyword">if</span> self.average:</span><br><span class="line">            vector /= len(tokens)</span><br><span class="line">        vector_norm = np.linalg.norm(vector)</span><br><span class="line">        <span class="keyword">if</span> vector_norm != 0:</span><br><span class="line">            unit_vector = vector / vector_norm</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            unit_vector = np.zeros(self.dim)</span><br><span class="line">        <span class="built_in">return</span> unit_vector</span><br><span class="line"></span><br><span class="line">    def load_or_tokenize_corpus(self, fname):</span><br><span class="line">        data = []</span><br><span class="line">        <span class="keyword">if</span> os.path.exists(fname + <span class="string">"-tokenized"</span>):</span><br><span class="line">            with open(fname + <span class="string">"-tokenized"</span>, <span class="string">"r"</span>) as f1:</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> f1:</span><br><span class="line">                    sentence, tokens, label = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                    data.append([sentence, tokens.split(), label])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            with open(fname, <span class="string">"r"</span>) as f2, open(fname + <span class="string">"-tokenized"</span>, <span class="string">"w"</span>) as f3:</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> f2:</span><br><span class="line">                    sentence, label = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                    tokens = self.tokenizer.morphs(sentence)</span><br><span class="line">                    data.append([sentence, tokens, label])</span><br><span class="line">                    f3.writelines(sentence + <span class="string">"\u241E"</span> + <span class="string">' '</span>.join(tokens) + <span class="string">"\u241E"</span> + label + <span class="string">"\n"</span>)</span><br><span class="line">        <span class="built_in">return</span> data</span><br><span class="line"></span><br><span class="line">    def compute_word_frequency(self, embedding_corpus_fname):</span><br><span class="line">        total_count = 0</span><br><span class="line">        <span class="comment"># &#123;단어 : 해당 단어 개수&#125;로 표현해주기 위해 다음과 같이 defaultdict을 사용했다.</span></span><br><span class="line">        <span class="comment"># defaultdict 을 사용한 이유는 값을 따로 지정해 주지 않는다면 default 값을 사용하기 위해서이다.</span></span><br><span class="line">        words_count = defaultdict(int)</span><br><span class="line">        with open(embedding_corpus_fname, <span class="string">"r"</span>) as f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                tokens = line.strip().split()</span><br><span class="line">                <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">                    words_count[token] += 1</span><br><span class="line">                    total_count += 1</span><br><span class="line">        <span class="built_in">return</span> words_count, total_count</span><br><span class="line"></span><br><span class="line">    def load_word_embeddings(self, vecs_fname, method):</span><br><span class="line">        <span class="keyword">if</span> method == <span class="string">"word2vec"</span>:</span><br><span class="line">            model = Word2Vec.load(vecs_fname)</span><br><span class="line">            words = model.wv.index2word</span><br><span class="line">            vecs = model.wv.vectors</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            words, vecs = [], []</span><br><span class="line">            with open(vecs_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f1:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">"fasttext"</span> <span class="keyword">in</span> method:</span><br><span class="line">                    next(f1)  <span class="comment"># skip head line</span></span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> f1:</span><br><span class="line">                    <span class="keyword">if</span> method == <span class="string">"swivel"</span>:</span><br><span class="line">                        splited_line = line.replace(<span class="string">"\n"</span>, <span class="string">""</span>).strip().split(<span class="string">"\t"</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        splited_line = line.replace(<span class="string">"\n"</span>, <span class="string">""</span>).strip().split(<span class="string">" "</span>)</span><br><span class="line">                    words.append(splited_line[0])</span><br><span class="line">                    vec = [<span class="built_in">float</span>(el) <span class="keyword">for</span> el <span class="keyword">in</span> splited_line[1:]]</span><br><span class="line">                    vecs.append(vec)</span><br><span class="line">        <span class="built_in">return</span> words, vecs</span><br><span class="line"></span><br><span class="line">    def load_or_construct_weighted_embedding(self, embedding_fname, embedding_method, embedding_corpus_fname, a=0.0001):</span><br><span class="line">        dictionary = &#123;&#125;</span><br><span class="line">        <span class="comment"># 이미 만들어진 가중합 embedding이 존재할 경우</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(embedding_fname + <span class="string">"-weighted"</span>):</span><br><span class="line">            with open(embedding_fname + <span class="string">"-weighted"</span>, <span class="string">"r"</span>) as f2:</span><br><span class="line">                <span class="keyword">for</span> line <span class="keyword">in</span> f2:</span><br><span class="line">                    <span class="comment"># \u241E : Symbol for Record Seperator</span></span><br><span class="line">                    word, weighted_vector = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                    weighted_vector = [<span class="built_in">float</span>(el) <span class="keyword">for</span> el <span class="keyword">in</span> weighted_vector.split()]</span><br><span class="line">                    dictionary[word] = weighted_vector</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 위에서 embedding-weighted 파일이 존재하지 않는다면 훈련을 해야하므로</span></span><br><span class="line">            <span class="comment"># 우선 이미 embedding된 파일의 단어와 해당단어의 임베딩 벡터를 불러온다.</span></span><br><span class="line">            <span class="comment"># load pretrained word embeddings</span></span><br><span class="line">            <span class="comment"># 해당 임베딩 파일에 있는 단어와 그에 해당하는 임베딩벡터를 순서대로 불러온다.</span></span><br><span class="line">            words, vecs = self.load_word_embeddings(embedding_fname, embedding_method)</span><br><span class="line">            <span class="comment"># compute word frequency</span></span><br><span class="line">            words_count, total_word_count = self.compute_word_frequency(embedding_corpus_fname)</span><br><span class="line">            <span class="comment"># construct weighted word embeddings</span></span><br><span class="line">            <span class="comment"># embedding_fname - weighted로 가중합을 계산한 임베딩벡터 파일을 생성한다.</span></span><br><span class="line">            with open(embedding_fname + <span class="string">"-weighted"</span>, <span class="string">"w"</span>) as f3:</span><br><span class="line">                <span class="keyword">for</span> word, vec <span class="keyword">in</span> zip(words, vecs):</span><br><span class="line">                    <span class="keyword">if</span> word <span class="keyword">in</span> words_count.keys():</span><br><span class="line">                        word_prob = words_count[word] / total_word_count</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        word_prob = 0.0</span><br><span class="line">                    weighted_vector = (a / (word_prob + a)) * np.asarray(vec)</span><br><span class="line">                    dictionary[word] = weighted_vector</span><br><span class="line">                    f3.writelines(word + <span class="string">"\u241E"</span> + <span class="string">" "</span>.join([str(el) <span class="keyword">for</span> el <span class="keyword">in</span> weighted_vector]) + <span class="string">"\n"</span>)</span><br><span class="line">        <span class="built_in">return</span> dictionary</span><br><span class="line"></span><br><span class="line">    def train_model(self, train_data_fname, model_fname):</span><br><span class="line">        model = &#123;<span class="string">"vectors"</span>: [], <span class="string">"labels"</span>: [], <span class="string">"sentences"</span>: []&#125;</span><br><span class="line">        <span class="comment"># [sentence, tokens, label]형태로 출력</span></span><br><span class="line">        train_data = self.load_or_tokenize_corpus(train_data_fname)</span><br><span class="line">        with open(model_fname, <span class="string">"w"</span>) as f:</span><br><span class="line">            <span class="keyword">for</span> sentence, tokens, label <span class="keyword">in</span> train_data:</span><br><span class="line">                tokens = self.tokenizer.morphs(sentence)</span><br><span class="line">                sentence_vector = self.get_sentence_vector(tokens)</span><br><span class="line">                model[<span class="string">"sentences"</span>].append(sentence)</span><br><span class="line">                model[<span class="string">"vectors"</span>].append(sentence_vector)</span><br><span class="line">                model[<span class="string">"labels"</span>].append(label)</span><br><span class="line">                str_vector = <span class="string">" "</span>.join([str(el) <span class="keyword">for</span> el <span class="keyword">in</span> sentence_vector])</span><br><span class="line">                f.writelines(sentence + <span class="string">"\u241E"</span> + <span class="string">" "</span>.join(tokens) + <span class="string">"\u241E"</span> + str_vector + <span class="string">"\u241E"</span> + label + <span class="string">"\n"</span>)</span><br><span class="line">        <span class="built_in">return</span> model</span><br><span class="line"></span><br><span class="line">    def load_model(self, model_fname):</span><br><span class="line">        model = &#123;<span class="string">"vectors"</span>: [], <span class="string">"labels"</span>: [], <span class="string">"sentences"</span>: []&#125;</span><br><span class="line">        with open(model_fname, <span class="string">"r"</span>) as f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                sentence, _, vector, label = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                vector = np.array([<span class="built_in">float</span>(el) <span class="keyword">for</span> el <span class="keyword">in</span> vector.split()])</span><br><span class="line">                model[<span class="string">"sentences"</span>].append(sentence)</span><br><span class="line">                model[<span class="string">"vectors"</span>].append(vector)</span><br><span class="line">                model[<span class="string">"labels"</span>].append(label)</span><br><span class="line">        <span class="built_in">return</span> model</span><br></pre></td></tr></table></figure>
<h4 id="모델의-파라미터-값-설정"><a href="#모델의-파라미터-값-설정" class="headerlink" title="모델의 파라미터 값 설정"></a>모델의 파라미터 값 설정</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">train_fname = <span class="string">"./data/processed/processed_ratings_train.txt"</span></span><br><span class="line">embedding_fname = <span class="string">"./data/word-embeddings/word2vec/word2vec"</span></span><br><span class="line">model_fname = <span class="string">"./data/word-embeddings/cbow/word2vec"</span></span><br><span class="line">embedding_corpus_fname = <span class="string">"./data/tokenized/ratings_mecab.txt"</span></span><br><span class="line">embedding_method = <span class="string">"word2vec"</span></span><br><span class="line">test_data_fname = <span class="string">"./data/processed/processed_ratings_test.txt"</span></span><br></pre></td></tr></table></figure>
<h3 id="모델-학습-및-평가"><a href="#모델-학습-및-평가" class="headerlink" title="모델 학습 및 평가"></a>모델 학습 및 평가</h3><h4 id="해당-문장에-대한-단어벡터들의-합만을-가지고-예측"><a href="#해당-문장에-대한-단어벡터들의-합만을-가지고-예측" class="headerlink" title="해당 문장에 대한 단어벡터들의 합만을 가지고 예측"></a>해당 문장에 대한 단어벡터들의 합만을 가지고 예측</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">original_Model=CBoWModel(train_fname=train_fname, embedding_fname=embedding_fname, model_fname=model_fname, embedding_corpus_fname=None,</span><br><span class="line">                         embedding_method=embedding_method, is_weighted=False, average=False, dim=100, tokenizer_name=<span class="string">"mecab"</span>)</span><br><span class="line"></span><br><span class="line">original_Model.evaluate(test_data_fname)</span><br></pre></td></tr></table></figure>
<h4 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loading original embeddings, complete!</span><br><span class="line">train Continuous Bag of Words model</span><br><span class="line"></span><br><span class="line">evaluation start!</span><br><span class="line">number of correct: 36498 , total: 49997 , score: 0.7300038002280137</span><br></pre></td></tr></table></figure>
<h4 id="해당-문장에-대한-단어벡터들의-가중합을-가중합을-가지고-예측"><a href="#해당-문장에-대한-단어벡터들의-가중합을-가중합을-가지고-예측" class="headerlink" title="해당 문장에 대한 단어벡터들의 가중합을 가중합을 가지고 예측"></a>해당 문장에 대한 단어벡터들의 가중합을 가중합을 가지고 예측</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">weighted_Model=CBoWModel(train_fname=train_fname, embedding_fname=embedding_fname, model_fname=model_fname,</span><br><span class="line">                         embedding_corpus_fname=embedding_corpus_fname,embedding_method=embedding_method,</span><br><span class="line">                         is_weighted=True, average=False, dim=100, tokenizer_name=<span class="string">"mecab"</span>)</span><br><span class="line"></span><br><span class="line">weighted_Model.evaluate(test_data_fname)</span><br></pre></td></tr></table></figure>
<h4 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loading weighted embeddings, complete!</span><br><span class="line">train Continuous Bag of Words model</span><br><span class="line"></span><br><span class="line">evaluation start!</span><br><span class="line">number of correct: 34208 , total: 49997 , score: 0.6842010520631238</span><br></pre></td></tr></table></figure>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "NLP - 단어 수준 임베딩",
        "image": "https://heung-bae-lee.github.io/image/NPLM_principal.png",
        "keywords": "",
        "genre": "NLP",
        "datePublished": "2020-02-01",
        "dateCreated": "2020-02-01",
        "dateModified": "2020-02-08",
        "url": "https://heung-bae-lee.github.io/2020/02/01/NLP_06/",
        "description": "단어 수준 임베딩
예측 기반 모델

NPLM
Word2Vec
FastText


행렬 분해 기반 모델

LSA
GloVe
Swivel


단어 임베딩을 문장 수준 임베딩으로 확장하는 방법

가중 임베딩(Weighted Embedding)



NPLM(Neural Probabilistic Language Model)
NLP 분야에서 임베딩 개념을 널리 퍼뜨"
        "wordCount": 8299
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2020/02/06/NLP_08/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            NLP 문장 수준 임베딩 - 01
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2020/02/01/NLP_05/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/06/09/linear_algebra_05/" class="title">Linear Transformation &amp; onto, ono-to-one의 개념</a></p>
                            <p class="item-date"><time datetime="2020-06-09T05:23:12.000Z" itemprop="datePublished">2020-06-09</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/06/08/linear_algebra_04/" class="title">Linear Independence, Span, and Subspace</a></p>
                            <p class="item-date"><time datetime="2020-06-08T06:52:22.000Z" itemprop="datePublished">2020-06-08</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/06/machine_learning_20/" class="title">Imbalanced Data</a></p>
                            <p class="item-date"><time datetime="2020-06-05T16:52:20.000Z" itemprop="datePublished">2020-06-06</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/04/machine_learning_19/" class="title">Clustering - Hierarchical, DBSCAN, Affinity Propagation</a></p>
                            <p class="item-date"><time datetime="2020-06-04T13:46:15.000Z" itemprop="datePublished">2020-06-04</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/30/machine_learning_18/" class="title">Clustering - K-means, K-medoid</a></p>
                            <p class="item-date"><time datetime="2020-05-29T16:01:30.000Z" itemprop="datePublished">2020-05-30</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">21</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/02/01/NLP_06/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
