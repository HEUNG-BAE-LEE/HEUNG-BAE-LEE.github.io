<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="순환신경망 분류 모델 앞선 모델들과 달리 이미 주어진 단어 특징 벡터를 활용해 모델을 학습하지 않고 텍스트 정보를 입력해서 문장에 대한 특징 정보를 추출한다.  RNN은 현재 정보는 이전 정보가 점층적으로 쌓이면서 정보를 표현할 수 있는 모델이다. 따라서 시간에 의존적인 또는 순차적인 데이터에 대한 문제에 활용된다. 이 모델은 한단에 대한 정보를 입력하면">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/02/01/NLP_05/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="순환신경망 분류 모델 앞선 모델들과 달리 이미 주어진 단어 특징 벡터를 활용해 모델을 학습하지 않고 텍스트 정보를 입력해서 문장에 대한 특징 정보를 추출한다.  RNN은 현재 정보는 이전 정보가 점층적으로 쌓이면서 정보를 표현할 수 있는 모델이다. 따라서 시간에 의존적인 또는 순차적인 데이터에 대한 문제에 활용된다. 이 모델은 한단에 대한 정보를 입력하면">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/BOW_LSTM_performence.png">
<meta property="og:updated_time" content="2020-02-03T14:30:51.344Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03">
<meta name="twitter:description" content="순환신경망 분류 모델 앞선 모델들과 달리 이미 주어진 단어 특징 벡터를 활용해 모델을 학습하지 않고 텍스트 정보를 입력해서 문장에 대한 특징 정보를 추출한다.  RNN은 현재 정보는 이전 정보가 점층적으로 쌓이면서 정보를 표현할 수 있는 모델이다. 따라서 시간에 의존적인 또는 순차적인 데이터에 대한 문제에 활용된다. 이 모델은 한단에 대한 정보를 입력하면">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/BOW_LSTM_performence.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/02/01/nlp_05/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- data-ad-client="ca-pub-4604833066889492" -->
<script>
(adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-4604833066889492",
    enabel_page_level_ads: true
});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/NLP/">NLP</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- data-ad-client="ca-pub-4604833066889492" -->
<script>
(adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-4604833066889492",
    enabel_page_level_ads: true
});
</script>

			    <article id="post-NLP_05" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/02/01/NLP_05/" class="article-date">
            <time datetime="2020-02-01T07:57:48.000Z" itemprop="datePublished">2020-02-01</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h3 id="순환신경망-분류-모델"><a href="#순환신경망-분류-모델" class="headerlink" title="순환신경망 분류 모델"></a>순환신경망 분류 모델</h3><ul>
<li><p>앞선 모델들과 달리 이미 주어진 단어 특징 벡터를 활용해 모델을 학습하지 않고 텍스트 정보를 입력해서 문장에 대한 특징 정보를 추출한다.</p>
</li>
<li><p>RNN은 현재 정보는 이전 정보가 점층적으로 쌓이면서 정보를 표현할 수 있는 모델이다. 따라서 <code>시간에 의존적인 또는 순차적인 데이터에 대한 문제에 활용</code>된다. 이 모델은 한단에 대한 정보를 입력하면 이 단어 다음에 나올 단어를 맞추는 모델이라 순차적인 데이터에 대한 모델링이 가능한 것이다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">INPUT_TRAIN_DATA_FILE_NAME = <span class="string">'train_input.npy'</span></span><br><span class="line">LABEL_TRAIN_DATA_FILE_NAME = <span class="string">'train_label.npy'</span></span><br><span class="line">DATA_CONFIGS_FILE_NAME = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line">train_input = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line">train_label = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line"></span><br><span class="line">prepro_configs = None</span><br><span class="line"></span><br><span class="line">with open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, <span class="string">'r'</span>) as f:</span><br><span class="line">  prepro_configs = json.load(f)</span><br></pre></td></tr></table></figure>
<h4 id="학습과-검증-데이터셋-분리"><a href="#학습과-검증-데이터셋-분리" class="headerlink" title="학습과 검증 데이터셋 분리"></a>학습과 검증 데이터셋 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">TEST_SPLIT=0.1</span><br><span class="line">RANDOM_SEED=13371447</span><br><span class="line"></span><br><span class="line">input_train, input_eval, label_train, label_eval = train_test_split(train_input, train_label, test_size=TEST_SPLIT, random_state=RANDOM_SEED)</span><br></pre></td></tr></table></figure>
<h4 id="데이터-입력-함수"><a href="#데이터-입력-함수" class="headerlink" title="데이터 입력 함수"></a>데이터 입력 함수</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = 16</span><br><span class="line">NUM_EPOCHS = 20</span><br><span class="line"></span><br><span class="line">def mapping_fn(X, Y):</span><br><span class="line">  inputs, labels = &#123;<span class="string">'x'</span> : X&#125;, Y</span><br><span class="line">  <span class="built_in">return</span> inputs, labels</span><br><span class="line"></span><br><span class="line">def train_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((input_train, label_train))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=50000)</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  dataset = dataset.repeat(count=NUM_EPOCHS)</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br><span class="line"></span><br><span class="line">def eval_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((input_eval, label_eval))</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br></pre></td></tr></table></figure>
<h4 id="모델-함수"><a href="#모델-함수" class="headerlink" title="모델 함수"></a>모델 함수</h4><h5 id="모델-하이퍼파라미터-정의"><a href="#모델-하이퍼파라미터-정의" class="headerlink" title="모델 하이퍼파라미터 정의"></a>모델 하이퍼파라미터 정의</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = prepro_configs[<span class="string">'vocab_size'</span>]</span><br><span class="line"></span><br><span class="line">WORD_EMBEDDING_DIM = 100</span><br><span class="line"></span><br><span class="line">HIDDEN_STATE_DIM = 150</span><br><span class="line"></span><br><span class="line">DENSE_FEATURE_DIM = 150</span><br><span class="line"></span><br><span class="line">learning_rate = 0.001</span><br></pre></td></tr></table></figure>
<h5 id="모델-구현"><a href="#모델-구현" class="headerlink" title="모델 구현"></a>모델 구현</h5><ul>
<li><p>먼저 모델에서 배치 데이터를 받게 된다면 단어 인덱스로 구성된 Sequence 형태로 입력이 들어온다. 데이터 입력 함수에서 정의했듯이 모델 함수의 입력 인자인 features는 Python dictionary 형태로 구성돼 있다.</p>
</li>
<li><p>모델에 들어온 입력 데이터는 보통 Embedding Layer를 거친다. 구현하고자 하는 모델에서는 tf.keras.Embedding함수가 이 같은 역할을 수행한다.</p>
</li>
<li><p>Embedding Layer를 거쳐 나온 데이터는 순환 신경망 층을 거쳐 문자의 벡터를 출력한다. 여기서는 간단한 심층 순환 신경망 모델로 LSTM 모델을 통해 구현한다. 순환 신경망을 구현하기 위해서는 RNNCell이란 객체를 활용함ㄴ다. RNNCell은 순환 신경망 객체라 보면된다. LSTM으로 순환 신경망을 구현하기 위해 tf.nn.rnn_cell.LSTMCell객체를 생성하며, 이 객체는 하나의 LSTM Cell을 의미한다. 따라서 해당 Cell 객체를 여러개 생성해서 하나의 리스트로 만들어 준다. LSTMCell을 생성할 때는 은닉 상태 벡터(Hidden state vector)에 대한 차원만 정의하면 된다.</p>
</li>
<li><p>여러 LSTMCell을 쌀게 되면 이를 하나의 MultiRNN으로 묶어야, 즉 wrapping해야한다. tf.nn.rnn_cell.MultiRNNCell을 생성함으로써 Stack 구조의 LSTM 신경망을 구현할 수 있다. 단순히 RNNCell 만으로 구성해 모델 연산 그래프를 만들 수 있다. RNNCell 객체는 Sequence 한 스텝에 대한 연산만 가능하다. 따라서 여러 스텝에 대한 연산을 하기 위해서는 for 문을 활용해 연산을 할 수 있게 구현해야한다. 하지만 이보다 더 간단하게 구현할 수 있는 방법은 tf.nn.dynamic_rnn 함수를 사용하는 것이다. 이 함수는 for 문 없이 자동으로 순환 신경망을 만들어 주는 역할을 한다.</p>
</li>
<li><p>dynamic_rnn 함수에 필요한 입력 인자는 2개다. 첫 번째 순환 신경망 객체인 MultiRNNCell 객체이고, 나머지 하나는 입력값을 넣어주면된다.</p>
</li>
<li><p>Dense에 적용시키는 입력값은 LSTM 신경망의 마지막 출력값을 넣어준다. <code>출력값에 [:, -1, :]로 마지막 값만 뽑아낸 후 Dense에 적용</code>시킨다.</p>
</li>
<li><p>마지막으로 감정이 긍정인지 부정인지 판단할 수 있도록 출력값을 하나로 만들어야 한다. 보통 선형변환을 통해 입력 벡터에 대한 차원수를 바꾼다.</p>
</li>
</ul>
<h4 id="모델-학습-검정-및-테스트를-위한-구현"><a href="#모델-학습-검정-및-테스트를-위한-구현" class="headerlink" title="모델 학습, 검정 및 테스트를 위한 구현"></a>모델 학습, 검정 및 테스트를 위한 구현</h4><ul>
<li><p>앞서 모델에서 구현한 값과 정답 label을 가지고 loss 값을 구해 Adam optimizer를 활용해 모델 parameter를 최적화 해 볼 것이다.</p>
</li>
<li><p>모델 예측 loss값은 모델에서 구한 logits 변수의 경우 아직 Logistic 함수를 통해 0~1 사이의 값으로 스케일을 맞춰두지 않았다. 물론 앞서 dense 층에서 activation 인자를 tf.nn.sigmoid로 설정해둘 수 있다. 하지만 여기서는 tf.losses.sigmoid_cross_entropy 함수를 활용해 손실값을 구할 수 있기 때문에 dense 층에서 설정하지 않았다.</p>
</li>
<li><p>예측 loss값을 구하고 나면 이제 parameter optimization을 하고자 SGD를 진행한다. 여기서는 tf.train.AdamOptimizer클래스를 활용할 것이다. tf.train.AdamOptimizer.minimize 함수를 선언 할 때 전체 학습에 대한 global step값을 넣어야 한다. tf.train.get_global_step을 선언하면 현재 학습 global step을 얻을 수 있다.</p>
</li>
<li><p>보통 직접 모델 함수를 구현하게 되면 tf.estimator.EstimatorSpec 객체를 생성해서 반환하게 한다. 이 객체는 현재 함수가 어느 모드에서 실행되고 있는지 확인한다. 그리고 각 모드에 따라 필요한 입력 인자가 다르다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def model_fn(features, labels, mode):</span><br><span class="line">  TRAIN = mode == tf.estimator.ModeKeys.TRAIN</span><br><span class="line">  EVAL = mode == tf.estimator.ModeKeys.EVAL</span><br><span class="line">  PREDICT = mode == tf.estimator.ModeKeys.PREDICT</span><br><span class="line"></span><br><span class="line">  embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM)(features[<span class="string">'x'</span>])</span><br><span class="line"></span><br><span class="line">  embedding_layer = tf.keras.layers.Dropout(0.2)(embedding_layer)</span><br><span class="line"></span><br><span class="line">  rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) <span class="keyword">for</span> size <span class="keyword">in</span> [HIDDEN_STATE_DIM, HIDDEN_STATE_DIM]]</span><br><span class="line"></span><br><span class="line">  multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)</span><br><span class="line"></span><br><span class="line">  outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell, inputs=embedding_layer, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">  outputs = tf.keras.layers.Dropout(0.2)(outputs)</span><br><span class="line"></span><br><span class="line">  hidden_layer = tf.keras.layers.Dense(DENSE_FEATURE_DIM, activation=tf.nn.tanh)(outputs[:, -1, :])</span><br><span class="line">  hidden_layer = tf.keras.layers.Dropout(0.2)(hidden_layer)</span><br><span class="line"></span><br><span class="line">  logits = tf.keras.layers.Dense(1)(hidden_layer)</span><br><span class="line">  logits = tf.squeeze(logits, axis=-1)</span><br><span class="line"></span><br><span class="line">  sigmoid_logits = tf.nn.sigmoid(logits)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> PREDICT:</span><br><span class="line">    predictions = &#123;<span class="string">'sentiment'</span>: sigmoid_logits&#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)</span><br><span class="line"></span><br><span class="line">  loss = tf.losses.sigmoid_cross_entropy(labels, logits)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> EVAL:</span><br><span class="line">    accuracy = tf.metrics.accuracy(labels, tf.round(sigmoid_logits))</span><br><span class="line">    eval_metric_ops = &#123;<span class="string">'acc'</span>:accuracy&#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=eval_metric_ops)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> TRAIN:</span><br><span class="line">    global_step = tf.train.get_global_step()</span><br><span class="line">    train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)</span><br></pre></td></tr></table></figure>
<h4 id="TF-Estimator-활용한-모델-학습-및-성능-검증"><a href="#TF-Estimator-활용한-모델-학습-및-성능-검증" class="headerlink" title="TF Estimator 활용한 모델 학습 및 성능 검증"></a>TF Estimator 활용한 모델 학습 및 성능 검증</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">est = tf.estimator.Estimator(model_fn, model_dir=DATA_OUT_PATH + <span class="string">'checkpoint'</span>)</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">"CUDA_VISIBLE_DEVICES"</span>]=<span class="string">"4"</span></span><br><span class="line"></span><br><span class="line">est.train(train_input_fn)</span><br></pre></td></tr></table></figure>
<ul>
<li>validation data에 대한 성능이 약 85%정도였다. 오히려 앞의 머신러닝 기법들 중 어떤 기법보다는 성능이 떨어진다는 것을 볼 수 있었지만, test data에 대한 성능을 한번 체크해 보아야 할 것 같다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">est.evaluate(eval_input_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># &#123;'acc': 0.8472, 'global_step': 18291, 'loss': 0.6007853&#125;</span></span><br></pre></td></tr></table></figure>
<h4 id="데이터-제출"><a href="#데이터-제출" class="headerlink" title="데이터 제출"></a>데이터 제출</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line">TEST_INPUT_DATA = <span class="string">'test_input.npy'</span></span><br><span class="line"></span><br><span class="line">test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, <span class="string">'rb'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>estimator를 통해 예측하기 위해서는 데이터 입력 함수를 정의해야 했다. 이 경우는 tf.estimator.inputs.numpy_input_fn 함수를 활용해 데이터 입력 함수를 생성한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">"x"</span>: test_input_data&#125;, shuffle=False)</span><br><span class="line"></span><br><span class="line">predictions = np.array([p[<span class="string">'sentiment'</span>] <span class="keyword">for</span> p <span class="keyword">in</span> est.predict(input_fn=predict_input_fn)])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">TEST_ID_DATA = <span class="string">'test_id.npy'</span></span><br><span class="line">test_id = np.load(open(DATA_IN_PATH + TEST_ID_DATA, <span class="string">'rb'</span>), allow_pickle=<span class="string">'True'</span>)</span><br><span class="line">output = pd.DataFrame(&#123;<span class="string">'id'</span>: test_id, <span class="string">'sentiment'</span>: list(predictions)&#125;)</span><br><span class="line">output.to_csv(DATA_OUT_PATH + <span class="string">"rnn_predic.csv"</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"rnn_predic.csv"</span> -m <span class="string">"LSTM Model with Epoch 10"</span></span><br></pre></td></tr></table></figure>
<p><img src="/image/BOW_LSTM_performence.png" alt="LSTM의 성능"></p>
<h3 id="CNN을-이용한-문장-분류"><a href="#CNN을-이용한-문장-분류" class="headerlink" title="CNN을 이용한 문장 분류"></a>CNN을 이용한 문장 분류</h3><ul>
<li>CNN은 보통 image에서 많이 사용된다고 생각들지만, 텍스트에서도 좋은 효과를 낼 수 있다는 점을 Yoon Kimm(2014) 박사가 쓴 “Convolutional Neural Network for Sentence Classification”을 통해 입증되었다. <code>RNN이 단어의 입력 순서를 중요하게 반영한다면 CNN은 문장의 지역정보를 보존하면서 각 문장 성분의 등장 정보를 학습에 반영하는 구조로 풀어가고 있다. 학습할 때 각 필터 크기를 조절하면서 언어의 특징 값을 추출하게 되는데, 기존의 n-gram(2그램, 3그램) 방식과 유사하다고 볼 수 있다.</code></li>
</ul>
<p><img src="/image/Convnets_with_text_classification.png" alt="합성곱 신경망"></p>
<p><img src="/image/2D_Convnets_with_text_classification.png" alt="2-D 합성곱 신경망"></p>
<p><img src="/image/1D_Convnets_with_text_classification.png" alt="1-D 합성곱 신경망"></p>
<p><img src="/image/Convnets_with_text_classification_explanin.png" alt="CNN을 이용한 문장 분류"></p>
<h4 id="모델-구현-1"><a href="#모델-구현-1" class="headerlink" title="모델 구현"></a>모델 구현</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 기본적인 라이브러리들을 불러온다</span></span><br><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import json</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow import keras</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 이전에 저장했던 학습에 필요한 디렉터리 설정 및 학습/평가 데이터를 불러온다.</span></span><br><span class="line"></span><br><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line">INPUT_TRAIN_DATA_FILE_NAME = <span class="string">'train_input.npy'</span></span><br><span class="line">LABEL_TRAIN_DATA_FILE_NAME = <span class="string">'train_label.npy'</span></span><br><span class="line">INPUT_TEST_DATA_FILE_NAME = <span class="string">'test_input.npy'</span></span><br><span class="line"></span><br><span class="line">DATA_CONFIGS_FILE_NAME = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line">train_input_data = np.load(open(DATA_IN_PATH + INPUT_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line">train_label_data = np.load(open(DATA_IN_PATH + LABEL_TRAIN_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line">test_input_data = np.load(open(DATA_IN_PATH + INPUT_TEST_DATA_FILE_NAME, <span class="string">'rb'</span>))</span><br><span class="line"></span><br><span class="line">with open(DATA_IN_PATH + DATA_CONFIGS_FILE_NAME, <span class="string">'r'</span>) as f:</span><br><span class="line">  prepro_configs = json.load(f)</span><br><span class="line">  <span class="built_in">print</span>(prepro_configs.keys())</span><br></pre></td></tr></table></figure>
<h4 id="학습과-검증-데이터셋-분리-1"><a href="#학습과-검증-데이터셋-분리-1" class="headerlink" title="학습과 검증 데이터셋 분리"></a>학습과 검증 데이터셋 분리</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 파라미터 변수</span></span><br><span class="line">RNG_SEED = 1234</span><br><span class="line">BATCH_SIZE = 16</span><br><span class="line">NUM_EPOCHS = 10</span><br><span class="line">VOCAB_SIZE = prepro_configs[<span class="string">'vocab_size'</span>]</span><br><span class="line">EMB_SIZE = 128</span><br><span class="line">VALID_SPLIT = 0.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 학습 데이터와 검증 데이터를 train_test_split 함수를 활용해 나눈다.</span></span><br><span class="line">train_input, eval_input, train_label, eval_label = train_test_split(train_input_data, train_label_data, test_size=VALID_SPLIT, random_state=RNG_SEED)</span><br></pre></td></tr></table></figure>
<h4 id="데이터-입력-함수-1"><a href="#데이터-입력-함수-1" class="headerlink" title="데이터 입력 함수"></a>데이터 입력 함수</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 전처리 학습을 위해 tf.data를 설정한다.</span></span><br><span class="line">def mapping_fn(X, Y=None):</span><br><span class="line">  input, label = &#123;<span class="string">'x'</span>: X&#125;, Y</span><br><span class="line">  <span class="built_in">return</span> input, label</span><br><span class="line"></span><br><span class="line">def train_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((train_input, train_label))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=len(train_input))</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line">  dataset = dataset.repeat(count=NUM_EPOCHS)</span><br><span class="line"></span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def eval_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((eval_input, eval_label))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=len(eval_input))</span><br><span class="line">  dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">  dataset = dataset.map(mapping_fn)</span><br><span class="line"></span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br></pre></td></tr></table></figure>
<h4 id="모델-구현-2"><a href="#모델-구현-2" class="headerlink" title="모델 구현"></a>모델 구현</h4><ul>
<li>합성곱 연산의 경우 케라스 모듈 중 Conv1D를 활용해 진행한다. 총 3개의 합성곱 층을 사용하는데, 각각 필터의 크기를 다르게 해서 적용한다. 즉, kernel_size를 3,4,5로 설정할 것이다. 그리고 이렇게 각각 다른 필터의 크기로 적용한 합성곱 층 출력값을 하나로 합칠 것이다. 그리고 추가로 각 합성곱 신경망 이후에 max pooling 층을 적용한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">def model_fn(features, labels, mode):</span><br><span class="line"></span><br><span class="line">  TRAIN = mode == tf.estimator.ModeKeys.TRAIN</span><br><span class="line">  EVAL = mode == tf.estimator.ModeKeys.EVAL</span><br><span class="line">  PREDICT = mode == tf.estimator.ModeKeys.PREDICT</span><br><span class="line"></span><br><span class="line">  <span class="comment"># embedding layer를 선언</span></span><br><span class="line">  embedding_layer = keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)(features[<span class="string">'x'</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># embedding layer에 대한 output에 대해 dropout을 취한다.</span></span><br><span class="line">  dropout_emb = keras.layers.Dropout(0.5)(embedding_layer)</span><br><span class="line"></span><br><span class="line">  <span class="comment">## filters = 128이고 kernel_size = 3,4,5이다.</span></span><br><span class="line">  <span class="comment">## 길이기ㅏ 3, 4, 5인 128개의 다른 필터를 생성한다. 3, 4, 5 gram의 효과처럼 다양한 각도에서 문장을 보는 효과가 있다.</span></span><br><span class="line">  <span class="comment">## conv1d는 (배치 크기, 길이, 채널)로 입력값을 받는데, 배치 사이즈 : 문장 숫자 | 길이 : 각 문장의 단어의 개수 | 채널 : 임베딩 출력 차원수</span></span><br><span class="line"></span><br><span class="line">  conv1 = keras.layers.Conv1D(filters=128, kernel_size=3, padding=<span class="string">'valid'</span>, activation=tf.nn.relu)(dropout_emb)</span><br><span class="line">  pool1 = keras.layers.GlobalMaxPool1D()(conv1)</span><br><span class="line"></span><br><span class="line">  conv2 = keras.layers.Conv1D(filters=128, kernel_size=4, padding=<span class="string">'valid'</span>, activation=tf.nn.relu)(dropout_emb)</span><br><span class="line">  pool2 = keras.layers.GlobalMaxPool1D()(conv2)</span><br><span class="line"></span><br><span class="line">  conv3 = keras.layers.Conv1D(filters=128, kernel_size=5, padding=<span class="string">'valid'</span>, activation=tf.nn.relu)(dropout_emb)</span><br><span class="line">  pool3 = keras.layers.GlobalMaxPool1D()(conv3)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3,4,5 gram이후 모아주기</span></span><br><span class="line">  concat = keras.layers.concatenate([pool1, pool2, pool3])</span><br><span class="line"></span><br><span class="line">  hidden = keras.layers.Dense(250, activation=tf.nn.relu)(concat)</span><br><span class="line">  dropout_hidden = keras.layers.Dropout(0.5)(hidden)</span><br><span class="line">  logits = keras.layers.Dense(1, name=<span class="string">'logits'</span>)(dropout_hidden)</span><br><span class="line">  logits = tf.squeeze(logits, axis=-1)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 최종적으로 학습, 검증, 평가의 단계로 나누어 활용</span></span><br><span class="line">  <span class="keyword">if</span> PREDICT:</span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, predictions=&#123;<span class="string">'prob'</span>: tf.nn.sigmoid(logits)&#125;)</span><br><span class="line"></span><br><span class="line">  loss = tf.losses.sigmoid_cross_entropy(labels, logits)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> EVAL:</span><br><span class="line">    pred = tf.nn.sigmoid(logits)</span><br><span class="line">    accuracy = tf.metrics.accuracy(labels, tf.round(pred))</span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=&#123;<span class="string">'acc'</span>:accuracy&#125;)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> TRAIN:</span><br><span class="line">    global_step = tf.train.get_global_step()</span><br><span class="line">    train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)</span><br></pre></td></tr></table></figure>
<h4 id="모델-학습"><a href="#모델-학습" class="headerlink" title="모델 학습"></a>모델 학습</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model_dir = os.path.join(os.getcwd(), <span class="string">"data_out/checkpoint/cnn"</span>)</span><br><span class="line">os.makedirs(model_dir, exist_ok=True)</span><br><span class="line"></span><br><span class="line">config_tf = tf.estimator.RunConfig(save_checkpoints_steps=200, keep_checkpoint_max=2,</span><br><span class="line">                                    log_step_count_steps=400)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Estimator 객체 생성</span></span><br><span class="line">cnn_est = tf.estimator.Estimator(model_fn, model_dir=model_dir)</span><br><span class="line">cnn_est.train(train_input_fn)</span><br></pre></td></tr></table></figure>
<h4 id="검증-데이터-평가"><a href="#검증-데이터-평가" class="headerlink" title="검증 데이터 평가"></a>검증 데이터 평가</h4><ul>
<li>검증 데이터에 대한 정확도가 약 88%정도로 측정되었다. 지금껏 간단한 모델들 중 제일 높은 성능을 보이고 있어 필자는 약간 기대하고 있었다. 이에따른 test data의 성능을 알아보기 위해 캐글에 test data의 예측값을 제출해 볼 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cnn_est.evaluate(eval_input_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 결과</span></span><br><span class="line"><span class="comment"># &#123;'acc': 0.8774, 'global_step': 94200, 'loss': 1.3248637&#125;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line">TEST_INPUT_DATA = <span class="string">'test_input.npy'</span></span><br><span class="line">TEST_ID_DATA = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">test_input_data = np.load(open(DATA_IN_PATH + TEST_INPUT_DATA, <span class="string">'rb'</span>))</span><br><span class="line">ids = np.load(open(DATA_IN_PATH + TEST_ID_DATA, <span class="string">'rb'</span>), allow_pickle=True)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">'x'</span>:test_input_data&#125;, shuffle=False)</span><br><span class="line"></span><br><span class="line">predictions = np.array([p[<span class="string">'prob'</span>] <span class="keyword">for</span> p <span class="keyword">in</span> cnn_est.predict(input_fn=predict_input_fn)])</span><br><span class="line"></span><br><span class="line">output = pd.DataFrame(&#123;<span class="string">"id"</span>: list(ids), <span class="string">"sentiment"</span>:list(predictions)&#125;)</span><br><span class="line"></span><br><span class="line">output.to_csv(DATA_OUT_PATH + <span class="string">"Bag_of_Words_model_test.csv"</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit word2vec-nlp-tutorial -f <span class="string">"Bag_of_Words_model_test.csv"</span> -m <span class="string">"CNN 1d Model with EPOCHS 10"</span></span><br></pre></td></tr></table></figure>
<p><img src="/image/BOW_CNN_performence.png" alt="3,4,5-gram 을 활용한 CNN의 성능"></p>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- data-ad-client="ca-pub-4604833066889492" -->
<script>
(adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-4604833066889492",
    enabel_page_level_ads: true
});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03",
        "image": "https://heung-bae-lee.github.io/image/BOW_LSTM_performence.png",
        "keywords": "",
        "genre": "NLP",
        "datePublished": "2020-02-01",
        "dateCreated": "2020-02-01",
        "dateModified": "2020-02-03",
        "url": "https://heung-bae-lee.github.io/2020/02/01/NLP_05/",
        "description": "순환신경망 분류 모델
앞선 모델들과 달리 이미 주어진 단어 특징 벡터를 활용해 모델을 학습하지 않고 텍스트 정보를 입력해서 문장에 대한 특징 정보를 추출한다.

RNN은 현재 정보는 이전 정보가 점층적으로 쌓이면서 정보를 표현할 수 있는 모델이다. 따라서 시간에 의존적인 또는 순차적인 데이터에 대한 문제에 활용된다. 이 모델은 한단에 대한 정보를 입력하면 "
        "wordCount": 2320
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2020/02/01/NLP_06/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            NLP - 단어 수준 임베딩
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2020/01/30/NLP_04/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">NLP 실습 텍스트 분류(TF-IDF, CountVectorizer, Word2Vec) -02</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/06/NLP_08/" class="title">NLP 문장 수준 임베딩</a></p>
                            <p class="item-date"><time datetime="2020-02-05T15:32:51.000Z" itemprop="datePublished">2020-02-06</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/01/NLP_06/" class="title">NLP - 단어 수준 임베딩</a></p>
                            <p class="item-date"><time datetime="2020-02-01T11:47:03.000Z" itemprop="datePublished">2020-02-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/01/NLP_05/" class="title">NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03</a></p>
                            <p class="item-date"><time datetime="2020-02-01T07:57:48.000Z" itemprop="datePublished">2020-02-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/01/30/NLP_04/" class="title">NLP 실습 텍스트 분류(TF-IDF, CountVectorizer, Word2Vec) -02</a></p>
                            <p class="item-date"><time datetime="2020-01-29T15:13:48.000Z" itemprop="datePublished">2020-01-30</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/01/29/NLP_03/" class="title">NLP 실습 텍스트 분류 -01</a></p>
                            <p class="item-date"><time datetime="2020-01-29T14:40:09.000Z" itemprop="datePublished">2020-01-29</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">5</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- data-ad-client="ca-pub-4604833066889492" -->
<script>
(adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-4604833066889492",
    enabel_page_level_ads: true
});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/02/01/NLP_05/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
