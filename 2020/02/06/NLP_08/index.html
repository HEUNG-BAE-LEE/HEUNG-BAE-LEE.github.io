<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>NLP 문장 수준 임베딩 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.문장 수준 임베딩 크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다.  행렬 분해  LSA(잠재 의미 분석)   확률 모형  LDA(잠재 디리클레 할당)   Neural Network  Doc2Vec ELMo GPT (tran">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP 문장 수준 임베딩">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/02/06/NLP_08/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.문장 수준 임베딩 크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다.  행렬 분해  LSA(잠재 의미 분석)   확률 모형  LDA(잠재 디리클레 할당)   Neural Network  Doc2Vec ELMo GPT (tran">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/read_lines.png">
<meta property="og:updated_time" content="2020-02-07T09:58:39.363Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP 문장 수준 임베딩">
<meta name="twitter:description" content="참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.문장 수준 임베딩 크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다.  행렬 분해  LSA(잠재 의미 분석)   확률 모형  LDA(잠재 디리클레 할당)   Neural Network  Doc2Vec ELMo GPT (tran">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/read_lines.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/02/06/nlp_08/">
   
    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script data-ad-client="ca-pub-4604833066889492" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/NLP/">NLP</a>
    </h1>
</div>

                        <div class="main-body-content">
			    <article id="post-NLP_08" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        NLP 문장 수준 임베딩
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/02/06/NLP_08/" class="article-date">
            <time datetime="2020-02-05T15:32:51.000Z" itemprop="datePublished">2020-02-06</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h4 id="참고로-이-모든-내용은-이기창-님의-한국어-임베딩이라는-책을-기반으로-작성하고-있다"><a href="#참고로-이-모든-내용은-이기창-님의-한국어-임베딩이라는-책을-기반으로-작성하고-있다" class="headerlink" title="참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다."></a>참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.</h4><h1 id="문장-수준-임베딩"><a href="#문장-수준-임베딩" class="headerlink" title="문장 수준 임베딩"></a>문장 수준 임베딩</h1><ul>
<li><p>크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다.</p>
</li>
<li><p>행렬 분해</p>
<ul>
<li>LSA(잠재 의미 분석)</li>
</ul>
</li>
<li><p>확률 모형</p>
<ul>
<li>LDA(잠재 디리클레 할당)</li>
</ul>
</li>
<li><p>Neural Network</p>
<ul>
<li>Doc2Vec</li>
<li>ELMo</li>
<li>GPT (transformer 구조 - self-attention)</li>
<li>BERT (transformer 구조 - self-attention)</li>
</ul>
</li>
</ul>
<h2 id="잠재-의미-분석-LSA-Latent-Semantic-Analysis"><a href="#잠재-의미-분석-LSA-Latent-Semantic-Analysis" class="headerlink" title="잠재 의미 분석(LSA, Latent Semantic Analysis)"></a>잠재 의미 분석(LSA, Latent Semantic Analysis)</h2><ul>
<li><p>단어 수준 임베딩에서의 LSA 방법론들은 word-documents 행렬이나 TF-IDF 행렬, word-context 행렬 또는 PMI 행렬에 SVD로 차원 축소를 시행하고, 여기에서 단어에 해당하는 벡터를 취해 임베딩을 만드는 방법이었다. <code>문장 수준 입베딩에서의 LSA 방법은 단어 수준 임베딩에서의 LSA 방법론을 통해 얻게된 정확히 말하자면 SVD를 통해 축소된 행렬에서 문서에 대응하는 벡터를 취해 문서 임베딩을 만드는 방식이다.</code></p>
</li>
<li><p>실습 대상 데이터는 ratsgo.github.uo의 아티클 하나로 markdwon 문서의 제목과 본문을 그대로 텍스트로 저장한 형태이다. 1개 라인이 1개 문서에 해당한다. 불필요한 기호나 LaTex math 패기지의 문법으로 작성되어있는 부분들이 다수 존재한다. 우선 <code>이 실습의 가정을 수식이나 기호는 분석에 있어서 큰 의미를 갖지 않는다라고 가정하고 시작하겠다.</code></p>
</li>
<li><p>우선, 형태소분석기를 어떤것을 사용하던 가능하게 함수를 하나 만들어준다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from khaiii import KhaiiiApi</span><br><span class="line">from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_tokenizer(tokenizer_name):</span><br><span class="line">    <span class="keyword">if</span> tokenizer_name == <span class="string">"komoran"</span>:</span><br><span class="line">        tokenizer = Komoran()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"okt"</span>:</span><br><span class="line">        tokenizer = Okt()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"mecab"</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"hannanum"</span>:</span><br><span class="line">        tokenizer = Hannanum()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"kkma"</span>:</span><br><span class="line">        tokenizer = Kkma()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"khaiii"</span>:</span><br><span class="line">        tokenizer = KhaiiiApi()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="built_in">return</span> tokenizer</span><br></pre></td></tr></table></figure>
<ul>
<li>한 문단 별로 구분자를 어떤것으로 했는지 확인하기 하나씩 프린트해보았다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">corpus_fname = <span class="string">"./data/processed/processed_blog.txt"</span></span><br><span class="line"></span><br><span class="line">with open(corpus_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"---------------------------------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"---------------------------------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"---------------------------------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br></pre></td></tr></table></figure>
<p><img src="/image/read_lines.png" alt="라인 하나씩 출력"></p>
<ul>
<li>아래 코드를 실행하면 제일 처음 문서의 임베딩과 코사인 유사도가 가장 높은 문서 임베딩의 제목을 return해준다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from sklearn.decomposition import TruncatedSVD</span><br><span class="line">from sklearn.preprocessing import normalize</span><br><span class="line">from sklearn.manifold import TSNE</span><br><span class="line">from sklearn.metrics.pairwise import cosine_similarity</span><br><span class="line"></span><br><span class="line">from bokeh.io import export_png, output_notebook, show</span><br><span class="line">from bokeh.plotting import figure</span><br><span class="line">from bokeh.models import Plot, Range1d, MultiLine, Circle, HoverTool, TapTool, BoxSelectTool, LinearColorMapper, ColumnDataSource, LabelSet, SaveTool, ColorBar, BasicTicker</span><br><span class="line">from bokeh.models.graphs import from_networkx, NodesAndLinkedEdges, EdgesAndLinkedNodes</span><br><span class="line">from bokeh.palettes import Spectral8</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def LSAeval(corpus_file, doc_idx, nth_top):</span><br><span class="line">    tokenizer =  get_tokenizer(<span class="string">"mecab"</span>)</span><br><span class="line">    titles, raw_corpus, noun_corpus = [], [], []</span><br><span class="line"></span><br><span class="line">    with open(corpus_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            try:</span><br><span class="line">                title, document = line.strip().split(<span class="string">'\u241E'</span>)</span><br><span class="line">                titles.append(title)</span><br><span class="line">                raw_corpus.append(document)</span><br><span class="line">                nouns = tokenizer.nouns(document)</span><br><span class="line">                noun_corpus.append(<span class="string">' '</span>.join(nouns))</span><br><span class="line">            except:</span><br><span class="line">                <span class="built_in">continue</span></span><br><span class="line">    <span class="comment"># 문서(단락)에서 기호들과 조사를 제외하고 명사들만 추출한 데이터 중 Unigram(ngram_range(1,1)),</span></span><br><span class="line">    <span class="comment"># DF가 1이상(min_df=1)인 데이터를 추려 TF-IDF 행렬을 만들 것이다.</span></span><br><span class="line">    vectorizer = TfidfVectorizer(min_df=1,</span><br><span class="line">                             ngram_range=(1,1),</span><br><span class="line">                             <span class="comment"># tokenizing전에 모든 문자를 소문자로 바꿔준다.</span></span><br><span class="line">                             lowercase=True,</span><br><span class="line">                             <span class="comment"># analyzer == 'word'인 경우만 사용가능.</span></span><br><span class="line">                             tokenizer=lambda x: x.split())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 행은 문서, 열은 단어에 각각 대응한다. (204 x 37153)</span></span><br><span class="line">    input_matrix = vectorizer.fit_transform(noun_corpus)</span><br><span class="line"></span><br><span class="line">    id2vocab = &#123;vectorizer.vocabulary_[token]:token <span class="keyword">for</span> token <span class="keyword">in</span> vectorizer.vocabulary_.keys()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># curr_doc : Corpus 첫번 째 문서의 TF-IDF 벡터</span></span><br><span class="line">    curr_doc, result = input_matrix[doc_idx], []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># curr_doc에서 TF-IDF 값이 0이 아닌 요소들은 내림차순 정렬</span></span><br><span class="line">    <span class="comment"># curr_doc은 105개의 원소(단어)만이 저장되어 있는 Compressed Sparse Row format이다.</span></span><br><span class="line">    <span class="comment"># 그러므로 indices(CSR format index array of the matrix)로 해당 index에 위치하는 단어와 그에대한 tf-idf값을 쌍으로 tuple형태로 넣어준다.</span></span><br><span class="line">    <span class="keyword">for</span> idx, el <span class="keyword">in</span> zip(curr_doc.indices, curr_doc.data):</span><br><span class="line">        result.append((id2vocab[idx], el))</span><br><span class="line"></span><br><span class="line">    sorted(result, key=lambda x : x[1], reverse=True)[:5]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 이번에는 이 TF-IDF 행렬에 100차원 SVD를 수행할 것이다. 204 x 37153의 희소 행렬을</span></span><br><span class="line">    <span class="comment"># 204 x 100 크기의 Dense Matrix로 linear Transforamtion하는 것이다.</span></span><br><span class="line">    svd =  TruncatedSVD(n_components=100)</span><br><span class="line">    vecs = svd.fit_transform(input_matrix)</span><br><span class="line">    svd_l2norm_vectors = normalize(vecs, axis=1, norm=<span class="string">'l2'</span>)</span><br><span class="line">    cosine_similarity = np.dot(svd_l2norm_vectors, svd_l2norm_vectors[doc_idx])</span><br><span class="line">    query_sentence = titles[doc_idx]</span><br><span class="line">    <span class="built_in">return</span> titles, svd_l2norm_vectors, [query_sentence, sorted(zip(titles, cosine_similarity), key=lambda x: x[1], reverse=True)[1:nth_top + 1]]</span><br></pre></td></tr></table></figure>
<p><img src="/image/noun_corpus.png" alt="임베딩 벡터를 만든 Corpus는 명사만 추출"></p>
<ul>
<li>상위 5개의 벡터의 내적이 높은 순으로 내림차순 정력했을때의 결과물 출력</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">titles, svd_l2norm_vectors, top_five = LSAeval(corpus_file=<span class="string">"./data/processed/processed_blog.txt"</span>, doc_idx=0, nth_top=5)</span><br><span class="line">top_five</span><br></pre></td></tr></table></figure>
<p><img src="/image/top_5_similarity.png" alt="첫번째 문서와의 높은 유사도를 갖는 상위 5개의 문서"></p>
<h3 id="임베딩-시각화"><a href="#임베딩-시각화" class="headerlink" title="임베딩 시각화"></a>임베딩 시각화</h3><ul>
<li>t-SNE 기법을 사용해서 벡터공간을 2차원으로 줄여준 뒤 시각화 할 것이다. 또한 벡터들간의 전체적인 유사도는 시각적으로 그리기보다는 상관행렬 방식으로 나타내 줄 것이다.</li>
</ul>
<ul>
<li><p>시각화에 필요한 함수들 정의</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def visualize(titles, vectors, mode=<span class="string">"between"</span>, num_sents=30, palette=<span class="string">"Viridis256"</span>, use_notebook=False):</span><br><span class="line">        doc_idxes = random.sample(range(len(titles)), num_sents)</span><br><span class="line">        sentences = [titles[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> doc_idxes]</span><br><span class="line">        vecs = [vectors[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> doc_idxes]</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">"between"</span>:</span><br><span class="line">            visualize_between_sentences(sentences, vecs, palette, use_notebook=use_notebook)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            visualize_sentences(vecs, sentences, palette, use_notebook=use_notebook)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def visualize_between_sentences(sentences, vec_list, palette=<span class="string">"Viridis256"</span>,</span><br><span class="line">                                filename=<span class="string">"between-sentences.png"</span>,</span><br><span class="line">                                use_notebook=False):</span><br><span class="line">    df_list, score_list = [], []</span><br><span class="line">    <span class="keyword">for</span> sent1_idx, sentence1 <span class="keyword">in</span> enumerate(sentences):</span><br><span class="line">        <span class="keyword">for</span> sent2_idx, sentence2 <span class="keyword">in</span> enumerate(sentences):</span><br><span class="line">            vec1, vec2 = vec_list[sent1_idx], vec_list[sent2_idx]</span><br><span class="line">            <span class="keyword">if</span> np.any(vec1) and np.any(vec2):</span><br><span class="line">                score = cosine_similarity(X=[vec1], Y=[vec2])</span><br><span class="line">                <span class="comment"># [0][0]인 이유는 값만 뽑아 내기 위해서이다.</span></span><br><span class="line">                df_list.append(&#123;<span class="string">'x'</span>: sentence1, <span class="string">'y'</span>: sentence2, <span class="string">'similarity'</span>: score[0][0]&#125;)</span><br><span class="line">                score_list.append(score[0][0])</span><br><span class="line">    df = pd.DataFrame(df_list)</span><br><span class="line">    color_mapper = LinearColorMapper(palette=palette, low=np.max(score_list), high=np.min(score_list))</span><br><span class="line">    TOOLS = <span class="string">"hover,save,pan,box_zoom,reset,wheel_zoom"</span></span><br><span class="line">    p = figure(x_range=sentences, y_range=list(reversed(sentences)),</span><br><span class="line">                x_axis_location=<span class="string">"above"</span>, plot_width=900, plot_height=900,</span><br><span class="line">                toolbar_location=<span class="string">'below'</span>, tools=TOOLS,</span><br><span class="line">                tooltips=[(<span class="string">'sentences'</span>, <span class="string">'@x @y'</span>), (<span class="string">'similarity'</span>, <span class="string">'@similarity'</span>)])</span><br><span class="line">    p.grid.grid_line_color = None</span><br><span class="line">    p.axis.axis_line_color = None</span><br><span class="line">    p.axis.major_tick_line_color = None</span><br><span class="line">    p.axis.major_label_standoff = 0</span><br><span class="line">    p.xaxis.major_label_orientation = 3.14 / 3</span><br><span class="line">    p.rect(x=<span class="string">"x"</span>, y=<span class="string">"y"</span>, width=1, height=1,</span><br><span class="line">            <span class="built_in">source</span>=df,</span><br><span class="line">            fill_color=&#123;<span class="string">'field'</span>: <span class="string">'similarity'</span>, <span class="string">'transform'</span>: color_mapper&#125;,</span><br><span class="line">            line_color=None)</span><br><span class="line">    color_bar = ColorBar(ticker=BasicTicker(desired_num_ticks=5),</span><br><span class="line">                        color_mapper=color_mapper, major_label_text_font_size=<span class="string">"7pt"</span>,</span><br><span class="line">                        label_standoff=6, border_line_color=None, location=(0, 0))</span><br><span class="line">    p.add_layout(color_bar, <span class="string">'right'</span>)</span><br><span class="line">    <span class="keyword">if</span> use_notebook:</span><br><span class="line">        output_notebook()</span><br><span class="line">        show(p)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        export_png(p, filename)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"save @ "</span> + filename)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def visualize_sentences(vecs, sentences, palette=<span class="string">"Viridis256"</span>, filename=<span class="string">"/notebooks/embedding/sentences.png"</span>,</span><br><span class="line">                        use_notebook=False):</span><br><span class="line">    tsne = TSNE(n_components=2)</span><br><span class="line">    tsne_results = tsne.fit_transform(vecs)</span><br><span class="line">    df = pd.DataFrame(columns=[<span class="string">'x'</span>, <span class="string">'y'</span>, <span class="string">'sentence'</span>])</span><br><span class="line">    df[<span class="string">'x'</span>], df[<span class="string">'y'</span>], df[<span class="string">'sentence'</span>] = tsne_results[:, 0], tsne_results[:, 1], sentences</span><br><span class="line">    <span class="built_in">source</span> = ColumnDataSource(ColumnDataSource.from_df(df))</span><br><span class="line">    labels = LabelSet(x=<span class="string">"x"</span>, y=<span class="string">"y"</span>, text=<span class="string">"sentence"</span>, y_offset=8,</span><br><span class="line">                      text_font_size=<span class="string">"12pt"</span>, text_color=<span class="string">"#555555"</span>,</span><br><span class="line">                      <span class="built_in">source</span>=<span class="built_in">source</span>, text_align=<span class="string">'center'</span>)</span><br><span class="line">    color_mapper = LinearColorMapper(palette=palette, low=min(tsne_results[:, 1]), high=max(tsne_results[:, 1]))</span><br><span class="line">    plot = figure(plot_width=900, plot_height=900)</span><br><span class="line">    plot.scatter(<span class="string">"x"</span>, <span class="string">"y"</span>, size=12, <span class="built_in">source</span>=<span class="built_in">source</span>, color=&#123;<span class="string">'field'</span>: <span class="string">'y'</span>, <span class="string">'transform'</span>: color_mapper&#125;, line_color=None, fill_alpha=0.8)</span><br><span class="line">    plot.add_layout(labels)</span><br><span class="line">    <span class="keyword">if</span> use_notebook:</span><br><span class="line">        output_notebook()</span><br><span class="line">        show(plot)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        export_png(plot, filename)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"save @ "</span> + filename)</span><br></pre></td></tr></table></figure>
</li>
<li><p>혹시 이러한 error가 난다면, 다음과 같이 PhantomJS를 설치한다. 간단히 말하자면 PhantomJS도 Selenium같이 웹브라우져 개발용으로 만들어진 프로그램이다. bokeh는 javascript기반으로 짜여져있어서 필요한 것 같다.</p>
</li>
</ul>
<p><img src="/image/phantomjs_error.png" alt="phantomjs error"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge phantomjs</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visualize(titles, svd_l2norm_vectors, mode=<span class="string">"between"</span>, num_sents=30, palette=<span class="string">"Viridis256"</span>, use_notebook=True)</span><br></pre></td></tr></table></figure>
<p><img src="/image/sentence_between_cosine_simularity.png" alt="벡터들간의 유성성 상관행렬"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visualize(titles, svd_l2norm_vectors, mode=<span class="string">"tsne"</span>, num_sents=30, palette=<span class="string">"Viridis256"</span>, use_notebook=True)</span><br></pre></td></tr></table></figure>
<p><img src="/image/t_SNE_plot_bog_source.png" alt="t-SNE를 활용한 임베딩 벡터 시각화"></p>
<h2 id="Doc2Vec"><a href="#Doc2Vec" class="headerlink" title="Doc2Vec"></a>Doc2Vec</h2><h4 id="모델-개요"><a href="#모델-개요" class="headerlink" title="모델 개요"></a>모델 개요</h4><ul>
<li><p>Word2Vec에 이어 구글 연구 팀이 개발한 문서 임베딩 기법이다. <code>이전 단어 sequence k개가 주어졌을 때 그 다음 단어를 맞추는 언어 모델</code>을 만들었다. <code>이 모델은 문장 전체를 처음부터 끝까지 한 단어씩 슬라이딩해 가면서 다음 단어가 무엇일지 예측한다.</code></p>
</li>
<li><p>로그 확률 평균의 값이 커진다는 의미는 이전 k개 단어들을 입력하면 모델이 다음 단어를 잘 예측하므로 로그 확률 평균을 최대화는 과정에서 학습된다.</p>
</li>
<li><p>NPLM에서 설명했던 방식처럼 문장 전체를 한 단어씩 슬라이딩해가면서 다음 target word를 맞추는 과정에서 context word에 해당하는 $(w_{t-k}, \cdots, w_{t-1})$에 해당하는 W 행렬의 벡터들이 업데이트 한다. <code>따라서 주변 이웃 단어 집합 즉 context가 유사한 단어벡터는 벡터 공간에 가깝게 임베딩 된다.</code>학습이 종료되면 W를 각 단어의 임베딩으로 사용한다.</p>
</li>
</ul>
<h5 id="Doc2vec-언어-모델"><a href="#Doc2vec-언어-모델" class="headerlink" title="Doc2vec 언어 모델"></a>Doc2vec 언어 모델</h5><ul>
<li>$T$ : 학습 데이터 문장 하나의 단어 개수</li>
<li>$w_{t}$ : 문장의 t번째 단어</li>
<li>$y_{i}$ : corpus 전체 어휘 집합 중 i번째 단어에 해당하는 점수<ul>
<li>1) 이전 k개 단어들을 W라는 단어 행렬에서 참조한 뒤 평균을 취하거나 이어 붙인다. 여기에 U라는 행렬을 내적하고 bias 벡터인 b를 더해준 뒤 softmax를 취한다. U의 크기는 어휘집합 크기 $\times$ 임베딩 차원 수 이다.</li>
</ul>
</li>
<li>$h$ : 벡터 sequence가 주어졌을 때 평균을 취하거나 concatenate하여 고정된 길이의 벡터 하나를 반환하는 역할을 하는 함수이다.</li>
</ul>
<script type="math/tex; mode=display">L = frac{1}{T} \sum^{T-k}-{t=k} log(w_{t}|w_{t-k}, \cdots, w_{t-1})</script><h5 id="Doc2Vec-언어-모델-Score-계산"><a href="#Doc2Vec-언어-모델-Score-계산" class="headerlink" title="Doc2Vec 언어 모델 Score 계산"></a>Doc2Vec 언어 모델 Score 계산</h5><script type="math/tex; mode=display">P(w_{t}| w_{t-k}, \cdots, w_{t-1}) = \frac{exp(y_{w_{t}})}{\sum_{i} exp(y_{i})}</script><script type="math/tex; mode=display">y = b + U \cdot h(w_{t-k}, \cdots, w_{t-1}; W)</script><ul>
<li>위의 초기 구조에서 <code>문서 id를 추가해 이전 k개 단어들과 문서 id를 넣어서 다음 단어를 예측</code>하게 했다. <code>y를 계산할 때 D라는 문서 행렬(Paragraph matrix)에서 해당 문서 ID에 해당하는 벡터를 참조해 h 함수에 다른 단어 벡터들과 함께 입력하는 것 외에 나머지 과정은 동일</code>하다. 이런 구조를 <code>PV-DM(the Distributed Memory Model of Paragraph Vectors)</code>이라고 부른다. 학습이 종료되면 문서 수 $\times$ 임베딩 차원 수 크기를 가지는 문서 행렬 D를 각 문서의 임베딩으로 사용한다. 이렇게 만든 <code>문서 임베딩이 해당 문서의 주제 정보를 함축한다고 설명</code>한다. <code>PV-DM은 단어 등장 순서를 고려하는 방식으로 학습하기 때문에 순서 정보를 무시하는 Bag of Words 기법 대비 강점이 있다고 할 수 있을 것</code>이다.</li>
</ul>
<p><img src="/image/Doc2vec_model_principal.png" alt="Doc2vec 모델"></p>
<ul>
<li>또한, Word2Vec의 Skip-gram을 본뜬 <code>PV-DBOW</code>(the Distributed Bag of Words version of Paragraph Vectors)도 제안했다. Skip-gram은 target word를 가지고 context word들을 예측하는 과정에서 학습되었다. PV-DBOW도 문서 id를 가지고 context word들을 맞춘다. 따라서 <code>문서 id에 해당하는 문서 임베딩엔 문서에 등장하는 모든 단어의 의미 정보가 반영</code>된다.</li>
</ul>
<p><img src="/image/Doc2Vec_DBOW_model.png" alt="Doc2Vec CBOW 모델"></p>
<h2 id="Doc2Vec-실습"><a href="#Doc2Vec-실습" class="headerlink" title="Doc2Vec 실습"></a>Doc2Vec 실습</h2><h3 id="실습-데이터"><a href="#실습-데이터" class="headerlink" title="실습 데이터"></a>실습 데이터</h3><ul>
<li>영화 댓글과 해당 영화의 ID가 라인 하나를 구성하고 있다. 영화하나를 문서로 보고 Doc2Vec 모델을 학습할 예정이다. 따라서 영화 ID가 동일한 문장들을 하나의 문서로 처리해 줄 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">with open(<span class="string">"./data/processed/processed_review_movieid.txt"</span>) as f:</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    count = 4</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> f:</span><br><span class="line">        count+=1</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"총 sentence의 개수 : &#123;&#125;개"</span>.format(count))</span><br></pre></td></tr></table></figure>
<p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">종합 평점은 4점 드립니다.␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">원작이 칭송받는 이유는 웹툰 계 자체의 질적 저하가 심각하기 때문.  원작이나 영화나 별로인건 마찬가지.␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">나름의  감동도 있고 안타까운 마음에 가슴도 먹먹  배우들의 연기가 good 김수현 최고~␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">이런걸 돈주고 본 내자신이 후회스럽다 최악의 쓰레기 영화 김수현 밖에없는 저질 삼류영화␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">총 sentence의 개수 : 712532개</span><br></pre></td></tr></table></figure></p>
<ul>
<li>Doc2Vec 모델 학습을 위해 Python gensim 라이브러리의 Doc2Vec 클래스를 사용하는데 Doc2VecInput은 이 클래스가 요구하는 입력 형태를 맞춰주는 역할을 한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">from khaiii import KhaiiiApi</span><br><span class="line">from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma</span><br><span class="line">from gensim.models.doc2vec import TaggedDocument</span><br><span class="line"></span><br><span class="line">def get_tokenizer(tokenizer_name):</span><br><span class="line">    <span class="keyword">if</span> tokenizer_name == <span class="string">"komoran"</span>:</span><br><span class="line">        tokenizer = Komoran()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"okt"</span>:</span><br><span class="line">        tokenizer = Okt()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"mecab"</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"hannanum"</span>:</span><br><span class="line">        tokenizer = Hannanum()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"kkma"</span>:</span><br><span class="line">        tokenizer = Kkma()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"khaiii"</span>:</span><br><span class="line">        tokenizer = KhaiiiApi()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="built_in">return</span> tokenizer</span><br><span class="line"></span><br><span class="line">class Doc2VecInput:</span><br><span class="line"></span><br><span class="line">    def __init__(self, fname, tokenizer_name=<span class="string">'mecab'</span>):</span><br><span class="line">        self.fname = fname</span><br><span class="line">        self.tokenizer = get_tokenizer(tokenizer_name)</span><br><span class="line"></span><br><span class="line">    def __iter__(self):</span><br><span class="line">        with open(self.fname, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                try:</span><br><span class="line">                    sentence, movie_id = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                    tokens = self.tokenizer.morphs(sentence)</span><br><span class="line">                    tagged_doc = TaggedDocument(words=tokens, tags=[<span class="string">'movie_%s'</span> % movie_id])</span><br><span class="line">                    yield tagged_doc</span><br><span class="line">                except:</span><br><span class="line">                    <span class="built_in">continue</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>dm : 1 (default) -&gt; PV-DM, 0- &gt; PV-DBOW</code></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import Doc2Vec</span><br><span class="line"></span><br><span class="line">corpus_fname = <span class="string">'./data/processed/processed_review_movieid.txt'</span></span><br><span class="line">output_fname = <span class="string">'./doc2vec.model'</span></span><br><span class="line"></span><br><span class="line">corpus = Doc2VecInput(corpus_fname)</span><br><span class="line">model = Doc2Vec(corpus, dm=1, vector_size=100)</span><br><span class="line">model.save(output_fname)</span><br></pre></td></tr></table></figure>
<ul>
<li>학습이 잘 되었는지를 평가하기 위해서 아래와 같이 평가 클래스를 이용할 것이며, 평가를 하면 tag된 영화 id가 나올텐데 직관적으로 그 영화가 어떤 영화인지 모를 것이다. 그러므로 직관적으로 결과를 이해하기 위해 학습 데이터에는 없는 영화 제목을 네이버 영화 사이트에 접속해 id에 맞는 영화 제목을 스크래핑해 올 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from lxml import html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Doc2VecEvaluator:</span><br><span class="line"></span><br><span class="line">    def __init__(self, model_fname=<span class="string">"data/doc2vec.vecs"</span>, use_notebook=False):</span><br><span class="line">        self.model = Doc2Vec.load(model_fname)</span><br><span class="line">        self.doc2idx = &#123;el:idx <span class="keyword">for</span> idx, el <span class="keyword">in</span> enumerate(self.model.docvecs.doctags.keys())&#125;</span><br><span class="line">        self.use_notebook = use_notebook</span><br><span class="line"></span><br><span class="line">    def most_similar(self, movie_id, topn=10):</span><br><span class="line">        similar_movies = self.model.docvecs.most_similar(<span class="string">'movie_'</span> + str(movie_id), topn=topn)</span><br><span class="line">        <span class="keyword">for</span> movie_id, score <span class="keyword">in</span> similar_movies:</span><br><span class="line">            <span class="built_in">print</span>(self.get_movie_title(movie_id), score)</span><br><span class="line"></span><br><span class="line">    def get_titles_in_corpus(self, n_sample=5):</span><br><span class="line">        movie_ids = random.sample(self.model.docvecs.doctags.keys(), n_sample)</span><br><span class="line">        <span class="built_in">return</span> &#123;movie_id: self.get_movie_title(movie_id) <span class="keyword">for</span> movie_id <span class="keyword">in</span> movie_ids&#125;</span><br><span class="line"></span><br><span class="line">    def get_movie_title(self, movie_id):</span><br><span class="line">        url = <span class="string">'http://movie.naver.com/movie/point/af/list.nhn?st=mcode&amp;target=after&amp;sword=%s'</span> % movie_id.split(<span class="string">"_"</span>)[1]</span><br><span class="line">        resp = requests.get(url)</span><br><span class="line">        root = html.fromstring(resp.text)</span><br><span class="line">        try:</span><br><span class="line">            title = root.xpath(<span class="string">'//div[@class="choice_movie_info"]//h5//a/text()'</span>)[0]</span><br><span class="line">        except:</span><br><span class="line">            title = <span class="string">""</span></span><br><span class="line">        <span class="built_in">return</span> title</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_fname=<span class="string">'./doc2vec.model'</span></span><br><span class="line">model = Doc2VecEvaluator(model_fname)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"영화의 종류 : &#123;&#125; 개"</span>.format(len(model.doc2idx.keys())))</span><br></pre></td></tr></table></figure>
<p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">영화의 종류 : 14730 개</span><br></pre></td></tr></table></figure></p>
<ul>
<li>학습 데이터에 포함된 아무 영화 10개의 제목을 보여준다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.get_titles_in_corpus(n_sample=14730)</span><br></pre></td></tr></table></figure>
<p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'movie_89743'</span>: <span class="string">'여자가 두 번 화장할 때'</span>,</span><br><span class="line"> <span class="string">'movie_16490'</span>: <span class="string">'투 문 정션 2'</span>,</span><br><span class="line"> <span class="string">'movie_100953'</span>: <span class="string">'더 퍼지'</span>,</span><br><span class="line"> <span class="string">'movie_84375'</span>: <span class="string">'퍼펙트 센스'</span>,</span><br><span class="line"> <span class="string">'movie_24203'</span>: <span class="string">'섀터드 이미지'</span>,</span><br><span class="line"> <span class="string">'movie_12054'</span>: <span class="string">'나폴레옹'</span>,</span><br><span class="line"> <span class="string">'movie_10721'</span>: <span class="string">'더티 해리'</span>,</span><br><span class="line"> <span class="string">'movie_11440'</span>: <span class="string">'킹 뉴욕'</span>,</span><br><span class="line"> <span class="string">'movie_20896'</span>: <span class="string">'미망인'</span>,</span><br><span class="line"> <span class="string">'movie_123068'</span>: <span class="string">'캠걸'</span>&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>해당 id와 유사한 영화 상위 5개를 보여준다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.most_similar(37758, topn=5)</span><br></pre></td></tr></table></figure>
<p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">돈비 어프레이드-어둠 속의 속삭임 0.746237576007843</span><br><span class="line">더 퍼지:거리의 반란 0.7402248382568359</span><br><span class="line">고양이: 죽음을 보는 두 개의 눈 0.6967850923538208</span><br><span class="line">ATM 0.690518856048584</span><br><span class="line">힛쳐 0.6751468777656555</span><br></pre></td></tr></table></figure></p>
<h2 id="잠재-디리클레-할당-LDA-Latent-Dirichlet-Allocation"><a href="#잠재-디리클레-할당-LDA-Latent-Dirichlet-Allocation" class="headerlink" title="잠재 디리클레 할당(LDA, Latent Dirichlet Allocation)"></a>잠재 디리클레 할당(LDA, Latent Dirichlet Allocation)</h2><ul>
<li><code>주어진 문서에 대하여 각 문서에 어떤 topic들이 존재하는지에 대한 확률 모형</code>이다. corpus의 이면에 잠재된 topic을 추출한다는 의미에서 topic modeling이라고 부르기도 한다. <code>문서를 topic 확률 분포로 나타내 각각을 벡터화한다는 점에서 LDA를 임베딩 기법의 일종으로 이해</code>할 수 있다.</li>
</ul>
<h4 id="모델-개요-1"><a href="#모델-개요-1" class="headerlink" title="모델 개요"></a>모델 개요</h4><ul>
<li>LDA는 topic별 단어의 분포, 문서별 topic의 분포를 모두 추정해 낸다.</li>
</ul>
<p><img src="/image/LDA_brief_out.png" alt="LDA의 개략적 도식"></p>
<ul>
<li><p>LDA는 topic에 특정 단어가 나타날 확률을 내어 준다. 위의 그림에서 각 색깔별로 topic에 따른 단어가 등장할 확률을 보여주고있다. 문서를 보면 노란색 topic에 해당하는 단어가 많기 때문에 위 문서의 메인 주제는 노란색 topic인 유전자 관련 topic일 가능성이 클 것이다. 이렇듯 문서의 topic 비중 또한 LDA의 산출 결과가 된다.</p>
</li>
<li><p><code>LDA가 가정하는 문서 생성 과정</code>은 우리가 글을 쓸때와 같다. 실제 글을 작성할 때는 글감 내지 주제를 먼저 결정한 후 어떤 단어를 써야 할지 결정한다. 이와 마찬가지로 LDA는 <code>우선 corpus로 부터 얻은 topic 분포로부터 topic을 뽑는다. 이후 해당 topic에 해당하는 단어들을 뽑는다.</code> 그런데 corpus에 등장하는 단어들 각각에 꼬리표가 달려 있는 것은 아니기 때문에 현재 문서에 등장한 단어들은 어떤 topic에서 뽑힌 단어들인지 우리가 명시적으로 알기는 어려울 것이다. <code>하지만, LDA는 이런 corpus 이면에 존재하는 정보를 추론해낼 수 있다.</code></p>
</li>
</ul>
<h4 id="아키텍처"><a href="#아키텍처" class="headerlink" title="아키텍처"></a>아키텍처</h4><ul>
<li>LDA가 가정하는 문서 생성 과정은 아래의 그림과 같다.<ul>
<li>D : corpus 전체 문서 개수</li>
<li>K : 전체 topic 수(hyper parameter)</li>
<li>N : d번째 문서의 단어 수</li>
<li>네모칸 : 해당 횟수만큼 반복하라는 의미</li>
<li>$\phi_{k}$ : k번째 topic에 해당하는 벡터 $\phi_{k} \in R^{|V|}$, $\phi_{k}$는 word-topic 행렬의 k번째 열을 의미한다. $\phi_{k}$의 각 요소 값은 해당 단어가 k번째 토픽에서 차지하는 비중을 의미하며 확률값이므로 이 벡터의 요소값의 합은 1이 된다. 이런 topic의 단어비중을 의미하는 $\phi_{k}$는 디리클레 분포를 따른 다는 가정을 취하므로 $\beta$의 영향을 받는다.</li>
<li>$\theta_{d}$ : d번째 문서가 가진 topic 비중을 나타내는 벡터이다. 그러므로 전체 topic의 개수만큼의 길이 갖으며, 각 벡터는 확률을 의미하므로 합은 1이된다. 문서의 topic 비중 $\theta_{d}$는 디리클레 분포를 따른다는 가정을 취하므로 $\alpha$의 영향을 받는다.</li>
<li>$\z_{d,n}$ : d번째 문서에서 n번째 단어가 어떤 topic인지를 나타내는 변수이다. 그러므로 이 변수는 d번째 문서의 topic 확률 분포인 $\theta_{d}$에 영향을 받는다.</li>
<li>동그라미 : 변수를 의미</li>
<li>화살표가 시작되는 변수는 조건, 화살표가 향하는 변수는 결과에 해당하는 변수</li>
</ul>
</li>
</ul>
<ul>
<li><code>관찰 가능한 변수는 d번째 문서에 등장한 n번째 단어 $w_{d,n}$이 유일</code>하다. 우리는 이 정보만을 가지고 하이퍼파라메터(사용자 지정) α,β를 제외한 모든 잠재 변수를 추정해야 한다.</li>
</ul>
<p><img src="/image/graphical_model_LDA.png" alt="graphical model로 표현한 LDA"></p>
<ul>
<li>예시를 들자면, 아래 Document-topic 행렬을 살펴보자. 3번째 문서에 속한 단어들은 가장 높은 확률값 0.625를 갖는 topic-2일 가능성이 높다. $w_{d,n}$은 d번째 문서 내에 n번째로 등장하는 단어를 의미하며, 동시에 우리가 유일하게 corpus에서 관찰할 수 있는 데이터이다. 이는 $\theta_{d}$와 $\z_{d,n}$에 동시에 영향을 받는다. 예를 들면, $z_{3,1}$가 topic-2이라고 가정했을 경우, $w_{3,1}$은 word-topic 행렬에서 보게되면 제일 높은 확률값 0.393을 갖는 ‘코로나 바이러스’일 가능성이 높다.</li>
</ul>
<h4 id="이처럼-LDA는-topic의-word-분포-phi-와-문서의-topic-분포-theta-의-결합으로-문서-내-단어들이-생성된다고-가정한다"><a href="#이처럼-LDA는-topic의-word-분포-phi-와-문서의-topic-분포-theta-의-결합으로-문서-내-단어들이-생성된다고-가정한다" class="headerlink" title="이처럼 LDA는 topic의 word 분포($\phi$)와 문서의 topic 분포($\theta$)의 결합으로 문서 내 단어들이 생성된다고 가정한다."></a>이처럼 LDA는 topic의 word 분포($\phi$)와 문서의 topic 분포($\theta$)의 결합으로 문서 내 단어들이 생성된다고 가정한다.</h4><p>Document-topic 행렬<br>| 문서  | topic-1 | topic-2 | topic-3 |<br>|———-|————-|————-|————-|<br>| 문서1 | 0.400   | 0.000   | 0.600   |<br>| 문서2 | 0.000   | 0.600   | 0.400   |<br>| 문서3 | 0.375   | 0.625   | 0.000   |</p>
<p>word-topic 행렬<br>| 단어              | topic-1 | topic-2 | topic-3 |<br>|—————————-|————-|————-|————-|<br>| 코로나 바이러스   | 0.000   | 0.393   | 0.000   |<br>| 우한폐렴              | 0.000   | 0.313   | 0.000   |<br>| AWS               | 0.119   | 0.000   | 0.000   |<br>| 데이터 엔지니어링 | 0.181   | 0.000   | 0.000   |<br>| Hadoop            | 0.276   | 0.000   | 0.000   |<br>| Spark             | 0.142   | 0.000   | 0.000   |<br>| 낭만 닥터 김사부2 | 0.000   | 0.012   | 0.468   |<br>| tensorflow        | 0.282   | 0.000   | 0.000   |<br>| 마스크            | 0.000   | 0.282   | 0.000   |<br>| 사랑의 불시착     | 0.000   | 0.000   | 0.532   |<br>| 합                | 1.0     | 1.0     | 1.0     |</p>
<ul>
<li>실제 관찰 가능한 corpus를 가지고 알고 싶은 topic의 word 분포, 문서의 topic 분포를 추정하는 과정을 통해 LDA는 학습한다. 즉, <code>topic의 word 분포와 문서의 topic 분포의 결합 확률이 커지는 방향으로 학습을 한다는 의미</code>이다.</li>
</ul>
<h5 id="LDA의-단어-생성-과정"><a href="#LDA의-단어-생성-과정" class="headerlink" title="LDA의 단어 생성 과정"></a>LDA의 단어 생성 과정</h5><script type="math/tex; mode=display">p($\phi_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D}) =</script><script type="math/tex; mode=display">\prod^{K}_{i=1} p(\phi_{i}|\beta) \prod^{D}_{d=1} p(\theta_{d}|\alpha) \left(\prod^{N}_{n=1} p(z_{d,n}|\theta_{d}) p(w_{d,n}|\phi_{1:K}, z_{d, n}) \right)</script><ul>
<li>우리가 구해야할 사후확률 분포는 $ p(z, \phi, \theta|w) = p(z, \phi, \theta, w)/p(w)$를 최대로 만드는 $ z, \phi, \theta$를 찾아야 한다. 이 사후확률을 직접 계산하려면 분자도 계산하기 어렵겠지만 분모가 되는 $ p(w)$도 반드시 구해야 확률값으로 만들어 줄 수 있다. $ p(w)$는 잠재변수 $ z, \phi, \theta$의 모든 경우의 수를 고려한 각 단어(w)의 등장 확률을 의미하는데, $ z, \phi, \theta$를 직접 관찰하는 것은 불가능하다. 이러한 이유로 <code>깁스 샘플링(gibbs sampling)</code>같은 표본 추출 기법을 사용해 사후확률을 근사시키게 된다. 깁스 샘플링이란 <code>나머지 변수는 고정시킨 채 하나의 랜덤변수만을 대상으로 표본을 뽑는 기법</code>이다. LDA에서는 사후확률 분포 $ p(z, \phi, \theta|w)$를 구할 때 topic의 단어 분포($\phi$)와 문서의 topic 분포($\theta$)를 계산에서 생략하고 topic(z)만을 추론한다. <code>z만 알 수 있으면 나미저 변수를 이를 통해 계산 할 수 있도록 설계 했기 때문이다.</code></li>
</ul>
<h5 id="깁스-샘플링을-활용한-LDA"><a href="#깁스-샘플링을-활용한-LDA" class="headerlink" title="깁스 샘플링을 활용한 LDA"></a>깁스 샘플링을 활용한 LDA</h5><script type="math/tex; mode=display">p(z_{d, i} = j|z_{-i}, w) = \frac{ n_{d,k} + \alpha_{j} }{ \sum^{K}_{i=1} (n_{d,i}) + \alpha_{i} } \times \frac{ v_{k, w_{d,n} } + \beta_{w_{d,n}} }{ \sum^{V}_{ㅓ=1} ( v_{k,j} + \beta_{j} ) } = AB</script><h6 id="LDA-변수-표기법"><a href="#LDA-변수-표기법" class="headerlink" title="LDA 변수 표기법"></a>LDA 변수 표기법</h6><div class="table-container">
<table>
<thead>
<tr>
<th>표기</th>
<th>내용</th>
</tr>
</thead>
<tbody>
<tr>
<td>$n_{d,k}$</td>
<td>k번째 topic에 할당된 d번째 문서의 빈도</td>
</tr>
<tr>
<td>$v_{k,w_{d,n}}$</td>
<td>전체 corpus에서 k번째 topic에 할당된 단어 $w_{d,n}$의 빈도</td>
</tr>
<tr>
<td>$w_{d,n}$</td>
<td>d번째 문서에 n번째로 등장한 단어</td>
</tr>
<tr>
<td>$\alpha$</td>
<td>문서의 topic 분포 생성을 위한 디리클레 분포 파라미터</td>
</tr>
<tr>
<td>$\beta$</td>
<td>topic의 word 분포 생성을 위한 디리클레 분포 파라미터</td>
</tr>
<tr>
<td>K</td>
<td>사용자가 지정하는 topic 개수</td>
</tr>
<tr>
<td>V</td>
<td>corpus에 등장하는 전체 word의 수</td>
</tr>
<tr>
<td>A</td>
<td>d번째 문서가 k번째 topic과 맺고 있는 연관성 정도</td>
</tr>
<tr>
<td>B</td>
<td>d번째 문서의 n번째 단어($w_{d,n}$)가 k번째 topic과 맺고 있는 연관성 정도</td>
</tr>
</tbody>
</table>
</div>

        </div>
        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "NLP 문장 수준 임베딩",
        "image": "https://heung-bae-lee.github.io/image/read_lines.png",
        "keywords": "",
        "genre": "NLP",
        "datePublished": "2020-02-06",
        "dateCreated": "2020-02-06",
        "dateModified": "2020-02-07",
        "url": "https://heung-bae-lee.github.io/2020/02/06/NLP_08/",
        "description": "참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.문장 수준 임베딩
크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다.

행렬 분해

LSA(잠재 의미 분석)


확률 모형

LDA(잠재 디리클레 할당)


Neural Network

Doc2Vec
ELMo
GPT (tran"
        "wordCount": 4981
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
    
        <a href="/2020/02/01/NLP_06/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">NLP - 단어 수준 임베딩</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/06/NLP_08/" class="title">NLP 문장 수준 임베딩</a></p>
                            <p class="item-date"><time datetime="2020-02-05T15:32:51.000Z" itemprop="datePublished">2020-02-06</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/01/NLP_06/" class="title">NLP - 단어 수준 임베딩</a></p>
                            <p class="item-date"><time datetime="2020-02-01T11:47:03.000Z" itemprop="datePublished">2020-02-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/01/NLP_05/" class="title">NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03</a></p>
                            <p class="item-date"><time datetime="2020-02-01T07:57:48.000Z" itemprop="datePublished">2020-02-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/01/30/NLP_04/" class="title">NLP 실습 텍스트 분류(TF-IDF, CountVectorizer, Word2Vec) -02</a></p>
                            <p class="item-date"><time datetime="2020-01-29T15:13:48.000Z" itemprop="datePublished">2020-01-30</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/01/29/NLP_03/" class="title">NLP 실습 텍스트 분류 -01</a></p>
                            <p class="item-date"><time datetime="2020-01-29T14:40:09.000Z" itemprop="datePublished">2020-01-29</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">5</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/02/06/NLP_08/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
