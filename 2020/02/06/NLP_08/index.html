<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>NLP 문장 수준 임베딩 - 01 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.문장 수준 임베딩 크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다.  행렬 분해  LSA(잠재 의미 분석)   확률 모형  LDA(잠재 디리클레 할당)   Neural Network  Doc2Vec ELMo GPT (tran">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP 문장 수준 임베딩 - 01">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/02/06/NLP_08/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.문장 수준 임베딩 크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다.  행렬 분해  LSA(잠재 의미 분석)   확률 모형  LDA(잠재 디리클레 할당)   Neural Network  Doc2Vec ELMo GPT (tran">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/read_lines.png">
<meta property="og:updated_time" content="2020-02-07T19:58:36.698Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP 문장 수준 임베딩 - 01">
<meta name="twitter:description" content="참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.문장 수준 임베딩 크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다.  행렬 분해  LSA(잠재 의미 분석)   확률 모형  LDA(잠재 디리클레 할당)   Neural Network  Doc2Vec ELMo GPT (tran">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/read_lines.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/02/06/nlp_08/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">
    <link rel="stylesheet" type="text/css" href>
    <link rel="stylesheet" href="https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css">	
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothiccoding.css">
    <link rel="stylesheet" href="/css/style.css">
   
    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/NLP/">NLP</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-NLP_08" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        NLP 문장 수준 임베딩 - 01
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/02/06/NLP_08/" class="article-date">
            <time datetime="2020-02-05T15:32:51.000Z" itemprop="datePublished">2020-02-06</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h4 id="참고로-이-모든-내용은-이기창-님의-한국어-임베딩이라는-책을-기반으로-작성하고-있다"><a href="#참고로-이-모든-내용은-이기창-님의-한국어-임베딩이라는-책을-기반으로-작성하고-있다" class="headerlink" title="참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다."></a>참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.</h4><h1 id="문장-수준-임베딩"><a href="#문장-수준-임베딩" class="headerlink" title="문장 수준 임베딩"></a>문장 수준 임베딩</h1><ul>
<li><p>크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다.</p>
</li>
<li><p>행렬 분해</p>
<ul>
<li>LSA(잠재 의미 분석)</li>
</ul>
</li>
<li><p>확률 모형</p>
<ul>
<li>LDA(잠재 디리클레 할당)</li>
</ul>
</li>
<li><p>Neural Network</p>
<ul>
<li>Doc2Vec</li>
<li>ELMo</li>
<li>GPT (transformer 구조 - self-attention)</li>
<li>BERT (transformer 구조 - self-attention)</li>
</ul>
</li>
</ul>
<h2 id="잠재-의미-분석-LSA-Latent-Semantic-Analysis"><a href="#잠재-의미-분석-LSA-Latent-Semantic-Analysis" class="headerlink" title="잠재 의미 분석(LSA, Latent Semantic Analysis)"></a>잠재 의미 분석(LSA, Latent Semantic Analysis)</h2><ul>
<li><p>단어 수준 임베딩에서의 LSA 방법론들은 word-documents 행렬이나 TF-IDF 행렬, word-context 행렬 또는 PMI 행렬에 SVD로 차원 축소를 시행하고, 여기에서 단어에 해당하는 벡터를 취해 임베딩을 만드는 방법이었다. <code>문장 수준 입베딩에서의 LSA 방법은 단어 수준 임베딩에서의 LSA 방법론을 통해 얻게된 정확히 말하자면 SVD를 통해 축소된 행렬에서 문서에 대응하는 벡터를 취해 문서 임베딩을 만드는 방식이다.</code></p>
</li>
<li><p>실습 대상 데이터는 ratsgo.github.uo의 아티클 하나로 markdwon 문서의 제목과 본문을 그대로 텍스트로 저장한 형태이다. 1개 라인이 1개 문서에 해당한다. 불필요한 기호나 LaTex math 패기지의 문법으로 작성되어있는 부분들이 다수 존재한다. 우선 <code>이 실습의 가정을 수식이나 기호는 분석에 있어서 큰 의미를 갖지 않는다라고 가정하고 시작하겠다.</code></p>
</li>
<li><p>우선, 형태소분석기를 어떤것을 사용하던 가능하게 함수를 하나 만들어준다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from khaiii import KhaiiiApi</span><br><span class="line">from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_tokenizer(tokenizer_name):</span><br><span class="line">    <span class="keyword">if</span> tokenizer_name == <span class="string">"komoran"</span>:</span><br><span class="line">        tokenizer = Komoran()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"okt"</span>:</span><br><span class="line">        tokenizer = Okt()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"mecab"</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"hannanum"</span>:</span><br><span class="line">        tokenizer = Hannanum()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"kkma"</span>:</span><br><span class="line">        tokenizer = Kkma()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"khaiii"</span>:</span><br><span class="line">        tokenizer = KhaiiiApi()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="built_in">return</span> tokenizer</span><br></pre></td></tr></table></figure>
<ul>
<li>한 문단 별로 구분자를 어떤것으로 했는지 확인하기 하나씩 프린트해보았다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">corpus_fname = <span class="string">"./data/processed/processed_blog.txt"</span></span><br><span class="line"></span><br><span class="line">with open(corpus_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"---------------------------------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"---------------------------------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"---------------------------------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br></pre></td></tr></table></figure>
<p><img src="/image/read_lines.png" alt="라인 하나씩 출력"></p>
<ul>
<li>아래 코드를 실행하면 제일 처음 문서의 임베딩과 코사인 유사도가 가장 높은 문서 임베딩의 제목을 return해준다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from sklearn.decomposition import TruncatedSVD</span><br><span class="line">from sklearn.preprocessing import normalize</span><br><span class="line">from sklearn.manifold import TSNE</span><br><span class="line">from sklearn.metrics.pairwise import cosine_similarity</span><br><span class="line"></span><br><span class="line">from bokeh.io import export_png, output_notebook, show</span><br><span class="line">from bokeh.plotting import figure</span><br><span class="line">from bokeh.models import Plot, Range1d, MultiLine, Circle, HoverTool, TapTool, BoxSelectTool, LinearColorMapper, ColumnDataSource, LabelSet, SaveTool, ColorBar, BasicTicker</span><br><span class="line">from bokeh.models.graphs import from_networkx, NodesAndLinkedEdges, EdgesAndLinkedNodes</span><br><span class="line">from bokeh.palettes import Spectral8</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def LSAeval(corpus_file, doc_idx, nth_top):</span><br><span class="line">    tokenizer =  get_tokenizer(<span class="string">"mecab"</span>)</span><br><span class="line">    titles, raw_corpus, noun_corpus = [], [], []</span><br><span class="line"></span><br><span class="line">    with open(corpus_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            try:</span><br><span class="line">                title, document = line.strip().split(<span class="string">'\u241E'</span>)</span><br><span class="line">                titles.append(title)</span><br><span class="line">                raw_corpus.append(document)</span><br><span class="line">                nouns = tokenizer.nouns(document)</span><br><span class="line">                noun_corpus.append(<span class="string">' '</span>.join(nouns))</span><br><span class="line">            except:</span><br><span class="line">                <span class="built_in">continue</span></span><br><span class="line">    <span class="comment"># 문서(단락)에서 기호들과 조사를 제외하고 명사들만 추출한 데이터 중 Unigram(ngram_range(1,1)),</span></span><br><span class="line">    <span class="comment"># DF가 1이상(min_df=1)인 데이터를 추려 TF-IDF 행렬을 만들 것이다.</span></span><br><span class="line">    vectorizer = TfidfVectorizer(min_df=1,</span><br><span class="line">                             ngram_range=(1,1),</span><br><span class="line">                             <span class="comment"># tokenizing전에 모든 문자를 소문자로 바꿔준다.</span></span><br><span class="line">                             lowercase=True,</span><br><span class="line">                             <span class="comment"># analyzer == 'word'인 경우만 사용가능.</span></span><br><span class="line">                             tokenizer=lambda x: x.split())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 행은 문서, 열은 단어에 각각 대응한다. (204 x 37153)</span></span><br><span class="line">    input_matrix = vectorizer.fit_transform(noun_corpus)</span><br><span class="line"></span><br><span class="line">    id2vocab = &#123;vectorizer.vocabulary_[token]:token <span class="keyword">for</span> token <span class="keyword">in</span> vectorizer.vocabulary_.keys()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># curr_doc : Corpus 첫번 째 문서의 TF-IDF 벡터</span></span><br><span class="line">    curr_doc, result = input_matrix[doc_idx], []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># curr_doc에서 TF-IDF 값이 0이 아닌 요소들은 내림차순 정렬</span></span><br><span class="line">    <span class="comment"># curr_doc은 105개의 원소(단어)만이 저장되어 있는 Compressed Sparse Row format이다.</span></span><br><span class="line">    <span class="comment"># 그러므로 indices(CSR format index array of the matrix)로 해당 index에 위치하는 단어와 그에대한 tf-idf값을 쌍으로 tuple형태로 넣어준다.</span></span><br><span class="line">    <span class="keyword">for</span> idx, el <span class="keyword">in</span> zip(curr_doc.indices, curr_doc.data):</span><br><span class="line">        result.append((id2vocab[idx], el))</span><br><span class="line"></span><br><span class="line">    sorted(result, key=lambda x : x[1], reverse=True)[:5]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 이번에는 이 TF-IDF 행렬에 100차원 SVD를 수행할 것이다. 204 x 37153의 희소 행렬을</span></span><br><span class="line">    <span class="comment"># 204 x 100 크기의 Dense Matrix로 linear Transforamtion하는 것이다.</span></span><br><span class="line">    svd =  TruncatedSVD(n_components=100)</span><br><span class="line">    vecs = svd.fit_transform(input_matrix)</span><br><span class="line">    svd_l2norm_vectors = normalize(vecs, axis=1, norm=<span class="string">'l2'</span>)</span><br><span class="line">    cosine_similarity = np.dot(svd_l2norm_vectors, svd_l2norm_vectors[doc_idx])</span><br><span class="line">    query_sentence = titles[doc_idx]</span><br><span class="line">    <span class="built_in">return</span> titles, svd_l2norm_vectors, [query_sentence, sorted(zip(titles, cosine_similarity), key=lambda x: x[1], reverse=True)[1:nth_top + 1]]</span><br></pre></td></tr></table></figure>
<p><img src="/image/noun_corpus.png" alt="임베딩 벡터를 만든 Corpus는 명사만 추출"></p>
<ul>
<li>상위 5개의 벡터의 내적이 높은 순으로 내림차순 정력했을때의 결과물 출력</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">titles, svd_l2norm_vectors, top_five = LSAeval(corpus_file=<span class="string">"./data/processed/processed_blog.txt"</span>, doc_idx=0, nth_top=5)</span><br><span class="line">top_five</span><br></pre></td></tr></table></figure>
<p><img src="/image/top_5_similarity.png" alt="첫번째 문서와의 높은 유사도를 갖는 상위 5개의 문서"></p>
<h3 id="임베딩-시각화"><a href="#임베딩-시각화" class="headerlink" title="임베딩 시각화"></a>임베딩 시각화</h3><ul>
<li>t-SNE 기법을 사용해서 벡터공간을 2차원으로 줄여준 뒤 시각화 할 것이다. 또한 벡터들간의 전체적인 유사도는 시각적으로 그리기보다는 상관행렬 방식으로 나타내 줄 것이다.</li>
</ul>
<ul>
<li><p>시각화에 필요한 함수들 정의</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">def visualize(titles, vectors, mode=<span class="string">"between"</span>, num_sents=30, palette=<span class="string">"Viridis256"</span>, use_notebook=False):</span><br><span class="line">        doc_idxes = random.sample(range(len(titles)), num_sents)</span><br><span class="line">        sentences = [titles[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> doc_idxes]</span><br><span class="line">        vecs = [vectors[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> doc_idxes]</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">"between"</span>:</span><br><span class="line">            visualize_between_sentences(sentences, vecs, palette, use_notebook=use_notebook)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            visualize_sentences(vecs, sentences, palette, use_notebook=use_notebook)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def visualize_between_sentences(sentences, vec_list, palette=<span class="string">"Viridis256"</span>,</span><br><span class="line">                                filename=<span class="string">"between-sentences.png"</span>,</span><br><span class="line">                                use_notebook=False):</span><br><span class="line">    df_list, score_list = [], []</span><br><span class="line">    <span class="keyword">for</span> sent1_idx, sentence1 <span class="keyword">in</span> enumerate(sentences):</span><br><span class="line">        <span class="keyword">for</span> sent2_idx, sentence2 <span class="keyword">in</span> enumerate(sentences):</span><br><span class="line">            vec1, vec2 = vec_list[sent1_idx], vec_list[sent2_idx]</span><br><span class="line">            <span class="keyword">if</span> np.any(vec1) and np.any(vec2):</span><br><span class="line">                score = cosine_similarity(X=[vec1], Y=[vec2])</span><br><span class="line">                <span class="comment"># [0][0]인 이유는 값만 뽑아 내기 위해서이다.</span></span><br><span class="line">                df_list.append(&#123;<span class="string">'x'</span>: sentence1, <span class="string">'y'</span>: sentence2, <span class="string">'similarity'</span>: score[0][0]&#125;)</span><br><span class="line">                score_list.append(score[0][0])</span><br><span class="line">    df = pd.DataFrame(df_list)</span><br><span class="line">    color_mapper = LinearColorMapper(palette=palette, low=np.max(score_list), high=np.min(score_list))</span><br><span class="line">    TOOLS = <span class="string">"hover,save,pan,box_zoom,reset,wheel_zoom"</span></span><br><span class="line">    p = figure(x_range=sentences, y_range=list(reversed(sentences)),</span><br><span class="line">                x_axis_location=<span class="string">"above"</span>, plot_width=900, plot_height=900,</span><br><span class="line">                toolbar_location=<span class="string">'below'</span>, tools=TOOLS,</span><br><span class="line">                tooltips=[(<span class="string">'sentences'</span>, <span class="string">'@x @y'</span>), (<span class="string">'similarity'</span>, <span class="string">'@similarity'</span>)])</span><br><span class="line">    p.grid.grid_line_color = None</span><br><span class="line">    p.axis.axis_line_color = None</span><br><span class="line">    p.axis.major_tick_line_color = None</span><br><span class="line">    p.axis.major_label_standoff = 0</span><br><span class="line">    p.xaxis.major_label_orientation = 3.14 / 3</span><br><span class="line">    p.rect(x=<span class="string">"x"</span>, y=<span class="string">"y"</span>, width=1, height=1,</span><br><span class="line">            <span class="built_in">source</span>=df,</span><br><span class="line">            fill_color=&#123;<span class="string">'field'</span>: <span class="string">'similarity'</span>, <span class="string">'transform'</span>: color_mapper&#125;,</span><br><span class="line">            line_color=None)</span><br><span class="line">    color_bar = ColorBar(ticker=BasicTicker(desired_num_ticks=5),</span><br><span class="line">                        color_mapper=color_mapper, major_label_text_font_size=<span class="string">"7pt"</span>,</span><br><span class="line">                        label_standoff=6, border_line_color=None, location=(0, 0))</span><br><span class="line">    p.add_layout(color_bar, <span class="string">'right'</span>)</span><br><span class="line">    <span class="keyword">if</span> use_notebook:</span><br><span class="line">        output_notebook()</span><br><span class="line">        show(p)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        export_png(p, filename)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"save @ "</span> + filename)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def visualize_sentences(vecs, sentences, palette=<span class="string">"Viridis256"</span>, filename=<span class="string">"/notebooks/embedding/sentences.png"</span>,</span><br><span class="line">                        use_notebook=False):</span><br><span class="line">    tsne = TSNE(n_components=2)</span><br><span class="line">    tsne_results = tsne.fit_transform(vecs)</span><br><span class="line">    df = pd.DataFrame(columns=[<span class="string">'x'</span>, <span class="string">'y'</span>, <span class="string">'sentence'</span>])</span><br><span class="line">    df[<span class="string">'x'</span>], df[<span class="string">'y'</span>], df[<span class="string">'sentence'</span>] = tsne_results[:, 0], tsne_results[:, 1], sentences</span><br><span class="line">    <span class="built_in">source</span> = ColumnDataSource(ColumnDataSource.from_df(df))</span><br><span class="line">    labels = LabelSet(x=<span class="string">"x"</span>, y=<span class="string">"y"</span>, text=<span class="string">"sentence"</span>, y_offset=8,</span><br><span class="line">                      text_font_size=<span class="string">"12pt"</span>, text_color=<span class="string">"#555555"</span>,</span><br><span class="line">                      <span class="built_in">source</span>=<span class="built_in">source</span>, text_align=<span class="string">'center'</span>)</span><br><span class="line">    color_mapper = LinearColorMapper(palette=palette, low=min(tsne_results[:, 1]), high=max(tsne_results[:, 1]))</span><br><span class="line">    plot = figure(plot_width=900, plot_height=900)</span><br><span class="line">    plot.scatter(<span class="string">"x"</span>, <span class="string">"y"</span>, size=12, <span class="built_in">source</span>=<span class="built_in">source</span>, color=&#123;<span class="string">'field'</span>: <span class="string">'y'</span>, <span class="string">'transform'</span>: color_mapper&#125;, line_color=None, fill_alpha=0.8)</span><br><span class="line">    plot.add_layout(labels)</span><br><span class="line">    <span class="keyword">if</span> use_notebook:</span><br><span class="line">        output_notebook()</span><br><span class="line">        show(plot)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        export_png(plot, filename)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"save @ "</span> + filename)</span><br></pre></td></tr></table></figure>
</li>
<li><p>혹시 이러한 error가 난다면, 다음과 같이 PhantomJS를 설치한다. 간단히 말하자면 PhantomJS도 Selenium같이 웹브라우져 개발용으로 만들어진 프로그램이다. bokeh는 javascript기반으로 짜여져있어서 필요한 것 같다.</p>
</li>
</ul>
<p><img src="/image/phantomjs_error.png" alt="phantomjs error"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -c conda-forge phantomjs</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visualize(titles, svd_l2norm_vectors, mode=<span class="string">"between"</span>, num_sents=30, palette=<span class="string">"Viridis256"</span>, use_notebook=True)</span><br></pre></td></tr></table></figure>
<p><img src="/image/sentence_between_cosine_simularity.png" alt="벡터들간의 유성성 상관행렬"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visualize(titles, svd_l2norm_vectors, mode=<span class="string">"tsne"</span>, num_sents=30, palette=<span class="string">"Viridis256"</span>, use_notebook=True)</span><br></pre></td></tr></table></figure>
<p><img src="/image/t_SNE_plot_bog_source.png" alt="t-SNE를 활용한 임베딩 벡터 시각화"></p>
<h2 id="Doc2Vec"><a href="#Doc2Vec" class="headerlink" title="Doc2Vec"></a>Doc2Vec</h2><h4 id="모델-개요"><a href="#모델-개요" class="headerlink" title="모델 개요"></a>모델 개요</h4><ul>
<li><p>Word2Vec에 이어 구글 연구 팀이 개발한 문서 임베딩 기법이다. <code>이전 단어 sequence k개가 주어졌을 때 그 다음 단어를 맞추는 언어 모델</code>을 만들었다. <code>이 모델은 문장 전체를 처음부터 끝까지 한 단어씩 슬라이딩해 가면서 다음 단어가 무엇일지 예측한다.</code></p>
</li>
<li><p>로그 확률 평균의 값이 커진다는 의미는 이전 k개 단어들을 입력하면 모델이 다음 단어를 잘 예측하므로 로그 확률 평균을 최대화는 과정에서 학습된다.</p>
</li>
<li><p>NPLM에서 설명했던 방식처럼 문장 전체를 한 단어씩 슬라이딩해가면서 다음 target word를 맞추는 과정에서 context word에 해당하는 $(w_{t-k}, \cdots, w_{t-1})$에 해당하는 W 행렬의 벡터들이 업데이트 한다. <code>따라서 주변 이웃 단어 집합 즉 context가 유사한 단어벡터는 벡터 공간에 가깝게 임베딩 된다.</code>학습이 종료되면 W를 각 단어의 임베딩으로 사용한다.</p>
</li>
</ul>
<h5 id="Doc2vec-언어-모델"><a href="#Doc2vec-언어-모델" class="headerlink" title="Doc2vec 언어 모델"></a>Doc2vec 언어 모델</h5><ul>
<li>$T$ : 학습 데이터 문장 하나의 단어 개수</li>
<li>$w_{t}$ : 문장의 t번째 단어</li>
<li>$y_{i}$ : corpus 전체 어휘 집합 중 i번째 단어에 해당하는 점수<ul>
<li>1) 이전 k개 단어들을 W라는 단어 행렬에서 참조한 뒤 평균을 취하거나 이어 붙인다. 여기에 U라는 행렬을 내적하고 bias 벡터인 b를 더해준 뒤 softmax를 취한다. U의 크기는 어휘집합 크기 $\times$ 임베딩 차원 수 이다.</li>
</ul>
</li>
<li>$h$ : 벡터 sequence가 주어졌을 때 평균을 취하거나 concatenate하여 고정된 길이의 벡터 하나를 반환하는 역할을 하는 함수이다.</li>
</ul>
<script type="math/tex; mode=display">L = frac{1}{T} \sum^{T-1}-{t=k} log(w_{t}|w_{t-k}, \cdots, w_{t-1})</script><h5 id="Doc2Vec-언어-모델-Score-계산"><a href="#Doc2Vec-언어-모델-Score-계산" class="headerlink" title="Doc2Vec 언어 모델 Score 계산"></a>Doc2Vec 언어 모델 Score 계산</h5><script type="math/tex; mode=display">P(w_{t}| w_{t-k}, \cdots, w_{t-1}) = \frac{exp(y_{w_{t}})}{\sum_{i} exp(y_{i})}</script><script type="math/tex; mode=display">y = b + U \cdot h(w_{t-k}, \cdots, w_{t-1}; W)</script><ul>
<li>위의 초기 구조에서 <code>문서 id를 추가해 이전 k개 단어들과 문서 id를 넣어서 다음 단어를 예측</code>하게 했다. <code>y를 계산할 때 D라는 문서 행렬(Paragraph matrix)에서 해당 문서 ID에 해당하는 벡터를 참조해 h 함수에 다른 단어 벡터들과 함께 입력하는 것 외에 나머지 과정은 동일</code>하다. 이런 구조를 <code>PV-DM(the Distributed Memory Model of Paragraph Vectors)</code>이라고 부른다. 학습이 종료되면 문서 수 $\times$ 임베딩 차원 수 크기를 가지는 문서 행렬 D를 각 문서의 임베딩으로 사용한다. 이렇게 만든 <code>문서 임베딩이 해당 문서의 주제 정보를 함축한다고 설명</code>한다. <code>PV-DM은 단어 등장 순서를 고려하는 방식으로 학습하기 때문에 순서 정보를 무시하는 Bag of Words 기법 대비 강점이 있다고 할 수 있을 것</code>이다.</li>
</ul>
<p><img src="/image/Doc2vec_model_principal.png" alt="Doc2vec 모델"></p>
<ul>
<li>또한, Word2Vec의 Skip-gram을 본뜬 <code>PV-DBOW</code>(the Distributed Bag of Words version of Paragraph Vectors)도 제안했다. Skip-gram은 target word를 가지고 context word들을 예측하는 과정에서 학습되었다. PV-DBOW도 문서 id를 가지고 context word들을 맞춘다. 따라서 <code>문서 id에 해당하는 문서 임베딩엔 문서에 등장하는 모든 단어의 의미 정보가 반영</code>된다.</li>
</ul>
<p><img src="/image/Doc2Vec_DBOW_model.png" alt="Doc2Vec CBOW 모델"></p>
<h2 id="Doc2Vec-실습"><a href="#Doc2Vec-실습" class="headerlink" title="Doc2Vec 실습"></a>Doc2Vec 실습</h2><h3 id="실습-데이터"><a href="#실습-데이터" class="headerlink" title="실습 데이터"></a>실습 데이터</h3><ul>
<li>영화 댓글과 해당 영화의 ID가 라인 하나를 구성하고 있다. 영화하나를 문서로 보고 Doc2Vec 모델을 학습할 예정이다. 따라서 영화 ID가 동일한 문장들을 하나의 문서로 처리해 줄 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">with open(<span class="string">"./data/processed/processed_review_movieid.txt"</span>) as f:</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    <span class="built_in">print</span>(f.readline())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"--------------------------------------------------------------------------------------------"</span>)</span><br><span class="line">    count = 4</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> f:</span><br><span class="line">        count+=1</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"총 sentence의 개수 : &#123;&#125;개"</span>.format(count))</span><br></pre></td></tr></table></figure>
<p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">종합 평점은 4점 드립니다.␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">원작이 칭송받는 이유는 웹툰 계 자체의 질적 저하가 심각하기 때문.  원작이나 영화나 별로인건 마찬가지.␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">나름의  감동도 있고 안타까운 마음에 가슴도 먹먹  배우들의 연기가 good 김수현 최고~␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">이런걸 돈주고 본 내자신이 후회스럽다 최악의 쓰레기 영화 김수현 밖에없는 저질 삼류영화␞92575</span><br><span class="line"></span><br><span class="line">--------------------------------------------------------------------------------------------</span><br><span class="line">총 sentence의 개수 : 712532개</span><br></pre></td></tr></table></figure></p>
<ul>
<li>Doc2Vec 모델 학습을 위해 Python gensim 라이브러리의 Doc2Vec 클래스를 사용하는데 Doc2VecInput은 이 클래스가 요구하는 입력 형태를 맞춰주는 역할을 한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">from khaiii import KhaiiiApi</span><br><span class="line">from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma</span><br><span class="line">from gensim.models.doc2vec import TaggedDocument</span><br><span class="line"></span><br><span class="line">def get_tokenizer(tokenizer_name):</span><br><span class="line">    <span class="keyword">if</span> tokenizer_name == <span class="string">"komoran"</span>:</span><br><span class="line">        tokenizer = Komoran()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"okt"</span>:</span><br><span class="line">        tokenizer = Okt()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"mecab"</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"hannanum"</span>:</span><br><span class="line">        tokenizer = Hannanum()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"kkma"</span>:</span><br><span class="line">        tokenizer = Kkma()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"khaiii"</span>:</span><br><span class="line">        tokenizer = KhaiiiApi()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="built_in">return</span> tokenizer</span><br><span class="line"></span><br><span class="line">class Doc2VecInput:</span><br><span class="line"></span><br><span class="line">    def __init__(self, fname, tokenizer_name=<span class="string">'mecab'</span>):</span><br><span class="line">        self.fname = fname</span><br><span class="line">        self.tokenizer = get_tokenizer(tokenizer_name)</span><br><span class="line"></span><br><span class="line">    def __iter__(self):</span><br><span class="line">        with open(self.fname, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                try:</span><br><span class="line">                    sentence, movie_id = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                    tokens = self.tokenizer.morphs(sentence)</span><br><span class="line">                    tagged_doc = TaggedDocument(words=tokens, tags=[<span class="string">'movie_%s'</span> % movie_id])</span><br><span class="line">                    yield tagged_doc</span><br><span class="line">                except:</span><br><span class="line">                    <span class="built_in">continue</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>dm : 1 (default) -&gt; PV-DM, 0- &gt; PV-DBOW</code></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import Doc2Vec</span><br><span class="line"></span><br><span class="line">corpus_fname = <span class="string">'./data/processed/processed_review_movieid.txt'</span></span><br><span class="line">output_fname = <span class="string">'./doc2vec.model'</span></span><br><span class="line"></span><br><span class="line">corpus = Doc2VecInput(corpus_fname)</span><br><span class="line">model = Doc2Vec(corpus, dm=1, vector_size=100)</span><br><span class="line">model.save(output_fname)</span><br></pre></td></tr></table></figure>
<ul>
<li>학습이 잘 되었는지를 평가하기 위해서 아래와 같이 평가 클래스를 이용할 것이며, 평가를 하면 tag된 영화 id가 나올텐데 직관적으로 그 영화가 어떤 영화인지 모를 것이다. 그러므로 직관적으로 결과를 이해하기 위해 학습 데이터에는 없는 영화 제목을 네이버 영화 사이트에 접속해 id에 맞는 영화 제목을 스크래핑해 올 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from lxml import html</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Doc2VecEvaluator:</span><br><span class="line"></span><br><span class="line">    def __init__(self, model_fname=<span class="string">"data/doc2vec.vecs"</span>, use_notebook=False):</span><br><span class="line">        self.model = Doc2Vec.load(model_fname)</span><br><span class="line">        self.doc2idx = &#123;el:idx <span class="keyword">for</span> idx, el <span class="keyword">in</span> enumerate(self.model.docvecs.doctags.keys())&#125;</span><br><span class="line">        self.use_notebook = use_notebook</span><br><span class="line"></span><br><span class="line">    def most_similar(self, movie_id, topn=10):</span><br><span class="line">        similar_movies = self.model.docvecs.most_similar(<span class="string">'movie_'</span> + str(movie_id), topn=topn)</span><br><span class="line">        <span class="keyword">for</span> movie_id, score <span class="keyword">in</span> similar_movies:</span><br><span class="line">            <span class="built_in">print</span>(self.get_movie_title(movie_id), score)</span><br><span class="line"></span><br><span class="line">    def get_titles_in_corpus(self, n_sample=5):</span><br><span class="line">        movie_ids = random.sample(self.model.docvecs.doctags.keys(), n_sample)</span><br><span class="line">        <span class="built_in">return</span> &#123;movie_id: self.get_movie_title(movie_id) <span class="keyword">for</span> movie_id <span class="keyword">in</span> movie_ids&#125;</span><br><span class="line"></span><br><span class="line">    def get_movie_title(self, movie_id):</span><br><span class="line">        url = <span class="string">'http://movie.naver.com/movie/point/af/list.nhn?st=mcode&amp;target=after&amp;sword=%s'</span> % movie_id.split(<span class="string">"_"</span>)[1]</span><br><span class="line">        resp = requests.get(url)</span><br><span class="line">        root = html.fromstring(resp.text)</span><br><span class="line">        try:</span><br><span class="line">            title = root.xpath(<span class="string">'//div[@class="choice_movie_info"]//h5//a/text()'</span>)[0]</span><br><span class="line">        except:</span><br><span class="line">            title = <span class="string">""</span></span><br><span class="line">        <span class="built_in">return</span> title</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_fname=<span class="string">'./doc2vec.model'</span></span><br><span class="line">model = Doc2VecEvaluator(model_fname)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"영화의 종류 : &#123;&#125; 개"</span>.format(len(model.doc2idx.keys())))</span><br></pre></td></tr></table></figure>
<p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">영화의 종류 : 14730 개</span><br></pre></td></tr></table></figure></p>
<ul>
<li>학습 데이터에 포함된 아무 영화 10개의 제목을 보여준다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.get_titles_in_corpus(n_sample=14730)</span><br></pre></td></tr></table></figure>
<p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'movie_89743'</span>: <span class="string">'여자가 두 번 화장할 때'</span>,</span><br><span class="line"> <span class="string">'movie_16490'</span>: <span class="string">'투 문 정션 2'</span>,</span><br><span class="line"> <span class="string">'movie_100953'</span>: <span class="string">'더 퍼지'</span>,</span><br><span class="line"> <span class="string">'movie_84375'</span>: <span class="string">'퍼펙트 센스'</span>,</span><br><span class="line"> <span class="string">'movie_24203'</span>: <span class="string">'섀터드 이미지'</span>,</span><br><span class="line"> <span class="string">'movie_12054'</span>: <span class="string">'나폴레옹'</span>,</span><br><span class="line"> <span class="string">'movie_10721'</span>: <span class="string">'더티 해리'</span>,</span><br><span class="line"> <span class="string">'movie_11440'</span>: <span class="string">'킹 뉴욕'</span>,</span><br><span class="line"> <span class="string">'movie_20896'</span>: <span class="string">'미망인'</span>,</span><br><span class="line"> <span class="string">'movie_123068'</span>: <span class="string">'캠걸'</span>&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>해당 id와 유사한 영화 상위 5개를 보여준다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.most_similar(37758, topn=5)</span><br></pre></td></tr></table></figure>
<p>결과<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">돈비 어프레이드-어둠 속의 속삭임 0.746237576007843</span><br><span class="line">더 퍼지:거리의 반란 0.7402248382568359</span><br><span class="line">고양이: 죽음을 보는 두 개의 눈 0.6967850923538208</span><br><span class="line">ATM 0.690518856048584</span><br><span class="line">힛쳐 0.6751468777656555</span><br></pre></td></tr></table></figure></p>
<h2 id="잠재-디리클레-할당-LDA-Latent-Dirichlet-Allocation"><a href="#잠재-디리클레-할당-LDA-Latent-Dirichlet-Allocation" class="headerlink" title="잠재 디리클레 할당(LDA, Latent Dirichlet Allocation)"></a>잠재 디리클레 할당(LDA, Latent Dirichlet Allocation)</h2><ul>
<li><code>주어진 문서에 대하여 각 문서에 어떤 topic들이 존재하는지에 대한 확률 모형</code>이다. corpus의 이면에 잠재된 topic을 추출한다는 의미에서 topic modeling이라고 부르기도 한다. <code>문서를 topic 확률 분포로 나타내 각각을 벡터화한다는 점에서 LDA를 임베딩 기법의 일종으로 이해</code>할 수 있다.</li>
</ul>
<h4 id="모델-개요-1"><a href="#모델-개요-1" class="headerlink" title="모델 개요"></a>모델 개요</h4><ul>
<li>LDA는 topic별 단어의 분포, 문서별 topic의 분포를 모두 추정해 낸다.</li>
</ul>
<p><img src="/image/LDA_brief_out.png" alt="LDA의 개략적 도식"></p>
<ul>
<li><p>LDA는 topic에 특정 단어가 나타날 확률을 내어 준다. 위의 그림에서 각 색깔별로 topic에 따른 단어가 등장할 확률을 보여주고있다. 문서를 보면 노란색 topic에 해당하는 단어가 많기 때문에 위 문서의 메인 주제는 노란색 topic인 유전자 관련 topic일 가능성이 클 것이다. 이렇듯 문서의 topic 비중 또한 LDA의 산출 결과가 된다.</p>
</li>
<li><p><code>LDA가 가정하는 문서 생성 과정</code>은 우리가 글을 쓸때와 같다. 실제 글을 작성할 때는 글감 내지 주제를 먼저 결정한 후 어떤 단어를 써야 할지 결정한다. 이와 마찬가지로 LDA는 <code>우선 corpus로 부터 얻은 topic 분포로부터 topic을 뽑는다. 이후 해당 topic에 해당하는 단어들을 뽑는다.</code> 그런데 corpus에 등장하는 단어들 각각에 꼬리표가 달려 있는 것은 아니기 때문에 현재 문서에 등장한 단어들은 어떤 topic에서 뽑힌 단어들인지 우리가 명시적으로 알기는 어려울 것이다. <code>하지만, LDA는 이런 corpus 이면에 존재하는 정보를 추론해낼 수 있다.</code></p>
</li>
</ul>
<h4 id="아키텍처"><a href="#아키텍처" class="headerlink" title="아키텍처"></a>아키텍처</h4><ul>
<li>LDA가 가정하는 문서 생성 과정은 아래의 그림과 같다.<ul>
<li>D : corpus 전체 문서 개수</li>
<li>K : 전체 topic 수(hyper parameter)</li>
<li>N : d번째 문서의 단어 수</li>
<li>네모칸 : 해당 횟수만큼 반복하라는 의미</li>
<li>$\phi_{k}$ : k번째 topic에 해당하는 벡터 $\phi_{k} \in R^{|V|}$, $\phi_{k}$는 word-topic 행렬의 k번째 열을 의미한다. $\phi_{k}$의 각 요소 값은 해당 단어가 k번째 토픽에서 차지하는 비중을 의미하며 확률값이므로 이 벡터의 요소값의 합은 1이 된다. 이런 topic의 단어비중을 의미하는 $\phi_{k}$는 디리클레 분포를 따른 다는 가정을 취하므로 $\beta$의 영향을 받는다.</li>
<li>$\theta_{d}$ : d번째 문서가 가진 topic 비중을 나타내는 벡터이다. 그러므로 전체 topic의 개수만큼의 길이 갖으며, 각 벡터는 확률을 의미하므로 합은 1이된다. 문서의 topic 비중 $\theta_{d}$는 디리클레 분포를 따른다는 가정을 취하므로 $\alpha$의 영향을 받는다.</li>
<li>$ z_{d,n}$ : d번째 문서에서 n번째 단어가 어떤 topic인지를 나타내는 변수이다. 그러므로 이 변수는 d번째 문서의 topic 확률 분포인 $\theta_{d}$에 영향을 받는다.</li>
<li>동그라미 : 변수를 의미</li>
<li>화살표가 시작되는 변수는 조건, 화살표가 향하는 변수는 결과에 해당하는 변수</li>
</ul>
</li>
</ul>
<ul>
<li><code>관찰 가능한 변수는 d번째 문서에 등장한 n번째 단어 $w_{d,n}$이 유일</code>하다. 우리는 이 정보만을 가지고 하이퍼파라메터(사용자 지정) α,β를 제외한 모든 잠재 변수를 추정해야 한다.</li>
</ul>
<p><img src="/image/graphical_model_LDA.png" alt="graphical model로 표현한 LDA"></p>
<ul>
<li>예시를 들자면, 아래 Document-topic 행렬을 살펴보자. 3번째 문서에 속한 단어들은 가장 높은 확률값 0.625를 갖는 topic-2일 가능성이 높다. $w_{d,n}$은 d번째 문서 내에 n번째로 등장하는 단어를 의미하며, 동시에 우리가 유일하게 corpus에서 관찰할 수 있는 데이터이다. 이는 $\phi_{k}$와 $\z_{d,n}$에 동시에 영향을 받는다. 예를 들면, $z_{3,1}$가 topic-2이라고 가정했을 경우, $w_{3,1}$은 word-topic 행렬에서 보게되면 제일 높은 확률값 0.393을 갖는 ‘코로나 바이러스’일 가능성이 높다.</li>
</ul>
<h4 id="이처럼-LDA는-topic의-word-분포-phi-와-문서의-topic-분포-theta-의-결합으로-문서-내-단어들이-생성된다고-가정한다"><a href="#이처럼-LDA는-topic의-word-분포-phi-와-문서의-topic-분포-theta-의-결합으로-문서-내-단어들이-생성된다고-가정한다" class="headerlink" title="이처럼 LDA는 topic의 word 분포($\phi$)와 문서의 topic 분포($\theta$)의 결합으로 문서 내 단어들이 생성된다고 가정한다."></a>이처럼 LDA는 topic의 word 분포($\phi$)와 문서의 topic 분포($\theta$)의 결합으로 문서 내 단어들이 생성된다고 가정한다.</h4><h5 id="Document-topic-행렬"><a href="#Document-topic-행렬" class="headerlink" title="Document-topic 행렬"></a>Document-topic 행렬</h5><div class="table-container">
<table>
<thead>
<tr>
<th>문서</th>
<th>topic-1</th>
<th>topic-2</th>
<th>topic-3</th>
</tr>
</thead>
<tbody>
<tr>
<td>문서1</td>
<td>0.400</td>
<td>0.000</td>
<td>0.600</td>
</tr>
<tr>
<td>문서2</td>
<td>0.000</td>
<td>0.600</td>
<td>0.400</td>
</tr>
<tr>
<td>문서3</td>
<td>0.375</td>
<td>0.625</td>
<td>0.000</td>
</tr>
</tbody>
</table>
</div>
<h6 id="word-topic-행렬"><a href="#word-topic-행렬" class="headerlink" title="word-topic 행렬"></a>word-topic 행렬</h6><div class="table-container">
<table>
<thead>
<tr>
<th>단어</th>
<th>topic-1</th>
<th>topic-2</th>
<th>topic-3</th>
</tr>
</thead>
<tbody>
<tr>
<td>코로나 바이러스</td>
<td>0.000</td>
<td>0.393</td>
<td>0.000</td>
</tr>
<tr>
<td>우한폐렴</td>
<td>0.000</td>
<td>0.313</td>
<td>0.000</td>
</tr>
<tr>
<td>AWS</td>
<td>0.119</td>
<td>0.000</td>
<td>0.000</td>
</tr>
<tr>
<td>데이터 엔지니어링</td>
<td>0.181</td>
<td>0.000</td>
<td>0.000</td>
</tr>
<tr>
<td>Hadoop</td>
<td>0.276</td>
<td>0.000</td>
<td>0.000</td>
</tr>
<tr>
<td>Spark</td>
<td>0.142</td>
<td>0.000</td>
<td>0.000</td>
</tr>
<tr>
<td>낭만 닥터 김사부2</td>
<td>0.000</td>
<td>0.012</td>
<td>0.468</td>
</tr>
<tr>
<td>tensorflow</td>
<td>0.282</td>
<td>0.000</td>
<td>0.000</td>
</tr>
<tr>
<td>마스크</td>
<td>0.000</td>
<td>0.282</td>
<td>0.000</td>
</tr>
<tr>
<td>사랑의 불시착</td>
<td>0.000</td>
<td>0.000</td>
<td>0.532</td>
</tr>
<tr>
<td>합</td>
<td>1.0</td>
<td>1.0</td>
<td>1.0</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>실제 관찰 가능한 corpus를 가지고 알고 싶은 topic의 word 분포, 문서의 topic 분포를 추정하는 과정을 통해 LDA는 학습한다. 즉, <code>topic의 word 분포와 문서의 topic 분포의 결합 확률이 커지는 방향으로 학습을 한다는 의미</code>이다.</li>
</ul>
<h5 id="LDA의-단어-생성-과정"><a href="#LDA의-단어-생성-과정" class="headerlink" title="LDA의 단어 생성 과정"></a>LDA의 단어 생성 과정</h5><script type="math/tex; mode=display">p(\phi_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D}) =</script><script type="math/tex; mode=display">\prod^{K}_{i=1} p(\phi_{i}|\beta) \prod^{D}_{d=1} p(\theta_{d}|\alpha) \left(\prod^{N}_{n=1} p(z_{d,n}|\theta_{d}) p(w_{d,n}|\phi_{1:K}, z_{d, n}) \right)</script><ul>
<li><p>우리가 구해야할 사후확률 분포는 $ p(z, \phi, \theta|w) = p(z, \phi, \theta, w)/p(w)$를 최대로 만드는 $ z, \phi, \theta$를 찾아야 한다. 이 사후확률을 직접 계산하려면 분자도 계산하기 어렵겠지만 분모가 되는 $ p(w)$도 반드시 구해야 확률값으로 만들어 줄 수 있다. $ p(w)$는 잠재변수 $ z, \phi, \theta$의 모든 경우의 수를 고려한 각 단어(w)의 등장 확률을 의미하는데, $ z, \phi, \theta$를 직접 관찰하는 것은 불가능하다. 이러한 이유로 <code>깁스 샘플링(gibbs sampling)</code>같은 표본 추출 기법을 사용해 사후확률을 근사시키게 된다. 깁스 샘플링이란 <code>나머지 변수는 고정시킨 채 하나의 랜덤변수만을 대상으로 표본을 뽑는 기법</code>이다. LDA에서는 사후확률 분포 $ p(z, \phi, \theta|w)$를 구할 때 topic의 단어 분포($\phi$)와 문서의 topic 분포($\theta$)를 계산에서 생략하고 topic(z)만을 추론한다. <code>z만 알 수 있으면 나미저 변수를 이를 통해 계산 할 수 있도록 설계 했기 때문이다.</code></p>
</li>
<li><p><a href="https://ratsgo.github.io/statistics/2017/05/31/gibbs/" target="_blank" rel="noopener">깁스 샘플링 참조</a></p>
</li>
</ul>
<h5 id="깁스-샘플링을-활용한-LDA"><a href="#깁스-샘플링을-활용한-LDA" class="headerlink" title="깁스 샘플링을 활용한 LDA"></a>깁스 샘플링을 활용한 LDA</h5><script type="math/tex; mode=display">p(z_{d, i} = j|z_{-i}, w) = \frac{ n_{d,k} + \alpha_{j} }{ \sum^{K}_{i=1} (n_{d,i}) + \alpha_{i} } \times \frac{ v_{k, w_{d,n} } + \beta_{w_{d,n}} }{ \sum^{V}_{ㅓ=1} ( v_{k,j} + \beta_{j} ) } = AB</script><h6 id="LDA-변수-표기법"><a href="#LDA-변수-표기법" class="headerlink" title="LDA 변수 표기법"></a>LDA 변수 표기법</h6><div class="table-container">
<table>
<thead>
<tr>
<th>표기</th>
<th>내용</th>
</tr>
</thead>
<tbody>
<tr>
<td>$n_{d,k}$</td>
<td>k번째 topic에 할당된 d번째 문서의 빈도</td>
</tr>
<tr>
<td>$v_{k,w_{d,n}}$</td>
<td>전체 corpus에서 k번째 topic에 할당된 단어 $w_{d,n}$의 빈도</td>
</tr>
<tr>
<td>$w_{d,n}$</td>
<td>d번째 문서에 n번째로 등장한 단어</td>
</tr>
<tr>
<td>$\alpha$</td>
<td>문서의 topic 분포 생성을 위한 디리클레 분포 파라미터</td>
</tr>
<tr>
<td>$\beta$</td>
<td>topic의 word 분포 생성을 위한 디리클레 분포 파라미터</td>
</tr>
<tr>
<td>K</td>
<td>사용자가 지정하는 topic 개수</td>
</tr>
<tr>
<td>V</td>
<td>corpus에 등장하는 전체 word의 수</td>
</tr>
<tr>
<td>A</td>
<td>d번째 문서가 k번째 topic과 맺고 있는 연관성 정도</td>
</tr>
<tr>
<td>B</td>
<td>d번째 문서의 n번째 단어($ w_{d,n}$)가 k번째 topic과 맺고 있는 연관성 정도</td>
</tr>
</tbody>
</table>
</div>
<h4 id="LDA와-깁스-샘플링"><a href="#LDA와-깁스-샘플링" class="headerlink" title="LDA와 깁스 샘플링"></a>LDA와 깁스 샘플링</h4><ul>
<li>LDA가 각 단어에 잠재된 주제를 추론하는 방식을 살펴본다. 아래표와 같이 단어 5개로 구성된 문서1의 모든 단어에 주제(z)가 이미 할당돼 있다고 가정해보자. <code>LDA는 이렇게 문서 전체의 모든 단어의 주제를 랜덤하게 할당을 하고 학습을 시작하기 때문에 이렇게 가정하는 게 크게 무리가 없다. 또한, topic 수는 사용자가 3개로 이미 지정해 놓은 상태라고 하자.</code> 문서1의 첫 번째 단어($ w_{11} = 천주교)$ 의 주제($z_{11}$)는 3번 topic이다. 마찬가지로 문서1의 3 번째 단어($ w_{13} = 가격$)의 주제($z_{13}$)는 1번 topic이다. 이런 방식으로 Corpus 전체 문서 모든 단어에 topic이 이미 할당됐다고 가정한다. 이로부터 word-topic 행렬을 만들수 있다. <code>전체 문서 모든 단어에 달린 주제들을 일일이 세어서 만든다. 같은 단어라도 topic이 다른 배(동음다의어)같은 경우가 있으므로 각 단어별로 topic 분포가 생겨난다.</code></li>
</ul>
<h5 id="문서-1의-단어별-topic-분포-theta"><a href="#문서-1의-단어별-topic-분포-theta" class="headerlink" title="문서 1의 단어별 topic 분포($\theta$)"></a>문서 1의 단어별 topic 분포($\theta$)</h5><div class="table-container">
<table>
<thead>
<tr>
<th>$z_{1i}$</th>
<th>3</th>
<th>2</th>
<th>1</th>
<th>3</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr>
<td>$w_{1,n}$</td>
<td>천주교</td>
<td>무역</td>
<td>가격</td>
<td>불교</td>
<td>시장</td>
</tr>
</tbody>
</table>
</div>
<h5 id="word-topic-행렬-1"><a href="#word-topic-행렬-1" class="headerlink" title="word-topic 행렬"></a>word-topic 행렬</h5><div class="table-container">
<table>
<thead>
<tr>
<th>단어</th>
<th>topic-1</th>
<th>topic-2</th>
<th>topic-3</th>
</tr>
</thead>
<tbody>
<tr>
<td>천주교</td>
<td>1</td>
<td>0</td>
<td>35</td>
</tr>
<tr>
<td>시장</td>
<td>50</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>가격</td>
<td>42</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>불교</td>
<td>0</td>
<td>0</td>
<td>20</td>
</tr>
<tr>
<td>무역</td>
<td>10</td>
<td>8</td>
<td>1</td>
</tr>
<tr>
<td>$\cdots$</td>
<td>$\cdots$</td>
<td>$\cdots$</td>
<td>$\cdots$</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>깁스 샘플링으로 문서1 두 번째 단어의 잠재된 topic이 무엇인지 추론해보자면, 깁스 샘플링을 적용하기 위해 문서1의 두 번째 topic정보를 지울것이다. 그렇다면 아래와 같은 표로 변화될 것이다.</li>
</ul>
<h5 id="문서1의-단어별-topic-분포"><a href="#문서1의-단어별-topic-분포" class="headerlink" title="문서1의 단어별 topic 분포"></a>문서1의 단어별 topic 분포</h5><div class="table-container">
<table>
<thead>
<tr>
<th>$z_{1i}$</th>
<th>3</th>
<th>?</th>
<th>1</th>
<th>3</th>
<th>1</th>
</tr>
</thead>
<tbody>
<tr>
<td>$w_{1,n}$</td>
<td>천주교</td>
<td>무역</td>
<td>가격</td>
<td>불교</td>
<td>시장</td>
</tr>
</tbody>
</table>
</div>
<h5 id="word-topic-행렬-2"><a href="#word-topic-행렬-2" class="headerlink" title="word-topic 행렬"></a>word-topic 행렬</h5><div class="table-container">
<table>
<thead>
<tr>
<th>단어</th>
<th>topic-1</th>
<th>topic-2</th>
<th>topic-3</th>
</tr>
</thead>
<tbody>
<tr>
<td>천주교</td>
<td>1</td>
<td>0</td>
<td>35</td>
</tr>
<tr>
<td>시장</td>
<td>50</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>가격</td>
<td>42</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>불교</td>
<td>0</td>
<td>0</td>
<td>20</td>
</tr>
<tr>
<td>무역</td>
<td>10</td>
<td>7 = (8 - 1)</td>
<td>1</td>
</tr>
<tr>
<td>$\cdots$</td>
<td>$\cdots$</td>
<td>$\cdots$</td>
<td>$\cdots$</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>p($ z_{1,2} $)는 A와 B의 곱으로 도출된다. A값은 파란색 영역을 의미하며, 문서 내 단어들의 topic 분포에($\theta$)에 영향을 받는다. 또한, B값은 topic의 단어 분포($\phi$)에 영향을 받는다. A와 B를 각각 직사각형의 높이와 너비로 둔다면, p($ z_{1,2} $)는 아래와 같이 직사각형의 넓이로 이해할 수 있다. 이와 같은 방식으로 모든 문서, 모든 단어에 관해 깁스 샘플링을 수행하면 모든 단어마다 topic을 할당해줄 수가 있게 된다. 즉, word-topic 행렬을 완성할 수 있다는 것이다. <code>보통 1,000회 ~ 10,000회 반복  수행하면 그 결과가 수렴한다고 하며, 이를 토대로 문서의 topic 분포, topic 단어 분포 또한 구할 수 있게 된다.</code> $\theta$의 경우 각 문서에 어떤 단어사 쓰였는지 조사해 그 단어의 topic 분포를 더해주는 방식으로 계산한다. 사용자가 지정하는 하이퍼파라메터 α 존재 덕분에 A가 아예 0으로 되는 일을 막을 수 있게 된다. 일종의 smoothing 역할을 한다. 따라서 <code>α가 클수록 토픽들의 분포가 비슷해지고, 작을 수록 특정 토픽이 크게 나타나게 된다. 이는 β가 B에서 차지하는 역할도 동일</code>하다.</li>
</ul>
<p><img src="/image/LDA_probability_with_gibbs_sampling.png" alt="문서 1 두 번째 단어의 topic 추론"></p>
<h4 id="최적-토픽-수-찾기"><a href="#최적-토픽-수-찾기" class="headerlink" title="최적 토픽 수 찾기"></a>최적 토픽 수 찾기</h4><ul>
<li>LDA의 토픽수 K는 여러 실험을 통해 사용자가 지정하는 미지수인 hyper parameter이다. 최적 토픽수를 구하는 데 쓰는 Perplexity 지표있다. p(w)는 클수록 좋은 inference이므로 exp(−log(p(w)))는 작을수록 좋다. 따라서 토픽 수 K를 바꿔가면서 <code>Perplexity를 구한 뒤 가장 작은 값을 내는 K를 최적의 토픽수로 삼으면 된다.</code></li>
</ul>
<script type="math/tex; mode=display">Perplexity(w)=exp\left[ -\frac { log\left\{ p(w) \right\}  }{ \sum _{ d=1 }^{ D }{ \sum _{ j=1 }^{ V }{ { n }^{ jd } }  }  }  \right]</script><h3 id="LDA-실습"><a href="#LDA-실습" class="headerlink" title="LDA 실습"></a>LDA 실습</h3><h4 id="데이터-소개"><a href="#데이터-소개" class="headerlink" title="데이터 소개|"></a>데이터 소개|</h4><ul>
<li>네이버 영화 corpus를 soynlp로 띄어쓰기 교정한 결과를 LDA의 학습 데이터로 사용할 것이다.</li>
</ul>
<ul>
<li>아래의 코드는 LDA 모델 피처를 생성하는 역할을 한다. LDA의 입력값은 문서 내 단어의 등장 순서를 고려하지 않고 해당 단어가 몇 번 쓰였는지 그 빈도만을 따진다. 그런데 ‘노잼! 노잼! 노잼!’ 같이 특정 단어가 중복으로 사용된 문서가 있다면 해당 문서의 topic 분포가 한쪽으로 너무 쏠릴 염려가 있다. 이 때문에 token의 순서를 고려하지 않고 중복을 제거한 형태로 LDA 피처를 만들 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from gensim import corpora</span><br><span class="line">from khaiii import KhaiiiApi</span><br><span class="line">from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_tokenizer(tokenizer_name):</span><br><span class="line">    <span class="keyword">if</span> tokenizer_name == <span class="string">"komoran"</span>:</span><br><span class="line">        tokenizer = Komoran()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"okt"</span>:</span><br><span class="line">        tokenizer = Okt()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"mecab"</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"hannanum"</span>:</span><br><span class="line">        tokenizer = Hannanum()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"kkma"</span>:</span><br><span class="line">        tokenizer = Kkma()</span><br><span class="line">    <span class="keyword">elif</span> tokenizer_name == <span class="string">"khaiii"</span>:</span><br><span class="line">        tokenizer = KhaiiiApi()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        tokenizer = Mecab()</span><br><span class="line">    <span class="built_in">return</span> tokenizer</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">corpus_fname = <span class="string">'./data/processed/corrected_ratings_corpus.txt'</span></span><br><span class="line"></span><br><span class="line">documents, tokenized_corpus = [], []</span><br><span class="line">tokenizer = get_tokenizer(<span class="string">'mecab'</span>)</span><br><span class="line"></span><br><span class="line">with open(corpus_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> f:</span><br><span class="line">        tokens = list(<span class="built_in">set</span>(tokenizer.morphs(document.strip())))</span><br><span class="line">        documents.append(document)</span><br><span class="line">        tokenized_corpus.append(tokens)</span><br><span class="line"></span><br><span class="line">dictionary = corpora.Dictionary(tokenized_corpus)</span><br><span class="line">corpus = [dictionary.doc2bow(text) <span class="keyword">for</span> text <span class="keyword">in</span> tokenized_corpus]</span><br></pre></td></tr></table></figure>
<p><a href="https://radimrehurek.com/gensim/corpora/dictionary.html" target="_blank" rel="noopener">corpora.Dictionary 참조</a></p>
<ul>
<li><p>dictionary형태로 vocabulary dictionary를 만들어주는 것이다.</p>
<ul>
<li>add_documents로 문서를 추가할 수도 있다.</li>
<li>doc2bow는 bag-of-words (BoW) 형태로 문서를 변환시켜준다. [(token_id, token_count)]형태이다.</li>
<li>doc2bow의 옵션으로 return_missing=True를 주면 해당 sentence의 단어중 미등록 단어와 카운트를 같이 출력해준다.</li>
</ul>
</li>
<li><p>아래 코드를 실행하면 LDA를 학습하고 그 결과를 확인 할 수 있다. LdaMulticore에서 num_topicss는 토픽 수(K)에 해당되는 parameter이다. get_document_topics라는 함수는 학습이 끝난 LDA 모델로부터 각 문서별 topic 분포를 리턴한다. minimum_probability 인자를 0.5를 줬는데, 이는 0.5미만의 topic 분포는 무시한다는 뜻이다. 특정 토픽의 확률이 0.5보다 클 경우에만 데이터를 리턴한다. 확률의 합은 1이기 때문에 해당 토픽이 해당 문서에서 확률값이 가장 큰 토픽이 된다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import ldamulticore</span><br><span class="line"></span><br><span class="line">LDA = ldamulticore.LdaMulticore(corpus, id2word=dictionary, num_topics=30, workers=4)</span><br><span class="line"></span><br><span class="line">all_topics = LDA.get_document_topics(corpus, minimum_probability=0.5, per_word_topics=False)</span><br></pre></td></tr></table></figure>
<ul>
<li>아래 결과를 해석하자면, 0번 문서는 전체 topic 30개 중 19번에 해당하는 topic의 확률 값이 제일 높으며 그 값은 0.7227057이다. 3번 문서 같은 경우 전체 topic 중 0.5를 넘는 topic이 없음을 확인 할 수 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> doc_idx, topic <span class="keyword">in</span> enumerate(all_topics[:5]):</span><br><span class="line">    <span class="built_in">print</span>(doc_idx, topic)</span><br></pre></td></tr></table></figure>
<h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">0 [(19, 0.7227057)]</span><br><span class="line">1 [(9, 0.5350808)]</span><br><span class="line">2 []</span><br><span class="line">3 [(19, 0.7778823)]</span><br><span class="line">4 [(14, 0.80464107)]</span><br></pre></td></tr></table></figure>
<h5 id="모델-저장"><a href="#모델-저장" class="headerlink" title="모델 저장"></a>모델 저장</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">output_fname = <span class="string">'./lda'</span></span><br><span class="line">all_topics = LDA.get_document_topics(corpus, minimum_probability=0.5, per_word_topics=False)</span><br><span class="line">with open(output_fname + <span class="string">".results"</span>, <span class="string">'w'</span>) as f:</span><br><span class="line">    <span class="keyword">for</span> doc_idx, topic <span class="keyword">in</span> enumerate(all_topics):</span><br><span class="line">        <span class="keyword">if</span> len(topic) == 1:</span><br><span class="line">            <span class="comment"># tuple 형태로 되어있는 데이터로 가져와서 나눠줌</span></span><br><span class="line">            topic_id, prob = topic[0]</span><br><span class="line">            f.writelines(documents[doc_idx].strip() + <span class="string">"\u241E"</span> + <span class="string">' '</span>.join(tokenized_corpus[doc_idx]) + <span class="string">"\u241E"</span> + str(topic_id) + <span class="string">"\u241E"</span> + str(prob) + <span class="string">"\n"</span>)</span><br><span class="line">LDA.save(output_fname + <span class="string">".model"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="LDA-평가"><a href="#LDA-평가" class="headerlink" title="LDA 평가"></a>LDA 평가</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import LdaModel</span><br><span class="line">from collections import defaultdict</span><br><span class="line"></span><br><span class="line">class LDAEvaluator:</span><br><span class="line"></span><br><span class="line">    def __init__(self, model_path=<span class="string">"./lda"</span>, tokenizer_name=<span class="string">"mecab"</span>):</span><br><span class="line">        self.tokenizer = get_tokenizer(tokenizer_name)</span><br><span class="line">        self.all_topics = self.load_results(model_path + <span class="string">".results"</span>)</span><br><span class="line">        self.model = LdaModel.load(model_path + <span class="string">".model"</span>)</span><br><span class="line"></span><br><span class="line">    def load_results(self, results_fname):</span><br><span class="line">        topic_dict = defaultdict(list)</span><br><span class="line">        with open(results_fname, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) as f:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">                sentence, _, topic_id, prob = line.strip().split(<span class="string">"\u241E"</span>)</span><br><span class="line">                topic_dict[int(topic_id)].append((sentence, <span class="built_in">float</span>(prob)))</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> topic_dict.keys():</span><br><span class="line">            topic_dict[key] = sorted(topic_dict[key], key=lambda x: x[1], reverse=True)</span><br><span class="line">        <span class="built_in">return</span> topic_dict</span><br><span class="line"></span><br><span class="line">    def show_topic_docs(self, topic_id, topn=10):</span><br><span class="line">        <span class="built_in">return</span> self.all_topics[topic_id][:topn]</span><br><span class="line"></span><br><span class="line">    def show_topic_words(self, topic_id, topn=10):</span><br><span class="line">        <span class="built_in">return</span> self.model.show_topic(topic_id, topn=topn)</span><br><span class="line"></span><br><span class="line">    def show_new_document_topic(self, documents):</span><br><span class="line">        tokenized_documents = [self.tokenizer.morphs(document) <span class="keyword">for</span> document <span class="keyword">in</span> documents]</span><br><span class="line">        curr_corpus = [self.model.id2word.doc2bow(tokenized_document) <span class="keyword">for</span> tokenized_document <span class="keyword">in</span> tokenized_documents]</span><br><span class="line">        topics = self.model.get_document_topics(curr_corpus, minimum_probability=0.5, per_word_topics=False)</span><br><span class="line">        <span class="keyword">for</span> doc_idx, topic <span class="keyword">in</span> enumerate(topics):</span><br><span class="line">            <span class="keyword">if</span> len(topic) == 1:</span><br><span class="line">                topic_id, prob = topic[0]</span><br><span class="line">                <span class="built_in">print</span>(documents[doc_idx], <span class="string">", topic id:"</span>, str(topic_id), <span class="string">", prob:"</span>, str(prob))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(documents[doc_idx], <span class="string">", there is no dominant topic"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="모델-초기화"><a href="#모델-초기화" class="headerlink" title="모델 초기화"></a>모델 초기화</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = LDAEvaluator(<span class="string">'./lda'</span>)</span><br></pre></td></tr></table></figure>
<h4 id="topic-문서-확인"><a href="#topic-문서-확인" class="headerlink" title="topic 문서 확인"></a>topic 문서 확인</h4><ul>
<li>show_topic_docs 함수에 topic ID를 인자로 주어 실행하면 해당 topic 확률 값이 가장 높은 문서 상위 10개를 출력한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.show_topic_docs(topic_id=0)</span><br></pre></td></tr></table></figure>
<h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">'내가 가장 좋아하는 영화! 색감과 영상 인물들의 감정이입 대사 한마디 한마디가 너무나 완벽한. 몇번을 봐도 또 보고싶은 영화.'</span>,</span><br><span class="line">  0.9707017),</span><br><span class="line"> (<span class="string">'영화보다가 운적은 정우 주연 영화 바람말고는 없는데 이건 감정이입이 되서 그런지 몰라도 진짜 눈 충혈되도록 펑펑울었음'</span>,</span><br><span class="line">  0.9677608),</span><br><span class="line"> (<span class="string">'세 명품배우, 몰입도 최고의 현출,간결하고 시같은 대사,상처받은 사람들의 아름다운 치유!'</span>, 0.95923144),</span><br><span class="line"> (<span class="string">'"아.. 따듯하다.. ""천국의 아이들"" 못보신 분 꼭 보세요.. 같은 감독임.."'</span>, 0.957968),</span><br><span class="line"> (<span class="string">'몸이 마음처럼 움직여 주지 않는 지체장애우들의 혼신의 노력과 열연이 돋보이는 영화였다'</span>, 0.95778286),</span><br><span class="line"> (<span class="string">'영상미 아름답고 주인공의 사랑이 순수하고 풋풋하다. 한 번 더 보고싶은 영화!'</span>, 0.9539619),</span><br><span class="line"> (<span class="string">'음악이 아름답고 가슴이 뭉클하니 감동적이었어. 마음이 따뜻해지는 영화예요.'</span>, 0.9539556),</span><br><span class="line"> (<span class="string">'피아니스트와 같은 동급 영화라 생각합니다 보시면 후회없을거예요 실화영화이니요'</span>, 0.953952),</span><br><span class="line"> (<span class="string">'그냥 고민말고 보세요.진짜 이건 명작이라는 말로는 부족합니다..꼭 보세요!'</span>, 0.9491169),</span><br><span class="line"> (<span class="string">'영화를 보는내내 감정이 이입되고 첫사랑이 보고싶어지는 그런 영화입니다.'</span>, 0.9491167)]</span><br></pre></td></tr></table></figure>
<h4 id="topic-별-단어-확인"><a href="#topic-별-단어-확인" class="headerlink" title="topic 별 단어 확인"></a>topic 별 단어 확인</h4><ul>
<li>해당 topic ID에서 가장 높은 확률 값을 지니는 단어들 중 상위 n개의 목록을 확인할 수 있다.<ul>
<li><code>어미나 조사가 많이 끼어 있음을 확인할 수 있다. LDA의 품질을 끌어 올리기 위해 피처를 만드는 과정에서 명사만 쓰기도 한다.</code></li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.show_topic_words(topic_id=0)</span><br></pre></td></tr></table></figure>
<h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">'보'</span>, 0.03599964),</span><br><span class="line"> (<span class="string">'는'</span>, 0.03479155),</span><br><span class="line"> (<span class="string">'.'</span>, 0.030642439),</span><br><span class="line"> (<span class="string">'고'</span>, 0.030473521),</span><br><span class="line"> (<span class="string">'영화'</span>, 0.029688885),</span><br><span class="line"> (<span class="string">'이'</span>, 0.018665483),</span><br><span class="line"> (<span class="string">'로'</span>, 0.017465018),</span><br><span class="line"> (<span class="string">'내내'</span>, 0.016784767),</span><br><span class="line"> (<span class="string">'다'</span>, 0.016693212),</span><br><span class="line"> (<span class="string">'한'</span>, 0.013309637)]</span><br></pre></td></tr></table></figure>
<h4 id="새로운-문서의-topic-확인"><a href="#새로운-문서의-topic-확인" class="headerlink" title="새로운 문서의 topic 확인"></a>새로운 문서의 topic 확인</h4><ul>
<li>show_new_document_topic 함수는 새로운 문서의 topic을 확인하는 역할을 한다. 문서를 형태소 분석한 뒤 이를 LDA 모델에 넣어 topic을 추론해 가장 높은 확률 값을 지니는 topic id와 그 확률을 리턴해준다.</li>
</ul>
<ul>
<li>해당 문서의 topic 분포 중 0.5를 넘는 지배적인 topic이 존재하지 않을 경우 ‘there is no dominant topic’메시지를 리턴한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.show_new_document_topic([<span class="string">"너무 사랑스러운 영화"</span>, <span class="string">"인생을 말하는 영화"</span>])</span><br></pre></td></tr></table></figure>
<h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">너무 사랑스러운 영화 , topic id: 28 , prob: 0.8066608</span><br><span class="line">인생을 말하는 영화 , topic id: 9 , prob: 0.7323683</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "NLP 문장 수준 임베딩 - 01",
        "image": "https://heung-bae-lee.github.io/image/read_lines.png",
        "keywords": "",
        "genre": "NLP",
        "datePublished": "2020-02-06",
        "dateCreated": "2020-02-06",
        "dateModified": "2020-02-08",
        "url": "https://heung-bae-lee.github.io/2020/02/06/NLP_08/",
        "description": "참고로 이 모든 내용은 이기창 님의 한국어 임베딩이라는 책을 기반으로 작성하고 있다.문장 수준 임베딩
크게는 행렬 분해, 확률 모형, Neural Network 기반 모델 등 세 종류를 소개할 것이다.

행렬 분해

LSA(잠재 의미 분석)


확률 모형

LDA(잠재 디리클레 할당)


Neural Network

Doc2Vec
ELMo
GPT (tran"
        "wordCount": 6685
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2020/02/08/NLP_09/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            NLP 문장 수준 임베딩 - 02
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2020/02/01/NLP_06/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">NLP - 단어 수준 임베딩</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/06/09/linear_algebra_05/" class="title">Linear Transformation &amp; onto, ono-to-one의 개념</a></p>
                            <p class="item-date"><time datetime="2020-06-09T05:23:12.000Z" itemprop="datePublished">2020-06-09</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/06/08/linear_algebra_04/" class="title">Linear Independence, Span, and Subspace</a></p>
                            <p class="item-date"><time datetime="2020-06-08T06:52:22.000Z" itemprop="datePublished">2020-06-08</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/06/machine_learning_20/" class="title">Imbalanced Data</a></p>
                            <p class="item-date"><time datetime="2020-06-05T16:52:20.000Z" itemprop="datePublished">2020-06-06</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/04/machine_learning_19/" class="title">Clustering - Hierarchical, DBSCAN, Affinity Propagation</a></p>
                            <p class="item-date"><time datetime="2020-06-04T13:46:15.000Z" itemprop="datePublished">2020-06-04</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/30/machine_learning_18/" class="title">Clustering - K-means, K-medoid</a></p>
                            <p class="item-date"><time datetime="2020-05-29T16:01:30.000Z" itemprop="datePublished">2020-05-30</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">21</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/02/06/NLP_08/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
