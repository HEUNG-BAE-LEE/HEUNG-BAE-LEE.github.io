<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>NLP 실습 텍스트 유사도 - 02 (XGBoost, 1D-CNN, MaLSTM) | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="모델은 총 3가지를 종류를 만들어 볼 것이다. XGBoost CNN MaLSTM    XGBoost 앙상블 모델 중 하나인 XGBoost 모델은 ‘eXtream Gradient Boosting’의 약자로 캐글 사용자에 큰 인기를 얻은 모델 중 하나이다. 앙상블 기법이란 여러 개의 학습 알고즘을 사용해 더 좋은 성능을 얻는 방법을 뜻한다. 앙상블 기법에는">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP 실습 텍스트 유사도 - 02 (XGBoost, 1D-CNN, MaLSTM)">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/02/11/NLP_11/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="모델은 총 3가지를 종류를 만들어 볼 것이다. XGBoost CNN MaLSTM    XGBoost 앙상블 모델 중 하나인 XGBoost 모델은 ‘eXtream Gradient Boosting’의 약자로 캐글 사용자에 큰 인기를 얻은 모델 중 하나이다. 앙상블 기법이란 여러 개의 학습 알고즘을 사용해 더 좋은 성능을 얻는 방법을 뜻한다. 앙상블 기법에는">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/bagging_boosting_difference.png">
<meta property="og:updated_time" content="2020-02-11T08:10:01.052Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP 실습 텍스트 유사도 - 02 (XGBoost, 1D-CNN, MaLSTM)">
<meta name="twitter:description" content="모델은 총 3가지를 종류를 만들어 볼 것이다. XGBoost CNN MaLSTM    XGBoost 앙상블 모델 중 하나인 XGBoost 모델은 ‘eXtream Gradient Boosting’의 약자로 캐글 사용자에 큰 인기를 얻은 모델 중 하나이다. 앙상블 기법이란 여러 개의 학습 알고즘을 사용해 더 좋은 성능을 얻는 방법을 뜻한다. 앙상블 기법에는">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/bagging_boosting_difference.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/02/11/nlp_11/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/NLP/">NLP</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-NLP_11" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        NLP 실습 텍스트 유사도 - 02 (XGBoost, 1D-CNN, MaLSTM)
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/02/11/NLP_11/" class="article-date">
            <time datetime="2020-02-10T16:36:58.000Z" itemprop="datePublished">2020-02-11</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <ul>
<li>모델은 총 3가지를 종류를 만들어 볼 것이다.<ul>
<li>XGBoost</li>
<li>CNN</li>
<li>MaLSTM</li>
</ul>
</li>
</ul>
<h2 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h2><ul>
<li><p>앙상블 모델 중 하나인 XGBoost 모델은 ‘eXtream Gradient Boosting’의 약자로 캐글 사용자에 큰 인기를 얻은 모델 중 하나이다. 앙상블 기법이란 여러 개의 학습 알고즘을 사용해 더 좋은 성능을 얻는 방법을 뜻한다. 앙상블 기법에는 Bagging과 Boosting이라는 방법이 있다.</p>
<ul>
<li><p><code>Bagging</code>은 여러 개의 학습 알고리즘, 모델을 통해 각각 결과를 예측하고 모든 결과를 동등하게 보고 취합해서 결과를 얻는 방식이다. Random Forest도 여러개의 decision tree 결과값의 평균을 통해 결과를 얻는 Bagging의 일종이다.</p>
</li>
<li><p><code>Boosting</code>은 여러 알고리즘, 모델의 결과를 순차적으로 취합하는데, 단순히 하나씩 취하는 방법이 아니라 이전 알고리즘, 모델이 학습 후 잘못 예측한 부분에 가중치를 줘서 다시 모델로 가서 학습하는 방식이다.</p>
</li>
</ul>
</li>
</ul>
<p><img src="/image/bagging_boosting_difference.png" alt="bagging과 boosting의 차이"></p>
<ul>
<li><p><code>XGBoost는 Boosting 기법 중 Tree Bossting 기법을 활용한 모델</code>이다. 쉽게 말해 Random Forest와 비슷한 원리에 Boosting 기법을 적용했다고 생각하면된다. 여러개의 Decision Tree를 사용하지만 단순히 결과를 평균내는 것이 아니라 결과를 보고 오답에 대해 가중치를 부여한다. 그리고 가중치가 적용된 오답에 대해서는 관심을 가지고 정답이 될 수 있도록 결과를 만들고 해당 결과에 대한 다른 오답을 찾아 다시 똑같은 작업을 반복적으로 진행하는 것이다.</p>
</li>
<li><p>최종적으로는 XGBoost란 이러한 Tree Boosting 방식에 경사하강법을 통해 optimization을 하는 방법이다. 그리고 연산량을 줄이기 위해 Decision Tree를 구성할 때 병렬 처리를 사용해 빠른 시간에 학습이 가능하다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TRAIN_Q1_DATA_FILE = <span class="string">'q1_train.npy'</span></span><br><span class="line">TRAIN_Q2_DATA_FILE = <span class="string">'q2_train.npy'</span></span><br><span class="line">TRAIN_LABEL_DATA_FILE = <span class="string">'label_train.npy'</span></span><br><span class="line"></span><br><span class="line">train_q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">train_q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">train_labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, <span class="string">'rb'</span>))</span><br></pre></td></tr></table></figure>
<ul>
<li>numpy의 stack 함수를 사용해 두 질문을 하나의 쌍으로 만들었다. 예를 들어, 질문 [A]와 질문 [B]가 있을 때 이 질문을 하나로 묶어 [[A], [B]] 형태로 만들었다. 이와 같은 형태는 다음과 같이 여러가지 방법으로 구현할 수 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_input_expand = np.concatenate((np.expand_dims(train_q1_data, 1), np.expand_dims(train_q2_data, 1)), axis=1)</span><br><span class="line">train_input_expand.shape</span><br></pre></td></tr></table></figure>
<h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(298526, 2, 31)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_input_concate = np.concatenate((train_q1_data[:,np.newaxis,:], train_q2_data[:,np.newaxis,:]), axis=1)</span><br><span class="line">train_input_concate.shape</span><br></pre></td></tr></table></figure>
<h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(298526, 2, 31)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_input_stack = np.stack((train_q1_data, train_q2_data), axis=1)</span><br><span class="line">train_input_stack.shape</span><br></pre></td></tr></table></figure>
<h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(298526, 2, 31)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_input_concate ==  train_input_stack).all() and (train_input_stack == train_input_expand).all() and (train_input_concate == train_input_expand).all()</span><br></pre></td></tr></table></figure>
<h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True</span><br></pre></td></tr></table></figure>
<ul>
<li>전체 29만개 정도의 데이터에 대해 두 질문이 각각 31개의 질문 길이를 가지고 있음을 확인 할 수 있다. 두 질문 쌍이 하나로 묶여 있는 것도 확인할 수 있다. 이제 학습 데이터의 20%를 모델 검증을 위한 validation set으로 만들어 둘 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">train_input, eval_input, train_label, eval_label = train_test_split(train_input_stack, train_labels, test_size=0.2, random_state=4242)</span><br></pre></td></tr></table></figure>
<ul>
<li><p>XGBoost를 사용하려면 입력값을 xgb 라이브러리의 데이터 형식인 DMatrix 형태로 만들어야 한다. 학습 데이터와 검증 데이터 모두 적용해서 해당 데이터 형식으로 만든다. 적용 과정에서 각 데이터에 대해 sum 함수를 사용하는데 이는 각 데이터의 두 질문을 하나의 값으로 만들어 주기 위해서이다. 그리고 두 개의 데이터를 묶어서 하나의 리스트로 만든다. 이때 학습 데이터와 검증 데이터는 각 상태의 문자열과 함께 tuple형태로 구성한다.</p>
</li>
<li><p>참고로 XGBoost와  sklearn의 ensemble.GradientBoostingClassifier은 동일하게 Tree Boosting 모델을 가지고 있지만 속도면에서 XGBoost가 훨씬 빠르다. (사용법도 조금 다름)</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import xgboost as xgb</span><br><span class="line"></span><br><span class="line">train_data = xgb.DMatrix(train_input.sum(axis=1), label=train_label)</span><br><span class="line">eval_data = xgb.DMatrix(eval_input.sum(axis=1), label=eval_label)</span><br><span class="line"></span><br><span class="line">data_list = [(train_data, <span class="string">'train'</span>), (eval_data, <span class="string">'valid'</span>)]</span><br></pre></td></tr></table></figure>
<ul>
<li>우선 모델을 만들고 학습하기 위해 몇 가지 선택해야 하는 옵션은 dictionary를 만들어 넣으면 된다. 이때 dictionary에는 모델의 objective(loss) function와 평가 지표를 정해서 넣어야 하는데 여기서는 우선 objective(loss) function의 경우 이진 로지스틱 함수를 사용한다. 평가 지표의 경우 RMSE를 사용한다. 이렇게 만든 인자와 학습 데이터, 데이터를 반복하는 횟수인 num_boost_round, 모델 검증 시 사용할 전체 데이터 쌍, 그리고 early stopping을 위한 횟수를 정한다. 데이터를 반복하는 횟수, 즉 Epoch을 의미하는 값으로는 1000을 설정했다. 전체 데이터를 1000번 반복해야 끝나도록 설정한 것이다. 그리고 early stopping을 위한 횟수값으로는 10을 설정해서 만약 10 epoch 동안 error값이 크게 줄지 않는다면 학습을 종료시키도록 하였다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;&#125;</span><br><span class="line">params[<span class="string">'objective'</span>] = <span class="string">'binary:logistic'</span></span><br><span class="line">params[<span class="string">'eval_metric'</span>] = <span class="string">'rmse'</span></span><br><span class="line"></span><br><span class="line">bst = xgb.train(params, train_data, num_boost_round = 1000, evals = data_list, early_stopping_rounds=10)</span><br></pre></td></tr></table></figure>
<h3 id="예측하기"><a href="#예측하기" class="headerlink" title="예측하기"></a>예측하기</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_Q1_DATA_FILE = <span class="string">'test_q1.npy'</span></span><br><span class="line">TEST_Q2_DATA_FILE = <span class="string">'test_q2.npy'</span></span><br><span class="line">TEST_ID_DATA_FILE = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA, <span class="string">'rb'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_input = np.stack((test_q1_data, test_q2_data), axis=1)</span><br><span class="line">test_data = xgb.DMatrix(test_input.sum(axis=1))</span><br><span class="line">test_predict = bst.predict(test_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.makedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">output = pd.DataFrame(&#123;<span class="string">'test_id'</span>: test_id_data, <span class="string">'is_duplicate'</span>: test_predict&#125;)</span><br><span class="line">output.to_csv(DATA_OUT_PATH + <span class="string">'sample_xgb.csv'</span>, index=False)</span><br></pre></td></tr></table></figure>
<ul>
<li>kaggle API를 통해서 바로 파일 올려주었다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit quora-question-pairs -f <span class="string">"sample_xgb.csv"</span> -m <span class="string">"XGBoost Model"</span></span><br></pre></td></tr></table></figure>
<ul>
<li>3294팀 중 2714등이다. 물론, 임베딩 벡터라든지 아무런 조치를 취하지 않았기 때문에 score가 안좋을 수 밖에 없다. 추후에 TF-IDF 행렬을 사용하거나 더 좋은 임베딩 기법을 사용해서 다시 올려볼 것이다. 지금은 유사도를 구하는 방법에 대한 기본 튜토리얼이므로 이 정도에서 그치겠다.</li>
</ul>
<p><img src="/image/XGBoost_first_none.png" alt="결과"></p>
<h2 id="CNN-텍스트-유사도-분석-모델"><a href="#CNN-텍스트-유사도-분석-모델" class="headerlink" title="CNN 텍스트 유사도 분석 모델"></a>CNN 텍스트 유사도 분석 모델</h2><ul>
<li>합성곱 신경망 구조를 활용해 텍스트 유사도를 측정하는 모델을 만들어 보겠다. 기본적인 구조는 이전 장의 합성곱 모델과 유사하지만 이번 경우에는 <code>각 데이터가 두 개의 텍스트 문장으로 돼 있기 때문에 병렬적인 구조를 가진 모델을 만들어야 한다.</code></li>
</ul>
<ul>
<li>모델에 입력하고자 하는 데이터는 문장 2개다. 문장에 대한 유사도를 보기 위해서는 기준이 되는 문장이 필요하다. 이를 ‘기준 문장’이라 정의한다. 그리고 ‘기준 문장’에 대해 비교해야 하는 문장이 있는데 이를 ‘대상문장’이라 한다. 만약 모델에 입력하고자 하는 기준 문장이 ‘I love deep NLP’이고 이를 비교할 대상 문장이 ‘Deep NLP is awesome’이라 하자. 이 두 문장은 의미가 상당히 유사하다. 만약 학습이 진행된 후에 두 문장에 대한 유사도를 측정하고하 한다마녀 아마도 높은 유사도 점수를 보일 것이다. 이처럼 문장이 의미적으로 가까우면 유사도 점수는 높게 표현 될 것이고 그렇지 않을 경우에는 낮게 표현될 것이다.</li>
</ul>
<ul>
<li>전반적인 유사도 분석 모델 구조에 대한 흐름을 보자. 모델에 데이터를 입력하기 전에 기준 문장과 대상 문장에 대해서 인덱싱을 거쳐 문자열 형태의 문장을 인덱스 벡터 형태로 구성한다. 인덱스 벡터로 구성된 문장 정보는 임베딩 과정을 통해 각 단어들이 임베딩 벡터로 바뀐 행렬로 구성 될 것이다. 임베딩 과정을 통해 나온 문장 행렬은 기준 문장과 대상 문장 각각에 해당하는 CNN 블록을 거치게 한다. CNN 블록은 Convolution 층과 Max Pooling층을 합친 하나의 신경망을 의미한다. 두 블록을 거쳐 나온 벡터는 문장에 대한 의미 벡터가 된다. 두 문장에 대한 의미 벡터를 가지고 여러 방식으로 유사도를 구할 수 있다. 여기서는 FC layer를 거친 후 최종적으로 logistic regression 방법을 통해 문자 유사도 점수를 측정할 것이다. 이렇게 측정한 점수에 따라 두 문장의 유사 여부를 판단할 것이다.</li>
</ul>
<h3 id="모델-구현-준비"><a href="#모델-구현-준비" class="headerlink" title="모델 구현 준비"></a>모델 구현 준비</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">import json</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">TRAIN_Q1_DATA_FILE = <span class="string">'q1_train.npy'</span></span><br><span class="line">TRAIN_Q2_DATA_FILE = <span class="string">'q2_train.npy'</span></span><br><span class="line">TRAIN_LABEL_DATA_FILE = <span class="string">'label_train.npy'</span></span><br><span class="line">DATA_CONFIGS = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line">TEST_SPLIT = 0.1</span><br><span class="line">RNG_SEED = 13371447</span><br></pre></td></tr></table></figure>
<ul>
<li>모델 파라미터 설정</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">EPOCH=10</span><br><span class="line">BATCH_SIZE=1024</span><br><span class="line"></span><br><span class="line">MAX_SEQUENCE_LENGTH = 26 <span class="comment"># 31</span></span><br><span class="line"></span><br><span class="line">WORD_EMBEDDING_DIM = 100</span><br><span class="line">CONV_FEATURE_DIM = 300</span><br><span class="line">CONV_OUTPUT_DIM = 128</span><br><span class="line">CONV_WINDOW_SIZE = 3</span><br><span class="line">SIMILARITY_DENSE_FEATURE_DIM = 200</span><br><span class="line"></span><br><span class="line">prepro_configs = None</span><br><span class="line"></span><br><span class="line">with open(DATA_IN_PATH + DATA_CONFIGS, <span class="string">'r'</span>) as f:</span><br><span class="line">    prepro_configs = json.load(f)</span><br><span class="line"></span><br><span class="line">VOCAB_SIZE = prepro_configs[<span class="string">'vocab_size'</span>] <span class="comment">#76464개</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, <span class="string">'rb'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">X = np.stack((q1_data, q2_data), axis=1)</span><br><span class="line">y = labels</span><br><span class="line"></span><br><span class="line">train_X, eval_X, train_y, eval_y = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)</span><br><span class="line"></span><br><span class="line">train_Q1 = train_X[:, 0]</span><br><span class="line">train_Q2 = train_X[:, 1]</span><br><span class="line">eval_Q1 = eval_X[:,0]</span><br><span class="line">eval_Q2 = eval_X[:,1]</span><br></pre></td></tr></table></figure>
<ul>
<li><p>estimator에 활용할 데이터 입력 함수를 만들 것이다.</p>
<ul>
<li>map 함수</li>
<li>학습 입력 함수</li>
<li>검증 입력 함수</li>
</ul>
</li>
<li><p>우선 map 함수로 정의한 rearrange 함수부터 설명하면 3개의 값이 인자로 들어오는데, 각각 기준 질문, 대상 질문, 라벨값이다. 이렇게 들어온 인자 값을 통해 2개의 질문을 하나의 dictionary 형태의 입력값으로 만든다. 그리고 이렇게 만든 dictionary와 label을 return하는 구조로 돼 있다. 이 함수를 학습 입력함수와 검증 입력 함수에 적용할 것이다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">def rearrange(base, hypothesis, label):</span><br><span class="line">  features = &#123;<span class="string">'x1'</span> : base, <span class="string">'x2'</span> : hypothesis&#125;</span><br><span class="line">  <span class="built_in">return</span> features, label</span><br><span class="line"></span><br><span class="line">def train_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=100)</span><br><span class="line">  dataset = dataset.batch(16)</span><br><span class="line">  dataset = dataset.map(rearrange)</span><br><span class="line">  dataset = dataset.repeat(EPOCH)</span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br><span class="line"></span><br><span class="line">def eval_input_fn():</span><br><span class="line">  dataset = tf.data.Dataset.from_tensor_slices((eval_Q1, eval_Q2, eval_y))</span><br><span class="line">  dataset = dataset.shuffle(buffer_size=100)</span><br><span class="line">  dataset = dataset.batch(16)</span><br><span class="line">  dataset = dataset.map(rearrange)</span><br><span class="line">  iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> iterator.get_next()</span><br></pre></td></tr></table></figure>
<h3 id="모델-구현"><a href="#모델-구현" class="headerlink" title="모델 구현"></a>모델 구현</h3><ul>
<li>CNN 블록 함수를 먼저 정의할 것이다. CNN 블록 함수는 convolution layer와 Pooling, Dense를 하나로 합친 형태로 정의할 것이다.</li>
</ul>
<ul>
<li>이 함수는 2개의 인자값을 받는데, 각각 입력값과 이름을 의미한다. 이 함수에서 합성곱의 경우 이전 장의 CNN 모델을 구성할 때와 동일하게 Conv1D를 사용할 것이다. Max Pooling도 마찬사지로 MaxPooling1D 객체를 활용한다. 그리고 이렇게 합성곱과 Max Pooling을 적용한 값에 대해 차원을 바꾸기 위해 Dense 층을 통과 시킨다.  </li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def basic_conv_sementic_network(inputs, name):</span><br><span class="line">  conv_layer = tf.keras.layers.Conv1D(CONV_FEATURE_DIM,</span><br><span class="line">                                      CONV_WINDOW_SIZE,</span><br><span class="line">                                      activation=tf.nn.relu,</span><br><span class="line">                                      name=name + <span class="string">'conv_1d'</span>,</span><br><span class="line">                                      padding=<span class="string">'same'</span>)(inputs) <span class="comment">#1024 X 26 X 300</span></span><br><span class="line"></span><br><span class="line">  max_pool_layer = tf.keras.layers.MaxPool1D(MAX_SEQUENCE_LENGTH, 1)(conv_layer) <span class="comment"># 1024 X 1 X 300</span></span><br><span class="line"></span><br><span class="line">  output_layer = tf.keras.layers.Dense(CONV_OUTPUT_DIM, activation=tf.nn.relu, name=name + <span class="string">'dense'</span>)(max_pool_layer) <span class="comment">#1024 X 1 X 128</span></span><br><span class="line"></span><br><span class="line">  output_layer = tf.squeeze(output_layer, 1) <span class="comment"># 1024 X 128</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span> output_layer</span><br></pre></td></tr></table></figure>
<ul>
<li>이제 모델 함수를 설명 할 것이다. 먼저 현재 튜토리얼은 임베딩 벡터에 크게 신경쓰지 않고 모델의 구조에 대해 집중하는 튜토리얼이므로 특별한 기법 없이 tf.keras.layers.Embedding으로 임베딩 벡터를 만들어준 뒤 Conv1D 구조를 3번 거쳐 최종적으로 dense layer를 통해 1개의 노드로 맞춰준 logit 값을 sigmoid함수를 통해 마치 로지스틱 회귀와 같은 구조를 만들어 줄 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">def model_fn(features, labels, mode):</span><br><span class="line">  TRAIN = mode == tf.estimator.ModeKeys.TRAIN</span><br><span class="line">  EVAL = mode == tf.estimator.ModeKeys.EVAL</span><br><span class="line">  PREDICT = mode == tf.estimator.ModeKeys.PREDICT</span><br><span class="line"></span><br><span class="line">  embedding = tf.keras.layers.Embedding(VOCAB_SIZE, WORD_EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">  base_embedded_matrix = embedding(features[<span class="string">'x1'</span>]) <span class="comment"># 1024 X 31 X 100</span></span><br><span class="line">  hypothesis_embedded_matrix = embedding(features[<span class="string">'x2'</span>]) <span class="comment"># 1024 X 31 X 100</span></span><br><span class="line"></span><br><span class="line">  base_embedded_matrix = tf.keras.layers.Dropout(0.2)(base_embedded_matrix)</span><br><span class="line">  hypothesis_embedded_matrix = tf.keras.layers.Dropout(0.2)(hypothesis_embedded_matrix)</span><br><span class="line"></span><br><span class="line">  conv_layer_base_first = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, CONV_WINDOW_SIZE, activation=tf.nn.relu, padding=<span class="string">'same'</span>)(base_embedded_matrix) <span class="comment">#1024 X 31 X 300</span></span><br><span class="line">  max_pool_layer_base_first = tf.keras.layers.MaxPool1D(2, 1)(conv_layer_base_first) <span class="comment"># 1024 X 30 X 300</span></span><br><span class="line"></span><br><span class="line">  conv_layer_hypothesis_first = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, CONV_WINDOW_SIZE, activation=tf.nn.relu, padding=<span class="string">'same'</span>)(hypothesis_embedded_matrix) <span class="comment">#1024 X 31 X 300</span></span><br><span class="line">  max_pool_layer_hypothesis_first = tf.keras.layers.MaxPool1D(2, 1)(conv_layer_hypothesis_first) <span class="comment"># 1024 X 30 X 300</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  conv_layer_base_second = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, 5, activation=tf.nn.relu, padding=<span class="string">'same'</span>)(max_pool_layer_base_first) <span class="comment">#1024 X 30 X 300</span></span><br><span class="line">  max_pool_layer_base_second = tf.keras.layers.MaxPool1D(5, 1)(conv_layer_base_second) <span class="comment"># 1024 X 26 X 300</span></span><br><span class="line"></span><br><span class="line">  conv_layer_hypothesis_second = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, 5, activation=tf.nn.relu, padding=<span class="string">'same'</span>)(max_pool_layer_hypothesis_first) <span class="comment">#1024 X 30 X 300</span></span><br><span class="line">  max_pool_layer_hypothesis_second = tf.keras.layers.MaxPool1D(5, 1)(conv_layer_hypothesis_second) <span class="comment"># 1024 X 26 X 300</span></span><br><span class="line"></span><br><span class="line">  base_sementic_matrix = basic_conv_sementic_network(max_pool_layer_base_second, <span class="string">'base'</span>) <span class="comment"># 1024 X 128</span></span><br><span class="line">  hypothesis_sementic_matrix = basic_conv_sementic_network(max_pool_layer_hypothesis_second, <span class="string">'hypothesis'</span>) <span class="comment"># 1024 X 128</span></span><br><span class="line"></span><br><span class="line">  merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1) <span class="comment"># 1024 X 256</span></span><br><span class="line"></span><br><span class="line">  similarity_dense_layer = tf.keras.layers.Dense(SIMILARITY_DENSE_FEATURE_DIM, activation=tf.nn.relu)(merged_matrix) <span class="comment"># 1024 X 200</span></span><br><span class="line"></span><br><span class="line">  similarity_dense_layer = tf.keras.layers.Dropout(0.2)(similarity_dense_layer)</span><br><span class="line"></span><br><span class="line">  logit_layer = tf.keras.layers.Dense(1)(similarity_dense_layer) <span class="comment"># 1024 X 1</span></span><br><span class="line">  logit_layer = tf.squeeze(logit_layer, 1) <span class="comment"># (1024, )</span></span><br><span class="line">  similarity = tf.nn.sigmoid(logit_layer)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> PREDICT:</span><br><span class="line">        <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                  mode=mode,</span><br><span class="line">                  predictions=&#123;</span><br><span class="line">                      <span class="string">'is_duplicate'</span>:similarity</span><br><span class="line">                  &#125;)</span><br><span class="line"></span><br><span class="line">  loss = tf.losses.sigmoid_cross_entropy(labels, logit_layer)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> EVAL:</span><br><span class="line">      accuracy = tf.metrics.accuracy(labels, tf.round(similarity))</span><br><span class="line">      <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                mode=mode,</span><br><span class="line">                eval_metric_ops= &#123;<span class="string">'acc'</span>: accuracy&#125;,</span><br><span class="line">                loss=loss)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> TRAIN:</span><br><span class="line">      global_step = tf.train.get_global_step()</span><br><span class="line">      train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">      <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                mode=mode,</span><br><span class="line">                train_op=train_op,</span><br><span class="line">                loss=loss)</span><br></pre></td></tr></table></figure>
<ul>
<li>먼저, 변수값 등 모델과 관련된 내용을 담은 체크포인트 파일을 저장할 경로를 설정해야한다. 경로를 지정한 후 해당 경로가 없다면 생성하고 Estimator 객체를 생성할 때 해당 경로를 설정한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"><span class="keyword">if</span> not os.path.exists(DATA_OUT_PATH):</span><br><span class="line">  os.akedirs(DATA_OUT_PATH)</span><br><span class="line"></span><br><span class="line">est = tf.estimator.Estimator(model_fn, model_dir=DATA_OUT_PATH + <span class="string">'checkpoint'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">est.train(train_input_fn)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">est.evaluate(eval_input_fn)</span><br></pre></td></tr></table></figure>
<h3 id="데이터-제출"><a href="#데이터-제출" class="headerlink" title="데이터 제출"></a>데이터 제출</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_Q1_DATA_FILE = <span class="string">'test_q1.npy'</span></span><br><span class="line">TEST_Q2_DATA_FILE = <span class="string">'test_q2.npy'</span></span><br><span class="line">TEST_ID_DATA_FILE = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br><span class="line">test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br><span class="line">test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br></pre></td></tr></table></figure>
<ul>
<li>입력 함수나 검증 함수처럼 별도의 함수로 정의하지 않고 Estimator의 기본 numpy_input_fn 함수를 사용한다. 입력 형태는 앞서 다른 입력 함수와 마찬가지로 두 질문을 dictionary 형태로 만들었다. 그리고 shffle=False로 설정하는데 이는 두 개의 질문쌍이 같은 순서로 입력돼야 하기 때문이다. 이제 이 함수를 활용해 Estimator의 predict 함수를 실행할 것이다.</li>
</ul>
<ul>
<li>predict 함수를 활용하고 반복문을 통해 데이터에 대한 예측값을 받을 수 있게 한다. 이때 받고자 하는 예측값에 대해서는 is_duplicate라는 key 값으로 정의했기 때문에 아래와 같이 유사도 예측값만을 받을 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">"x1"</span>:test_q1_data,</span><br><span class="line">                                                         <span class="string">"x2"</span>:test_q2_data&#125;,</span><br><span class="line">                                                      shuffle=False)</span><br><span class="line"></span><br><span class="line">predictions = np.array([p[<span class="string">'is_duplicate'</span>] <span class="keyword">for</span> p <span class="keyword">in</span> est.predict(input_fn=predict_input_fn)])</span><br><span class="line"></span><br><span class="line">output = pd.DataFrame( data=&#123;<span class="string">"test_id"</span>:test_id_data, <span class="string">"is_duplicate"</span>: list(predictions)&#125; )</span><br><span class="line">output.to_csv(<span class="string">"cnn_predict.csv"</span>, index=False, quoting=3)</span><br></pre></td></tr></table></figure>
<h5 id="kaggle-제출"><a href="#kaggle-제출" class="headerlink" title="kaggle 제출"></a>kaggle 제출</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit quora-question-pairs -f <span class="string">"cnn_predict.csv"</span> -m <span class="string">"cnn conv1d 3layer 10 Epoches"</span></span><br></pre></td></tr></table></figure>
<p><img src="/image/CNN_3layer_10EPOCHs.png" alt="CNN 3 layer를 통한 결과"></p>
<h2 id="MaLSTM"><a href="#MaLSTM" class="headerlink" title="MaLSTM"></a>MaLSTM</h2><ul>
<li>마지막으로 텍스트 유사도 측정을 위해 사용할 모델은 MaLSTM 모델이다. 순서가 있는 입력 데이터에 적합하다는 평을 받는 RNN 모델을 통해 유사도를 측정한다.</li>
</ul>
<ul>
<li><p>유사도를 구하기 위해 활용하는 대표적인 모델인 MaLSTM 모델은 2016년 MIT에서 Jonas Mueller가 쓴 “Siamese Recurrent Architectures for Learning Sentence Similarity”라는 논문에서 처음 소개 되었다. <code>MaLSTM이란 Manhattan Distance + LSTM 의 줄임말로써, 일반적으로 문장의 유사도를 계산할 때 코사인 유사도를 사용하는 대신 맨하탄 거리를 사용하는 모델</code>이다.</p>
</li>
<li><p>이전의 합성곱 신경망 모델에서도 두 개의 문장 입력값에 대해 각각 합성곱 층을 적용한 후 최종적으로 각 문장에 대해 의미 벡터를 각각 뽑아내서 concatenate한 후 dese layer를 통해 선형 변환 해준 뒤 로지스틱 모형과 같이 값을 구해 두 문장의 유사도를 구했다. 이번에는 맨하탄 거리로 비교하는 형태의 모델로서, LSTM의 마지막 스텝의 LSTM hidden state는 문장의 모든 단어에 대한 정보가 반영된 값으로 전체 문장을 대표하는 벡터가 된다. 이렇게 뽑은 두 벡터에 대해 맨하탄 거리를 계산해서 두 문장 사이의 유사도를 측정 할 것이다. 그리고 이렇게 계산한 유사도를 실제 라벨과 비교해서 학습하는 방식으로 모델을 설계할 것이다.</p>
</li>
</ul>
<p><img src="/image/MaLSTM_structure.png" alt="MaLSTM의 모델 구조"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import sys</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">import json</span><br></pre></td></tr></table></figure>
<h3 id="모델-구현-1"><a href="#모델-구현-1" class="headerlink" title="모델 구현"></a>모델 구현</h3><ul>
<li>미리 Global 변수를 지정하자. 파일 명, 파일 위치, 디렉토리 등이 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">DATA_IN_PATH = <span class="string">'/content/'</span></span><br><span class="line">DATA_OUT_PATH = <span class="string">'/content/'</span></span><br><span class="line"></span><br><span class="line">TRAIN_Q1_DATA_FILE = <span class="string">'q1_train.npy'</span></span><br><span class="line">TRAIN_Q2_DATA_FILE = <span class="string">'q2_train.npy'</span></span><br><span class="line">TRAIN_LABEL_DATA_FILE = <span class="string">'label_train.npy'</span></span><br><span class="line">NB_WORDS_DATA_FILE = <span class="string">'data_configs.json'</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 학습에 필요한 파라메터들에 대해서 지정하는 부분이다.</span></span><br><span class="line"><span class="comment">## CPU에서는 Epoch 크기를 줄이는 걸 권장한다.</span></span><br><span class="line">BATCH_SIZE = 4096</span><br><span class="line">EPOCH = 50</span><br><span class="line">HIDDEN = 64</span><br><span class="line"></span><br><span class="line">NUM_LAYERS = 3</span><br><span class="line">DROPOUT_RATIO = 0.2</span><br><span class="line"></span><br><span class="line">TEST_SPLIT = 0.1</span><br><span class="line">RNG_SEED = 13371447</span><br><span class="line">EMBEDDING_DIM = 128</span><br><span class="line">MAX_SEQ_LEN = 31</span><br></pre></td></tr></table></figure>
<h3 id="데이터-불러오기"><a href="#데이터-불러오기" class="headerlink" title="데이터 불러오기"></a>데이터 불러오기</h3><ul>
<li>데이터를 불러오는 부분이다. 효과적인 데이터 불러오기를 위해, 미리 넘파이 형태로 저장시킨 데이터를 로드한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">q1_data = np.load(open(DATA_IN_PATH + TRAIN_Q1_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">q2_data = np.load(open(DATA_IN_PATH + TRAIN_Q2_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">labels = np.load(open(DATA_IN_PATH + TRAIN_LABEL_DATA_FILE, <span class="string">'rb'</span>))</span><br><span class="line">prepro_configs = None</span><br><span class="line"></span><br><span class="line">with open(DATA_IN_PATH + NB_WORDS_DATA_FILE, <span class="string">'r'</span>) as f:</span><br><span class="line">    prepro_configs = json.load(f)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">VOCAB_SIZE = prepro_configs[<span class="string">'vocab_size'</span>]</span><br><span class="line">BUFFER_SIZE = len(labels)</span><br></pre></td></tr></table></figure>
<h3 id="테스트-및-검증-데이터-나누기"><a href="#테스트-및-검증-데이터-나누기" class="headerlink" title="테스트 및 검증 데이터 나누기"></a>테스트 및 검증 데이터 나누기</h3><ul>
<li>데이터를 나누어 저장하자. sklearn의 train_test_split을 사용하면 유용하다. 하지만, 쿼라 데이터의 경우는 입력이 1개가 아니라 2개이다. 따라서, np.stack을 사용하여 두개를 하나로 쌓은다음 활용하여 분류한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">X = np.stack((q1_data, q2_data), axis=1)</span><br><span class="line">y = labels</span><br><span class="line">train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)</span><br><span class="line"></span><br><span class="line">train_Q1 = train_X[:,0]</span><br><span class="line">train_Q2 = train_X[:,1]</span><br><span class="line">test_Q1 = test_X[:,0]</span><br><span class="line">test_Q2 = test_X[:,1]</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def rearrange(base, hypothesis, labels):</span><br><span class="line">    features = &#123;<span class="string">"base"</span>: base, <span class="string">"hypothesis"</span>: hypothesis&#125;</span><br><span class="line">    <span class="built_in">return</span> features, labels</span><br><span class="line"></span><br><span class="line">def train_input_fn():</span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y))</span><br><span class="line">    dataset = dataset.shuffle(buffer_size=len(train_Q1))</span><br><span class="line">    dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">    dataset = dataset.map(rearrange)</span><br><span class="line">    dataset = dataset.repeat(EPOCH)</span><br><span class="line">    iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> iterator.get_next()</span><br><span class="line"></span><br><span class="line">def eval_input_fn():</span><br><span class="line">    dataset = tf.data.Dataset.from_tensor_slices((test_Q1, test_Q2, test_y))</span><br><span class="line">    dataset = dataset.batch(BATCH_SIZE)</span><br><span class="line">    dataset = dataset.map(rearrange)</span><br><span class="line">    iterator = dataset.make_one_shot_iterator()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> iterator.get_next()</span><br></pre></td></tr></table></figure>
<h3 id="모델-설계"><a href="#모델-설계" class="headerlink" title="모델 설계"></a>모델 설계</h3><ul>
<li><p>양방향 LSTM을 사용할 것이다. 즉, 2개의 LSTM을 먼저 정의해야 한다. 정방향 LSTM층과 역방향 LSTM 층을 먼저 정의할 것이다. 그리고 나선 이 2개의 LSTM 층에 데이터를 적용한 후 결과값을 하나로 concatenate한다.</p>
</li>
<li><p>양방향 순환 신경망 함수의 경우 2개의 return 값이 있는데, 하나는 순환 신경망의 출력 값이고, 나머지 하나는 순환 신경망 마지막 스텝의 hidden state 벡터 값이다. 사용해야 할 것은 마지막 hidden state 벡터이므로 각각 q_output_states와 sim_output_states로 할당한다. 이렇게 뽑은 hidden state 벡터의 경우 해당 모델이 <code>양방향 순환 신경망을 활용해 2개의 hidden state 값을 concatenate하여 하나의 벡터로 만든다. 이는 순환 신경망이 문장의 순방향과 역방향 모두 학습함으로써 성능 개선에 도움을 준다.</code></p>
</li>
<li><p>맨하탄 거리의 경우 두 벡터를 뺀 후 절대값을 취하면 된다. 이렇게 뺀 값의 경우 벡터 형태이기 때문에 하나의 상수, 즉 scalar값으로 만들기 위해 reduce_sum 함수를 이용한다. 이렇게 되면 구한 값이 0~1사이의 값을 갖게 될 것이다.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">def Malstm(features, labels, mode):</span><br><span class="line"></span><br><span class="line">    TRAIN = mode == tf.estimator.ModeKeys.TRAIN</span><br><span class="line">    EVAL = mode == tf.estimator.ModeKeys.EVAL</span><br><span class="line">    PREDICT = mode == tf.estimator.ModeKeys.PREDICT</span><br><span class="line"></span><br><span class="line">    def basic_bilstm_network(inputs, name):</span><br><span class="line">        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):</span><br><span class="line">            lstm_fw = [</span><br><span class="line">                    tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(HIDDEN), output_keep_prob=DROPOUT_RATIO)</span><br><span class="line">                    <span class="keyword">for</span> layer <span class="keyword">in</span> range(NUM_LAYERS)</span><br><span class="line">                    ]</span><br><span class="line">            lstm_bw = [</span><br><span class="line">                    tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(HIDDEN), output_keep_prob=DROPOUT_RATIO)</span><br><span class="line">                    <span class="keyword">for</span> layer <span class="keyword">in</span> range(NUM_LAYERS)</span><br><span class="line">                    ]</span><br><span class="line"></span><br><span class="line">            multi_lstm_fw = tf.nn.rnn_cell.MultiRNNCell(lstm_fw)</span><br><span class="line">            multi_lstm_bw = tf.nn.rnn_cell.MultiRNNCell(lstm_bw)</span><br><span class="line"></span><br><span class="line">            (fw_outputs, bw_outputs), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw=multi_lstm_fw,</span><br><span class="line">                                                cell_bw=multi_lstm_bw,</span><br><span class="line">                                                inputs=inputs,</span><br><span class="line">                                                dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">            outputs = tf.concat([fw_outputs, bw_outputs], 2)</span><br><span class="line"></span><br><span class="line">            <span class="built_in">return</span> outputs[:,-1,:]</span><br><span class="line"></span><br><span class="line">    embedding = tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">    base_embedded_matrix = embedding(features[<span class="string">'base'</span>])</span><br><span class="line">    hypothesis_embedded_matrix = embedding(features[<span class="string">'hypothesis'</span>])</span><br><span class="line"></span><br><span class="line">    base_sementic_matrix = basic_bilstm_network(base_embedded_matrix, <span class="string">'base'</span>)</span><br><span class="line">    hypothesis_sementic_matrix = basic_bilstm_network(hypothesis_embedded_matrix, <span class="string">'hypothesis'</span>)</span><br><span class="line"></span><br><span class="line">    base_sementic_matrix = tf.keras.layers.Dropout(DROPOUT_RATIO)(base_sementic_matrix)</span><br><span class="line">    hypothesis_sementic_matrix = tf.keras.layers.Dropout(DROPOUT_RATIO)(hypothesis_sementic_matrix)</span><br><span class="line"></span><br><span class="line"><span class="comment">#     merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1)</span></span><br><span class="line"><span class="comment">#     logit_layer = tf.keras.layers.dot([base_sementic_matrix, hypothesis_sementic_matrix], axes=1, normalize=True)    </span></span><br><span class="line"><span class="comment">#     logit_layer = K.exp(-K.sum(K.abs(base_sementic_matrix - hypothesis_sementic_matrix), axis=1, keepdims=True))</span></span><br><span class="line"></span><br><span class="line">    logit_layer = tf.exp(-tf.reduce_sum(tf.abs(base_sementic_matrix - hypothesis_sementic_matrix), axis=1, keepdims=True))</span><br><span class="line">    logit_layer = tf.squeeze(logit_layer, axis=-1)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> PREDICT:</span><br><span class="line">        <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                  mode=mode,</span><br><span class="line">                  predictions=&#123;</span><br><span class="line">                      <span class="string">'is_duplicate'</span>:logit_layer</span><br><span class="line">                  &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#prediction 진행 시, None</span></span><br><span class="line">    <span class="keyword">if</span> labels is not None:</span><br><span class="line">        labels = tf.to_float(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment">#     loss = tf.reduce_mean(tf.keras.metrics.binary_crossentropy(y_true=labels, y_pred=logit_layer))</span></span><br><span class="line">    loss = tf.losses.mean_squared_error(labels=labels, predictions=logit_layer)</span><br><span class="line"><span class="comment">#     loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(labels, logit_layer))</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> EVAL:</span><br><span class="line">        accuracy = tf.metrics.accuracy(labels, tf.round(logit_layer))</span><br><span class="line">        eval_metric_ops = &#123;<span class="string">'acc'</span>: accuracy&#125;</span><br><span class="line">        <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                  mode=mode,</span><br><span class="line">                  eval_metric_ops= eval_metric_ops,</span><br><span class="line">                  loss=loss)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">elif</span> TRAIN:</span><br><span class="line"></span><br><span class="line">        global_step = tf.train.get_global_step()</span><br><span class="line">        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">return</span> tf.estimator.EstimatorSpec(</span><br><span class="line">                  mode=mode,</span><br><span class="line">                  train_op=train_op,</span><br><span class="line">                  loss=loss)</span><br></pre></td></tr></table></figure>
<h3 id="학습-및-평가"><a href="#학습-및-평가" class="headerlink" title="학습 및 평가"></a>학습 및 평가</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># os.environ["CUDA_VISIBLE_DEVICES"]="0" #For GPU</span></span><br><span class="line"></span><br><span class="line">model_dir = os.path.join(os.getcwd(), DATA_OUT_PATH + <span class="string">"checkpoint/rnn2/"</span>)</span><br><span class="line">os.makedirs(model_dir, exist_ok=True)</span><br><span class="line"></span><br><span class="line">config_tf = tf.estimator.RunConfig()</span><br><span class="line"></span><br><span class="line">lstm_est = tf.estimator.Estimator(Malstm, model_dir=model_dir)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lstm_est.train(train_input_fn)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lstm_est.evaluate(eval_input_fn)</span><br></pre></td></tr></table></figure>
<h3 id="테스트-데이터-예측-및-캐글-제출하기"><a href="#테스트-데이터-예측-및-캐글-제출하기" class="headerlink" title="테스트 데이터 예측 및 캐글 제출하기"></a>테스트 데이터 예측 및 캐글 제출하기</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">TEST_Q1_DATA_FILE = <span class="string">'test_q1.npy'</span></span><br><span class="line">TEST_Q2_DATA_FILE = <span class="string">'test_q2.npy'</span></span><br><span class="line">TEST_ID_DATA_FILE = <span class="string">'test_id.npy'</span></span><br><span class="line"></span><br><span class="line">test_q1_data = np.load(open(DATA_IN_PATH + TEST_Q1_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br><span class="line">test_q2_data = np.load(open(DATA_IN_PATH + TEST_Q2_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br><span class="line">test_id_data = np.load(open(DATA_IN_PATH + TEST_ID_DATA_FILE, <span class="string">'rb'</span>), allow_pickle=True)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">predict_input_fn = tf.estimator.inputs.numpy_input_fn(x=&#123;<span class="string">"base"</span>:test_q1_data,</span><br><span class="line">                                                         <span class="string">"hypothesis"</span>:test_q2_data&#125;,</span><br><span class="line">                                                      shuffle=False)</span><br><span class="line"></span><br><span class="line">predictions = np.array([p[<span class="string">'is_duplicate'</span>] <span class="keyword">for</span> p <span class="keyword">in</span> lstm_est.predict(input_fn=predict_input_fn)])</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(len(predictions)) <span class="comment">#2345796</span></span><br><span class="line"></span><br><span class="line">output = pd.DataFrame( data=&#123;<span class="string">"test_id"</span>:test_id_data, <span class="string">"is_duplicate"</span>: list(predictions)&#125; )</span><br><span class="line">output.to_csv( <span class="string">"rnn_predict.csv"</span>, index=False, quoting=3 )</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!kaggle competitions submit quora-question-pairs -f <span class="string">"rnn_predict.csv"</span> -m <span class="string">"MaLSTM Model with 5layers BiLSTM 50 Epoches"</span></span><br></pre></td></tr></table></figure>
<p><img src="/image/MaLSTM_result_with_50 Epoches.png" alt="MaLSTM 결과"></p>
<h5 id="구조에-대한-튜토리얼형식으로-만들다보니-임베딩의-질이-떨어진다면-머신러닝-기법이-딥러닝-방식보다-결과가-더-좋을-수-있다는-사실을-다시-한번-체감할-수-있는-작업이었다-추후에-TF-IDF행렬-Word2Vec과-문장-단위-LSA를-시행해-얻은-임베딩을-사용하여-다시-한번-결과를-비교할-것이다"><a href="#구조에-대한-튜토리얼형식으로-만들다보니-임베딩의-질이-떨어진다면-머신러닝-기법이-딥러닝-방식보다-결과가-더-좋을-수-있다는-사실을-다시-한번-체감할-수-있는-작업이었다-추후에-TF-IDF행렬-Word2Vec과-문장-단위-LSA를-시행해-얻은-임베딩을-사용하여-다시-한번-결과를-비교할-것이다" class="headerlink" title="구조에 대한 튜토리얼형식으로 만들다보니 임베딩의 질이 떨어진다면 머신러닝 기법이 딥러닝 방식보다 결과가 더 좋을 수 있다는 사실을 다시 한번 체감할 수 있는 작업이었다. 추후에 TF-IDF행렬, Word2Vec과 문장 단위 LSA를 시행해 얻은 임베딩을 사용하여 다시 한번 결과를 비교할 것이다."></a>구조에 대한 튜토리얼형식으로 만들다보니 임베딩의 질이 떨어진다면 머신러닝 기법이 딥러닝 방식보다 결과가 더 좋을 수 있다는 사실을 다시 한번 체감할 수 있는 작업이었다. 추후에 TF-IDF행렬, Word2Vec과 문장 단위 LSA를 시행해 얻은 임베딩을 사용하여 다시 한번 결과를 비교할 것이다.</h5>
        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "NLP 실습 텍스트 유사도 - 02 (XGBoost, 1D-CNN, MaLSTM)",
        "image": "https://heung-bae-lee.github.io/image/bagging_boosting_difference.png",
        "keywords": "",
        "genre": "NLP",
        "datePublished": "2020-02-11",
        "dateCreated": "2020-02-11",
        "dateModified": "2020-02-11",
        "url": "https://heung-bae-lee.github.io/2020/02/11/NLP_11/",
        "description": "
모델은 총 3가지를 종류를 만들어 볼 것이다.
XGBoost
CNN
MaLSTM



XGBoost
앙상블 모델 중 하나인 XGBoost 모델은 ‘eXtream Gradient Boosting’의 약자로 캐글 사용자에 큰 인기를 얻은 모델 중 하나이다. 앙상블 기법이란 여러 개의 학습 알고즘을 사용해 더 좋은 성능을 얻는 방법을 뜻한다. 앙상블 기법에는 "
        "wordCount": 4977
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2020/02/11/NLP_12/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            NLP 실습 유사도를 반영한 검색 키워드 최적화
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2020/02/10/NLP_10/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">NLP 실습 텍스트 유사도 - 01 (데이터 EDA 및 전처리)</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></p>
                            <p class="item-title"><a href="/2020/04/30/data_structure_04/" class="title">내가 정리하는 자료구조 03 - 시간복잡도</a></p>
                            <p class="item-date"><time datetime="2020-04-30T09:12:50.000Z" itemprop="datePublished">2020-04-30</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></p>
                            <p class="item-title"><a href="/2020/04/28/data_structure_03/" class="title">내가 정리하는 자료구조 02 Linked List</a></p>
                            <p class="item-date"><time datetime="2020-04-27T19:41:46.000Z" itemprop="datePublished">2020-04-28</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></p>
                            <p class="item-title"><a href="/2020/04/27/data_structure_02/" class="title">내가 정리하는 자료구조 01 Stack</a></p>
                            <p class="item-date"><time datetime="2020-04-27T12:38:39.000Z" itemprop="datePublished">2020-04-27</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/04/26/machine_learning_13/" class="title">의사결정나무</a></p>
                            <p class="item-date"><time datetime="2020-04-25T18:27:27.000Z" itemprop="datePublished">2020-04-26</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/04/25/machine_learning_12/" class="title">Support Vector Machine(SVM) - 02</a></p>
                            <p class="item-date"><time datetime="2020-04-25T11:03:51.000Z" itemprop="datePublished">2020-04-25</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">14</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/02/11/NLP_11/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
