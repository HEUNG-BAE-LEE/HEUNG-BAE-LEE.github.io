<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>NLP 문장 수준 임베딩 - 01 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="ELMo(Embedding from Language Models) 미국 연구기관 Allen Institute for Artificial Intelligence와 미국 워싱턴대학교 공동연구팀이 발표한 문장 임베딩 기법이다. Computer vision 분야에서 널리 쓰이고 있었던 Transfer leaning을 자연어 처리에 접목하여 주목받았다. Transf">
<meta property="og:type" content="article">
<meta property="og:title" content="NLP 문장 수준 임베딩 - 01">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/02/08/NLP_09/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="ELMo(Embedding from Language Models) 미국 연구기관 Allen Institute for Artificial Intelligence와 미국 워싱턴대학교 공동연구팀이 발표한 문장 임베딩 기법이다. Computer vision 분야에서 널리 쓰이고 있었던 Transfer leaning을 자연어 처리에 접목하여 주목받았다. Transf">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/ELMo_bidirectional_LSTM.png">
<meta property="og:updated_time" content="2020-02-09T12:22:21.185Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="NLP 문장 수준 임베딩 - 01">
<meta name="twitter:description" content="ELMo(Embedding from Language Models) 미국 연구기관 Allen Institute for Artificial Intelligence와 미국 워싱턴대학교 공동연구팀이 발표한 문장 임베딩 기법이다. Computer vision 분야에서 널리 쓰이고 있었던 Transfer leaning을 자연어 처리에 접목하여 주목받았다. Transf">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/ELMo_bidirectional_LSTM.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/02/08/nlp_09/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <!-- <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script> -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- data-ad-client="ca-pub-4604833066889492" -->
<script>
(adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-4604833066889492",
    enabel_page_level_ads: true
});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/NLP/">NLP</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <!-- <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script> -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- data-ad-client="ca-pub-4604833066889492" -->
<script>
(adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-4604833066889492",
    enabel_page_level_ads: true
});
</script>

			    <article id="post-NLP_09" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        NLP 문장 수준 임베딩 - 01
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/02/08/NLP_09/" class="article-date">
            <time datetime="2020-02-07T16:02:58.000Z" itemprop="datePublished">2020-02-08</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h2 id="ELMo-Embedding-from-Language-Models"><a href="#ELMo-Embedding-from-Language-Models" class="headerlink" title="ELMo(Embedding from Language Models)"></a>ELMo(Embedding from Language Models)</h2><ul>
<li><p>미국 연구기관 Allen Institute for Artificial Intelligence와 미국 워싱턴대학교 공동연구팀이 발표한 문장 임베딩 기법이다. <code>Computer vision 분야에서 널리 쓰이고 있었던 Transfer leaning을 자연어 처리에 접목하여 주목</code>받았다. Transfer learning이란 이미 학습된 모델을 다른 Deep learning 모델의 입력값 또는 부분으로 재사용하는 기법을 일컫는다. ELMo가 제안된 이후 자연어 처리 분야에서는 모델을 Pretrain한 후 이를 각종 DownStream Task에 적용하는 양상이 일반화됐다. BERT(Bidirectional Encoder Representations from Transfomer), GPT(Generative Pre-Training)등이 이 방식을 따른다. <code>Pretrain한 모델을 downstream task에 맞게 업데이트하는 과정을 Fine-tuning</code>이라고 한다.</p>
</li>
<li><p>ELMo는 Language Model이다. 단어 sequence가 얼마나 자연스러운지 확률값을 부여한다. 예를들어, ‘발 없는 말이 천리’라는 단어 sequence 다음에 ‘간다’라는 단어가 자주 등장했다면, 모델은 ‘발 없는 말이 천리’를 일력박아 ‘간다’를 출력해야 한다.</p>
</li>
<li><p>ELMo는 크게 3가지 요소로 구성돼 있다.</p>
<ul>
<li>1) <code>문자 단위 Convolution Layer</code><ul>
<li>각 단어 내 <code>문자들 사이의 의미적, 문법적 관계를 도출</code>한다.</li>
</ul>
</li>
<li>2) <code>양방향 LSTM Layer</code><ul>
<li><code>단어들 사이의 의미적, 문법적 관계를 추출해내는 역할</code>을 한다.</li>
</ul>
</li>
<li>3) <code>ELMo Layer</code><ul>
<li>문자 단위 convolution Layer와 양방야 LSTM Layer는 ELMo를 Pretrain하는 과정에서 학습된다. 하지만 <code>ELMo Layer는 Pretrain이 끝난 후 구체적인 DownStream task를 수행하는 과정에 학습</code>된다.</li>
<li><code>문자단위 conv layer와 양방향 LSTM layer의 출력벡터등을 가중합하는 방식으로 계산</code>된다. 이들 가중치들은 downstream task의 학습 손실을 최소화하는 방향으로 업데이트되면서 학습된다.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="문자-단위-Convolution-Layer"><a href="#문자-단위-Convolution-Layer" class="headerlink" title="문자 단위 Convolution Layer"></a>문자 단위 Convolution Layer</h4><ul>
<li><p><code>ELMo 입력은 문자</code>다 구체적으로는 <code>유니코드 ID</code>이다. 그러므로 <code>corpus를 해당 단어를 유니코드로 변환</code>해야한다. 한글 유니코드 블록은 UTF-8에서 3byte로 표현되기 때문에 예를 들어 ‘밥’이라는 단어의 유니코드를 10진수로 바꾸면 3가지 숫자가 된다. 여기서 단어(문자가 아님)의 시작과 끝을 알게하기 위해 BOW와 EOW에 해당하는 값을 유니코드 앞 뒤로 붙인다. 이후에 문자 임베딩 행렬에서 각각의 ID에 해당하는 행 벡터를 참조해 붙인다. 문자의 길이가 각각 다르므로 처음에 문자의 최대 길이를 정해주면 그에따른 padding처리를 해준다. Convolution filter의 크기는 (같이 보고 싶은 문자길이) $\times$ (문자 임베딩의 차원 수)가 된다. 이를통해 피처맵을 얻고 여기서 max pooling을 해주어 결과를 낸다.</p>
</li>
<li><p>위에서 같이 보고 싶은 문자길이를 조정해 가면서 여러개의 filter를 사용해 얻은 풀링 벡터들을 concatenate한 뒤 highway network와 projection(차원 조정)을 한다.</p>
</li>
</ul>
<h4 id="양방향-LSTM-스코어-레이어"><a href="#양방향-LSTM-스코어-레이어" class="headerlink" title="양방향 LSTM, 스코어 레이어"></a>양방향 LSTM, 스코어 레이어</h4><ul>
<li>문자 단위 convolution layer가 반환 한 단어벡터 sequence(맨 하단의 보라색 벡터들)에서 시작과 끝을 알이는 <bos>, <eos> 토큰을 앞 뒤로 붙인 뒤 학습 시킨다. 순방향 LSTM layer와 역방향 LSTM layer에 모두 위의 벡터 sequence들을 입력하는데 각각 n개의 LSTM layer를 구성한다. ELMo 기본 모델은 n=2로 설정하고 있다. <code>ELMo에는 LSTM layer에 residual connection 구조를 적용</code>시켜 일부 계산 노드를 생략하게하여 효율적인 Gradient 전파를 돕는다. 프리트레인 단계에서 Word2vec에서 사용되었던 negative sampling 기법이 사용된다.</eos></bos></li>
</ul>
<p><img src="/image/ELMo_bidirectional_LSTM.png" alt="ELMo 양방향 LSTM 및 출력 레이어"></p>
<h4 id="ELMo-레이어"><a href="#ELMo-레이어" class="headerlink" title="ELMo 레이어"></a>ELMo 레이어</h4><ul>
<li><p>ELMo 입베딩은 Pretrain이 끝나고 구체적인 DownStream task를 학습하는 과정에서 도출되며, <code>각 layer별 hidden state를 가중합한 결과</code>이다. 임의의 task를 수행하기 위한 문장 k번째 Token의 ELMo 임베딩의 구체적인 수식은 다음과 같다.</p>
<ul>
<li>$h_{k,j}^{LM}$ : k번째 Token의 j번째 layer의 순방향, 역방향 LSTM hidden state를 concatenate한 벡터를 의미한다.</li>
<li>$s_{j}^{task}$ : j번째 layer가 해당 task 수행에 얼마나 중요한지를 의미하는 scalar값이다. downstream task를 학습하는 과정에서 loss를 최소화하는 방향을 업데이트한다.</li>
<li>$\gamma^{task}$ : ELMo 벡터의 크기를 scaling하여 해당 task 수행을 돕는 역할을 한다.</li>
<li>L : 양방향 LSTM layer 개수(보통 2로 설정함)<ul>
<li>j=0 -&gt; 문자 단위 convolution Layer</li>
<li>j=1 -&gt; 양방향 LSTM layer의 첫번째 출력</li>
<li>j=2 -&gt; 양방향 LSTM layer의 두번째 출력</li>
</ul>
</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">ELMo_{k}^{task} = \gamma^{task} \sum^{L}_{j=0} s_{j}^{task} h_{k,j}^{LM}</script><h2 id="트랜스포메-네트워크"><a href="#트랜스포메-네트워크" class="headerlink" title="트랜스포메 네트워크"></a>트랜스포메 네트워크</h2><ul>
<li>트랜스포머 네트워크는 구글 연구 팀이 NIPS에 공개한 딥러닝 아키텍처다. 뛰어난 성능으로 주목받았다. 이후 발표된 GPT, BERT 등 기법은 트랜스포머 블록을 기본 모델로 쓰고 있다. 크게 두가지 작동원리로 나눌수 있다. <code>Multi-head Attention</code>과 <code>feedforward Network</code>이다.</li>
</ul>
<p><img src="/image/transformer_mechanism_structure_image.png" alt="트랜스포머 구조"></p>
<h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><ul>
<li>Scaled Dot-Product Attention의 입력(x)는 기본적으로 행렬 형태를 가지며 그 크기는 입력 문장의 단어수 $\times$ 입력 임베딩의 차원 수이다. 트랜스 포어믜 Scaled Dot-Product Attention 매커니즘은 query, key, value 3가지 사이의 관계가 핵심이다. 입력행렬 X와 Query, Key, Value에 따르는 가중치 행렬($W^{q}, W^{k}, W^{v}$)을 각각 곱해 계산한다. 이후 query와 key가 얼마나 유사한지를 구하기 위해 두 벡터간 내적을 구해 코사인 유사도를 구한다. 이를 통해 <code>어떤 query와 key가 특정 task 수행에 중요한 역할을 하고 있다면 트랜스포머 블록은 이들 사이의 내적값을 키우는 방식으로 학습</code>한다. 아래 식에서 제곱근 스케일을 하는 이유는 <code>query-key 내적 행렬의 분산을 줄이게 돼 softmax 함수의 gradient가 지나치게 작아지는 것을 방지</code>할 수 있기 때문이다. softmax 노드의 gradient는 softmax 확률 벡터 y의 개별 요소 값에 아주 민감하기 때문에 softmax 확률 벡터의 일부 값이 지나치게 작다면 gradient vanishing 문제가 나타날 수 있다.  </li>
</ul>
<h5 id="Scaled-Dot-Product-Attention-1"><a href="#Scaled-Dot-Product-Attention-1" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h5><script type="math/tex; mode=display">Attention(Q, K, V) = softmax(\frac{QK^{T}}{\sqrt(d_{k})}) \cdot V</script><h5 id="소프트맥스-노드의-gradient"><a href="#소프트맥스-노드의-gradient" class="headerlink" title="소프트맥스 노드의 gradient"></a>소프트맥스 노드의 gradient</h5><script type="math/tex; mode=display">\frac{\delta y_{i}}{\delta x_{i}} = y_{i}(1-y_{i})</script><script type="math/tex; mode=display">\frac{\delta y_{i}}{\delta x_{j}} = -y_{i}y_{j}</script><p><img src="/image/Scaled_dot_product_Attention_mechanism_weighted_matrix.png" alt="Scaled Dot-Product Attention"></p>
<ul>
<li>아래 그림은 Scaled Dot-Product Attention 기법으로 Query, Key, Value 사이의 관계들이 농축된 새로운 Z를 만드는 예시이다. 파란색 선으로 둘러싸인 행렬은 Query, Key 내적을 $\sqrt(d_{k})$로 나눈 뒤 softmax 함수를 취한 결과이다. 이 행렬의 <code>행은 Query 단어들에 대응</code>하며, <code>열은 Key 단어들에 대응</code>한다. 아래 그림처럼 Query와 Key값이 동일한 attention을 self-attention이라고 한다. <code>이는 같은 문장 내 모든 단어 쌍 사이의 의미적, 문법적 관계를 포착해낸다는 의미</code>이다. softmax를 취한 결과는 확률이 된다. 따라서 각 행의 합은 1이다. <code>&#39;드디어-금요일&#39;값이 가장 높아 벡터 공간상에서도 가까이 있을 가능성이 높고 두 단어사이의 관계가 task 수행(번역, 분류등)에  중요하다는 이야기</code>이다. 마지막으로는 softmax 확률을 가중치 삼아 각 값 벡터들을 가중합하는 것과 같다. <code>새롭게 만들어진 &#39;드디어&#39;에 해당하는 벡터는 해당 문장 내 단어 쌍간 관계가 모두 농축된 결과이다.</code></li>
</ul>
<p><img src="/image/Scaled_Dot_Product_Attention_example.png" alt="Scaled Dot-Product Attention"></p>
<ul>
<li>self-attention은 RNN, CNN보다 장점이 많다. CNN의 경우 사용자가 지정한 window내의 context만 살피기 때문에 문장이 길고 처음 단어와 마지막 단어 사이의 연관성 파악이 task 수행에 중요한 데이터라면 해결하기 어렵다. RNN은 sequence의 길이가 길어질수록 gradient vanishing이 일어나기 쉽기 때문에 처음 입력받았던 단어를 기억하기 쉽지 않다. 하지만 <code>self-attention은 문장 내 모든 단어쌍 사이의 관계를 늘 전체적으로 파알할 수 있다.</code></li>
</ul>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><ul>
<li><code>Scaled Dot-Product Attention을 여러 번 시행하는 것</code>을 가리킨다. <code>동일한 문장을 여러 명의 독자가 동시에 분석해 최선의 결과를 내려고 하는 것에 비유</code>할 수 있다. Multi-Head attention의 계산 과정은 아래의 수식과 그림과 같이 이루어진다. Query, Key, Value를 Scaled Dot-Product를 통해 얻은 Attention Value를 concatenate한다. 그 후 여기에 $W^{0}$를 내적해 <code>Multi-Head Attention 수행 결과 행렬의 크기를 Scaled Dot-Product Attention의 입력 행렬과 동일하게 맞춘다.</code></li>
</ul>
<h5 id="Multi-Head-Attention-수식"><a href="#Multi-Head-Attention-수식" class="headerlink" title="Multi-Head Attention 수식"></a>Multi-Head Attention 수식</h5><script type="math/tex; mode=display">MultiHead(Q, K, V) = Concat(head_{1}, \cdots, head_{h})W^{0}</script><script type="math/tex; mode=display">head_{i} = Attention(QW^{Q}_{i}, KW^{K}_{i}, VW^{V}_{i})</script><p><img src="/image/multi_head_attention_structure_NLP.png" alt="Multi-Head Attention"></p>
<h3 id="Position-wise-Feedforward-Networks"><a href="#Position-wise-Feedforward-Networks" class="headerlink" title="Position-wise Feedforward Networks"></a>Position-wise Feedforward Networks</h3><ul>
<li>Multi-Head Attention Layer의 입력 행렬과 출력 행렬의 크기는 입력 단어 수 $\times$ 히든 벡터 차원 수로 동일하다. Position-wise Feedforward Networks Layer에서는 Multi-Head Attention Layer의 출력 행렬을 행 벡터 단위로, 다시 말해 단어 벡터 각각에 관해 아래의 수식을 적용한다. Multi-Head Layer의 출력 행렬 가운데 하나의 단어 벡터를 x라고 하자. 이 x에 관해 두 번의 선형변환을 하는데 그 사이에 activation을 해서 적용한다.</li>
</ul>
<h5 id="Position-wise-Feedforward-Networks-수식"><a href="#Position-wise-Feedforward-Networks-수식" class="headerlink" title="Position-wise Feedforward Networks 수식"></a>Position-wise Feedforward Networks 수식</h5><script type="math/tex; mode=display">FFN(x) = max(0, x \cdot W_{1} + b_{1})W_{2} + b_{2}</script><h3 id="트랜스포머의-학습-전략"><a href="#트랜스포머의-학습-전략" class="headerlink" title="트랜스포머의 학습 전략"></a>트랜스포머의 학습 전략</h3><ul>
<li>트랜스포머의 학습 전략은 warm up이다. 아래 그림과 같이 <code>사용자가 정한 step수에 이르기 까지 learning rate를 높였다가 step 수를 만족하면 조금씩 떨어끄리는 방식</code>이다. 대규모 데이터, 큰 모델 학습에 적합하다. 이 전략은 BERT 등 이후 제안된 모델에도 널리 쓰이고 있다. 이밖에 Layer Normalization 등도 transformer의 안정적인 학습에 기여하고 있는 것으로 보인다. 좀더 자세한 사항을 알고 싶다면 <a href="https://heung-bae-lee.github.io/2020/01/21/deep_learning_10/">여기</a>를 눌러 공부해보자.</li>
</ul>
<p><img src="/image/warm_up_step.png" alt="warm up 트랜스포머의 learning rate 조정 방식"></p>

        </div>
        <!-- <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script> -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- data-ad-client="ca-pub-4604833066889492" -->
<script>
(adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-4604833066889492",
    enabel_page_level_ads: true
});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "NLP 문장 수준 임베딩 - 01",
        "image": "https://heung-bae-lee.github.io/image/ELMo_bidirectional_LSTM.png",
        "keywords": "",
        "genre": "NLP",
        "datePublished": "2020-02-08",
        "dateCreated": "2020-02-08",
        "dateModified": "2020-02-09",
        "url": "https://heung-bae-lee.github.io/2020/02/08/NLP_09/",
        "description": "ELMo(Embedding from Language Models)
미국 연구기관 Allen Institute for Artificial Intelligence와 미국 워싱턴대학교 공동연구팀이 발표한 문장 임베딩 기법이다. Computer vision 분야에서 널리 쓰이고 있었던 Transfer leaning을 자연어 처리에 접목하여 주목받았다. Transf"
        "wordCount": 1132
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
    
        <a href="/2020/02/06/NLP_08/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">NLP 문장 수준 임베딩 - 01</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/08/NLP_09/" class="title">NLP 문장 수준 임베딩 - 01</a></p>
                            <p class="item-date"><time datetime="2020-02-07T16:02:58.000Z" itemprop="datePublished">2020-02-08</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/06/NLP_08/" class="title">NLP 문장 수준 임베딩 - 01</a></p>
                            <p class="item-date"><time datetime="2020-02-05T15:32:51.000Z" itemprop="datePublished">2020-02-06</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/01/NLP_06/" class="title">NLP - 단어 수준 임베딩</a></p>
                            <p class="item-date"><time datetime="2020-02-01T11:47:03.000Z" itemprop="datePublished">2020-02-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/02/01/NLP_05/" class="title">NLP 실습 텍스트 분류(Conv1d CNN, LSTM) -03</a></p>
                            <p class="item-date"><time datetime="2020-02-01T07:57:48.000Z" itemprop="datePublished">2020-02-01</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/NLP/">NLP</a></p>
                            <p class="item-title"><a href="/2020/01/30/NLP_04/" class="title">NLP 실습 텍스트 분류(TF-IDF, CountVectorizer, Word2Vec) -02</a></p>
                            <p class="item-date"><time datetime="2020-01-29T15:13:48.000Z" itemprop="datePublished">2020-01-30</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">5</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <!-- <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script> -->
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- data-ad-client="ca-pub-4604833066889492" -->
<script>
(adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-4604833066889492",
    enabel_page_level_ads: true
});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/02/08/NLP_09/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
