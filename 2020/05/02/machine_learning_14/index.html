<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>Ensemble Learning - 01 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="Ensemble Learning이란? 모형 결합(model combining)방법은 앙상블 방법론(ensemble methods)라고도 한다. 이는 특정한 하나의 예측 방법이 아니라 복수의 예측모형을 결합하여 더 나은 성능의 예측을 하려는 시도이다.   모형 결합 방법을 사용하면 일반적으로 계산량은 증가하지만 다음과 같은 효과가 있다.  단일 모형을 사용할">
<meta property="og:type" content="article">
<meta property="og:title" content="Ensemble Learning - 01">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/05/02/machine_learning_14/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="Ensemble Learning이란? 모형 결합(model combining)방법은 앙상블 방법론(ensemble methods)라고도 한다. 이는 특정한 하나의 예측 방법이 아니라 복수의 예측모형을 결합하여 더 나은 성능의 예측을 하려는 시도이다.   모형 결합 방법을 사용하면 일반적으로 계산량은 증가하지만 다음과 같은 효과가 있다.  단일 모형을 사용할">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/Ensemble_dictionary_mean.png">
<meta property="og:updated_time" content="2020-05-12T04:16:30.077Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Ensemble Learning - 01">
<meta name="twitter:description" content="Ensemble Learning이란? 모형 결합(model combining)방법은 앙상블 방법론(ensemble methods)라고도 한다. 이는 특정한 하나의 예측 방법이 아니라 복수의 예측모형을 결합하여 더 나은 성능의 예측을 하려는 시도이다.   모형 결합 방법을 사용하면 일반적으로 계산량은 증가하지만 다음과 같은 효과가 있다.  단일 모형을 사용할">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/Ensemble_dictionary_mean.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/05/02/machine_learning_14/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/machine-learning/">machine learning</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-machine_learning_14" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        Ensemble Learning - 01
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/05/02/machine_learning_14/" class="article-date">
            <time datetime="2020-05-02T12:00:10.000Z" itemprop="datePublished">2020-05-02</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h2 id="Ensemble-Learning이란"><a href="#Ensemble-Learning이란" class="headerlink" title="Ensemble Learning이란?"></a>Ensemble Learning이란?</h2><ul>
<li>모형 결합(model combining)방법은 앙상블 방법론(ensemble methods)라고도 한다. 이는 특정한 하나의 예측 방법이 아니라 복수의 예측모형을 결합하여 더 나은 성능의 예측을 하려는 시도이다.</li>
</ul>
<ul>
<li><p>모형 결합 방법을 사용하면 <code>일반적으로 계산량은 증가하지만 다음과 같은 효과가 있다.</code></p>
<ul>
<li>단일 모형을 사용할 때 보다 성능 분산이 감소하기에 <code>과최적화(overfitting)을 방지</code>한다.</li>
<li><code>개별 모형이 성능이 안좋을 경우에는 결합 모형의 성능이 더 향상</code>된다.</li>
</ul>
</li>
<li><p>Ensemble Learning은 캐글이나 다른 대회에서 높은 성능을 자랑하며 여러 차례 우승을 차지한 알고리즘으로 그만큼 강력하지만, 현업에서는 Ensemble Learning을 사용하지 않을 가능성이 매우 높다. 왜냐하면 굉장히 강력하지만 다른 모델들과의 성능차이가 엄청나게 차이나는 것이 아니며, 실제 Domain에서 중요한 변수가 무엇인지와 같은 원인을 찾는 feature selection 부분이 더 중요할 수 있기 때문이다. 물론, 성능측면이 중요한 Domain분야에서는 Ensemble Learning이 중요할 것이다.</p>
</li>
<li><p>Ensemble이라는 의미의 사전적 정의는 ‘합창단’, ‘조화’라는 의미를 지닌다. 즉, 머신러닝에서의 개념은 여러개의 모델을 조합을 시킨다라는 의미로 받아들일 수 있다.</p>
</li>
</ul>
<p><img src="/image/Ensemble_dictionary_mean.png" alt="Ensemble의 의미"></p>
<ul>
<li>통계학에서의 대수의 법칙이라는 개념이 있는데, 큰 모집단에서 무작위로 뽑은 표본의 수가 많아 질수록(보통은 30개이상의 관측이) 모집단의 평균에 가까울 확률이 높아진다는 개념이다. <code>많은 시행의 결과가 수학적으로 합리적인 결과를 보여준다</code>는 것을 의미하는데, Ensemble learning에 적용하여 생각해보면 다수의 모델이 더 합리적인 성능을 가져올 수 있다는 것으로 해석할 수도 있다.</li>
</ul>
<p><img src="/image/the_law_of_large_number.png" alt="대수의 법칙과 Ensemble"></p>
<ul>
<li>하지만 아래에서 <code>합치는 모델의 성능 자체가 떨어지는 모델을 가지고 Ensemble learning을 진행한다고 해도 성능을 올릴 수는 없다.</code></li>
</ul>
<p><img src="/image/Ensemble_learning_conception_01.png" alt="Ensemble learning의 개념 - 01"></p>
<p><img src="/image/Ensemble_learning_conception_02.png" alt="Ensemble learning의 개념 - 02"></p>
<ul>
<li>아래에서는 이진분류에 대해서만 언급했지만, 대부분의 classification 문제에서는 One VS Rest 방식으로 문제를 풀기에 이진 분류 뿐만아니라, class가 여러개인 multi class 문제에서도 적용되는 내용이다.</li>
</ul>
<p><img src="/image/why_performence_prob_higher_than_half.png" alt="Ensemble learning에서 base model의 성능의 전제조건"></p>
<ul>
<li>아래에서와 같이 각각의 성능이 0.5인 분류기들을 voting을 통해 결과를 내게 되는데, 각각의 weak한 분류기들의 조합을 통해 최종적으로는 0.625라는 성능을 내게 된다.</li>
</ul>
<p><img src="/image/how_to_decision_predicted_value_on_ensemble.png" alt="Ensemble을 통한 결과 도출"></p>
<ul>
<li>다수결 모형이 개별 모형보다 더 나은 성능을 보이는 이유는 다음 실험에서도 확인 할 수 있다. 만약 개별 모형이 정답을 출력할 확률이 $p$인 경우에 <code>서로 다르고 독립적인 모형</code> $N$개를 모아서 다수결 모형을 만들면 정답을 출력할 확률이 다음과 같아진다.</li>
</ul>
<script type="math/tex; mode=display">\sum_{k>\frac{N}{2}}^N \binom N k p^k (1-p)^{N-k}</script><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def total_error(p, N):</span><br><span class="line">    te = 0.0</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(int(np.ceil(N/2)), N + 1):</span><br><span class="line">        te += sp.misc.comb(N, k) * p**k * (1-p)**(N-k)</span><br><span class="line">    <span class="built_in">return</span> te</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = np.linspace(0, 1, 100)</span><br><span class="line">plt.plot(x, x, <span class="string">'g:'</span>, lw=3, label=<span class="string">"개별 모형"</span>)</span><br><span class="line">plt.plot(x, total_error(x, 10), <span class="string">'b-'</span>, label=<span class="string">"다수결 모형 (N=10)"</span>)</span><br><span class="line">plt.plot(x, total_error(x, 100), <span class="string">'r-'</span>, label=<span class="string">"다수결 모형 (N=100)"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"개별 모형의 성능"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"다수결 모형의 성능"</span>)</span><br><span class="line">plt.legend(loc=0)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/each_model_and_voting_medel_performance_differ.png" alt="개별모델과 앙상블 모델의 성능 비교"></p>
<ul>
<li>각각의 분류기(모델)를 통해 최종적으로는 해당 데이터들의 decision boundary의 평균을 사용하는 것과 동일한 결과를 얻을 수 있다.</li>
</ul>
<p><img src="/image/Ensemble_learning_classifier_decision_boundary_visualization.png" alt="Ensemble을 통한 예측의 decision boundary"></p>
<ul>
<li>아래에서와 같이 <code>Test data에 대해 일정부분 bias되는 부분을 줄이기 위해 overfitting이 잘 되는 트리기반의 모형을 주로 베이스 모델로 사용</code>한다.</li>
</ul>
<p><img src="/image/base_line_model_why_is_based_on_tree.png" alt="Ensemble 모델에서 트리기반을 주로 사용하는 이유"></p>
<ul>
<li>Ensemble Learning의 종류는 아래 그림과 같이 나눌 수 있다. 쉽게 말하면 <code>Bagging</code>은 여러 개의 모델을 만들기 위해서는 Tree기반이나 선형회귀 분석 같은 경우는 동일한 feature와 동일한 data를 사용했을 경우 동일한 결과를 내주기 때문에, <code>모델을 여러 개 만들기 위해서 데이터를 나누어서 각 모델에 fitting시키는 것을 의미</code>한다. <code>Random Forest</code>는 데이터 뿐만 아니라 feature들의 선택도 각 모델별로 달리하여 fitting하는 것이며, <code>Boosting</code>은 분류기가 틀리게 예측한 데이터들에 대해 그 다음 학습기는 좀 더 학습을 잘 할 수 있도록 가중치를 주는 개념이다. 마지막으로 <code>Stacking</code>은 성능순으로 점수를 매기는 캐글에서는 0.1%라도 올리는 것이 중요하기 때문에 사용되어 지는데, 다른 Ensemble 기법들 보다 많은 성능을 높이지는 못하여 잘 사용되지는 않는다. 굉장히 많은 학습 연산량을 필요로 하기 때문에 실제 Domain에서 사용되어지기에는 쉽지 않다.</li>
</ul>
<p><img src="/image/Ensmble_learning_types.png" alt="Ensemble learning의 종류"></p>
<ul>
<li>위에서 언급한 것과 같이 모형 결합 방법은 크게 나누어 취합(aggregation) 방법론과 부스팅(boosting)방법론으로 나눌 수 있다.<ul>
<li>취합 방법론은 사용할 모형의 집합이 이미 결정되어있다.</li>
<li>부스팅 방법론은 사용할 모형을 점진적으로 늘려간다.</li>
</ul>
</li>
</ul>
<ul>
<li><p>각 방법론의 대표적인 방법들은 아래와 같다.</p>
<ul>
<li><p>취합 방법론</p>
<ul>
<li>다수결(Majority Voting)</li>
<li>배깅(Bagging)</li>
<li>랜덤 포레스트(Random Forests)</li>
</ul>
</li>
<li><p>부스팅 방법론</p>
<ul>
<li>에이다부스트(AdaBoost)</li>
<li>그레디언트 부스트(Gradient Boost)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="다수결-방법-Voting"><a href="#다수결-방법-Voting" class="headerlink" title="다수결 방법(Voting)"></a>다수결 방법(Voting)</h2><ul>
<li>다수결 방법은 가장 단순한 모형 결합 방법으로 전혀 다른 모형도 결합할 수 있다. 다수결 방법은 Hard Voting과 Soft Voting 두 가지로 나뉘어진다.<ul>
<li>hard voting: 단순 투표. 개별 모형의 결과 기준</li>
<li>soft voting: 가중치 투표. 개별 모형의 <code>조건부 확률의 합</code> 기준</li>
<li><code>일반적으로 hard voting보다는 soft voting이 예측 성능이 좋아서 더 많이 사용된다.</code></li>
</ul>
</li>
</ul>
<ul>
<li><p>Scikit-Learn의 ensemble 서브 패키지는 다수결 방법을 위한 <code>VotingClassifier</code>클래스를 제공한다. 입력인수는 다음과 같다.</p>
<ul>
<li><code>estimators</code>: 개별 모형 목록, 리스트나 named parameter 형식으로 입력</li>
<li><code>voting</code>: 문자열 {hard, soft} hard voting과 soft voting 선택. 디폴트는 hard</li>
<li><code>weights</code>: 사용자 가중치 리스트</li>
</ul>
</li>
<li><p>다음과 같은 예제 데이터를 가지는 이진 분류 문제를 생각해보자.</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([[0, -0.5], [-1.5, -1.5], [1, 0.5], [-3.5, -2.5], [0, 1], [1, 1.5], [-2, -0.5]])</span><br><span class="line">y = np.array([1, 1, 1, 2, 2, 2, 2])</span><br><span class="line">x_new = [0, -1.5]</span><br><span class="line">plt.scatter(X[y == 1, 0], X[y == 1, 1], s=100, marker=<span class="string">'o'</span>, c=<span class="string">'r'</span>, label=<span class="string">"클래스 1"</span>)</span><br><span class="line">plt.scatter(X[y == 2, 0], X[y == 2, 1], s=100, marker=<span class="string">'x'</span>, c=<span class="string">'b'</span>, label=<span class="string">"클래스 2"</span>)</span><br><span class="line">plt.scatter(x_new[0], x_new[1], s=100, marker=<span class="string">'^'</span>, c=<span class="string">'g'</span>, label=<span class="string">"테스트 데이터"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"x1"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"x2"</span>)</span><br><span class="line">plt.title(<span class="string">"이진 분류 예제 데이터"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Ensemble_voting_method_data.png" alt="Ensemble 방법 중 Voting 방법을 사용할 데이터"></p>
<ul>
<li>먼저, 이 문제를 3가지 다른 방법으로 풀어볼 것이다.<ul>
<li>로지스틱 회귀모형</li>
<li>QDA 모형</li>
<li>가우시안 나이브베이즈 모형</li>
</ul>
</li>
</ul>
<ul>
<li>마지막으로 3가지 모형을 다수결로 합친 모형을 <code>VotingClassifier</code>클래스로 만들었다. 다만 3가지 모형의 가중치가 각각 1,1,2로 가우시안 나이브베이즈 모형의 가중치를 높였다.</li>
</ul>
<ul>
<li>결과는 다음과 같이, 로지스틱 회귀모형과 가우시안 나이브베이즈 모형은 클래스 1이라는 결과를 보이지만 QDA모형은 클래스 2라는 결과를 보였다. 소프트 방식의 다수결 모형은 클래스 2라는 결론을 보인다. 만약 하드 방식의 다수결 모형이었다면 예측 결과는 클래스 1이 될 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.naive_bayes import GaussianNB</span><br><span class="line">from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis</span><br><span class="line">from sklearn.ensemble import VotingClassifier</span><br><span class="line"></span><br><span class="line">model1 = LogisticRegression(random_state=1)</span><br><span class="line">model2 = QuadraticDiscriminantAnalysis()</span><br><span class="line">model3 = GaussianNB()</span><br><span class="line">ensemble = VotingClassifier(estimators=[(<span class="string">'lr'</span>, model1), (<span class="string">'qda'</span>, model2), (<span class="string">'gnb'</span>, model3)], voting=<span class="string">'soft'</span>)</span><br><span class="line"></span><br><span class="line">probas = [c.fit(X, y).predict_proba([x_new]) <span class="keyword">for</span> c <span class="keyword">in</span> (model1, model2, model3, ensemble)]</span><br><span class="line">class1_1 = [pr[0, 0] <span class="keyword">for</span> pr <span class="keyword">in</span> probas]</span><br><span class="line">class2_1 = [pr[0, 1] <span class="keyword">for</span> pr <span class="keyword">in</span> probas]</span><br><span class="line"></span><br><span class="line">ind = np.arange(4)</span><br><span class="line">width = 0.35  <span class="comment"># bar width</span></span><br><span class="line">p1 = plt.bar(ind, np.hstack(([class1_1[:-1], [0]])), width, color=<span class="string">'green'</span>)</span><br><span class="line">p2 = plt.bar(ind + width, np.hstack(([class2_1[:-1], [0]])), width, color=<span class="string">'lightgreen'</span>)</span><br><span class="line">p3 = plt.bar(ind, [0, 0, 0, class1_1[-1]], width, color=<span class="string">'blue'</span>)</span><br><span class="line">p4 = plt.bar(ind + width, [0, 0, 0, class2_1[-1]], width, color=<span class="string">'steelblue'</span>)</span><br><span class="line"></span><br><span class="line">plt.xticks(ind + 0.5 * width, [<span class="string">'로지스틱 회귀 모형'</span>, <span class="string">'QDA 모형'</span>, <span class="string">'가우시안 나이브베이즈'</span>, <span class="string">'소프트 다수결 모형'</span>])</span><br><span class="line">plt.ylim([0, 1.1])</span><br><span class="line">plt.title(<span class="string">'세가지 다른 분류 모형과 소프트 다수결 모형의 분류 결과'</span>)</span><br><span class="line">plt.legend([p1[0], p2[0]], [<span class="string">'클래스 1'</span>, <span class="string">'클래스 2'</span>], loc=<span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/soft_voting_method_on_data_test.png" alt="소프트 보팅을 사용한 결과와 개별모델 예측 결과와의 비교"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">from itertools import product</span><br><span class="line"></span><br><span class="line">x_min, x_max = -4, 2</span><br><span class="line">y_min, y_max = -3, 2</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.005),</span><br><span class="line">                     np.arange(y_min, y_max, 0.005))</span><br><span class="line">f, axarr = plt.subplots(2, 2)</span><br><span class="line"><span class="keyword">for</span> idx, clf, tt <span class="keyword">in</span> zip(product([0, 1], [0, 1]),</span><br><span class="line">                        [model1, model2, model3, ensemble],</span><br><span class="line">                        [<span class="string">'로지스틱 회귀'</span>, <span class="string">'QDA'</span>, <span class="string">'가우시안 나이브베이즈'</span>, <span class="string">'다수결 모형'</span>]):</span><br><span class="line">    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])</span><br><span class="line">    Z = Z.reshape(xx.shape)</span><br><span class="line">    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.2, cmap=mpl.cm.jet)</span><br><span class="line">    axarr[idx[0], idx[1]].scatter(</span><br><span class="line">        X[:, 0], X[:, 1], c=y, alpha=0.5, s=50, cmap=mpl.cm.jet)</span><br><span class="line">    axarr[idx[0], idx[1]].scatter(x_new[0], x_new[1], marker=<span class="string">'x'</span>)</span><br><span class="line">    axarr[idx[0], idx[1]].set_title(tt)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li>아래 각 모형별로 decision boundary를 살펴 보았을 경우 어떠한 생각이 드는가? 필자의 생각엔 물론 전제조건이 아래 train 데이터가 모집단의 분포를 대표할 수 있는 데이터들이라는 가정하에 soft 방식으로 한 결과는 옳지 못한 결과라고 생각한다. 2클래스로 분류되기엔 1 클래스가 많은 영역에 존재하기 때문이다. 이렇게 시각화를 통해 살펴보는 방법도 결과에 대한 검증을 위해 필요할 것이다. 허나, 다변량인 경우는 몇가지 중요한 변수들에 대해서만 시각화를 해 본다던가 아니면 해당 조합들에 대해 모두 그려보는 것도 때론 좋은 방법일 수 있을 것이다.</li>
</ul>
<p><img src="/image/each_model_decision_boundary_differ_result.png" alt="모델 별 decision boundary"></p>
<ul>
<li>앞서 모형 결합에서 사용하는 독립적인 모형의 수가 많을 수록 성능 향상이 일어날 가능성이 높다는 것을 알았다. <code>각각 다른 확률 모형을 사용하는데에는 한계가 있으므로 보통은 배깅 방법을 사용하여 같은 확률 모형을 쓰지만 서로 다른 결과를 출력하는 다수의 모형을 만든다.</code></li>
</ul>
<h2 id="배깅-Bagging"><a href="#배깅-Bagging" class="headerlink" title="배깅(Bagging)"></a>배깅(Bagging)</h2><ul>
<li><p>Bagging은 Bootstrap Aggregating의 약자로 Sampling을 하는 방식이 Bootstrap방식을 사용하기 때문이다. 아래 그림에서 볼 수 있듯이 복원추출의 방식이라고 생각하면된다. 하나의 모델에 대하여 데이터를 추출할 경우 해당 모델에 들어가있는 데이터는 중복된 데이터가 있을 수 있다.(오른쪽 첫번째 데이터세트에서와 같이)</p>
<ul>
<li>같은 데이터 샘플을 중복사용(replacement)하지 않으면: Pasting</li>
<li>같은 데이터 샘플을 중복사용(replacement)하면: Bagging</li>
<li>데이터가 아니라 다차원 독립 변수 중 일부 차원을 선택하는 경우에는: Random Subspaces</li>
<li>데이터 샘플과 독립 변수 차원 모두 일부만 랜덤하게 사용하면: Random Patches</li>
</ul>
</li>
</ul>
<p><img src="/image/What_is_bagging_01.png" alt="Bagging의 개념 - 01"></p>
<ul>
<li>이렇게 추출하는 데이터는 전체 데이터 중 약 63%정도만 추출을 하게 된다. 아래 첫 번째 그림에서는 밑줄이 그러진 원의 데이터는 추출되지 않는 데이터들이다. 2번째 그림은 Bootstrap size가 5라면 5개씩 12개의 데이터 set를 복원추출을 통하여 뽑는 것이다. 여기서의 $k$는 임의로 정할 수 있다.</li>
</ul>
<p><img src="/image/What_is_bagging_02.png" alt="Bagging의 개념 - 02"></p>
<p><img src="/image/What_is_bagging_03.png" alt="Bagging의 개념 - 03"></p>
<ul>
<li>위에서 언급했듯이 추출되지 않은 데이터 set이 있는 것은 학습에 활용되지 않았으므로 그대로 두면 데이터를 낭비하는 것과 동일하다. 물론 Test set을 미리 나누어 놓고 해당 Test set을 prediction한 결과를 voting하여 성능을 측정하지만, 사용되지 않은 데이터(Out-of_Bag data)에 대해서도 모델별 성능을 계산한다.</li>
</ul>
<p><img src="/image/What_is_bagging_04.png" alt="Bagging의 개념 - 04"></p>
<ul>
<li>트리(Tree)와 배깅(Bagging)을 비교하자면 깊이 성장한 트리는 overfitting이 굉장히 심해지기 때문에 분산이 증가하기 때문에 편향은 줄어들 것이다. 그러나 배깅은 이러한 트리들을 결합시키므로 <code>편향이 유지되며, 분사은 감소하는 모델</code>이 될 것이다. <code>학습데이터의 noise에 robust</code>하다. 그러나 모형해석이 어려워지는 단점이 있다. 이러한 단점이 실제 Domain에서 사용되지 못하는 이유가 될 수 있다.</li>
</ul>
<p><img src="/image/What_is_bagging_05.png" alt="Bagging의 개념 - 05"></p>
<ul>
<li>Scikit-Learn의 ensemble 서브 패키지는 배깅 모형 결합을 위한 <code>BaggingClassifier</code> 클래스를 제공한다. 사용법은 아래와 같다. 참고로 <code>BaggingRegressor</code>도 존재하며 사용법은 동일하다.<ul>
<li><code>base_estimator</code>: 기본모형</li>
<li><code>n_estimators</code>: 모형 갯수. default=10</li>
<li><code>bootstrap</code>: 데이터 중복 사용 여부. default=True</li>
<li><code>max_samples</code>: 데이터 샘플 중 선택할 샘플의 수 혹은 비율. default=1.0</li>
<li><code>bootstrap_features</code>: feature의 중복 사용 여부. default=False</li>
<li><code>max_features</code>: 다차원 독립 변수 중 선택할 차원의 수 혹은 비율. default=1.0</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.ensemble import BaggingClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data[:, [0, 2]], iris.target</span><br><span class="line"></span><br><span class="line">model1 = DecisionTreeClassifier(max_depth=10, random_state=0).fit(X, y)</span><br><span class="line">model2 = BaggingClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=100, random_state=0).fit(X, y)</span><br><span class="line"></span><br><span class="line">x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">Z1 = model1.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z1, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"개별 모형"</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">Z2 = model2.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z2, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"배깅 모형"</span>)</span><br><span class="line">plt.suptitle(<span class="string">"붓꽃 데이터의 분류 결과"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li>왼쪽은 단일 모형으로 max_depth=10으로 설정한 Decision tree의 decision boundary의 모습이다. 트리의 깊이가 깊으므로 과최적화(overfitting)이 발생되었다. 오른쪽의 그림은 max_depth=2로 설정한 Decision tree모형을 100개 결합한 배깅 모형의 decision boundary 모습이다. 물론 depth를 작게하여 기본적인 모형자체도 과최적화(overfitting)를 방지하였지만, 배깅을 함으로써 모형의 분산이 줄어들며 더 train data에 robust하게 decision boundary가 그려진것을 확인할 수 있다.</li>
</ul>
<p><img src="/image/bagging_and_single_model_performance_differ_result.png" alt="배깅과 단일모형간의 decision boundary의 차이"></p>
<h2 id="랜덤-포레스트-Random-Forest"><a href="#랜덤-포레스트-Random-Forest" class="headerlink" title="랜덤 포레스트(Random Forest)"></a>랜덤 포레스트(Random Forest)</h2><ul>
<li>아래 그림과 같이 배깅은 여러 모델들을 결합하지만 부트스트랩 방식을 사용하여 모델들간의 사용되어지는 데이터가 동일한 집합들이 있을 수 있다. 그러므로 <code>Ensemble Learning의 개념에서 언급했었던 각 모델별 독립이라는 가정에 크게 위반</code>되어진다. 결국 비슷한 트리가 만들어지게 되어 모델들간의 공분산이 크게 되어 모델이 많아짐에 따라 점점 전체 Ensemble 모델의 분산은 커진다는 것이다. 분산이 커진다면 편향이 감소되어 더 좋은것이 아닌가라고 생각이 들수도 있겠지만, 모델의 예측 성능의 변동폭이 너무 크게 되면(분산이 크게 되어) 그만큼 불확신성도 높아지기 때문이다. 게다가, 애초에 다양한 모델에 대한 결합을 한 Ensemble 모델을 만들려고 한 의도조차 변질되어진다.</li>
</ul>
<p><img src="/image/why_is_randomforest_better_than_bagging.png" alt="배깅의 유의점 보완을 위한 방안"></p>
<ul>
<li>랜덤포레스트(Random Forest)는 의사 결정 나무(Decision Tree)를 개별 모형으로 사용하는 모형 결합 방법을 말한다. 랜덤 포레스트는 데이터 특징차원의 일부만 선택하여 사용한다. 하지만 <code>노드 분리시 모든 독립 변수들을 비교하여 최선의 독립 변수를 선택하는 것이 아니라 독립 변수 차원을 랜덤하게 감소시킨 다음 그 중에서 독립 변수를 선택</code>한다. 이렇게 하면 <code>개별 모형들 사이의 상관관계가 줄어들기 때문에 모형 성능의 변동이 감소하는 효과</code>가 있다. 이러한 방법을 극단적으로 적용한 것이 Extremely Randomized Trees 모형으로 이 경우에는 각 노드에서 랜덤하게 독립 변수를 선택한다.</li>
</ul>
<p><img src="/image/random_forest_conception.png" alt="랜덤포레스트의 개념"></p>
<p><img src="/image/random_forest_how_to_decide_parameter.png" alt="랜덤 포레스트의 개념 및 특징"></p>
<ul>
<li>랜덤 포레스트와 Extremely Randomized Trees 모형은 각각 <code>RandomForestClassifier</code> 클래스와 <code>ExtraTreesClassifier</code> 클래스로 구현되어 있다.</li>
</ul>
<ul>
<li>랜덤 포레스트는 CPU 병렬 처리도 효과적으로 수행되어 빠른 학습이 가능하기 때문에 뒤에 소개할 그레디언트 부스팅보다 예측 성능이 약간 떨어지더라도 랜덤 포레스트로 일단 기반 모델을 먼저 구축하는 경우가 많다. 멀티 코어 환경에서는 RandomForestClassifier 생성자와 GridSearchCV 생성 시 n_jobs = -1 파라미터를 추가하면 모든 CPU 코어을 이용해 학습할 수 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_iris</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.svm import SVC</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">X, y = iris.data[:, [0, 2]], iris.target</span><br><span class="line"></span><br><span class="line">model1 = DecisionTreeClassifier(max_depth=10, random_state=0).fit(X, y)</span><br><span class="line">model2 = RandomForestClassifier(max_depth=2, n_estimators=100, random_state=0).fit(X, y)</span><br><span class="line"></span><br><span class="line">x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))</span><br><span class="line">plt.subplot(121)</span><br><span class="line">Z1 = model1.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z1, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"개별 모형"</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">Z2 = model2.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)</span><br><span class="line">plt.contourf(xx, yy, Z2, alpha=0.6, cmap=mpl.cm.jet)</span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1, s=50, cmap=mpl.cm.jet, edgecolors=<span class="string">"k"</span>)</span><br><span class="line">plt.title(<span class="string">"배깅 모형"</span>)</span><br><span class="line">plt.suptitle(<span class="string">"붓꽃 데이터의 분류 결과"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<ul>
<li>아래 그림에서 오른쪽은 max_depth=2로 설정하고 모형의 수를 100개로 한 RandomForest 모델의 decision boundary의 시각화한 것이다.</li>
</ul>
<p><img src="/image/randomforest_decision_boundary_reuslt_viz.png" alt="랜덤포레스트와 단일 Decision Tree 모형의 decision boundary 비교"></p>
<ul>
<li>랜덤 포레스트의 장점 중 하나는 각 독립 변수의 중요도(feature importance)를 계산할 수 있다는 점이다. 포레스트 안에서 사용된 모든 노드에 대해 어떤 독립 변수를 사용하였고 그 노드에서 얻은 information gain을 구할 수 있으므로 각각의 독립 변수들이 얻어낸 information gain의 평균을 비교하면 어떤 독립 변수가 중요한지를 비교할 수 있다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_classification</span><br><span class="line">from sklearn.ensemble import ExtraTreesClassifier</span><br><span class="line"></span><br><span class="line">X, y = make_classification(n_samples=1000, n_features=10, n_informative=3, n_redundant=0, n_repeated=0,</span><br><span class="line">                           n_classes=2, random_state=0, shuffle=False)</span><br><span class="line"></span><br><span class="line">forest = ExtraTreesClassifier(n_estimators=250, random_state=0)</span><br><span class="line">forest.fit(X, y)</span><br><span class="line"></span><br><span class="line">importances = forest.feature_importances_</span><br><span class="line"></span><br><span class="line">std = np.std([tree.feature_importances_ <span class="keyword">for</span> tree <span class="keyword">in</span> forest.estimators_], axis=0)</span><br><span class="line">indices = np.argsort(importances)[::-1]</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"특성 중요도"</span>)</span><br><span class="line">plt.bar(range(X.shape[1]), importances[indices],</span><br><span class="line">        color=<span class="string">"r"</span>, yerr=std[indices], align=<span class="string">"center"</span>)</span><br><span class="line">plt.xticks(range(X.shape[1]), indices)</span><br><span class="line">plt.xlim([-1, X.shape[1]])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/feature_importance_using_random_forests.png" alt="랜덤포레스트를 통한 특성 중요도"></p>
<ul>
<li>다음은 올리베티 얼굴 사진을 Extreme 랜덤 포레스트로 구한 뒤 특징(pixel) 중요도를 이미지로 나타낸 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import fetch_olivetti_faces</span><br><span class="line">from sklearn.ensemble import ExtraTreesClassifier</span><br><span class="line"></span><br><span class="line">data = fetch_olivetti_faces()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line"></span><br><span class="line">forest = ExtraTreesClassifier(n_estimators=1000, random_state=0)</span><br><span class="line">forest.fit(X, y)</span><br><span class="line"></span><br><span class="line">importances = forest.feature_importances_</span><br><span class="line">importances = importances.reshape(data.images[0].shape)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(8, 8))</span><br><span class="line">plt.imshow(importances, cmap=plt.cm.bone_r)</span><br><span class="line">plt.grid(False)</span><br><span class="line">plt.title(<span class="string">"픽셀 중요도(pixel importance)"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/pixel_importance_using_random_forests.png" alt="랜덤포레스트를 이용해 시각화한 픽셀 중요도"></p>
<h2 id="Boosting"><a href="#Boosting" class="headerlink" title="Boosting"></a>Boosting</h2><ul>
<li>앞에서 언급했던 Bagging이나 Random Forests는 부트스트랩 방식으로 데이터를 뽑긴해도 각 모델에 대해 독립적이라고 가정하지만, <code>Boosting은 resampling을 할 때 오분류된 데이터에 더 가중치를 주어서 오분류된 데이터가 뽑힐 확률이 높도록 하여 복원 추출을 하고 다시 학습하기 때문에 모델들이 Sequential한 것이다.</code></li>
</ul>
<p><img src="/image/Boosting_conception.png" alt="Boosting 개념"></p>
<ul>
<li>부스트(boost) 방법은 미리 정해진 갯수의 모형 집합을 사용하는 것이 아니라 하나의 모형에서 시작하여 모형 집합에 포함할 개별 모형을 하나씩 추가한다. 모형의 집합은 위원회(commit) $C$라고 하고 $m$개의 모형을 포함하는 위원회를 $C_{m}$으로 표시한다. 위원회에 들어가는 개별 모형을 약 분류기(weak classifier)라고 하며 $k$로 표시한다.</li>
</ul>
<ul>
<li><code>부스트 방법의 특징은 한번에 하나씩 모형을 추가한다는 것</code>이다.</li>
</ul>
<script type="math/tex; mode=display">C_1 = \{ k_1 \}</script><script type="math/tex; mode=display">C_2 = C_1 \cup k_2 = \{ k_1, k_2 \}</script><script type="math/tex; mode=display">C_3 = C_2 \cup k_3 = \{ k_1, k_2, k_3 \}</script><script type="math/tex; mode=display">\vdots</script><script type="math/tex; mode=display">C_m = C_{m-1} \cup k_m = \{ k_1, k_2, \ldots, k_m \}</script><ul>
<li>그리고 m번째로 위원회에 추가할 개별 모형 $k_{m}$의 선택 기준은 그 전단계의 위원회 $C_{m-1}$의 성능을 보완하는 것이다. <code>위원회 $C_{m}$의 최종 결정은 다수결 방법을 사용하지 않고 각각의 개별 모형의 출력을 가중치 $\alpha$로 가중 선형조합한 값을 판별함수로 사용</code>한다. 또한 부스트 방법은 이진 분류에만 사용할 수 있으며 $y$값은 1 또는 -1의 값을 가진다.</li>
</ul>
<script type="math/tex; mode=display">y = -1 \text{ or } 1</script><script type="math/tex; mode=display">C_{m}(x_i) =  \text{sign} \left( \alpha_1k_1(x_i) + \cdots + \alpha_{m}k_{m}(x_i) \right)</script><h2 id="AdaBoost-에이다부스트"><a href="#AdaBoost-에이다부스트" class="headerlink" title="AdaBoost(에이다부스트)"></a>AdaBoost(에이다부스트)</h2><p><img src="/image/Adaboosting_conception.png" alt="AdaBoost 개념"></p>
<ul>
<li>에이다 부스트(adaboost)라는 이름은 적응 부스트(adaptive boost)라는 용어에서 나왔다. 에이다부스트는 위원회에 넣을 개별 모형 $k_{m}$을 선별하는 방법으로학습데이터 집합의 $i$번째 데이터에 가중치 $w_{i}$를 주고 분류 모형이 틀리게 예측한 데이터의 가중치를 합한 값을 손실함수 $L$로 사용한다. 이 손실함수를 최소화하는 모형이 k_{m}으로 선택된다.</li>
</ul>
<script type="math/tex; mode=display">L_m = \sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)</script><ul>
<li>위 식에서 $I$는 $k(x_{i}) \neq y_{i}$라는 조건이 만족되면 1, 아니면 0을 갖는 indicator function이다. 즉 예측을 틀리게한 데이터들에 대한 가중치의 합이다. 위원회 $C_{m}$에 포함될 개별 모형 $k_{m}$이 선택된 후에는 가중치 $\alpha_{m}$을 결정해야 한다. 이 값은 다음처럼 계산한다.</li>
</ul>
<script type="math/tex; mode=display">\epsilon_m = \dfrac{\sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)}{\sum_{i=1}^N w_{m,i}}</script><script type="math/tex; mode=display">\alpha_m = \frac{1}{2}\log\left( \frac{1 - \epsilon_m}{\epsilon_m}\right)</script><ul>
<li>데이터에 대한 가중치 $w_{m, i}$는 최초에는$(m=1)$ 모든 데이터에 대해 동일한 값을 갖지만, 위원회가 증가하면서 값이 바뀐다. 가중치의 값은 지시함수를 사용하여 위원회 $C_{m-1}$이 맞춘 문제는 작게, 틀린 문제는 크게 확대(boosting)된다.</li>
</ul>
<script type="math/tex; mode=display">w_{m,i} = w_{m-1,i}  \exp (-y_iC_{m-1}) = \begin{cases} w_{m-1,i}e^{-1}  & \text{ if } C_{m-1} = y_i\\ w_{m-1,i}e & \text{ if } C_{m-1} \neq y_i \end{cases}</script><ul>
<li>$m$번째 멤버의 모든 후보에 대해 위 손실함수를 적용하여 가장 값이 작은 후보를 $m$번째 멤버로 선정한다.</li>
</ul>
<ul>
<li>에이다 부스팅은 사실 다음과 같은 손실함수를 최소화하는 $C_{m}$을 찾아가는 방법이라는 것을 증명할 수 있다.</li>
</ul>
<script type="math/tex; mode=display">L_m = \sum_{i=1}^N \exp(−y_i C_m(x_i))</script><ul>
<li>개별 멤버 $k_{m}$과 위원회 관계는</li>
</ul>
<script type="math/tex; mode=display">C_m(x_i) = \sum_{j=1}^m \alpha_j k_j(x_i) = C_{m-1}(x_i) + \alpha_m k_m(x_i)</script><ul>
<li>이고 이 식을 대입하면</li>
</ul>
<script type="math/tex; mode=display">\begin{eqnarray} L_m &=& \sum_{i=1}^N \exp(−y_i C_m(x_i)) \\ &=& \sum_{i=1}^N \exp\left(−y_iC_{m-1}(x_i) - \alpha_m y_i k_m(x_i) \right) \\ &=& \sum_{i=1}^N \exp(−y_iC_{m-1}(x_i)) \exp\left(-\alpha_m y_i k_m(x_i)\right) \\ &=& \sum_{i=1}^N w_{m,i} \exp\left(-\alpha_m y_i k_m(x_i)\right) \\ \end{eqnarray}</script><ul>
<li>$y_{i}$와 $k_{M}(x_{i})$ 1 또는 -1값만 가질 수 있다는 점을 이용하면,</li>
</ul>
<script type="math/tex; mode=display">\begin{eqnarray} L_m &=& e^{-\alpha_m}\sum_{k_m(x_i) = y_i} w_{m,i} + e^{\alpha_m}\sum_{k_m(x_i) \neq y_i} w_{m,i} \\ &=& \left(e^{\alpha_m}-e^{-\alpha_m}\right) \sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right) + e^{-\alpha_m}\sum_{i=1}^N w_{m,i} \end{eqnarray}</script><ul>
<li>$L_{m}$을 최소화하려면 $\sum_{i=1}^N w_{m,i} I\left(k_m(x_i) \neq y_i\right)$을 최소화하는 $k_{m}$ 함수를 찾은 다음 $L_{m}$을 최소화하는 $\alpha_{m}$을 찾아야 한다.</li>
</ul>
<script type="math/tex; mode=display">\dfrac{d L_m}{d \alpha_m} = 0</script><ul>
<li>이 조건으로부터 $\alpha_{m}$ 공식을 유도할 수 있다.</li>
</ul>
<p><img src="/image/cost_function_of_adaboost.png" alt="Adaboost 비용함수"></p>
<ul>
<li>다음은 Scikit-Learn의 ensemble 서브패키지가 제공하는 <code>AdaBoostClassifier</code> 클래스를 사용하여 분류 예측을 하는 예이다. 약분류기로는 깊이가 1인 단순한 의사결정나무를 채택하였다. 여기에서는 각 표본 데이터의 가중치 값을 알아보기 위해 기존의 <code>AdaBoostClassifier</code> 클래스를 서브 클래싱하여 가중치를 속성으로 저장하도록 수정한 모형을 사용하였다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_gaussian_quantiles</span><br><span class="line">from sklearn.tree import DecisionTreeClassifier</span><br><span class="line">from sklearn.ensemble import AdaBoostClassifier</span><br><span class="line"></span><br><span class="line">X1, y1 = make_gaussian_quantiles(cov=2.,</span><br><span class="line">                                 n_samples=100, n_features=2,</span><br><span class="line">                                 n_classes=2, random_state=1)</span><br><span class="line">X2, y2 = make_gaussian_quantiles(mean=(3, 3), cov=1.5,</span><br><span class="line">                                 n_samples=200, n_features=2,</span><br><span class="line">                                 n_classes=2, random_state=1)</span><br><span class="line">X = np.concatenate((X1, X2))</span><br><span class="line">y = np.concatenate((y1, - y2 + 1))</span><br><span class="line"></span><br><span class="line">class MyAdaBoostClassifier(AdaBoostClassifier):</span><br><span class="line"></span><br><span class="line">    def __init__(self,</span><br><span class="line">                 base_estimator=None,</span><br><span class="line">                 n_estimators=50,</span><br><span class="line">                 learning_rate=1.,</span><br><span class="line">                 algorithm=<span class="string">'SAMME.R'</span>,</span><br><span class="line">                 random_state=None):</span><br><span class="line"></span><br><span class="line">        super(MyAdaBoostClassifier, self).__init__(</span><br><span class="line">            base_estimator=base_estimator,</span><br><span class="line">            n_estimators=n_estimators,</span><br><span class="line">            learning_rate=learning_rate,</span><br><span class="line">            random_state=random_state)</span><br><span class="line">        self.sample_weight = [None] * n_estimators</span><br><span class="line"></span><br><span class="line">    def _boost(self, iboost, X, y, sample_weight, random_state):</span><br><span class="line">        sample_weight, estimator_weight, estimator_error = \</span><br><span class="line">        super(MyAdaBoostClassifier, self)._boost(iboost, X, y, sample_weight, random_state)</span><br><span class="line">        self.sample_weight[iboost] = sample_weight.copy()</span><br><span class="line">        <span class="built_in">return</span> sample_weight, estimator_weight, estimator_error</span><br><span class="line"></span><br><span class="line">model_ada = MyAdaBoostClassifier(DecisionTreeClassifier(max_depth=1, random_state=0), n_estimators=20)</span><br><span class="line">model_ada.fit(X, y)</span><br><span class="line"></span><br><span class="line">def plot_result(model, title=<span class="string">"분류결과"</span>, legend=False, s=50):</span><br><span class="line">    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1</span><br><span class="line">    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1</span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.02), np.arange(x2_min, x2_max, 0.02))</span><br><span class="line">    <span class="keyword">if</span> isinstance(model, list):</span><br><span class="line">        Y = model[0].predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(model) - 1):</span><br><span class="line">            Y += model[i + 1].predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Y = model.predict(np.c_[xx1.ravel(), xx2.ravel()]).reshape(xx1.shape)</span><br><span class="line">    cs = plt.contourf(xx1, xx2, Y, cmap=plt.cm.Paired, alpha=0.5)</span><br><span class="line">    <span class="keyword">for</span> i, n, c <span class="keyword">in</span> zip(range(2), <span class="string">"01"</span>, <span class="string">"br"</span>):</span><br><span class="line">        idx = np.where(y == i)</span><br><span class="line">        plt.scatter(X[idx, 0], X[idx, 1], c=c, s=s, alpha=0.5, label=<span class="string">"Class %s"</span> % n)</span><br><span class="line">    plt.xlim(x1_min, x1_max)</span><br><span class="line">    plt.ylim(x2_min, x2_max)</span><br><span class="line">    plt.xlabel(<span class="string">'x1'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'x2'</span>)</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.colorbar(cs)</span><br><span class="line">    <span class="keyword">if</span> legend:</span><br><span class="line">        plt.legend()</span><br><span class="line">    plt.grid(False)</span><br><span class="line"></span><br><span class="line">plot_result(model_ada, <span class="string">"에이다부스트(m=20) 분류 결과"</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/Adaboost_m_20_result.png" alt="Adaboost 20번째 모델까지의 결과"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(10, 15))</span><br><span class="line">plt.subplot(421);</span><br><span class="line">plot_result(model_ada.estimators_[0], <span class="string">"1번 분류모형의 분류 결과"</span>, s=10)</span><br><span class="line">plt.subplot(422);</span><br><span class="line">plot_result(model_ada.estimators_[1], <span class="string">"2번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[0]).astype(int))</span><br><span class="line">plt.subplot(423);</span><br><span class="line">plot_result(model_ada.estimators_[2], <span class="string">"3번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[1]).astype(int))</span><br><span class="line">plt.subplot(424);</span><br><span class="line">plot_result(model_ada.estimators_[3], <span class="string">"4번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[2]).astype(int))</span><br><span class="line">plt.subplot(425);</span><br><span class="line">plot_result(model_ada.estimators_[4], <span class="string">"5번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[3]).astype(int))</span><br><span class="line">plt.subplot(426);</span><br><span class="line">plot_result(model_ada.estimators_[5], <span class="string">"6번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[4]).astype(int))</span><br><span class="line">plt.subplot(427);</span><br><span class="line">plot_result(model_ada.estimators_[6], <span class="string">"7번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[5]).astype(int))</span><br><span class="line">plt.subplot(428);</span><br><span class="line">plot_result(model_ada.estimators_[7], <span class="string">"8번 분류모형의 분류 결과"</span>, s=(4000*model_ada.sample_weight[6]).astype(int))</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure>
<p><img src="/image/Adaboost_m_20_result_01.png" alt="Adaboost 모델의 가중치의 변화에 따른 decision boundary의 변화 - 01"></p>
<p><img src="/image/Adaboost_m_20_result_02.png" alt="Adaboost 모델의 가중치의 변화에 따른 decision boundary의 변화 - 02"></p>
<h3 id="Adaboost-모형의-정규화"><a href="#Adaboost-모형의-정규화" class="headerlink" title="Adaboost 모형의 정규화"></a>Adaboost 모형의 정규화</h3><ul>
<li>Adaboost 모형이 과최적화(overfitting)가 되는 경우에는 학습 속도(learning rate)를 조정하여 정규화를 할 수 있다. 이는 <code>필요한 멤버의 수를 강제로 증가시켜서 과최적화를 막는 역할을 한다.</code> 즉, 새롭게 적용되는 모형에 대한 가중치를 줄여서 동일한 모형의 횟수를 거치더라도 가중치가 크게 영향을 받지 않도록 하여 과최적화를 없애는 방법이다.</li>
</ul>
<script type="math/tex; mode=display">C_m = C_{m-1} + \mu \alpha_m k_m</script><ul>
<li><code>AdaBoostClassifier</code> 클래스에서는  <code>learning_rate</code>인수를 1보다 적게 주면 새로운 멤버의 가중치를 강제로 낮춘다.</li>
</ul>
<h3 id="그레디언트-부스팅-Gradient-boosting"><a href="#그레디언트-부스팅-Gradient-boosting" class="headerlink" title="그레디언트 부스팅 (Gradient boosting)"></a>그레디언트 부스팅 (Gradient boosting)</h3><ul>
<li>기본적으로 부스팅은 다음 Round에서 이전에 잘못 예측한 데이터들에 대한 처리를 어떻게 하느냐에 따라 종류별로 차이가 존재한다. <code>Gradient Boosting은 이전 Round의 분류기로 예측한 error를 다음 Round의 분류기가 예측할 수 있도록 학습하면서 진행</code>한다.</li>
</ul>
<p><img src="/image/what_is_gradient_boosting.png" alt="그레디언트 부스팅의 개념"></p>
<ul>
<li>이전 모델의 error를 다음 모델이 예측할 수 있게끔 학습시켜 해당 분류기들의 학습된 결과를 계속해서 합해 나가면 마지막에는 최소한의 error만 남으므로, error를 최대한 줄일 수 있게 된다.</li>
</ul>
<p><img src="/image/gradient_boosting_method_principal.png" alt="그레디언트 부스팅의 원리"></p>
<ul>
<li>위에서 언급했던 것과 같이 error를 예측하게 하므로 이해하기 쉽게 regression을 통한 예시로 설명하겠다. 처음 모델의 error를 다음 모델은 예측하도록 학습하므로 이전 모델보다 오차가 더 줄어들 것이다. 그 다음 모델도 이전 모델의 오차를 학습하게 되므로 더 오차가 줄어들 것이다. 이렇게 최종적으로는 error가 최대한 0에 가까워질 때 까지 학습하여 train set에 대해서는 과최적화가 이루어 질 것이다.</li>
</ul>
<p><img src="/image/gradient_boosting_steps.png" alt="그레디언트 부스팅의 이해"></p>
<ul>
<li>최종적으로는 학습 데이터에 대한 error를 작게 하는 것이므로 아래 그림에서와 같이 negative gradient를 최소화시키면서 학습 될 것이다.</li>
</ul>
<p><img src="/image/cost_function_with_gradient_boosting.png" alt="그레디언트 부스팅의 cost function"></p>
<ul>
<li>위의 그림에서 볼 수 있듯이 <code>그레디언트 부스트 모형은 변분법(calculus of variations)을 사용한 모형</code>이다. 학습 $f(x)$를 최소화하는 $x$는 다음과 같이 gradient descent 방법으로 찾을 수 있다.</li>
</ul>
<script type="math/tex; mode=display">x_{m} = x_{m-1} - \alpha_m \dfrac{df}{dx}</script><ul>
<li>그레디언트 부스트 모형에서는 손실 범함수(loss functional) $L(y, C_{m-1})$을 최소화하는 개별 분류함수 $k_{m}$를 찾는다. 이론적으로 가장 최적의 함수는 범함수의 미분이다.</li>
</ul>
<script type="math/tex; mode=display">C_{m} = C_{m-1} - \alpha_m \dfrac{\delta L(y, C_{m-1})}{\delta C_{m-1}} = C_{m-1} + \alpha_m k_m</script><ul>
<li><code>따라서 그레디언트 부스트 모형은 분류/회귀 문제에 상관없이 개별 멤버 모형으로 회귀분석 모형을 사용</code>한다. 가장 많이 사용되는 회귀분석 모형은 의사결정 회귀나무(decision tree regression model)모형이다.</li>
</ul>
<ul>
<li><p>그레디언트 부스트 모형에서는 다음과 같은 과정을 반복하여 멤버와 그 가중치를 계산한다.</p>
<ul>
<li><ol>
<li>$-\tfrac{\delta L(y, C_m)}{\delta C_m}$를 목표값으로 개별 멤버 모형 $k_{m}$을 찾는다.</li>
</ol>
</li>
<li><ol>
<li>$ \left( y - (C_{m-1} + \alpha_m k_m) \right)^2 $ 를 최소화하는 스텝사이즈 $\alpha_{m}$을 찾는다.</li>
</ol>
</li>
<li><ol>
<li>$ C_m = C_{m-1} + \alpha_m k_m $ 최종 모형을 갱신한다.</li>
</ol>
</li>
</ul>
</li>
<li><p>만약 손실 범함수가 오차 제곱 형태라면</p>
</li>
</ul>
<script type="math/tex; mode=display">L(y, C_{m-1}) = \dfrac{1}{2}(y - C_{m-1})^2</script><ul>
<li>범함수의 미분은 실제 목표값 $y$와 $C_{m-1}$과의 차이 즉, 잔차(residual)가 된다.</li>
</ul>
<script type="math/tex; mode=display">-\dfrac{dL(y, C_m)}{dC_m} = y - C_{m-1}</script><ul>
<li><p>Scikit-Learn의 GradientBoostingClassifier는 약한 학습기의 순차적인 예측 오류 보정을 통해 학습을 수행하므로 멀티 CPU 코어 시스템을 사용하더라도 병렬처리가 지원되지 않아서 대용량 데이터의 경우 학습에 매우 많은 시간이 필요하다. 또한 일반적으로 GBM이 랜덤 포레스트보다는 예측 성능이 조금 뛰어난 경우가 많다. 그러나 수행시간이 오래 걸리고, 하이퍼 파라미터 튜닝 노력도 더 필요하다.</p>
</li>
<li><p><code>loss</code>: 경사 하강법에서 사용할 비용 함수를 저장한다. 특별한 이유가 없으면 default인 ‘deviance’를 그대로 적용한다.</p>
</li>
</ul>
<ul>
<li><code>learning_rate</code>: GBM이 학습을 진행할 때마다 적용하는 학습률이다. Weak learner가 순차적으로 오류 값을 보정해 나가는 데 적용하는 계수이다. 0~1 사이의 값을 지정할 수 있으며 default=0.1이다. 너무 작은 값을 적용하면 업데이트 되는 값이 작아져서 최소 오류 값을 찾아 예측 성능이 높아질 가능성이 높다. 하지만 많은 weak learner는 순차적인 반복이 필요해서 수행 시간이 오래 걸리고, 또 너무 작게 설정하면 모든 weak learner의 반복이 완료돼도 최소 오류 값을 찾지 못할 수 있다. 반대로 큰 값을 적용하면 최소 오류 값을 찾지 못하고 그냥 지나챠 버려 예측 성능이 떨어질 가능성이 높아지지만, 빠른 수행이 가능하다. <code>이러한 특성 때문에 learning_rate는 n_estimators와 상호 보완적으로 조합해 사용한다. learning_rate를 작게하고 n_estimators를 크게 하면 더 이상 성능이 좋아지지 않는 한계점까지는 예측 성능이 조금씩 좋아질 수 있다.</code></li>
</ul>
<ul>
<li><code>subsample</code>: weak learner가 학습에 사용하는 데이터의 샘플링 비율이다. default=1이며, 이는 전체 학습 데이터를 기반으로 학습한다는 의미이다.(0.5이면 학습데이터의 50%를 의미) 과적합이 염려되는 경우 subsample을 1보다 작은 값으로 설정한다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble import GradientBoostingClassifier</span><br><span class="line"></span><br><span class="line">model_grad = GradientBoostingClassifier(n_estimators=100, max_depth=2, random_state=0)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">model_grad.fit(X, y)</span><br></pre></td></tr></table></figure>
<h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">CPU <span class="built_in">times</span>: user 50 ms, sys: 0 ns, total: 50 ms</span><br><span class="line">Wall time: 50.4 ms</span><br><span class="line"></span><br><span class="line">GradientBoostingClassifier(criterion=<span class="string">'friedman_mse'</span>, init=None,</span><br><span class="line">                           learning_rate=0.1, loss=<span class="string">'deviance'</span>, max_depth=2,</span><br><span class="line">                           max_features=None, max_leaf_nodes=None,</span><br><span class="line">                           min_impurity_decrease=0.0, min_impurity_split=None,</span><br><span class="line">                           min_samples_leaf=1, min_samples_split=2,</span><br><span class="line">                           min_weight_fraction_leaf=0.0, n_estimators=100,</span><br><span class="line">                           n_iter_no_change=None, presort=<span class="string">'auto'</span>,</span><br><span class="line">                           random_state=0, subsample=1.0, tol=0.0001,</span><br><span class="line">                           validation_fraction=0.1, verbose=0,</span><br><span class="line">                           warm_start=False)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_result(model_grad)</span><br></pre></td></tr></table></figure>
<p><img src="/image/result_of_gradient_boost_plot_decision_boundary.png" alt="그레디언트 부스트의 decision boundary"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.subplot(121)</span><br><span class="line">plot_result(model_grad.estimators_[3][0])</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_result([model_grad.estimators_[0][0],</span><br><span class="line">             model_grad.estimators_[1][0],</span><br><span class="line">             model_grad.estimators_[2][0],</span><br><span class="line">             model_grad.estimators_[3][0]])</span><br></pre></td></tr></table></figure>
<p><img src="/image/result_of_gradient_boost_plot_decision_boundary_01.png" alt="그레디언트 부스트에 사용된 모형들의 4번째 까지의 각각의 decision decision boundary"></p>
<h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><p><img src="/image/XGBoost_conception.png" alt="XGBoost 개념"></p>
<ul>
<li>XGboost는 GBM에 기반하고 있지만, GBM의 단점인 느린 수행 시간 및 과적합 규제(Regularization) 부재 등의 문제를 해결해서 매우 각광을 받고 있다. 특히 XGBoost는 병렬 CPU 환경에서 병렬 학습이 가능해 기존 GBM보다 빠르게 학습을 완료할 수 있다. 다음은 XGboost의 장점이다.</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>항목</th>
<th>설명</th>
</tr>
</thead>
<tbody>
<tr>
<td>뛰어난 예측성능</td>
<td>일반적으로 분류와 회귀 영역에서 뛰어난 예측 성능을 발휘한다.</td>
</tr>
<tr>
<td>GBM 대비 빠른 수행 시간</td>
<td>일반적인 GBM은 순차적으로 Weak Learner가 가중치를 증감하는 방법으로 학습하기 때문에 전반적으로 \\ 속도가 느리다. 하지만 XGBoost는 병렬 수행 및 다양한 기능으로 GBM에 비해 빠른 수행 성능을 보장한다.\\ 아쉽게도 XGBoost가 일반적인 GBM에 비해 수행 시간이 빠르다는 것이지, 다른 머신러닝 알고리즘\\ (예를 들어 랜덤 포레스트)에 비해서 빠르다는 의미는 아니다.</td>
</tr>
<tr>
<td>과적합 규제\\ (Regularization)</td>
<td>표준 GBM의 경우 과적합 규제 기능이 없으나 XGBoost는 자체에 과적합 규제 기능으로 과적합에 \\ 좀 더 강한 내구성을 가질 수 있다.</td>
</tr>
<tr>
<td>Tree pruning (나무 가지치기)</td>
<td>일반적으로 GBM은 분할 시 부정 손실이 발생하면 분할을 더 이상 수행하지 않지만, 이러한 방식도 자칫\\ 지나치게 많은 분할을 발생할 수 있다. 다른 GBM과 마찬가지로 XGBoost도 max_depth 파라미터로 \\ 분할 깊이를 조정하기도 하지만, tree pruning으로 더 이상 긍정 이득이 없는 분할을 가지치기 해서\\ 분할 수를 더 줄이는 추가적인 장점을 가지고 있다.</td>
</tr>
<tr>
<td>자체 내장된 교차 검증</td>
<td>XGBoost는 반복 수행 시마다 내부적으로 학습 데이터 세트와 평가 데이터 세트에 대한 교차 검증을 수행해\\ 최적화된 반복 수행 횟수를 가질 수 있다. 지정된 반복 횟수가 아니라 교차 검증을 통해 평가 데이터 세트의\\ 평가값이 최적화 되면 반복을 중간에 멈출 수 있는 조기 중단 기능이 있다.</td>
</tr>
<tr>
<td>결손값 자체 처리</td>
<td>XGBoost는 결손값을 자체 처리할 수 있는 기능을 가지고 있다.</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/image/XGboost_better_than_gbm.png" alt="XGBoost의 장점"></p>
<ul>
<li>XGBoost의 핵심 라이브러리는 C/C++로 작성돼 있다. XGBoost 개발 그룹은 파이썬에서도 XGBoost를 구동할 수 있도록 파이썬 패키지를 제공한다. 이 파이썬 패키지의 역할은 대부분 C/C++ 핵심 라이브러리를 호출하는 것이다.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># window용</span></span><br><span class="line"><span class="comment"># conda install -c anaconda py-xgboost</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Linux용</span></span><br><span class="line">conda install -c conda-forge xgboost</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import xgboost</span><br><span class="line"></span><br><span class="line">model_xgb = xgboost.XGBClassifier(n_estimators=100, max_depth=1, random_state=0)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "Ensemble Learning - 01",
        "image": "https://heung-bae-lee.github.io/image/Ensemble_dictionary_mean.png",
        "keywords": "",
        "genre": "machine learning",
        "datePublished": "2020-05-02",
        "dateCreated": "2020-05-02",
        "dateModified": "2020-05-12",
        "url": "https://heung-bae-lee.github.io/2020/05/02/machine_learning_14/",
        "description": "Ensemble Learning이란?
모형 결합(model combining)방법은 앙상블 방법론(ensemble methods)라고도 한다. 이는 특정한 하나의 예측 방법이 아니라 복수의 예측모형을 결합하여 더 나은 성능의 예측을 하려는 시도이다.


모형 결합 방법을 사용하면 일반적으로 계산량은 증가하지만 다음과 같은 효과가 있다.

단일 모형을 사용할"
        "wordCount": 5277
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2020/05/12/linear_algebra_01/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            선형대수 요소(Elements in linear algebra)
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2020/05/02/data_structure_06/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">내가 정리하는 자료구조 05 - 트리(Tree)</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/05/13/linear_algebra_02/" class="title">선형 시스템(Linear system)</a></p>
                            <p class="item-date"><time datetime="2020-05-13T07:50:33.000Z" itemprop="datePublished">2020-05-13</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/05/12/linear_algebra_01/" class="title">선형대수 요소(Elements in linear algebra)</a></p>
                            <p class="item-date"><time datetime="2020-05-12T13:41:59.000Z" itemprop="datePublished">2020-05-12</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/02/machine_learning_14/" class="title">Ensemble Learning - 01</a></p>
                            <p class="item-date"><time datetime="2020-05-02T12:00:10.000Z" itemprop="datePublished">2020-05-02</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></p>
                            <p class="item-title"><a href="/2020/05/02/data_structure_06/" class="title">내가 정리하는 자료구조 05 - 트리(Tree)</a></p>
                            <p class="item-date"><time datetime="2020-05-02T10:27:21.000Z" itemprop="datePublished">2020-05-02</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></p>
                            <p class="item-title"><a href="/2020/04/30/data_structure_05/" class="title">내가 정리하는 자료구조 04 - 해쉬 테이블</a></p>
                            <p class="item-date"><time datetime="2020-04-30T14:32:54.000Z" itemprop="datePublished">2020-04-30</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">15</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/05/02/machine_learning_14/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
