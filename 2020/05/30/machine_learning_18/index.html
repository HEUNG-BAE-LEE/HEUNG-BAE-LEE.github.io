<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>Clustering - K-means, K-medoid | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="K-means Clusterig 각 군집에 할당된 포인트들의 평균 좌표를 이용해 중심점을 반복적으로 업데이트하면서 군집을 분류해 나가는 방법 가장 단순하고 빠른 군집화 방법     초기에 제일 처음 랜덤하게 포인트를 하나 잡아서 그 포인트에 가까운 데이터들을 같은 군집으로 할당해준다. 그 다음 아래와 같은 방법으로 반복한다. 다음과 같은 목저함수 값이 최소">
<meta property="og:type" content="article">
<meta property="og:title" content="Clustering - K-means, K-medoid">
<meta property="og:url" content="https://heung-bae-lee.github.io/2020/05/30/machine_learning_18/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="K-means Clusterig 각 군집에 할당된 포인트들의 평균 좌표를 이용해 중심점을 반복적으로 업데이트하면서 군집을 분류해 나가는 방법 가장 단순하고 빠른 군집화 방법     초기에 제일 처음 랜덤하게 포인트를 하나 잡아서 그 포인트에 가까운 데이터들을 같은 군집으로 할당해준다. 그 다음 아래와 같은 방법으로 반복한다. 다음과 같은 목저함수 값이 최소">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/how_to_decide_mean_point_in_K_means_clustering_01.png">
<meta property="og:updated_time" content="2020-06-05T16:53:44.923Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Clustering - K-means, K-medoid">
<meta name="twitter:description" content="K-means Clusterig 각 군집에 할당된 포인트들의 평균 좌표를 이용해 중심점을 반복적으로 업데이트하면서 군집을 분류해 나가는 방법 가장 단순하고 빠른 군집화 방법     초기에 제일 처음 랜덤하게 포인트를 하나 잡아서 그 포인트에 가까운 데이터들을 같은 군집으로 할당해준다. 그 다음 아래와 같은 방법으로 반복한다. 다음과 같은 목저함수 값이 최소">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/how_to_decide_mean_point_in_K_means_clustering_01.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2020/05/30/machine_learning_18/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">
    <link rel="stylesheet" type="text/css" href>
    <link rel="stylesheet" href="https://cdn.rawgit.com/innks/NanumSquareRound/master/nanumsquareround.css">	
    <link rel="stylesheet" href="https://fonts.googleapis.com/earlyaccess/nanumgothiccoding.css">
    <link rel="stylesheet" href="/css/style.css">
   
    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/machine-learning/">machine learning</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-machine_learning_18" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        Clustering - K-means, K-medoid
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2020/05/30/machine_learning_18/" class="article-date">
            <time datetime="2020-05-29T16:01:30.000Z" itemprop="datePublished">2020-05-30</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h2 id="K-means-Clusterig"><a href="#K-means-Clusterig" class="headerlink" title="K-means Clusterig"></a>K-means Clusterig</h2><ul>
<li><code>각 군집에 할당된 포인트들의 평균 좌표를 이용해 중심점을 반복적으로 업데이트</code>하면서 군집을 분류해 나가는 방법<ul>
<li>가장 단순하고 빠른 군집화 방법</li>
</ul>
</li>
</ul>
<ul>
<li>초기에 제일 처음 랜덤하게 포인트를 하나 잡아서 그 포인트에 가까운 데이터들을 같은 군집으로 할당해준다. 그 다음 아래와 같은 방법으로 반복한다.<ul>
<li>다음과 같은 목저함수 값이 최소화될 때까지 군집의 중심위치와 각 데이터가 소속될 군집을 반복해서 찾는다. 이 값을 관성(inertia)이라 한다.</li>
</ul>
</li>
</ul>
<script type="math/tex; mode=display">J = \sum_{k=1}^K \sum_{i \in C_k} d(x_i, \mu_k)</script><ul>
<li>이 식에서 $ K $ 는 군집의 갯수이고 $ C_{k} $ 는 $ k $ 번쨰 군집에 속하는 데이터의 집합, $ \mu_{k} $ 는 $ k $ 번째 군집의 중심위치(centroid), $ d $ 는 $ x_{i}, \mu_{k} $ 두 데이터 사이의 거리 혹은 비유사도(dissimilarity)로 정의한다. 만약 유클리드 거리를 사용한다면 다음과 같다.</li>
</ul>
<script type="math/tex; mode=display">d(x_i, \mu_k) = || x_i - \mu_k ||^2</script><ul>
<li>위 식은 다음처럼 표현할 수 도 있다.</li>
</ul>
<script type="math/tex; mode=display">J = \sum_{i=1}^{N}\min_{\mu_j \in C}(||x_i - \mu_j||^2)</script><ul>
<li>세부 알고리즘은 다음과 같다.<ul>
<li>1) 임의의 중심위치 $ \mu_{k} (k=1, \ldots , K) $ 를 고른다. 보통 데이터 표본 중에서 $ K $ 개를 선택한다.</li>
<li>2) 모든 데이터 $ x_{i} (i = 1, \ldots , N) $ 에서 각각의 중심위치 $ \mu_{k} $ 까지의 거리를 계산한다.</li>
<li>3) 각 데이터에서 가장 가까운 중심위치를 선택하여 각 데이터가 속하는 군집을 정한다.</li>
<li>4) 각 군집에 대해 중심위치 $ \mu_{k} $ 를 다시 계산한다.</li>
<li>5) 2~4를 반복한다.</li>
</ul>
</li>
</ul>
<ul>
<li>K-means 군집화는 <code>항상 수렴하지만 최종 군집화 결과가 전역 최적점이라는 보장은 없다. 군집화 결과는 초기 중심위치에 따라 달라질 수 있다.</code></li>
</ul>
<ul>
<li><code>K-means clusetering은 유클리드 거리를 사용하므로 너무 차원이 높을 때는 군집화 성능이 떨어질 수 있다.</code></li>
</ul>
<p><img src="/image/how_to_decide_mean_point_in_K_means_clustering_01.png" alt="K-means clustering의 작동 원리"></p>
<ul>
<li><p>Scikit-Learn의 cluster 서브패키지는 K-means 군집화를 위한 <code>KMeans</code> 클래스를 제공한다. 다음과 같은 인수를 받을 수 있다.</p>
<ul>
<li><code>n_clusters</code> : 군집의 개수</li>
<li><code>init</code> : 초기화 방법 <code>random</code>이면 무작위, <code>k-means++</code> 이면 K-means++ 방법. 또는 각 데이터의 군집 label</li>
<li><code>n_init</code> : 초기 중심위치 시도 횟수. default 10이고 10개의 무작위 중심위치 목록 중 가장 좋은 값을 선택한다.</li>
<li><code>max_iter</code> : 최대 반복 횟수</li>
<li><code>random_state</code> : 시드값</li>
</ul>
</li>
<li><p>다음은 <code>make_blobs</code> 커맨드를 통해 만든 데이터를 2개로 K-means 군집화하는 과정을 나타낸 것이다. 각각의 그림은 군집을 정하는 단계 3에서 멈춘 것이다. 마커(marker)의 모양은 소속된 군집을 나타내고 크기가 큰 마커가 해당 군집의 중심위치이다. 각 단계에서 중심위치는 전단계의 군집의 평균으로 다시 계산되는 것을 확인할 수 있다.</p>
</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line"></span><br><span class="line">X, _ = make_blobs(n_samples=20, random_state=4)</span><br><span class="line"></span><br><span class="line">def plot_KMeans(n):</span><br><span class="line">    model = KMeans(n_clusters=2, init=<span class="string">"random"</span>, n_init=1, max_iter=n, random_state=6).fit(X)</span><br><span class="line">    c0, c1 = model.cluster_centers_</span><br><span class="line">    plt.scatter(X[model.labels_ == 0, 0], X[model.labels_ == 0, 1], marker=<span class="string">'v'</span>, facecolor=<span class="string">'r'</span>, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    plt.scatter(X[model.labels_ == 1, 0], X[model.labels_ == 1, 1], marker=<span class="string">'^'</span>, facecolor=<span class="string">'y'</span>, edgecolors=<span class="string">'k'</span>)</span><br><span class="line">    plt.scatter(c0[0], c0[1], marker=<span class="string">'v'</span>, c=<span class="string">"r"</span>, s=200)</span><br><span class="line">    plt.scatter(c1[0], c1[1], marker=<span class="string">'^'</span>, c=<span class="string">"y"</span>, s=200)</span><br><span class="line">    plt.grid(False)</span><br><span class="line">    plt.title(<span class="string">"반복횟수=&#123;&#125;, 관성=&#123;:5.2f&#125;"</span>.format(n, -model.score(X)))</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(12, 10))</span><br><span class="line">plt.subplot(321)</span><br><span class="line">plot_KMeans(1)</span><br><span class="line">plt.subplot(322)</span><br><span class="line">plot_KMeans(2)</span><br><span class="line">plt.subplot(323)</span><br><span class="line">plot_KMeans(3)</span><br><span class="line">plt.subplot(324)</span><br><span class="line">plot_KMeans(4)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/k_means_center_moving_each_iter_step.png" alt="k-means 반복횟수 증가에 따른 중심 위치의 변화"></p>
<h3 id="K-means-알고리즘"><a href="#K-means-알고리즘" class="headerlink" title="K-means++ 알고리즘"></a>K-means++ 알고리즘</h3><ul>
<li><p>K-means++ 알고리즘은 <code>초기 중심위치를 설정하기 위한 알고리즘</code>이다. 다음과 같은 방법을 통해 <code>되도록 멀리 떨어진 중심위치 집합을 찾아낸다.</code></p>
<ul>
<li><ol>
<li>중심위치를 저장할 집합 $ M $ 준비</li>
</ol>
</li>
<li><ol>
<li>우선 하나의 중심위치 $ \mu_{0} $ 를 랜덤하게 선택하여 $ M $ 에 넣는다.</li>
</ol>
</li>
<li><ol>
<li>$ M $ 에 속하지 않는 모든 표본 $ x_{i} $ 에 대해 거리 $ d(M, x_{i}) $ 를 계산. $ d(M, x_{i}) $ 는 $ M $ 안의 모든 샘플 $ \mu_{k} $ 에 대해 $ d(\mu_{k}, x_{i}) $ 를 계산하여 가장 작은 값 선택</li>
</ol>
</li>
<li><ol>
<li>$ d(M, x_{i}) $ 에 비례한 확률로 다음 중심위치 $ \mu $를 선택</li>
</ol>
</li>
<li><ol>
<li>K 개의 중심위치를 선택할 때 까지 반복</li>
</ol>
</li>
<li><ol>
<li>K-means 방법 사용</li>
</ol>
</li>
</ul>
</li>
</ul>
<ul>
<li>중심을 구할 경우에는 각 데이터(포인트)들간의 거리의 합이 최소화하는 되는 지점이 중심이 될 것이므로 포인트간의 거리를 계산해야 한다. 여기서는 가장 간단하고 많이들 알고있는 유클리디안 거리와 맨하탄 거리를 소개할 것이다. 이외에도 마할라노비스의 거리등 <code>거리를 구하는 방법</code>은 여러가지가 있다. 이를 결정하는 것은 <code>해당 데이터에 성질(예를 들어 위도 경도 데이터라고 해서 무조건적으로 Haversine distance를 사용하는 것은아니지만 위도와 경도의 데이터를 통해 항공기들간의 시간대 별 위치의 중심점을 찾아야 하는 경우 Haversine distance를 사용하듯)에 따라 분석자가 결정해야 할 몫</code>이다.</li>
</ul>
<p><a href="https://en.wikipedia.org/wiki/Euclidean_distance" target="_blank" rel="noopener">참조 Distance type</a></p>
<p><img src="/image/how_to_decide_mean_point_in_K_means_clustering_02.png" alt="K-means clustering의 작동 원리"></p>
<h3 id="최적의-K를-찾는-방법"><a href="#최적의-K를-찾는-방법" class="headerlink" title="최적의 K를 찾는 방법"></a>최적의 K를 찾는 방법</h3><ul>
<li>K값을 설정하는 방법 중 가장 좋은 것은 군집의 개수를 미리 알고있는 경우이다. 예를 들어 뉴스기사를 각 카테고리별로 묶는다고 가정하면 크롤링해온 뉴스 기사의 카테고리 수를 미리 알고 있기 때문에 정확히 군집의 수를 결정할 수 있다. 허나, 대부분의 경우 우리는 데이터의 군집을 정확히 알고 있지 않은 상황에서 분석을 진행한다. 그러므로 다음과 같이 대체로 <code>Elbow method</code> 또는 <code>Silhouette method</code></li>
</ul>
<p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_01.png" alt="최적의 k를 찾는 방법 - 01"></p>
<ul>
<li><code>Elbow method</code>는 $ \frac{군집간 분산}{전체 분산} $인 비율을 보고 k를 결정한다. <code>WSS(군집 내 분산)은 작을 수록 군집의 중심에 많이 모여있는 것이므로 WSS(군집 내 분산)이 작을 수록 좋다는 말은 달리 말해 BSS(군집 간 분산)이 클수록 좋다는 의미</code>이기도 하다. 전체 데이터의 중심과 군집의 중심들간의 거리가 떨어져 있을 수록 군집이 잘 분류 되었다고 판단할 수 있기 때문이다. <code>군집의 개수가 늘어날 수록 BSS는 높은 값을 갖아 데이터의 개수만큼 K를 설정하면 비율의 값은 100%가 될 것</code>이다.</li>
</ul>
<p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_02.png" alt="최적의 k를 찾는 방법 - 02"></p>
<ul>
<li>위의 방법에서는 $ \frac{군집간 분산}{전체 분산} $의 비율 증가분을 보고 k를 결정하였다면 TSS(전체 분산)=BSS(군집 간 분산)+WSS(군집 내 분산)이므로 WSS가 가장 낮아지는 cluster의 개수를 찾는 방법도 동일한 결과를 얻을 수 있다는 사실을 알 수 있다.</li>
</ul>
<p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_03.png" alt="최적의 k를 찾는 방법 - 03"></p>
<ul>
<li>다른 한가지 방법인 <code>Silhouette method</code>도 거의 유사한 원리로 k를 결정하는 데 군집 내의 임의의 데이터와 다른 데이터들 간의 거리로 유사성을 파악하고, 다른 군집 중 제일 가까운 데이터와의 거리로 군집간의 유사성을 대표하는 지표로 삼아 제일 가까운 군집의 데이터의 거리와 객체내의 데이터들과의 거리의 차이가 크면 클수록 잘 군집이 형성되어졌다고 판단한다.</li>
</ul>
<p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_04.png" alt="최적의 k를 찾는 방법 - 04"></p>
<ul>
<li>최적의 k로 군집이 형성되었을 경우 가까운 군집과의 거리가 군집내 다른 데이터들의 거리보다 크기 때문에 분모에 해당하는 값은 b(i)로 되며, 분자의 가까운 군집과의 거리는 멀어질 것이며 군집내 데이터들과의 거리 차이는 0에 가까울 수록 최적의 k임을 의미하는 것이기 때문에 전체적인 값을 최대 1을 갖게되고 반대로 생각해보면 -1의 값을 최소로 갖게된다. <code>s(i)값이 클수록 적절한 k가 될 확률이 높다.</code></li>
</ul>
<p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_05.png" alt="최적의 k를 찾는 방법 - 05"></p>
<ul>
<li>실루엣 메서드의 간단한 예시를 보면 아래와 같으며, 실루엣 값의 평균값인 <code>실루엣 계수가 높은 값을 갖는 k를 최적의 k로 결정</code>한다.</li>
</ul>
<p><img src="/image/how_to_determine_optimal_k_in_k_means_clustering_06.png" alt="최적의 k를 찾는 방법 - 06"></p>
<blockquote>
<p>Elbow method를 사용하는 것이 일반적인데, 사실 k는 분석자가 미리 알고있을 경우가 가장 좋을 것이다. 허나, 모를 경우 최적의 k를 찾아도 고차원의 데이터에 대해서는 정확히 맞아 떨어지기 힘들다. <code>고차원 상에서도 데이터간의 거리를 계산할 순 있지만, 차원이 높아질수록 거리의 개념이 실질적으로 가까운지에 대한 감각이 무뎌지기 때문</code>이다. <code>그런 이유로 거리를 기반으로 하는 군집 분석 같은 경우에는 차원이 높아질수록 최적의 k로 결정한 군집이 실제로 맞지 않을 가능성이 커진다.</code></p>
</blockquote>
<ul>
<li>이러한 단점을 보완하여 나온 방법이 <code>K-medoid clustering</code>이다.</li>
</ul>
<h2 id="K-medoid-Clustering"><a href="#K-medoid-Clustering" class="headerlink" title="K-medoid Clustering"></a>K-medoid Clustering</h2><ul>
<li><code>Elbow method와 Silhouette method도 k를 찾을 수 있는 하나의 방법인 것이지 찾은 k가 절대적으로 최적이라는 것은 아니다.</code> 그래서 군집의 갯수를 찾는 것이 상당히 어려운 문제이다. k-means clustering은 아래와 같은 단점이 존재한다.</li>
</ul>
<p><img src="/image/disadvantages_of_k_means_clustering.png" alt="k-means clustering의 단점"></p>
<ul>
<li>위에서 언급한 k-means의 단점을 보완하기 위한 방법으로 가장 간단히 <code>평균대신 중간점을 사용</code>하는 방벙이다.</li>
</ul>
<p><img src="/image/what_is_k_medoid_clustering.png" alt="k-medoid clustering이란?"></p>
<ul>
<li>k-medoid clustering도 다른 clustering과 동일하게 찾은 k가 절대적으로 최적인 k를 의미하진 않는다. 예를 들어 아래 그림에서와 같이 <code>초승달 모양의 데이터 군집 분포는 거리에 기반한 모델인 k-means와 k-medoid clustering은 명확히 2군집으로 분류하기 어렵다.</code></li>
</ul>
<p><img src="/image/k_means_versus_k_medoid.png" alt="k-means와 k-medoid 비교"></p>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import datasets</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.cluster import KMeans</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="Iris-데이터를-활용하여-Kmeans-clustering"><a href="#Iris-데이터를-활용하여-Kmeans-clustering" class="headerlink" title="Iris 데이터를 활용하여 Kmeans clustering"></a>Iris 데이터를 활용하여 Kmeans clustering</h4><ul>
<li>iris data에서 2가지 feature를 가지고만 군집 분석을 실행할 것이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data[:, :2]</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X[:,0], X[:,1], c=y, cmap=<span class="string">'gist_rainbow'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Spea1 Length'</span>, fontsize=18)</span><br><span class="line">plt.ylabel(<span class="string">'Sepal Width'</span>, fontsize=18)</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/iris_data_scatter_plot_for_k_means_clustering.png" alt="iris 데이터 scatter plot"></p>
<ul>
<li>k=3으로 설정하고 k-means 군집분석을 진행해 보았다. 물론 이미 class label이 3개라는 사실을 알고 있으므로 여기선 3으로 설정하였으므로, 추후에 역으로 3으로 설정한 것이 타당한지에 대한 검증을 해 볼 것이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">km = KMeans(n_clusters = 3, n_jobs = 4, random_state=21)</span><br><span class="line">km.fit(X)</span><br></pre></td></tr></table></figure>
<h5 id="결과"><a href="#결과" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">KMeans(algorithm=<span class="string">'auto'</span>, copy_x=True, init=<span class="string">'k-means++'</span>, max_iter=300,</span><br><span class="line">       n_clusters=3, n_init=10, n_jobs=4, precompute_distances=<span class="string">'auto'</span>,</span><br><span class="line">       random_state=21, tol=0.0001, verbose=0)</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>k=3으로 설정한 k-means 군집 분석의 결과의 군집 중심은 다음과 같다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">centers = km.cluster_centers_</span><br><span class="line"><span class="built_in">print</span>(centers)</span><br></pre></td></tr></table></figure>
<h5 id="결과-1"><a href="#결과-1" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[[5.77358491 2.69245283]</span><br><span class="line"> [5.006      3.428     ]</span><br><span class="line"> [6.81276596 3.07446809]]</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>해당 k-means 군집 분석의 결과로 예측한 label을 토대로 동일한 scatter plot을 그려 실제 데이터와 비교해 볼 것이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">new_labels = km.labels_</span><br><span class="line">fig, axes = plt.subplots(1, 2, figsize=(16,8))</span><br><span class="line">axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap=<span class="string">'gist_rainbow'</span>,</span><br><span class="line">edgecolor=<span class="string">'k'</span>, s=150)</span><br><span class="line">axes[1].scatter(X[:, 0], X[:, 1], c=new_labels, cmap=<span class="string">'jet'</span>,</span><br><span class="line">edgecolor=<span class="string">'k'</span>, s=150)</span><br><span class="line">axes[0].set_xlabel(<span class="string">'Sepal length'</span>, fontsize=18)</span><br><span class="line">axes[0].set_ylabel(<span class="string">'Sepal width'</span>, fontsize=18)</span><br><span class="line">axes[1].set_xlabel(<span class="string">'Sepal length'</span>, fontsize=18)</span><br><span class="line">axes[1].set_ylabel(<span class="string">'Sepal width'</span>, fontsize=18)</span><br><span class="line">axes[0].tick_params(direction=<span class="string">'in'</span>, length=10, width=5, colors=<span class="string">'k'</span>, labelsize=20)</span><br><span class="line">axes[1].tick_params(direction=<span class="string">'in'</span>, length=10, width=5, colors=<span class="string">'k'</span>, labelsize=20)</span><br><span class="line">axes[0].set_title(<span class="string">'Actual'</span>, fontsize=18)</span><br><span class="line">axes[1].set_title(<span class="string">'Predicted'</span>, fontsize=18)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/difference_between_result_of_clustering_and_true_label.png" alt="clustering 결과와 실제 데이터 비교"></p>
<h4 id="2차원의-가상-데이터에-Kmeans-clustering"><a href="#2차원의-가상-데이터에-Kmeans-clustering" class="headerlink" title="2차원의 가상 데이터에 Kmeans clustering"></a>2차원의 가상 데이터에 Kmeans clustering</h4><ul>
<li>새롭게 군집분석을 위한 데이터를 가상으로 만들어 진행해 볼 것이다. feature의 개수는 2개이고 데이터의 수는 150개로 하고 군집의 표준편차는 0.5로 하여 3개의 군집을 띄게 데이터를 생성해 주었다. 이는 위의 iris 데이터와 거의 동일한 분포를 띄는 데이터를 얻을 수 있다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line"><span class="comment"># create dataset</span></span><br><span class="line">X, y = make_blobs(</span><br><span class="line">   n_samples=150, n_features=2,</span><br><span class="line">   centers=3, cluster_std=0.5,</span><br><span class="line">   shuffle=True, random_state=0</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">plt.scatter(</span><br><span class="line">   X[:, 0], X[:, 1],</span><br><span class="line">   c=<span class="string">'white'</span>, marker=<span class="string">'o'</span>,</span><br><span class="line">   edgecolor=<span class="string">'black'</span>, s=50</span><br><span class="line">)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/make_classification_blob_and_scatter_plot.png" alt="새롭게 생성한 2차원 데이터"></p>
<ul>
<li>새롭게 만든 데이터도 k=3으로 하여 군집분석을 진행한다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">km = KMeans(</span><br><span class="line">    n_clusters=3, init=<span class="string">'random'</span>,</span><br><span class="line">    n_init=10, max_iter=300,</span><br><span class="line">    tol=1e-04, random_state=0</span><br><span class="line">)</span><br><span class="line">y_km = km.fit_predict(X)</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>새롭게 만든 데이터를 군집 분석 결과로 예측된 label의 값에 따라 marker와 색을 다르게 주어 2차원 plot을 그려 보았다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(10,6))</span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 0, 0], X[y_km == 0, 1],</span><br><span class="line">    s=50, c=<span class="string">'lightgreen'</span>,</span><br><span class="line">    marker=<span class="string">'s'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 1'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 1, 0], X[y_km == 1, 1],</span><br><span class="line">    s=50, c=<span class="string">'orange'</span>,</span><br><span class="line">    marker=<span class="string">'o'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 2'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 2, 0], X[y_km == 2, 1],</span><br><span class="line">    s=50, c=<span class="string">'lightblue'</span>,</span><br><span class="line">    marker=<span class="string">'v'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 3'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the centroids</span></span><br><span class="line">plt.scatter(</span><br><span class="line">    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],</span><br><span class="line">    s=250, marker=<span class="string">'*'</span>,</span><br><span class="line">    c=<span class="string">'red'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'centroids'</span></span><br><span class="line">)</span><br><span class="line">plt.legend(scatterpoints=1)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/make_classification_blob_and_scatter_plot_predicted_value.png" alt="가상의 생성 데이터에 대한 군집 분석 결과"></p>
<h4 id="k-를-4로-할경우"><a href="#k-를-4로-할경우" class="headerlink" title="k 를 4로 할경우"></a>k 를 4로 할경우</h4><hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">km = KMeans(</span><br><span class="line">    n_clusters=4, init=<span class="string">'random'</span>,</span><br><span class="line">    n_init=10, max_iter=300,</span><br><span class="line">    tol=1e-04, random_state=0</span><br><span class="line">)</span><br><span class="line">y_km = km.fit_predict(X)</span><br></pre></td></tr></table></figure>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(10,6))</span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 0, 0], X[y_km == 0, 1],</span><br><span class="line">    s=50, c=<span class="string">'lightgreen'</span>,</span><br><span class="line">    marker=<span class="string">'s'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 1'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 1, 0], X[y_km == 1, 1],</span><br><span class="line">    s=50, c=<span class="string">'orange'</span>,</span><br><span class="line">    marker=<span class="string">'o'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 2'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 2, 0], X[y_km == 2, 1],</span><br><span class="line">    s=50, c=<span class="string">'lightblue'</span>,</span><br><span class="line">    marker=<span class="string">'v'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 3'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.scatter(</span><br><span class="line">    X[y_km == 3, 0], X[y_km == 3, 1],</span><br><span class="line">    s=50, c=<span class="string">'lightblue'</span>,</span><br><span class="line">    marker=<span class="string">'d'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'cluster 4'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the centroids</span></span><br><span class="line">plt.scatter(</span><br><span class="line">    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],</span><br><span class="line">    s=250, marker=<span class="string">'*'</span>,</span><br><span class="line">    c=<span class="string">'red'</span>, edgecolor=<span class="string">'black'</span>,</span><br><span class="line">    label=<span class="string">'centroids'</span></span><br><span class="line">)</span><br><span class="line">plt.legend(scatterpoints=1)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/make_classification_blob_and_scatter_plot_predicted_value_k_is_4.png" alt="k=4로 한 경우의 scatter plot"></p>
<ul>
<li>Elbow method를 통해 최적의 k를 찾기 위해 각 cluster의 개수 별로 WSS(군집 내 분산)을 계산하여 그래프로 시각화한다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">distortions = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(1, 11):</span><br><span class="line">    km = KMeans(</span><br><span class="line">        n_clusters=i, init=<span class="string">'random'</span>,</span><br><span class="line">        n_init=10, max_iter=300,</span><br><span class="line">        tol=1e-04, random_state=0</span><br><span class="line">    )</span><br><span class="line">    km.fit(X)</span><br><span class="line">    <span class="comment">#inertia가 군집 내의 분산을 의미</span></span><br><span class="line">    distortions.append(km.inertia_)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot</span></span><br><span class="line">plt.figure(figsize=(10,6))</span><br><span class="line">plt.plot(range(1, 11), distortions, marker=<span class="string">'o'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Number of clusters'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Distortion'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/decide_to_optimal_k_using_Elbow_method.png" alt="Elbow method를 사용한 최적의 k 추정"></p>
<h4 id="비정형-데이터에-대한-K-means-군집-분석"><a href="#비정형-데이터에-대한-K-means-군집-분석" class="headerlink" title="비정형 데이터에 대한 K-means 군집 분석"></a>비정형 데이터에 대한 K-means 군집 분석</h4><ul>
<li>정형 데이터에 관한 군집 분석 말고도 비정형 데이터인 문자열 데이터에 관한 군집 분석도 실행해 볼 것이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from sklearn.cluster import KMeans</span><br><span class="line">from sklearn.metrics import adjusted_rand_score</span><br><span class="line"></span><br><span class="line">documents = [<span class="string">"This little kitty came to play when I was eating at a restaurant."</span>,<span class="string">"hello kitty is my favorite character"</span>,</span><br><span class="line">             <span class="string">"Merley has the best squooshy kitten belly."</span>,<span class="string">"Is Google translator so good?"</span>,<span class="string">"google google"</span></span><br><span class="line">             <span class="string">"google Translate app is incredible."</span>,<span class="string">"My dog s name is Kong"</span>,<span class="string">"dog dog dog"</span>,<span class="string">"cat cat"</span></span><br><span class="line">             <span class="string">"If you open 100 tab in google you get a smiley face."</span>,<span class="string">"Kong is a very cute and lovely dog"</span>,</span><br><span class="line">             <span class="string">"Best cat photo I've ever taken."</span>,<span class="string">"This is a cat house"</span></span><br><span class="line">             <span class="string">"Climbing ninja cat kitty."</span>,<span class="string">"What's your dog's name?"</span>,<span class="string">"Cat s paws look like jelly"</span>,</span><br><span class="line">             <span class="string">"Impressed with google map feedback."</span>,<span class="string">"I want to join google"</span>,<span class="string">"You have to wear a collar when you walk the dog"</span>,</span><br><span class="line">             <span class="string">"Key promoter extension for google Chrome."</span>,<span class="string">"Google is the best company"</span>,<span class="string">"Google researcher"</span>]</span><br><span class="line"></span><br><span class="line">vectorizer = TfidfVectorizer(stop_words=<span class="string">'english'</span>)</span><br><span class="line">X = vectorizer.fit_transform(documents)</span><br></pre></td></tr></table></figure>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">true_k = 3</span><br><span class="line">model = KMeans(n_clusters=true_k, init=<span class="string">'k-means++'</span>, max_iter=100, n_init=1)</span><br><span class="line">model.fit(X)</span><br></pre></td></tr></table></figure>
<h5 id="결과-2"><a href="#결과-2" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">KMeans(algorithm=<span class="string">'auto'</span>, copy_x=True, init=<span class="string">'k-means++'</span>, max_iter=100,</span><br><span class="line">       n_clusters=3, n_init=1, n_jobs=None, precompute_distances=<span class="string">'auto'</span>,</span><br><span class="line">       random_state=None, tol=0.0001, verbose=0)</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li><code>아래의 label은 위의 군집분석을 실행 할 때마다 다르기에 군집의 결과를 살펴보면서 어떤 데이터들이 같은 군집으로 묶였는지 확인해 본다.</code>그러므로 위의 코드를 실행시 필자와 다른 결과를 볼 수 있을 것이다. 허나 군집으로 묶여있는 데이터들은 동일 할 것이다.</li>
</ul>
<h4 id="예측-결과-군집-0인-데이터들의-문서"><a href="#예측-결과-군집-0인-데이터들의-문서" class="headerlink" title="예측 결과 군집 0인 데이터들의 문서"></a>예측 결과 군집 0인 데이터들의 문서</h4><ul>
<li>주로 고양이를 의미하는 kitty와 cat을 갖는 문서를 하나의 군집으로 묶어 주었다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(documents, model.labels_) <span class="keyword">if</span>  y == 0]</span><br></pre></td></tr></table></figure>
<h5 id="결과-3"><a href="#결과-3" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'This little kitty came to play when I was eating at a restaurant.'</span>,</span><br><span class="line"> <span class="string">'hello kitty is my favorite character'</span>,</span><br><span class="line"> <span class="string">'Merley has the best squooshy kitten belly.'</span>,</span><br><span class="line"> <span class="string">'cat catIf you open 100 tab in google you get a smiley face.'</span>,</span><br><span class="line"> <span class="string">"Best cat photo I've ever taken."</span>,</span><br><span class="line"> <span class="string">'This is a cat houseClimbing ninja cat kitty.'</span>,</span><br><span class="line"> <span class="string">'Cat s paws look like jelly'</span>]</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="예측-결과-군집-1인-데이터들의-문서"><a href="#예측-결과-군집-1인-데이터들의-문서" class="headerlink" title="예측 결과 군집 1인 데이터들의 문서"></a>예측 결과 군집 1인 데이터들의 문서</h4><ul>
<li>주로 dog를 갖는 문서를 하나의 군집으로 묶어주었다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(documents, model.labels_) <span class="keyword">if</span>  y == 1]</span><br></pre></td></tr></table></figure>
<h5 id="결과-4"><a href="#결과-4" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'My dog s name is Kong'</span>,</span><br><span class="line"> <span class="string">'dog dog dog'</span>,</span><br><span class="line"> <span class="string">'Kong is a very cute and lovely dog'</span>,</span><br><span class="line"> <span class="string">"What's your dog's name?"</span>,</span><br><span class="line"> <span class="string">'You have to wear a collar when you walk the dog'</span>]</span><br></pre></td></tr></table></figure>
<hr>
<h4 id="예측-결과-군집-2인-데이터들의-문서"><a href="#예측-결과-군집-2인-데이터들의-문서" class="headerlink" title="예측 결과 군집 2인 데이터들의 문서"></a>예측 결과 군집 2인 데이터들의 문서</h4><hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[x <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(documents, model.labels_) <span class="keyword">if</span>  y == 2]</span><br></pre></td></tr></table></figure>
<h5 id="결과-5"><a href="#결과-5" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">'Is Google translator so good?'</span>,</span><br><span class="line"> <span class="string">'google googlegoogle Translate app is incredible.'</span>,</span><br><span class="line"> <span class="string">'Impressed with google map feedback.'</span>,</span><br><span class="line"> <span class="string">'I want to join google'</span>,</span><br><span class="line"> <span class="string">'Key promoter extension for google Chrome.'</span>,</span><br><span class="line"> <span class="string">'Google is the best company'</span>,</span><br><span class="line"> <span class="string">'Google researcher'</span>]</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>아래와 같은 새로운 문자열의 예측한 경우 위에서 살펴본 키워드를 갖으면 해당 군집으로 예측하는 것을 볼 수 있다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Y = vectorizer.transform([<span class="string">"chrome browser to open."</span>])</span><br><span class="line">prediction = model.predict(Y)</span><br><span class="line"><span class="built_in">print</span>(prediction)</span><br><span class="line"></span><br><span class="line">Y = vectorizer.transform([<span class="string">"I want to have a dog"</span>])</span><br><span class="line">prediction = model.predict(Y)</span><br><span class="line"><span class="built_in">print</span>(prediction)</span><br><span class="line"></span><br><span class="line">Y = vectorizer.transform([<span class="string">"My cat is hungry."</span>])</span><br><span class="line">prediction = model.predict(Y)</span><br><span class="line"><span class="built_in">print</span>(prediction)</span><br></pre></td></tr></table></figure>
<h5 id="결과-6"><a href="#결과-6" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[0]</span><br><span class="line">[1]</span><br><span class="line">[0]</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>다음은 K-means++ 방법을 사용하여 MNIST Digit 이미지 데이터를 군집화한 결과이다. 각 군집에서 10개씩의 데이터만 표시하였다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.datasets import load_digits</span><br><span class="line"></span><br><span class="line">digits = load_digits()</span><br><span class="line"></span><br><span class="line">model = KMeans(init=<span class="string">"k-means++"</span>, n_clusters=10, random_state=0)</span><br><span class="line">model.fit(digits.data)</span><br><span class="line">y_pred = model.labels_</span><br><span class="line"></span><br><span class="line">def show_digits(images, labels):</span><br><span class="line">    f = plt.figure(figsize=(8, 2))</span><br><span class="line">    i = 0</span><br><span class="line">    <span class="keyword">while</span> (i &lt; 10 and i &lt; images.shape[0]):</span><br><span class="line">        ax = f.add_subplot(1, 10, i + 1)</span><br><span class="line">        ax.imshow(images[i], cmap=plt.cm.bone)</span><br><span class="line">        ax.grid(False)</span><br><span class="line">        ax.set_title(labels[i])</span><br><span class="line">        ax.xaxis.set_ticks([])</span><br><span class="line">        ax.yaxis.set_ticks([])</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">        i += 1</span><br><span class="line"></span><br><span class="line">def show_cluster(images, y_pred, cluster_number):</span><br><span class="line">    images = images[y_pred == cluster_number]</span><br><span class="line">    y_pred = y_pred[y_pred == cluster_number]</span><br><span class="line">    show_digits(images, y_pred)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(10):</span><br><span class="line">    show_cluster(digits.images, y_pred, i)</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/mnist_k_means_cluster_result.png" alt="MNIST 데이터 K-means 군집화한 결과"></p>
<ul>
<li><code>이미지의 제목에 있는 숫자는 군집 번호에 지나지 않으므로 원래 숫자의 번호와 일치하지 않는다.</code> 하지만 이를 예측문제라고 가정하고 분류 결과 행렬을 만들면 아래와 같다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics import confusion_matrix</span><br><span class="line"></span><br><span class="line">confusion_matrix(digits.target, y_pred)</span><br></pre></td></tr></table></figure>
<h5 id="결과-7"><a href="#결과-7" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array([[  1,   0,   0,   0,   0, 177,   0,   0,   0,   0],</span><br><span class="line">       [  0,   1,  55,  24,   0,   0,   0,   2,   1,  99],</span><br><span class="line">       [  0,  13,   2, 148,   2,   1,   3,   0,   0,   8],</span><br><span class="line">       [  0, 155,   0,   1,  11,   0,   7,   0,   2,   7],</span><br><span class="line">       [163,   0,   7,   0,   0,   0,   7,   0,   0,   4],</span><br><span class="line">       [  2,   1,   0,   0,  42,   0,   0,   1, 136,   0],</span><br><span class="line">       [  0,   0,   1,   0,   0,   1,   0, 177,   0,   2],</span><br><span class="line">       [  0,   0,   0,   0,   0,   0, 177,   0,   0,   2],</span><br><span class="line">       [  0,   4,   6,   3,  48,   0,   5,   2,   4, 102],</span><br><span class="line">       [  0,   6,  20,   0, 139,   0,   7,   0,   6,   2]])</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>이 군집화 결과의 ARI, AMI, 실루엣 계수 값은 다음과 같다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"ARI:"</span>, adjusted_rand_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"AMI:"</span>, adjusted_mutual_info_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Silhouette Score:"</span>, silhouette_score(digits.data, y_pred))</span><br></pre></td></tr></table></figure>
<h5 id="결과-8"><a href="#결과-8" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ARI: 0.6703800183468681</span><br><span class="line">AMI: 0.7417664506416767</span><br><span class="line">Silhouette Score: 0.18249069204151275</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>군집화 결과를 주성분 분석을 통해 2차원에 투영하면 다음과 같다. 겹쳐져 있는 부분은 고차원상에서는 떨어져 있을 수 있다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.decomposition import PCA</span><br><span class="line"></span><br><span class="line">pca = PCA(n_components=2)</span><br><span class="line">X = pca.fit_transform(digits.data)</span><br><span class="line"></span><br><span class="line">plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=plt.cm.Set1)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<p><img src="/image/MNIST_K_means_clustering_PCA_2_ponents.png" alt="MNIST 데이터를 PCA를 통해 2개의 주성분 벡터를 갖도록 차원을 축소한 그래프"></p>
<ul>
<li><code>K-means clusetering은 유클리드 거리를 사용하므로 너무 차원이 높을 때는 군집화 성능이 떨어질 수 있다. 이 때는 차원 축소를 한 후 군집화를 하는 것이 도움이 될 수도 있다.</code></li>
</ul>
<ul>
<li>MNIST Digit 데이터를 10차원으로 차원 축소하여 K-means 군집화하고 ARI, AMI, 실루엣 계수를 각각 계산하여 차원축소를 하지 않았을 때와 비교하라.</li>
</ul>
<h2 id="차원을-축소하여도-실루엣계수를-포함한-모든-지표들이-살펴보았을-때-오히려-낮아졌다-이를-통해-복잡한-데이터에-대해서는-적합하지-않은-실루엣계수의-단점을-확인-할-수-있으며-무조건-적으로-PCA를-하더라도-높아지지-않는다는-점을-확인할-수-있다"><a href="#차원을-축소하여도-실루엣계수를-포함한-모든-지표들이-살펴보았을-때-오히려-낮아졌다-이를-통해-복잡한-데이터에-대해서는-적합하지-않은-실루엣계수의-단점을-확인-할-수-있으며-무조건-적으로-PCA를-하더라도-높아지지-않는다는-점을-확인할-수-있다" class="headerlink" title="-  차원을 축소하여도 실루엣계수를 포함한 모든 지표들이 살펴보았을 때 오히려 낮아졌다. 이를 통해 복잡한 데이터에 대해서는 적합하지 않은 실루엣계수의 단점을 확인 할 수 있으며, 무조건 적으로 PCA를 하더라도 높아지지 않는다는 점을 확인할 수 있다."></a>-  차원을 축소하여도 실루엣계수를 포함한 모든 지표들이 살펴보았을 때 오히려 낮아졌다. 이를 통해 복잡한 데이터에 대해서는 적합하지 않은 실루엣계수의 단점을 확인 할 수 있으며, 무조건 적으로 PCA를 하더라도 높아지지 않는다는 점을 확인할 수 있다.</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score</span><br><span class="line"></span><br><span class="line">model = KMeans(init=<span class="string">"k-means++"</span>, n_clusters=10, random_state=0)</span><br><span class="line">model.fit(digits.data)</span><br><span class="line">y_pred = model.labels_</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"ARI:"</span>, adjusted_rand_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"AMI:"</span>, adjusted_mutual_info_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Silhouette Score:"</span>, silhouette_score(digits.data, y_pred))</span><br></pre></td></tr></table></figure>
<h5 id="결과-9"><a href="#결과-9" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ARI: 0.6686991223627669</span><br><span class="line">AMI: 0.7397973157276612</span><br><span class="line">Silhouette Score: 0.18251916424600556</span><br></pre></td></tr></table></figure>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pca = PCA(n_components=10)</span><br><span class="line">X = pca.fit_transform(digits.data)</span><br><span class="line"></span><br><span class="line">model = KMeans(init=<span class="string">"k-means++"</span>, n_clusters=10, random_state=0)</span><br><span class="line">model.fit(X)</span><br><span class="line">y_pred = model.labels_</span><br><span class="line"></span><br><span class="line">from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score, silhouette_score</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"ARI:"</span>, adjusted_rand_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"AMI:"</span>, adjusted_mutual_info_score(digits.target, y_pred))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Silhouette Score:"</span>, silhouette_score(digits.data, y_pred))</span><br></pre></td></tr></table></figure>
<h5 id="결과-10"><a href="#결과-10" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ARI: 0.6492572540256956</span><br><span class="line">AMI: 0.7183058275931469</span><br><span class="line">Silhouette Score: 0.18116247153029966</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="미니배치-K-means-군집화"><a href="#미니배치-K-means-군집화" class="headerlink" title="미니배치 K-means 군집화"></a>미니배치 K-means 군집화</h3><ul>
<li>K-means 방법에서는 중심위치와 모든 데이터 사이의 거리를 계산해야 하기 때문에 데이터의 갯수가 많아지면 계산량도 늘어난다. <code>데이터의 수가 너무 많을 때는 미니 배치 K-means 군집화 방법을 사용하면 계산량을 줄일 수 있다.</code> 미니배치 K-means 군집화는 데이터를 미니배치 크기만큼 무작위로 분리하여 K-means 군집화를 한다. 모든 데이터를 한꺼번에 썼을 때와 결과가 다를 수는 있지만 큰 차이가 없다.</li>
</ul>
<ul>
<li>Scikit-Learn의 cluster 서브패키지는 미니배치 K-means 군집화를 위한 <code>MiniBatchKMeans</code> 클래스를 제공한다. 미니배치 크기 <code>batch_size</code>인수를 추가로 받는다.</li>
</ul>
<ul>
<li>150,000개의 데이터를 사용하여 실행 시간을 비교할 것이다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.cluster import MiniBatchKMeans</span><br><span class="line">from sklearn.datasets import make_blobs</span><br><span class="line"></span><br><span class="line">X, y = make_blobs(n_samples=150000, cluster_std=[1.0, 2.5, 0.5], random_state=170)</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>미니배치 군집화의 속도가 훨씬 빠른 것을 알 수 있다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line">model1 = KMeans(n_clusters=3).fit(X)</span><br></pre></td></tr></table></figure>
<h5 id="결과-11"><a href="#결과-11" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CPU <span class="built_in">times</span>: user 1.01 s, sys: 66.8 ms, total: 1.08 s</span><br><span class="line">Wall time: 604 ms</span><br></pre></td></tr></table></figure>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line"></span><br><span class="line">model2 = MiniBatchKMeans(n_clusters=3, batch_size=1000, compute_labels=True).fit(X)</span><br></pre></td></tr></table></figure>
<h5 id="결과-12"><a href="#결과-12" class="headerlink" title="결과"></a>결과</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CPU <span class="built_in">times</span>: user 191 ms, sys: 7.71 ms, total: 199 ms</span><br><span class="line">Wall time: 158 ms</span><br></pre></td></tr></table></figure>
<hr>
<ul>
<li>군집화 결과는 그다지 차이가 없다.</li>
</ul>
<hr>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">idx = np.random.randint(150000, size=300)</span><br><span class="line">plt.subplot(121)</span><br><span class="line">plt.scatter(X[idx, 0], X[idx, 1], c=model1.labels_[idx])</span><br><span class="line">plt.title(<span class="string">"K-평균 군집화"</span>)</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plt.scatter(X[idx, 0], X[idx, 1], c=model2.labels_[idx])</span><br><span class="line">plt.title(<span class="string">"미니배치 K-평균 군집화"</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/image/mini_batch_k_means_vs_k_means_clustering_result.png" alt="미니배치와 일반적인 K-means 군집화 결과"></p>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "Clustering - K-means, K-medoid",
        "image": "https://heung-bae-lee.github.io/image/how_to_decide_mean_point_in_K_means_clustering_01.png",
        "keywords": "",
        "genre": "machine learning",
        "datePublished": "2020-05-30",
        "dateCreated": "2020-05-30",
        "dateModified": "2020-06-06",
        "url": "https://heung-bae-lee.github.io/2020/05/30/machine_learning_18/",
        "description": "K-means Clusterig
각 군집에 할당된 포인트들의 평균 좌표를 이용해 중심점을 반복적으로 업데이트하면서 군집을 분류해 나가는 방법
가장 단순하고 빠른 군집화 방법




초기에 제일 처음 랜덤하게 포인트를 하나 잡아서 그 포인트에 가까운 데이터들을 같은 군집으로 할당해준다. 그 다음 아래와 같은 방법으로 반복한다.
다음과 같은 목저함수 값이 최소"
        "wordCount": 4327
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2020/06/04/machine_learning_19/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            Clustering - Hierarchical, DBSCAN, Affinity Propagation
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2020/05/29/machine_learning_17/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">Clustering</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/06/09/linear_algebra_06/" class="title">Least Squares Problem &amp; Orthogonal Projection</a></p>
                            <p class="item-date"><time datetime="2020-06-09T14:12:36.000Z" itemprop="datePublished">2020-06-09</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/06/09/linear_algebra_05/" class="title">Linear Transformation &amp; onto, ono-to-one의 개념</a></p>
                            <p class="item-date"><time datetime="2020-06-09T05:23:12.000Z" itemprop="datePublished">2020-06-09</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/06/08/linear_algebra_04/" class="title">Linear Independence, Span, and Subspace</a></p>
                            <p class="item-date"><time datetime="2020-06-08T06:52:22.000Z" itemprop="datePublished">2020-06-08</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/06/machine_learning_20/" class="title">Imbalanced Data</a></p>
                            <p class="item-date"><time datetime="2020-06-05T16:52:20.000Z" itemprop="datePublished">2020-06-06</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/06/04/machine_learning_19/" class="title">Clustering - Hierarchical, DBSCAN, Affinity Propagation</a></p>
                            <p class="item-date"><time datetime="2020-06-04T13:46:15.000Z" itemprop="datePublished">2020-06-04</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">21</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">June 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2020/05/30/machine_learning_18/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
