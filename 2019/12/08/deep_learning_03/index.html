<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">

    

    
    <title>심층 신경망의 구조 | DataLatte&#39;s IT Blog</title>
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    
        <meta name="keywords" content>
    
    
    <meta name="description" content="심층 신경망의 구조     은닉 계층 추가 = 특징의 비선형 변환 추가!!선형 변환의 이해  선형대수의 선형 변환을 함수의 개념에서 보았을때, 입력 차원(n)이 출력 차원(m)보다 크다면 Onto(전사함수: 모든 공역이 치역이 되있는 상태)가 될 수 있지만, 그 반대인 경우는 적은 차원을 갖는 입력벡터의 차원으로 일부분의 출력 벡터의 차원을 커버하는 것이">
<meta property="og:type" content="article">
<meta property="og:title" content="심층 신경망의 구조">
<meta property="og:url" content="https://heung-bae-lee.github.io/2019/12/08/deep_learning_03/index.html">
<meta property="og:site_name" content="DataLatte&#39;s IT Blog">
<meta property="og:description" content="심층 신경망의 구조     은닉 계층 추가 = 특징의 비선형 변환 추가!!선형 변환의 이해  선형대수의 선형 변환을 함수의 개념에서 보았을때, 입력 차원(n)이 출력 차원(m)보다 크다면 Onto(전사함수: 모든 공역이 치역이 되있는 상태)가 될 수 있지만, 그 반대인 경우는 적은 차원을 갖는 입력벡터의 차원으로 일부분의 출력 벡터의 차원을 커버하는 것이">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://heung-bae-lee.github.io/image/Neuron_01.png">
<meta property="og:updated_time" content="2019-12-20T04:38:06.937Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="심층 신경망의 구조">
<meta name="twitter:description" content="심층 신경망의 구조     은닉 계층 추가 = 특징의 비선형 변환 추가!!선형 변환의 이해  선형대수의 선형 변환을 함수의 개념에서 보았을때, 입력 차원(n)이 출력 차원(m)보다 크다면 Onto(전사함수: 모든 공역이 치역이 되있는 상태)가 될 수 있지만, 그 반대인 경우는 적은 차원을 갖는 입력벡터의 차원으로 일부분의 출력 벡터의 차원을 커버하는 것이">
<meta name="twitter:image" content="https://heung-bae-lee.github.io/image/Neuron_01.png">
<meta property="fb:app_id" content="100003222637819">


    <link rel="canonical" href="https://heung-bae-lee.github.io/2019/12/08/deep_learning_03/">

    

    

    <link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="/libs/titillium-web/styles.css">
    <link rel="stylesheet" href="/libs/source-code-pro/styles.css">

    <link rel="stylesheet" href="/css/style.css">

    <script src="/libs/jquery/3.3.1/jquery.min.js"></script>
    
    
        <link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">
    
    
        <link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">
    
    
        <script type="text/javascript">
(function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-154199624-1', 'auto');
ga('send', 'pageview');

</script>

    
    


    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle" style="display:block" data-ad-client="ca-pub-4604833066889492" data-ad-slot="4588503508" data-ad-format="auto" data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

<link rel="alternate" href="/rss2.xml" title="DataLatte's IT Blog" type="application/rss+xml">
</head>
</html>
<body>
    <div id="wrap">
        <header id="header">
    <div id="header-outer" class="outer">
        <div class="container">
            <div class="container-inner">
                <div id="header-title">
                    <h1 class="logo-wrap">
                        <a href="/" class="logo"></a>
                    </h1>
                    
                        <h2 class="subtitle-wrap">
                            <p class="subtitle">DataLatte&#39;s IT Blog using Hexo</p>
                        </h2>
                    
                </div>
                <div id="header-inner" class="nav-container">
                    <a id="main-nav-toggle" class="nav-icon fa fa-bars"></a>
                    <div class="nav-container-inner">
                        <ul id="main-nav">
                            
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/">Home</a>
                                </li>
                            
                                        <ul class="main-nav-list"><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Bayes/">Bayes</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/CS231n/">CS231n</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Front-end/">Front end</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Kaggle/">Kaggle</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/NLP/">NLP</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Python/">Python</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Recommendation-System/">Recommendation System</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/crawling/">crawling</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/data-engineering/">data engineering</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/deep-learning/">deep learning</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/growth-hacking/">growth hacking</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/hexo/">hexo</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/linear-algebra/">linear algebra</a></li><li class="main-nav-list-item"><a class="main-nav-list-link" href="/categories/machine-learning/">machine learning</a></li></ul>
                                    
                                <li class="main-nav-list-item" >
                                    <a class="main-nav-list-link" href="/about/index.html">About</a>
                                </li>
                            
                        </ul>
                        <nav id="sub-nav">
                            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search" />
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something..." />
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>
<script src="/js/insight.js"></script>

</div>
                        </nav>
                    </div>
                </div>
            </div>
        </div>
    </div>
</header>

        <div class="container">
            <div class="main-body container-inner">
                <div class="main-body-inner">
                    <section id="main">
                        <div class="main-body-header">
    <h1 class="header">
    
    <a class="page-title-link" href="/categories/deep-learning/">deep learning</a>
    </h1>
</div>

                        <div class="main-body-content">
                            <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 상단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="4588503508"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

			    <article id="post-deep_learning_03" class="article article-single article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
            <header class="article-header">
                
    
        <h1 class="article-title" itemprop="name">
        심층 신경망의 구조
        </h1>
    

            </header>
        
        
            <div class="article-meta">
                
    <div class="article-date">
        <a href="/2019/12/08/deep_learning_03/" class="article-date">
            <time datetime="2019-12-07T15:00:00.000Z" itemprop="datePublished">2019-12-08</time>
        </a>
    </div>

		

                
            </div>
        
        
        <div class="article-entry" itemprop="articleBody">
            <h2 id="심층-신경망의-구조"><a href="#심층-신경망의-구조" class="headerlink" title="심층 신경망의 구조"></a>심층 신경망의 구조</h2><p><img src="/image/Neuron_01.png" alt="Neuron"></p>
<p><img src="/image/shallowNN_01.png" alt="shallow NN"></p>
<p><img src="/image/Deep_Neural_Network.png" alt="심층 신경망(Deep Neural Network)"></p>
<p><img src="/image/what_is_difference with_DNN.png" alt="심층신경망은 무엇이 다를까?"></p>
<ul>
<li><p><code>은닉 계층 추가 = 특징의 비선형 변환 추가!!</code><br><a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" target="_blank" rel="noopener">선형 변환의 이해</a></p>
</li>
<li><p>선형대수의 선형 변환을 함수의 개념에서 보았을때, 입력 차원(n)이 출력 차원(m)보다 크다면 Onto(전사함수: 모든 공역이 치역이 되있는 상태)가 될 수 있지만, 그 반대인 경우는 적은 차원을 갖는 입력벡터의 차원으로 일부분의 출력 벡터의 차원을 커버하는 것이 되는 것이므로, 전사함수가 될 수 없다. 또한, 이런 입력차원(n)이 출력 차원(m)보다 작은 경우의 구조를 우리는 딥러닝 네트워크 구조에서도 볼 수 있다. 예를 들면 GAN이나 Auto Encoder의 decoder구조가 가장 쉬운 예시일 것이다. 여기서의 의문은 그렇다면, 일부분의 차원으로 피처를 잘 배울 수 있는지가 의문일 것이다. <code>허나, 그 일부의 차원이 원래 갖고 있던 특성에서 나올법한 특성들만을 생성해 주므로 걱정하지 않아도 된다.</code></p>
</li>
<li><p>또한, 선형시스템의 곱으로 노드들의 연산을 표현할 수 있는데, 여기서, 예를 들어, 특징벡터1과 특징벡터2간의 방향이 비슷한 즉, Orthogonal하지 않고 방향이 비슷한 벡터를 통해 연산을 진행하면 다음 층에서는 노드들 중에 비슷한 특징에 대한 정보를 포함하고 있을 것이다. Inner product를 projection의 개념에서 살펴보면, 어떠한 벡터가 다른 방향의 벡터에 projection을 하는 것은 그 projection한 벡터가 그 방향의 벡터가 어느 정도의 성분을 가지고 있는지를 의미하므로 <code>선형대수 측면에서 위에서 각 피처들간의 곱의 연산들에 의한 새로운 피처들의 생성은 projection된 길이를 비교하는 행위와 동일할 것이다.</code></p>
</li>
</ul>
<h2 id="역전파-학습법의-개념"><a href="#역전파-학습법의-개념" class="headerlink" title="역전파 학습법의 개념"></a>역전파 학습법의 개념</h2><p><img src="/image/Algorithm_learning_and_differentiation.png" alt="알고리즘의 학습과 미분"></p>
<p><img src="/image/compute_function_with_dependency.png" alt="의존성이 있는 함수의 계산"></p>
<ul>
<li>y를 구하려면 x와 z를 알아야 하는데, x와 z에는 중복된 연산이 있어서 비효율적이다.</li>
</ul>
<p><img src="/image/Dynamic_Programming.png" alt="동적 계획법(Dynamic Programming)"></p>
<ul>
<li><code>처음 계산할 때 값을 저장해주어서 중복계산이 발생하지 않도록 해준다.</code></li>
</ul>
<p><img src="/image/chain_rule.png" alt="Chain rule"></p>
<p><img src="/image/differentiation_of_DNN.png" alt="심층 신경망의 미분(출력계층)"></p>
<p><img src="/image/differentiation_of_DNN_01.png" alt="심층 신경망의 미분(은닉계층1)"></p>
<p><img src="/image/differentiation_of_DNN_02.png" alt="심층 신경망의 미분(은닉계층2)"></p>
<p><img src="/image/Forward_inference.png" alt="순방향 전파(Forward Propagation)"></p>
<ul>
<li>학습을 마친 후 validation set이나 test set에 적용할 때는 더 이상 학습을 하지 않으므로 이 순방향 추론만을 사용한다.</li>
</ul>
<p><img src="/image/Back_Propagation.png" alt="역전파 학습법(Back-Propagation)"></p>
<h2 id="심층-신경망의-수학적-이해"><a href="#심층-신경망의-수학적-이해" class="headerlink" title="심층 신경망의 수학적 이해"></a>심층 신경망의 수학적 이해</h2><p><img src="/image/FC_Layer.png" alt="전결합 계층"></p>
<p><img src="/image/DNN_00.png" alt="심층 신경망"></p>
<h2 id="역전파-학습의-필요성"><a href="#역전파-학습의-필요성" class="headerlink" title="역전파 학습의 필요성"></a>역전파 학습의 필요성</h2><p><img src="/image/BlackBoxModel.png" alt="블랙박스 모델"></p>
<p><img src="/image/learning_BlackBoxModel.png" alt="블랙박스 모델의 학습"></p>
<p><img src="/image/Numerical_Gradient.png" alt="수치적 기울기(Numerical Gradient"></p>
<p><img src="/image/Numerical_Gradient_of_BlackBoxmodel.png" alt="블랙박스 모델의 수치적 기울기"></p>
<ul>
<li>(N+1번) 손실함수를 평가한다고 하는데 그 이유는 기준점이 되는 손실함수를 먼저 한번 계산하고 나머지 편미분시에 가각 N번 평가하기 때문이다.</li>
</ul>
<p><img src="/image/Numerical_Gradient_of_DNN.png" alt="심층 신경망의 수치적 기울기"></p>
<h2 id="합성함수와-연쇄-법칙"><a href="#합성함수와-연쇄-법칙" class="headerlink" title="합성함수와 연쇄 법칙"></a>합성함수와 연쇄 법칙</h2><p><img src="/image/chain_rule_01.png" alt="연쇄 법칙"></p>
<p><img src="/image/series_connection_of_two_function.png" alt="직렬 연결된 두 함수의 미분"></p>
<p><img src="/image/differentiation_and_chain_rule.png" alt="미분과 연쇄 법칙"></p>
<p><img src="/image/expansion_of_chain_rule.png" alt="연쇄법칙의 확장"></p>
<h2 id="역전파-학습법의-수식적-이해"><a href="#역전파-학습법의-수식적-이해" class="headerlink" title="역전파 학습법의 수식적 이해"></a>역전파 학습법의 수식적 이해</h2><p><img src="/image/DNN_aspect_of_composite function.png" alt="합성 함수로서의 심층 신경망"></p>
<p><img src="/image/DNN_aspect_of_learning.png" alt="학습관점에서 본 심층 신경망"></p>
<p><img src="/image/DNN_chain_rule.png" alt="심층신경망의 연쇄법칙"></p>
<ul>
<li><code>미분하고자 하는 경로 사이에 있는 모든 미분값을 알아야 원하는 미분을 구할 수 있다는 의미이다.</code></li>
</ul>
<p><img src="/image/FCLayer_differentiation.png" alt="전결합 계층의 미분(1)"></p>
<p><img src="/image/FCLayer_differentiation_01.png" alt="전결합 계층의 미분(2)"></p>
<p><img src="/image/differentiation_of_sigmoid_function.png" alt="Sigmoid 함수의 미분"></p>
<p><img src="/image/Back_Propagation_algorithm.png" alt="역전파 알고리즘"></p>
<ul>
<li><code>수치적 미분에서는 N+1번을 계산하여야 했지만, 역전파 알고리즘으로 인해 단 한번의 손실함수 평가로 미분을 구할 수 있다.</code></li>
</ul>
<h2 id="수치-미분을-이용한-심층-신경망-학습"><a href="#수치-미분을-이용한-심층-신경망-학습" class="headerlink" title="수치 미분을 이용한 심층 신경망 학습"></a>수치 미분을 이용한 심층 신경망 학습</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 수치 미분을 이용한 심층 신경망 학습</span></span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"><span class="comment">## 유틸리티 함수</span></span><br><span class="line">epsilon = 0.0001</span><br><span class="line"></span><br><span class="line">def _t(x):</span><br><span class="line">    <span class="built_in">return</span> np.transpose(x)</span><br><span class="line"></span><br><span class="line">def _m(A, B):</span><br><span class="line">    <span class="built_in">return</span> np.matmul(A, B)</span><br><span class="line"></span><br><span class="line">def sigmoid(x):</span><br><span class="line">    <span class="built_in">return</span> 1 / (1 + np.exp(-x))</span><br><span class="line"></span><br><span class="line">def mean_squared_error(h, y):</span><br><span class="line">    <span class="built_in">return</span> 1 / 2 * np.mean(np.square(h - y))</span><br><span class="line"></span><br><span class="line"><span class="comment">## 뉴런 구현</span></span><br><span class="line">class Neuron:</span><br><span class="line">    def __init__(self, W, b, a):</span><br><span class="line">        self.W = W</span><br><span class="line">        self.b = b</span><br><span class="line">        self.a = a</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Gradient</span></span><br><span class="line">        self.dW = np.zeros_like(self.W)</span><br><span class="line">        self.db = np.zeros_like(self.b)</span><br><span class="line"></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        <span class="built_in">return</span> self.a(_m(_t(self.W), x) + self.b) <span class="comment"># activation((W^T)x + b)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 심층신경망 구현</span></span><br><span class="line">class DNN:</span><br><span class="line">    <span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    hidden_depth : hidden_layer의 갯수</span></span><br><span class="line"><span class="string">    num_neuron : hidden_layer 하나당 neuron의 갯수</span></span><br><span class="line"><span class="string">    num_input : input_layer의 neuron의 갯수</span></span><br><span class="line"><span class="string">    num_output : output_layer의 neuron의 갯수</span></span><br><span class="line"><span class="string">    activation : activation funtion으로 사용할 함수</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span></span><br><span class="line">    def __init__(self, hidden_depth, num_neuron, num_input, num_output, activation=sigmoid):</span><br><span class="line">        <span class="comment"># W, b initialize</span></span><br><span class="line">        def init_var(i, o):</span><br><span class="line">            <span class="built_in">return</span> np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))</span><br><span class="line"></span><br><span class="line">        self.sequence = list()</span><br><span class="line">        <span class="comment"># First hidden layer</span></span><br><span class="line">        W, b = init_var(num_input, num_neuron)</span><br><span class="line">        self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Hidden layers</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(hidden_depth - 1):</span><br><span class="line">            W, b = init_var(num_neuron, num_neuron)</span><br><span class="line">            self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Output layer</span></span><br><span class="line">        <span class="comment"># 단순히 심층신경망 구현 후에 수치미분을 사용한 역전파학습을 보이기 위한 코드이므로</span></span><br><span class="line">        <span class="comment"># Output layer의 activation function을 따로 바꾸지 않고 sigmoid로 사용하겠다.</span></span><br><span class="line">        W, b = init_var(num_neuron, num_output)</span><br><span class="line">        self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        <span class="comment"># layer를 call하는 것은 결국 위에서 정의한 Neuron의 call이 될 것이고</span></span><br><span class="line">        <span class="comment"># x는 activation((W^T)x + b)이 될 것이다.</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.sequence:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="built_in">return</span> x</span><br><span class="line"></span><br><span class="line">    def calc_gradient(self, x, y, loss_func):</span><br><span class="line">        def get_new_sequence(layer_index, new_neuron):</span><br><span class="line">        <span class="comment"># 특정한 변수하나(weight나 bias)만 변화를 줘서 그 때 loss가 얼마나 변하는지를 보고</span></span><br><span class="line">        <span class="comment"># numerical gradient를 계산하려하기 때문에 변화된 변수가 있는 새로운 Sequence가 필요하다.</span></span><br><span class="line">            new_sequence = list()</span><br><span class="line">            <span class="keyword">for</span> i, layer <span class="keyword">in</span> enumerate(self.sequence):</span><br><span class="line">                <span class="keyword">if</span> i == layer_index:</span><br><span class="line">                    new_sequence.append(new_neuron)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    new_sequence.append(layer)</span><br><span class="line">            <span class="built_in">return</span> new_sequence</span><br><span class="line"></span><br><span class="line">        def eval_sequence(x, sequence):</span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> sequence:</span><br><span class="line">                x = layer(x)</span><br><span class="line">            <span class="built_in">return</span> x</span><br><span class="line"></span><br><span class="line">        loss = loss_func(self(x), y)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer_id, layer <span class="keyword">in</span> enumerate(self.sequence): <span class="comment"># iterate layer</span></span><br><span class="line">            <span class="keyword">for</span> w_i, w <span class="keyword">in</span> enumerate(layer.W): <span class="comment"># iterate W (row)</span></span><br><span class="line">                <span class="keyword">for</span> w_j, ww <span class="keyword">in</span> enumerate(w): <span class="comment"># iterate W (col)</span></span><br><span class="line">                    W = np.copy(layer.W)</span><br><span class="line">                    W[w_i][w_j] = ww + epsilon</span><br><span class="line"></span><br><span class="line">                    new_neuron = Neuron(W, layer.b, layer.a)</span><br><span class="line">                    new_seq = get_new_sequence(layer_id, new_neuron)</span><br><span class="line">                    h = eval_sequence(x, new_seq)</span><br><span class="line"></span><br><span class="line">                    num_grad = (loss_func(h, y) - loss) / epsilon  <span class="comment"># (f(x+eps) - f(x)) / epsilon</span></span><br><span class="line">                    layer.dW[w_i][w_j] = num_grad</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> b_i, bb <span class="keyword">in</span> enumerate(layer.b): <span class="comment"># iterate b</span></span><br><span class="line">                    b = np.copy(layer.b)</span><br><span class="line">                    b[b_i] = bb + epsilon</span><br><span class="line"></span><br><span class="line">                    new_neuron = Neuron(layer.W, b, layer.a)</span><br><span class="line">                    new_seq = get_new_sequence(layer_id, new_neuron)</span><br><span class="line">                    h = eval_sequence(x, new_seq)</span><br><span class="line"></span><br><span class="line">                    num_grad = (loss_func(h, y) - loss) / epsilon  <span class="comment"># (f(x+eps) - f(x)) / epsilon</span></span><br><span class="line">                    layer.db[b_i] = num_grad</span><br><span class="line">        <span class="comment"># gradient를 계산할 때 loss를 return해야 학습과정에 loss가 어떻게 되는지를 알 수 있기때문에 return 해준다.  </span></span><br><span class="line">        <span class="built_in">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment">## 경사하강법</span></span><br><span class="line">def gradient_descent(network, x, y, loss_obj, alpha=0.01):</span><br><span class="line">    loss = network.calc_gradient(x, y, loss_obj)</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> network.sequence:</span><br><span class="line">        layer.W += -alpha * layer.dW</span><br><span class="line">        layer.b += -alpha * layer.db</span><br><span class="line">    <span class="built_in">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment">## 동작 테스트</span></span><br><span class="line">x = np.random.normal(0.0, 1.0, (10,))</span><br><span class="line">y = np.random.normal(0.0, 1.0, (2,))</span><br><span class="line"></span><br><span class="line">dnn = DNN(hidden_depth=5, num_neuron=32, num_input=10, num_output=2, activation=sigmoid)</span><br><span class="line"></span><br><span class="line">t = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(100):</span><br><span class="line">    loss = gradient_descent(dnn, x, y, mean_squared_error, 0.01)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'Epoch &#123;&#125;: Test loss &#123;&#125;'</span>.format(epoch, loss))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'&#123;&#125; seconds elapsed.'</span>.format(time.time() - t))</span><br></pre></td></tr></table></figure>
<h2 id="역전파-알고리즘을-이용한-심층-신경망-학습"><a href="#역전파-알고리즘을-이용한-심층-신경망-학습" class="headerlink" title="역전파 알고리즘을 이용한 심층 신경망 학습"></a>역전파 알고리즘을 이용한 심층 신경망 학습</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 역전파 학습을 이용한 심층 신경망 학습</span></span><br><span class="line">import time</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"><span class="comment">## 유틸리티 함수</span></span><br><span class="line">def _t(x):</span><br><span class="line">    <span class="built_in">return</span> np.transpose(x)</span><br><span class="line"></span><br><span class="line">def _m(A, B):</span><br><span class="line">    <span class="built_in">return</span> np.matmul(A, B)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Sigmoid 구현</span></span><br><span class="line">class Sigmoid:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="comment"># 곱의 형태로 나오게 되므로 처음에 1로해서 추후에 입력될 수치에 영향을 덜 주게 해준다.</span></span><br><span class="line">        self.last_o = 1</span><br><span class="line"></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        self.last_o =  1 / (1.0 + np.exp(-x))</span><br><span class="line">        <span class="built_in">return</span> self.last_o</span><br><span class="line"></span><br><span class="line">    def grad(self):</span><br><span class="line">        <span class="comment"># sigmoid(x) * (1- sigmoid(x))</span></span><br><span class="line">        <span class="built_in">return</span> self.last_o*(1-self.last_o)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Mean Squared Error 구현</span></span><br><span class="line">class MeanSquaredError:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        <span class="comment"># chain rule을 할 때 MSE로 부터 gradient를 계속해서 가져와야하므로 저장해놓기 위해</span></span><br><span class="line">        self.dh = 1</span><br><span class="line">        self.last_diff = 1</span><br><span class="line"></span><br><span class="line">    def __call__(self, h, y): <span class="comment"># 1/2 * mean((h - y)^2)</span></span><br><span class="line">            self.last_diff = h - y</span><br><span class="line">            <span class="built_in">return</span> 1 / 2 * np.mean(np.square(h - y))</span><br><span class="line"></span><br><span class="line">    def grad(self): <span class="comment"># h - y</span></span><br><span class="line">        <span class="built_in">return</span> self.last_diff</span><br><span class="line"></span><br><span class="line"><span class="comment">## 뉴런 구현</span></span><br><span class="line">class Neuron:</span><br><span class="line">    def __init__(self, W, b, a_obj):</span><br><span class="line">        self.W = W</span><br><span class="line">        self.b = b</span><br><span class="line">        <span class="comment"># activation이 이전과 다르게 class로 작성되었으므로 instanctiation을 해주어야한다.  </span></span><br><span class="line">        self.a = a_obj()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># gradient</span></span><br><span class="line">        self.dW = np.zeros_like(self.W)</span><br><span class="line">        self.db = np.zeros_like(self.b)</span><br><span class="line">        self.dh = np.zeros_like(_t(self.W))</span><br><span class="line"></span><br><span class="line">        <span class="comment">## 아래의 grad_W를 위해 저장해놓는다.</span></span><br><span class="line">        <span class="comment">## W로 미분했을 경우 이전 입력을 갖고 있어야 바로 사용할 수 있으므로</span></span><br><span class="line">        self.last_x = np.zeros((self.W.shape[0]))</span><br><span class="line">        self.last_h = np.zeros((self.W.shape[1]))</span><br><span class="line"></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        self.last_x = x</span><br><span class="line">        self.last_h = _m(_t(self.W), x) + self.b</span><br><span class="line">        <span class="built_in">return</span> self.a(self.last_h)</span><br><span class="line"></span><br><span class="line">    def grad(self): <span class="comment"># dy/dh = W</span></span><br><span class="line">        <span class="built_in">return</span> self.W * self.a.grad()</span><br><span class="line"></span><br><span class="line">    def grad_W(self, dh):</span><br><span class="line">        grad = np.ones_like(self.W)</span><br><span class="line">        grad_a = self.a.grad()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(grad.shape[1]): <span class="comment"># dy/dw = x</span></span><br><span class="line">            grad[:, j] = dh[j] * grad_a[j] * self.last_x</span><br><span class="line">        <span class="built_in">return</span> grad</span><br><span class="line"></span><br><span class="line">    def grad_b(self, dh): <span class="comment"># dy/db = 1</span></span><br><span class="line">        <span class="built_in">return</span> dh * self.a.grad() * 1</span><br><span class="line"></span><br><span class="line"><span class="comment">## 심층신경망 구현</span></span><br><span class="line">class DNN:</span><br><span class="line">    def __init__(self, hidden_depth, num_neuron, input, output, activation=Sigmoid):</span><br><span class="line">        def init_var(i, o):</span><br><span class="line">            <span class="built_in">return</span> np.random.normal(0.0, 0.01, (i, o)), np.zeros((o,))</span><br><span class="line"></span><br><span class="line">        self.sequence = list()</span><br><span class="line">        <span class="comment"># First hidden layer</span></span><br><span class="line">        W, b = init_var(input, num_neuron)</span><br><span class="line">        self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Hidden Layers</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(hidden_depth):</span><br><span class="line">            W, b = init_var(num_neuron, num_neuron)</span><br><span class="line">            self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Output Layer</span></span><br><span class="line">        W, b = init_var(num_neuron, output)</span><br><span class="line">        self.sequence.append(Neuron(W, b, activation))</span><br><span class="line"></span><br><span class="line">    def __call__(self, x):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.sequence:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="built_in">return</span> x</span><br><span class="line"></span><br><span class="line">    def calc_gradient(self, loss_obj):</span><br><span class="line">        loss_obj.dh = loss_obj.grad()</span><br><span class="line">        <span class="comment"># for문에서 한번에 처리하기 위해서 loss object를 넣어준다.</span></span><br><span class="line">        self.sequence.append(loss_obj)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># back_propagation loop</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(self.sequence) -1, 0 , -1):</span><br><span class="line">            l1 = self.sequence[i]</span><br><span class="line">            l0 = self.sequence[i - 1]</span><br><span class="line"></span><br><span class="line">            l0.dh = _m(l0.grad(), l1.dh)</span><br><span class="line">            l0.dw = l0.grad_W(l1.dh)</span><br><span class="line">            l0.db = l0.grad_b(l1.dh)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># loss object가 들어 있으면 출력을 얻지 못하고 loss 만 얻게 될 것이기 때문이다.</span></span><br><span class="line">        self.sequence.remove(loss_obj)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 경사하강 학습법</span></span><br><span class="line">def gradient_descent(network, x, y, loss_obj, alpha=0.01):</span><br><span class="line">    loss = loss_obj(network(x), y)  <span class="comment"># Forward inference</span></span><br><span class="line">    network.calc_gradient(loss_obj)  <span class="comment"># Back-propagation</span></span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> network.sequence:</span><br><span class="line">        layer.W += -alpha * layer.dW</span><br><span class="line">        layer.b += -alpha * layer.db</span><br><span class="line">    <span class="built_in">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="comment">## 동작 테스트</span></span><br><span class="line">x = np.random.normal(0.0, 1.0, (10,))</span><br><span class="line">y = np.random.normal(0.0, 1.0, (2,))</span><br><span class="line"></span><br><span class="line">t = time.time()</span><br><span class="line">dnn = DNN(hidden_depth=5, num_neuron=32, input=10, output=2, activation=Sigmoid)</span><br><span class="line">loss_obj = MeanSquaredError()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(100):</span><br><span class="line">    loss = gradient_descent(dnn, x, y, loss_obj, alpha=0.01)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'Epoch &#123;&#125;: Test loss &#123;&#125;'</span>.format(epoch, loss))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'&#123;&#125; seconds elapsed.'</span>.format(time.time() - t))</span><br></pre></td></tr></table></figure>

        </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 하단형 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="9861011486"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

        <footer class="article-footer">
            


    <div class="a2a_kit a2a_default_style">
    <a class="a2a_dd" href="https://www.addtoany.com/share">Share</a>
    <span class="a2a_divider"></span>
    <a class="a2a_button_facebook"></a>
    <a class="a2a_button_twitter"></a>
    <a class="a2a_button_google_plus"></a>
    <a class="a2a_button_pinterest"></a>
    <a class="a2a_button_tumblr"></a>
</div>
<script type="text/javascript" src="//static.addtoany.com/menu/page.js"></script>
<style>
    .a2a_menu {
        border-radius: 4px;
    }
    .a2a_menu a {
        margin: 2px 0;
        font-size: 14px;
        line-height: 16px;
        border-radius: 4px;
        color: inherit !important;
        font-family: 'Microsoft Yahei';
    }
    #a2apage_dropdown {
        margin: 10px 0;
    }
    .a2a_mini_services {
        padding: 10px;
    }
    a.a2a_i,
    i.a2a_i {
        width: 122px;
        line-height: 16px;
    }
    a.a2a_i .a2a_svg,
    a.a2a_more .a2a_svg {
        width: 16px;
        height: 16px;
        line-height: 16px;
        vertical-align: top;
        background-size: 16px;
    }
    a.a2a_i {
        border: none !important;
    }
    a.a2a_menu_show_more_less {
        margin: 0;
        padding: 10px 0;
        line-height: 16px;
    }
    .a2a_mini_services:after{content:".";display:block;height:0;clear:both;visibility:hidden}
    .a2a_mini_services{*+height:1%;}
</style>


        </footer>
    </div>
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BlogPosting",
        "author": {
            "@type": "Person",
            "name": "HeungBae Lee"
        },
        "headline": "심층 신경망의 구조",
        "image": "https://heung-bae-lee.github.io/image/Neuron_01.png",
        "keywords": "",
        "genre": "deep learning",
        "datePublished": "2019-12-08",
        "dateCreated": "2019-12-08",
        "dateModified": "2019-12-20",
        "url": "https://heung-bae-lee.github.io/2019/12/08/deep_learning_03/",
        "description": "심층 신경망의 구조




은닉 계층 추가 = 특징의 비선형 변환 추가!!선형 변환의 이해

선형대수의 선형 변환을 함수의 개념에서 보았을때, 입력 차원(n)이 출력 차원(m)보다 크다면 Onto(전사함수: 모든 공역이 치역이 되있는 상태)가 될 수 있지만, 그 반대인 경우는 적은 차원을 갖는 입력벡터의 차원으로 일부분의 출력 벡터의 차원을 커버하는 것이 "
        "wordCount": 3425
    }
</script>

</article>

    <section id="comments">
    
        
    <div id="disqus_thread">
        <noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>

    
    </section>



                        </div>
                    </section>
                    <aside id="sidebar">
    <a class="sidebar-toggle" title="Expand Sidebar"><i class="toggle icon"></i></a>
    <div class="sidebar-top">
        <p>follow:</p>
        <ul class="social-links">
            
                
                <li>
                    <a class="social-tooltip" title="facebook" href="https://www.facebook.com/heungbae.lee" target="_blank" rel="noopener">
                        <i class="icon fa fa-facebook"></i>
                    </a>
                </li>
                
            
                
                <li>
                    <a class="social-tooltip" title="github" href="https://github.com/HEUNG-BAE-LEE" target="_blank" rel="noopener">
                        <i class="icon fa fa-github"></i>
                    </a>
                </li>
                
            
        </ul>
    </div>
    
        
<nav id="article-nav">
    
        <a href="/2019/12/09/data_engineering_01/" id="article-nav-newer" class="article-nav-link-wrap">
        <strong class="article-nav-caption">newer</strong>
        <p class="article-nav-title">
        
            data engineering basic(Unix환경 및 커맨드)
        
        </p>
        <i class="icon fa fa-chevron-right" id="icon-chevron-right"></i>
    </a>
    
    
        <a href="/2019/12/08/deep_learning_02/" id="article-nav-older" class="article-nav-link-wrap">
        <strong class="article-nav-caption">older</strong>
        <p class="article-nav-title">쉽게 배우는 경사하강 학습법</p>
        <i class="icon fa fa-chevron-left" id="icon-chevron-left"></i>
        </a>
    
</nav>

    
    <div class="widgets-container">
        
            
                

            
                
    <div class="widget-wrap">
        <h3 class="widget-title">recents</h3>
        <div class="widget">
            <ul id="recent-post" class="no-thumbnail">
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/05/13/linear_algebra_02/" class="title">선형 시스템(Linear system)</a></p>
                            <p class="item-date"><time datetime="2020-05-13T07:50:33.000Z" itemprop="datePublished">2020-05-13</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/linear-algebra/">linear algebra</a></p>
                            <p class="item-title"><a href="/2020/05/12/linear_algebra_01/" class="title">선형대수 요소(Elements in linear algebra)</a></p>
                            <p class="item-date"><time datetime="2020-05-12T13:41:59.000Z" itemprop="datePublished">2020-05-12</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/machine-learning/">machine learning</a></p>
                            <p class="item-title"><a href="/2020/05/02/machine_learning_14/" class="title">Ensemble Learning - 01</a></p>
                            <p class="item-date"><time datetime="2020-05-02T12:00:10.000Z" itemprop="datePublished">2020-05-02</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></p>
                            <p class="item-title"><a href="/2020/05/02/data_structure_06/" class="title">내가 정리하는 자료구조 05 - 트리(Tree)</a></p>
                            <p class="item-date"><time datetime="2020-05-02T10:27:21.000Z" itemprop="datePublished">2020-05-02</time></p>
                        </div>
                    </li>
                
                    <li>
                        
                        <div class="item-inner">
                            <p class="item-category"><a class="article-category-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a></p>
                            <p class="item-title"><a href="/2020/04/30/data_structure_05/" class="title">내가 정리하는 자료구조 04 - 해쉬 테이블</a></p>
                            <p class="item-date"><time datetime="2020-04-30T14:32:54.000Z" itemprop="datePublished">2020-04-30</time></p>
                        </div>
                    </li>
                
            </ul>
        </div>
    </div>

            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">categories</h3>
        <div class="widget">
            <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Bayes/">Bayes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/C-C-자료구조/">C/C++/자료구조</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/CS231n/">CS231n</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Front-end/">Front end</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">14</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Recommendation-System/">Recommendation System</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Statistics-Mathematical-Statistics/">Statistics - Mathematical Statistics</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/crawling/">crawling</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/data-engineering/">data engineering</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/growth-hacking/">growth hacking</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linear-algebra/">linear algebra</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">15</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">archives</h3>
        <div class="widget">
            <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">May 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">April 2020</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">March 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">January 2020</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/08/">August 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/07/">July 2019</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">tags</h3>
        <div class="widget">
            <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CS231n/">CS231n</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/crawling/">crawling</a><span class="tag-list-count">2</span></li></ul>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-float">
        <h3 class="widget-title">tag cloud</h3>
        <div class="widget tagcloud">
            <a href="/tags/CS231n/" style="font-size: 10px;">CS231n</a> <a href="/tags/crawling/" style="font-size: 20px;">crawling</a>
        </div>
    </div>


            
                
    <div class="widget-wrap widget-list">
        <h3 class="widget-title">links</h3>
        <div class="widget">
            <ul>
                
                    <li>
                        <a href="http://hexo.io">Hexo</a>
                    </li>
                
            </ul>
        </div>
    </div>


            
        
    </div>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- 사이드바 -->
<ins class="adsbygoogle"
     style="display:block"
     data-ad-client="ca-pub-4604833066889492"
     data-ad-slot="3275421833"
     data-ad-format="auto"
     data-full-width-responsive="true"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>

</aside>

                </div>
            </div>
        </div>
        <footer id="footer">
    <div class="container">
        <div class="container-inner">
            <a id="back-to-top" href="javascript:;"><i class="icon fa fa-angle-up"></i></a>
            <div class="credit">
                <h1 class="logo-wrap">
                    <a href="/" class="logo"></a>
                </h1>
                <p>&copy; 2020 HeungBae Lee</p>
                <p>Powered by <a href="//hexo.io/" target="_blank">Hexo</a>. Theme by <a href="//github.com/ppoffice" target="_blank">PPOffice</a></p>
            </div>
            <div class="footer-plugins">
              
    


            </div>
        </div>
    </div>
</footer>

        
    
    <script>
    var disqus_shortname = 'hexo-theme-hueman';
    
    
    var disqus_url = 'https://heung-bae-lee.github.io/2019/12/08/deep_learning_03/';
    
    (function() {
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
    </script>




    
        <script src="/libs/lightgallery/js/lightgallery.min.js"></script>
        <script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>
        <script src="/libs/lightgallery/js/lg-pager.min.js"></script>
        <script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>
        <script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>
        <script src="/libs/lightgallery/js/lg-zoom.min.js"></script>
        <script src="/libs/lightgallery/js/lg-hash.min.js"></script>
        <script src="/libs/lightgallery/js/lg-share.min.js"></script>
        <script src="/libs/lightgallery/js/lg-video.min.js"></script>
    
    
        <script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>
    
    
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    



<!-- Custom Scripts -->
<script src="/js/main.js"></script>

    </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</body>
</html>
